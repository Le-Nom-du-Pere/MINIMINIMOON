<html>
<head>
<title>_torch_docs.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #969896;}
.s1 { color: #333333;}
.s2 { color: #a71d5d;}
.s3 { color: #63a35c;}
.s4 { color: #183691;}
.s5 { color: #0086b3;}
.ln { color: #333333; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_torch_docs.py</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0"># mypy: allow-untyped-defs</span>
<a name="l2"><span class="ln">2    </span></a><span class="s0">&quot;&quot;&quot;Adds docstrings to functions defined in the torch._C module.&quot;&quot;&quot;</span>
<a name="l3"><span class="ln">3    </span></a>
<a name="l4"><span class="ln">4    </span></a><span class="s2">import </span><span class="s1">re</span>
<a name="l5"><span class="ln">5    </span></a>
<a name="l6"><span class="ln">6    </span></a><span class="s2">import </span><span class="s1">torch</span><span class="s3">.</span><span class="s1">_C</span>
<a name="l7"><span class="ln">7    </span></a><span class="s2">from </span><span class="s1">torch</span><span class="s3">.</span><span class="s1">_C </span><span class="s2">import </span><span class="s1">_add_docstr </span><span class="s2">as </span><span class="s1">add_docstr</span>
<a name="l8"><span class="ln">8    </span></a>
<a name="l9"><span class="ln">9    </span></a>
<a name="l10"><span class="ln">10   </span></a><span class="s2">def </span><span class="s1">parse_kwargs</span><span class="s3">(</span><span class="s1">desc</span><span class="s3">)</span><span class="s2">:</span>
<a name="l11"><span class="ln">11   </span></a>    <span class="s0">r&quot;&quot;&quot;Map a description of args to a dictionary of {argname: description}. 
<a name="l12"><span class="ln">12   </span></a> 
<a name="l13"><span class="ln">13   </span></a>    Input: 
<a name="l14"><span class="ln">14   </span></a>        ('    weight (Tensor): a weight tensor\n' + 
<a name="l15"><span class="ln">15   </span></a>         '        Some optional description') 
<a name="l16"><span class="ln">16   </span></a>    Output: { 
<a name="l17"><span class="ln">17   </span></a>        'weight': \ 
<a name="l18"><span class="ln">18   </span></a>        'weight (Tensor): a weight tensor\n        Some optional description' 
<a name="l19"><span class="ln">19   </span></a>    } 
<a name="l20"><span class="ln">20   </span></a>    &quot;&quot;&quot;</span>
<a name="l21"><span class="ln">21   </span></a>    <span class="s0"># Split on exactly 4 spaces after a newline</span>
<a name="l22"><span class="ln">22   </span></a>    <span class="s1">regx </span><span class="s2">= </span><span class="s1">re</span><span class="s3">.</span><span class="s1">compile</span><span class="s3">(</span><span class="s4">r&quot;\n\s{4}(?!\s)&quot;</span><span class="s3">)</span>
<a name="l23"><span class="ln">23   </span></a>    <span class="s1">kwargs </span><span class="s2">= </span><span class="s3">[</span><span class="s1">section</span><span class="s3">.</span><span class="s1">strip</span><span class="s3">() </span><span class="s2">for </span><span class="s1">section </span><span class="s2">in </span><span class="s1">regx</span><span class="s3">.</span><span class="s1">split</span><span class="s3">(</span><span class="s1">desc</span><span class="s3">)]</span>
<a name="l24"><span class="ln">24   </span></a>    <span class="s1">kwargs </span><span class="s2">= </span><span class="s3">[</span><span class="s1">section </span><span class="s2">for </span><span class="s1">section </span><span class="s2">in </span><span class="s1">kwargs </span><span class="s2">if </span><span class="s1">len</span><span class="s3">(</span><span class="s1">section</span><span class="s3">) </span><span class="s2">&gt; </span><span class="s5">0</span><span class="s3">]</span>
<a name="l25"><span class="ln">25   </span></a>    <span class="s2">return </span><span class="s3">{</span><span class="s1">desc</span><span class="s3">.</span><span class="s1">split</span><span class="s3">(</span><span class="s4">&quot; &quot;</span><span class="s3">)[</span><span class="s5">0</span><span class="s3">]</span><span class="s2">: </span><span class="s1">desc </span><span class="s2">for </span><span class="s1">desc </span><span class="s2">in </span><span class="s1">kwargs</span><span class="s3">}</span>
<a name="l26"><span class="ln">26   </span></a>
<a name="l27"><span class="ln">27   </span></a>
<a name="l28"><span class="ln">28   </span></a><span class="s2">def </span><span class="s1">merge_dicts</span><span class="s3">(</span><span class="s2">*</span><span class="s1">dicts</span><span class="s3">)</span><span class="s2">:</span>
<a name="l29"><span class="ln">29   </span></a>    <span class="s0">&quot;&quot;&quot;Merge dictionaries into a single dictionary.&quot;&quot;&quot;</span>
<a name="l30"><span class="ln">30   </span></a>    <span class="s2">return </span><span class="s3">{</span><span class="s1">x</span><span class="s2">: </span><span class="s1">d</span><span class="s3">[</span><span class="s1">x</span><span class="s3">] </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">dicts </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">d</span><span class="s3">}</span>
<a name="l31"><span class="ln">31   </span></a>
<a name="l32"><span class="ln">32   </span></a>
<a name="l33"><span class="ln">33   </span></a><span class="s1">common_args </span><span class="s2">= </span><span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l34"><span class="ln">34   </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l35"><span class="ln">35   </span></a>    input (Tensor): the input tensor. 
<a name="l36"><span class="ln">36   </span></a>    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l37"><span class="ln">37   </span></a>    out (Tensor, optional): the output tensor. 
<a name="l38"><span class="ln">38   </span></a>    memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l39"><span class="ln">39   </span></a>        returned tensor. Default: ``torch.preserve_format``. 
<a name="l40"><span class="ln">40   </span></a>&quot;&quot;&quot;</span>
<a name="l41"><span class="ln">41   </span></a><span class="s3">)</span>
<a name="l42"><span class="ln">42   </span></a>
<a name="l43"><span class="ln">43   </span></a><span class="s1">reduceops_common_args </span><span class="s2">= </span><span class="s1">merge_dicts</span><span class="s3">(</span>
<a name="l44"><span class="ln">44   </span></a>    <span class="s1">common_args</span><span class="s3">,</span>
<a name="l45"><span class="ln">45   </span></a>    <span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l46"><span class="ln">46   </span></a>        <span class="s4">&quot;&quot;&quot; 
<a name="l47"><span class="ln">47   </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l48"><span class="ln">48   </span></a>        If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l49"><span class="ln">49   </span></a>        is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l50"><span class="ln">50   </span></a>    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. 
<a name="l51"><span class="ln">51   </span></a>&quot;&quot;&quot;</span>
<a name="l52"><span class="ln">52   </span></a>    <span class="s3">),</span>
<a name="l53"><span class="ln">53   </span></a>    <span class="s3">{</span>
<a name="l54"><span class="ln">54   </span></a>        <span class="s4">&quot;opt_keepdim&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot; 
<a name="l55"><span class="ln">55   </span></a>    keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l56"><span class="ln">56   </span></a>&quot;&quot;&quot;</span>
<a name="l57"><span class="ln">57   </span></a>    <span class="s3">},</span>
<a name="l58"><span class="ln">58   </span></a><span class="s3">)</span>
<a name="l59"><span class="ln">59   </span></a>
<a name="l60"><span class="ln">60   </span></a><span class="s1">multi_dim_common </span><span class="s2">= </span><span class="s1">merge_dicts</span><span class="s3">(</span>
<a name="l61"><span class="ln">61   </span></a>    <span class="s1">reduceops_common_args</span><span class="s3">,</span>
<a name="l62"><span class="ln">62   </span></a>    <span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l63"><span class="ln">63   </span></a>        <span class="s4">&quot;&quot;&quot; 
<a name="l64"><span class="ln">64   </span></a>    dim (int or tuple of ints): the dimension or dimensions to reduce. 
<a name="l65"><span class="ln">65   </span></a>&quot;&quot;&quot;</span>
<a name="l66"><span class="ln">66   </span></a>    <span class="s3">),</span>
<a name="l67"><span class="ln">67   </span></a>    <span class="s3">{</span>
<a name="l68"><span class="ln">68   </span></a>        <span class="s4">&quot;keepdim_details&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot; 
<a name="l69"><span class="ln">69   </span></a>If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l70"><span class="ln">70   </span></a>as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l71"><span class="ln">71   </span></a>Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l72"><span class="ln">72   </span></a>output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l73"><span class="ln">73   </span></a>&quot;&quot;&quot;</span>
<a name="l74"><span class="ln">74   </span></a>    <span class="s3">},</span>
<a name="l75"><span class="ln">75   </span></a>    <span class="s3">{</span>
<a name="l76"><span class="ln">76   </span></a>        <span class="s4">&quot;opt_dim&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot; 
<a name="l77"><span class="ln">77   </span></a>    dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l78"><span class="ln">78   </span></a>&quot;&quot;&quot;</span>
<a name="l79"><span class="ln">79   </span></a>    <span class="s3">},</span>
<a name="l80"><span class="ln">80   </span></a>    <span class="s3">{</span>
<a name="l81"><span class="ln">81   </span></a>        <span class="s4">&quot;opt_dim_all_reduce&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot; 
<a name="l82"><span class="ln">82   </span></a>    dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l83"><span class="ln">83   </span></a>        If ``None``, all dimensions are reduced. 
<a name="l84"><span class="ln">84   </span></a>&quot;&quot;&quot;</span>
<a name="l85"><span class="ln">85   </span></a>    <span class="s3">},</span>
<a name="l86"><span class="ln">86   </span></a><span class="s3">)</span>
<a name="l87"><span class="ln">87   </span></a>
<a name="l88"><span class="ln">88   </span></a><span class="s1">single_dim_common </span><span class="s2">= </span><span class="s1">merge_dicts</span><span class="s3">(</span>
<a name="l89"><span class="ln">89   </span></a>    <span class="s1">reduceops_common_args</span><span class="s3">,</span>
<a name="l90"><span class="ln">90   </span></a>    <span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l91"><span class="ln">91   </span></a>        <span class="s4">&quot;&quot;&quot; 
<a name="l92"><span class="ln">92   </span></a>    dim (int): the dimension to reduce. 
<a name="l93"><span class="ln">93   </span></a>&quot;&quot;&quot;</span>
<a name="l94"><span class="ln">94   </span></a>    <span class="s3">),</span>
<a name="l95"><span class="ln">95   </span></a>    <span class="s3">{</span>
<a name="l96"><span class="ln">96   </span></a>        <span class="s4">&quot;opt_dim&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot; 
<a name="l97"><span class="ln">97   </span></a>    dim (int, optional): the dimension to reduce. 
<a name="l98"><span class="ln">98   </span></a>&quot;&quot;&quot;</span>
<a name="l99"><span class="ln">99   </span></a>    <span class="s3">},</span>
<a name="l100"><span class="ln">100  </span></a>    <span class="s3">{</span>
<a name="l101"><span class="ln">101  </span></a>        <span class="s4">&quot;opt_dim_all_reduce&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot; 
<a name="l102"><span class="ln">102  </span></a>    dim (int, optional): the dimension to reduce. 
<a name="l103"><span class="ln">103  </span></a>        If ``None``, all dimensions are reduced. 
<a name="l104"><span class="ln">104  </span></a>&quot;&quot;&quot;</span>
<a name="l105"><span class="ln">105  </span></a>    <span class="s3">},</span>
<a name="l106"><span class="ln">106  </span></a>    <span class="s3">{</span>
<a name="l107"><span class="ln">107  </span></a>        <span class="s4">&quot;keepdim_details&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot;If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l108"><span class="ln">108  </span></a>as :attr:`input` except in the dimension :attr:`dim` where it is of size 1. 
<a name="l109"><span class="ln">109  </span></a>Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l110"><span class="ln">110  </span></a>the output tensor having 1 fewer dimension than :attr:`input`.&quot;&quot;&quot;</span>
<a name="l111"><span class="ln">111  </span></a>    <span class="s3">},</span>
<a name="l112"><span class="ln">112  </span></a><span class="s3">)</span>
<a name="l113"><span class="ln">113  </span></a>
<a name="l114"><span class="ln">114  </span></a><span class="s1">factory_common_args </span><span class="s2">= </span><span class="s1">merge_dicts</span><span class="s3">(</span>
<a name="l115"><span class="ln">115  </span></a>    <span class="s1">common_args</span><span class="s3">,</span>
<a name="l116"><span class="ln">116  </span></a>    <span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l117"><span class="ln">117  </span></a>        <span class="s4">&quot;&quot;&quot; 
<a name="l118"><span class="ln">118  </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l119"><span class="ln">119  </span></a>        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l120"><span class="ln">120  </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l121"><span class="ln">121  </span></a>        Default: ``torch.strided``. 
<a name="l122"><span class="ln">122  </span></a>    device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l123"><span class="ln">123  </span></a>        Default: if ``None``, uses the current device for the default tensor type 
<a name="l124"><span class="ln">124  </span></a>        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l125"><span class="ln">125  </span></a>        for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l126"><span class="ln">126  </span></a>    requires_grad (bool, optional): If autograd should record operations on the 
<a name="l127"><span class="ln">127  </span></a>        returned tensor. Default: ``False``. 
<a name="l128"><span class="ln">128  </span></a>    pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l129"><span class="ln">129  </span></a>        the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l130"><span class="ln">130  </span></a>    memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l131"><span class="ln">131  </span></a>        returned Tensor. Default: ``torch.contiguous_format``. 
<a name="l132"><span class="ln">132  </span></a>    check_invariants (bool, optional): If sparse tensor invariants are checked. 
<a name="l133"><span class="ln">133  </span></a>        Default: as returned by :func:`torch.sparse.check_sparse_tensor_invariants.is_enabled`, 
<a name="l134"><span class="ln">134  </span></a>        initially False. 
<a name="l135"><span class="ln">135  </span></a>&quot;&quot;&quot;</span>
<a name="l136"><span class="ln">136  </span></a>    <span class="s3">),</span>
<a name="l137"><span class="ln">137  </span></a>    <span class="s3">{</span>
<a name="l138"><span class="ln">138  </span></a>        <span class="s4">&quot;sparse_factory_device_note&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot;\ 
<a name="l139"><span class="ln">139  </span></a>.. note:: 
<a name="l140"><span class="ln">140  </span></a> 
<a name="l141"><span class="ln">141  </span></a>   If the ``device`` argument is not specified the device of the given 
<a name="l142"><span class="ln">142  </span></a>   :attr:`values` and indices tensor(s) must match. If, however, the 
<a name="l143"><span class="ln">143  </span></a>   argument is specified the input Tensors will be converted to the 
<a name="l144"><span class="ln">144  </span></a>   given device and in turn determine the device of the constructed 
<a name="l145"><span class="ln">145  </span></a>   sparse tensor.&quot;&quot;&quot;</span>
<a name="l146"><span class="ln">146  </span></a>    <span class="s3">},</span>
<a name="l147"><span class="ln">147  </span></a><span class="s3">)</span>
<a name="l148"><span class="ln">148  </span></a>
<a name="l149"><span class="ln">149  </span></a><span class="s1">factory_like_common_args </span><span class="s2">= </span><span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l150"><span class="ln">150  </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l151"><span class="ln">151  </span></a>    input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l152"><span class="ln">152  </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l153"><span class="ln">153  </span></a>        Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l154"><span class="ln">154  </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l155"><span class="ln">155  </span></a>        Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l156"><span class="ln">156  </span></a>    device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l157"><span class="ln">157  </span></a>        Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l158"><span class="ln">158  </span></a>    requires_grad (bool, optional): If autograd should record operations on the 
<a name="l159"><span class="ln">159  </span></a>        returned tensor. Default: ``False``. 
<a name="l160"><span class="ln">160  </span></a>    pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l161"><span class="ln">161  </span></a>        the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l162"><span class="ln">162  </span></a>    memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l163"><span class="ln">163  </span></a>        returned Tensor. Default: ``torch.preserve_format``. 
<a name="l164"><span class="ln">164  </span></a>&quot;&quot;&quot;</span>
<a name="l165"><span class="ln">165  </span></a><span class="s3">)</span>
<a name="l166"><span class="ln">166  </span></a>
<a name="l167"><span class="ln">167  </span></a><span class="s1">factory_data_common_args </span><span class="s2">= </span><span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l168"><span class="ln">168  </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l169"><span class="ln">169  </span></a>    data (array_like): Initial data for the tensor. Can be a list, tuple, 
<a name="l170"><span class="ln">170  </span></a>        NumPy ``ndarray``, scalar, and other types. 
<a name="l171"><span class="ln">171  </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l172"><span class="ln">172  </span></a>        Default: if ``None``, infers data type from :attr:`data`. 
<a name="l173"><span class="ln">173  </span></a>    device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l174"><span class="ln">174  </span></a>        Default: if ``None``, uses the current device for the default tensor type 
<a name="l175"><span class="ln">175  </span></a>        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l176"><span class="ln">176  </span></a>        for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l177"><span class="ln">177  </span></a>    requires_grad (bool, optional): If autograd should record operations on the 
<a name="l178"><span class="ln">178  </span></a>        returned tensor. Default: ``False``. 
<a name="l179"><span class="ln">179  </span></a>    pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l180"><span class="ln">180  </span></a>        the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l181"><span class="ln">181  </span></a>&quot;&quot;&quot;</span>
<a name="l182"><span class="ln">182  </span></a><span class="s3">)</span>
<a name="l183"><span class="ln">183  </span></a>
<a name="l184"><span class="ln">184  </span></a><span class="s1">tf32_notes </span><span class="s2">= </span><span class="s3">{</span>
<a name="l185"><span class="ln">185  </span></a>    <span class="s4">&quot;tf32_note&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot;This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.&quot;&quot;&quot;</span>
<a name="l186"><span class="ln">186  </span></a><span class="s3">}</span>
<a name="l187"><span class="ln">187  </span></a>
<a name="l188"><span class="ln">188  </span></a><span class="s1">rocm_fp16_notes </span><span class="s2">= </span><span class="s3">{</span>
<a name="l189"><span class="ln">189  </span></a>    <span class="s4">&quot;rocm_fp16_note&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot;On certain ROCm devices, when using float16 inputs this module will use \ 
<a name="l190"><span class="ln">190  </span></a>:ref:`different precision&lt;fp16_on_mi200&gt;` for backward.&quot;&quot;&quot;</span>
<a name="l191"><span class="ln">191  </span></a><span class="s3">}</span>
<a name="l192"><span class="ln">192  </span></a>
<a name="l193"><span class="ln">193  </span></a><span class="s1">reproducibility_notes</span><span class="s2">: </span><span class="s1">dict</span><span class="s3">[</span><span class="s1">str</span><span class="s3">, </span><span class="s1">str</span><span class="s3">] </span><span class="s2">= </span><span class="s3">{</span>
<a name="l194"><span class="ln">194  </span></a>    <span class="s4">&quot;forward_reproducibility_note&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot;This operation may behave nondeterministically when given tensors on \ 
<a name="l195"><span class="ln">195  </span></a>a CUDA device. See :doc:`/notes/randomness` for more information.&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l196"><span class="ln">196  </span></a>    <span class="s4">&quot;backward_reproducibility_note&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot;This operation may produce nondeterministic gradients when given tensors on \ 
<a name="l197"><span class="ln">197  </span></a>a CUDA device. See :doc:`/notes/randomness` for more information.&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l198"><span class="ln">198  </span></a>    <span class="s4">&quot;cudnn_reproducibility_note&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot;In some circumstances when given tensors on a CUDA device \ 
<a name="l199"><span class="ln">199  </span></a>and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is \ 
<a name="l200"><span class="ln">200  </span></a>undesirable, you can try to make the operation deterministic (potentially at \ 
<a name="l201"><span class="ln">201  </span></a>a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. \ 
<a name="l202"><span class="ln">202  </span></a>See :doc:`/notes/randomness` for more information.&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l203"><span class="ln">203  </span></a><span class="s3">}</span>
<a name="l204"><span class="ln">204  </span></a>
<a name="l205"><span class="ln">205  </span></a><span class="s1">sparse_support_notes </span><span class="s2">= </span><span class="s3">{</span>
<a name="l206"><span class="ln">206  </span></a>    <span class="s4">&quot;sparse_beta_warning&quot;</span><span class="s2">: </span><span class="s4">&quot;&quot;&quot; 
<a name="l207"><span class="ln">207  </span></a>.. warning:: 
<a name="l208"><span class="ln">208  </span></a>    Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l209"><span class="ln">209  </span></a>    or may not have autograd support. If you notice missing functionality please 
<a name="l210"><span class="ln">210  </span></a>    open a feature request.&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l211"><span class="ln">211  </span></a><span class="s3">}</span>
<a name="l212"><span class="ln">212  </span></a>
<a name="l213"><span class="ln">213  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l214"><span class="ln">214  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">abs</span><span class="s3">,</span>
<a name="l215"><span class="ln">215  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l216"><span class="ln">216  </span></a>abs(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l217"><span class="ln">217  </span></a> 
<a name="l218"><span class="ln">218  </span></a>Computes the absolute value of each element in :attr:`input`. 
<a name="l219"><span class="ln">219  </span></a> 
<a name="l220"><span class="ln">220  </span></a>.. math:: 
<a name="l221"><span class="ln">221  </span></a>    \text{out}_{i} = |\text{input}_{i}| 
<a name="l222"><span class="ln">222  </span></a>&quot;&quot;&quot;</span>
<a name="l223"><span class="ln">223  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l224"><span class="ln">224  </span></a>Args: 
<a name="l225"><span class="ln">225  </span></a>    {input} 
<a name="l226"><span class="ln">226  </span></a> 
<a name="l227"><span class="ln">227  </span></a>Keyword args: 
<a name="l228"><span class="ln">228  </span></a>    {out} 
<a name="l229"><span class="ln">229  </span></a> 
<a name="l230"><span class="ln">230  </span></a>Example:: 
<a name="l231"><span class="ln">231  </span></a> 
<a name="l232"><span class="ln">232  </span></a>    &gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3])) 
<a name="l233"><span class="ln">233  </span></a>    tensor([ 1,  2,  3]) 
<a name="l234"><span class="ln">234  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l235"><span class="ln">235  </span></a><span class="s3">)</span>
<a name="l236"><span class="ln">236  </span></a>
<a name="l237"><span class="ln">237  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l238"><span class="ln">238  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">absolute</span><span class="s3">,</span>
<a name="l239"><span class="ln">239  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l240"><span class="ln">240  </span></a>absolute(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l241"><span class="ln">241  </span></a> 
<a name="l242"><span class="ln">242  </span></a>Alias for :func:`torch.abs` 
<a name="l243"><span class="ln">243  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l244"><span class="ln">244  </span></a><span class="s3">)</span>
<a name="l245"><span class="ln">245  </span></a>
<a name="l246"><span class="ln">246  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l247"><span class="ln">247  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">acos</span><span class="s3">,</span>
<a name="l248"><span class="ln">248  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l249"><span class="ln">249  </span></a>acos(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l250"><span class="ln">250  </span></a> 
<a name="l251"><span class="ln">251  </span></a>Computes the inverse cosine of each element in :attr:`input`. 
<a name="l252"><span class="ln">252  </span></a> 
<a name="l253"><span class="ln">253  </span></a>.. math:: 
<a name="l254"><span class="ln">254  </span></a>    \text{out}_{i} = \cos^{-1}(\text{input}_{i}) 
<a name="l255"><span class="ln">255  </span></a>&quot;&quot;&quot;</span>
<a name="l256"><span class="ln">256  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l257"><span class="ln">257  </span></a>Args: 
<a name="l258"><span class="ln">258  </span></a>    {input} 
<a name="l259"><span class="ln">259  </span></a> 
<a name="l260"><span class="ln">260  </span></a>Keyword args: 
<a name="l261"><span class="ln">261  </span></a>    {out} 
<a name="l262"><span class="ln">262  </span></a> 
<a name="l263"><span class="ln">263  </span></a>Example:: 
<a name="l264"><span class="ln">264  </span></a> 
<a name="l265"><span class="ln">265  </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l266"><span class="ln">266  </span></a>    &gt;&gt;&gt; a 
<a name="l267"><span class="ln">267  </span></a>    tensor([ 0.3348, -0.5889,  0.2005, -0.1584]) 
<a name="l268"><span class="ln">268  </span></a>    &gt;&gt;&gt; torch.acos(a) 
<a name="l269"><span class="ln">269  </span></a>    tensor([ 1.2294,  2.2004,  1.3690,  1.7298]) 
<a name="l270"><span class="ln">270  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l271"><span class="ln">271  </span></a><span class="s3">)</span>
<a name="l272"><span class="ln">272  </span></a>
<a name="l273"><span class="ln">273  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l274"><span class="ln">274  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arccos</span><span class="s3">,</span>
<a name="l275"><span class="ln">275  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l276"><span class="ln">276  </span></a>arccos(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l277"><span class="ln">277  </span></a> 
<a name="l278"><span class="ln">278  </span></a>Alias for :func:`torch.acos`. 
<a name="l279"><span class="ln">279  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l280"><span class="ln">280  </span></a><span class="s3">)</span>
<a name="l281"><span class="ln">281  </span></a>
<a name="l282"><span class="ln">282  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l283"><span class="ln">283  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">acosh</span><span class="s3">,</span>
<a name="l284"><span class="ln">284  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l285"><span class="ln">285  </span></a>acosh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l286"><span class="ln">286  </span></a> 
<a name="l287"><span class="ln">287  </span></a>Returns a new tensor with the inverse hyperbolic cosine of the elements of :attr:`input`. 
<a name="l288"><span class="ln">288  </span></a> 
<a name="l289"><span class="ln">289  </span></a>.. math:: 
<a name="l290"><span class="ln">290  </span></a>    \text{out}_{i} = \cosh^{-1}(\text{input}_{i}) 
<a name="l291"><span class="ln">291  </span></a> 
<a name="l292"><span class="ln">292  </span></a>Note: 
<a name="l293"><span class="ln">293  </span></a>    The domain of the inverse hyperbolic cosine is `[1, inf)` and values outside this range 
<a name="l294"><span class="ln">294  </span></a>    will be mapped to ``NaN``, except for `+ INF` for which the output is mapped to `+ INF`. 
<a name="l295"><span class="ln">295  </span></a>&quot;&quot;&quot;</span>
<a name="l296"><span class="ln">296  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l297"><span class="ln">297  </span></a>Args: 
<a name="l298"><span class="ln">298  </span></a>    {input} 
<a name="l299"><span class="ln">299  </span></a> 
<a name="l300"><span class="ln">300  </span></a>Keyword arguments: 
<a name="l301"><span class="ln">301  </span></a>    {out} 
<a name="l302"><span class="ln">302  </span></a> 
<a name="l303"><span class="ln">303  </span></a>Example:: 
<a name="l304"><span class="ln">304  </span></a> 
<a name="l305"><span class="ln">305  </span></a>    &gt;&gt;&gt; a = torch.randn(4).uniform_(1, 2) 
<a name="l306"><span class="ln">306  </span></a>    &gt;&gt;&gt; a 
<a name="l307"><span class="ln">307  </span></a>    tensor([ 1.3192, 1.9915, 1.9674, 1.7151 ]) 
<a name="l308"><span class="ln">308  </span></a>    &gt;&gt;&gt; torch.acosh(a) 
<a name="l309"><span class="ln">309  </span></a>    tensor([ 0.7791, 1.3120, 1.2979, 1.1341 ]) 
<a name="l310"><span class="ln">310  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l311"><span class="ln">311  </span></a><span class="s3">)</span>
<a name="l312"><span class="ln">312  </span></a>
<a name="l313"><span class="ln">313  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l314"><span class="ln">314  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arccosh</span><span class="s3">,</span>
<a name="l315"><span class="ln">315  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l316"><span class="ln">316  </span></a>arccosh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l317"><span class="ln">317  </span></a> 
<a name="l318"><span class="ln">318  </span></a>Alias for :func:`torch.acosh`. 
<a name="l319"><span class="ln">319  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l320"><span class="ln">320  </span></a><span class="s3">)</span>
<a name="l321"><span class="ln">321  </span></a>
<a name="l322"><span class="ln">322  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l323"><span class="ln">323  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">index_add</span><span class="s3">,</span>
<a name="l324"><span class="ln">324  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l325"><span class="ln">325  </span></a>index_add(input: Tensor, dim: int, index: Tensor, source: Tensor, *, alpha: Union[Number, _complex] = 1, out: Optional[Tensor]) -&gt; Tensor # noqa: B950 
<a name="l326"><span class="ln">326  </span></a> 
<a name="l327"><span class="ln">327  </span></a>See :meth:`~Tensor.index_add_` for function description. 
<a name="l328"><span class="ln">328  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l329"><span class="ln">329  </span></a><span class="s3">)</span>
<a name="l330"><span class="ln">330  </span></a>
<a name="l331"><span class="ln">331  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l332"><span class="ln">332  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">index_copy</span><span class="s3">,</span>
<a name="l333"><span class="ln">333  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l334"><span class="ln">334  </span></a>index_copy(input: Tensor, dim: int, index: Tensor, source: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l335"><span class="ln">335  </span></a> 
<a name="l336"><span class="ln">336  </span></a>See :meth:`~Tensor.index_add_` for function description. 
<a name="l337"><span class="ln">337  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l338"><span class="ln">338  </span></a><span class="s3">)</span>
<a name="l339"><span class="ln">339  </span></a>
<a name="l340"><span class="ln">340  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l341"><span class="ln">341  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">index_reduce</span><span class="s3">,</span>
<a name="l342"><span class="ln">342  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l343"><span class="ln">343  </span></a>index_reduce(input: Tensor, dim: int, index: Tensor, source: Tensor, reduce: str, *, include_self: bool = True, out: Optional[Tensor]) -&gt; Tensor # noqa: B950 
<a name="l344"><span class="ln">344  </span></a> 
<a name="l345"><span class="ln">345  </span></a>See :meth:`~Tensor.index_reduce_` for function description. 
<a name="l346"><span class="ln">346  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l347"><span class="ln">347  </span></a><span class="s3">)</span>
<a name="l348"><span class="ln">348  </span></a>
<a name="l349"><span class="ln">349  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l350"><span class="ln">350  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">add</span><span class="s3">,</span>
<a name="l351"><span class="ln">351  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l352"><span class="ln">352  </span></a>add(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l353"><span class="ln">353  </span></a> 
<a name="l354"><span class="ln">354  </span></a>Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`. 
<a name="l355"><span class="ln">355  </span></a> 
<a name="l356"><span class="ln">356  </span></a>.. math:: 
<a name="l357"><span class="ln">357  </span></a>    \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i 
<a name="l358"><span class="ln">358  </span></a>&quot;&quot;&quot;</span>
<a name="l359"><span class="ln">359  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l360"><span class="ln">360  </span></a> 
<a name="l361"><span class="ln">361  </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l362"><span class="ln">362  </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l363"><span class="ln">363  </span></a> 
<a name="l364"><span class="ln">364  </span></a>Args: 
<a name="l365"><span class="ln">365  </span></a>    {input} 
<a name="l366"><span class="ln">366  </span></a>    other (Tensor or Number): the tensor or number to add to :attr:`input`. 
<a name="l367"><span class="ln">367  </span></a> 
<a name="l368"><span class="ln">368  </span></a>Keyword arguments: 
<a name="l369"><span class="ln">369  </span></a>    alpha (Number): the multiplier for :attr:`other`. 
<a name="l370"><span class="ln">370  </span></a>    {out} 
<a name="l371"><span class="ln">371  </span></a> 
<a name="l372"><span class="ln">372  </span></a>Examples:: 
<a name="l373"><span class="ln">373  </span></a> 
<a name="l374"><span class="ln">374  </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l375"><span class="ln">375  </span></a>    &gt;&gt;&gt; a 
<a name="l376"><span class="ln">376  </span></a>    tensor([ 0.0202,  1.0985,  1.3506, -0.6056]) 
<a name="l377"><span class="ln">377  </span></a>    &gt;&gt;&gt; torch.add(a, 20) 
<a name="l378"><span class="ln">378  </span></a>    tensor([ 20.0202,  21.0985,  21.3506,  19.3944]) 
<a name="l379"><span class="ln">379  </span></a> 
<a name="l380"><span class="ln">380  </span></a>    &gt;&gt;&gt; b = torch.randn(4) 
<a name="l381"><span class="ln">381  </span></a>    &gt;&gt;&gt; b 
<a name="l382"><span class="ln">382  </span></a>    tensor([-0.9732, -0.3497,  0.6245,  0.4022]) 
<a name="l383"><span class="ln">383  </span></a>    &gt;&gt;&gt; c = torch.randn(4, 1) 
<a name="l384"><span class="ln">384  </span></a>    &gt;&gt;&gt; c 
<a name="l385"><span class="ln">385  </span></a>    tensor([[ 0.3743], 
<a name="l386"><span class="ln">386  </span></a>            [-1.7724], 
<a name="l387"><span class="ln">387  </span></a>            [-0.5811], 
<a name="l388"><span class="ln">388  </span></a>            [-0.8017]]) 
<a name="l389"><span class="ln">389  </span></a>    &gt;&gt;&gt; torch.add(b, c, alpha=10) 
<a name="l390"><span class="ln">390  </span></a>    tensor([[  2.7695,   3.3930,   4.3672,   4.1450], 
<a name="l391"><span class="ln">391  </span></a>            [-18.6971, -18.0736, -17.0994, -17.3216], 
<a name="l392"><span class="ln">392  </span></a>            [ -6.7845,  -6.1610,  -5.1868,  -5.4090], 
<a name="l393"><span class="ln">393  </span></a>            [ -8.9902,  -8.3667,  -7.3925,  -7.6147]]) 
<a name="l394"><span class="ln">394  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l395"><span class="ln">395  </span></a><span class="s3">)</span>
<a name="l396"><span class="ln">396  </span></a>
<a name="l397"><span class="ln">397  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l398"><span class="ln">398  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">addbmm</span><span class="s3">,</span>
<a name="l399"><span class="ln">399  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l400"><span class="ln">400  </span></a>addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l401"><span class="ln">401  </span></a> 
<a name="l402"><span class="ln">402  </span></a>Performs a batch matrix-matrix product of matrices stored 
<a name="l403"><span class="ln">403  </span></a>in :attr:`batch1` and :attr:`batch2`, 
<a name="l404"><span class="ln">404  </span></a>with a reduced add step (all matrix multiplications get accumulated 
<a name="l405"><span class="ln">405  </span></a>along the first dimension). 
<a name="l406"><span class="ln">406  </span></a>:attr:`input` is added to the final result. 
<a name="l407"><span class="ln">407  </span></a> 
<a name="l408"><span class="ln">408  </span></a>:attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the 
<a name="l409"><span class="ln">409  </span></a>same number of matrices. 
<a name="l410"><span class="ln">410  </span></a> 
<a name="l411"><span class="ln">411  </span></a>If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l412"><span class="ln">412  </span></a>:math:`(b \times m \times p)` tensor, :attr:`input` must be 
<a name="l413"><span class="ln">413  </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l414"><span class="ln">414  </span></a>and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l415"><span class="ln">415  </span></a> 
<a name="l416"><span class="ln">416  </span></a>.. math:: 
<a name="l417"><span class="ln">417  </span></a>    out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l418"><span class="ln">418  </span></a> 
<a name="l419"><span class="ln">419  </span></a>If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l420"><span class="ln">420  </span></a>it will not be propagated. 
<a name="l421"><span class="ln">421  </span></a>&quot;&quot;&quot;</span>
<a name="l422"><span class="ln">422  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l423"><span class="ln">423  </span></a>For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha` 
<a name="l424"><span class="ln">424  </span></a>must be real numbers, otherwise they should be integers. 
<a name="l425"><span class="ln">425  </span></a> 
<a name="l426"><span class="ln">426  </span></a>{tf32_note} 
<a name="l427"><span class="ln">427  </span></a> 
<a name="l428"><span class="ln">428  </span></a>{rocm_fp16_note} 
<a name="l429"><span class="ln">429  </span></a> 
<a name="l430"><span class="ln">430  </span></a>Args: 
<a name="l431"><span class="ln">431  </span></a>    input (Tensor): matrix to be added 
<a name="l432"><span class="ln">432  </span></a>    batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l433"><span class="ln">433  </span></a>    batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l434"><span class="ln">434  </span></a> 
<a name="l435"><span class="ln">435  </span></a>Keyword args: 
<a name="l436"><span class="ln">436  </span></a>    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l437"><span class="ln">437  </span></a>    alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`) 
<a name="l438"><span class="ln">438  </span></a>    {out} 
<a name="l439"><span class="ln">439  </span></a> 
<a name="l440"><span class="ln">440  </span></a>Example:: 
<a name="l441"><span class="ln">441  </span></a> 
<a name="l442"><span class="ln">442  </span></a>    &gt;&gt;&gt; M = torch.randn(3, 5) 
<a name="l443"><span class="ln">443  </span></a>    &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l444"><span class="ln">444  </span></a>    &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l445"><span class="ln">445  </span></a>    &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) 
<a name="l446"><span class="ln">446  </span></a>    tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653], 
<a name="l447"><span class="ln">447  </span></a>            [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743], 
<a name="l448"><span class="ln">448  </span></a>            [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]]) 
<a name="l449"><span class="ln">449  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">, </span><span class="s2">**</span><span class="s1">tf32_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">rocm_fp16_notes</span><span class="s3">),</span>
<a name="l450"><span class="ln">450  </span></a><span class="s3">)</span>
<a name="l451"><span class="ln">451  </span></a>
<a name="l452"><span class="ln">452  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l453"><span class="ln">453  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">addcdiv</span><span class="s3">,</span>
<a name="l454"><span class="ln">454  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l455"><span class="ln">455  </span></a>addcdiv(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l456"><span class="ln">456  </span></a> 
<a name="l457"><span class="ln">457  </span></a>Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`, 
<a name="l458"><span class="ln">458  </span></a>multiplies the result by the scalar :attr:`value` and adds it to :attr:`input`. 
<a name="l459"><span class="ln">459  </span></a> 
<a name="l460"><span class="ln">460  </span></a>.. warning:: 
<a name="l461"><span class="ln">461  </span></a>    Integer division with addcdiv is no longer supported, and in a future 
<a name="l462"><span class="ln">462  </span></a>    release addcdiv will perform a true division of tensor1 and tensor2. 
<a name="l463"><span class="ln">463  </span></a>    The historic addcdiv behavior can be implemented as 
<a name="l464"><span class="ln">464  </span></a>    (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) 
<a name="l465"><span class="ln">465  </span></a>    for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. 
<a name="l466"><span class="ln">466  </span></a>    The future addcdiv behavior is just the latter implementation: 
<a name="l467"><span class="ln">467  </span></a>    (input + value * tensor1 / tensor2), for all dtypes. 
<a name="l468"><span class="ln">468  </span></a> 
<a name="l469"><span class="ln">469  </span></a>.. math:: 
<a name="l470"><span class="ln">470  </span></a>    \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i} 
<a name="l471"><span class="ln">471  </span></a>&quot;&quot;&quot;</span>
<a name="l472"><span class="ln">472  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l473"><span class="ln">473  </span></a> 
<a name="l474"><span class="ln">474  </span></a>The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l475"><span class="ln">475  </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l476"><span class="ln">476  </span></a> 
<a name="l477"><span class="ln">477  </span></a>For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l478"><span class="ln">478  </span></a>a real number, otherwise an integer. 
<a name="l479"><span class="ln">479  </span></a> 
<a name="l480"><span class="ln">480  </span></a>Args: 
<a name="l481"><span class="ln">481  </span></a>    input (Tensor): the tensor to be added 
<a name="l482"><span class="ln">482  </span></a>    tensor1 (Tensor): the numerator tensor 
<a name="l483"><span class="ln">483  </span></a>    tensor2 (Tensor): the denominator tensor 
<a name="l484"><span class="ln">484  </span></a> 
<a name="l485"><span class="ln">485  </span></a>Keyword args: 
<a name="l486"><span class="ln">486  </span></a>    value (Number, optional): multiplier for :math:`\text{{tensor1}} / \text{{tensor2}}` 
<a name="l487"><span class="ln">487  </span></a>    {out} 
<a name="l488"><span class="ln">488  </span></a> 
<a name="l489"><span class="ln">489  </span></a>Example:: 
<a name="l490"><span class="ln">490  </span></a> 
<a name="l491"><span class="ln">491  </span></a>    &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l492"><span class="ln">492  </span></a>    &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l493"><span class="ln">493  </span></a>    &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l494"><span class="ln">494  </span></a>    &gt;&gt;&gt; torch.addcdiv(t, t1, t2, value=0.1) 
<a name="l495"><span class="ln">495  </span></a>    tensor([[-0.2312, -3.6496,  0.1312], 
<a name="l496"><span class="ln">496  </span></a>            [-1.0428,  3.4292, -0.1030], 
<a name="l497"><span class="ln">497  </span></a>            [-0.5369, -0.9829,  0.0430]]) 
<a name="l498"><span class="ln">498  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l499"><span class="ln">499  </span></a><span class="s3">)</span>
<a name="l500"><span class="ln">500  </span></a>
<a name="l501"><span class="ln">501  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l502"><span class="ln">502  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">addcmul</span><span class="s3">,</span>
<a name="l503"><span class="ln">503  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l504"><span class="ln">504  </span></a>addcmul(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l505"><span class="ln">505  </span></a> 
<a name="l506"><span class="ln">506  </span></a>Performs the element-wise multiplication of :attr:`tensor1` 
<a name="l507"><span class="ln">507  </span></a>by :attr:`tensor2`, multiplies the result by the scalar :attr:`value` 
<a name="l508"><span class="ln">508  </span></a>and adds it to :attr:`input`. 
<a name="l509"><span class="ln">509  </span></a> 
<a name="l510"><span class="ln">510  </span></a>.. math:: 
<a name="l511"><span class="ln">511  </span></a>    \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i 
<a name="l512"><span class="ln">512  </span></a>&quot;&quot;&quot;</span>
<a name="l513"><span class="ln">513  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l514"><span class="ln">514  </span></a>The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l515"><span class="ln">515  </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l516"><span class="ln">516  </span></a> 
<a name="l517"><span class="ln">517  </span></a>For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l518"><span class="ln">518  </span></a>a real number, otherwise an integer. 
<a name="l519"><span class="ln">519  </span></a> 
<a name="l520"><span class="ln">520  </span></a>Args: 
<a name="l521"><span class="ln">521  </span></a>    input (Tensor): the tensor to be added 
<a name="l522"><span class="ln">522  </span></a>    tensor1 (Tensor): the tensor to be multiplied 
<a name="l523"><span class="ln">523  </span></a>    tensor2 (Tensor): the tensor to be multiplied 
<a name="l524"><span class="ln">524  </span></a> 
<a name="l525"><span class="ln">525  </span></a>Keyword args: 
<a name="l526"><span class="ln">526  </span></a>    value (Number, optional): multiplier for :math:`tensor1 .* tensor2` 
<a name="l527"><span class="ln">527  </span></a>    {out} 
<a name="l528"><span class="ln">528  </span></a> 
<a name="l529"><span class="ln">529  </span></a>Example:: 
<a name="l530"><span class="ln">530  </span></a> 
<a name="l531"><span class="ln">531  </span></a>    &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l532"><span class="ln">532  </span></a>    &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l533"><span class="ln">533  </span></a>    &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l534"><span class="ln">534  </span></a>    &gt;&gt;&gt; torch.addcmul(t, t1, t2, value=0.1) 
<a name="l535"><span class="ln">535  </span></a>    tensor([[-0.8635, -0.6391,  1.6174], 
<a name="l536"><span class="ln">536  </span></a>            [-0.7617, -0.5879,  1.7388], 
<a name="l537"><span class="ln">537  </span></a>            [-0.8353, -0.6249,  1.6511]]) 
<a name="l538"><span class="ln">538  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l539"><span class="ln">539  </span></a><span class="s3">)</span>
<a name="l540"><span class="ln">540  </span></a>
<a name="l541"><span class="ln">541  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l542"><span class="ln">542  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">addmm</span><span class="s3">,</span>
<a name="l543"><span class="ln">543  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l544"><span class="ln">544  </span></a>addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l545"><span class="ln">545  </span></a> 
<a name="l546"><span class="ln">546  </span></a>Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`. 
<a name="l547"><span class="ln">547  </span></a>The matrix :attr:`input` is added to the final result. 
<a name="l548"><span class="ln">548  </span></a> 
<a name="l549"><span class="ln">549  </span></a>If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l550"><span class="ln">550  </span></a>:math:`(m \times p)` tensor, then :attr:`input` must be 
<a name="l551"><span class="ln">551  </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l552"><span class="ln">552  </span></a>and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l553"><span class="ln">553  </span></a> 
<a name="l554"><span class="ln">554  </span></a>:attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l555"><span class="ln">555  </span></a>:attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively. 
<a name="l556"><span class="ln">556  </span></a> 
<a name="l557"><span class="ln">557  </span></a>.. math:: 
<a name="l558"><span class="ln">558  </span></a>    \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i) 
<a name="l559"><span class="ln">559  </span></a> 
<a name="l560"><span class="ln">560  </span></a>If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l561"><span class="ln">561  </span></a>it will not be propagated. 
<a name="l562"><span class="ln">562  </span></a>&quot;&quot;&quot;</span>
<a name="l563"><span class="ln">563  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l564"><span class="ln">564  </span></a>For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l565"><span class="ln">565  </span></a>:attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l566"><span class="ln">566  </span></a> 
<a name="l567"><span class="ln">567  </span></a>This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. If 
<a name="l568"><span class="ln">568  </span></a>:attr:`input` is sparse the result will have the same layout and if :attr:`out` 
<a name="l569"><span class="ln">569  </span></a>is provided it must have the same layout as :attr:`input`. 
<a name="l570"><span class="ln">570  </span></a> 
<a name="l571"><span class="ln">571  </span></a>{sparse_beta_warning} 
<a name="l572"><span class="ln">572  </span></a> 
<a name="l573"><span class="ln">573  </span></a>{tf32_note} 
<a name="l574"><span class="ln">574  </span></a> 
<a name="l575"><span class="ln">575  </span></a>{rocm_fp16_note} 
<a name="l576"><span class="ln">576  </span></a> 
<a name="l577"><span class="ln">577  </span></a>Args: 
<a name="l578"><span class="ln">578  </span></a>    input (Tensor): matrix to be added 
<a name="l579"><span class="ln">579  </span></a>    mat1 (Tensor): the first matrix to be matrix multiplied 
<a name="l580"><span class="ln">580  </span></a>    mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l581"><span class="ln">581  </span></a>    out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l582"><span class="ln">582  </span></a>        Supported only on CUDA and for torch.float32 given 
<a name="l583"><span class="ln">583  </span></a>        torch.float16/torch.bfloat16 input dtypes 
<a name="l584"><span class="ln">584  </span></a> 
<a name="l585"><span class="ln">585  </span></a>Keyword args: 
<a name="l586"><span class="ln">586  </span></a>    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l587"><span class="ln">587  </span></a>    alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l588"><span class="ln">588  </span></a>    {out} 
<a name="l589"><span class="ln">589  </span></a> 
<a name="l590"><span class="ln">590  </span></a>Example:: 
<a name="l591"><span class="ln">591  </span></a> 
<a name="l592"><span class="ln">592  </span></a>    &gt;&gt;&gt; M = torch.randn(2, 3) 
<a name="l593"><span class="ln">593  </span></a>    &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l594"><span class="ln">594  </span></a>    &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l595"><span class="ln">595  </span></a>    &gt;&gt;&gt; torch.addmm(M, mat1, mat2) 
<a name="l596"><span class="ln">596  </span></a>    tensor([[-4.8716,  1.4671, -1.3746], 
<a name="l597"><span class="ln">597  </span></a>            [ 0.7573, -3.9555, -2.8681]]) 
<a name="l598"><span class="ln">598  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">, </span><span class="s2">**</span><span class="s1">tf32_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">rocm_fp16_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">sparse_support_notes</span><span class="s3">),</span>
<a name="l599"><span class="ln">599  </span></a><span class="s3">)</span>
<a name="l600"><span class="ln">600  </span></a>
<a name="l601"><span class="ln">601  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l602"><span class="ln">602  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">adjoint</span><span class="s3">,</span>
<a name="l603"><span class="ln">603  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l604"><span class="ln">604  </span></a>adjoint(input: Tensor) -&gt; Tensor 
<a name="l605"><span class="ln">605  </span></a>Returns a view of the tensor conjugated and with the last two dimensions transposed. 
<a name="l606"><span class="ln">606  </span></a> 
<a name="l607"><span class="ln">607  </span></a>``x.adjoint()`` is equivalent to ``x.transpose(-2, -1).conj()`` for complex tensors and 
<a name="l608"><span class="ln">608  </span></a>to ``x.transpose(-2, -1)`` for real tensors. 
<a name="l609"><span class="ln">609  </span></a> 
<a name="l610"><span class="ln">610  </span></a>Args: 
<a name="l611"><span class="ln">611  </span></a>    {input} 
<a name="l612"><span class="ln">612  </span></a> 
<a name="l613"><span class="ln">613  </span></a>Example:: 
<a name="l614"><span class="ln">614  </span></a> 
<a name="l615"><span class="ln">615  </span></a>    &gt;&gt;&gt; x = torch.arange(4, dtype=torch.float) 
<a name="l616"><span class="ln">616  </span></a>    &gt;&gt;&gt; A = torch.complex(x, x).reshape(2, 2) 
<a name="l617"><span class="ln">617  </span></a>    &gt;&gt;&gt; A 
<a name="l618"><span class="ln">618  </span></a>    tensor([[0.+0.j, 1.+1.j], 
<a name="l619"><span class="ln">619  </span></a>            [2.+2.j, 3.+3.j]]) 
<a name="l620"><span class="ln">620  </span></a>    &gt;&gt;&gt; A.adjoint() 
<a name="l621"><span class="ln">621  </span></a>    tensor([[0.-0.j, 2.-2.j], 
<a name="l622"><span class="ln">622  </span></a>            [1.-1.j, 3.-3.j]]) 
<a name="l623"><span class="ln">623  </span></a>    &gt;&gt;&gt; (A.adjoint() == A.mH).all() 
<a name="l624"><span class="ln">624  </span></a>    tensor(True) 
<a name="l625"><span class="ln">625  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l626"><span class="ln">626  </span></a><span class="s3">)</span>
<a name="l627"><span class="ln">627  </span></a>
<a name="l628"><span class="ln">628  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l629"><span class="ln">629  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sspaddmm</span><span class="s3">,</span>
<a name="l630"><span class="ln">630  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l631"><span class="ln">631  </span></a>sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l632"><span class="ln">632  </span></a> 
<a name="l633"><span class="ln">633  </span></a>Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor 
<a name="l634"><span class="ln">634  </span></a>:attr:`mat2`, then adds the sparse tensor :attr:`input` to the result. 
<a name="l635"><span class="ln">635  </span></a> 
<a name="l636"><span class="ln">636  </span></a>Note: This function is equivalent to :func:`torch.addmm`, except 
<a name="l637"><span class="ln">637  </span></a>:attr:`input` and :attr:`mat1` are sparse. 
<a name="l638"><span class="ln">638  </span></a> 
<a name="l639"><span class="ln">639  </span></a>Args: 
<a name="l640"><span class="ln">640  </span></a>    input (Tensor): a sparse matrix to be added 
<a name="l641"><span class="ln">641  </span></a>    mat1 (Tensor): a sparse matrix to be matrix multiplied 
<a name="l642"><span class="ln">642  </span></a>    mat2 (Tensor): a dense matrix to be matrix multiplied 
<a name="l643"><span class="ln">643  </span></a> 
<a name="l644"><span class="ln">644  </span></a>Keyword args: 
<a name="l645"><span class="ln">645  </span></a>    beta (Number, optional): multiplier for :attr:`mat` (:math:`\beta`) 
<a name="l646"><span class="ln">646  </span></a>    alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l647"><span class="ln">647  </span></a>    {out} 
<a name="l648"><span class="ln">648  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l649"><span class="ln">649  </span></a><span class="s3">)</span>
<a name="l650"><span class="ln">650  </span></a>
<a name="l651"><span class="ln">651  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l652"><span class="ln">652  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">smm</span><span class="s3">,</span>
<a name="l653"><span class="ln">653  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l654"><span class="ln">654  </span></a>smm(input, mat) -&gt; Tensor 
<a name="l655"><span class="ln">655  </span></a> 
<a name="l656"><span class="ln">656  </span></a>Performs a matrix multiplication of the sparse matrix :attr:`input` 
<a name="l657"><span class="ln">657  </span></a>with the dense matrix :attr:`mat`. 
<a name="l658"><span class="ln">658  </span></a> 
<a name="l659"><span class="ln">659  </span></a>Args: 
<a name="l660"><span class="ln">660  </span></a>    input (Tensor): a sparse matrix to be matrix multiplied 
<a name="l661"><span class="ln">661  </span></a>    mat (Tensor): a dense matrix to be matrix multiplied 
<a name="l662"><span class="ln">662  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l663"><span class="ln">663  </span></a><span class="s3">)</span>
<a name="l664"><span class="ln">664  </span></a>
<a name="l665"><span class="ln">665  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l666"><span class="ln">666  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">addmv</span><span class="s3">,</span>
<a name="l667"><span class="ln">667  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l668"><span class="ln">668  </span></a>addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l669"><span class="ln">669  </span></a> 
<a name="l670"><span class="ln">670  </span></a>Performs a matrix-vector product of the matrix :attr:`mat` and 
<a name="l671"><span class="ln">671  </span></a>the vector :attr:`vec`. 
<a name="l672"><span class="ln">672  </span></a>The vector :attr:`input` is added to the final result. 
<a name="l673"><span class="ln">673  </span></a> 
<a name="l674"><span class="ln">674  </span></a>If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l675"><span class="ln">675  </span></a>size `m`, then :attr:`input` must be 
<a name="l676"><span class="ln">676  </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 1-D tensor of size `n` and 
<a name="l677"><span class="ln">677  </span></a>:attr:`out` will be 1-D tensor of size `n`. 
<a name="l678"><span class="ln">678  </span></a> 
<a name="l679"><span class="ln">679  </span></a>:attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l680"><span class="ln">680  </span></a>:attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively. 
<a name="l681"><span class="ln">681  </span></a> 
<a name="l682"><span class="ln">682  </span></a>.. math:: 
<a name="l683"><span class="ln">683  </span></a>    \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec}) 
<a name="l684"><span class="ln">684  </span></a> 
<a name="l685"><span class="ln">685  </span></a>If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l686"><span class="ln">686  </span></a>it will not be propagated. 
<a name="l687"><span class="ln">687  </span></a>&quot;&quot;&quot;</span>
<a name="l688"><span class="ln">688  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l689"><span class="ln">689  </span></a>For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l690"><span class="ln">690  </span></a>:attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l691"><span class="ln">691  </span></a> 
<a name="l692"><span class="ln">692  </span></a>Args: 
<a name="l693"><span class="ln">693  </span></a>    input (Tensor): vector to be added 
<a name="l694"><span class="ln">694  </span></a>    mat (Tensor): matrix to be matrix multiplied 
<a name="l695"><span class="ln">695  </span></a>    vec (Tensor): vector to be matrix multiplied 
<a name="l696"><span class="ln">696  </span></a> 
<a name="l697"><span class="ln">697  </span></a>Keyword args: 
<a name="l698"><span class="ln">698  </span></a>    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l699"><span class="ln">699  </span></a>    alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`) 
<a name="l700"><span class="ln">700  </span></a>    {out} 
<a name="l701"><span class="ln">701  </span></a> 
<a name="l702"><span class="ln">702  </span></a>Example:: 
<a name="l703"><span class="ln">703  </span></a> 
<a name="l704"><span class="ln">704  </span></a>    &gt;&gt;&gt; M = torch.randn(2) 
<a name="l705"><span class="ln">705  </span></a>    &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l706"><span class="ln">706  </span></a>    &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l707"><span class="ln">707  </span></a>    &gt;&gt;&gt; torch.addmv(M, mat, vec) 
<a name="l708"><span class="ln">708  </span></a>    tensor([-0.3768, -5.5565]) 
<a name="l709"><span class="ln">709  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l710"><span class="ln">710  </span></a><span class="s3">)</span>
<a name="l711"><span class="ln">711  </span></a>
<a name="l712"><span class="ln">712  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l713"><span class="ln">713  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">addr</span><span class="s3">,</span>
<a name="l714"><span class="ln">714  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l715"><span class="ln">715  </span></a>addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l716"><span class="ln">716  </span></a> 
<a name="l717"><span class="ln">717  </span></a>Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2` 
<a name="l718"><span class="ln">718  </span></a>and adds it to the matrix :attr:`input`. 
<a name="l719"><span class="ln">719  </span></a> 
<a name="l720"><span class="ln">720  </span></a>Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the 
<a name="l721"><span class="ln">721  </span></a>outer product between :attr:`vec1` and :attr:`vec2` and the added matrix 
<a name="l722"><span class="ln">722  </span></a>:attr:`input` respectively. 
<a name="l723"><span class="ln">723  </span></a> 
<a name="l724"><span class="ln">724  </span></a>.. math:: 
<a name="l725"><span class="ln">725  </span></a>    \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2}) 
<a name="l726"><span class="ln">726  </span></a> 
<a name="l727"><span class="ln">727  </span></a>If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l728"><span class="ln">728  </span></a>it will not be propagated. 
<a name="l729"><span class="ln">729  </span></a>&quot;&quot;&quot;</span>
<a name="l730"><span class="ln">730  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l731"><span class="ln">731  </span></a>If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector 
<a name="l732"><span class="ln">732  </span></a>of size `m`, then :attr:`input` must be 
<a name="l733"><span class="ln">733  </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with a matrix of size 
<a name="l734"><span class="ln">734  </span></a>:math:`(n \times m)` and :attr:`out` will be a matrix of size 
<a name="l735"><span class="ln">735  </span></a>:math:`(n \times m)`. 
<a name="l736"><span class="ln">736  </span></a> 
<a name="l737"><span class="ln">737  </span></a>Args: 
<a name="l738"><span class="ln">738  </span></a>    input (Tensor): matrix to be added 
<a name="l739"><span class="ln">739  </span></a>    vec1 (Tensor): the first vector of the outer product 
<a name="l740"><span class="ln">740  </span></a>    vec2 (Tensor): the second vector of the outer product 
<a name="l741"><span class="ln">741  </span></a> 
<a name="l742"><span class="ln">742  </span></a>Keyword args: 
<a name="l743"><span class="ln">743  </span></a>    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l744"><span class="ln">744  </span></a>    alpha (Number, optional): multiplier for :math:`\text{{vec1}} \otimes \text{{vec2}}` (:math:`\alpha`) 
<a name="l745"><span class="ln">745  </span></a>    {out} 
<a name="l746"><span class="ln">746  </span></a> 
<a name="l747"><span class="ln">747  </span></a>Example:: 
<a name="l748"><span class="ln">748  </span></a> 
<a name="l749"><span class="ln">749  </span></a>    &gt;&gt;&gt; vec1 = torch.arange(1., 4.) 
<a name="l750"><span class="ln">750  </span></a>    &gt;&gt;&gt; vec2 = torch.arange(1., 3.) 
<a name="l751"><span class="ln">751  </span></a>    &gt;&gt;&gt; M = torch.zeros(3, 2) 
<a name="l752"><span class="ln">752  </span></a>    &gt;&gt;&gt; torch.addr(M, vec1, vec2) 
<a name="l753"><span class="ln">753  </span></a>    tensor([[ 1.,  2.], 
<a name="l754"><span class="ln">754  </span></a>            [ 2.,  4.], 
<a name="l755"><span class="ln">755  </span></a>            [ 3.,  6.]]) 
<a name="l756"><span class="ln">756  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l757"><span class="ln">757  </span></a><span class="s3">)</span>
<a name="l758"><span class="ln">758  </span></a>
<a name="l759"><span class="ln">759  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l760"><span class="ln">760  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">allclose</span><span class="s3">,</span>
<a name="l761"><span class="ln">761  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l762"><span class="ln">762  </span></a>allclose(input: Tensor, other: Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -&gt; bool 
<a name="l763"><span class="ln">763  </span></a> 
<a name="l764"><span class="ln">764  </span></a>This function checks if :attr:`input` and :attr:`other` satisfy the condition: 
<a name="l765"><span class="ln">765  </span></a> 
<a name="l766"><span class="ln">766  </span></a>.. math:: 
<a name="l767"><span class="ln">767  </span></a>    \lvert \text{input}_i - \text{other}_i \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other}_i \rvert 
<a name="l768"><span class="ln">768  </span></a>&quot;&quot;&quot;</span>
<a name="l769"><span class="ln">769  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l770"><span class="ln">770  </span></a>elementwise, for all elements of :attr:`input` and :attr:`other`. The behaviour of this function is analogous to 
<a name="l771"><span class="ln">771  </span></a>`numpy.allclose &lt;https://numpy.org/doc/stable/reference/generated/numpy.allclose.html&gt;`_ 
<a name="l772"><span class="ln">772  </span></a> 
<a name="l773"><span class="ln">773  </span></a>Args: 
<a name="l774"><span class="ln">774  </span></a>    input (Tensor): first tensor to compare 
<a name="l775"><span class="ln">775  </span></a>    other (Tensor): second tensor to compare 
<a name="l776"><span class="ln">776  </span></a>    atol (float, optional): absolute tolerance. Default: 1e-08 
<a name="l777"><span class="ln">777  </span></a>    rtol (float, optional): relative tolerance. Default: 1e-05 
<a name="l778"><span class="ln">778  </span></a>    equal_nan (bool, optional): if ``True``, then two ``NaN`` s will be considered equal. Default: ``False`` 
<a name="l779"><span class="ln">779  </span></a> 
<a name="l780"><span class="ln">780  </span></a>Example:: 
<a name="l781"><span class="ln">781  </span></a> 
<a name="l782"><span class="ln">782  </span></a>    &gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08])) 
<a name="l783"><span class="ln">783  </span></a>    False 
<a name="l784"><span class="ln">784  </span></a>    &gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09])) 
<a name="l785"><span class="ln">785  </span></a>    True 
<a name="l786"><span class="ln">786  </span></a>    &gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')])) 
<a name="l787"><span class="ln">787  </span></a>    False 
<a name="l788"><span class="ln">788  </span></a>    &gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True) 
<a name="l789"><span class="ln">789  </span></a>    True 
<a name="l790"><span class="ln">790  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l791"><span class="ln">791  </span></a><span class="s3">)</span>
<a name="l792"><span class="ln">792  </span></a>
<a name="l793"><span class="ln">793  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l794"><span class="ln">794  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">all</span><span class="s3">,</span>
<a name="l795"><span class="ln">795  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l796"><span class="ln">796  </span></a>all(input: Tensor, *, out=None) -&gt; Tensor 
<a name="l797"><span class="ln">797  </span></a> 
<a name="l798"><span class="ln">798  </span></a>Tests if all elements in :attr:`input` evaluate to `True`. 
<a name="l799"><span class="ln">799  </span></a> 
<a name="l800"><span class="ln">800  </span></a>.. note:: This function matches the behaviour of NumPy in returning 
<a name="l801"><span class="ln">801  </span></a>          output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l802"><span class="ln">802  </span></a>          For `uint8` the dtype of output is `uint8` itself. 
<a name="l803"><span class="ln">803  </span></a> 
<a name="l804"><span class="ln">804  </span></a>Args: 
<a name="l805"><span class="ln">805  </span></a>    {input} 
<a name="l806"><span class="ln">806  </span></a> 
<a name="l807"><span class="ln">807  </span></a>Keyword args: 
<a name="l808"><span class="ln">808  </span></a>    {out} 
<a name="l809"><span class="ln">809  </span></a> 
<a name="l810"><span class="ln">810  </span></a>Example:: 
<a name="l811"><span class="ln">811  </span></a> 
<a name="l812"><span class="ln">812  </span></a>    &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l813"><span class="ln">813  </span></a>    &gt;&gt;&gt; a 
<a name="l814"><span class="ln">814  </span></a>    tensor([[False, True]], dtype=torch.bool) 
<a name="l815"><span class="ln">815  </span></a>    &gt;&gt;&gt; torch.all(a) 
<a name="l816"><span class="ln">816  </span></a>    tensor(False, dtype=torch.bool) 
<a name="l817"><span class="ln">817  </span></a>    &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l818"><span class="ln">818  </span></a>    &gt;&gt;&gt; a 
<a name="l819"><span class="ln">819  </span></a>    tensor([0, 1, 2]) 
<a name="l820"><span class="ln">820  </span></a>    &gt;&gt;&gt; torch.all(a) 
<a name="l821"><span class="ln">821  </span></a>    tensor(False) 
<a name="l822"><span class="ln">822  </span></a> 
<a name="l823"><span class="ln">823  </span></a>.. function:: all(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l824"><span class="ln">824  </span></a>   :noindex: 
<a name="l825"><span class="ln">825  </span></a> 
<a name="l826"><span class="ln">826  </span></a>For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l827"><span class="ln">827  </span></a>returns `True` if all elements in the row evaluate to `True` and `False` otherwise. 
<a name="l828"><span class="ln">828  </span></a> 
<a name="l829"><span class="ln">829  </span></a>{keepdim_details} 
<a name="l830"><span class="ln">830  </span></a> 
<a name="l831"><span class="ln">831  </span></a>Args: 
<a name="l832"><span class="ln">832  </span></a>    {input} 
<a name="l833"><span class="ln">833  </span></a>    {opt_dim_all_reduce} 
<a name="l834"><span class="ln">834  </span></a>    {opt_keepdim} 
<a name="l835"><span class="ln">835  </span></a> 
<a name="l836"><span class="ln">836  </span></a>Keyword args: 
<a name="l837"><span class="ln">837  </span></a>    {out} 
<a name="l838"><span class="ln">838  </span></a> 
<a name="l839"><span class="ln">839  </span></a>Example:: 
<a name="l840"><span class="ln">840  </span></a> 
<a name="l841"><span class="ln">841  </span></a>    &gt;&gt;&gt; a = torch.rand(4, 2).bool() 
<a name="l842"><span class="ln">842  </span></a>    &gt;&gt;&gt; a 
<a name="l843"><span class="ln">843  </span></a>    tensor([[True, True], 
<a name="l844"><span class="ln">844  </span></a>            [True, False], 
<a name="l845"><span class="ln">845  </span></a>            [True, True], 
<a name="l846"><span class="ln">846  </span></a>            [True, True]], dtype=torch.bool) 
<a name="l847"><span class="ln">847  </span></a>    &gt;&gt;&gt; torch.all(a, dim=1) 
<a name="l848"><span class="ln">848  </span></a>    tensor([ True, False,  True,  True], dtype=torch.bool) 
<a name="l849"><span class="ln">849  </span></a>    &gt;&gt;&gt; torch.all(a, dim=0) 
<a name="l850"><span class="ln">850  </span></a>    tensor([ True, False], dtype=torch.bool) 
<a name="l851"><span class="ln">851  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l852"><span class="ln">852  </span></a><span class="s3">)</span>
<a name="l853"><span class="ln">853  </span></a>
<a name="l854"><span class="ln">854  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l855"><span class="ln">855  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">any</span><span class="s3">,</span>
<a name="l856"><span class="ln">856  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l857"><span class="ln">857  </span></a>any(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l858"><span class="ln">858  </span></a> 
<a name="l859"><span class="ln">859  </span></a>Tests if any element in :attr:`input` evaluates to `True`. 
<a name="l860"><span class="ln">860  </span></a> 
<a name="l861"><span class="ln">861  </span></a>.. note:: This function matches the behaviour of NumPy in returning 
<a name="l862"><span class="ln">862  </span></a>          output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l863"><span class="ln">863  </span></a>          For `uint8` the dtype of output is `uint8` itself. 
<a name="l864"><span class="ln">864  </span></a> 
<a name="l865"><span class="ln">865  </span></a>Args: 
<a name="l866"><span class="ln">866  </span></a>    {input} 
<a name="l867"><span class="ln">867  </span></a> 
<a name="l868"><span class="ln">868  </span></a>Keyword args: 
<a name="l869"><span class="ln">869  </span></a>    {out} 
<a name="l870"><span class="ln">870  </span></a> 
<a name="l871"><span class="ln">871  </span></a>Example:: 
<a name="l872"><span class="ln">872  </span></a> 
<a name="l873"><span class="ln">873  </span></a>    &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l874"><span class="ln">874  </span></a>    &gt;&gt;&gt; a 
<a name="l875"><span class="ln">875  </span></a>    tensor([[False, True]], dtype=torch.bool) 
<a name="l876"><span class="ln">876  </span></a>    &gt;&gt;&gt; torch.any(a) 
<a name="l877"><span class="ln">877  </span></a>    tensor(True, dtype=torch.bool) 
<a name="l878"><span class="ln">878  </span></a>    &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l879"><span class="ln">879  </span></a>    &gt;&gt;&gt; a 
<a name="l880"><span class="ln">880  </span></a>    tensor([0, 1, 2]) 
<a name="l881"><span class="ln">881  </span></a>    &gt;&gt;&gt; torch.any(a) 
<a name="l882"><span class="ln">882  </span></a>    tensor(True) 
<a name="l883"><span class="ln">883  </span></a> 
<a name="l884"><span class="ln">884  </span></a>.. function:: any(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l885"><span class="ln">885  </span></a>   :noindex: 
<a name="l886"><span class="ln">886  </span></a> 
<a name="l887"><span class="ln">887  </span></a>For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l888"><span class="ln">888  </span></a>returns `True` if any element in the row evaluate to `True` and `False` otherwise. 
<a name="l889"><span class="ln">889  </span></a> 
<a name="l890"><span class="ln">890  </span></a>{keepdim_details} 
<a name="l891"><span class="ln">891  </span></a> 
<a name="l892"><span class="ln">892  </span></a>Args: 
<a name="l893"><span class="ln">893  </span></a>    {input} 
<a name="l894"><span class="ln">894  </span></a>    {opt_dim_all_reduce} 
<a name="l895"><span class="ln">895  </span></a>    {opt_keepdim} 
<a name="l896"><span class="ln">896  </span></a> 
<a name="l897"><span class="ln">897  </span></a>Keyword args: 
<a name="l898"><span class="ln">898  </span></a>    {out} 
<a name="l899"><span class="ln">899  </span></a> 
<a name="l900"><span class="ln">900  </span></a>Example:: 
<a name="l901"><span class="ln">901  </span></a> 
<a name="l902"><span class="ln">902  </span></a>    &gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0 
<a name="l903"><span class="ln">903  </span></a>    &gt;&gt;&gt; a 
<a name="l904"><span class="ln">904  </span></a>    tensor([[ True,  True], 
<a name="l905"><span class="ln">905  </span></a>            [False,  True], 
<a name="l906"><span class="ln">906  </span></a>            [ True,  True], 
<a name="l907"><span class="ln">907  </span></a>            [False, False]]) 
<a name="l908"><span class="ln">908  </span></a>    &gt;&gt;&gt; torch.any(a, 1) 
<a name="l909"><span class="ln">909  </span></a>    tensor([ True,  True,  True, False]) 
<a name="l910"><span class="ln">910  </span></a>    &gt;&gt;&gt; torch.any(a, 0) 
<a name="l911"><span class="ln">911  </span></a>    tensor([True, True]) 
<a name="l912"><span class="ln">912  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l913"><span class="ln">913  </span></a><span class="s3">)</span>
<a name="l914"><span class="ln">914  </span></a>
<a name="l915"><span class="ln">915  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l916"><span class="ln">916  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">angle</span><span class="s3">,</span>
<a name="l917"><span class="ln">917  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l918"><span class="ln">918  </span></a>angle(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l919"><span class="ln">919  </span></a> 
<a name="l920"><span class="ln">920  </span></a>Computes the element-wise angle (in radians) of the given :attr:`input` tensor. 
<a name="l921"><span class="ln">921  </span></a> 
<a name="l922"><span class="ln">922  </span></a>.. math:: 
<a name="l923"><span class="ln">923  </span></a>    \text{out}_{i} = angle(\text{input}_{i}) 
<a name="l924"><span class="ln">924  </span></a>&quot;&quot;&quot;</span>
<a name="l925"><span class="ln">925  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l926"><span class="ln">926  </span></a>Args: 
<a name="l927"><span class="ln">927  </span></a>    {input} 
<a name="l928"><span class="ln">928  </span></a> 
<a name="l929"><span class="ln">929  </span></a>Keyword args: 
<a name="l930"><span class="ln">930  </span></a>    {out} 
<a name="l931"><span class="ln">931  </span></a> 
<a name="l932"><span class="ln">932  </span></a>.. note:: Starting in PyTorch 1.8, angle returns pi for negative real numbers, 
<a name="l933"><span class="ln">933  </span></a>          zero for non-negative real numbers, and propagates NaNs. Previously 
<a name="l934"><span class="ln">934  </span></a>          the function would return zero for all real numbers and not propagate 
<a name="l935"><span class="ln">935  </span></a>          floating-point NaNs. 
<a name="l936"><span class="ln">936  </span></a> 
<a name="l937"><span class="ln">937  </span></a>Example:: 
<a name="l938"><span class="ln">938  </span></a> 
<a name="l939"><span class="ln">939  </span></a>    &gt;&gt;&gt; torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159 
<a name="l940"><span class="ln">940  </span></a>    tensor([ 135.,  135,  -45]) 
<a name="l941"><span class="ln">941  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l942"><span class="ln">942  </span></a><span class="s3">)</span>
<a name="l943"><span class="ln">943  </span></a>
<a name="l944"><span class="ln">944  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l945"><span class="ln">945  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">as_strided</span><span class="s3">,</span>
<a name="l946"><span class="ln">946  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l947"><span class="ln">947  </span></a>as_strided(input, size, stride, storage_offset=None) -&gt; Tensor 
<a name="l948"><span class="ln">948  </span></a> 
<a name="l949"><span class="ln">949  </span></a>Create a view of an existing `torch.Tensor` :attr:`input` with specified 
<a name="l950"><span class="ln">950  </span></a>:attr:`size`, :attr:`stride` and :attr:`storage_offset`. 
<a name="l951"><span class="ln">951  </span></a> 
<a name="l952"><span class="ln">952  </span></a>.. warning:: 
<a name="l953"><span class="ln">953  </span></a>    Prefer using other view functions, like :meth:`torch.Tensor.view` or 
<a name="l954"><span class="ln">954  </span></a>    :meth:`torch.Tensor.expand`, to setting a view's strides manually with 
<a name="l955"><span class="ln">955  </span></a>    `as_strided`, as this function will throw an error on non-standard Pytorch 
<a name="l956"><span class="ln">956  </span></a>    backends (that do not have a concept of stride) and the result will depend 
<a name="l957"><span class="ln">957  </span></a>    on the current layout in memory. The constructed view must only refer to 
<a name="l958"><span class="ln">958  </span></a>    elements within the Tensor's storage or a runtime error will be thrown. 
<a name="l959"><span class="ln">959  </span></a>    If the generated view is &quot;overlapped&quot; (with multiple indices referring to 
<a name="l960"><span class="ln">960  </span></a>    the same element in memory), the behavior of inplace operations on this view 
<a name="l961"><span class="ln">961  </span></a>    is undefined (and might not throw runtime errors). 
<a name="l962"><span class="ln">962  </span></a> 
<a name="l963"><span class="ln">963  </span></a>Args: 
<a name="l964"><span class="ln">964  </span></a>    {input} 
<a name="l965"><span class="ln">965  </span></a>    size (tuple or ints): the shape of the output tensor 
<a name="l966"><span class="ln">966  </span></a>    stride (tuple or ints): the stride of the output tensor 
<a name="l967"><span class="ln">967  </span></a>    storage_offset (int, optional): the offset in the underlying storage of the output tensor. 
<a name="l968"><span class="ln">968  </span></a>        If ``None``, the storage_offset of the output tensor will match the input tensor. 
<a name="l969"><span class="ln">969  </span></a> 
<a name="l970"><span class="ln">970  </span></a>Example:: 
<a name="l971"><span class="ln">971  </span></a> 
<a name="l972"><span class="ln">972  </span></a>    &gt;&gt;&gt; x = torch.randn(3, 3) 
<a name="l973"><span class="ln">973  </span></a>    &gt;&gt;&gt; x 
<a name="l974"><span class="ln">974  </span></a>    tensor([[ 0.9039,  0.6291,  1.0795], 
<a name="l975"><span class="ln">975  </span></a>            [ 0.1586,  2.1939, -0.4900], 
<a name="l976"><span class="ln">976  </span></a>            [-0.1909, -0.7503,  1.9355]]) 
<a name="l977"><span class="ln">977  </span></a>    &gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2)) 
<a name="l978"><span class="ln">978  </span></a>    &gt;&gt;&gt; t 
<a name="l979"><span class="ln">979  </span></a>    tensor([[0.9039, 1.0795], 
<a name="l980"><span class="ln">980  </span></a>            [0.6291, 0.1586]]) 
<a name="l981"><span class="ln">981  </span></a>    &gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2), 1) 
<a name="l982"><span class="ln">982  </span></a>    tensor([[0.6291, 0.1586], 
<a name="l983"><span class="ln">983  </span></a>            [1.0795, 2.1939]]) 
<a name="l984"><span class="ln">984  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l985"><span class="ln">985  </span></a><span class="s3">)</span>
<a name="l986"><span class="ln">986  </span></a>
<a name="l987"><span class="ln">987  </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l988"><span class="ln">988  </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">as_tensor</span><span class="s3">,</span>
<a name="l989"><span class="ln">989  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l990"><span class="ln">990  </span></a>as_tensor(data: Any, dtype: Optional[dtype] = None, device: Optional[DeviceLikeType]) -&gt; Tensor 
<a name="l991"><span class="ln">991  </span></a> 
<a name="l992"><span class="ln">992  </span></a>Converts :attr:`data` into a tensor, sharing data and preserving autograd 
<a name="l993"><span class="ln">993  </span></a>history if possible. 
<a name="l994"><span class="ln">994  </span></a> 
<a name="l995"><span class="ln">995  </span></a>If :attr:`data` is already a tensor with the requested dtype and device 
<a name="l996"><span class="ln">996  </span></a>then :attr:`data` itself is returned, but if :attr:`data` is a 
<a name="l997"><span class="ln">997  </span></a>tensor with a different dtype or device then it's copied as if using 
<a name="l998"><span class="ln">998  </span></a>`data.to(dtype=dtype, device=device)`. 
<a name="l999"><span class="ln">999  </span></a> 
<a name="l1000"><span class="ln">1000 </span></a>If :attr:`data` is a NumPy array (an ndarray) with the same dtype and device then a 
<a name="l1001"><span class="ln">1001 </span></a>tensor is constructed using :func:`torch.from_numpy`. 
<a name="l1002"><span class="ln">1002 </span></a> 
<a name="l1003"><span class="ln">1003 </span></a>If :attr:`data` is a CuPy array, the returned tensor will be located on the same device as the CuPy array unless 
<a name="l1004"><span class="ln">1004 </span></a>specifically overwritten by :attr:`device` or a default device. 
<a name="l1005"><span class="ln">1005 </span></a> 
<a name="l1006"><span class="ln">1006 </span></a>.. seealso:: 
<a name="l1007"><span class="ln">1007 </span></a> 
<a name="l1008"><span class="ln">1008 </span></a>    :func:`torch.tensor` never shares its data and creates a new &quot;leaf tensor&quot; (see :doc:`/notes/autograd`). 
<a name="l1009"><span class="ln">1009 </span></a> 
<a name="l1010"><span class="ln">1010 </span></a> 
<a name="l1011"><span class="ln">1011 </span></a>Args: 
<a name="l1012"><span class="ln">1012 </span></a>    {data} 
<a name="l1013"><span class="ln">1013 </span></a>    {dtype} 
<a name="l1014"><span class="ln">1014 </span></a>    device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor 
<a name="l1015"><span class="ln">1015 </span></a>        then the device of data is used. If None and data is not a tensor then 
<a name="l1016"><span class="ln">1016 </span></a>        the result tensor is constructed on the current device. 
<a name="l1017"><span class="ln">1017 </span></a> 
<a name="l1018"><span class="ln">1018 </span></a> 
<a name="l1019"><span class="ln">1019 </span></a>Example:: 
<a name="l1020"><span class="ln">1020 </span></a> 
<a name="l1021"><span class="ln">1021 </span></a>    &gt;&gt;&gt; a = numpy.array([1, 2, 3]) 
<a name="l1022"><span class="ln">1022 </span></a>    &gt;&gt;&gt; t = torch.as_tensor(a) 
<a name="l1023"><span class="ln">1023 </span></a>    &gt;&gt;&gt; t 
<a name="l1024"><span class="ln">1024 </span></a>    tensor([ 1,  2,  3]) 
<a name="l1025"><span class="ln">1025 </span></a>    &gt;&gt;&gt; t[0] = -1 
<a name="l1026"><span class="ln">1026 </span></a>    &gt;&gt;&gt; a 
<a name="l1027"><span class="ln">1027 </span></a>    array([-1,  2,  3]) 
<a name="l1028"><span class="ln">1028 </span></a> 
<a name="l1029"><span class="ln">1029 </span></a>    &gt;&gt;&gt; a = numpy.array([1, 2, 3]) 
<a name="l1030"><span class="ln">1030 </span></a>    &gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda')) 
<a name="l1031"><span class="ln">1031 </span></a>    &gt;&gt;&gt; t 
<a name="l1032"><span class="ln">1032 </span></a>    tensor([ 1,  2,  3]) 
<a name="l1033"><span class="ln">1033 </span></a>    &gt;&gt;&gt; t[0] = -1 
<a name="l1034"><span class="ln">1034 </span></a>    &gt;&gt;&gt; a 
<a name="l1035"><span class="ln">1035 </span></a>    array([1,  2,  3]) 
<a name="l1036"><span class="ln">1036 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_data_common_args</span><span class="s3">),</span>
<a name="l1037"><span class="ln">1037 </span></a><span class="s3">)</span>
<a name="l1038"><span class="ln">1038 </span></a>
<a name="l1039"><span class="ln">1039 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1040"><span class="ln">1040 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">asin</span><span class="s3">,</span>
<a name="l1041"><span class="ln">1041 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1042"><span class="ln">1042 </span></a>asin(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1043"><span class="ln">1043 </span></a> 
<a name="l1044"><span class="ln">1044 </span></a>Returns a new tensor with the arcsine of the elements of :attr:`input`. 
<a name="l1045"><span class="ln">1045 </span></a> 
<a name="l1046"><span class="ln">1046 </span></a>.. math:: 
<a name="l1047"><span class="ln">1047 </span></a>    \text{out}_{i} = \sin^{-1}(\text{input}_{i}) 
<a name="l1048"><span class="ln">1048 </span></a>&quot;&quot;&quot;</span>
<a name="l1049"><span class="ln">1049 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l1050"><span class="ln">1050 </span></a>Args: 
<a name="l1051"><span class="ln">1051 </span></a>    {input} 
<a name="l1052"><span class="ln">1052 </span></a> 
<a name="l1053"><span class="ln">1053 </span></a>Keyword args: 
<a name="l1054"><span class="ln">1054 </span></a>    {out} 
<a name="l1055"><span class="ln">1055 </span></a> 
<a name="l1056"><span class="ln">1056 </span></a>Example:: 
<a name="l1057"><span class="ln">1057 </span></a> 
<a name="l1058"><span class="ln">1058 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l1059"><span class="ln">1059 </span></a>    &gt;&gt;&gt; a 
<a name="l1060"><span class="ln">1060 </span></a>    tensor([-0.5962,  1.4985, -0.4396,  1.4525]) 
<a name="l1061"><span class="ln">1061 </span></a>    &gt;&gt;&gt; torch.asin(a) 
<a name="l1062"><span class="ln">1062 </span></a>    tensor([-0.6387,     nan, -0.4552,     nan]) 
<a name="l1063"><span class="ln">1063 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1064"><span class="ln">1064 </span></a><span class="s3">)</span>
<a name="l1065"><span class="ln">1065 </span></a>
<a name="l1066"><span class="ln">1066 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1067"><span class="ln">1067 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arcsin</span><span class="s3">,</span>
<a name="l1068"><span class="ln">1068 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1069"><span class="ln">1069 </span></a>arcsin(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1070"><span class="ln">1070 </span></a> 
<a name="l1071"><span class="ln">1071 </span></a>Alias for :func:`torch.asin`. 
<a name="l1072"><span class="ln">1072 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1073"><span class="ln">1073 </span></a><span class="s3">)</span>
<a name="l1074"><span class="ln">1074 </span></a>
<a name="l1075"><span class="ln">1075 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1076"><span class="ln">1076 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">asinh</span><span class="s3">,</span>
<a name="l1077"><span class="ln">1077 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1078"><span class="ln">1078 </span></a>asinh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1079"><span class="ln">1079 </span></a> 
<a name="l1080"><span class="ln">1080 </span></a>Returns a new tensor with the inverse hyperbolic sine of the elements of :attr:`input`. 
<a name="l1081"><span class="ln">1081 </span></a> 
<a name="l1082"><span class="ln">1082 </span></a>.. math:: 
<a name="l1083"><span class="ln">1083 </span></a>    \text{out}_{i} = \sinh^{-1}(\text{input}_{i}) 
<a name="l1084"><span class="ln">1084 </span></a>&quot;&quot;&quot;</span>
<a name="l1085"><span class="ln">1085 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l1086"><span class="ln">1086 </span></a>Args: 
<a name="l1087"><span class="ln">1087 </span></a>    {input} 
<a name="l1088"><span class="ln">1088 </span></a> 
<a name="l1089"><span class="ln">1089 </span></a>Keyword arguments: 
<a name="l1090"><span class="ln">1090 </span></a>    {out} 
<a name="l1091"><span class="ln">1091 </span></a> 
<a name="l1092"><span class="ln">1092 </span></a>Example:: 
<a name="l1093"><span class="ln">1093 </span></a> 
<a name="l1094"><span class="ln">1094 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l1095"><span class="ln">1095 </span></a>    &gt;&gt;&gt; a 
<a name="l1096"><span class="ln">1096 </span></a>    tensor([ 0.1606, -1.4267, -1.0899, -1.0250 ]) 
<a name="l1097"><span class="ln">1097 </span></a>    &gt;&gt;&gt; torch.asinh(a) 
<a name="l1098"><span class="ln">1098 </span></a>    tensor([ 0.1599, -1.1534, -0.9435, -0.8990 ]) 
<a name="l1099"><span class="ln">1099 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1100"><span class="ln">1100 </span></a><span class="s3">)</span>
<a name="l1101"><span class="ln">1101 </span></a>
<a name="l1102"><span class="ln">1102 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1103"><span class="ln">1103 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arcsinh</span><span class="s3">,</span>
<a name="l1104"><span class="ln">1104 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1105"><span class="ln">1105 </span></a>arcsinh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1106"><span class="ln">1106 </span></a> 
<a name="l1107"><span class="ln">1107 </span></a>Alias for :func:`torch.asinh`. 
<a name="l1108"><span class="ln">1108 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1109"><span class="ln">1109 </span></a><span class="s3">)</span>
<a name="l1110"><span class="ln">1110 </span></a>
<a name="l1111"><span class="ln">1111 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1112"><span class="ln">1112 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">atan</span><span class="s3">,</span>
<a name="l1113"><span class="ln">1113 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1114"><span class="ln">1114 </span></a>atan(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1115"><span class="ln">1115 </span></a> 
<a name="l1116"><span class="ln">1116 </span></a>Returns a new tensor with the arctangent of the elements of :attr:`input`. 
<a name="l1117"><span class="ln">1117 </span></a> 
<a name="l1118"><span class="ln">1118 </span></a>.. math:: 
<a name="l1119"><span class="ln">1119 </span></a>    \text{out}_{i} = \tan^{-1}(\text{input}_{i}) 
<a name="l1120"><span class="ln">1120 </span></a>&quot;&quot;&quot;</span>
<a name="l1121"><span class="ln">1121 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l1122"><span class="ln">1122 </span></a>Args: 
<a name="l1123"><span class="ln">1123 </span></a>    {input} 
<a name="l1124"><span class="ln">1124 </span></a> 
<a name="l1125"><span class="ln">1125 </span></a>Keyword args: 
<a name="l1126"><span class="ln">1126 </span></a>    {out} 
<a name="l1127"><span class="ln">1127 </span></a> 
<a name="l1128"><span class="ln">1128 </span></a>Example:: 
<a name="l1129"><span class="ln">1129 </span></a> 
<a name="l1130"><span class="ln">1130 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l1131"><span class="ln">1131 </span></a>    &gt;&gt;&gt; a 
<a name="l1132"><span class="ln">1132 </span></a>    tensor([ 0.2341,  0.2539, -0.6256, -0.6448]) 
<a name="l1133"><span class="ln">1133 </span></a>    &gt;&gt;&gt; torch.atan(a) 
<a name="l1134"><span class="ln">1134 </span></a>    tensor([ 0.2299,  0.2487, -0.5591, -0.5727]) 
<a name="l1135"><span class="ln">1135 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1136"><span class="ln">1136 </span></a><span class="s3">)</span>
<a name="l1137"><span class="ln">1137 </span></a>
<a name="l1138"><span class="ln">1138 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1139"><span class="ln">1139 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arctan</span><span class="s3">,</span>
<a name="l1140"><span class="ln">1140 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1141"><span class="ln">1141 </span></a>arctan(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1142"><span class="ln">1142 </span></a> 
<a name="l1143"><span class="ln">1143 </span></a>Alias for :func:`torch.atan`. 
<a name="l1144"><span class="ln">1144 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1145"><span class="ln">1145 </span></a><span class="s3">)</span>
<a name="l1146"><span class="ln">1146 </span></a>
<a name="l1147"><span class="ln">1147 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1148"><span class="ln">1148 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">atan2</span><span class="s3">,</span>
<a name="l1149"><span class="ln">1149 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1150"><span class="ln">1150 </span></a>atan2(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1151"><span class="ln">1151 </span></a> 
<a name="l1152"><span class="ln">1152 </span></a>Element-wise arctangent of :math:`\text{{input}}_{{i}} / \text{{other}}_{{i}}` 
<a name="l1153"><span class="ln">1153 </span></a>with consideration of the quadrant. Returns a new tensor with the signed angles 
<a name="l1154"><span class="ln">1154 </span></a>in radians between vector :math:`(\text{{other}}_{{i}}, \text{{input}}_{{i}})` 
<a name="l1155"><span class="ln">1155 </span></a>and vector :math:`(1, 0)`. (Note that :math:`\text{{other}}_{{i}}`, the second 
<a name="l1156"><span class="ln">1156 </span></a>parameter, is the x-coordinate, while :math:`\text{{input}}_{{i}}`, the first 
<a name="l1157"><span class="ln">1157 </span></a>parameter, is the y-coordinate.) 
<a name="l1158"><span class="ln">1158 </span></a> 
<a name="l1159"><span class="ln">1159 </span></a>The shapes of ``input`` and ``other`` must be 
<a name="l1160"><span class="ln">1160 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l1161"><span class="ln">1161 </span></a> 
<a name="l1162"><span class="ln">1162 </span></a>Args: 
<a name="l1163"><span class="ln">1163 </span></a>    input (Tensor): the first input tensor 
<a name="l1164"><span class="ln">1164 </span></a>    other (Tensor): the second input tensor 
<a name="l1165"><span class="ln">1165 </span></a> 
<a name="l1166"><span class="ln">1166 </span></a>Keyword args: 
<a name="l1167"><span class="ln">1167 </span></a>    {out} 
<a name="l1168"><span class="ln">1168 </span></a> 
<a name="l1169"><span class="ln">1169 </span></a>Example:: 
<a name="l1170"><span class="ln">1170 </span></a> 
<a name="l1171"><span class="ln">1171 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l1172"><span class="ln">1172 </span></a>    &gt;&gt;&gt; a 
<a name="l1173"><span class="ln">1173 </span></a>    tensor([ 0.9041,  0.0196, -0.3108, -2.4423]) 
<a name="l1174"><span class="ln">1174 </span></a>    &gt;&gt;&gt; torch.atan2(a, torch.randn(4)) 
<a name="l1175"><span class="ln">1175 </span></a>    tensor([ 0.9833,  0.0811, -1.9743, -1.4151]) 
<a name="l1176"><span class="ln">1176 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1177"><span class="ln">1177 </span></a><span class="s3">)</span>
<a name="l1178"><span class="ln">1178 </span></a>
<a name="l1179"><span class="ln">1179 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1180"><span class="ln">1180 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arctan2</span><span class="s3">,</span>
<a name="l1181"><span class="ln">1181 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1182"><span class="ln">1182 </span></a>arctan2(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1183"><span class="ln">1183 </span></a>Alias for :func:`torch.atan2`. 
<a name="l1184"><span class="ln">1184 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1185"><span class="ln">1185 </span></a><span class="s3">)</span>
<a name="l1186"><span class="ln">1186 </span></a>
<a name="l1187"><span class="ln">1187 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1188"><span class="ln">1188 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">atanh</span><span class="s3">,</span>
<a name="l1189"><span class="ln">1189 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1190"><span class="ln">1190 </span></a>atanh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1191"><span class="ln">1191 </span></a> 
<a name="l1192"><span class="ln">1192 </span></a>Returns a new tensor with the inverse hyperbolic tangent of the elements of :attr:`input`. 
<a name="l1193"><span class="ln">1193 </span></a> 
<a name="l1194"><span class="ln">1194 </span></a>Note: 
<a name="l1195"><span class="ln">1195 </span></a>    The domain of the inverse hyperbolic tangent is `(-1, 1)` and values outside this range 
<a name="l1196"><span class="ln">1196 </span></a>    will be mapped to ``NaN``, except for the values `1` and `-1` for which the output is 
<a name="l1197"><span class="ln">1197 </span></a>    mapped to `+/-INF` respectively. 
<a name="l1198"><span class="ln">1198 </span></a> 
<a name="l1199"><span class="ln">1199 </span></a>.. math:: 
<a name="l1200"><span class="ln">1200 </span></a>    \text{out}_{i} = \tanh^{-1}(\text{input}_{i}) 
<a name="l1201"><span class="ln">1201 </span></a>&quot;&quot;&quot;</span>
<a name="l1202"><span class="ln">1202 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l1203"><span class="ln">1203 </span></a>Args: 
<a name="l1204"><span class="ln">1204 </span></a>    {input} 
<a name="l1205"><span class="ln">1205 </span></a> 
<a name="l1206"><span class="ln">1206 </span></a>Keyword arguments: 
<a name="l1207"><span class="ln">1207 </span></a>    {out} 
<a name="l1208"><span class="ln">1208 </span></a> 
<a name="l1209"><span class="ln">1209 </span></a>Example:: 
<a name="l1210"><span class="ln">1210 </span></a> 
<a name="l1211"><span class="ln">1211 </span></a>    &gt;&gt;&gt; a = torch.randn(4).uniform_(-1, 1) 
<a name="l1212"><span class="ln">1212 </span></a>    &gt;&gt;&gt; a 
<a name="l1213"><span class="ln">1213 </span></a>    tensor([ -0.9385, 0.2968, -0.8591, -0.1871 ]) 
<a name="l1214"><span class="ln">1214 </span></a>    &gt;&gt;&gt; torch.atanh(a) 
<a name="l1215"><span class="ln">1215 </span></a>    tensor([ -1.7253, 0.3060, -1.2899, -0.1893 ]) 
<a name="l1216"><span class="ln">1216 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1217"><span class="ln">1217 </span></a><span class="s3">)</span>
<a name="l1218"><span class="ln">1218 </span></a>
<a name="l1219"><span class="ln">1219 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1220"><span class="ln">1220 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arctanh</span><span class="s3">,</span>
<a name="l1221"><span class="ln">1221 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1222"><span class="ln">1222 </span></a>arctanh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1223"><span class="ln">1223 </span></a> 
<a name="l1224"><span class="ln">1224 </span></a>Alias for :func:`torch.atanh`. 
<a name="l1225"><span class="ln">1225 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1226"><span class="ln">1226 </span></a><span class="s3">)</span>
<a name="l1227"><span class="ln">1227 </span></a>
<a name="l1228"><span class="ln">1228 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1229"><span class="ln">1229 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">asarray</span><span class="s3">,</span>
<a name="l1230"><span class="ln">1230 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1231"><span class="ln">1231 </span></a>asarray(obj: Any, *, dtype: Optional[dtype], device: Optional[DeviceLikeType], copy: Optional[bool] = None, requires_grad: bool = False) -&gt; Tensor # noqa: B950 
<a name="l1232"><span class="ln">1232 </span></a> 
<a name="l1233"><span class="ln">1233 </span></a>Converts :attr:`obj` to a tensor. 
<a name="l1234"><span class="ln">1234 </span></a> 
<a name="l1235"><span class="ln">1235 </span></a>:attr:`obj` can be one of: 
<a name="l1236"><span class="ln">1236 </span></a> 
<a name="l1237"><span class="ln">1237 </span></a>1. a tensor 
<a name="l1238"><span class="ln">1238 </span></a>2. a NumPy array or a NumPy scalar 
<a name="l1239"><span class="ln">1239 </span></a>3. a DLPack capsule 
<a name="l1240"><span class="ln">1240 </span></a>4. an object that implements Python's buffer protocol 
<a name="l1241"><span class="ln">1241 </span></a>5. a scalar 
<a name="l1242"><span class="ln">1242 </span></a>6. a sequence of scalars 
<a name="l1243"><span class="ln">1243 </span></a> 
<a name="l1244"><span class="ln">1244 </span></a>When :attr:`obj` is a tensor, NumPy array, or DLPack capsule the returned tensor will, 
<a name="l1245"><span class="ln">1245 </span></a>by default, not require a gradient, have the same datatype as :attr:`obj`, be on the 
<a name="l1246"><span class="ln">1246 </span></a>same device, and share memory with it. These properties can be controlled with the 
<a name="l1247"><span class="ln">1247 </span></a>:attr:`dtype`, :attr:`device`, :attr:`copy`, and :attr:`requires_grad` keyword arguments. 
<a name="l1248"><span class="ln">1248 </span></a>If the returned tensor is of a different datatype, on a different device, or a copy is 
<a name="l1249"><span class="ln">1249 </span></a>requested then it will not share its memory with :attr:`obj`. If :attr:`requires_grad` 
<a name="l1250"><span class="ln">1250 </span></a>is ``True`` then the returned tensor will require a gradient, and if :attr:`obj` is 
<a name="l1251"><span class="ln">1251 </span></a>also a tensor with an autograd history then the returned tensor will have the same history. 
<a name="l1252"><span class="ln">1252 </span></a> 
<a name="l1253"><span class="ln">1253 </span></a>When :attr:`obj` is not a tensor, NumPy array, or DLPack capsule but implements Python's 
<a name="l1254"><span class="ln">1254 </span></a>buffer protocol then the buffer is interpreted as an array of bytes grouped according to 
<a name="l1255"><span class="ln">1255 </span></a>the size of the datatype passed to the :attr:`dtype` keyword argument. (If no datatype is 
<a name="l1256"><span class="ln">1256 </span></a>passed then the default floating point datatype is used, instead.) The returned tensor 
<a name="l1257"><span class="ln">1257 </span></a>will have the specified datatype (or default floating point datatype if none is specified) 
<a name="l1258"><span class="ln">1258 </span></a>and, by default, be on the CPU device and share memory with the buffer. 
<a name="l1259"><span class="ln">1259 </span></a> 
<a name="l1260"><span class="ln">1260 </span></a>When :attr:`obj` is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on 
<a name="l1261"><span class="ln">1261 </span></a>the CPU and that doesn't share its memory (i.e. ``copy=True``). By default datatype will 
<a name="l1262"><span class="ln">1262 </span></a>be the PyTorch datatype corresponding to the NumPy's scalar's datatype. 
<a name="l1263"><span class="ln">1263 </span></a> 
<a name="l1264"><span class="ln">1264 </span></a>When :attr:`obj` is none of the above but a scalar, or a sequence of scalars then the 
<a name="l1265"><span class="ln">1265 </span></a>returned tensor will, by default, infer its datatype from the scalar values, be on the 
<a name="l1266"><span class="ln">1266 </span></a>current default device, and not share its memory. 
<a name="l1267"><span class="ln">1267 </span></a> 
<a name="l1268"><span class="ln">1268 </span></a>.. seealso:: 
<a name="l1269"><span class="ln">1269 </span></a> 
<a name="l1270"><span class="ln">1270 </span></a>    :func:`torch.tensor` creates a tensor that always copies the data from the input object. 
<a name="l1271"><span class="ln">1271 </span></a>    :func:`torch.from_numpy` creates a tensor that always shares memory from NumPy arrays. 
<a name="l1272"><span class="ln">1272 </span></a>    :func:`torch.frombuffer` creates a tensor that always shares memory from objects that 
<a name="l1273"><span class="ln">1273 </span></a>    implement the buffer protocol. 
<a name="l1274"><span class="ln">1274 </span></a>    :func:`torch.from_dlpack` creates a tensor that always shares memory from 
<a name="l1275"><span class="ln">1275 </span></a>    DLPack capsules. 
<a name="l1276"><span class="ln">1276 </span></a> 
<a name="l1277"><span class="ln">1277 </span></a>Args: 
<a name="l1278"><span class="ln">1278 </span></a>    obj (object): a tensor, NumPy array, DLPack Capsule, object that implements Python's 
<a name="l1279"><span class="ln">1279 </span></a>           buffer protocol, scalar, or sequence of scalars. 
<a name="l1280"><span class="ln">1280 </span></a> 
<a name="l1281"><span class="ln">1281 </span></a>Keyword args: 
<a name="l1282"><span class="ln">1282 </span></a>    dtype (:class:`torch.dtype`, optional): the datatype of the returned tensor. 
<a name="l1283"><span class="ln">1283 </span></a>           Default: ``None``, which causes the datatype of the returned tensor to be 
<a name="l1284"><span class="ln">1284 </span></a>           inferred from :attr:`obj`. 
<a name="l1285"><span class="ln">1285 </span></a>    copy (bool, optional): controls whether the returned tensor shares memory with :attr:`obj`. 
<a name="l1286"><span class="ln">1286 </span></a>           Default: ``None``, which causes the returned tensor to share memory with :attr:`obj` 
<a name="l1287"><span class="ln">1287 </span></a>           whenever possible. If ``True`` then the returned tensor does not share its memory. 
<a name="l1288"><span class="ln">1288 </span></a>           If ``False`` then the returned tensor shares its memory with :attr:`obj` and an 
<a name="l1289"><span class="ln">1289 </span></a>           error is thrown if it cannot. 
<a name="l1290"><span class="ln">1290 </span></a>    device (:class:`torch.device`, optional): the device of the returned tensor. 
<a name="l1291"><span class="ln">1291 </span></a>           Default: ``None``, which causes the device of :attr:`obj` to be used. Or, if 
<a name="l1292"><span class="ln">1292 </span></a>           :attr:`obj` is a Python sequence, the current default device will be used. 
<a name="l1293"><span class="ln">1293 </span></a>    requires_grad (bool, optional): whether the returned tensor requires grad. 
<a name="l1294"><span class="ln">1294 </span></a>           Default: ``False``, which causes the returned tensor not to require a gradient. 
<a name="l1295"><span class="ln">1295 </span></a>           If ``True``, then the returned tensor will require a gradient, and if :attr:`obj` 
<a name="l1296"><span class="ln">1296 </span></a>           is also a tensor with an autograd history then the returned tensor will have 
<a name="l1297"><span class="ln">1297 </span></a>           the same history. 
<a name="l1298"><span class="ln">1298 </span></a> 
<a name="l1299"><span class="ln">1299 </span></a>Example:: 
<a name="l1300"><span class="ln">1300 </span></a> 
<a name="l1301"><span class="ln">1301 </span></a>    &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l1302"><span class="ln">1302 </span></a>    &gt;&gt;&gt; # Shares memory with tensor 'a' 
<a name="l1303"><span class="ln">1303 </span></a>    &gt;&gt;&gt; b = torch.asarray(a) 
<a name="l1304"><span class="ln">1304 </span></a>    &gt;&gt;&gt; a.data_ptr() == b.data_ptr() 
<a name="l1305"><span class="ln">1305 </span></a>    True 
<a name="l1306"><span class="ln">1306 </span></a>    &gt;&gt;&gt; # Forces memory copy 
<a name="l1307"><span class="ln">1307 </span></a>    &gt;&gt;&gt; c = torch.asarray(a, copy=True) 
<a name="l1308"><span class="ln">1308 </span></a>    &gt;&gt;&gt; a.data_ptr() == c.data_ptr() 
<a name="l1309"><span class="ln">1309 </span></a>    False 
<a name="l1310"><span class="ln">1310 </span></a> 
<a name="l1311"><span class="ln">1311 </span></a>    &gt;&gt;&gt; a = torch.tensor([1., 2., 3.], requires_grad=True) 
<a name="l1312"><span class="ln">1312 </span></a>    &gt;&gt;&gt; b = a + 2 
<a name="l1313"><span class="ln">1313 </span></a>    &gt;&gt;&gt; b 
<a name="l1314"><span class="ln">1314 </span></a>    tensor([3., 4., 5.], grad_fn=&lt;AddBackward0&gt;) 
<a name="l1315"><span class="ln">1315 </span></a>    &gt;&gt;&gt; # Shares memory with tensor 'b', with no grad 
<a name="l1316"><span class="ln">1316 </span></a>    &gt;&gt;&gt; c = torch.asarray(b) 
<a name="l1317"><span class="ln">1317 </span></a>    &gt;&gt;&gt; c 
<a name="l1318"><span class="ln">1318 </span></a>    tensor([3., 4., 5.]) 
<a name="l1319"><span class="ln">1319 </span></a>    &gt;&gt;&gt; # Shares memory with tensor 'b', retaining autograd history 
<a name="l1320"><span class="ln">1320 </span></a>    &gt;&gt;&gt; d = torch.asarray(b, requires_grad=True) 
<a name="l1321"><span class="ln">1321 </span></a>    &gt;&gt;&gt; d 
<a name="l1322"><span class="ln">1322 </span></a>    tensor([3., 4., 5.], grad_fn=&lt;AddBackward0&gt;) 
<a name="l1323"><span class="ln">1323 </span></a> 
<a name="l1324"><span class="ln">1324 </span></a>    &gt;&gt;&gt; array = numpy.array([1, 2, 3]) 
<a name="l1325"><span class="ln">1325 </span></a>    &gt;&gt;&gt; # Shares memory with array 'array' 
<a name="l1326"><span class="ln">1326 </span></a>    &gt;&gt;&gt; t1 = torch.asarray(array) 
<a name="l1327"><span class="ln">1327 </span></a>    &gt;&gt;&gt; array.__array_interface__['data'][0] == t1.data_ptr() 
<a name="l1328"><span class="ln">1328 </span></a>    True 
<a name="l1329"><span class="ln">1329 </span></a>    &gt;&gt;&gt; # Copies memory due to dtype mismatch 
<a name="l1330"><span class="ln">1330 </span></a>    &gt;&gt;&gt; t2 = torch.asarray(array, dtype=torch.float32) 
<a name="l1331"><span class="ln">1331 </span></a>    &gt;&gt;&gt; array.__array_interface__['data'][0] == t2.data_ptr() 
<a name="l1332"><span class="ln">1332 </span></a>    False 
<a name="l1333"><span class="ln">1333 </span></a> 
<a name="l1334"><span class="ln">1334 </span></a>    &gt;&gt;&gt; scalar = numpy.float64(0.5) 
<a name="l1335"><span class="ln">1335 </span></a>    &gt;&gt;&gt; torch.asarray(scalar) 
<a name="l1336"><span class="ln">1336 </span></a>    tensor(0.5000, dtype=torch.float64) 
<a name="l1337"><span class="ln">1337 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1338"><span class="ln">1338 </span></a><span class="s3">)</span>
<a name="l1339"><span class="ln">1339 </span></a>
<a name="l1340"><span class="ln">1340 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1341"><span class="ln">1341 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">baddbmm</span><span class="s3">,</span>
<a name="l1342"><span class="ln">1342 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1343"><span class="ln">1343 </span></a>baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l1344"><span class="ln">1344 </span></a> 
<a name="l1345"><span class="ln">1345 </span></a>Performs a batch matrix-matrix product of matrices in :attr:`batch1` 
<a name="l1346"><span class="ln">1346 </span></a>and :attr:`batch2`. 
<a name="l1347"><span class="ln">1347 </span></a>:attr:`input` is added to the final result. 
<a name="l1348"><span class="ln">1348 </span></a> 
<a name="l1349"><span class="ln">1349 </span></a>:attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same 
<a name="l1350"><span class="ln">1350 </span></a>number of matrices. 
<a name="l1351"><span class="ln">1351 </span></a> 
<a name="l1352"><span class="ln">1352 </span></a>If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l1353"><span class="ln">1353 </span></a>:math:`(b \times m \times p)` tensor, then :attr:`input` must be 
<a name="l1354"><span class="ln">1354 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 
<a name="l1355"><span class="ln">1355 </span></a>:math:`(b \times n \times p)` tensor and :attr:`out` will be a 
<a name="l1356"><span class="ln">1356 </span></a>:math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the 
<a name="l1357"><span class="ln">1357 </span></a>same as the scaling factors used in :meth:`torch.addbmm`. 
<a name="l1358"><span class="ln">1358 </span></a> 
<a name="l1359"><span class="ln">1359 </span></a>.. math:: 
<a name="l1360"><span class="ln">1360 </span></a>    \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l1361"><span class="ln">1361 </span></a> 
<a name="l1362"><span class="ln">1362 </span></a>If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l1363"><span class="ln">1363 </span></a>it will not be propagated. 
<a name="l1364"><span class="ln">1364 </span></a>&quot;&quot;&quot;</span>
<a name="l1365"><span class="ln">1365 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l1366"><span class="ln">1366 </span></a>For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l1367"><span class="ln">1367 </span></a>:attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l1368"><span class="ln">1368 </span></a> 
<a name="l1369"><span class="ln">1369 </span></a>{tf32_note} 
<a name="l1370"><span class="ln">1370 </span></a> 
<a name="l1371"><span class="ln">1371 </span></a>{rocm_fp16_note} 
<a name="l1372"><span class="ln">1372 </span></a> 
<a name="l1373"><span class="ln">1373 </span></a>Args: 
<a name="l1374"><span class="ln">1374 </span></a>    input (Tensor): the tensor to be added 
<a name="l1375"><span class="ln">1375 </span></a>    batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l1376"><span class="ln">1376 </span></a>    batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l1377"><span class="ln">1377 </span></a>    out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l1378"><span class="ln">1378 </span></a>        Supported only on CUDA and for torch.float32 given 
<a name="l1379"><span class="ln">1379 </span></a>        torch.float16/torch.bfloat16 input dtypes 
<a name="l1380"><span class="ln">1380 </span></a> 
<a name="l1381"><span class="ln">1381 </span></a>Keyword args: 
<a name="l1382"><span class="ln">1382 </span></a>    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l1383"><span class="ln">1383 </span></a>    alpha (Number, optional): multiplier for :math:`\text{{batch1}} \mathbin{{@}} \text{{batch2}}` (:math:`\alpha`) 
<a name="l1384"><span class="ln">1384 </span></a>    {out} 
<a name="l1385"><span class="ln">1385 </span></a> 
<a name="l1386"><span class="ln">1386 </span></a>Example:: 
<a name="l1387"><span class="ln">1387 </span></a> 
<a name="l1388"><span class="ln">1388 </span></a>    &gt;&gt;&gt; M = torch.randn(10, 3, 5) 
<a name="l1389"><span class="ln">1389 </span></a>    &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l1390"><span class="ln">1390 </span></a>    &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l1391"><span class="ln">1391 </span></a>    &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() 
<a name="l1392"><span class="ln">1392 </span></a>    torch.Size([10, 3, 5]) 
<a name="l1393"><span class="ln">1393 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">, </span><span class="s2">**</span><span class="s1">tf32_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">rocm_fp16_notes</span><span class="s3">),</span>
<a name="l1394"><span class="ln">1394 </span></a><span class="s3">)</span>
<a name="l1395"><span class="ln">1395 </span></a>
<a name="l1396"><span class="ln">1396 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1397"><span class="ln">1397 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bernoulli</span><span class="s3">,</span>
<a name="l1398"><span class="ln">1398 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1399"><span class="ln">1399 </span></a>bernoulli(input: Tensor, *, generator: Optional[Generator], out: Optional[Tensor]) -&gt; Tensor 
<a name="l1400"><span class="ln">1400 </span></a> 
<a name="l1401"><span class="ln">1401 </span></a>Draws binary random numbers (0 or 1) from a Bernoulli distribution. 
<a name="l1402"><span class="ln">1402 </span></a> 
<a name="l1403"><span class="ln">1403 </span></a>The :attr:`input` tensor should be a tensor containing probabilities 
<a name="l1404"><span class="ln">1404 </span></a>to be used for drawing the binary random number. 
<a name="l1405"><span class="ln">1405 </span></a>Hence, all values in :attr:`input` have to be in the range: 
<a name="l1406"><span class="ln">1406 </span></a>:math:`0 \leq \text{input}_i \leq 1`. 
<a name="l1407"><span class="ln">1407 </span></a> 
<a name="l1408"><span class="ln">1408 </span></a>The :math:`\text{i}^{th}` element of the output tensor will draw a 
<a name="l1409"><span class="ln">1409 </span></a>value :math:`1` according to the :math:`\text{i}^{th}` probability value given 
<a name="l1410"><span class="ln">1410 </span></a>in :attr:`input`. 
<a name="l1411"><span class="ln">1411 </span></a> 
<a name="l1412"><span class="ln">1412 </span></a>.. math:: 
<a name="l1413"><span class="ln">1413 </span></a>    \text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i}) 
<a name="l1414"><span class="ln">1414 </span></a>&quot;&quot;&quot;</span>
<a name="l1415"><span class="ln">1415 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l1416"><span class="ln">1416 </span></a>The returned :attr:`out` tensor only has values 0 or 1 and is of the same 
<a name="l1417"><span class="ln">1417 </span></a>shape as :attr:`input`. 
<a name="l1418"><span class="ln">1418 </span></a> 
<a name="l1419"><span class="ln">1419 </span></a>:attr:`out` can have integral ``dtype``, but :attr:`input` must have floating 
<a name="l1420"><span class="ln">1420 </span></a>point ``dtype``. 
<a name="l1421"><span class="ln">1421 </span></a> 
<a name="l1422"><span class="ln">1422 </span></a>Args: 
<a name="l1423"><span class="ln">1423 </span></a>    input (Tensor): the input tensor of probability values for the Bernoulli distribution 
<a name="l1424"><span class="ln">1424 </span></a> 
<a name="l1425"><span class="ln">1425 </span></a>Keyword args: 
<a name="l1426"><span class="ln">1426 </span></a>    {generator} 
<a name="l1427"><span class="ln">1427 </span></a>    {out} 
<a name="l1428"><span class="ln">1428 </span></a> 
<a name="l1429"><span class="ln">1429 </span></a>Example:: 
<a name="l1430"><span class="ln">1430 </span></a> 
<a name="l1431"><span class="ln">1431 </span></a>    &gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1] 
<a name="l1432"><span class="ln">1432 </span></a>    &gt;&gt;&gt; a 
<a name="l1433"><span class="ln">1433 </span></a>    tensor([[ 0.1737,  0.0950,  0.3609], 
<a name="l1434"><span class="ln">1434 </span></a>            [ 0.7148,  0.0289,  0.2676], 
<a name="l1435"><span class="ln">1435 </span></a>            [ 0.9456,  0.8937,  0.7202]]) 
<a name="l1436"><span class="ln">1436 </span></a>    &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l1437"><span class="ln">1437 </span></a>    tensor([[ 1.,  0.,  0.], 
<a name="l1438"><span class="ln">1438 </span></a>            [ 0.,  0.,  0.], 
<a name="l1439"><span class="ln">1439 </span></a>            [ 1.,  1.,  1.]]) 
<a name="l1440"><span class="ln">1440 </span></a> 
<a name="l1441"><span class="ln">1441 </span></a>    &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1 
<a name="l1442"><span class="ln">1442 </span></a>    &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l1443"><span class="ln">1443 </span></a>    tensor([[ 1.,  1.,  1.], 
<a name="l1444"><span class="ln">1444 </span></a>            [ 1.,  1.,  1.], 
<a name="l1445"><span class="ln">1445 </span></a>            [ 1.,  1.,  1.]]) 
<a name="l1446"><span class="ln">1446 </span></a>    &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0 
<a name="l1447"><span class="ln">1447 </span></a>    &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l1448"><span class="ln">1448 </span></a>    tensor([[ 0.,  0.,  0.], 
<a name="l1449"><span class="ln">1449 </span></a>            [ 0.,  0.,  0.], 
<a name="l1450"><span class="ln">1450 </span></a>            [ 0.,  0.,  0.]]) 
<a name="l1451"><span class="ln">1451 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1452"><span class="ln">1452 </span></a><span class="s3">)</span>
<a name="l1453"><span class="ln">1453 </span></a>
<a name="l1454"><span class="ln">1454 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1455"><span class="ln">1455 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bincount</span><span class="s3">,</span>
<a name="l1456"><span class="ln">1456 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1457"><span class="ln">1457 </span></a>bincount(input, weights=None, minlength=0) -&gt; Tensor 
<a name="l1458"><span class="ln">1458 </span></a> 
<a name="l1459"><span class="ln">1459 </span></a>Count the frequency of each value in an array of non-negative ints. 
<a name="l1460"><span class="ln">1460 </span></a> 
<a name="l1461"><span class="ln">1461 </span></a>The number of bins (size 1) is one larger than the largest value in 
<a name="l1462"><span class="ln">1462 </span></a>:attr:`input` unless :attr:`input` is empty, in which case the result is a 
<a name="l1463"><span class="ln">1463 </span></a>tensor of size 0. If :attr:`minlength` is specified, the number of bins is at least 
<a name="l1464"><span class="ln">1464 </span></a>:attr:`minlength` and if :attr:`input` is empty, then the result is tensor of size 
<a name="l1465"><span class="ln">1465 </span></a>:attr:`minlength` filled with zeros. If ``n`` is the value at position ``i``, 
<a name="l1466"><span class="ln">1466 </span></a>``out[n] += weights[i]`` if :attr:`weights` is specified else 
<a name="l1467"><span class="ln">1467 </span></a>``out[n] += 1``. 
<a name="l1468"><span class="ln">1468 </span></a> 
<a name="l1469"><span class="ln">1469 </span></a>Note: 
<a name="l1470"><span class="ln">1470 </span></a>    {backward_reproducibility_note} 
<a name="l1471"><span class="ln">1471 </span></a> 
<a name="l1472"><span class="ln">1472 </span></a>Arguments: 
<a name="l1473"><span class="ln">1473 </span></a>    input (Tensor): 1-d int tensor 
<a name="l1474"><span class="ln">1474 </span></a>    weights (Tensor): optional, weight for each value in the input tensor. 
<a name="l1475"><span class="ln">1475 </span></a>        Should be of same size as input tensor. 
<a name="l1476"><span class="ln">1476 </span></a>    minlength (int): optional, minimum number of bins. Should be non-negative. 
<a name="l1477"><span class="ln">1477 </span></a> 
<a name="l1478"><span class="ln">1478 </span></a>Returns: 
<a name="l1479"><span class="ln">1479 </span></a>    output (Tensor): a tensor of shape ``Size([max(input) + 1])`` if 
<a name="l1480"><span class="ln">1480 </span></a>    :attr:`input` is non-empty, else ``Size(0)`` 
<a name="l1481"><span class="ln">1481 </span></a> 
<a name="l1482"><span class="ln">1482 </span></a>Example:: 
<a name="l1483"><span class="ln">1483 </span></a> 
<a name="l1484"><span class="ln">1484 </span></a>    &gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64) 
<a name="l1485"><span class="ln">1485 </span></a>    &gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5) 
<a name="l1486"><span class="ln">1486 </span></a>    &gt;&gt;&gt; input, weights 
<a name="l1487"><span class="ln">1487 </span></a>    (tensor([4, 3, 6, 3, 4]), 
<a name="l1488"><span class="ln">1488 </span></a>     tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000]) 
<a name="l1489"><span class="ln">1489 </span></a> 
<a name="l1490"><span class="ln">1490 </span></a>    &gt;&gt;&gt; torch.bincount(input) 
<a name="l1491"><span class="ln">1491 </span></a>    tensor([0, 0, 0, 2, 2, 0, 1]) 
<a name="l1492"><span class="ln">1492 </span></a> 
<a name="l1493"><span class="ln">1493 </span></a>    &gt;&gt;&gt; input.bincount(weights) 
<a name="l1494"><span class="ln">1494 </span></a>    tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000]) 
<a name="l1495"><span class="ln">1495 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reproducibility_notes</span><span class="s3">),</span>
<a name="l1496"><span class="ln">1496 </span></a><span class="s3">)</span>
<a name="l1497"><span class="ln">1497 </span></a>
<a name="l1498"><span class="ln">1498 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1499"><span class="ln">1499 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bitwise_not</span><span class="s3">,</span>
<a name="l1500"><span class="ln">1500 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1501"><span class="ln">1501 </span></a>bitwise_not(input, *, out=None) -&gt; Tensor 
<a name="l1502"><span class="ln">1502 </span></a> 
<a name="l1503"><span class="ln">1503 </span></a>Computes the bitwise NOT of the given input tensor. The input tensor must be of 
<a name="l1504"><span class="ln">1504 </span></a>integral or Boolean types. For bool tensors, it computes the logical NOT. 
<a name="l1505"><span class="ln">1505 </span></a> 
<a name="l1506"><span class="ln">1506 </span></a>Args: 
<a name="l1507"><span class="ln">1507 </span></a>    {input} 
<a name="l1508"><span class="ln">1508 </span></a> 
<a name="l1509"><span class="ln">1509 </span></a>Keyword args: 
<a name="l1510"><span class="ln">1510 </span></a>    {out} 
<a name="l1511"><span class="ln">1511 </span></a> 
<a name="l1512"><span class="ln">1512 </span></a>Example:: 
<a name="l1513"><span class="ln">1513 </span></a> 
<a name="l1514"><span class="ln">1514 </span></a>    &gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8)) 
<a name="l1515"><span class="ln">1515 </span></a>    tensor([ 0,  1, -4], dtype=torch.int8) 
<a name="l1516"><span class="ln">1516 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1517"><span class="ln">1517 </span></a><span class="s3">)</span>
<a name="l1518"><span class="ln">1518 </span></a>
<a name="l1519"><span class="ln">1519 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1520"><span class="ln">1520 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bmm</span><span class="s3">,</span>
<a name="l1521"><span class="ln">1521 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1522"><span class="ln">1522 </span></a>bmm(input, mat2, out_dtype=None, *, out=None) -&gt; Tensor 
<a name="l1523"><span class="ln">1523 </span></a> 
<a name="l1524"><span class="ln">1524 </span></a>Performs a batch matrix-matrix product of matrices stored in :attr:`input` 
<a name="l1525"><span class="ln">1525 </span></a>and :attr:`mat2`. 
<a name="l1526"><span class="ln">1526 </span></a> 
<a name="l1527"><span class="ln">1527 </span></a>:attr:`input` and :attr:`mat2` must be 3-D tensors each containing 
<a name="l1528"><span class="ln">1528 </span></a>the same number of matrices. 
<a name="l1529"><span class="ln">1529 </span></a> 
<a name="l1530"><span class="ln">1530 </span></a>If :attr:`input` is a :math:`(b \times n \times m)` tensor, :attr:`mat2` is a 
<a name="l1531"><span class="ln">1531 </span></a>:math:`(b \times m \times p)` tensor, :attr:`out` will be a 
<a name="l1532"><span class="ln">1532 </span></a>:math:`(b \times n \times p)` tensor. 
<a name="l1533"><span class="ln">1533 </span></a> 
<a name="l1534"><span class="ln">1534 </span></a>.. math:: 
<a name="l1535"><span class="ln">1535 </span></a>    \text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i 
<a name="l1536"><span class="ln">1536 </span></a>&quot;&quot;&quot;</span>
<a name="l1537"><span class="ln">1537 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l1538"><span class="ln">1538 </span></a>{tf32_note} 
<a name="l1539"><span class="ln">1539 </span></a> 
<a name="l1540"><span class="ln">1540 </span></a>{rocm_fp16_note} 
<a name="l1541"><span class="ln">1541 </span></a> 
<a name="l1542"><span class="ln">1542 </span></a>.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l1543"><span class="ln">1543 </span></a>          For broadcasting matrix products, see :func:`torch.matmul`. 
<a name="l1544"><span class="ln">1544 </span></a> 
<a name="l1545"><span class="ln">1545 </span></a>Args: 
<a name="l1546"><span class="ln">1546 </span></a>    input (Tensor): the first batch of matrices to be multiplied 
<a name="l1547"><span class="ln">1547 </span></a>    mat2 (Tensor): the second batch of matrices to be multiplied 
<a name="l1548"><span class="ln">1548 </span></a>    out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l1549"><span class="ln">1549 </span></a>        Supported only on CUDA and for torch.float32 given 
<a name="l1550"><span class="ln">1550 </span></a>        torch.float16/torch.bfloat16 input dtypes 
<a name="l1551"><span class="ln">1551 </span></a> 
<a name="l1552"><span class="ln">1552 </span></a>Keyword Args: 
<a name="l1553"><span class="ln">1553 </span></a>    {out} 
<a name="l1554"><span class="ln">1554 </span></a> 
<a name="l1555"><span class="ln">1555 </span></a>Example:: 
<a name="l1556"><span class="ln">1556 </span></a> 
<a name="l1557"><span class="ln">1557 </span></a>    &gt;&gt;&gt; input = torch.randn(10, 3, 4) 
<a name="l1558"><span class="ln">1558 </span></a>    &gt;&gt;&gt; mat2 = torch.randn(10, 4, 5) 
<a name="l1559"><span class="ln">1559 </span></a>    &gt;&gt;&gt; res = torch.bmm(input, mat2) 
<a name="l1560"><span class="ln">1560 </span></a>    &gt;&gt;&gt; res.size() 
<a name="l1561"><span class="ln">1561 </span></a>    torch.Size([10, 3, 5]) 
<a name="l1562"><span class="ln">1562 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">, </span><span class="s2">**</span><span class="s1">tf32_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">rocm_fp16_notes</span><span class="s3">),</span>
<a name="l1563"><span class="ln">1563 </span></a><span class="s3">)</span>
<a name="l1564"><span class="ln">1564 </span></a>
<a name="l1565"><span class="ln">1565 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1566"><span class="ln">1566 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bitwise_and</span><span class="s3">,</span>
<a name="l1567"><span class="ln">1567 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1568"><span class="ln">1568 </span></a>bitwise_and(input, other, *, out=None) -&gt; Tensor 
<a name="l1569"><span class="ln">1569 </span></a> 
<a name="l1570"><span class="ln">1570 </span></a>Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l1571"><span class="ln">1571 </span></a>integral or Boolean types. For bool tensors, it computes the logical AND. 
<a name="l1572"><span class="ln">1572 </span></a> 
<a name="l1573"><span class="ln">1573 </span></a>Args: 
<a name="l1574"><span class="ln">1574 </span></a>    input: the first input tensor 
<a name="l1575"><span class="ln">1575 </span></a>    other: the second input tensor 
<a name="l1576"><span class="ln">1576 </span></a> 
<a name="l1577"><span class="ln">1577 </span></a>Keyword args: 
<a name="l1578"><span class="ln">1578 </span></a>    {out} 
<a name="l1579"><span class="ln">1579 </span></a> 
<a name="l1580"><span class="ln">1580 </span></a>Example:: 
<a name="l1581"><span class="ln">1581 </span></a> 
<a name="l1582"><span class="ln">1582 </span></a>    &gt;&gt;&gt; torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l1583"><span class="ln">1583 </span></a>    tensor([1, 0,  3], dtype=torch.int8) 
<a name="l1584"><span class="ln">1584 </span></a>    &gt;&gt;&gt; torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l1585"><span class="ln">1585 </span></a>    tensor([ False, True, False]) 
<a name="l1586"><span class="ln">1586 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1587"><span class="ln">1587 </span></a><span class="s3">)</span>
<a name="l1588"><span class="ln">1588 </span></a>
<a name="l1589"><span class="ln">1589 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1590"><span class="ln">1590 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bitwise_or</span><span class="s3">,</span>
<a name="l1591"><span class="ln">1591 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1592"><span class="ln">1592 </span></a>bitwise_or(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l1593"><span class="ln">1593 </span></a> 
<a name="l1594"><span class="ln">1594 </span></a>Computes the bitwise OR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l1595"><span class="ln">1595 </span></a>integral or Boolean types. For bool tensors, it computes the logical OR. 
<a name="l1596"><span class="ln">1596 </span></a> 
<a name="l1597"><span class="ln">1597 </span></a>Args: 
<a name="l1598"><span class="ln">1598 </span></a>    input: the first input tensor 
<a name="l1599"><span class="ln">1599 </span></a>    other: the second input tensor 
<a name="l1600"><span class="ln">1600 </span></a> 
<a name="l1601"><span class="ln">1601 </span></a>Keyword args: 
<a name="l1602"><span class="ln">1602 </span></a>    {out} 
<a name="l1603"><span class="ln">1603 </span></a> 
<a name="l1604"><span class="ln">1604 </span></a>Example:: 
<a name="l1605"><span class="ln">1605 </span></a> 
<a name="l1606"><span class="ln">1606 </span></a>    &gt;&gt;&gt; torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l1607"><span class="ln">1607 </span></a>    tensor([-1, -2,  3], dtype=torch.int8) 
<a name="l1608"><span class="ln">1608 </span></a>    &gt;&gt;&gt; torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l1609"><span class="ln">1609 </span></a>    tensor([ True, True, False]) 
<a name="l1610"><span class="ln">1610 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1611"><span class="ln">1611 </span></a><span class="s3">)</span>
<a name="l1612"><span class="ln">1612 </span></a>
<a name="l1613"><span class="ln">1613 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1614"><span class="ln">1614 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bitwise_xor</span><span class="s3">,</span>
<a name="l1615"><span class="ln">1615 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1616"><span class="ln">1616 </span></a>bitwise_xor(input, other, *, out=None) -&gt; Tensor 
<a name="l1617"><span class="ln">1617 </span></a> 
<a name="l1618"><span class="ln">1618 </span></a>Computes the bitwise XOR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l1619"><span class="ln">1619 </span></a>integral or Boolean types. For bool tensors, it computes the logical XOR. 
<a name="l1620"><span class="ln">1620 </span></a> 
<a name="l1621"><span class="ln">1621 </span></a>Args: 
<a name="l1622"><span class="ln">1622 </span></a>    input: the first input tensor 
<a name="l1623"><span class="ln">1623 </span></a>    other: the second input tensor 
<a name="l1624"><span class="ln">1624 </span></a> 
<a name="l1625"><span class="ln">1625 </span></a>Keyword args: 
<a name="l1626"><span class="ln">1626 </span></a>    {out} 
<a name="l1627"><span class="ln">1627 </span></a> 
<a name="l1628"><span class="ln">1628 </span></a>Example:: 
<a name="l1629"><span class="ln">1629 </span></a> 
<a name="l1630"><span class="ln">1630 </span></a>    &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l1631"><span class="ln">1631 </span></a>    tensor([-2, -2,  0], dtype=torch.int8) 
<a name="l1632"><span class="ln">1632 </span></a>    &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l1633"><span class="ln">1633 </span></a>    tensor([ True, False, False]) 
<a name="l1634"><span class="ln">1634 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1635"><span class="ln">1635 </span></a><span class="s3">)</span>
<a name="l1636"><span class="ln">1636 </span></a>
<a name="l1637"><span class="ln">1637 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1638"><span class="ln">1638 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bitwise_left_shift</span><span class="s3">,</span>
<a name="l1639"><span class="ln">1639 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1640"><span class="ln">1640 </span></a>bitwise_left_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l1641"><span class="ln">1641 </span></a> 
<a name="l1642"><span class="ln">1642 </span></a>Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l1643"><span class="ln">1643 </span></a>The input tensor must be of integral type. This operator supports 
<a name="l1644"><span class="ln">1644 </span></a>:ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l1645"><span class="ln">1645 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l1646"><span class="ln">1646 </span></a> 
<a name="l1647"><span class="ln">1647 </span></a>The operation applied is: 
<a name="l1648"><span class="ln">1648 </span></a> 
<a name="l1649"><span class="ln">1649 </span></a>.. math:: 
<a name="l1650"><span class="ln">1650 </span></a>    \text{{out}}_i = \text{{input}}_i &lt;&lt; \text{{other}}_i 
<a name="l1651"><span class="ln">1651 </span></a> 
<a name="l1652"><span class="ln">1652 </span></a>Args: 
<a name="l1653"><span class="ln">1653 </span></a>    input (Tensor or Scalar): the first input tensor 
<a name="l1654"><span class="ln">1654 </span></a>    other (Tensor or Scalar): the second input tensor 
<a name="l1655"><span class="ln">1655 </span></a> 
<a name="l1656"><span class="ln">1656 </span></a>Keyword args: 
<a name="l1657"><span class="ln">1657 </span></a>    {out} 
<a name="l1658"><span class="ln">1658 </span></a> 
<a name="l1659"><span class="ln">1659 </span></a>Example:: 
<a name="l1660"><span class="ln">1660 </span></a> 
<a name="l1661"><span class="ln">1661 </span></a>    &gt;&gt;&gt; torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l1662"><span class="ln">1662 </span></a>    tensor([-2, -2, 24], dtype=torch.int8) 
<a name="l1663"><span class="ln">1663 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1664"><span class="ln">1664 </span></a><span class="s3">)</span>
<a name="l1665"><span class="ln">1665 </span></a>
<a name="l1666"><span class="ln">1666 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1667"><span class="ln">1667 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bitwise_right_shift</span><span class="s3">,</span>
<a name="l1668"><span class="ln">1668 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1669"><span class="ln">1669 </span></a>bitwise_right_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l1670"><span class="ln">1670 </span></a> 
<a name="l1671"><span class="ln">1671 </span></a>Computes the right arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l1672"><span class="ln">1672 </span></a>The input tensor must be of integral type. This operator supports 
<a name="l1673"><span class="ln">1673 </span></a>:ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l1674"><span class="ln">1674 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l1675"><span class="ln">1675 </span></a>In any case, if the value of the right operand is negative or is greater 
<a name="l1676"><span class="ln">1676 </span></a>or equal to the number of bits in the promoted left operand, the behavior is undefined. 
<a name="l1677"><span class="ln">1677 </span></a> 
<a name="l1678"><span class="ln">1678 </span></a>The operation applied is: 
<a name="l1679"><span class="ln">1679 </span></a> 
<a name="l1680"><span class="ln">1680 </span></a>.. math:: 
<a name="l1681"><span class="ln">1681 </span></a>    \text{{out}}_i = \text{{input}}_i &gt;&gt; \text{{other}}_i 
<a name="l1682"><span class="ln">1682 </span></a> 
<a name="l1683"><span class="ln">1683 </span></a>Args: 
<a name="l1684"><span class="ln">1684 </span></a>    input (Tensor or Scalar): the first input tensor 
<a name="l1685"><span class="ln">1685 </span></a>    other (Tensor or Scalar): the second input tensor 
<a name="l1686"><span class="ln">1686 </span></a> 
<a name="l1687"><span class="ln">1687 </span></a>Keyword args: 
<a name="l1688"><span class="ln">1688 </span></a>    {out} 
<a name="l1689"><span class="ln">1689 </span></a> 
<a name="l1690"><span class="ln">1690 </span></a>Example:: 
<a name="l1691"><span class="ln">1691 </span></a> 
<a name="l1692"><span class="ln">1692 </span></a>    &gt;&gt;&gt; torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l1693"><span class="ln">1693 </span></a>    tensor([-1, -7,  3], dtype=torch.int8) 
<a name="l1694"><span class="ln">1694 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1695"><span class="ln">1695 </span></a><span class="s3">)</span>
<a name="l1696"><span class="ln">1696 </span></a>
<a name="l1697"><span class="ln">1697 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1698"><span class="ln">1698 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">broadcast_to</span><span class="s3">,</span>
<a name="l1699"><span class="ln">1699 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1700"><span class="ln">1700 </span></a>broadcast_to(input, shape) -&gt; Tensor 
<a name="l1701"><span class="ln">1701 </span></a> 
<a name="l1702"><span class="ln">1702 </span></a>Broadcasts :attr:`input` to the shape :attr:`\shape`. 
<a name="l1703"><span class="ln">1703 </span></a>Equivalent to calling ``input.expand(shape)``. See :meth:`~Tensor.expand` for details. 
<a name="l1704"><span class="ln">1704 </span></a> 
<a name="l1705"><span class="ln">1705 </span></a>Args: 
<a name="l1706"><span class="ln">1706 </span></a>    {input} 
<a name="l1707"><span class="ln">1707 </span></a>    shape (list, tuple, or :class:`torch.Size`): the new shape. 
<a name="l1708"><span class="ln">1708 </span></a> 
<a name="l1709"><span class="ln">1709 </span></a>Example:: 
<a name="l1710"><span class="ln">1710 </span></a> 
<a name="l1711"><span class="ln">1711 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l1712"><span class="ln">1712 </span></a>    &gt;&gt;&gt; torch.broadcast_to(x, (3, 3)) 
<a name="l1713"><span class="ln">1713 </span></a>    tensor([[1, 2, 3], 
<a name="l1714"><span class="ln">1714 </span></a>            [1, 2, 3], 
<a name="l1715"><span class="ln">1715 </span></a>            [1, 2, 3]]) 
<a name="l1716"><span class="ln">1716 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1717"><span class="ln">1717 </span></a><span class="s3">)</span>
<a name="l1718"><span class="ln">1718 </span></a>
<a name="l1719"><span class="ln">1719 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1720"><span class="ln">1720 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">stack</span><span class="s3">,</span>
<a name="l1721"><span class="ln">1721 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1722"><span class="ln">1722 </span></a>stack(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l1723"><span class="ln">1723 </span></a> 
<a name="l1724"><span class="ln">1724 </span></a>Concatenates a sequence of tensors along a new dimension. 
<a name="l1725"><span class="ln">1725 </span></a> 
<a name="l1726"><span class="ln">1726 </span></a>All tensors need to be of the same size. 
<a name="l1727"><span class="ln">1727 </span></a> 
<a name="l1728"><span class="ln">1728 </span></a>.. seealso:: 
<a name="l1729"><span class="ln">1729 </span></a> 
<a name="l1730"><span class="ln">1730 </span></a>    :func:`torch.cat` concatenates the given sequence along an existing dimension. 
<a name="l1731"><span class="ln">1731 </span></a> 
<a name="l1732"><span class="ln">1732 </span></a>Arguments: 
<a name="l1733"><span class="ln">1733 </span></a>    tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l1734"><span class="ln">1734 </span></a>    dim (int, optional): dimension to insert. Has to be between 0 and the number 
<a name="l1735"><span class="ln">1735 </span></a>        of dimensions of concatenated tensors (inclusive). Default: 0 
<a name="l1736"><span class="ln">1736 </span></a> 
<a name="l1737"><span class="ln">1737 </span></a>Keyword args: 
<a name="l1738"><span class="ln">1738 </span></a>    {out} 
<a name="l1739"><span class="ln">1739 </span></a> 
<a name="l1740"><span class="ln">1740 </span></a>Example:: 
<a name="l1741"><span class="ln">1741 </span></a> 
<a name="l1742"><span class="ln">1742 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l1743"><span class="ln">1743 </span></a>    &gt;&gt;&gt; x 
<a name="l1744"><span class="ln">1744 </span></a>    tensor([[ 0.3367,  0.1288,  0.2345], 
<a name="l1745"><span class="ln">1745 </span></a>            [ 0.2303, -1.1229, -0.1863]]) 
<a name="l1746"><span class="ln">1746 </span></a>    &gt;&gt;&gt; torch.stack((x, x)) # same as torch.stack((x, x), dim=0) 
<a name="l1747"><span class="ln">1747 </span></a>    tensor([[[ 0.3367,  0.1288,  0.2345], 
<a name="l1748"><span class="ln">1748 </span></a>             [ 0.2303, -1.1229, -0.1863]], 
<a name="l1749"><span class="ln">1749 </span></a> 
<a name="l1750"><span class="ln">1750 </span></a>            [[ 0.3367,  0.1288,  0.2345], 
<a name="l1751"><span class="ln">1751 </span></a>             [ 0.2303, -1.1229, -0.1863]]]) 
<a name="l1752"><span class="ln">1752 </span></a>    &gt;&gt;&gt; torch.stack((x, x)).size() 
<a name="l1753"><span class="ln">1753 </span></a>    torch.Size([2, 2, 3]) 
<a name="l1754"><span class="ln">1754 </span></a>    &gt;&gt;&gt; torch.stack((x, x), dim=1) 
<a name="l1755"><span class="ln">1755 </span></a>    tensor([[[ 0.3367,  0.1288,  0.2345], 
<a name="l1756"><span class="ln">1756 </span></a>             [ 0.3367,  0.1288,  0.2345]], 
<a name="l1757"><span class="ln">1757 </span></a> 
<a name="l1758"><span class="ln">1758 </span></a>            [[ 0.2303, -1.1229, -0.1863], 
<a name="l1759"><span class="ln">1759 </span></a>             [ 0.2303, -1.1229, -0.1863]]]) 
<a name="l1760"><span class="ln">1760 </span></a>    &gt;&gt;&gt; torch.stack((x, x), dim=2) 
<a name="l1761"><span class="ln">1761 </span></a>    tensor([[[ 0.3367,  0.3367], 
<a name="l1762"><span class="ln">1762 </span></a>             [ 0.1288,  0.1288], 
<a name="l1763"><span class="ln">1763 </span></a>             [ 0.2345,  0.2345]], 
<a name="l1764"><span class="ln">1764 </span></a> 
<a name="l1765"><span class="ln">1765 </span></a>            [[ 0.2303,  0.2303], 
<a name="l1766"><span class="ln">1766 </span></a>             [-1.1229, -1.1229], 
<a name="l1767"><span class="ln">1767 </span></a>             [-0.1863, -0.1863]]]) 
<a name="l1768"><span class="ln">1768 </span></a>    &gt;&gt;&gt; torch.stack((x, x), dim=-1) 
<a name="l1769"><span class="ln">1769 </span></a>    tensor([[[ 0.3367,  0.3367], 
<a name="l1770"><span class="ln">1770 </span></a>             [ 0.1288,  0.1288], 
<a name="l1771"><span class="ln">1771 </span></a>             [ 0.2345,  0.2345]], 
<a name="l1772"><span class="ln">1772 </span></a> 
<a name="l1773"><span class="ln">1773 </span></a>            [[ 0.2303,  0.2303], 
<a name="l1774"><span class="ln">1774 </span></a>             [-1.1229, -1.1229], 
<a name="l1775"><span class="ln">1775 </span></a>             [-0.1863, -0.1863]]]) 
<a name="l1776"><span class="ln">1776 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1777"><span class="ln">1777 </span></a><span class="s3">)</span>
<a name="l1778"><span class="ln">1778 </span></a>
<a name="l1779"><span class="ln">1779 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1780"><span class="ln">1780 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">hstack</span><span class="s3">,</span>
<a name="l1781"><span class="ln">1781 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1782"><span class="ln">1782 </span></a>hstack(tensors, *, out=None) -&gt; Tensor 
<a name="l1783"><span class="ln">1783 </span></a> 
<a name="l1784"><span class="ln">1784 </span></a>Stack tensors in sequence horizontally (column wise). 
<a name="l1785"><span class="ln">1785 </span></a> 
<a name="l1786"><span class="ln">1786 </span></a>This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors. 
<a name="l1787"><span class="ln">1787 </span></a> 
<a name="l1788"><span class="ln">1788 </span></a>Args: 
<a name="l1789"><span class="ln">1789 </span></a>    tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l1790"><span class="ln">1790 </span></a> 
<a name="l1791"><span class="ln">1791 </span></a>Keyword args: 
<a name="l1792"><span class="ln">1792 </span></a>    {out} 
<a name="l1793"><span class="ln">1793 </span></a> 
<a name="l1794"><span class="ln">1794 </span></a>Example:: 
<a name="l1795"><span class="ln">1795 </span></a> 
<a name="l1796"><span class="ln">1796 </span></a>    &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l1797"><span class="ln">1797 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l1798"><span class="ln">1798 </span></a>    &gt;&gt;&gt; torch.hstack((a,b)) 
<a name="l1799"><span class="ln">1799 </span></a>    tensor([1, 2, 3, 4, 5, 6]) 
<a name="l1800"><span class="ln">1800 </span></a>    &gt;&gt;&gt; a = torch.tensor([[1],[2],[3]]) 
<a name="l1801"><span class="ln">1801 </span></a>    &gt;&gt;&gt; b = torch.tensor([[4],[5],[6]]) 
<a name="l1802"><span class="ln">1802 </span></a>    &gt;&gt;&gt; torch.hstack((a,b)) 
<a name="l1803"><span class="ln">1803 </span></a>    tensor([[1, 4], 
<a name="l1804"><span class="ln">1804 </span></a>            [2, 5], 
<a name="l1805"><span class="ln">1805 </span></a>            [3, 6]]) 
<a name="l1806"><span class="ln">1806 </span></a> 
<a name="l1807"><span class="ln">1807 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1808"><span class="ln">1808 </span></a><span class="s3">)</span>
<a name="l1809"><span class="ln">1809 </span></a>
<a name="l1810"><span class="ln">1810 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1811"><span class="ln">1811 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">vstack</span><span class="s3">,</span>
<a name="l1812"><span class="ln">1812 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1813"><span class="ln">1813 </span></a>vstack(tensors, *, out=None) -&gt; Tensor 
<a name="l1814"><span class="ln">1814 </span></a> 
<a name="l1815"><span class="ln">1815 </span></a>Stack tensors in sequence vertically (row wise). 
<a name="l1816"><span class="ln">1816 </span></a> 
<a name="l1817"><span class="ln">1817 </span></a>This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by :func:`torch.atleast_2d`. 
<a name="l1818"><span class="ln">1818 </span></a> 
<a name="l1819"><span class="ln">1819 </span></a>Args: 
<a name="l1820"><span class="ln">1820 </span></a>    tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l1821"><span class="ln">1821 </span></a> 
<a name="l1822"><span class="ln">1822 </span></a>Keyword args: 
<a name="l1823"><span class="ln">1823 </span></a>    {out} 
<a name="l1824"><span class="ln">1824 </span></a> 
<a name="l1825"><span class="ln">1825 </span></a>Example:: 
<a name="l1826"><span class="ln">1826 </span></a> 
<a name="l1827"><span class="ln">1827 </span></a>    &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l1828"><span class="ln">1828 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l1829"><span class="ln">1829 </span></a>    &gt;&gt;&gt; torch.vstack((a,b)) 
<a name="l1830"><span class="ln">1830 </span></a>    tensor([[1, 2, 3], 
<a name="l1831"><span class="ln">1831 </span></a>            [4, 5, 6]]) 
<a name="l1832"><span class="ln">1832 </span></a>    &gt;&gt;&gt; a = torch.tensor([[1],[2],[3]]) 
<a name="l1833"><span class="ln">1833 </span></a>    &gt;&gt;&gt; b = torch.tensor([[4],[5],[6]]) 
<a name="l1834"><span class="ln">1834 </span></a>    &gt;&gt;&gt; torch.vstack((a,b)) 
<a name="l1835"><span class="ln">1835 </span></a>    tensor([[1], 
<a name="l1836"><span class="ln">1836 </span></a>            [2], 
<a name="l1837"><span class="ln">1837 </span></a>            [3], 
<a name="l1838"><span class="ln">1838 </span></a>            [4], 
<a name="l1839"><span class="ln">1839 </span></a>            [5], 
<a name="l1840"><span class="ln">1840 </span></a>            [6]]) 
<a name="l1841"><span class="ln">1841 </span></a> 
<a name="l1842"><span class="ln">1842 </span></a> 
<a name="l1843"><span class="ln">1843 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1844"><span class="ln">1844 </span></a><span class="s3">)</span>
<a name="l1845"><span class="ln">1845 </span></a>
<a name="l1846"><span class="ln">1846 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1847"><span class="ln">1847 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">dstack</span><span class="s3">,</span>
<a name="l1848"><span class="ln">1848 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1849"><span class="ln">1849 </span></a>dstack(tensors, *, out=None) -&gt; Tensor 
<a name="l1850"><span class="ln">1850 </span></a> 
<a name="l1851"><span class="ln">1851 </span></a>Stack tensors in sequence depthwise (along third axis). 
<a name="l1852"><span class="ln">1852 </span></a> 
<a name="l1853"><span class="ln">1853 </span></a>This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by :func:`torch.atleast_3d`. 
<a name="l1854"><span class="ln">1854 </span></a> 
<a name="l1855"><span class="ln">1855 </span></a>Args: 
<a name="l1856"><span class="ln">1856 </span></a>    tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l1857"><span class="ln">1857 </span></a> 
<a name="l1858"><span class="ln">1858 </span></a>Keyword args: 
<a name="l1859"><span class="ln">1859 </span></a>    {out} 
<a name="l1860"><span class="ln">1860 </span></a> 
<a name="l1861"><span class="ln">1861 </span></a>Example:: 
<a name="l1862"><span class="ln">1862 </span></a> 
<a name="l1863"><span class="ln">1863 </span></a>    &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l1864"><span class="ln">1864 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l1865"><span class="ln">1865 </span></a>    &gt;&gt;&gt; torch.dstack((a,b)) 
<a name="l1866"><span class="ln">1866 </span></a>    tensor([[[1, 4], 
<a name="l1867"><span class="ln">1867 </span></a>             [2, 5], 
<a name="l1868"><span class="ln">1868 </span></a>             [3, 6]]]) 
<a name="l1869"><span class="ln">1869 </span></a>    &gt;&gt;&gt; a = torch.tensor([[1],[2],[3]]) 
<a name="l1870"><span class="ln">1870 </span></a>    &gt;&gt;&gt; b = torch.tensor([[4],[5],[6]]) 
<a name="l1871"><span class="ln">1871 </span></a>    &gt;&gt;&gt; torch.dstack((a,b)) 
<a name="l1872"><span class="ln">1872 </span></a>    tensor([[[1, 4]], 
<a name="l1873"><span class="ln">1873 </span></a>            [[2, 5]], 
<a name="l1874"><span class="ln">1874 </span></a>            [[3, 6]]]) 
<a name="l1875"><span class="ln">1875 </span></a> 
<a name="l1876"><span class="ln">1876 </span></a> 
<a name="l1877"><span class="ln">1877 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1878"><span class="ln">1878 </span></a><span class="s3">)</span>
<a name="l1879"><span class="ln">1879 </span></a>
<a name="l1880"><span class="ln">1880 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1881"><span class="ln">1881 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">tensor_split</span><span class="s3">,</span>
<a name="l1882"><span class="ln">1882 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1883"><span class="ln">1883 </span></a>tensor_split(input, indices_or_sections, dim=0) -&gt; List of Tensors 
<a name="l1884"><span class="ln">1884 </span></a> 
<a name="l1885"><span class="ln">1885 </span></a>Splits a tensor into multiple sub-tensors, all of which are views of :attr:`input`, 
<a name="l1886"><span class="ln">1886 </span></a>along dimension :attr:`dim` according to the indices or number of sections specified 
<a name="l1887"><span class="ln">1887 </span></a>by :attr:`indices_or_sections`. This function is based on NumPy's 
<a name="l1888"><span class="ln">1888 </span></a>:func:`numpy.array_split`. 
<a name="l1889"><span class="ln">1889 </span></a> 
<a name="l1890"><span class="ln">1890 </span></a>Args: 
<a name="l1891"><span class="ln">1891 </span></a>    input (Tensor): the tensor to split 
<a name="l1892"><span class="ln">1892 </span></a>    indices_or_sections (Tensor, int or list or tuple of ints): 
<a name="l1893"><span class="ln">1893 </span></a>        If :attr:`indices_or_sections` is an integer ``n`` or a zero dimensional long tensor 
<a name="l1894"><span class="ln">1894 </span></a>        with value ``n``, :attr:`input` is split into ``n`` sections along dimension :attr:`dim`. 
<a name="l1895"><span class="ln">1895 </span></a>        If :attr:`input` is divisible by ``n`` along dimension :attr:`dim`, each 
<a name="l1896"><span class="ln">1896 </span></a>        section will be of equal size, :code:`input.size(dim) / n`. If :attr:`input` 
<a name="l1897"><span class="ln">1897 </span></a>        is not divisible by ``n``, the sizes of the first :code:`int(input.size(dim) % n)` 
<a name="l1898"><span class="ln">1898 </span></a>        sections will have size :code:`int(input.size(dim) / n) + 1`, and the rest will 
<a name="l1899"><span class="ln">1899 </span></a>        have size :code:`int(input.size(dim) / n)`. 
<a name="l1900"><span class="ln">1900 </span></a> 
<a name="l1901"><span class="ln">1901 </span></a>        If :attr:`indices_or_sections` is a list or tuple of ints, or a one-dimensional long 
<a name="l1902"><span class="ln">1902 </span></a>        tensor, then :attr:`input` is split along dimension :attr:`dim` at each of the indices 
<a name="l1903"><span class="ln">1903 </span></a>        in the list, tuple or tensor. For instance, :code:`indices_or_sections=[2, 3]` and :code:`dim=0` 
<a name="l1904"><span class="ln">1904 </span></a>        would result in the tensors :code:`input[:2]`, :code:`input[2:3]`, and :code:`input[3:]`. 
<a name="l1905"><span class="ln">1905 </span></a> 
<a name="l1906"><span class="ln">1906 </span></a>        If :attr:`indices_or_sections` is a tensor, it must be a zero-dimensional or one-dimensional 
<a name="l1907"><span class="ln">1907 </span></a>        long tensor on the CPU. 
<a name="l1908"><span class="ln">1908 </span></a> 
<a name="l1909"><span class="ln">1909 </span></a>    dim (int, optional): dimension along which to split the tensor. Default: ``0`` 
<a name="l1910"><span class="ln">1910 </span></a> 
<a name="l1911"><span class="ln">1911 </span></a>Example:: 
<a name="l1912"><span class="ln">1912 </span></a> 
<a name="l1913"><span class="ln">1913 </span></a>    &gt;&gt;&gt; x = torch.arange(8) 
<a name="l1914"><span class="ln">1914 </span></a>    &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l1915"><span class="ln">1915 </span></a>    (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7])) 
<a name="l1916"><span class="ln">1916 </span></a> 
<a name="l1917"><span class="ln">1917 </span></a>    &gt;&gt;&gt; x = torch.arange(7) 
<a name="l1918"><span class="ln">1918 </span></a>    &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l1919"><span class="ln">1919 </span></a>    (tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6])) 
<a name="l1920"><span class="ln">1920 </span></a>    &gt;&gt;&gt; torch.tensor_split(x, (1, 6)) 
<a name="l1921"><span class="ln">1921 </span></a>    (tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6])) 
<a name="l1922"><span class="ln">1922 </span></a> 
<a name="l1923"><span class="ln">1923 </span></a>    &gt;&gt;&gt; x = torch.arange(14).reshape(2, 7) 
<a name="l1924"><span class="ln">1924 </span></a>    &gt;&gt;&gt; x 
<a name="l1925"><span class="ln">1925 </span></a>    tensor([[ 0,  1,  2,  3,  4,  5,  6], 
<a name="l1926"><span class="ln">1926 </span></a>            [ 7,  8,  9, 10, 11, 12, 13]]) 
<a name="l1927"><span class="ln">1927 </span></a>    &gt;&gt;&gt; torch.tensor_split(x, 3, dim=1) 
<a name="l1928"><span class="ln">1928 </span></a>    (tensor([[0, 1, 2], 
<a name="l1929"><span class="ln">1929 </span></a>            [7, 8, 9]]), 
<a name="l1930"><span class="ln">1930 </span></a>     tensor([[ 3,  4], 
<a name="l1931"><span class="ln">1931 </span></a>            [10, 11]]), 
<a name="l1932"><span class="ln">1932 </span></a>     tensor([[ 5,  6], 
<a name="l1933"><span class="ln">1933 </span></a>            [12, 13]])) 
<a name="l1934"><span class="ln">1934 </span></a>    &gt;&gt;&gt; torch.tensor_split(x, (1, 6), dim=1) 
<a name="l1935"><span class="ln">1935 </span></a>    (tensor([[0], 
<a name="l1936"><span class="ln">1936 </span></a>            [7]]), 
<a name="l1937"><span class="ln">1937 </span></a>     tensor([[ 1,  2,  3,  4,  5], 
<a name="l1938"><span class="ln">1938 </span></a>            [ 8,  9, 10, 11, 12]]), 
<a name="l1939"><span class="ln">1939 </span></a>     tensor([[ 6], 
<a name="l1940"><span class="ln">1940 </span></a>            [13]])) 
<a name="l1941"><span class="ln">1941 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1942"><span class="ln">1942 </span></a><span class="s3">)</span>
<a name="l1943"><span class="ln">1943 </span></a>
<a name="l1944"><span class="ln">1944 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1945"><span class="ln">1945 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">chunk</span><span class="s3">,</span>
<a name="l1946"><span class="ln">1946 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1947"><span class="ln">1947 </span></a>chunk(input: Tensor, chunks: int, dim: int = 0) -&gt; Tuple[Tensor, ...] 
<a name="l1948"><span class="ln">1948 </span></a> 
<a name="l1949"><span class="ln">1949 </span></a>Attempts to split a tensor into the specified number of chunks. Each chunk is a view of 
<a name="l1950"><span class="ln">1950 </span></a>the input tensor. 
<a name="l1951"><span class="ln">1951 </span></a> 
<a name="l1952"><span class="ln">1952 </span></a> 
<a name="l1953"><span class="ln">1953 </span></a>.. note:: 
<a name="l1954"><span class="ln">1954 </span></a> 
<a name="l1955"><span class="ln">1955 </span></a>    This function may return fewer than the specified number of chunks! 
<a name="l1956"><span class="ln">1956 </span></a> 
<a name="l1957"><span class="ln">1957 </span></a>.. seealso:: 
<a name="l1958"><span class="ln">1958 </span></a> 
<a name="l1959"><span class="ln">1959 </span></a>    :func:`torch.tensor_split` a function that always returns exactly the specified number of chunks 
<a name="l1960"><span class="ln">1960 </span></a> 
<a name="l1961"><span class="ln">1961 </span></a>If the tensor size along the given dimension :attr:`dim` is divisible by :attr:`chunks`, 
<a name="l1962"><span class="ln">1962 </span></a>all returned chunks will be the same size. 
<a name="l1963"><span class="ln">1963 </span></a>If the tensor size along the given dimension :attr:`dim` is not divisible by :attr:`chunks`, 
<a name="l1964"><span class="ln">1964 </span></a>all returned chunks will be the same size, except the last one. 
<a name="l1965"><span class="ln">1965 </span></a>If such division is not possible, this function may return fewer 
<a name="l1966"><span class="ln">1966 </span></a>than the specified number of chunks. 
<a name="l1967"><span class="ln">1967 </span></a> 
<a name="l1968"><span class="ln">1968 </span></a>Arguments: 
<a name="l1969"><span class="ln">1969 </span></a>    input (Tensor): the tensor to split 
<a name="l1970"><span class="ln">1970 </span></a>    chunks (int): number of chunks to return 
<a name="l1971"><span class="ln">1971 </span></a>    dim (int): dimension along which to split the tensor 
<a name="l1972"><span class="ln">1972 </span></a> 
<a name="l1973"><span class="ln">1973 </span></a>Example: 
<a name="l1974"><span class="ln">1974 </span></a>    &gt;&gt;&gt; torch.arange(11).chunk(6) 
<a name="l1975"><span class="ln">1975 </span></a>    (tensor([0, 1]), 
<a name="l1976"><span class="ln">1976 </span></a>     tensor([2, 3]), 
<a name="l1977"><span class="ln">1977 </span></a>     tensor([4, 5]), 
<a name="l1978"><span class="ln">1978 </span></a>     tensor([6, 7]), 
<a name="l1979"><span class="ln">1979 </span></a>     tensor([8, 9]), 
<a name="l1980"><span class="ln">1980 </span></a>     tensor([10])) 
<a name="l1981"><span class="ln">1981 </span></a>    &gt;&gt;&gt; torch.arange(12).chunk(6) 
<a name="l1982"><span class="ln">1982 </span></a>    (tensor([0, 1]), 
<a name="l1983"><span class="ln">1983 </span></a>     tensor([2, 3]), 
<a name="l1984"><span class="ln">1984 </span></a>     tensor([4, 5]), 
<a name="l1985"><span class="ln">1985 </span></a>     tensor([6, 7]), 
<a name="l1986"><span class="ln">1986 </span></a>     tensor([8, 9]), 
<a name="l1987"><span class="ln">1987 </span></a>     tensor([10, 11])) 
<a name="l1988"><span class="ln">1988 </span></a>    &gt;&gt;&gt; torch.arange(13).chunk(6) 
<a name="l1989"><span class="ln">1989 </span></a>    (tensor([0, 1, 2]), 
<a name="l1990"><span class="ln">1990 </span></a>     tensor([3, 4, 5]), 
<a name="l1991"><span class="ln">1991 </span></a>     tensor([6, 7, 8]), 
<a name="l1992"><span class="ln">1992 </span></a>     tensor([ 9, 10, 11]), 
<a name="l1993"><span class="ln">1993 </span></a>     tensor([12])) 
<a name="l1994"><span class="ln">1994 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1995"><span class="ln">1995 </span></a><span class="s3">)</span>
<a name="l1996"><span class="ln">1996 </span></a>
<a name="l1997"><span class="ln">1997 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l1998"><span class="ln">1998 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unsafe_chunk</span><span class="s3">,</span>
<a name="l1999"><span class="ln">1999 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2000"><span class="ln">2000 </span></a>unsafe_chunk(input, chunks, dim=0) -&gt; List of Tensors 
<a name="l2001"><span class="ln">2001 </span></a> 
<a name="l2002"><span class="ln">2002 </span></a>Works like :func:`torch.chunk` but without enforcing the autograd restrictions 
<a name="l2003"><span class="ln">2003 </span></a>on inplace modification of the outputs. 
<a name="l2004"><span class="ln">2004 </span></a> 
<a name="l2005"><span class="ln">2005 </span></a>.. warning:: 
<a name="l2006"><span class="ln">2006 </span></a>    This function is safe to use as long as only the input, or only the outputs 
<a name="l2007"><span class="ln">2007 </span></a>    are modified inplace after calling this function. It is user's 
<a name="l2008"><span class="ln">2008 </span></a>    responsibility to ensure that is the case. If both the input and one or more 
<a name="l2009"><span class="ln">2009 </span></a>    of the outputs are modified inplace, gradients computed by autograd will be 
<a name="l2010"><span class="ln">2010 </span></a>    silently incorrect. 
<a name="l2011"><span class="ln">2011 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2012"><span class="ln">2012 </span></a><span class="s3">)</span>
<a name="l2013"><span class="ln">2013 </span></a>
<a name="l2014"><span class="ln">2014 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2015"><span class="ln">2015 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unsafe_split</span><span class="s3">,</span>
<a name="l2016"><span class="ln">2016 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2017"><span class="ln">2017 </span></a>unsafe_split(tensor, split_size_or_sections, dim=0) -&gt; List of Tensors 
<a name="l2018"><span class="ln">2018 </span></a> 
<a name="l2019"><span class="ln">2019 </span></a>Works like :func:`torch.split` but without enforcing the autograd restrictions 
<a name="l2020"><span class="ln">2020 </span></a>on inplace modification of the outputs. 
<a name="l2021"><span class="ln">2021 </span></a> 
<a name="l2022"><span class="ln">2022 </span></a>.. warning:: 
<a name="l2023"><span class="ln">2023 </span></a>    This function is safe to use as long as only the input, or only the outputs 
<a name="l2024"><span class="ln">2024 </span></a>    are modified inplace after calling this function. It is user's 
<a name="l2025"><span class="ln">2025 </span></a>    responsibility to ensure that is the case. If both the input and one or more 
<a name="l2026"><span class="ln">2026 </span></a>    of the outputs are modified inplace, gradients computed by autograd will be 
<a name="l2027"><span class="ln">2027 </span></a>    silently incorrect. 
<a name="l2028"><span class="ln">2028 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2029"><span class="ln">2029 </span></a><span class="s3">)</span>
<a name="l2030"><span class="ln">2030 </span></a>
<a name="l2031"><span class="ln">2031 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2032"><span class="ln">2032 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">hsplit</span><span class="s3">,</span>
<a name="l2033"><span class="ln">2033 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2034"><span class="ln">2034 </span></a>hsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l2035"><span class="ln">2035 </span></a> 
<a name="l2036"><span class="ln">2036 </span></a>Splits :attr:`input`, a tensor with one or more dimensions, into multiple tensors 
<a name="l2037"><span class="ln">2037 </span></a>horizontally according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l2038"><span class="ln">2038 </span></a>:attr:`input`. 
<a name="l2039"><span class="ln">2039 </span></a> 
<a name="l2040"><span class="ln">2040 </span></a>If :attr:`input` is one dimensional this is equivalent to calling 
<a name="l2041"><span class="ln">2041 </span></a>torch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is 
<a name="l2042"><span class="ln">2042 </span></a>zero), and if :attr:`input` has two or more dimensions it's equivalent to calling 
<a name="l2043"><span class="ln">2043 </span></a>torch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1), 
<a name="l2044"><span class="ln">2044 </span></a>except that if :attr:`indices_or_sections` is an integer it must evenly divide 
<a name="l2045"><span class="ln">2045 </span></a>the split dimension or a runtime error will be thrown. 
<a name="l2046"><span class="ln">2046 </span></a> 
<a name="l2047"><span class="ln">2047 </span></a>This function is based on NumPy's :func:`numpy.hsplit`. 
<a name="l2048"><span class="ln">2048 </span></a> 
<a name="l2049"><span class="ln">2049 </span></a>Args: 
<a name="l2050"><span class="ln">2050 </span></a>    input (Tensor): tensor to split. 
<a name="l2051"><span class="ln">2051 </span></a>    indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l2052"><span class="ln">2052 </span></a> 
<a name="l2053"><span class="ln">2053 </span></a>Example:: 
<a name="l2054"><span class="ln">2054 </span></a> 
<a name="l2055"><span class="ln">2055 </span></a>    &gt;&gt;&gt; t = torch.arange(16.0).reshape(4,4) 
<a name="l2056"><span class="ln">2056 </span></a>    &gt;&gt;&gt; t 
<a name="l2057"><span class="ln">2057 </span></a>    tensor([[ 0.,  1.,  2.,  3.], 
<a name="l2058"><span class="ln">2058 </span></a>            [ 4.,  5.,  6.,  7.], 
<a name="l2059"><span class="ln">2059 </span></a>            [ 8.,  9., 10., 11.], 
<a name="l2060"><span class="ln">2060 </span></a>            [12., 13., 14., 15.]]) 
<a name="l2061"><span class="ln">2061 </span></a>    &gt;&gt;&gt; torch.hsplit(t, 2) 
<a name="l2062"><span class="ln">2062 </span></a>    (tensor([[ 0.,  1.], 
<a name="l2063"><span class="ln">2063 </span></a>             [ 4.,  5.], 
<a name="l2064"><span class="ln">2064 </span></a>             [ 8.,  9.], 
<a name="l2065"><span class="ln">2065 </span></a>             [12., 13.]]), 
<a name="l2066"><span class="ln">2066 </span></a>     tensor([[ 2.,  3.], 
<a name="l2067"><span class="ln">2067 </span></a>             [ 6.,  7.], 
<a name="l2068"><span class="ln">2068 </span></a>             [10., 11.], 
<a name="l2069"><span class="ln">2069 </span></a>             [14., 15.]])) 
<a name="l2070"><span class="ln">2070 </span></a>    &gt;&gt;&gt; torch.hsplit(t, [3, 6]) 
<a name="l2071"><span class="ln">2071 </span></a>    (tensor([[ 0.,  1.,  2.], 
<a name="l2072"><span class="ln">2072 </span></a>             [ 4.,  5.,  6.], 
<a name="l2073"><span class="ln">2073 </span></a>             [ 8.,  9., 10.], 
<a name="l2074"><span class="ln">2074 </span></a>             [12., 13., 14.]]), 
<a name="l2075"><span class="ln">2075 </span></a>     tensor([[ 3.], 
<a name="l2076"><span class="ln">2076 </span></a>             [ 7.], 
<a name="l2077"><span class="ln">2077 </span></a>             [11.], 
<a name="l2078"><span class="ln">2078 </span></a>             [15.]]), 
<a name="l2079"><span class="ln">2079 </span></a>     tensor([], size=(4, 0))) 
<a name="l2080"><span class="ln">2080 </span></a> 
<a name="l2081"><span class="ln">2081 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2082"><span class="ln">2082 </span></a><span class="s3">)</span>
<a name="l2083"><span class="ln">2083 </span></a>
<a name="l2084"><span class="ln">2084 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2085"><span class="ln">2085 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">vsplit</span><span class="s3">,</span>
<a name="l2086"><span class="ln">2086 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2087"><span class="ln">2087 </span></a>vsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l2088"><span class="ln">2088 </span></a> 
<a name="l2089"><span class="ln">2089 </span></a>Splits :attr:`input`, a tensor with two or more dimensions, into multiple tensors 
<a name="l2090"><span class="ln">2090 </span></a>vertically according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l2091"><span class="ln">2091 </span></a>:attr:`input`. 
<a name="l2092"><span class="ln">2092 </span></a> 
<a name="l2093"><span class="ln">2093 </span></a>This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0) 
<a name="l2094"><span class="ln">2094 </span></a>(the split dimension is 0), except that if :attr:`indices_or_sections` is an integer 
<a name="l2095"><span class="ln">2095 </span></a>it must evenly divide the split dimension or a runtime error will be thrown. 
<a name="l2096"><span class="ln">2096 </span></a> 
<a name="l2097"><span class="ln">2097 </span></a>This function is based on NumPy's :func:`numpy.vsplit`. 
<a name="l2098"><span class="ln">2098 </span></a> 
<a name="l2099"><span class="ln">2099 </span></a>Args: 
<a name="l2100"><span class="ln">2100 </span></a>    input (Tensor): tensor to split. 
<a name="l2101"><span class="ln">2101 </span></a>    indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l2102"><span class="ln">2102 </span></a> 
<a name="l2103"><span class="ln">2103 </span></a>Example:: 
<a name="l2104"><span class="ln">2104 </span></a> 
<a name="l2105"><span class="ln">2105 </span></a>    &gt;&gt;&gt; t = torch.arange(16.0).reshape(4,4) 
<a name="l2106"><span class="ln">2106 </span></a>    &gt;&gt;&gt; t 
<a name="l2107"><span class="ln">2107 </span></a>    tensor([[ 0.,  1.,  2.,  3.], 
<a name="l2108"><span class="ln">2108 </span></a>            [ 4.,  5.,  6.,  7.], 
<a name="l2109"><span class="ln">2109 </span></a>            [ 8.,  9., 10., 11.], 
<a name="l2110"><span class="ln">2110 </span></a>            [12., 13., 14., 15.]]) 
<a name="l2111"><span class="ln">2111 </span></a>    &gt;&gt;&gt; torch.vsplit(t, 2) 
<a name="l2112"><span class="ln">2112 </span></a>    (tensor([[0., 1., 2., 3.], 
<a name="l2113"><span class="ln">2113 </span></a>             [4., 5., 6., 7.]]), 
<a name="l2114"><span class="ln">2114 </span></a>     tensor([[ 8.,  9., 10., 11.], 
<a name="l2115"><span class="ln">2115 </span></a>             [12., 13., 14., 15.]])) 
<a name="l2116"><span class="ln">2116 </span></a>    &gt;&gt;&gt; torch.vsplit(t, [3, 6]) 
<a name="l2117"><span class="ln">2117 </span></a>    (tensor([[ 0.,  1.,  2.,  3.], 
<a name="l2118"><span class="ln">2118 </span></a>             [ 4.,  5.,  6.,  7.], 
<a name="l2119"><span class="ln">2119 </span></a>             [ 8.,  9., 10., 11.]]), 
<a name="l2120"><span class="ln">2120 </span></a>     tensor([[12., 13., 14., 15.]]), 
<a name="l2121"><span class="ln">2121 </span></a>     tensor([], size=(0, 4))) 
<a name="l2122"><span class="ln">2122 </span></a> 
<a name="l2123"><span class="ln">2123 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2124"><span class="ln">2124 </span></a><span class="s3">)</span>
<a name="l2125"><span class="ln">2125 </span></a>
<a name="l2126"><span class="ln">2126 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2127"><span class="ln">2127 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">dsplit</span><span class="s3">,</span>
<a name="l2128"><span class="ln">2128 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2129"><span class="ln">2129 </span></a>dsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l2130"><span class="ln">2130 </span></a> 
<a name="l2131"><span class="ln">2131 </span></a>Splits :attr:`input`, a tensor with three or more dimensions, into multiple tensors 
<a name="l2132"><span class="ln">2132 </span></a>depthwise according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l2133"><span class="ln">2133 </span></a>:attr:`input`. 
<a name="l2134"><span class="ln">2134 </span></a> 
<a name="l2135"><span class="ln">2135 </span></a>This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=2) 
<a name="l2136"><span class="ln">2136 </span></a>(the split dimension is 2), except that if :attr:`indices_or_sections` is an integer 
<a name="l2137"><span class="ln">2137 </span></a>it must evenly divide the split dimension or a runtime error will be thrown. 
<a name="l2138"><span class="ln">2138 </span></a> 
<a name="l2139"><span class="ln">2139 </span></a>This function is based on NumPy's :func:`numpy.dsplit`. 
<a name="l2140"><span class="ln">2140 </span></a> 
<a name="l2141"><span class="ln">2141 </span></a>Args: 
<a name="l2142"><span class="ln">2142 </span></a>    input (Tensor): tensor to split. 
<a name="l2143"><span class="ln">2143 </span></a>    indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l2144"><span class="ln">2144 </span></a> 
<a name="l2145"><span class="ln">2145 </span></a>Example:: 
<a name="l2146"><span class="ln">2146 </span></a> 
<a name="l2147"><span class="ln">2147 </span></a>    &gt;&gt;&gt; t = torch.arange(16.0).reshape(2, 2, 4) 
<a name="l2148"><span class="ln">2148 </span></a>    &gt;&gt;&gt; t 
<a name="l2149"><span class="ln">2149 </span></a>    tensor([[[ 0.,  1.,  2.,  3.], 
<a name="l2150"><span class="ln">2150 </span></a>             [ 4.,  5.,  6.,  7.]], 
<a name="l2151"><span class="ln">2151 </span></a>            [[ 8.,  9., 10., 11.], 
<a name="l2152"><span class="ln">2152 </span></a>             [12., 13., 14., 15.]]]) 
<a name="l2153"><span class="ln">2153 </span></a>    &gt;&gt;&gt; torch.dsplit(t, 2) 
<a name="l2154"><span class="ln">2154 </span></a>    (tensor([[[ 0.,  1.], 
<a name="l2155"><span class="ln">2155 </span></a>            [ 4.,  5.]], 
<a name="l2156"><span class="ln">2156 </span></a>           [[ 8.,  9.], 
<a name="l2157"><span class="ln">2157 </span></a>            [12., 13.]]]), 
<a name="l2158"><span class="ln">2158 </span></a>     tensor([[[ 2.,  3.], 
<a name="l2159"><span class="ln">2159 </span></a>              [ 6.,  7.]], 
<a name="l2160"><span class="ln">2160 </span></a>             [[10., 11.], 
<a name="l2161"><span class="ln">2161 </span></a>              [14., 15.]]])) 
<a name="l2162"><span class="ln">2162 </span></a> 
<a name="l2163"><span class="ln">2163 </span></a>    &gt;&gt;&gt; torch.dsplit(t, [3, 6]) 
<a name="l2164"><span class="ln">2164 </span></a>    (tensor([[[ 0.,  1.,  2.], 
<a name="l2165"><span class="ln">2165 </span></a>              [ 4.,  5.,  6.]], 
<a name="l2166"><span class="ln">2166 </span></a>             [[ 8.,  9., 10.], 
<a name="l2167"><span class="ln">2167 </span></a>              [12., 13., 14.]]]), 
<a name="l2168"><span class="ln">2168 </span></a>     tensor([[[ 3.], 
<a name="l2169"><span class="ln">2169 </span></a>              [ 7.]], 
<a name="l2170"><span class="ln">2170 </span></a>             [[11.], 
<a name="l2171"><span class="ln">2171 </span></a>              [15.]]]), 
<a name="l2172"><span class="ln">2172 </span></a>     tensor([], size=(2, 2, 0))) 
<a name="l2173"><span class="ln">2173 </span></a> 
<a name="l2174"><span class="ln">2174 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2175"><span class="ln">2175 </span></a><span class="s3">)</span>
<a name="l2176"><span class="ln">2176 </span></a>
<a name="l2177"><span class="ln">2177 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2178"><span class="ln">2178 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">can_cast</span><span class="s3">,</span>
<a name="l2179"><span class="ln">2179 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2180"><span class="ln">2180 </span></a>can_cast(from_, to) -&gt; bool 
<a name="l2181"><span class="ln">2181 </span></a> 
<a name="l2182"><span class="ln">2182 </span></a>Determines if a type conversion is allowed under PyTorch casting rules 
<a name="l2183"><span class="ln">2183 </span></a>described in the type promotion :ref:`documentation &lt;type-promotion-doc&gt;`. 
<a name="l2184"><span class="ln">2184 </span></a> 
<a name="l2185"><span class="ln">2185 </span></a>Args: 
<a name="l2186"><span class="ln">2186 </span></a>    from\_ (dtype): The original :class:`torch.dtype`. 
<a name="l2187"><span class="ln">2187 </span></a>    to (dtype): The target :class:`torch.dtype`. 
<a name="l2188"><span class="ln">2188 </span></a> 
<a name="l2189"><span class="ln">2189 </span></a>Example:: 
<a name="l2190"><span class="ln">2190 </span></a> 
<a name="l2191"><span class="ln">2191 </span></a>    &gt;&gt;&gt; torch.can_cast(torch.double, torch.float) 
<a name="l2192"><span class="ln">2192 </span></a>    True 
<a name="l2193"><span class="ln">2193 </span></a>    &gt;&gt;&gt; torch.can_cast(torch.float, torch.int) 
<a name="l2194"><span class="ln">2194 </span></a>    False 
<a name="l2195"><span class="ln">2195 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2196"><span class="ln">2196 </span></a><span class="s3">)</span>
<a name="l2197"><span class="ln">2197 </span></a>
<a name="l2198"><span class="ln">2198 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2199"><span class="ln">2199 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">corrcoef</span><span class="s3">,</span>
<a name="l2200"><span class="ln">2200 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2201"><span class="ln">2201 </span></a>corrcoef(input) -&gt; Tensor 
<a name="l2202"><span class="ln">2202 </span></a> 
<a name="l2203"><span class="ln">2203 </span></a>Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the :attr:`input` matrix, 
<a name="l2204"><span class="ln">2204 </span></a>where rows are the variables and columns are the observations. 
<a name="l2205"><span class="ln">2205 </span></a> 
<a name="l2206"><span class="ln">2206 </span></a>.. note:: 
<a name="l2207"><span class="ln">2207 </span></a> 
<a name="l2208"><span class="ln">2208 </span></a>    The correlation coefficient matrix R is computed using the covariance matrix C as given by 
<a name="l2209"><span class="ln">2209 </span></a>    :math:`R_{ij} = \frac{ C_{ij} } { \sqrt{ C_{ii} * C_{jj} } }` 
<a name="l2210"><span class="ln">2210 </span></a> 
<a name="l2211"><span class="ln">2211 </span></a>.. note:: 
<a name="l2212"><span class="ln">2212 </span></a> 
<a name="l2213"><span class="ln">2213 </span></a>    Due to floating point rounding, the resulting array may not be Hermitian and its diagonal elements may not be 1. 
<a name="l2214"><span class="ln">2214 </span></a>    The real and imaginary values are clipped to the interval [-1, 1] in an attempt to improve this situation. 
<a name="l2215"><span class="ln">2215 </span></a> 
<a name="l2216"><span class="ln">2216 </span></a>Args: 
<a name="l2217"><span class="ln">2217 </span></a>    input (Tensor): A 2D matrix containing multiple variables and observations, or a 
<a name="l2218"><span class="ln">2218 </span></a>        Scalar or 1D vector representing a single variable. 
<a name="l2219"><span class="ln">2219 </span></a> 
<a name="l2220"><span class="ln">2220 </span></a>Returns: 
<a name="l2221"><span class="ln">2221 </span></a>    (Tensor) The correlation coefficient matrix of the variables. 
<a name="l2222"><span class="ln">2222 </span></a> 
<a name="l2223"><span class="ln">2223 </span></a>.. seealso:: 
<a name="l2224"><span class="ln">2224 </span></a> 
<a name="l2225"><span class="ln">2225 </span></a>        :func:`torch.cov` covariance matrix. 
<a name="l2226"><span class="ln">2226 </span></a> 
<a name="l2227"><span class="ln">2227 </span></a>Example:: 
<a name="l2228"><span class="ln">2228 </span></a> 
<a name="l2229"><span class="ln">2229 </span></a>    &gt;&gt;&gt; x = torch.tensor([[0, 1, 2], [2, 1, 0]]) 
<a name="l2230"><span class="ln">2230 </span></a>    &gt;&gt;&gt; torch.corrcoef(x) 
<a name="l2231"><span class="ln">2231 </span></a>    tensor([[ 1., -1.], 
<a name="l2232"><span class="ln">2232 </span></a>            [-1.,  1.]]) 
<a name="l2233"><span class="ln">2233 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 4) 
<a name="l2234"><span class="ln">2234 </span></a>    &gt;&gt;&gt; x 
<a name="l2235"><span class="ln">2235 </span></a>    tensor([[-0.2678, -0.0908, -0.3766,  0.2780], 
<a name="l2236"><span class="ln">2236 </span></a>            [-0.5812,  0.1535,  0.2387,  0.2350]]) 
<a name="l2237"><span class="ln">2237 </span></a>    &gt;&gt;&gt; torch.corrcoef(x) 
<a name="l2238"><span class="ln">2238 </span></a>    tensor([[1.0000, 0.3582], 
<a name="l2239"><span class="ln">2239 </span></a>            [0.3582, 1.0000]]) 
<a name="l2240"><span class="ln">2240 </span></a>    &gt;&gt;&gt; torch.corrcoef(x[0]) 
<a name="l2241"><span class="ln">2241 </span></a>    tensor(1.) 
<a name="l2242"><span class="ln">2242 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2243"><span class="ln">2243 </span></a><span class="s3">)</span>
<a name="l2244"><span class="ln">2244 </span></a>
<a name="l2245"><span class="ln">2245 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2246"><span class="ln">2246 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cov</span><span class="s3">,</span>
<a name="l2247"><span class="ln">2247 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2248"><span class="ln">2248 </span></a>cov(input, *, correction=1, fweights=None, aweights=None) -&gt; Tensor 
<a name="l2249"><span class="ln">2249 </span></a> 
<a name="l2250"><span class="ln">2250 </span></a>Estimates the covariance matrix of the variables given by the :attr:`input` matrix, where rows are 
<a name="l2251"><span class="ln">2251 </span></a>the variables and columns are the observations. 
<a name="l2252"><span class="ln">2252 </span></a> 
<a name="l2253"><span class="ln">2253 </span></a>A covariance matrix is a square matrix giving the covariance of each pair of variables. The diagonal contains 
<a name="l2254"><span class="ln">2254 </span></a>the variance of each variable (covariance of a variable with itself). By definition, if :attr:`input` represents 
<a name="l2255"><span class="ln">2255 </span></a>a single variable (Scalar or 1D) then its variance is returned. 
<a name="l2256"><span class="ln">2256 </span></a> 
<a name="l2257"><span class="ln">2257 </span></a>The sample covariance of the variables :math:`x` and :math:`y` is given by: 
<a name="l2258"><span class="ln">2258 </span></a> 
<a name="l2259"><span class="ln">2259 </span></a>.. math:: 
<a name="l2260"><span class="ln">2260 </span></a>    \text{cov}(x,y) = \frac{\sum^{N}_{i = 1}(x_{i} - \bar{x})(y_{i} - \bar{y})}{\max(0,~N~-~\delta N)} 
<a name="l2261"><span class="ln">2261 </span></a> 
<a name="l2262"><span class="ln">2262 </span></a>where :math:`\bar{x}` and :math:`\bar{y}` are the simple means of the :math:`x` and :math:`y` respectively, and 
<a name="l2263"><span class="ln">2263 </span></a>:math:`\delta N` is the :attr:`correction`. 
<a name="l2264"><span class="ln">2264 </span></a> 
<a name="l2265"><span class="ln">2265 </span></a>If :attr:`fweights` and/or :attr:`aweights` are provided, the weighted covariance 
<a name="l2266"><span class="ln">2266 </span></a>is calculated, which is given by: 
<a name="l2267"><span class="ln">2267 </span></a> 
<a name="l2268"><span class="ln">2268 </span></a>.. math:: 
<a name="l2269"><span class="ln">2269 </span></a>    \text{cov}_w(x,y) = \frac{\sum^{N}_{i = 1}w_i(x_{i} - \mu_x^*)(y_{i} - \mu_y^*)} 
<a name="l2270"><span class="ln">2270 </span></a>    {\max(0,~\sum^{N}_{i = 1}w_i~-~\frac{\sum^{N}_{i = 1}w_ia_i}{\sum^{N}_{i = 1}w_i}~\delta N)} 
<a name="l2271"><span class="ln">2271 </span></a> 
<a name="l2272"><span class="ln">2272 </span></a>where :math:`w` denotes :attr:`fweights` or :attr:`aweights` (``f`` and ``a`` for brevity) based on whichever is 
<a name="l2273"><span class="ln">2273 </span></a>provided, or :math:`w = f \times a` if both are provided, and 
<a name="l2274"><span class="ln">2274 </span></a>:math:`\mu_x^* = \frac{\sum^{N}_{i = 1}w_ix_{i} }{\sum^{N}_{i = 1}w_i}` is the weighted mean of the variable. If not 
<a name="l2275"><span class="ln">2275 </span></a>provided, ``f`` and/or ``a`` can be seen as a :math:`\mathbb{1}` vector of appropriate size. 
<a name="l2276"><span class="ln">2276 </span></a> 
<a name="l2277"><span class="ln">2277 </span></a>Args: 
<a name="l2278"><span class="ln">2278 </span></a>    input (Tensor): A 2D matrix containing multiple variables and observations, or a 
<a name="l2279"><span class="ln">2279 </span></a>        Scalar or 1D vector representing a single variable. 
<a name="l2280"><span class="ln">2280 </span></a> 
<a name="l2281"><span class="ln">2281 </span></a>Keyword Args: 
<a name="l2282"><span class="ln">2282 </span></a>    correction (int, optional): difference between the sample size and sample degrees of freedom. 
<a name="l2283"><span class="ln">2283 </span></a>        Defaults to Bessel's correction, ``correction = 1`` which returns the unbiased estimate, 
<a name="l2284"><span class="ln">2284 </span></a>        even if both :attr:`fweights` and :attr:`aweights` are specified. ``correction = 0`` 
<a name="l2285"><span class="ln">2285 </span></a>        will return the simple average. Defaults to ``1``. 
<a name="l2286"><span class="ln">2286 </span></a>    fweights (tensor, optional): A Scalar or 1D tensor of observation vector frequencies representing the number of 
<a name="l2287"><span class="ln">2287 </span></a>        times each observation should be repeated. Its numel must equal the number of columns of :attr:`input`. 
<a name="l2288"><span class="ln">2288 </span></a>        Must have integral dtype. Ignored if ``None``. Defaults to ``None``. 
<a name="l2289"><span class="ln">2289 </span></a>    aweights (tensor, optional): A Scalar or 1D array of observation vector weights. 
<a name="l2290"><span class="ln">2290 </span></a>        These relative weights are typically large for observations considered &quot;important&quot; and smaller for 
<a name="l2291"><span class="ln">2291 </span></a>        observations considered less &quot;important&quot;. Its numel must equal the number of columns of :attr:`input`. 
<a name="l2292"><span class="ln">2292 </span></a>        Must have floating point dtype. Ignored if ``None``. Defaults to ``None``. 
<a name="l2293"><span class="ln">2293 </span></a> 
<a name="l2294"><span class="ln">2294 </span></a>Returns: 
<a name="l2295"><span class="ln">2295 </span></a>    (Tensor) The covariance matrix of the variables. 
<a name="l2296"><span class="ln">2296 </span></a> 
<a name="l2297"><span class="ln">2297 </span></a>.. seealso:: 
<a name="l2298"><span class="ln">2298 </span></a> 
<a name="l2299"><span class="ln">2299 </span></a>        :func:`torch.corrcoef` normalized covariance matrix. 
<a name="l2300"><span class="ln">2300 </span></a> 
<a name="l2301"><span class="ln">2301 </span></a>Example:: 
<a name="l2302"><span class="ln">2302 </span></a> 
<a name="l2303"><span class="ln">2303 </span></a>    &gt;&gt;&gt; x = torch.tensor([[0, 2], [1, 1], [2, 0]]).T 
<a name="l2304"><span class="ln">2304 </span></a>    &gt;&gt;&gt; x 
<a name="l2305"><span class="ln">2305 </span></a>    tensor([[0, 1, 2], 
<a name="l2306"><span class="ln">2306 </span></a>            [2, 1, 0]]) 
<a name="l2307"><span class="ln">2307 </span></a>    &gt;&gt;&gt; torch.cov(x) 
<a name="l2308"><span class="ln">2308 </span></a>    tensor([[ 1., -1.], 
<a name="l2309"><span class="ln">2309 </span></a>            [-1.,  1.]]) 
<a name="l2310"><span class="ln">2310 </span></a>    &gt;&gt;&gt; torch.cov(x, correction=0) 
<a name="l2311"><span class="ln">2311 </span></a>    tensor([[ 0.6667, -0.6667], 
<a name="l2312"><span class="ln">2312 </span></a>            [-0.6667,  0.6667]]) 
<a name="l2313"><span class="ln">2313 </span></a>    &gt;&gt;&gt; fw = torch.randint(1, 10, (3,)) 
<a name="l2314"><span class="ln">2314 </span></a>    &gt;&gt;&gt; fw 
<a name="l2315"><span class="ln">2315 </span></a>    tensor([1, 6, 9]) 
<a name="l2316"><span class="ln">2316 </span></a>    &gt;&gt;&gt; aw = torch.rand(3) 
<a name="l2317"><span class="ln">2317 </span></a>    &gt;&gt;&gt; aw 
<a name="l2318"><span class="ln">2318 </span></a>    tensor([0.4282, 0.0255, 0.4144]) 
<a name="l2319"><span class="ln">2319 </span></a>    &gt;&gt;&gt; torch.cov(x, fweights=fw, aweights=aw) 
<a name="l2320"><span class="ln">2320 </span></a>    tensor([[ 0.4169, -0.4169], 
<a name="l2321"><span class="ln">2321 </span></a>            [-0.4169,  0.4169]]) 
<a name="l2322"><span class="ln">2322 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2323"><span class="ln">2323 </span></a><span class="s3">)</span>
<a name="l2324"><span class="ln">2324 </span></a>
<a name="l2325"><span class="ln">2325 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2326"><span class="ln">2326 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cat</span><span class="s3">,</span>
<a name="l2327"><span class="ln">2327 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2328"><span class="ln">2328 </span></a>cat(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l2329"><span class="ln">2329 </span></a> 
<a name="l2330"><span class="ln">2330 </span></a>Concatenates the given sequence of tensors in :attr:`tensors` in the given dimension. 
<a name="l2331"><span class="ln">2331 </span></a>All tensors must either have the same shape (except in the concatenating 
<a name="l2332"><span class="ln">2332 </span></a>dimension) or be a 1-D empty tensor with size ``(0,)``. 
<a name="l2333"><span class="ln">2333 </span></a> 
<a name="l2334"><span class="ln">2334 </span></a>:func:`torch.cat` can be seen as an inverse operation for :func:`torch.split` 
<a name="l2335"><span class="ln">2335 </span></a>and :func:`torch.chunk`. 
<a name="l2336"><span class="ln">2336 </span></a> 
<a name="l2337"><span class="ln">2337 </span></a>:func:`torch.cat` can be best understood via examples. 
<a name="l2338"><span class="ln">2338 </span></a> 
<a name="l2339"><span class="ln">2339 </span></a>.. seealso:: 
<a name="l2340"><span class="ln">2340 </span></a> 
<a name="l2341"><span class="ln">2341 </span></a>    :func:`torch.stack` concatenates the given sequence along a new dimension. 
<a name="l2342"><span class="ln">2342 </span></a> 
<a name="l2343"><span class="ln">2343 </span></a>Args: 
<a name="l2344"><span class="ln">2344 </span></a>    tensors (sequence of Tensors): Non-empty tensors provided must have the same shape, 
<a name="l2345"><span class="ln">2345 </span></a>        except in the cat dimension. 
<a name="l2346"><span class="ln">2346 </span></a> 
<a name="l2347"><span class="ln">2347 </span></a>    dim (int, optional): the dimension over which the tensors are concatenated 
<a name="l2348"><span class="ln">2348 </span></a> 
<a name="l2349"><span class="ln">2349 </span></a>Keyword args: 
<a name="l2350"><span class="ln">2350 </span></a>    {out} 
<a name="l2351"><span class="ln">2351 </span></a> 
<a name="l2352"><span class="ln">2352 </span></a>Example:: 
<a name="l2353"><span class="ln">2353 </span></a> 
<a name="l2354"><span class="ln">2354 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l2355"><span class="ln">2355 </span></a>    &gt;&gt;&gt; x 
<a name="l2356"><span class="ln">2356 </span></a>    tensor([[ 0.6580, -1.0969, -0.4614], 
<a name="l2357"><span class="ln">2357 </span></a>            [-0.1034, -0.5790,  0.1497]]) 
<a name="l2358"><span class="ln">2358 </span></a>    &gt;&gt;&gt; torch.cat((x, x, x), 0) 
<a name="l2359"><span class="ln">2359 </span></a>    tensor([[ 0.6580, -1.0969, -0.4614], 
<a name="l2360"><span class="ln">2360 </span></a>            [-0.1034, -0.5790,  0.1497], 
<a name="l2361"><span class="ln">2361 </span></a>            [ 0.6580, -1.0969, -0.4614], 
<a name="l2362"><span class="ln">2362 </span></a>            [-0.1034, -0.5790,  0.1497], 
<a name="l2363"><span class="ln">2363 </span></a>            [ 0.6580, -1.0969, -0.4614], 
<a name="l2364"><span class="ln">2364 </span></a>            [-0.1034, -0.5790,  0.1497]]) 
<a name="l2365"><span class="ln">2365 </span></a>    &gt;&gt;&gt; torch.cat((x, x, x), 1) 
<a name="l2366"><span class="ln">2366 </span></a>    tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580, 
<a name="l2367"><span class="ln">2367 </span></a>             -1.0969, -0.4614], 
<a name="l2368"><span class="ln">2368 </span></a>            [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034, 
<a name="l2369"><span class="ln">2369 </span></a>             -0.5790,  0.1497]]) 
<a name="l2370"><span class="ln">2370 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2371"><span class="ln">2371 </span></a><span class="s3">)</span>
<a name="l2372"><span class="ln">2372 </span></a>
<a name="l2373"><span class="ln">2373 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2374"><span class="ln">2374 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">concat</span><span class="s3">,</span>
<a name="l2375"><span class="ln">2375 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2376"><span class="ln">2376 </span></a>concat(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l2377"><span class="ln">2377 </span></a> 
<a name="l2378"><span class="ln">2378 </span></a>Alias of :func:`torch.cat`. 
<a name="l2379"><span class="ln">2379 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2380"><span class="ln">2380 </span></a><span class="s3">)</span>
<a name="l2381"><span class="ln">2381 </span></a>
<a name="l2382"><span class="ln">2382 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2383"><span class="ln">2383 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">,</span>
<a name="l2384"><span class="ln">2384 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2385"><span class="ln">2385 </span></a>concatenate(tensors, axis=0, out=None) -&gt; Tensor 
<a name="l2386"><span class="ln">2386 </span></a> 
<a name="l2387"><span class="ln">2387 </span></a>Alias of :func:`torch.cat`. 
<a name="l2388"><span class="ln">2388 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2389"><span class="ln">2389 </span></a><span class="s3">)</span>
<a name="l2390"><span class="ln">2390 </span></a>
<a name="l2391"><span class="ln">2391 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2392"><span class="ln">2392 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ceil</span><span class="s3">,</span>
<a name="l2393"><span class="ln">2393 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2394"><span class="ln">2394 </span></a>ceil(input, *, out=None) -&gt; Tensor 
<a name="l2395"><span class="ln">2395 </span></a> 
<a name="l2396"><span class="ln">2396 </span></a>Returns a new tensor with the ceil of the elements of :attr:`input`, 
<a name="l2397"><span class="ln">2397 </span></a>the smallest integer greater than or equal to each element. 
<a name="l2398"><span class="ln">2398 </span></a> 
<a name="l2399"><span class="ln">2399 </span></a>For integer inputs, follows the array-api convention of returning a 
<a name="l2400"><span class="ln">2400 </span></a>copy of the input tensor. 
<a name="l2401"><span class="ln">2401 </span></a> 
<a name="l2402"><span class="ln">2402 </span></a>.. math:: 
<a name="l2403"><span class="ln">2403 </span></a>    \text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil 
<a name="l2404"><span class="ln">2404 </span></a>&quot;&quot;&quot;</span>
<a name="l2405"><span class="ln">2405 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l2406"><span class="ln">2406 </span></a>Args: 
<a name="l2407"><span class="ln">2407 </span></a>    {input} 
<a name="l2408"><span class="ln">2408 </span></a> 
<a name="l2409"><span class="ln">2409 </span></a>Keyword args: 
<a name="l2410"><span class="ln">2410 </span></a>    {out} 
<a name="l2411"><span class="ln">2411 </span></a> 
<a name="l2412"><span class="ln">2412 </span></a>Example:: 
<a name="l2413"><span class="ln">2413 </span></a> 
<a name="l2414"><span class="ln">2414 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l2415"><span class="ln">2415 </span></a>    &gt;&gt;&gt; a 
<a name="l2416"><span class="ln">2416 </span></a>    tensor([-0.6341, -1.4208, -1.0900,  0.5826]) 
<a name="l2417"><span class="ln">2417 </span></a>    &gt;&gt;&gt; torch.ceil(a) 
<a name="l2418"><span class="ln">2418 </span></a>    tensor([-0., -1., -1.,  1.]) 
<a name="l2419"><span class="ln">2419 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2420"><span class="ln">2420 </span></a><span class="s3">)</span>
<a name="l2421"><span class="ln">2421 </span></a>
<a name="l2422"><span class="ln">2422 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2423"><span class="ln">2423 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">real</span><span class="s3">,</span>
<a name="l2424"><span class="ln">2424 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2425"><span class="ln">2425 </span></a>real(input) -&gt; Tensor 
<a name="l2426"><span class="ln">2426 </span></a> 
<a name="l2427"><span class="ln">2427 </span></a>Returns a new tensor containing real values of the :attr:`self` tensor. 
<a name="l2428"><span class="ln">2428 </span></a>The returned tensor and :attr:`self` share the same underlying storage. 
<a name="l2429"><span class="ln">2429 </span></a> 
<a name="l2430"><span class="ln">2430 </span></a>Args: 
<a name="l2431"><span class="ln">2431 </span></a>    {input} 
<a name="l2432"><span class="ln">2432 </span></a> 
<a name="l2433"><span class="ln">2433 </span></a>Example:: 
<a name="l2434"><span class="ln">2434 </span></a> 
<a name="l2435"><span class="ln">2435 </span></a>    &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l2436"><span class="ln">2436 </span></a>    &gt;&gt;&gt; x 
<a name="l2437"><span class="ln">2437 </span></a>    tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)]) 
<a name="l2438"><span class="ln">2438 </span></a>    &gt;&gt;&gt; x.real 
<a name="l2439"><span class="ln">2439 </span></a>    tensor([ 0.3100, -0.5445, -1.6492, -0.0638]) 
<a name="l2440"><span class="ln">2440 </span></a> 
<a name="l2441"><span class="ln">2441 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2442"><span class="ln">2442 </span></a><span class="s3">)</span>
<a name="l2443"><span class="ln">2443 </span></a>
<a name="l2444"><span class="ln">2444 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2445"><span class="ln">2445 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">imag</span><span class="s3">,</span>
<a name="l2446"><span class="ln">2446 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2447"><span class="ln">2447 </span></a>imag(input) -&gt; Tensor 
<a name="l2448"><span class="ln">2448 </span></a> 
<a name="l2449"><span class="ln">2449 </span></a>Returns a new tensor containing imaginary values of the :attr:`self` tensor. 
<a name="l2450"><span class="ln">2450 </span></a>The returned tensor and :attr:`self` share the same underlying storage. 
<a name="l2451"><span class="ln">2451 </span></a> 
<a name="l2452"><span class="ln">2452 </span></a>.. warning:: 
<a name="l2453"><span class="ln">2453 </span></a>    :func:`imag` is only supported for tensors with complex dtypes. 
<a name="l2454"><span class="ln">2454 </span></a> 
<a name="l2455"><span class="ln">2455 </span></a>Args: 
<a name="l2456"><span class="ln">2456 </span></a>    {input} 
<a name="l2457"><span class="ln">2457 </span></a> 
<a name="l2458"><span class="ln">2458 </span></a>Example:: 
<a name="l2459"><span class="ln">2459 </span></a> 
<a name="l2460"><span class="ln">2460 </span></a>    &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l2461"><span class="ln">2461 </span></a>    &gt;&gt;&gt; x 
<a name="l2462"><span class="ln">2462 </span></a>    tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)]) 
<a name="l2463"><span class="ln">2463 </span></a>    &gt;&gt;&gt; x.imag 
<a name="l2464"><span class="ln">2464 </span></a>    tensor([ 0.3553, -0.7896, -0.0633, -0.8119]) 
<a name="l2465"><span class="ln">2465 </span></a> 
<a name="l2466"><span class="ln">2466 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2467"><span class="ln">2467 </span></a><span class="s3">)</span>
<a name="l2468"><span class="ln">2468 </span></a>
<a name="l2469"><span class="ln">2469 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2470"><span class="ln">2470 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">view_as_real</span><span class="s3">,</span>
<a name="l2471"><span class="ln">2471 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2472"><span class="ln">2472 </span></a>view_as_real(input) -&gt; Tensor 
<a name="l2473"><span class="ln">2473 </span></a> 
<a name="l2474"><span class="ln">2474 </span></a>Returns a view of :attr:`input` as a real tensor. For an input complex tensor of 
<a name="l2475"><span class="ln">2475 </span></a>:attr:`size` :math:`m1, m2, \dots, mi`, this function returns a new 
<a name="l2476"><span class="ln">2476 </span></a>real tensor of size :math:`m1, m2, \dots, mi, 2`, where the last dimension of size 2 
<a name="l2477"><span class="ln">2477 </span></a>represents the real and imaginary components of complex numbers. 
<a name="l2478"><span class="ln">2478 </span></a> 
<a name="l2479"><span class="ln">2479 </span></a>.. warning:: 
<a name="l2480"><span class="ln">2480 </span></a>    :func:`view_as_real` is only supported for tensors with ``complex dtypes``. 
<a name="l2481"><span class="ln">2481 </span></a> 
<a name="l2482"><span class="ln">2482 </span></a>Args: 
<a name="l2483"><span class="ln">2483 </span></a>    {input} 
<a name="l2484"><span class="ln">2484 </span></a> 
<a name="l2485"><span class="ln">2485 </span></a>Example:: 
<a name="l2486"><span class="ln">2486 </span></a> 
<a name="l2487"><span class="ln">2487 </span></a>    &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l2488"><span class="ln">2488 </span></a>    &gt;&gt;&gt; x 
<a name="l2489"><span class="ln">2489 </span></a>    tensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)]) 
<a name="l2490"><span class="ln">2490 </span></a>    &gt;&gt;&gt; torch.view_as_real(x) 
<a name="l2491"><span class="ln">2491 </span></a>    tensor([[ 0.4737, -0.3839], 
<a name="l2492"><span class="ln">2492 </span></a>            [-0.2098, -0.6699], 
<a name="l2493"><span class="ln">2493 </span></a>            [ 0.3470, -0.9451], 
<a name="l2494"><span class="ln">2494 </span></a>            [-0.5174, -1.3136]]) 
<a name="l2495"><span class="ln">2495 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2496"><span class="ln">2496 </span></a><span class="s3">)</span>
<a name="l2497"><span class="ln">2497 </span></a>
<a name="l2498"><span class="ln">2498 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2499"><span class="ln">2499 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">view_as_complex</span><span class="s3">,</span>
<a name="l2500"><span class="ln">2500 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2501"><span class="ln">2501 </span></a>view_as_complex(input) -&gt; Tensor 
<a name="l2502"><span class="ln">2502 </span></a> 
<a name="l2503"><span class="ln">2503 </span></a>Returns a view of :attr:`input` as a complex tensor. For an input complex 
<a name="l2504"><span class="ln">2504 </span></a>tensor of :attr:`size` :math:`m1, m2, \dots, mi, 2`, this function returns a 
<a name="l2505"><span class="ln">2505 </span></a>new complex tensor of :attr:`size` :math:`m1, m2, \dots, mi` where the last 
<a name="l2506"><span class="ln">2506 </span></a>dimension of the input tensor is expected to represent the real and imaginary 
<a name="l2507"><span class="ln">2507 </span></a>components of complex numbers. 
<a name="l2508"><span class="ln">2508 </span></a> 
<a name="l2509"><span class="ln">2509 </span></a>.. warning:: 
<a name="l2510"><span class="ln">2510 </span></a>    :func:`view_as_complex` is only supported for tensors with 
<a name="l2511"><span class="ln">2511 </span></a>    :class:`torch.dtype` ``torch.float64`` and ``torch.float32``.  The input is 
<a name="l2512"><span class="ln">2512 </span></a>    expected to have the last dimension of :attr:`size` 2. In addition, the 
<a name="l2513"><span class="ln">2513 </span></a>    tensor must have a `stride` of 1 for its last dimension. The strides of all 
<a name="l2514"><span class="ln">2514 </span></a>    other dimensions must be even numbers. 
<a name="l2515"><span class="ln">2515 </span></a> 
<a name="l2516"><span class="ln">2516 </span></a>Args: 
<a name="l2517"><span class="ln">2517 </span></a>    {input} 
<a name="l2518"><span class="ln">2518 </span></a> 
<a name="l2519"><span class="ln">2519 </span></a>Example:: 
<a name="l2520"><span class="ln">2520 </span></a> 
<a name="l2521"><span class="ln">2521 </span></a>    &gt;&gt;&gt; x=torch.randn(4, 2) 
<a name="l2522"><span class="ln">2522 </span></a>    &gt;&gt;&gt; x 
<a name="l2523"><span class="ln">2523 </span></a>    tensor([[ 1.6116, -0.5772], 
<a name="l2524"><span class="ln">2524 </span></a>            [-1.4606, -0.9120], 
<a name="l2525"><span class="ln">2525 </span></a>            [ 0.0786, -1.7497], 
<a name="l2526"><span class="ln">2526 </span></a>            [-0.6561, -1.6623]]) 
<a name="l2527"><span class="ln">2527 </span></a>    &gt;&gt;&gt; torch.view_as_complex(x) 
<a name="l2528"><span class="ln">2528 </span></a>    tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)]) 
<a name="l2529"><span class="ln">2529 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2530"><span class="ln">2530 </span></a><span class="s3">)</span>
<a name="l2531"><span class="ln">2531 </span></a>
<a name="l2532"><span class="ln">2532 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2533"><span class="ln">2533 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">reciprocal</span><span class="s3">,</span>
<a name="l2534"><span class="ln">2534 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2535"><span class="ln">2535 </span></a>reciprocal(input, *, out=None) -&gt; Tensor 
<a name="l2536"><span class="ln">2536 </span></a> 
<a name="l2537"><span class="ln">2537 </span></a>Returns a new tensor with the reciprocal of the elements of :attr:`input` 
<a name="l2538"><span class="ln">2538 </span></a> 
<a name="l2539"><span class="ln">2539 </span></a>.. math:: 
<a name="l2540"><span class="ln">2540 </span></a>    \text{out}_{i} = \frac{1}{\text{input}_{i}} 
<a name="l2541"><span class="ln">2541 </span></a> 
<a name="l2542"><span class="ln">2542 </span></a>.. note:: 
<a name="l2543"><span class="ln">2543 </span></a>    Unlike NumPy's reciprocal, torch.reciprocal supports integral inputs. Integral 
<a name="l2544"><span class="ln">2544 </span></a>    inputs to reciprocal are automatically :ref:`promoted &lt;type-promotion-doc&gt;` to 
<a name="l2545"><span class="ln">2545 </span></a>    the default scalar type. 
<a name="l2546"><span class="ln">2546 </span></a>&quot;&quot;&quot;</span>
<a name="l2547"><span class="ln">2547 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l2548"><span class="ln">2548 </span></a>Args: 
<a name="l2549"><span class="ln">2549 </span></a>    {input} 
<a name="l2550"><span class="ln">2550 </span></a> 
<a name="l2551"><span class="ln">2551 </span></a>Keyword args: 
<a name="l2552"><span class="ln">2552 </span></a>    {out} 
<a name="l2553"><span class="ln">2553 </span></a> 
<a name="l2554"><span class="ln">2554 </span></a>Example:: 
<a name="l2555"><span class="ln">2555 </span></a> 
<a name="l2556"><span class="ln">2556 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l2557"><span class="ln">2557 </span></a>    &gt;&gt;&gt; a 
<a name="l2558"><span class="ln">2558 </span></a>    tensor([-0.4595, -2.1219, -1.4314,  0.7298]) 
<a name="l2559"><span class="ln">2559 </span></a>    &gt;&gt;&gt; torch.reciprocal(a) 
<a name="l2560"><span class="ln">2560 </span></a>    tensor([-2.1763, -0.4713, -0.6986,  1.3702]) 
<a name="l2561"><span class="ln">2561 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2562"><span class="ln">2562 </span></a><span class="s3">)</span>
<a name="l2563"><span class="ln">2563 </span></a>
<a name="l2564"><span class="ln">2564 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2565"><span class="ln">2565 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cholesky</span><span class="s3">,</span>
<a name="l2566"><span class="ln">2566 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2567"><span class="ln">2567 </span></a>cholesky(input, upper=False, *, out=None) -&gt; Tensor 
<a name="l2568"><span class="ln">2568 </span></a> 
<a name="l2569"><span class="ln">2569 </span></a>Computes the Cholesky decomposition of a symmetric positive-definite 
<a name="l2570"><span class="ln">2570 </span></a>matrix :math:`A` or for batches of symmetric positive-definite matrices. 
<a name="l2571"><span class="ln">2571 </span></a> 
<a name="l2572"><span class="ln">2572 </span></a>If :attr:`upper` is ``True``, the returned matrix ``U`` is upper-triangular, and 
<a name="l2573"><span class="ln">2573 </span></a>the decomposition has the form: 
<a name="l2574"><span class="ln">2574 </span></a> 
<a name="l2575"><span class="ln">2575 </span></a>.. math:: 
<a name="l2576"><span class="ln">2576 </span></a> 
<a name="l2577"><span class="ln">2577 </span></a>  A = U^TU 
<a name="l2578"><span class="ln">2578 </span></a> 
<a name="l2579"><span class="ln">2579 </span></a>If :attr:`upper` is ``False``, the returned matrix ``L`` is lower-triangular, and 
<a name="l2580"><span class="ln">2580 </span></a>the decomposition has the form: 
<a name="l2581"><span class="ln">2581 </span></a> 
<a name="l2582"><span class="ln">2582 </span></a>.. math:: 
<a name="l2583"><span class="ln">2583 </span></a> 
<a name="l2584"><span class="ln">2584 </span></a>    A = LL^T 
<a name="l2585"><span class="ln">2585 </span></a> 
<a name="l2586"><span class="ln">2586 </span></a>If :attr:`upper` is ``True``, and :math:`A` is a batch of symmetric positive-definite 
<a name="l2587"><span class="ln">2587 </span></a>matrices, then the returned tensor will be composed of upper-triangular Cholesky factors 
<a name="l2588"><span class="ln">2588 </span></a>of each of the individual matrices. Similarly, when :attr:`upper` is ``False``, the returned 
<a name="l2589"><span class="ln">2589 </span></a>tensor will be composed of lower-triangular Cholesky factors of each of the individual 
<a name="l2590"><span class="ln">2590 </span></a>matrices. 
<a name="l2591"><span class="ln">2591 </span></a> 
<a name="l2592"><span class="ln">2592 </span></a>.. warning:: 
<a name="l2593"><span class="ln">2593 </span></a> 
<a name="l2594"><span class="ln">2594 </span></a>    :func:`torch.cholesky` is deprecated in favor of :func:`torch.linalg.cholesky` 
<a name="l2595"><span class="ln">2595 </span></a>    and will be removed in a future PyTorch release. 
<a name="l2596"><span class="ln">2596 </span></a> 
<a name="l2597"><span class="ln">2597 </span></a>    ``L = torch.cholesky(A)`` should be replaced with 
<a name="l2598"><span class="ln">2598 </span></a> 
<a name="l2599"><span class="ln">2599 </span></a>    .. code:: python 
<a name="l2600"><span class="ln">2600 </span></a> 
<a name="l2601"><span class="ln">2601 </span></a>        L = torch.linalg.cholesky(A) 
<a name="l2602"><span class="ln">2602 </span></a> 
<a name="l2603"><span class="ln">2603 </span></a>    ``U = torch.cholesky(A, upper=True)`` should be replaced with 
<a name="l2604"><span class="ln">2604 </span></a> 
<a name="l2605"><span class="ln">2605 </span></a>    .. code:: python 
<a name="l2606"><span class="ln">2606 </span></a> 
<a name="l2607"><span class="ln">2607 </span></a>        U = torch.linalg.cholesky(A).mH 
<a name="l2608"><span class="ln">2608 </span></a> 
<a name="l2609"><span class="ln">2609 </span></a>    This transform will produce equivalent results for all valid (symmetric positive definite) inputs. 
<a name="l2610"><span class="ln">2610 </span></a> 
<a name="l2611"><span class="ln">2611 </span></a>Args: 
<a name="l2612"><span class="ln">2612 </span></a>    input (Tensor): the input tensor :math:`A` of size :math:`(*, n, n)` where `*` is zero or more 
<a name="l2613"><span class="ln">2613 </span></a>                batch dimensions consisting of symmetric positive-definite matrices. 
<a name="l2614"><span class="ln">2614 </span></a>    upper (bool, optional): flag that indicates whether to return a 
<a name="l2615"><span class="ln">2615 </span></a>                            upper or lower triangular matrix. Default: ``False`` 
<a name="l2616"><span class="ln">2616 </span></a> 
<a name="l2617"><span class="ln">2617 </span></a>Keyword args: 
<a name="l2618"><span class="ln">2618 </span></a>    out (Tensor, optional): the output matrix 
<a name="l2619"><span class="ln">2619 </span></a> 
<a name="l2620"><span class="ln">2620 </span></a>Example:: 
<a name="l2621"><span class="ln">2621 </span></a> 
<a name="l2622"><span class="ln">2622 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l2623"><span class="ln">2623 </span></a>    &gt;&gt;&gt; a = a @ a.mT + 1e-3 # make symmetric positive-definite 
<a name="l2624"><span class="ln">2624 </span></a>    &gt;&gt;&gt; l = torch.cholesky(a) 
<a name="l2625"><span class="ln">2625 </span></a>    &gt;&gt;&gt; a 
<a name="l2626"><span class="ln">2626 </span></a>    tensor([[ 2.4112, -0.7486,  1.4551], 
<a name="l2627"><span class="ln">2627 </span></a>            [-0.7486,  1.3544,  0.1294], 
<a name="l2628"><span class="ln">2628 </span></a>            [ 1.4551,  0.1294,  1.6724]]) 
<a name="l2629"><span class="ln">2629 </span></a>    &gt;&gt;&gt; l 
<a name="l2630"><span class="ln">2630 </span></a>    tensor([[ 1.5528,  0.0000,  0.0000], 
<a name="l2631"><span class="ln">2631 </span></a>            [-0.4821,  1.0592,  0.0000], 
<a name="l2632"><span class="ln">2632 </span></a>            [ 0.9371,  0.5487,  0.7023]]) 
<a name="l2633"><span class="ln">2633 </span></a>    &gt;&gt;&gt; l @ l.mT 
<a name="l2634"><span class="ln">2634 </span></a>    tensor([[ 2.4112, -0.7486,  1.4551], 
<a name="l2635"><span class="ln">2635 </span></a>            [-0.7486,  1.3544,  0.1294], 
<a name="l2636"><span class="ln">2636 </span></a>            [ 1.4551,  0.1294,  1.6724]]) 
<a name="l2637"><span class="ln">2637 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 2, 2) # Example for batched input 
<a name="l2638"><span class="ln">2638 </span></a>    &gt;&gt;&gt; a = a @ a.mT + 1e-03 # make symmetric positive-definite 
<a name="l2639"><span class="ln">2639 </span></a>    &gt;&gt;&gt; l = torch.cholesky(a) 
<a name="l2640"><span class="ln">2640 </span></a>    &gt;&gt;&gt; z = l @ l.mT 
<a name="l2641"><span class="ln">2641 </span></a>    &gt;&gt;&gt; torch.dist(z, a) 
<a name="l2642"><span class="ln">2642 </span></a>    tensor(2.3842e-07) 
<a name="l2643"><span class="ln">2643 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2644"><span class="ln">2644 </span></a><span class="s3">)</span>
<a name="l2645"><span class="ln">2645 </span></a>
<a name="l2646"><span class="ln">2646 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2647"><span class="ln">2647 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cholesky_solve</span><span class="s3">,</span>
<a name="l2648"><span class="ln">2648 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2649"><span class="ln">2649 </span></a>cholesky_solve(B, L, upper=False, *, out=None) -&gt; Tensor 
<a name="l2650"><span class="ln">2650 </span></a> 
<a name="l2651"><span class="ln">2651 </span></a>Computes the solution of a system of linear equations with complex Hermitian 
<a name="l2652"><span class="ln">2652 </span></a>or real symmetric positive-definite lhs given its Cholesky decomposition. 
<a name="l2653"><span class="ln">2653 </span></a> 
<a name="l2654"><span class="ln">2654 </span></a>Let :math:`A` be a complex Hermitian or real symmetric positive-definite matrix, 
<a name="l2655"><span class="ln">2655 </span></a>and :math:`L` its Cholesky decomposition such that: 
<a name="l2656"><span class="ln">2656 </span></a> 
<a name="l2657"><span class="ln">2657 </span></a>.. math:: 
<a name="l2658"><span class="ln">2658 </span></a> 
<a name="l2659"><span class="ln">2659 </span></a>    A = LL^{\text{H}} 
<a name="l2660"><span class="ln">2660 </span></a> 
<a name="l2661"><span class="ln">2661 </span></a>where :math:`L^{\text{H}}` is the conjugate transpose when :math:`L` is complex, 
<a name="l2662"><span class="ln">2662 </span></a>and the transpose when :math:`L` is real-valued. 
<a name="l2663"><span class="ln">2663 </span></a> 
<a name="l2664"><span class="ln">2664 </span></a>Returns the solution :math:`X` of the following linear system: 
<a name="l2665"><span class="ln">2665 </span></a> 
<a name="l2666"><span class="ln">2666 </span></a>.. math:: 
<a name="l2667"><span class="ln">2667 </span></a> 
<a name="l2668"><span class="ln">2668 </span></a>    AX = B 
<a name="l2669"><span class="ln">2669 </span></a> 
<a name="l2670"><span class="ln">2670 </span></a>Supports inputs of float, double, cfloat and cdouble dtypes. 
<a name="l2671"><span class="ln">2671 </span></a>Also supports batches of matrices, and if :math:`A` or :math:`B` is a batch of matrices 
<a name="l2672"><span class="ln">2672 </span></a>then the output has the same batch dimensions. 
<a name="l2673"><span class="ln">2673 </span></a> 
<a name="l2674"><span class="ln">2674 </span></a>Args: 
<a name="l2675"><span class="ln">2675 </span></a>    B (Tensor): right-hand side tensor of shape `(*, n, k)` 
<a name="l2676"><span class="ln">2676 </span></a>        where :math:`*` is zero or more batch dimensions 
<a name="l2677"><span class="ln">2677 </span></a>    L (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions 
<a name="l2678"><span class="ln">2678 </span></a>        consisting of lower or upper triangular Cholesky decompositions of 
<a name="l2679"><span class="ln">2679 </span></a>        symmetric or Hermitian positive-definite matrices. 
<a name="l2680"><span class="ln">2680 </span></a>    upper (bool, optional): flag that indicates whether :math:`L` is lower triangular 
<a name="l2681"><span class="ln">2681 </span></a>        or upper triangular. Default: ``False``. 
<a name="l2682"><span class="ln">2682 </span></a> 
<a name="l2683"><span class="ln">2683 </span></a>Keyword args: 
<a name="l2684"><span class="ln">2684 </span></a>    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`. 
<a name="l2685"><span class="ln">2685 </span></a> 
<a name="l2686"><span class="ln">2686 </span></a>Example:: 
<a name="l2687"><span class="ln">2687 </span></a> 
<a name="l2688"><span class="ln">2688 </span></a>    &gt;&gt;&gt; A = torch.randn(3, 3) 
<a name="l2689"><span class="ln">2689 </span></a>    &gt;&gt;&gt; A = A @ A.T + torch.eye(3) * 1e-3 # Creates a symmetric positive-definite matrix 
<a name="l2690"><span class="ln">2690 </span></a>    &gt;&gt;&gt; L = torch.linalg.cholesky(A) # Extract Cholesky decomposition 
<a name="l2691"><span class="ln">2691 </span></a>    &gt;&gt;&gt; B = torch.randn(3, 2) 
<a name="l2692"><span class="ln">2692 </span></a>    &gt;&gt;&gt; torch.cholesky_solve(B, L) 
<a name="l2693"><span class="ln">2693 </span></a>    tensor([[ -8.1625,  19.6097], 
<a name="l2694"><span class="ln">2694 </span></a>            [ -5.8398,  14.2387], 
<a name="l2695"><span class="ln">2695 </span></a>            [ -4.3771,  10.4173]]) 
<a name="l2696"><span class="ln">2696 </span></a>    &gt;&gt;&gt; A.inverse() @  B 
<a name="l2697"><span class="ln">2697 </span></a>    tensor([[ -8.1626,  19.6097], 
<a name="l2698"><span class="ln">2698 </span></a>            [ -5.8398,  14.2387], 
<a name="l2699"><span class="ln">2699 </span></a>            [ -4.3771,  10.4173]]) 
<a name="l2700"><span class="ln">2700 </span></a> 
<a name="l2701"><span class="ln">2701 </span></a>    &gt;&gt;&gt; A = torch.randn(3, 2, 2, dtype=torch.complex64) 
<a name="l2702"><span class="ln">2702 </span></a>    &gt;&gt;&gt; A = A @ A.mH + torch.eye(2) * 1e-3 # Batch of Hermitian positive-definite matrices 
<a name="l2703"><span class="ln">2703 </span></a>    &gt;&gt;&gt; L = torch.linalg.cholesky(A) 
<a name="l2704"><span class="ln">2704 </span></a>    &gt;&gt;&gt; B = torch.randn(2, 1, dtype=torch.complex64) 
<a name="l2705"><span class="ln">2705 </span></a>    &gt;&gt;&gt; X = torch.cholesky_solve(B, L) 
<a name="l2706"><span class="ln">2706 </span></a>    &gt;&gt;&gt; torch.dist(X, A.inverse() @ B) 
<a name="l2707"><span class="ln">2707 </span></a>    tensor(1.6881e-5) 
<a name="l2708"><span class="ln">2708 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2709"><span class="ln">2709 </span></a><span class="s3">)</span>
<a name="l2710"><span class="ln">2710 </span></a>
<a name="l2711"><span class="ln">2711 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2712"><span class="ln">2712 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cholesky_inverse</span><span class="s3">,</span>
<a name="l2713"><span class="ln">2713 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2714"><span class="ln">2714 </span></a>cholesky_inverse(L, upper=False, *, out=None) -&gt; Tensor 
<a name="l2715"><span class="ln">2715 </span></a> 
<a name="l2716"><span class="ln">2716 </span></a>Computes the inverse of a complex Hermitian or real symmetric 
<a name="l2717"><span class="ln">2717 </span></a>positive-definite matrix given its Cholesky decomposition. 
<a name="l2718"><span class="ln">2718 </span></a> 
<a name="l2719"><span class="ln">2719 </span></a>Let :math:`A` be a complex Hermitian or real symmetric positive-definite matrix, 
<a name="l2720"><span class="ln">2720 </span></a>and :math:`L` its Cholesky decomposition such that: 
<a name="l2721"><span class="ln">2721 </span></a> 
<a name="l2722"><span class="ln">2722 </span></a>.. math:: 
<a name="l2723"><span class="ln">2723 </span></a> 
<a name="l2724"><span class="ln">2724 </span></a>    A = LL^{\text{H}} 
<a name="l2725"><span class="ln">2725 </span></a> 
<a name="l2726"><span class="ln">2726 </span></a>where :math:`L^{\text{H}}` is the conjugate transpose when :math:`L` is complex, 
<a name="l2727"><span class="ln">2727 </span></a>and the transpose when :math:`L` is real-valued. 
<a name="l2728"><span class="ln">2728 </span></a> 
<a name="l2729"><span class="ln">2729 </span></a>Computes the inverse matrix :math:`A^{-1}`. 
<a name="l2730"><span class="ln">2730 </span></a> 
<a name="l2731"><span class="ln">2731 </span></a>Supports input of float, double, cfloat and cdouble dtypes. 
<a name="l2732"><span class="ln">2732 </span></a>Also supports batches of matrices, and if :math:`A` is a batch of matrices 
<a name="l2733"><span class="ln">2733 </span></a>then the output has the same batch dimensions. 
<a name="l2734"><span class="ln">2734 </span></a> 
<a name="l2735"><span class="ln">2735 </span></a>Args: 
<a name="l2736"><span class="ln">2736 </span></a>    L (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions 
<a name="l2737"><span class="ln">2737 </span></a>        consisting of lower or upper triangular Cholesky decompositions of 
<a name="l2738"><span class="ln">2738 </span></a>        symmetric or Hermitian positive-definite matrices. 
<a name="l2739"><span class="ln">2739 </span></a>    upper (bool, optional): flag that indicates whether :math:`L` is lower triangular 
<a name="l2740"><span class="ln">2740 </span></a>        or upper triangular. Default: ``False`` 
<a name="l2741"><span class="ln">2741 </span></a> 
<a name="l2742"><span class="ln">2742 </span></a>Keyword args: 
<a name="l2743"><span class="ln">2743 </span></a>    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`. 
<a name="l2744"><span class="ln">2744 </span></a> 
<a name="l2745"><span class="ln">2745 </span></a>Example:: 
<a name="l2746"><span class="ln">2746 </span></a> 
<a name="l2747"><span class="ln">2747 </span></a>    &gt;&gt;&gt; A = torch.randn(3, 3) 
<a name="l2748"><span class="ln">2748 </span></a>    &gt;&gt;&gt; A = A @ A.T + torch.eye(3) * 1e-3 # Creates a symmetric positive-definite matrix 
<a name="l2749"><span class="ln">2749 </span></a>    &gt;&gt;&gt; L = torch.linalg.cholesky(A) # Extract Cholesky decomposition 
<a name="l2750"><span class="ln">2750 </span></a>    &gt;&gt;&gt; torch.cholesky_inverse(L) 
<a name="l2751"><span class="ln">2751 </span></a>    tensor([[ 1.9314,  1.2251, -0.0889], 
<a name="l2752"><span class="ln">2752 </span></a>            [ 1.2251,  2.4439,  0.2122], 
<a name="l2753"><span class="ln">2753 </span></a>            [-0.0889,  0.2122,  0.1412]]) 
<a name="l2754"><span class="ln">2754 </span></a>    &gt;&gt;&gt; A.inverse() 
<a name="l2755"><span class="ln">2755 </span></a>    tensor([[ 1.9314,  1.2251, -0.0889], 
<a name="l2756"><span class="ln">2756 </span></a>            [ 1.2251,  2.4439,  0.2122], 
<a name="l2757"><span class="ln">2757 </span></a>            [-0.0889,  0.2122,  0.1412]]) 
<a name="l2758"><span class="ln">2758 </span></a> 
<a name="l2759"><span class="ln">2759 </span></a>    &gt;&gt;&gt; A = torch.randn(3, 2, 2, dtype=torch.complex64) 
<a name="l2760"><span class="ln">2760 </span></a>    &gt;&gt;&gt; A = A @ A.mH + torch.eye(2) * 1e-3 # Batch of Hermitian positive-definite matrices 
<a name="l2761"><span class="ln">2761 </span></a>    &gt;&gt;&gt; L = torch.linalg.cholesky(A) 
<a name="l2762"><span class="ln">2762 </span></a>    &gt;&gt;&gt; torch.dist(torch.inverse(A), torch.cholesky_inverse(L)) 
<a name="l2763"><span class="ln">2763 </span></a>    tensor(5.6358e-7) 
<a name="l2764"><span class="ln">2764 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2765"><span class="ln">2765 </span></a><span class="s3">)</span>
<a name="l2766"><span class="ln">2766 </span></a>
<a name="l2767"><span class="ln">2767 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2768"><span class="ln">2768 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">clone</span><span class="s3">,</span>
<a name="l2769"><span class="ln">2769 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2770"><span class="ln">2770 </span></a>clone(input, *, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l2771"><span class="ln">2771 </span></a> 
<a name="l2772"><span class="ln">2772 </span></a>Returns a copy of :attr:`input`. 
<a name="l2773"><span class="ln">2773 </span></a> 
<a name="l2774"><span class="ln">2774 </span></a>.. note:: 
<a name="l2775"><span class="ln">2775 </span></a> 
<a name="l2776"><span class="ln">2776 </span></a>    This function is differentiable, so gradients will flow back from the 
<a name="l2777"><span class="ln">2777 </span></a>    result of this operation to :attr:`input`. To create a tensor without an 
<a name="l2778"><span class="ln">2778 </span></a>    autograd relationship to :attr:`input` see :meth:`~Tensor.detach`. 
<a name="l2779"><span class="ln">2779 </span></a> 
<a name="l2780"><span class="ln">2780 </span></a>Args: 
<a name="l2781"><span class="ln">2781 </span></a>    {input} 
<a name="l2782"><span class="ln">2782 </span></a> 
<a name="l2783"><span class="ln">2783 </span></a>Keyword args: 
<a name="l2784"><span class="ln">2784 </span></a>    {memory_format} 
<a name="l2785"><span class="ln">2785 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2786"><span class="ln">2786 </span></a><span class="s3">)</span>
<a name="l2787"><span class="ln">2787 </span></a>
<a name="l2788"><span class="ln">2788 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2789"><span class="ln">2789 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">clamp</span><span class="s3">,</span>
<a name="l2790"><span class="ln">2790 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2791"><span class="ln">2791 </span></a>clamp(input, min=None, max=None, *, out=None) -&gt; Tensor 
<a name="l2792"><span class="ln">2792 </span></a> 
<a name="l2793"><span class="ln">2793 </span></a>Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`. 
<a name="l2794"><span class="ln">2794 </span></a>Letting min_value and max_value be :attr:`min` and :attr:`max`, respectively, this returns: 
<a name="l2795"><span class="ln">2795 </span></a> 
<a name="l2796"><span class="ln">2796 </span></a>.. math:: 
<a name="l2797"><span class="ln">2797 </span></a>    y_i = \min(\max(x_i, \text{min\_value}_i), \text{max\_value}_i) 
<a name="l2798"><span class="ln">2798 </span></a> 
<a name="l2799"><span class="ln">2799 </span></a>If :attr:`min` is ``None``, there is no lower bound. 
<a name="l2800"><span class="ln">2800 </span></a>Or, if :attr:`max` is ``None`` there is no upper bound. 
<a name="l2801"><span class="ln">2801 </span></a>&quot;&quot;&quot;</span>
<a name="l2802"><span class="ln">2802 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l2803"><span class="ln">2803 </span></a> 
<a name="l2804"><span class="ln">2804 </span></a>.. note:: 
<a name="l2805"><span class="ln">2805 </span></a>    If :attr:`min` is greater than :attr:`max` :func:`torch.clamp(..., min, max) &lt;torch.clamp&gt;` 
<a name="l2806"><span class="ln">2806 </span></a>    sets all elements in :attr:`input` to the value of :attr:`max`. 
<a name="l2807"><span class="ln">2807 </span></a> 
<a name="l2808"><span class="ln">2808 </span></a>Args: 
<a name="l2809"><span class="ln">2809 </span></a>    {input} 
<a name="l2810"><span class="ln">2810 </span></a>    min (Number or Tensor, optional): lower-bound of the range to be clamped to 
<a name="l2811"><span class="ln">2811 </span></a>    max (Number or Tensor, optional): upper-bound of the range to be clamped to 
<a name="l2812"><span class="ln">2812 </span></a> 
<a name="l2813"><span class="ln">2813 </span></a>Keyword args: 
<a name="l2814"><span class="ln">2814 </span></a>    {out} 
<a name="l2815"><span class="ln">2815 </span></a> 
<a name="l2816"><span class="ln">2816 </span></a>Example:: 
<a name="l2817"><span class="ln">2817 </span></a> 
<a name="l2818"><span class="ln">2818 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l2819"><span class="ln">2819 </span></a>    &gt;&gt;&gt; a 
<a name="l2820"><span class="ln">2820 </span></a>    tensor([-1.7120,  0.1734, -0.0478, -0.0922]) 
<a name="l2821"><span class="ln">2821 </span></a>    &gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5) 
<a name="l2822"><span class="ln">2822 </span></a>    tensor([-0.5000,  0.1734, -0.0478, -0.0922]) 
<a name="l2823"><span class="ln">2823 </span></a> 
<a name="l2824"><span class="ln">2824 </span></a>    &gt;&gt;&gt; min = torch.linspace(-1, 1, steps=4) 
<a name="l2825"><span class="ln">2825 </span></a>    &gt;&gt;&gt; torch.clamp(a, min=min) 
<a name="l2826"><span class="ln">2826 </span></a>    tensor([-1.0000,  0.1734,  0.3333,  1.0000]) 
<a name="l2827"><span class="ln">2827 </span></a> 
<a name="l2828"><span class="ln">2828 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2829"><span class="ln">2829 </span></a><span class="s3">)</span>
<a name="l2830"><span class="ln">2830 </span></a>
<a name="l2831"><span class="ln">2831 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2832"><span class="ln">2832 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">clip</span><span class="s3">,</span>
<a name="l2833"><span class="ln">2833 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2834"><span class="ln">2834 </span></a>clip(input, min=None, max=None, *, out=None) -&gt; Tensor 
<a name="l2835"><span class="ln">2835 </span></a> 
<a name="l2836"><span class="ln">2836 </span></a>Alias for :func:`torch.clamp`. 
<a name="l2837"><span class="ln">2837 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2838"><span class="ln">2838 </span></a><span class="s3">)</span>
<a name="l2839"><span class="ln">2839 </span></a>
<a name="l2840"><span class="ln">2840 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2841"><span class="ln">2841 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">column_stack</span><span class="s3">,</span>
<a name="l2842"><span class="ln">2842 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2843"><span class="ln">2843 </span></a>column_stack(tensors, *, out=None) -&gt; Tensor 
<a name="l2844"><span class="ln">2844 </span></a> 
<a name="l2845"><span class="ln">2845 </span></a>Creates a new tensor by horizontally stacking the tensors in :attr:`tensors`. 
<a name="l2846"><span class="ln">2846 </span></a> 
<a name="l2847"><span class="ln">2847 </span></a>Equivalent to ``torch.hstack(tensors)``, except each zero or one dimensional tensor ``t`` 
<a name="l2848"><span class="ln">2848 </span></a>in :attr:`tensors` is first reshaped into a ``(t.numel(), 1)`` column before being stacked horizontally. 
<a name="l2849"><span class="ln">2849 </span></a> 
<a name="l2850"><span class="ln">2850 </span></a>Args: 
<a name="l2851"><span class="ln">2851 </span></a>    tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l2852"><span class="ln">2852 </span></a> 
<a name="l2853"><span class="ln">2853 </span></a>Keyword args: 
<a name="l2854"><span class="ln">2854 </span></a>    {out} 
<a name="l2855"><span class="ln">2855 </span></a> 
<a name="l2856"><span class="ln">2856 </span></a>Example:: 
<a name="l2857"><span class="ln">2857 </span></a> 
<a name="l2858"><span class="ln">2858 </span></a>    &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l2859"><span class="ln">2859 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l2860"><span class="ln">2860 </span></a>    &gt;&gt;&gt; torch.column_stack((a, b)) 
<a name="l2861"><span class="ln">2861 </span></a>    tensor([[1, 4], 
<a name="l2862"><span class="ln">2862 </span></a>        [2, 5], 
<a name="l2863"><span class="ln">2863 </span></a>        [3, 6]]) 
<a name="l2864"><span class="ln">2864 </span></a>    &gt;&gt;&gt; a = torch.arange(5) 
<a name="l2865"><span class="ln">2865 </span></a>    &gt;&gt;&gt; b = torch.arange(10).reshape(5, 2) 
<a name="l2866"><span class="ln">2866 </span></a>    &gt;&gt;&gt; torch.column_stack((a, b, b)) 
<a name="l2867"><span class="ln">2867 </span></a>    tensor([[0, 0, 1, 0, 1], 
<a name="l2868"><span class="ln">2868 </span></a>            [1, 2, 3, 2, 3], 
<a name="l2869"><span class="ln">2869 </span></a>            [2, 4, 5, 4, 5], 
<a name="l2870"><span class="ln">2870 </span></a>            [3, 6, 7, 6, 7], 
<a name="l2871"><span class="ln">2871 </span></a>            [4, 8, 9, 8, 9]]) 
<a name="l2872"><span class="ln">2872 </span></a> 
<a name="l2873"><span class="ln">2873 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2874"><span class="ln">2874 </span></a><span class="s3">)</span>
<a name="l2875"><span class="ln">2875 </span></a>
<a name="l2876"><span class="ln">2876 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2877"><span class="ln">2877 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">complex</span><span class="s3">,</span>
<a name="l2878"><span class="ln">2878 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2879"><span class="ln">2879 </span></a>complex(real, imag, *, out=None) -&gt; Tensor 
<a name="l2880"><span class="ln">2880 </span></a> 
<a name="l2881"><span class="ln">2881 </span></a>Constructs a complex tensor with its real part equal to :attr:`real` and its 
<a name="l2882"><span class="ln">2882 </span></a>imaginary part equal to :attr:`imag`. 
<a name="l2883"><span class="ln">2883 </span></a> 
<a name="l2884"><span class="ln">2884 </span></a>Args: 
<a name="l2885"><span class="ln">2885 </span></a>    real (Tensor): The real part of the complex tensor. Must be half, float or double. 
<a name="l2886"><span class="ln">2886 </span></a>    imag (Tensor): The imaginary part of the complex tensor. Must be same dtype 
<a name="l2887"><span class="ln">2887 </span></a>        as :attr:`real`. 
<a name="l2888"><span class="ln">2888 </span></a> 
<a name="l2889"><span class="ln">2889 </span></a>Keyword args: 
<a name="l2890"><span class="ln">2890 </span></a>    out (Tensor): If the inputs are ``torch.float32``, must be 
<a name="l2891"><span class="ln">2891 </span></a>        ``torch.complex64``. If the inputs are ``torch.float64``, must be 
<a name="l2892"><span class="ln">2892 </span></a>        ``torch.complex128``. 
<a name="l2893"><span class="ln">2893 </span></a> 
<a name="l2894"><span class="ln">2894 </span></a>Example:: 
<a name="l2895"><span class="ln">2895 </span></a> 
<a name="l2896"><span class="ln">2896 </span></a>    &gt;&gt;&gt; real = torch.tensor([1, 2], dtype=torch.float32) 
<a name="l2897"><span class="ln">2897 </span></a>    &gt;&gt;&gt; imag = torch.tensor([3, 4], dtype=torch.float32) 
<a name="l2898"><span class="ln">2898 </span></a>    &gt;&gt;&gt; z = torch.complex(real, imag) 
<a name="l2899"><span class="ln">2899 </span></a>    &gt;&gt;&gt; z 
<a name="l2900"><span class="ln">2900 </span></a>    tensor([(1.+3.j), (2.+4.j)]) 
<a name="l2901"><span class="ln">2901 </span></a>    &gt;&gt;&gt; z.dtype 
<a name="l2902"><span class="ln">2902 </span></a>    torch.complex64 
<a name="l2903"><span class="ln">2903 </span></a> 
<a name="l2904"><span class="ln">2904 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2905"><span class="ln">2905 </span></a><span class="s3">)</span>
<a name="l2906"><span class="ln">2906 </span></a>
<a name="l2907"><span class="ln">2907 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2908"><span class="ln">2908 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">polar</span><span class="s3">,</span>
<a name="l2909"><span class="ln">2909 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2910"><span class="ln">2910 </span></a>polar(abs, angle, *, out=None) -&gt; Tensor 
<a name="l2911"><span class="ln">2911 </span></a> 
<a name="l2912"><span class="ln">2912 </span></a>Constructs a complex tensor whose elements are Cartesian coordinates 
<a name="l2913"><span class="ln">2913 </span></a>corresponding to the polar coordinates with absolute value :attr:`abs` and angle 
<a name="l2914"><span class="ln">2914 </span></a>:attr:`angle`. 
<a name="l2915"><span class="ln">2915 </span></a> 
<a name="l2916"><span class="ln">2916 </span></a>.. math:: 
<a name="l2917"><span class="ln">2917 </span></a>    \text{out} = \text{abs} \cdot \cos(\text{angle}) + \text{abs} \cdot \sin(\text{angle}) \cdot j 
<a name="l2918"><span class="ln">2918 </span></a> 
<a name="l2919"><span class="ln">2919 </span></a>.. note:: 
<a name="l2920"><span class="ln">2920 </span></a>    `torch.polar` is similar to 
<a name="l2921"><span class="ln">2921 </span></a>    `std::polar &lt;https://en.cppreference.com/w/cpp/numeric/complex/polar&gt;`_ 
<a name="l2922"><span class="ln">2922 </span></a>    and does not compute the polar decomposition 
<a name="l2923"><span class="ln">2923 </span></a>    of a complex tensor like Python's `cmath.polar` and SciPy's `linalg.polar` do. 
<a name="l2924"><span class="ln">2924 </span></a>    The behavior of this function is undefined if `abs` is negative or NaN, or if `angle` is 
<a name="l2925"><span class="ln">2925 </span></a>    infinite. 
<a name="l2926"><span class="ln">2926 </span></a> 
<a name="l2927"><span class="ln">2927 </span></a>&quot;&quot;&quot;</span>
<a name="l2928"><span class="ln">2928 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l2929"><span class="ln">2929 </span></a>Args: 
<a name="l2930"><span class="ln">2930 </span></a>    abs (Tensor): The absolute value the complex tensor. Must be float or double. 
<a name="l2931"><span class="ln">2931 </span></a>    angle (Tensor): The angle of the complex tensor. Must be same dtype as 
<a name="l2932"><span class="ln">2932 </span></a>        :attr:`abs`. 
<a name="l2933"><span class="ln">2933 </span></a> 
<a name="l2934"><span class="ln">2934 </span></a>Keyword args: 
<a name="l2935"><span class="ln">2935 </span></a>    out (Tensor): If the inputs are ``torch.float32``, must be 
<a name="l2936"><span class="ln">2936 </span></a>        ``torch.complex64``. If the inputs are ``torch.float64``, must be 
<a name="l2937"><span class="ln">2937 </span></a>        ``torch.complex128``. 
<a name="l2938"><span class="ln">2938 </span></a> 
<a name="l2939"><span class="ln">2939 </span></a>Example:: 
<a name="l2940"><span class="ln">2940 </span></a> 
<a name="l2941"><span class="ln">2941 </span></a>    &gt;&gt;&gt; import numpy as np 
<a name="l2942"><span class="ln">2942 </span></a>    &gt;&gt;&gt; abs = torch.tensor([1, 2], dtype=torch.float64) 
<a name="l2943"><span class="ln">2943 </span></a>    &gt;&gt;&gt; angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64) 
<a name="l2944"><span class="ln">2944 </span></a>    &gt;&gt;&gt; z = torch.polar(abs, angle) 
<a name="l2945"><span class="ln">2945 </span></a>    &gt;&gt;&gt; z 
<a name="l2946"><span class="ln">2946 </span></a>    tensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128) 
<a name="l2947"><span class="ln">2947 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2948"><span class="ln">2948 </span></a><span class="s3">)</span>
<a name="l2949"><span class="ln">2949 </span></a>
<a name="l2950"><span class="ln">2950 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2951"><span class="ln">2951 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">conj_physical</span><span class="s3">,</span>
<a name="l2952"><span class="ln">2952 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2953"><span class="ln">2953 </span></a>conj_physical(input, *, out=None) -&gt; Tensor 
<a name="l2954"><span class="ln">2954 </span></a> 
<a name="l2955"><span class="ln">2955 </span></a>Computes the element-wise conjugate of the given :attr:`input` tensor. 
<a name="l2956"><span class="ln">2956 </span></a>If :attr:`input` has a non-complex dtype, this function just returns :attr:`input`. 
<a name="l2957"><span class="ln">2957 </span></a> 
<a name="l2958"><span class="ln">2958 </span></a>.. note:: 
<a name="l2959"><span class="ln">2959 </span></a>   This performs the conjugate operation regardless of the fact conjugate bit is set or not. 
<a name="l2960"><span class="ln">2960 </span></a> 
<a name="l2961"><span class="ln">2961 </span></a>.. warning:: In the future, :func:`torch.conj_physical` may return a non-writeable view for an :attr:`input` of 
<a name="l2962"><span class="ln">2962 </span></a>             non-complex dtype. It's recommended that programs not modify the tensor returned by :func:`torch.conj_physical` 
<a name="l2963"><span class="ln">2963 </span></a>             when :attr:`input` is of non-complex dtype to be compatible with this change. 
<a name="l2964"><span class="ln">2964 </span></a> 
<a name="l2965"><span class="ln">2965 </span></a>.. math:: 
<a name="l2966"><span class="ln">2966 </span></a>    \text{out}_{i} = conj(\text{input}_{i}) 
<a name="l2967"><span class="ln">2967 </span></a>&quot;&quot;&quot;</span>
<a name="l2968"><span class="ln">2968 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l2969"><span class="ln">2969 </span></a>Args: 
<a name="l2970"><span class="ln">2970 </span></a>    {input} 
<a name="l2971"><span class="ln">2971 </span></a> 
<a name="l2972"><span class="ln">2972 </span></a>Keyword args: 
<a name="l2973"><span class="ln">2973 </span></a>    {out} 
<a name="l2974"><span class="ln">2974 </span></a> 
<a name="l2975"><span class="ln">2975 </span></a>Example:: 
<a name="l2976"><span class="ln">2976 </span></a> 
<a name="l2977"><span class="ln">2977 </span></a>    &gt;&gt;&gt; torch.conj_physical(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])) 
<a name="l2978"><span class="ln">2978 </span></a>    tensor([-1 - 1j, -2 - 2j, 3 + 3j]) 
<a name="l2979"><span class="ln">2979 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l2980"><span class="ln">2980 </span></a><span class="s3">)</span>
<a name="l2981"><span class="ln">2981 </span></a>
<a name="l2982"><span class="ln">2982 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l2983"><span class="ln">2983 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">conj</span><span class="s3">,</span>
<a name="l2984"><span class="ln">2984 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2985"><span class="ln">2985 </span></a>conj(input) -&gt; Tensor 
<a name="l2986"><span class="ln">2986 </span></a> 
<a name="l2987"><span class="ln">2987 </span></a>Returns a view of :attr:`input` with a flipped conjugate bit. If :attr:`input` has a non-complex dtype, 
<a name="l2988"><span class="ln">2988 </span></a>this function just returns :attr:`input`. 
<a name="l2989"><span class="ln">2989 </span></a> 
<a name="l2990"><span class="ln">2990 </span></a>.. note:: 
<a name="l2991"><span class="ln">2991 </span></a>    :func:`torch.conj` performs a lazy conjugation, but the actual conjugated tensor can be materialized 
<a name="l2992"><span class="ln">2992 </span></a>    at any time using :func:`torch.resolve_conj`. 
<a name="l2993"><span class="ln">2993 </span></a> 
<a name="l2994"><span class="ln">2994 </span></a>.. warning:: In the future, :func:`torch.conj` may return a non-writeable view for an :attr:`input` of 
<a name="l2995"><span class="ln">2995 </span></a>             non-complex dtype. It's recommended that programs not modify the tensor returned by :func:`torch.conj_physical` 
<a name="l2996"><span class="ln">2996 </span></a>             when :attr:`input` is of non-complex dtype to be compatible with this change. 
<a name="l2997"><span class="ln">2997 </span></a> 
<a name="l2998"><span class="ln">2998 </span></a>Args: 
<a name="l2999"><span class="ln">2999 </span></a>    {input} 
<a name="l3000"><span class="ln">3000 </span></a> 
<a name="l3001"><span class="ln">3001 </span></a>Example:: 
<a name="l3002"><span class="ln">3002 </span></a> 
<a name="l3003"><span class="ln">3003 </span></a>    &gt;&gt;&gt; x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]) 
<a name="l3004"><span class="ln">3004 </span></a>    &gt;&gt;&gt; x.is_conj() 
<a name="l3005"><span class="ln">3005 </span></a>    False 
<a name="l3006"><span class="ln">3006 </span></a>    &gt;&gt;&gt; y = torch.conj(x) 
<a name="l3007"><span class="ln">3007 </span></a>    &gt;&gt;&gt; y.is_conj() 
<a name="l3008"><span class="ln">3008 </span></a>    True 
<a name="l3009"><span class="ln">3009 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3010"><span class="ln">3010 </span></a><span class="s3">)</span>
<a name="l3011"><span class="ln">3011 </span></a>
<a name="l3012"><span class="ln">3012 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3013"><span class="ln">3013 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">resolve_conj</span><span class="s3">,</span>
<a name="l3014"><span class="ln">3014 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3015"><span class="ln">3015 </span></a>resolve_conj(input) -&gt; Tensor 
<a name="l3016"><span class="ln">3016 </span></a> 
<a name="l3017"><span class="ln">3017 </span></a>Returns a new tensor with materialized conjugation if :attr:`input`'s conjugate bit is set to `True`, 
<a name="l3018"><span class="ln">3018 </span></a>else returns :attr:`input`. The output tensor will always have its conjugate bit set to `False`. 
<a name="l3019"><span class="ln">3019 </span></a> 
<a name="l3020"><span class="ln">3020 </span></a>Args: 
<a name="l3021"><span class="ln">3021 </span></a>    {input} 
<a name="l3022"><span class="ln">3022 </span></a> 
<a name="l3023"><span class="ln">3023 </span></a>Example:: 
<a name="l3024"><span class="ln">3024 </span></a> 
<a name="l3025"><span class="ln">3025 </span></a>    &gt;&gt;&gt; x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]) 
<a name="l3026"><span class="ln">3026 </span></a>    &gt;&gt;&gt; y = x.conj() 
<a name="l3027"><span class="ln">3027 </span></a>    &gt;&gt;&gt; y.is_conj() 
<a name="l3028"><span class="ln">3028 </span></a>    True 
<a name="l3029"><span class="ln">3029 </span></a>    &gt;&gt;&gt; z = y.resolve_conj() 
<a name="l3030"><span class="ln">3030 </span></a>    &gt;&gt;&gt; z 
<a name="l3031"><span class="ln">3031 </span></a>    tensor([-1 - 1j, -2 - 2j, 3 + 3j]) 
<a name="l3032"><span class="ln">3032 </span></a>    &gt;&gt;&gt; z.is_conj() 
<a name="l3033"><span class="ln">3033 </span></a>    False 
<a name="l3034"><span class="ln">3034 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3035"><span class="ln">3035 </span></a><span class="s3">)</span>
<a name="l3036"><span class="ln">3036 </span></a>
<a name="l3037"><span class="ln">3037 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3038"><span class="ln">3038 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">resolve_neg</span><span class="s3">,</span>
<a name="l3039"><span class="ln">3039 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3040"><span class="ln">3040 </span></a>resolve_neg(input) -&gt; Tensor 
<a name="l3041"><span class="ln">3041 </span></a> 
<a name="l3042"><span class="ln">3042 </span></a>Returns a new tensor with materialized negation if :attr:`input`'s negative bit is set to `True`, 
<a name="l3043"><span class="ln">3043 </span></a>else returns :attr:`input`. The output tensor will always have its negative bit set to `False`. 
<a name="l3044"><span class="ln">3044 </span></a> 
<a name="l3045"><span class="ln">3045 </span></a>Args: 
<a name="l3046"><span class="ln">3046 </span></a>    {input} 
<a name="l3047"><span class="ln">3047 </span></a> 
<a name="l3048"><span class="ln">3048 </span></a>Example:: 
<a name="l3049"><span class="ln">3049 </span></a> 
<a name="l3050"><span class="ln">3050 </span></a>    &gt;&gt;&gt; x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]) 
<a name="l3051"><span class="ln">3051 </span></a>    &gt;&gt;&gt; y = x.conj() 
<a name="l3052"><span class="ln">3052 </span></a>    &gt;&gt;&gt; z = y.imag 
<a name="l3053"><span class="ln">3053 </span></a>    &gt;&gt;&gt; z.is_neg() 
<a name="l3054"><span class="ln">3054 </span></a>    True 
<a name="l3055"><span class="ln">3055 </span></a>    &gt;&gt;&gt; out = z.resolve_neg() 
<a name="l3056"><span class="ln">3056 </span></a>    &gt;&gt;&gt; out 
<a name="l3057"><span class="ln">3057 </span></a>    tensor([-1., -2., 3.]) 
<a name="l3058"><span class="ln">3058 </span></a>    &gt;&gt;&gt; out.is_neg() 
<a name="l3059"><span class="ln">3059 </span></a>    False 
<a name="l3060"><span class="ln">3060 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3061"><span class="ln">3061 </span></a><span class="s3">)</span>
<a name="l3062"><span class="ln">3062 </span></a>
<a name="l3063"><span class="ln">3063 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3064"><span class="ln">3064 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">copysign</span><span class="s3">,</span>
<a name="l3065"><span class="ln">3065 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3066"><span class="ln">3066 </span></a>copysign(input, other, *, out=None) -&gt; Tensor 
<a name="l3067"><span class="ln">3067 </span></a> 
<a name="l3068"><span class="ln">3068 </span></a>Create a new floating-point tensor with the magnitude of :attr:`input` and the sign of :attr:`other`, elementwise. 
<a name="l3069"><span class="ln">3069 </span></a> 
<a name="l3070"><span class="ln">3070 </span></a>.. math:: 
<a name="l3071"><span class="ln">3071 </span></a>    \text{out}_{i} = \begin{cases} 
<a name="l3072"><span class="ln">3072 </span></a>        -|\text{input}_{i}| &amp; \text{if } \text{other}_{i} \leq -0.0 \\ 
<a name="l3073"><span class="ln">3073 </span></a>         |\text{input}_{i}| &amp; \text{if } \text{other}_{i} \geq 0.0 \\ 
<a name="l3074"><span class="ln">3074 </span></a>    \end{cases} 
<a name="l3075"><span class="ln">3075 </span></a>&quot;&quot;&quot;</span>
<a name="l3076"><span class="ln">3076 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l3077"><span class="ln">3077 </span></a> 
<a name="l3078"><span class="ln">3078 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l3079"><span class="ln">3079 </span></a>and integer and float inputs. 
<a name="l3080"><span class="ln">3080 </span></a> 
<a name="l3081"><span class="ln">3081 </span></a>Args: 
<a name="l3082"><span class="ln">3082 </span></a>    input (Tensor): magnitudes. 
<a name="l3083"><span class="ln">3083 </span></a>    other (Tensor or Number): contains value(s) whose signbit(s) are 
<a name="l3084"><span class="ln">3084 </span></a>        applied to the magnitudes in :attr:`input`. 
<a name="l3085"><span class="ln">3085 </span></a> 
<a name="l3086"><span class="ln">3086 </span></a>Keyword args: 
<a name="l3087"><span class="ln">3087 </span></a>    {out} 
<a name="l3088"><span class="ln">3088 </span></a> 
<a name="l3089"><span class="ln">3089 </span></a>Example:: 
<a name="l3090"><span class="ln">3090 </span></a> 
<a name="l3091"><span class="ln">3091 </span></a>    &gt;&gt;&gt; a = torch.randn(5) 
<a name="l3092"><span class="ln">3092 </span></a>    &gt;&gt;&gt; a 
<a name="l3093"><span class="ln">3093 </span></a>    tensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244]) 
<a name="l3094"><span class="ln">3094 </span></a>    &gt;&gt;&gt; torch.copysign(a, 1) 
<a name="l3095"><span class="ln">3095 </span></a>    tensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244]) 
<a name="l3096"><span class="ln">3096 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l3097"><span class="ln">3097 </span></a>    &gt;&gt;&gt; a 
<a name="l3098"><span class="ln">3098 </span></a>    tensor([[ 0.7079,  0.2778, -1.0249,  0.5719], 
<a name="l3099"><span class="ln">3099 </span></a>            [-0.0059, -0.2600, -0.4475, -1.3948], 
<a name="l3100"><span class="ln">3100 </span></a>            [ 0.3667, -0.9567, -2.5757, -0.1751], 
<a name="l3101"><span class="ln">3101 </span></a>            [ 0.2046, -0.0742,  0.2998, -0.1054]]) 
<a name="l3102"><span class="ln">3102 </span></a>    &gt;&gt;&gt; b = torch.randn(4) 
<a name="l3103"><span class="ln">3103 </span></a>    tensor([ 0.2373,  0.3120,  0.3190, -1.1128]) 
<a name="l3104"><span class="ln">3104 </span></a>    &gt;&gt;&gt; torch.copysign(a, b) 
<a name="l3105"><span class="ln">3105 </span></a>    tensor([[ 0.7079,  0.2778,  1.0249, -0.5719], 
<a name="l3106"><span class="ln">3106 </span></a>            [ 0.0059,  0.2600,  0.4475, -1.3948], 
<a name="l3107"><span class="ln">3107 </span></a>            [ 0.3667,  0.9567,  2.5757, -0.1751], 
<a name="l3108"><span class="ln">3108 </span></a>            [ 0.2046,  0.0742,  0.2998, -0.1054]]) 
<a name="l3109"><span class="ln">3109 </span></a>    &gt;&gt;&gt; a = torch.tensor([1.]) 
<a name="l3110"><span class="ln">3110 </span></a>    &gt;&gt;&gt; b = torch.tensor([-0.]) 
<a name="l3111"><span class="ln">3111 </span></a>    &gt;&gt;&gt; torch.copysign(a, b) 
<a name="l3112"><span class="ln">3112 </span></a>    tensor([-1.]) 
<a name="l3113"><span class="ln">3113 </span></a> 
<a name="l3114"><span class="ln">3114 </span></a>.. note:: 
<a name="l3115"><span class="ln">3115 </span></a>    copysign handles signed zeros. If the other argument has a negative zero (-0), 
<a name="l3116"><span class="ln">3116 </span></a>    the corresponding output value will be negative. 
<a name="l3117"><span class="ln">3117 </span></a> 
<a name="l3118"><span class="ln">3118 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3119"><span class="ln">3119 </span></a><span class="s3">)</span>
<a name="l3120"><span class="ln">3120 </span></a>
<a name="l3121"><span class="ln">3121 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3122"><span class="ln">3122 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cos</span><span class="s3">,</span>
<a name="l3123"><span class="ln">3123 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3124"><span class="ln">3124 </span></a>cos(input, *, out=None) -&gt; Tensor 
<a name="l3125"><span class="ln">3125 </span></a> 
<a name="l3126"><span class="ln">3126 </span></a>Returns a new tensor with the cosine  of the elements of :attr:`input`. 
<a name="l3127"><span class="ln">3127 </span></a> 
<a name="l3128"><span class="ln">3128 </span></a>.. math:: 
<a name="l3129"><span class="ln">3129 </span></a>    \text{out}_{i} = \cos(\text{input}_{i}) 
<a name="l3130"><span class="ln">3130 </span></a>&quot;&quot;&quot;</span>
<a name="l3131"><span class="ln">3131 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l3132"><span class="ln">3132 </span></a>Args: 
<a name="l3133"><span class="ln">3133 </span></a>    {input} 
<a name="l3134"><span class="ln">3134 </span></a> 
<a name="l3135"><span class="ln">3135 </span></a>Keyword args: 
<a name="l3136"><span class="ln">3136 </span></a>    {out} 
<a name="l3137"><span class="ln">3137 </span></a> 
<a name="l3138"><span class="ln">3138 </span></a>Example:: 
<a name="l3139"><span class="ln">3139 </span></a> 
<a name="l3140"><span class="ln">3140 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l3141"><span class="ln">3141 </span></a>    &gt;&gt;&gt; a 
<a name="l3142"><span class="ln">3142 </span></a>    tensor([ 1.4309,  1.2706, -0.8562,  0.9796]) 
<a name="l3143"><span class="ln">3143 </span></a>    &gt;&gt;&gt; torch.cos(a) 
<a name="l3144"><span class="ln">3144 </span></a>    tensor([ 0.1395,  0.2957,  0.6553,  0.5574]) 
<a name="l3145"><span class="ln">3145 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3146"><span class="ln">3146 </span></a><span class="s3">)</span>
<a name="l3147"><span class="ln">3147 </span></a>
<a name="l3148"><span class="ln">3148 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3149"><span class="ln">3149 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cosh</span><span class="s3">,</span>
<a name="l3150"><span class="ln">3150 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3151"><span class="ln">3151 </span></a>cosh(input, *, out=None) -&gt; Tensor 
<a name="l3152"><span class="ln">3152 </span></a> 
<a name="l3153"><span class="ln">3153 </span></a>Returns a new tensor with the hyperbolic cosine  of the elements of 
<a name="l3154"><span class="ln">3154 </span></a>:attr:`input`. 
<a name="l3155"><span class="ln">3155 </span></a> 
<a name="l3156"><span class="ln">3156 </span></a>.. math:: 
<a name="l3157"><span class="ln">3157 </span></a>    \text{out}_{i} = \cosh(\text{input}_{i}) 
<a name="l3158"><span class="ln">3158 </span></a>&quot;&quot;&quot;</span>
<a name="l3159"><span class="ln">3159 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l3160"><span class="ln">3160 </span></a>Args: 
<a name="l3161"><span class="ln">3161 </span></a>    {input} 
<a name="l3162"><span class="ln">3162 </span></a> 
<a name="l3163"><span class="ln">3163 </span></a>Keyword args: 
<a name="l3164"><span class="ln">3164 </span></a>    {out} 
<a name="l3165"><span class="ln">3165 </span></a> 
<a name="l3166"><span class="ln">3166 </span></a>Example:: 
<a name="l3167"><span class="ln">3167 </span></a> 
<a name="l3168"><span class="ln">3168 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l3169"><span class="ln">3169 </span></a>    &gt;&gt;&gt; a 
<a name="l3170"><span class="ln">3170 </span></a>    tensor([ 0.1632,  1.1835, -0.6979, -0.7325]) 
<a name="l3171"><span class="ln">3171 </span></a>    &gt;&gt;&gt; torch.cosh(a) 
<a name="l3172"><span class="ln">3172 </span></a>    tensor([ 1.0133,  1.7860,  1.2536,  1.2805]) 
<a name="l3173"><span class="ln">3173 </span></a> 
<a name="l3174"><span class="ln">3174 </span></a>.. note:: 
<a name="l3175"><span class="ln">3175 </span></a>   When :attr:`input` is on the CPU, the implementation of torch.cosh may use 
<a name="l3176"><span class="ln">3176 </span></a>   the Sleef library, which rounds very large results to infinity or negative 
<a name="l3177"><span class="ln">3177 </span></a>   infinity. See `here &lt;https://sleef.org/purec.xhtml&gt;`_ for details. 
<a name="l3178"><span class="ln">3178 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3179"><span class="ln">3179 </span></a><span class="s3">)</span>
<a name="l3180"><span class="ln">3180 </span></a>
<a name="l3181"><span class="ln">3181 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3182"><span class="ln">3182 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cross</span><span class="s3">,</span>
<a name="l3183"><span class="ln">3183 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3184"><span class="ln">3184 </span></a>cross(input, other, dim=None, *, out=None) -&gt; Tensor 
<a name="l3185"><span class="ln">3185 </span></a> 
<a name="l3186"><span class="ln">3186 </span></a> 
<a name="l3187"><span class="ln">3187 </span></a>Returns the cross product of vectors in dimension :attr:`dim` of :attr:`input` 
<a name="l3188"><span class="ln">3188 </span></a>and :attr:`other`. 
<a name="l3189"><span class="ln">3189 </span></a> 
<a name="l3190"><span class="ln">3190 </span></a>Supports input of float, double, cfloat and cdouble dtypes. Also supports batches 
<a name="l3191"><span class="ln">3191 </span></a>of vectors, for which it computes the product along the dimension :attr:`dim`. 
<a name="l3192"><span class="ln">3192 </span></a>In this case, the output has the same batch dimensions as the inputs. 
<a name="l3193"><span class="ln">3193 </span></a> 
<a name="l3194"><span class="ln">3194 </span></a>.. warning:: 
<a name="l3195"><span class="ln">3195 </span></a>    If :attr:`dim` is not given, it defaults to the first dimension found 
<a name="l3196"><span class="ln">3196 </span></a>    with the size 3. Note that this might be unexpected. 
<a name="l3197"><span class="ln">3197 </span></a> 
<a name="l3198"><span class="ln">3198 </span></a>    This behavior is deprecated and will be changed to match that of :func:`torch.linalg.cross` 
<a name="l3199"><span class="ln">3199 </span></a>    in a future release. 
<a name="l3200"><span class="ln">3200 </span></a> 
<a name="l3201"><span class="ln">3201 </span></a>.. seealso:: 
<a name="l3202"><span class="ln">3202 </span></a>        :func:`torch.linalg.cross` which has dim=-1 as default. 
<a name="l3203"><span class="ln">3203 </span></a> 
<a name="l3204"><span class="ln">3204 </span></a> 
<a name="l3205"><span class="ln">3205 </span></a>Args: 
<a name="l3206"><span class="ln">3206 </span></a>    {input} 
<a name="l3207"><span class="ln">3207 </span></a>    other (Tensor): the second input tensor 
<a name="l3208"><span class="ln">3208 </span></a>    dim  (int, optional): the dimension to take the cross-product in. 
<a name="l3209"><span class="ln">3209 </span></a> 
<a name="l3210"><span class="ln">3210 </span></a>Keyword args: 
<a name="l3211"><span class="ln">3211 </span></a>    {out} 
<a name="l3212"><span class="ln">3212 </span></a> 
<a name="l3213"><span class="ln">3213 </span></a>Example:: 
<a name="l3214"><span class="ln">3214 </span></a> 
<a name="l3215"><span class="ln">3215 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 3) 
<a name="l3216"><span class="ln">3216 </span></a>    &gt;&gt;&gt; a 
<a name="l3217"><span class="ln">3217 </span></a>    tensor([[-0.3956,  1.1455,  1.6895], 
<a name="l3218"><span class="ln">3218 </span></a>            [-0.5849,  1.3672,  0.3599], 
<a name="l3219"><span class="ln">3219 </span></a>            [-1.1626,  0.7180, -0.0521], 
<a name="l3220"><span class="ln">3220 </span></a>            [-0.1339,  0.9902, -2.0225]]) 
<a name="l3221"><span class="ln">3221 </span></a>    &gt;&gt;&gt; b = torch.randn(4, 3) 
<a name="l3222"><span class="ln">3222 </span></a>    &gt;&gt;&gt; b 
<a name="l3223"><span class="ln">3223 </span></a>    tensor([[-0.0257, -1.4725, -1.2251], 
<a name="l3224"><span class="ln">3224 </span></a>            [-1.1479, -0.7005, -1.9757], 
<a name="l3225"><span class="ln">3225 </span></a>            [-1.3904,  0.3726, -1.1836], 
<a name="l3226"><span class="ln">3226 </span></a>            [-0.9688, -0.7153,  0.2159]]) 
<a name="l3227"><span class="ln">3227 </span></a>    &gt;&gt;&gt; torch.cross(a, b, dim=1) 
<a name="l3228"><span class="ln">3228 </span></a>    tensor([[ 1.0844, -0.5281,  0.6120], 
<a name="l3229"><span class="ln">3229 </span></a>            [-2.4490, -1.5687,  1.9792], 
<a name="l3230"><span class="ln">3230 </span></a>            [-0.8304, -1.3037,  0.5650], 
<a name="l3231"><span class="ln">3231 </span></a>            [-1.2329,  1.9883,  1.0551]]) 
<a name="l3232"><span class="ln">3232 </span></a>    &gt;&gt;&gt; torch.cross(a, b) 
<a name="l3233"><span class="ln">3233 </span></a>    tensor([[ 1.0844, -0.5281,  0.6120], 
<a name="l3234"><span class="ln">3234 </span></a>            [-2.4490, -1.5687,  1.9792], 
<a name="l3235"><span class="ln">3235 </span></a>            [-0.8304, -1.3037,  0.5650], 
<a name="l3236"><span class="ln">3236 </span></a>            [-1.2329,  1.9883,  1.0551]]) 
<a name="l3237"><span class="ln">3237 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3238"><span class="ln">3238 </span></a><span class="s3">)</span>
<a name="l3239"><span class="ln">3239 </span></a>
<a name="l3240"><span class="ln">3240 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3241"><span class="ln">3241 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logcumsumexp</span><span class="s3">,</span>
<a name="l3242"><span class="ln">3242 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3243"><span class="ln">3243 </span></a>logcumsumexp(input, dim, *, out=None) -&gt; Tensor 
<a name="l3244"><span class="ln">3244 </span></a>Returns the logarithm of the cumulative summation of the exponentiation of 
<a name="l3245"><span class="ln">3245 </span></a>elements of :attr:`input` in the dimension :attr:`dim`. 
<a name="l3246"><span class="ln">3246 </span></a> 
<a name="l3247"><span class="ln">3247 </span></a>For summation index :math:`j` given by `dim` and other indices :math:`i`, the result is 
<a name="l3248"><span class="ln">3248 </span></a> 
<a name="l3249"><span class="ln">3249 </span></a>    .. math:: 
<a name="l3250"><span class="ln">3250 </span></a>        \text{{logcumsumexp}}(x)_{{ij}} = \log \sum\limits_{{k=0}}^{{j}} \exp(x_{{ik}}) 
<a name="l3251"><span class="ln">3251 </span></a> 
<a name="l3252"><span class="ln">3252 </span></a>Args: 
<a name="l3253"><span class="ln">3253 </span></a>    {input} 
<a name="l3254"><span class="ln">3254 </span></a>    dim  (int): the dimension to do the operation over 
<a name="l3255"><span class="ln">3255 </span></a> 
<a name="l3256"><span class="ln">3256 </span></a>Keyword args: 
<a name="l3257"><span class="ln">3257 </span></a>    {out} 
<a name="l3258"><span class="ln">3258 </span></a> 
<a name="l3259"><span class="ln">3259 </span></a>Example:: 
<a name="l3260"><span class="ln">3260 </span></a> 
<a name="l3261"><span class="ln">3261 </span></a>    &gt;&gt;&gt; a = torch.randn(10) 
<a name="l3262"><span class="ln">3262 </span></a>    &gt;&gt;&gt; torch.logcumsumexp(a, dim=0) 
<a name="l3263"><span class="ln">3263 </span></a>    tensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811, 
<a name="l3264"><span class="ln">3264 </span></a>             1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475])) 
<a name="l3265"><span class="ln">3265 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reduceops_common_args</span><span class="s3">),</span>
<a name="l3266"><span class="ln">3266 </span></a><span class="s3">)</span>
<a name="l3267"><span class="ln">3267 </span></a>
<a name="l3268"><span class="ln">3268 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3269"><span class="ln">3269 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cummax</span><span class="s3">,</span>
<a name="l3270"><span class="ln">3270 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3271"><span class="ln">3271 </span></a>cummax(input, dim, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l3272"><span class="ln">3272 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative maximum of 
<a name="l3273"><span class="ln">3273 </span></a>elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index 
<a name="l3274"><span class="ln">3274 </span></a>location of each maximum value found in the dimension :attr:`dim`. 
<a name="l3275"><span class="ln">3275 </span></a> 
<a name="l3276"><span class="ln">3276 </span></a>.. math:: 
<a name="l3277"><span class="ln">3277 </span></a>    y_i = max(x_1, x_2, x_3, \dots, x_i) 
<a name="l3278"><span class="ln">3278 </span></a> 
<a name="l3279"><span class="ln">3279 </span></a>Args: 
<a name="l3280"><span class="ln">3280 </span></a>    {input} 
<a name="l3281"><span class="ln">3281 </span></a>    dim  (int): the dimension to do the operation over 
<a name="l3282"><span class="ln">3282 </span></a> 
<a name="l3283"><span class="ln">3283 </span></a>Keyword args: 
<a name="l3284"><span class="ln">3284 </span></a>    out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l3285"><span class="ln">3285 </span></a> 
<a name="l3286"><span class="ln">3286 </span></a>Example:: 
<a name="l3287"><span class="ln">3287 </span></a> 
<a name="l3288"><span class="ln">3288 </span></a>    &gt;&gt;&gt; a = torch.randn(10) 
<a name="l3289"><span class="ln">3289 </span></a>    &gt;&gt;&gt; a 
<a name="l3290"><span class="ln">3290 </span></a>    tensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284, 
<a name="l3291"><span class="ln">3291 </span></a>         1.9946, -0.8209]) 
<a name="l3292"><span class="ln">3292 </span></a>    &gt;&gt;&gt; torch.cummax(a, dim=0) 
<a name="l3293"><span class="ln">3293 </span></a>    torch.return_types.cummax( 
<a name="l3294"><span class="ln">3294 </span></a>        values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696, 
<a name="l3295"><span class="ln">3295 </span></a>         1.9946,  1.9946]), 
<a name="l3296"><span class="ln">3296 </span></a>        indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8])) 
<a name="l3297"><span class="ln">3297 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reduceops_common_args</span><span class="s3">),</span>
<a name="l3298"><span class="ln">3298 </span></a><span class="s3">)</span>
<a name="l3299"><span class="ln">3299 </span></a>
<a name="l3300"><span class="ln">3300 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3301"><span class="ln">3301 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cummin</span><span class="s3">,</span>
<a name="l3302"><span class="ln">3302 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3303"><span class="ln">3303 </span></a>cummin(input, dim, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l3304"><span class="ln">3304 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative minimum of 
<a name="l3305"><span class="ln">3305 </span></a>elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index 
<a name="l3306"><span class="ln">3306 </span></a>location of each maximum value found in the dimension :attr:`dim`. 
<a name="l3307"><span class="ln">3307 </span></a> 
<a name="l3308"><span class="ln">3308 </span></a>.. math:: 
<a name="l3309"><span class="ln">3309 </span></a>    y_i = min(x_1, x_2, x_3, \dots, x_i) 
<a name="l3310"><span class="ln">3310 </span></a> 
<a name="l3311"><span class="ln">3311 </span></a>Args: 
<a name="l3312"><span class="ln">3312 </span></a>    {input} 
<a name="l3313"><span class="ln">3313 </span></a>    dim  (int): the dimension to do the operation over 
<a name="l3314"><span class="ln">3314 </span></a> 
<a name="l3315"><span class="ln">3315 </span></a>Keyword args: 
<a name="l3316"><span class="ln">3316 </span></a>    out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l3317"><span class="ln">3317 </span></a> 
<a name="l3318"><span class="ln">3318 </span></a>Example:: 
<a name="l3319"><span class="ln">3319 </span></a> 
<a name="l3320"><span class="ln">3320 </span></a>    &gt;&gt;&gt; a = torch.randn(10) 
<a name="l3321"><span class="ln">3321 </span></a>    &gt;&gt;&gt; a 
<a name="l3322"><span class="ln">3322 </span></a>    tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762, 
<a name="l3323"><span class="ln">3323 </span></a>         0.9165,  1.6684]) 
<a name="l3324"><span class="ln">3324 </span></a>    &gt;&gt;&gt; torch.cummin(a, dim=0) 
<a name="l3325"><span class="ln">3325 </span></a>    torch.return_types.cummin( 
<a name="l3326"><span class="ln">3326 </span></a>        values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298, 
<a name="l3327"><span class="ln">3327 </span></a>        -1.3298, -1.3298]), 
<a name="l3328"><span class="ln">3328 </span></a>        indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4])) 
<a name="l3329"><span class="ln">3329 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reduceops_common_args</span><span class="s3">),</span>
<a name="l3330"><span class="ln">3330 </span></a><span class="s3">)</span>
<a name="l3331"><span class="ln">3331 </span></a>
<a name="l3332"><span class="ln">3332 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3333"><span class="ln">3333 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cumprod</span><span class="s3">,</span>
<a name="l3334"><span class="ln">3334 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3335"><span class="ln">3335 </span></a>cumprod(input, dim, *, dtype=None, out=None) -&gt; Tensor 
<a name="l3336"><span class="ln">3336 </span></a> 
<a name="l3337"><span class="ln">3337 </span></a>Returns the cumulative product of elements of :attr:`input` in the dimension 
<a name="l3338"><span class="ln">3338 </span></a>:attr:`dim`. 
<a name="l3339"><span class="ln">3339 </span></a> 
<a name="l3340"><span class="ln">3340 </span></a>For example, if :attr:`input` is a vector of size N, the result will also be 
<a name="l3341"><span class="ln">3341 </span></a>a vector of size N, with elements. 
<a name="l3342"><span class="ln">3342 </span></a> 
<a name="l3343"><span class="ln">3343 </span></a>.. math:: 
<a name="l3344"><span class="ln">3344 </span></a>    y_i = x_1 \times x_2\times x_3\times \dots \times x_i 
<a name="l3345"><span class="ln">3345 </span></a> 
<a name="l3346"><span class="ln">3346 </span></a>Args: 
<a name="l3347"><span class="ln">3347 </span></a>    {input} 
<a name="l3348"><span class="ln">3348 </span></a>    dim  (int): the dimension to do the operation over 
<a name="l3349"><span class="ln">3349 </span></a> 
<a name="l3350"><span class="ln">3350 </span></a>Keyword args: 
<a name="l3351"><span class="ln">3351 </span></a>    {dtype} 
<a name="l3352"><span class="ln">3352 </span></a>    {out} 
<a name="l3353"><span class="ln">3353 </span></a> 
<a name="l3354"><span class="ln">3354 </span></a>Example:: 
<a name="l3355"><span class="ln">3355 </span></a> 
<a name="l3356"><span class="ln">3356 </span></a>    &gt;&gt;&gt; a = torch.randn(10) 
<a name="l3357"><span class="ln">3357 </span></a>    &gt;&gt;&gt; a 
<a name="l3358"><span class="ln">3358 </span></a>    tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126, 
<a name="l3359"><span class="ln">3359 </span></a>            -0.2129, -0.4206,  0.1968]) 
<a name="l3360"><span class="ln">3360 </span></a>    &gt;&gt;&gt; torch.cumprod(a, dim=0) 
<a name="l3361"><span class="ln">3361 </span></a>    tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065, 
<a name="l3362"><span class="ln">3362 </span></a>             0.0014, -0.0006, -0.0001]) 
<a name="l3363"><span class="ln">3363 </span></a> 
<a name="l3364"><span class="ln">3364 </span></a>    &gt;&gt;&gt; a[5] = 0.0 
<a name="l3365"><span class="ln">3365 </span></a>    &gt;&gt;&gt; torch.cumprod(a, dim=0) 
<a name="l3366"><span class="ln">3366 </span></a>    tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000, 
<a name="l3367"><span class="ln">3367 </span></a>             0.0000, -0.0000, -0.0000]) 
<a name="l3368"><span class="ln">3368 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reduceops_common_args</span><span class="s3">),</span>
<a name="l3369"><span class="ln">3369 </span></a><span class="s3">)</span>
<a name="l3370"><span class="ln">3370 </span></a>
<a name="l3371"><span class="ln">3371 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3372"><span class="ln">3372 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cumsum</span><span class="s3">,</span>
<a name="l3373"><span class="ln">3373 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3374"><span class="ln">3374 </span></a>cumsum(input, dim, *, dtype=None, out=None) -&gt; Tensor 
<a name="l3375"><span class="ln">3375 </span></a> 
<a name="l3376"><span class="ln">3376 </span></a>Returns the cumulative sum of elements of :attr:`input` in the dimension 
<a name="l3377"><span class="ln">3377 </span></a>:attr:`dim`. 
<a name="l3378"><span class="ln">3378 </span></a> 
<a name="l3379"><span class="ln">3379 </span></a>For example, if :attr:`input` is a vector of size N, the result will also be 
<a name="l3380"><span class="ln">3380 </span></a>a vector of size N, with elements. 
<a name="l3381"><span class="ln">3381 </span></a> 
<a name="l3382"><span class="ln">3382 </span></a>.. math:: 
<a name="l3383"><span class="ln">3383 </span></a>    y_i = x_1 + x_2 + x_3 + \dots + x_i 
<a name="l3384"><span class="ln">3384 </span></a> 
<a name="l3385"><span class="ln">3385 </span></a>Args: 
<a name="l3386"><span class="ln">3386 </span></a>    {input} 
<a name="l3387"><span class="ln">3387 </span></a>    dim  (int): the dimension to do the operation over 
<a name="l3388"><span class="ln">3388 </span></a> 
<a name="l3389"><span class="ln">3389 </span></a>Keyword args: 
<a name="l3390"><span class="ln">3390 </span></a>    {dtype} 
<a name="l3391"><span class="ln">3391 </span></a>    {out} 
<a name="l3392"><span class="ln">3392 </span></a> 
<a name="l3393"><span class="ln">3393 </span></a>Example:: 
<a name="l3394"><span class="ln">3394 </span></a> 
<a name="l3395"><span class="ln">3395 </span></a>    &gt;&gt;&gt; a = torch.randint(1, 20, (10,)) 
<a name="l3396"><span class="ln">3396 </span></a>    &gt;&gt;&gt; a 
<a name="l3397"><span class="ln">3397 </span></a>    tensor([13,  7,  3, 10, 13,  3, 15, 10,  9, 10]) 
<a name="l3398"><span class="ln">3398 </span></a>    &gt;&gt;&gt; torch.cumsum(a, dim=0) 
<a name="l3399"><span class="ln">3399 </span></a>    tensor([13, 20, 23, 33, 46, 49, 64, 74, 83, 93]) 
<a name="l3400"><span class="ln">3400 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reduceops_common_args</span><span class="s3">),</span>
<a name="l3401"><span class="ln">3401 </span></a><span class="s3">)</span>
<a name="l3402"><span class="ln">3402 </span></a>
<a name="l3403"><span class="ln">3403 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3404"><span class="ln">3404 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">count_nonzero</span><span class="s3">,</span>
<a name="l3405"><span class="ln">3405 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3406"><span class="ln">3406 </span></a>count_nonzero(input, dim=None) -&gt; Tensor 
<a name="l3407"><span class="ln">3407 </span></a> 
<a name="l3408"><span class="ln">3408 </span></a>Counts the number of non-zero values in the tensor :attr:`input` along the given :attr:`dim`. 
<a name="l3409"><span class="ln">3409 </span></a>If no dim is specified then all non-zeros in the tensor are counted. 
<a name="l3410"><span class="ln">3410 </span></a> 
<a name="l3411"><span class="ln">3411 </span></a>Args: 
<a name="l3412"><span class="ln">3412 </span></a>    {input} 
<a name="l3413"><span class="ln">3413 </span></a>    dim (int or tuple of ints, optional): Dim or tuple of dims along which to count non-zeros. 
<a name="l3414"><span class="ln">3414 </span></a> 
<a name="l3415"><span class="ln">3415 </span></a>Example:: 
<a name="l3416"><span class="ln">3416 </span></a> 
<a name="l3417"><span class="ln">3417 </span></a>    &gt;&gt;&gt; x = torch.zeros(3,3) 
<a name="l3418"><span class="ln">3418 </span></a>    &gt;&gt;&gt; x[torch.randn(3,3) &gt; 0.5] = 1 
<a name="l3419"><span class="ln">3419 </span></a>    &gt;&gt;&gt; x 
<a name="l3420"><span class="ln">3420 </span></a>    tensor([[0., 1., 1.], 
<a name="l3421"><span class="ln">3421 </span></a>            [0., 0., 0.], 
<a name="l3422"><span class="ln">3422 </span></a>            [0., 0., 1.]]) 
<a name="l3423"><span class="ln">3423 </span></a>    &gt;&gt;&gt; torch.count_nonzero(x) 
<a name="l3424"><span class="ln">3424 </span></a>    tensor(3) 
<a name="l3425"><span class="ln">3425 </span></a>    &gt;&gt;&gt; torch.count_nonzero(x, dim=0) 
<a name="l3426"><span class="ln">3426 </span></a>    tensor([0, 1, 2]) 
<a name="l3427"><span class="ln">3427 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reduceops_common_args</span><span class="s3">),</span>
<a name="l3428"><span class="ln">3428 </span></a><span class="s3">)</span>
<a name="l3429"><span class="ln">3429 </span></a>
<a name="l3430"><span class="ln">3430 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3431"><span class="ln">3431 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">dequantize</span><span class="s3">,</span>
<a name="l3432"><span class="ln">3432 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3433"><span class="ln">3433 </span></a>dequantize(tensor) -&gt; Tensor 
<a name="l3434"><span class="ln">3434 </span></a> 
<a name="l3435"><span class="ln">3435 </span></a>Returns an fp32 Tensor by dequantizing a quantized Tensor 
<a name="l3436"><span class="ln">3436 </span></a> 
<a name="l3437"><span class="ln">3437 </span></a>Args: 
<a name="l3438"><span class="ln">3438 </span></a>    tensor (Tensor): A quantized Tensor 
<a name="l3439"><span class="ln">3439 </span></a> 
<a name="l3440"><span class="ln">3440 </span></a>.. function:: dequantize(tensors) -&gt; sequence of Tensors 
<a name="l3441"><span class="ln">3441 </span></a>   :noindex: 
<a name="l3442"><span class="ln">3442 </span></a> 
<a name="l3443"><span class="ln">3443 </span></a>Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors 
<a name="l3444"><span class="ln">3444 </span></a> 
<a name="l3445"><span class="ln">3445 </span></a>Args: 
<a name="l3446"><span class="ln">3446 </span></a>     tensors (sequence of Tensors): A list of quantized Tensors 
<a name="l3447"><span class="ln">3447 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3448"><span class="ln">3448 </span></a><span class="s3">)</span>
<a name="l3449"><span class="ln">3449 </span></a>
<a name="l3450"><span class="ln">3450 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3451"><span class="ln">3451 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">diag</span><span class="s3">,</span>
<a name="l3452"><span class="ln">3452 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3453"><span class="ln">3453 </span></a>diag(input, diagonal=0, *, out=None) -&gt; Tensor 
<a name="l3454"><span class="ln">3454 </span></a> 
<a name="l3455"><span class="ln">3455 </span></a>- If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor 
<a name="l3456"><span class="ln">3456 </span></a>  with the elements of :attr:`input` as the diagonal. 
<a name="l3457"><span class="ln">3457 </span></a>- If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with 
<a name="l3458"><span class="ln">3458 </span></a>  the diagonal elements of :attr:`input`. 
<a name="l3459"><span class="ln">3459 </span></a> 
<a name="l3460"><span class="ln">3460 </span></a>The argument :attr:`diagonal` controls which diagonal to consider: 
<a name="l3461"><span class="ln">3461 </span></a> 
<a name="l3462"><span class="ln">3462 </span></a>- If :attr:`diagonal` = 0, it is the main diagonal. 
<a name="l3463"><span class="ln">3463 </span></a>- If :attr:`diagonal` &gt; 0, it is above the main diagonal. 
<a name="l3464"><span class="ln">3464 </span></a>- If :attr:`diagonal` &lt; 0, it is below the main diagonal. 
<a name="l3465"><span class="ln">3465 </span></a> 
<a name="l3466"><span class="ln">3466 </span></a>Args: 
<a name="l3467"><span class="ln">3467 </span></a>    {input} 
<a name="l3468"><span class="ln">3468 </span></a>    diagonal (int, optional): the diagonal to consider 
<a name="l3469"><span class="ln">3469 </span></a> 
<a name="l3470"><span class="ln">3470 </span></a>Keyword args: 
<a name="l3471"><span class="ln">3471 </span></a>    {out} 
<a name="l3472"><span class="ln">3472 </span></a> 
<a name="l3473"><span class="ln">3473 </span></a>.. seealso:: 
<a name="l3474"><span class="ln">3474 </span></a> 
<a name="l3475"><span class="ln">3475 </span></a>        :func:`torch.diagonal` always returns the diagonal of its input. 
<a name="l3476"><span class="ln">3476 </span></a> 
<a name="l3477"><span class="ln">3477 </span></a>        :func:`torch.diagflat` always constructs a tensor with diagonal elements 
<a name="l3478"><span class="ln">3478 </span></a>        specified by the input. 
<a name="l3479"><span class="ln">3479 </span></a> 
<a name="l3480"><span class="ln">3480 </span></a>Examples: 
<a name="l3481"><span class="ln">3481 </span></a> 
<a name="l3482"><span class="ln">3482 </span></a>Get the square matrix where the input vector is the diagonal:: 
<a name="l3483"><span class="ln">3483 </span></a> 
<a name="l3484"><span class="ln">3484 </span></a>    &gt;&gt;&gt; a = torch.randn(3) 
<a name="l3485"><span class="ln">3485 </span></a>    &gt;&gt;&gt; a 
<a name="l3486"><span class="ln">3486 </span></a>    tensor([ 0.5950,-0.0872, 2.3298]) 
<a name="l3487"><span class="ln">3487 </span></a>    &gt;&gt;&gt; torch.diag(a) 
<a name="l3488"><span class="ln">3488 </span></a>    tensor([[ 0.5950, 0.0000, 0.0000], 
<a name="l3489"><span class="ln">3489 </span></a>            [ 0.0000,-0.0872, 0.0000], 
<a name="l3490"><span class="ln">3490 </span></a>            [ 0.0000, 0.0000, 2.3298]]) 
<a name="l3491"><span class="ln">3491 </span></a>    &gt;&gt;&gt; torch.diag(a, 1) 
<a name="l3492"><span class="ln">3492 </span></a>    tensor([[ 0.0000, 0.5950, 0.0000, 0.0000], 
<a name="l3493"><span class="ln">3493 </span></a>            [ 0.0000, 0.0000,-0.0872, 0.0000], 
<a name="l3494"><span class="ln">3494 </span></a>            [ 0.0000, 0.0000, 0.0000, 2.3298], 
<a name="l3495"><span class="ln">3495 </span></a>            [ 0.0000, 0.0000, 0.0000, 0.0000]]) 
<a name="l3496"><span class="ln">3496 </span></a> 
<a name="l3497"><span class="ln">3497 </span></a>Get the k-th diagonal of a given matrix:: 
<a name="l3498"><span class="ln">3498 </span></a> 
<a name="l3499"><span class="ln">3499 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l3500"><span class="ln">3500 </span></a>    &gt;&gt;&gt; a 
<a name="l3501"><span class="ln">3501 </span></a>    tensor([[-0.4264, 0.0255,-0.1064], 
<a name="l3502"><span class="ln">3502 </span></a>            [ 0.8795,-0.2429, 0.1374], 
<a name="l3503"><span class="ln">3503 </span></a>            [ 0.1029,-0.6482,-1.6300]]) 
<a name="l3504"><span class="ln">3504 </span></a>    &gt;&gt;&gt; torch.diag(a, 0) 
<a name="l3505"><span class="ln">3505 </span></a>    tensor([-0.4264,-0.2429,-1.6300]) 
<a name="l3506"><span class="ln">3506 </span></a>    &gt;&gt;&gt; torch.diag(a, 1) 
<a name="l3507"><span class="ln">3507 </span></a>    tensor([ 0.0255, 0.1374]) 
<a name="l3508"><span class="ln">3508 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3509"><span class="ln">3509 </span></a><span class="s3">)</span>
<a name="l3510"><span class="ln">3510 </span></a>
<a name="l3511"><span class="ln">3511 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3512"><span class="ln">3512 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">diag_embed</span><span class="s3">,</span>
<a name="l3513"><span class="ln">3513 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3514"><span class="ln">3514 </span></a>diag_embed(input, offset=0, dim1=-2, dim2=-1) -&gt; Tensor 
<a name="l3515"><span class="ln">3515 </span></a> 
<a name="l3516"><span class="ln">3516 </span></a>Creates a tensor whose diagonals of certain 2D planes (specified by 
<a name="l3517"><span class="ln">3517 </span></a>:attr:`dim1` and :attr:`dim2`) are filled by :attr:`input`. 
<a name="l3518"><span class="ln">3518 </span></a>To facilitate creating batched diagonal matrices, the 2D planes formed by 
<a name="l3519"><span class="ln">3519 </span></a>the last two dimensions of the returned tensor are chosen by default. 
<a name="l3520"><span class="ln">3520 </span></a> 
<a name="l3521"><span class="ln">3521 </span></a>The argument :attr:`offset` controls which diagonal to consider: 
<a name="l3522"><span class="ln">3522 </span></a> 
<a name="l3523"><span class="ln">3523 </span></a>- If :attr:`offset` = 0, it is the main diagonal. 
<a name="l3524"><span class="ln">3524 </span></a>- If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l3525"><span class="ln">3525 </span></a>- If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l3526"><span class="ln">3526 </span></a> 
<a name="l3527"><span class="ln">3527 </span></a>The size of the new matrix will be calculated to make the specified diagonal 
<a name="l3528"><span class="ln">3528 </span></a>of the size of the last input dimension. 
<a name="l3529"><span class="ln">3529 </span></a>Note that for :attr:`offset` other than :math:`0`, the order of :attr:`dim1` 
<a name="l3530"><span class="ln">3530 </span></a>and :attr:`dim2` matters. Exchanging them is equivalent to changing the 
<a name="l3531"><span class="ln">3531 </span></a>sign of :attr:`offset`. 
<a name="l3532"><span class="ln">3532 </span></a> 
<a name="l3533"><span class="ln">3533 </span></a>Applying :meth:`torch.diagonal` to the output of this function with 
<a name="l3534"><span class="ln">3534 </span></a>the same arguments yields a matrix identical to input. However, 
<a name="l3535"><span class="ln">3535 </span></a>:meth:`torch.diagonal` has different default dimensions, so those 
<a name="l3536"><span class="ln">3536 </span></a>need to be explicitly specified. 
<a name="l3537"><span class="ln">3537 </span></a> 
<a name="l3538"><span class="ln">3538 </span></a>Args: 
<a name="l3539"><span class="ln">3539 </span></a>    {input} Must be at least 1-dimensional. 
<a name="l3540"><span class="ln">3540 </span></a>    offset (int, optional): which diagonal to consider. Default: 0 
<a name="l3541"><span class="ln">3541 </span></a>        (main diagonal). 
<a name="l3542"><span class="ln">3542 </span></a>    dim1 (int, optional): first dimension with respect to which to 
<a name="l3543"><span class="ln">3543 </span></a>        take diagonal. Default: -2. 
<a name="l3544"><span class="ln">3544 </span></a>    dim2 (int, optional): second dimension with respect to which to 
<a name="l3545"><span class="ln">3545 </span></a>        take diagonal. Default: -1. 
<a name="l3546"><span class="ln">3546 </span></a> 
<a name="l3547"><span class="ln">3547 </span></a>Example:: 
<a name="l3548"><span class="ln">3548 </span></a> 
<a name="l3549"><span class="ln">3549 </span></a>    &gt;&gt;&gt; a = torch.randn(2, 3) 
<a name="l3550"><span class="ln">3550 </span></a>    &gt;&gt;&gt; torch.diag_embed(a) 
<a name="l3551"><span class="ln">3551 </span></a>    tensor([[[ 1.5410,  0.0000,  0.0000], 
<a name="l3552"><span class="ln">3552 </span></a>             [ 0.0000, -0.2934,  0.0000], 
<a name="l3553"><span class="ln">3553 </span></a>             [ 0.0000,  0.0000, -2.1788]], 
<a name="l3554"><span class="ln">3554 </span></a> 
<a name="l3555"><span class="ln">3555 </span></a>            [[ 0.5684,  0.0000,  0.0000], 
<a name="l3556"><span class="ln">3556 </span></a>             [ 0.0000, -1.0845,  0.0000], 
<a name="l3557"><span class="ln">3557 </span></a>             [ 0.0000,  0.0000, -1.3986]]]) 
<a name="l3558"><span class="ln">3558 </span></a> 
<a name="l3559"><span class="ln">3559 </span></a>    &gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2) 
<a name="l3560"><span class="ln">3560 </span></a>    tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000], 
<a name="l3561"><span class="ln">3561 </span></a>             [ 0.0000,  0.5684,  0.0000,  0.0000]], 
<a name="l3562"><span class="ln">3562 </span></a> 
<a name="l3563"><span class="ln">3563 </span></a>            [[ 0.0000,  0.0000, -0.2934,  0.0000], 
<a name="l3564"><span class="ln">3564 </span></a>             [ 0.0000,  0.0000, -1.0845,  0.0000]], 
<a name="l3565"><span class="ln">3565 </span></a> 
<a name="l3566"><span class="ln">3566 </span></a>            [[ 0.0000,  0.0000,  0.0000, -2.1788], 
<a name="l3567"><span class="ln">3567 </span></a>             [ 0.0000,  0.0000,  0.0000, -1.3986]], 
<a name="l3568"><span class="ln">3568 </span></a> 
<a name="l3569"><span class="ln">3569 </span></a>            [[ 0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l3570"><span class="ln">3570 </span></a>             [ 0.0000,  0.0000,  0.0000,  0.0000]]]) 
<a name="l3571"><span class="ln">3571 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3572"><span class="ln">3572 </span></a><span class="s3">)</span>
<a name="l3573"><span class="ln">3573 </span></a>
<a name="l3574"><span class="ln">3574 </span></a>
<a name="l3575"><span class="ln">3575 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3576"><span class="ln">3576 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">diagflat</span><span class="s3">,</span>
<a name="l3577"><span class="ln">3577 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3578"><span class="ln">3578 </span></a>diagflat(input, offset=0) -&gt; Tensor 
<a name="l3579"><span class="ln">3579 </span></a> 
<a name="l3580"><span class="ln">3580 </span></a>- If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor 
<a name="l3581"><span class="ln">3581 </span></a>  with the elements of :attr:`input` as the diagonal. 
<a name="l3582"><span class="ln">3582 </span></a>- If :attr:`input` is a tensor with more than one dimension, then returns a 
<a name="l3583"><span class="ln">3583 </span></a>  2-D tensor with diagonal elements equal to a flattened :attr:`input`. 
<a name="l3584"><span class="ln">3584 </span></a> 
<a name="l3585"><span class="ln">3585 </span></a>The argument :attr:`offset` controls which diagonal to consider: 
<a name="l3586"><span class="ln">3586 </span></a> 
<a name="l3587"><span class="ln">3587 </span></a>- If :attr:`offset` = 0, it is the main diagonal. 
<a name="l3588"><span class="ln">3588 </span></a>- If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l3589"><span class="ln">3589 </span></a>- If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l3590"><span class="ln">3590 </span></a> 
<a name="l3591"><span class="ln">3591 </span></a>Args: 
<a name="l3592"><span class="ln">3592 </span></a>    {input} 
<a name="l3593"><span class="ln">3593 </span></a>    offset (int, optional): the diagonal to consider. Default: 0 (main 
<a name="l3594"><span class="ln">3594 </span></a>        diagonal). 
<a name="l3595"><span class="ln">3595 </span></a> 
<a name="l3596"><span class="ln">3596 </span></a>Examples:: 
<a name="l3597"><span class="ln">3597 </span></a> 
<a name="l3598"><span class="ln">3598 </span></a>    &gt;&gt;&gt; a = torch.randn(3) 
<a name="l3599"><span class="ln">3599 </span></a>    &gt;&gt;&gt; a 
<a name="l3600"><span class="ln">3600 </span></a>    tensor([-0.2956, -0.9068,  0.1695]) 
<a name="l3601"><span class="ln">3601 </span></a>    &gt;&gt;&gt; torch.diagflat(a) 
<a name="l3602"><span class="ln">3602 </span></a>    tensor([[-0.2956,  0.0000,  0.0000], 
<a name="l3603"><span class="ln">3603 </span></a>            [ 0.0000, -0.9068,  0.0000], 
<a name="l3604"><span class="ln">3604 </span></a>            [ 0.0000,  0.0000,  0.1695]]) 
<a name="l3605"><span class="ln">3605 </span></a>    &gt;&gt;&gt; torch.diagflat(a, 1) 
<a name="l3606"><span class="ln">3606 </span></a>    tensor([[ 0.0000, -0.2956,  0.0000,  0.0000], 
<a name="l3607"><span class="ln">3607 </span></a>            [ 0.0000,  0.0000, -0.9068,  0.0000], 
<a name="l3608"><span class="ln">3608 </span></a>            [ 0.0000,  0.0000,  0.0000,  0.1695], 
<a name="l3609"><span class="ln">3609 </span></a>            [ 0.0000,  0.0000,  0.0000,  0.0000]]) 
<a name="l3610"><span class="ln">3610 </span></a> 
<a name="l3611"><span class="ln">3611 </span></a>    &gt;&gt;&gt; a = torch.randn(2, 2) 
<a name="l3612"><span class="ln">3612 </span></a>    &gt;&gt;&gt; a 
<a name="l3613"><span class="ln">3613 </span></a>    tensor([[ 0.2094, -0.3018], 
<a name="l3614"><span class="ln">3614 </span></a>            [-0.1516,  1.9342]]) 
<a name="l3615"><span class="ln">3615 </span></a>    &gt;&gt;&gt; torch.diagflat(a) 
<a name="l3616"><span class="ln">3616 </span></a>    tensor([[ 0.2094,  0.0000,  0.0000,  0.0000], 
<a name="l3617"><span class="ln">3617 </span></a>            [ 0.0000, -0.3018,  0.0000,  0.0000], 
<a name="l3618"><span class="ln">3618 </span></a>            [ 0.0000,  0.0000, -0.1516,  0.0000], 
<a name="l3619"><span class="ln">3619 </span></a>            [ 0.0000,  0.0000,  0.0000,  1.9342]]) 
<a name="l3620"><span class="ln">3620 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3621"><span class="ln">3621 </span></a><span class="s3">)</span>
<a name="l3622"><span class="ln">3622 </span></a>
<a name="l3623"><span class="ln">3623 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3624"><span class="ln">3624 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">diagonal</span><span class="s3">,</span>
<a name="l3625"><span class="ln">3625 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3626"><span class="ln">3626 </span></a>diagonal(input, offset=0, dim1=0, dim2=1) -&gt; Tensor 
<a name="l3627"><span class="ln">3627 </span></a> 
<a name="l3628"><span class="ln">3628 </span></a>Returns a partial view of :attr:`input` with the its diagonal elements 
<a name="l3629"><span class="ln">3629 </span></a>with respect to :attr:`dim1` and :attr:`dim2` appended as a dimension 
<a name="l3630"><span class="ln">3630 </span></a>at the end of the shape. 
<a name="l3631"><span class="ln">3631 </span></a> 
<a name="l3632"><span class="ln">3632 </span></a>The argument :attr:`offset` controls which diagonal to consider: 
<a name="l3633"><span class="ln">3633 </span></a> 
<a name="l3634"><span class="ln">3634 </span></a>- If :attr:`offset` = 0, it is the main diagonal. 
<a name="l3635"><span class="ln">3635 </span></a>- If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l3636"><span class="ln">3636 </span></a>- If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l3637"><span class="ln">3637 </span></a> 
<a name="l3638"><span class="ln">3638 </span></a>Applying :meth:`torch.diag_embed` to the output of this function with 
<a name="l3639"><span class="ln">3639 </span></a>the same arguments yields a diagonal matrix with the diagonal entries 
<a name="l3640"><span class="ln">3640 </span></a>of the input. However, :meth:`torch.diag_embed` has different default 
<a name="l3641"><span class="ln">3641 </span></a>dimensions, so those need to be explicitly specified. 
<a name="l3642"><span class="ln">3642 </span></a> 
<a name="l3643"><span class="ln">3643 </span></a>Args: 
<a name="l3644"><span class="ln">3644 </span></a>    {input} Must be at least 2-dimensional. 
<a name="l3645"><span class="ln">3645 </span></a>    offset (int, optional): which diagonal to consider. Default: 0 
<a name="l3646"><span class="ln">3646 </span></a>        (main diagonal). 
<a name="l3647"><span class="ln">3647 </span></a>    dim1 (int, optional): first dimension with respect to which to 
<a name="l3648"><span class="ln">3648 </span></a>        take diagonal. Default: 0. 
<a name="l3649"><span class="ln">3649 </span></a>    dim2 (int, optional): second dimension with respect to which to 
<a name="l3650"><span class="ln">3650 </span></a>        take diagonal. Default: 1. 
<a name="l3651"><span class="ln">3651 </span></a> 
<a name="l3652"><span class="ln">3652 </span></a>.. note::  To take a batch diagonal, pass in dim1=-2, dim2=-1. 
<a name="l3653"><span class="ln">3653 </span></a> 
<a name="l3654"><span class="ln">3654 </span></a>Examples:: 
<a name="l3655"><span class="ln">3655 </span></a> 
<a name="l3656"><span class="ln">3656 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l3657"><span class="ln">3657 </span></a>    &gt;&gt;&gt; a 
<a name="l3658"><span class="ln">3658 </span></a>    tensor([[-1.0854,  1.1431, -0.1752], 
<a name="l3659"><span class="ln">3659 </span></a>            [ 0.8536, -0.0905,  0.0360], 
<a name="l3660"><span class="ln">3660 </span></a>            [ 0.6927, -0.3735, -0.4945]]) 
<a name="l3661"><span class="ln">3661 </span></a> 
<a name="l3662"><span class="ln">3662 </span></a> 
<a name="l3663"><span class="ln">3663 </span></a>    &gt;&gt;&gt; torch.diagonal(a) 
<a name="l3664"><span class="ln">3664 </span></a>    tensor([-1.0854, -0.0905, -0.4945]) 
<a name="l3665"><span class="ln">3665 </span></a> 
<a name="l3666"><span class="ln">3666 </span></a> 
<a name="l3667"><span class="ln">3667 </span></a>    &gt;&gt;&gt; torch.diagonal(a, 1) 
<a name="l3668"><span class="ln">3668 </span></a>    tensor([ 1.1431,  0.0360]) 
<a name="l3669"><span class="ln">3669 </span></a> 
<a name="l3670"><span class="ln">3670 </span></a>    &gt;&gt;&gt; b = torch.randn(2, 5) 
<a name="l3671"><span class="ln">3671 </span></a>    &gt;&gt;&gt; b 
<a name="l3672"><span class="ln">3672 </span></a>    tensor([[-1.7948, -1.2731, -0.3181,  2.0200, -1.6745], 
<a name="l3673"><span class="ln">3673 </span></a>            [ 1.8262, -1.5049,  0.4114,  1.0704, -1.2607]]) 
<a name="l3674"><span class="ln">3674 </span></a> 
<a name="l3675"><span class="ln">3675 </span></a>    &gt;&gt;&gt; torch.diagonal(b, 1, 1, 0) 
<a name="l3676"><span class="ln">3676 </span></a>    tensor([1.8262]) 
<a name="l3677"><span class="ln">3677 </span></a> 
<a name="l3678"><span class="ln">3678 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 5, 4, 2) 
<a name="l3679"><span class="ln">3679 </span></a>    &gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2) 
<a name="l3680"><span class="ln">3680 </span></a>    tensor([[[-1.2631,  0.3755, -1.5977, -1.8172], 
<a name="l3681"><span class="ln">3681 </span></a>             [-1.1065,  1.0401, -0.2235, -0.7938]], 
<a name="l3682"><span class="ln">3682 </span></a> 
<a name="l3683"><span class="ln">3683 </span></a>            [[-1.7325, -0.3081,  0.6166,  0.2335], 
<a name="l3684"><span class="ln">3684 </span></a>             [ 1.0500,  0.7336, -0.3836, -1.1015]]]) 
<a name="l3685"><span class="ln">3685 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3686"><span class="ln">3686 </span></a><span class="s3">)</span>
<a name="l3687"><span class="ln">3687 </span></a>
<a name="l3688"><span class="ln">3688 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3689"><span class="ln">3689 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">diagonal_scatter</span><span class="s3">,</span>
<a name="l3690"><span class="ln">3690 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3691"><span class="ln">3691 </span></a>diagonal_scatter(input, src, offset=0, dim1=0, dim2=1) -&gt; Tensor 
<a name="l3692"><span class="ln">3692 </span></a> 
<a name="l3693"><span class="ln">3693 </span></a>Embeds the values of the :attr:`src` tensor into :attr:`input` along 
<a name="l3694"><span class="ln">3694 </span></a>the diagonal elements of :attr:`input`, with respect to :attr:`dim1` 
<a name="l3695"><span class="ln">3695 </span></a>and :attr:`dim2`. 
<a name="l3696"><span class="ln">3696 </span></a> 
<a name="l3697"><span class="ln">3697 </span></a>This function returns a tensor with fresh storage; it does not 
<a name="l3698"><span class="ln">3698 </span></a>return a view. 
<a name="l3699"><span class="ln">3699 </span></a> 
<a name="l3700"><span class="ln">3700 </span></a>The argument :attr:`offset` controls which diagonal to consider: 
<a name="l3701"><span class="ln">3701 </span></a> 
<a name="l3702"><span class="ln">3702 </span></a>- If :attr:`offset` = 0, it is the main diagonal. 
<a name="l3703"><span class="ln">3703 </span></a>- If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l3704"><span class="ln">3704 </span></a>- If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l3705"><span class="ln">3705 </span></a> 
<a name="l3706"><span class="ln">3706 </span></a>Args: 
<a name="l3707"><span class="ln">3707 </span></a>    {input} Must be at least 2-dimensional. 
<a name="l3708"><span class="ln">3708 </span></a>    src (Tensor): the tensor to embed into :attr:`input`. 
<a name="l3709"><span class="ln">3709 </span></a>    offset (int, optional): which diagonal to consider. Default: 0 
<a name="l3710"><span class="ln">3710 </span></a>        (main diagonal). 
<a name="l3711"><span class="ln">3711 </span></a>    dim1 (int, optional): first dimension with respect to which to 
<a name="l3712"><span class="ln">3712 </span></a>        take diagonal. Default: 0. 
<a name="l3713"><span class="ln">3713 </span></a>    dim2 (int, optional): second dimension with respect to which to 
<a name="l3714"><span class="ln">3714 </span></a>        take diagonal. Default: 1. 
<a name="l3715"><span class="ln">3715 </span></a> 
<a name="l3716"><span class="ln">3716 </span></a>.. note:: 
<a name="l3717"><span class="ln">3717 </span></a> 
<a name="l3718"><span class="ln">3718 </span></a>    :attr:`src` must be of the proper size in order to be embedded 
<a name="l3719"><span class="ln">3719 </span></a>    into :attr:`input`. Specifically, it should have the same shape as 
<a name="l3720"><span class="ln">3720 </span></a>    ``torch.diagonal(input, offset, dim1, dim2)`` 
<a name="l3721"><span class="ln">3721 </span></a> 
<a name="l3722"><span class="ln">3722 </span></a>Examples:: 
<a name="l3723"><span class="ln">3723 </span></a> 
<a name="l3724"><span class="ln">3724 </span></a>    &gt;&gt;&gt; a = torch.zeros(3, 3) 
<a name="l3725"><span class="ln">3725 </span></a>    &gt;&gt;&gt; a 
<a name="l3726"><span class="ln">3726 </span></a>    tensor([[0., 0., 0.], 
<a name="l3727"><span class="ln">3727 </span></a>            [0., 0., 0.], 
<a name="l3728"><span class="ln">3728 </span></a>            [0., 0., 0.]]) 
<a name="l3729"><span class="ln">3729 </span></a> 
<a name="l3730"><span class="ln">3730 </span></a>    &gt;&gt;&gt; torch.diagonal_scatter(a, torch.ones(3), 0) 
<a name="l3731"><span class="ln">3731 </span></a>    tensor([[1., 0., 0.], 
<a name="l3732"><span class="ln">3732 </span></a>            [0., 1., 0.], 
<a name="l3733"><span class="ln">3733 </span></a>            [0., 0., 1.]]) 
<a name="l3734"><span class="ln">3734 </span></a> 
<a name="l3735"><span class="ln">3735 </span></a>    &gt;&gt;&gt; torch.diagonal_scatter(a, torch.ones(2), 1) 
<a name="l3736"><span class="ln">3736 </span></a>    tensor([[0., 1., 0.], 
<a name="l3737"><span class="ln">3737 </span></a>            [0., 0., 1.], 
<a name="l3738"><span class="ln">3738 </span></a>            [0., 0., 0.]]) 
<a name="l3739"><span class="ln">3739 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3740"><span class="ln">3740 </span></a><span class="s3">)</span>
<a name="l3741"><span class="ln">3741 </span></a>
<a name="l3742"><span class="ln">3742 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3743"><span class="ln">3743 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">as_strided_scatter</span><span class="s3">,</span>
<a name="l3744"><span class="ln">3744 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3745"><span class="ln">3745 </span></a>as_strided_scatter(input, src, size, stride, storage_offset=None) -&gt; Tensor 
<a name="l3746"><span class="ln">3746 </span></a> 
<a name="l3747"><span class="ln">3747 </span></a>Embeds the values of the :attr:`src` tensor into :attr:`input` along 
<a name="l3748"><span class="ln">3748 </span></a>the elements corresponding to the result of calling 
<a name="l3749"><span class="ln">3749 </span></a>input.as_strided(size, stride, storage_offset). 
<a name="l3750"><span class="ln">3750 </span></a> 
<a name="l3751"><span class="ln">3751 </span></a>This function returns a tensor with fresh storage; it does not 
<a name="l3752"><span class="ln">3752 </span></a>return a view. 
<a name="l3753"><span class="ln">3753 </span></a> 
<a name="l3754"><span class="ln">3754 </span></a>Args: 
<a name="l3755"><span class="ln">3755 </span></a>    {input} 
<a name="l3756"><span class="ln">3756 </span></a>    size (tuple or ints): the shape of the output tensor 
<a name="l3757"><span class="ln">3757 </span></a>    stride (tuple or ints): the stride of the output tensor 
<a name="l3758"><span class="ln">3758 </span></a>    storage_offset (int, optional): the offset in the underlying storage of the output tensor 
<a name="l3759"><span class="ln">3759 </span></a> 
<a name="l3760"><span class="ln">3760 </span></a>.. note:: 
<a name="l3761"><span class="ln">3761 </span></a> 
<a name="l3762"><span class="ln">3762 </span></a>    :attr:`src` must be of the proper size in order to be embedded 
<a name="l3763"><span class="ln">3763 </span></a>    into :attr:`input`. Specifically, it should have the same shape as 
<a name="l3764"><span class="ln">3764 </span></a>    `torch.as_strided(input, size, stride, storage_offset)` 
<a name="l3765"><span class="ln">3765 </span></a> 
<a name="l3766"><span class="ln">3766 </span></a>Example:: 
<a name="l3767"><span class="ln">3767 </span></a> 
<a name="l3768"><span class="ln">3768 </span></a>    &gt;&gt;&gt; a = torch.arange(4).reshape(2, 2) + 1 
<a name="l3769"><span class="ln">3769 </span></a>    &gt;&gt;&gt; a 
<a name="l3770"><span class="ln">3770 </span></a>    tensor([[1, 2], 
<a name="l3771"><span class="ln">3771 </span></a>            [3, 4]]) 
<a name="l3772"><span class="ln">3772 </span></a>    &gt;&gt;&gt; b = torch.zeros(3, 3) 
<a name="l3773"><span class="ln">3773 </span></a>    &gt;&gt;&gt; b 
<a name="l3774"><span class="ln">3774 </span></a>    tensor([[0., 0., 0.], 
<a name="l3775"><span class="ln">3775 </span></a>            [0., 0., 0.], 
<a name="l3776"><span class="ln">3776 </span></a>            [0., 0., 0.]]) 
<a name="l3777"><span class="ln">3777 </span></a>    &gt;&gt;&gt; torch.as_strided_scatter(b, a, (2, 2), (1, 2)) 
<a name="l3778"><span class="ln">3778 </span></a>    tensor([[1., 3., 2.], 
<a name="l3779"><span class="ln">3779 </span></a>            [4., 0., 0.], 
<a name="l3780"><span class="ln">3780 </span></a>            [0., 0., 0.]]) 
<a name="l3781"><span class="ln">3781 </span></a> 
<a name="l3782"><span class="ln">3782 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3783"><span class="ln">3783 </span></a><span class="s3">)</span>
<a name="l3784"><span class="ln">3784 </span></a>
<a name="l3785"><span class="ln">3785 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3786"><span class="ln">3786 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">diff</span><span class="s3">,</span>
<a name="l3787"><span class="ln">3787 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3788"><span class="ln">3788 </span></a>diff(input, n=1, dim=-1, prepend=None, append=None) -&gt; Tensor 
<a name="l3789"><span class="ln">3789 </span></a> 
<a name="l3790"><span class="ln">3790 </span></a>Computes the n-th forward difference along the given dimension. 
<a name="l3791"><span class="ln">3791 </span></a> 
<a name="l3792"><span class="ln">3792 </span></a>The first-order differences are given by `out[i] = input[i + 1] - input[i]`. Higher-order 
<a name="l3793"><span class="ln">3793 </span></a>differences are calculated by using :func:`torch.diff` recursively. 
<a name="l3794"><span class="ln">3794 </span></a> 
<a name="l3795"><span class="ln">3795 </span></a>Args: 
<a name="l3796"><span class="ln">3796 </span></a>    input (Tensor): the tensor to compute the differences on 
<a name="l3797"><span class="ln">3797 </span></a>    n (int, optional): the number of times to recursively compute the difference 
<a name="l3798"><span class="ln">3798 </span></a>    dim (int, optional): the dimension to compute the difference along. 
<a name="l3799"><span class="ln">3799 </span></a>        Default is the last dimension. 
<a name="l3800"><span class="ln">3800 </span></a>    prepend, append (Tensor, optional): values to prepend or append to 
<a name="l3801"><span class="ln">3801 </span></a>        :attr:`input` along :attr:`dim` before computing the difference. 
<a name="l3802"><span class="ln">3802 </span></a>        Their dimensions must be equivalent to that of input, and their shapes 
<a name="l3803"><span class="ln">3803 </span></a>        must match input's shape except on :attr:`dim`. 
<a name="l3804"><span class="ln">3804 </span></a> 
<a name="l3805"><span class="ln">3805 </span></a>Keyword args: 
<a name="l3806"><span class="ln">3806 </span></a>    {out} 
<a name="l3807"><span class="ln">3807 </span></a> 
<a name="l3808"><span class="ln">3808 </span></a>Example:: 
<a name="l3809"><span class="ln">3809 </span></a> 
<a name="l3810"><span class="ln">3810 </span></a>    &gt;&gt;&gt; a = torch.tensor([1, 3, 2]) 
<a name="l3811"><span class="ln">3811 </span></a>    &gt;&gt;&gt; torch.diff(a) 
<a name="l3812"><span class="ln">3812 </span></a>    tensor([ 2, -1]) 
<a name="l3813"><span class="ln">3813 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 5]) 
<a name="l3814"><span class="ln">3814 </span></a>    &gt;&gt;&gt; torch.diff(a, append=b) 
<a name="l3815"><span class="ln">3815 </span></a>    tensor([ 2, -1,  2,  1]) 
<a name="l3816"><span class="ln">3816 </span></a>    &gt;&gt;&gt; c = torch.tensor([[1, 2, 3], [3, 4, 5]]) 
<a name="l3817"><span class="ln">3817 </span></a>    &gt;&gt;&gt; torch.diff(c, dim=0) 
<a name="l3818"><span class="ln">3818 </span></a>    tensor([[2, 2, 2]]) 
<a name="l3819"><span class="ln">3819 </span></a>    &gt;&gt;&gt; torch.diff(c, dim=1) 
<a name="l3820"><span class="ln">3820 </span></a>    tensor([[1, 1], 
<a name="l3821"><span class="ln">3821 </span></a>            [1, 1]]) 
<a name="l3822"><span class="ln">3822 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3823"><span class="ln">3823 </span></a><span class="s3">)</span>
<a name="l3824"><span class="ln">3824 </span></a>
<a name="l3825"><span class="ln">3825 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3826"><span class="ln">3826 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">digamma</span><span class="s3">,</span>
<a name="l3827"><span class="ln">3827 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3828"><span class="ln">3828 </span></a>digamma(input, *, out=None) -&gt; Tensor 
<a name="l3829"><span class="ln">3829 </span></a> 
<a name="l3830"><span class="ln">3830 </span></a>Alias for :func:`torch.special.digamma`. 
<a name="l3831"><span class="ln">3831 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3832"><span class="ln">3832 </span></a><span class="s3">)</span>
<a name="l3833"><span class="ln">3833 </span></a>
<a name="l3834"><span class="ln">3834 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3835"><span class="ln">3835 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">dist</span><span class="s3">,</span>
<a name="l3836"><span class="ln">3836 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3837"><span class="ln">3837 </span></a>dist(input, other, p=2) -&gt; Tensor 
<a name="l3838"><span class="ln">3838 </span></a> 
<a name="l3839"><span class="ln">3839 </span></a>Returns the p-norm of (:attr:`input` - :attr:`other`) 
<a name="l3840"><span class="ln">3840 </span></a> 
<a name="l3841"><span class="ln">3841 </span></a>The shapes of :attr:`input` and :attr:`other` must be 
<a name="l3842"><span class="ln">3842 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l3843"><span class="ln">3843 </span></a> 
<a name="l3844"><span class="ln">3844 </span></a>Args: 
<a name="l3845"><span class="ln">3845 </span></a>    {input} 
<a name="l3846"><span class="ln">3846 </span></a>    other (Tensor): the Right-hand-side input tensor 
<a name="l3847"><span class="ln">3847 </span></a>    p (float, optional): the norm to be computed 
<a name="l3848"><span class="ln">3848 </span></a> 
<a name="l3849"><span class="ln">3849 </span></a>Example:: 
<a name="l3850"><span class="ln">3850 </span></a> 
<a name="l3851"><span class="ln">3851 </span></a>    &gt;&gt;&gt; x = torch.randn(4) 
<a name="l3852"><span class="ln">3852 </span></a>    &gt;&gt;&gt; x 
<a name="l3853"><span class="ln">3853 </span></a>    tensor([-1.5393, -0.8675,  0.5916,  1.6321]) 
<a name="l3854"><span class="ln">3854 </span></a>    &gt;&gt;&gt; y = torch.randn(4) 
<a name="l3855"><span class="ln">3855 </span></a>    &gt;&gt;&gt; y 
<a name="l3856"><span class="ln">3856 </span></a>    tensor([ 0.0967, -1.0511,  0.6295,  0.8360]) 
<a name="l3857"><span class="ln">3857 </span></a>    &gt;&gt;&gt; torch.dist(x, y, 3.5) 
<a name="l3858"><span class="ln">3858 </span></a>    tensor(1.6727) 
<a name="l3859"><span class="ln">3859 </span></a>    &gt;&gt;&gt; torch.dist(x, y, 3) 
<a name="l3860"><span class="ln">3860 </span></a>    tensor(1.6973) 
<a name="l3861"><span class="ln">3861 </span></a>    &gt;&gt;&gt; torch.dist(x, y, 0) 
<a name="l3862"><span class="ln">3862 </span></a>    tensor(4.) 
<a name="l3863"><span class="ln">3863 </span></a>    &gt;&gt;&gt; torch.dist(x, y, 1) 
<a name="l3864"><span class="ln">3864 </span></a>    tensor(2.6537) 
<a name="l3865"><span class="ln">3865 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3866"><span class="ln">3866 </span></a><span class="s3">)</span>
<a name="l3867"><span class="ln">3867 </span></a>
<a name="l3868"><span class="ln">3868 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3869"><span class="ln">3869 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">div</span><span class="s3">,</span>
<a name="l3870"><span class="ln">3870 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3871"><span class="ln">3871 </span></a>div(input, other, *, rounding_mode=None, out=None) -&gt; Tensor 
<a name="l3872"><span class="ln">3872 </span></a> 
<a name="l3873"><span class="ln">3873 </span></a>Divides each element of the input ``input`` by the corresponding element of 
<a name="l3874"><span class="ln">3874 </span></a>:attr:`other`. 
<a name="l3875"><span class="ln">3875 </span></a> 
<a name="l3876"><span class="ln">3876 </span></a>.. math:: 
<a name="l3877"><span class="ln">3877 </span></a>    \text{{out}}_i = \frac{{\text{{input}}_i}}{{\text{{other}}_i}} 
<a name="l3878"><span class="ln">3878 </span></a> 
<a name="l3879"><span class="ln">3879 </span></a>.. note:: 
<a name="l3880"><span class="ln">3880 </span></a>    By default, this performs a &quot;true&quot; division like Python 3. 
<a name="l3881"><span class="ln">3881 </span></a>    See the :attr:`rounding_mode` argument for floor division. 
<a name="l3882"><span class="ln">3882 </span></a> 
<a name="l3883"><span class="ln">3883 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l3884"><span class="ln">3884 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l3885"><span class="ln">3885 </span></a>Always promotes integer types to the default scalar type. 
<a name="l3886"><span class="ln">3886 </span></a> 
<a name="l3887"><span class="ln">3887 </span></a>Args: 
<a name="l3888"><span class="ln">3888 </span></a>    input (Tensor): the dividend 
<a name="l3889"><span class="ln">3889 </span></a>    other (Tensor or Number): the divisor 
<a name="l3890"><span class="ln">3890 </span></a> 
<a name="l3891"><span class="ln">3891 </span></a>Keyword args: 
<a name="l3892"><span class="ln">3892 </span></a>    rounding_mode (str, optional): Type of rounding applied to the result: 
<a name="l3893"><span class="ln">3893 </span></a> 
<a name="l3894"><span class="ln">3894 </span></a>        * None - default behavior. Performs no rounding and, if both :attr:`input` and 
<a name="l3895"><span class="ln">3895 </span></a>          :attr:`other` are integer types, promotes the inputs to the default scalar type. 
<a name="l3896"><span class="ln">3896 </span></a>          Equivalent to true division in Python (the ``/`` operator) and NumPy's ``np.true_divide``. 
<a name="l3897"><span class="ln">3897 </span></a>        * ``&quot;trunc&quot;`` - rounds the results of the division towards zero. 
<a name="l3898"><span class="ln">3898 </span></a>          Equivalent to C-style integer division. 
<a name="l3899"><span class="ln">3899 </span></a>        * ``&quot;floor&quot;`` - rounds the results of the division down. 
<a name="l3900"><span class="ln">3900 </span></a>          Equivalent to floor division in Python (the ``//`` operator) and NumPy's ``np.floor_divide``. 
<a name="l3901"><span class="ln">3901 </span></a> 
<a name="l3902"><span class="ln">3902 </span></a>    {out} 
<a name="l3903"><span class="ln">3903 </span></a> 
<a name="l3904"><span class="ln">3904 </span></a>Examples:: 
<a name="l3905"><span class="ln">3905 </span></a> 
<a name="l3906"><span class="ln">3906 </span></a>    &gt;&gt;&gt; x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637]) 
<a name="l3907"><span class="ln">3907 </span></a>    &gt;&gt;&gt; torch.div(x, 0.5) 
<a name="l3908"><span class="ln">3908 </span></a>    tensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274]) 
<a name="l3909"><span class="ln">3909 </span></a> 
<a name="l3910"><span class="ln">3910 </span></a>    &gt;&gt;&gt; a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917], 
<a name="l3911"><span class="ln">3911 </span></a>    ...                   [ 0.1815, -1.0111,  0.9805, -1.5923], 
<a name="l3912"><span class="ln">3912 </span></a>    ...                   [ 0.1062,  1.4581,  0.7759, -1.2344], 
<a name="l3913"><span class="ln">3913 </span></a>    ...                   [-0.1830, -0.0313,  1.1908, -1.4757]]) 
<a name="l3914"><span class="ln">3914 </span></a>    &gt;&gt;&gt; b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308]) 
<a name="l3915"><span class="ln">3915 </span></a>    &gt;&gt;&gt; torch.div(a, b) 
<a name="l3916"><span class="ln">3916 </span></a>    tensor([[-0.4620, -6.6051,  0.5676,  1.2639], 
<a name="l3917"><span class="ln">3917 </span></a>            [ 0.2260, -3.4509, -1.2086,  6.8990], 
<a name="l3918"><span class="ln">3918 </span></a>            [ 0.1322,  4.9764, -0.9564,  5.3484], 
<a name="l3919"><span class="ln">3919 </span></a>            [-0.2278, -0.1068, -1.4678,  6.3938]]) 
<a name="l3920"><span class="ln">3920 </span></a> 
<a name="l3921"><span class="ln">3921 </span></a>    &gt;&gt;&gt; torch.div(a, b, rounding_mode='trunc') 
<a name="l3922"><span class="ln">3922 </span></a>    tensor([[-0., -6.,  0.,  1.], 
<a name="l3923"><span class="ln">3923 </span></a>            [ 0., -3., -1.,  6.], 
<a name="l3924"><span class="ln">3924 </span></a>            [ 0.,  4., -0.,  5.], 
<a name="l3925"><span class="ln">3925 </span></a>            [-0., -0., -1.,  6.]]) 
<a name="l3926"><span class="ln">3926 </span></a> 
<a name="l3927"><span class="ln">3927 </span></a>    &gt;&gt;&gt; torch.div(a, b, rounding_mode='floor') 
<a name="l3928"><span class="ln">3928 </span></a>    tensor([[-1., -7.,  0.,  1.], 
<a name="l3929"><span class="ln">3929 </span></a>            [ 0., -4., -2.,  6.], 
<a name="l3930"><span class="ln">3930 </span></a>            [ 0.,  4., -1.,  5.], 
<a name="l3931"><span class="ln">3931 </span></a>            [-1., -1., -2.,  6.]]) 
<a name="l3932"><span class="ln">3932 </span></a> 
<a name="l3933"><span class="ln">3933 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3934"><span class="ln">3934 </span></a><span class="s3">)</span>
<a name="l3935"><span class="ln">3935 </span></a>
<a name="l3936"><span class="ln">3936 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3937"><span class="ln">3937 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">divide</span><span class="s3">,</span>
<a name="l3938"><span class="ln">3938 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3939"><span class="ln">3939 </span></a>divide(input, other, *, rounding_mode=None, out=None) -&gt; Tensor 
<a name="l3940"><span class="ln">3940 </span></a> 
<a name="l3941"><span class="ln">3941 </span></a>Alias for :func:`torch.div`. 
<a name="l3942"><span class="ln">3942 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3943"><span class="ln">3943 </span></a><span class="s3">)</span>
<a name="l3944"><span class="ln">3944 </span></a>
<a name="l3945"><span class="ln">3945 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3946"><span class="ln">3946 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">dot</span><span class="s3">,</span>
<a name="l3947"><span class="ln">3947 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3948"><span class="ln">3948 </span></a>dot(input, tensor, *, out=None) -&gt; Tensor 
<a name="l3949"><span class="ln">3949 </span></a> 
<a name="l3950"><span class="ln">3950 </span></a>Computes the dot product of two 1D tensors. 
<a name="l3951"><span class="ln">3951 </span></a> 
<a name="l3952"><span class="ln">3952 </span></a>.. note:: 
<a name="l3953"><span class="ln">3953 </span></a> 
<a name="l3954"><span class="ln">3954 </span></a>    Unlike NumPy's dot, torch.dot intentionally only supports computing the dot product 
<a name="l3955"><span class="ln">3955 </span></a>    of two 1D tensors with the same number of elements. 
<a name="l3956"><span class="ln">3956 </span></a> 
<a name="l3957"><span class="ln">3957 </span></a>Args: 
<a name="l3958"><span class="ln">3958 </span></a>    input (Tensor): first tensor in the dot product, must be 1D. 
<a name="l3959"><span class="ln">3959 </span></a>    tensor (Tensor): second tensor in the dot product, must be 1D. 
<a name="l3960"><span class="ln">3960 </span></a> 
<a name="l3961"><span class="ln">3961 </span></a>Keyword args: 
<a name="l3962"><span class="ln">3962 </span></a>    {out} 
<a name="l3963"><span class="ln">3963 </span></a> 
<a name="l3964"><span class="ln">3964 </span></a>Example:: 
<a name="l3965"><span class="ln">3965 </span></a> 
<a name="l3966"><span class="ln">3966 </span></a>    &gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])) 
<a name="l3967"><span class="ln">3967 </span></a>    tensor(7) 
<a name="l3968"><span class="ln">3968 </span></a> 
<a name="l3969"><span class="ln">3969 </span></a>    &gt;&gt;&gt; t1, t2 = torch.tensor([0, 1]), torch.tensor([2, 3]) 
<a name="l3970"><span class="ln">3970 </span></a>    &gt;&gt;&gt; torch.dot(t1, t2) 
<a name="l3971"><span class="ln">3971 </span></a>    tensor(3) 
<a name="l3972"><span class="ln">3972 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l3973"><span class="ln">3973 </span></a><span class="s3">)</span>
<a name="l3974"><span class="ln">3974 </span></a>
<a name="l3975"><span class="ln">3975 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l3976"><span class="ln">3976 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">vdot</span><span class="s3">,</span>
<a name="l3977"><span class="ln">3977 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3978"><span class="ln">3978 </span></a>vdot(input, other, *, out=None) -&gt; Tensor 
<a name="l3979"><span class="ln">3979 </span></a> 
<a name="l3980"><span class="ln">3980 </span></a>Computes the dot product of two 1D vectors along a dimension. 
<a name="l3981"><span class="ln">3981 </span></a> 
<a name="l3982"><span class="ln">3982 </span></a>In symbols, this function computes 
<a name="l3983"><span class="ln">3983 </span></a> 
<a name="l3984"><span class="ln">3984 </span></a>.. math:: 
<a name="l3985"><span class="ln">3985 </span></a> 
<a name="l3986"><span class="ln">3986 </span></a>    \sum_{i=1}^n \overline{x_i}y_i. 
<a name="l3987"><span class="ln">3987 </span></a> 
<a name="l3988"><span class="ln">3988 </span></a>where :math:`\overline{x_i}` denotes the conjugate for complex 
<a name="l3989"><span class="ln">3989 </span></a>vectors, and it is the identity for real vectors. 
<a name="l3990"><span class="ln">3990 </span></a> 
<a name="l3991"><span class="ln">3991 </span></a>.. note:: 
<a name="l3992"><span class="ln">3992 </span></a> 
<a name="l3993"><span class="ln">3993 </span></a>    Unlike NumPy's vdot, torch.vdot intentionally only supports computing the dot product 
<a name="l3994"><span class="ln">3994 </span></a>    of two 1D tensors with the same number of elements. 
<a name="l3995"><span class="ln">3995 </span></a> 
<a name="l3996"><span class="ln">3996 </span></a>.. seealso:: 
<a name="l3997"><span class="ln">3997 </span></a> 
<a name="l3998"><span class="ln">3998 </span></a>        :func:`torch.linalg.vecdot` computes the dot product of two batches of vectors along a dimension. 
<a name="l3999"><span class="ln">3999 </span></a> 
<a name="l4000"><span class="ln">4000 </span></a>Args: 
<a name="l4001"><span class="ln">4001 </span></a>    input (Tensor): first tensor in the dot product, must be 1D. Its conjugate is used if it's complex. 
<a name="l4002"><span class="ln">4002 </span></a>    other (Tensor): second tensor in the dot product, must be 1D. 
<a name="l4003"><span class="ln">4003 </span></a> 
<a name="l4004"><span class="ln">4004 </span></a>Keyword args: 
<a name="l4005"><span class="ln">4005 </span></a>&quot;&quot;&quot;</span>
<a name="l4006"><span class="ln">4006 </span></a>    <span class="s2">+ </span><span class="s4">rf&quot;&quot;&quot;</span>
<a name="l4007"><span class="ln">4007 </span></a><span class="s4">.. note:: {</span><span class="s1">common_args</span><span class="s3">[</span><span class="s4">&quot;out&quot;</span><span class="s3">]</span><span class="s4">}</span>
<a name="l4008"><span class="ln">4008 </span></a><span class="s4">&quot;&quot;&quot;</span>
<a name="l4009"><span class="ln">4009 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l4010"><span class="ln">4010 </span></a> 
<a name="l4011"><span class="ln">4011 </span></a>Example:: 
<a name="l4012"><span class="ln">4012 </span></a> 
<a name="l4013"><span class="ln">4013 </span></a>    &gt;&gt;&gt; torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1])) 
<a name="l4014"><span class="ln">4014 </span></a>    tensor(7) 
<a name="l4015"><span class="ln">4015 </span></a>    &gt;&gt;&gt; a = torch.tensor((1 +2j, 3 - 1j)) 
<a name="l4016"><span class="ln">4016 </span></a>    &gt;&gt;&gt; b = torch.tensor((2 +1j, 4 - 0j)) 
<a name="l4017"><span class="ln">4017 </span></a>    &gt;&gt;&gt; torch.vdot(a, b) 
<a name="l4018"><span class="ln">4018 </span></a>    tensor([16.+1.j]) 
<a name="l4019"><span class="ln">4019 </span></a>    &gt;&gt;&gt; torch.vdot(b, a) 
<a name="l4020"><span class="ln">4020 </span></a>    tensor([16.-1.j]) 
<a name="l4021"><span class="ln">4021 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4022"><span class="ln">4022 </span></a><span class="s3">)</span>
<a name="l4023"><span class="ln">4023 </span></a>
<a name="l4024"><span class="ln">4024 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4025"><span class="ln">4025 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">eq</span><span class="s3">,</span>
<a name="l4026"><span class="ln">4026 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4027"><span class="ln">4027 </span></a>eq(input, other, *, out=None) -&gt; Tensor 
<a name="l4028"><span class="ln">4028 </span></a> 
<a name="l4029"><span class="ln">4029 </span></a>Computes element-wise equality 
<a name="l4030"><span class="ln">4030 </span></a> 
<a name="l4031"><span class="ln">4031 </span></a>The second argument can be a number or a tensor whose shape is 
<a name="l4032"><span class="ln">4032 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l4033"><span class="ln">4033 </span></a> 
<a name="l4034"><span class="ln">4034 </span></a>Args: 
<a name="l4035"><span class="ln">4035 </span></a>    input (Tensor): the tensor to compare 
<a name="l4036"><span class="ln">4036 </span></a>    other (Tensor or float): the tensor or value to compare 
<a name="l4037"><span class="ln">4037 </span></a> 
<a name="l4038"><span class="ln">4038 </span></a>Keyword args: 
<a name="l4039"><span class="ln">4039 </span></a>    {out} 
<a name="l4040"><span class="ln">4040 </span></a> 
<a name="l4041"><span class="ln">4041 </span></a>Returns: 
<a name="l4042"><span class="ln">4042 </span></a>    A boolean tensor that is True where :attr:`input` is equal to :attr:`other` and False elsewhere 
<a name="l4043"><span class="ln">4043 </span></a> 
<a name="l4044"><span class="ln">4044 </span></a>Example:: 
<a name="l4045"><span class="ln">4045 </span></a> 
<a name="l4046"><span class="ln">4046 </span></a>    &gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l4047"><span class="ln">4047 </span></a>    tensor([[ True, False], 
<a name="l4048"><span class="ln">4048 </span></a>            [False, True]]) 
<a name="l4049"><span class="ln">4049 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4050"><span class="ln">4050 </span></a><span class="s3">)</span>
<a name="l4051"><span class="ln">4051 </span></a>
<a name="l4052"><span class="ln">4052 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4053"><span class="ln">4053 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">equal</span><span class="s3">,</span>
<a name="l4054"><span class="ln">4054 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4055"><span class="ln">4055 </span></a>equal(input, other) -&gt; bool 
<a name="l4056"><span class="ln">4056 </span></a> 
<a name="l4057"><span class="ln">4057 </span></a>``True`` if two tensors have the same size and elements, ``False`` otherwise. 
<a name="l4058"><span class="ln">4058 </span></a> 
<a name="l4059"><span class="ln">4059 </span></a>.. note:: 
<a name="l4060"><span class="ln">4060 </span></a> 
<a name="l4061"><span class="ln">4061 </span></a>    Tensors containing NaNs are never equal to each other. Additionally, this function does not 
<a name="l4062"><span class="ln">4062 </span></a>    differentiate between the data types of the tensors during comparison. For more thorough tensor checks, 
<a name="l4063"><span class="ln">4063 </span></a>    use :meth:`torch.testing.assert_close`. 
<a name="l4064"><span class="ln">4064 </span></a> 
<a name="l4065"><span class="ln">4065 </span></a>Example:: 
<a name="l4066"><span class="ln">4066 </span></a> 
<a name="l4067"><span class="ln">4067 </span></a>    &gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2])) 
<a name="l4068"><span class="ln">4068 </span></a>    True 
<a name="l4069"><span class="ln">4069 </span></a>    &gt;&gt;&gt; torch.equal(torch.tensor([3, torch.nan]), torch.tensor([3, torch.nan])) 
<a name="l4070"><span class="ln">4070 </span></a>    False 
<a name="l4071"><span class="ln">4071 </span></a>    &gt;&gt;&gt; torch.equal(torch.tensor([1, 2, 3], dtype=torch.int32), torch.tensor([1, 2, 3], dtype=torch.float32)) 
<a name="l4072"><span class="ln">4072 </span></a>    True 
<a name="l4073"><span class="ln">4073 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4074"><span class="ln">4074 </span></a><span class="s3">)</span>
<a name="l4075"><span class="ln">4075 </span></a>
<a name="l4076"><span class="ln">4076 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4077"><span class="ln">4077 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">erf</span><span class="s3">,</span>
<a name="l4078"><span class="ln">4078 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4079"><span class="ln">4079 </span></a>erf(input, *, out=None) -&gt; Tensor 
<a name="l4080"><span class="ln">4080 </span></a> 
<a name="l4081"><span class="ln">4081 </span></a>Alias for :func:`torch.special.erf`. 
<a name="l4082"><span class="ln">4082 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4083"><span class="ln">4083 </span></a><span class="s3">)</span>
<a name="l4084"><span class="ln">4084 </span></a>
<a name="l4085"><span class="ln">4085 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4086"><span class="ln">4086 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">erfc</span><span class="s3">,</span>
<a name="l4087"><span class="ln">4087 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4088"><span class="ln">4088 </span></a>erfc(input, *, out=None) -&gt; Tensor 
<a name="l4089"><span class="ln">4089 </span></a> 
<a name="l4090"><span class="ln">4090 </span></a>Alias for :func:`torch.special.erfc`. 
<a name="l4091"><span class="ln">4091 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4092"><span class="ln">4092 </span></a><span class="s3">)</span>
<a name="l4093"><span class="ln">4093 </span></a>
<a name="l4094"><span class="ln">4094 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4095"><span class="ln">4095 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">erfinv</span><span class="s3">,</span>
<a name="l4096"><span class="ln">4096 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4097"><span class="ln">4097 </span></a>erfinv(input, *, out=None) -&gt; Tensor 
<a name="l4098"><span class="ln">4098 </span></a> 
<a name="l4099"><span class="ln">4099 </span></a>Alias for :func:`torch.special.erfinv`. 
<a name="l4100"><span class="ln">4100 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4101"><span class="ln">4101 </span></a><span class="s3">)</span>
<a name="l4102"><span class="ln">4102 </span></a>
<a name="l4103"><span class="ln">4103 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4104"><span class="ln">4104 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">exp</span><span class="s3">,</span>
<a name="l4105"><span class="ln">4105 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4106"><span class="ln">4106 </span></a>exp(input, *, out=None) -&gt; Tensor 
<a name="l4107"><span class="ln">4107 </span></a> 
<a name="l4108"><span class="ln">4108 </span></a>Returns a new tensor with the exponential of the elements 
<a name="l4109"><span class="ln">4109 </span></a>of the input tensor :attr:`input`. 
<a name="l4110"><span class="ln">4110 </span></a> 
<a name="l4111"><span class="ln">4111 </span></a>.. math:: 
<a name="l4112"><span class="ln">4112 </span></a>    y_{i} = e^{x_{i}} 
<a name="l4113"><span class="ln">4113 </span></a>&quot;&quot;&quot;</span>
<a name="l4114"><span class="ln">4114 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l4115"><span class="ln">4115 </span></a>Args: 
<a name="l4116"><span class="ln">4116 </span></a>    {input} 
<a name="l4117"><span class="ln">4117 </span></a> 
<a name="l4118"><span class="ln">4118 </span></a>Keyword args: 
<a name="l4119"><span class="ln">4119 </span></a>    {out} 
<a name="l4120"><span class="ln">4120 </span></a> 
<a name="l4121"><span class="ln">4121 </span></a>Example:: 
<a name="l4122"><span class="ln">4122 </span></a> 
<a name="l4123"><span class="ln">4123 </span></a>    &gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)])) 
<a name="l4124"><span class="ln">4124 </span></a>    tensor([ 1.,  2.]) 
<a name="l4125"><span class="ln">4125 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4126"><span class="ln">4126 </span></a><span class="s3">)</span>
<a name="l4127"><span class="ln">4127 </span></a>
<a name="l4128"><span class="ln">4128 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4129"><span class="ln">4129 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">exp2</span><span class="s3">,</span>
<a name="l4130"><span class="ln">4130 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4131"><span class="ln">4131 </span></a>exp2(input, *, out=None) -&gt; Tensor 
<a name="l4132"><span class="ln">4132 </span></a> 
<a name="l4133"><span class="ln">4133 </span></a>Alias for :func:`torch.special.exp2`. 
<a name="l4134"><span class="ln">4134 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4135"><span class="ln">4135 </span></a><span class="s3">)</span>
<a name="l4136"><span class="ln">4136 </span></a>
<a name="l4137"><span class="ln">4137 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4138"><span class="ln">4138 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">expm1</span><span class="s3">,</span>
<a name="l4139"><span class="ln">4139 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4140"><span class="ln">4140 </span></a>expm1(input, *, out=None) -&gt; Tensor 
<a name="l4141"><span class="ln">4141 </span></a> 
<a name="l4142"><span class="ln">4142 </span></a>Alias for :func:`torch.special.expm1`. 
<a name="l4143"><span class="ln">4143 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4144"><span class="ln">4144 </span></a><span class="s3">)</span>
<a name="l4145"><span class="ln">4145 </span></a>
<a name="l4146"><span class="ln">4146 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4147"><span class="ln">4147 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">eye</span><span class="s3">,</span>
<a name="l4148"><span class="ln">4148 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4149"><span class="ln">4149 </span></a>eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l4150"><span class="ln">4150 </span></a> 
<a name="l4151"><span class="ln">4151 </span></a>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. 
<a name="l4152"><span class="ln">4152 </span></a> 
<a name="l4153"><span class="ln">4153 </span></a>Args: 
<a name="l4154"><span class="ln">4154 </span></a>    n (int): the number of rows 
<a name="l4155"><span class="ln">4155 </span></a>    m (int, optional): the number of columns with default being :attr:`n` 
<a name="l4156"><span class="ln">4156 </span></a> 
<a name="l4157"><span class="ln">4157 </span></a>Keyword arguments: 
<a name="l4158"><span class="ln">4158 </span></a>    {out} 
<a name="l4159"><span class="ln">4159 </span></a>    {dtype} 
<a name="l4160"><span class="ln">4160 </span></a>    {layout} 
<a name="l4161"><span class="ln">4161 </span></a>    {device} 
<a name="l4162"><span class="ln">4162 </span></a>    {requires_grad} 
<a name="l4163"><span class="ln">4163 </span></a> 
<a name="l4164"><span class="ln">4164 </span></a>Returns: 
<a name="l4165"><span class="ln">4165 </span></a>    Tensor: A 2-D tensor with ones on the diagonal and zeros elsewhere 
<a name="l4166"><span class="ln">4166 </span></a> 
<a name="l4167"><span class="ln">4167 </span></a>Example:: 
<a name="l4168"><span class="ln">4168 </span></a> 
<a name="l4169"><span class="ln">4169 </span></a>    &gt;&gt;&gt; torch.eye(3) 
<a name="l4170"><span class="ln">4170 </span></a>    tensor([[ 1.,  0.,  0.], 
<a name="l4171"><span class="ln">4171 </span></a>            [ 0.,  1.,  0.], 
<a name="l4172"><span class="ln">4172 </span></a>            [ 0.,  0.,  1.]]) 
<a name="l4173"><span class="ln">4173 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l4174"><span class="ln">4174 </span></a><span class="s3">)</span>
<a name="l4175"><span class="ln">4175 </span></a>
<a name="l4176"><span class="ln">4176 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4177"><span class="ln">4177 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">floor</span><span class="s3">,</span>
<a name="l4178"><span class="ln">4178 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4179"><span class="ln">4179 </span></a>floor(input, *, out=None) -&gt; Tensor 
<a name="l4180"><span class="ln">4180 </span></a> 
<a name="l4181"><span class="ln">4181 </span></a>Returns a new tensor with the floor of the elements of :attr:`input`, 
<a name="l4182"><span class="ln">4182 </span></a>the largest integer less than or equal to each element. 
<a name="l4183"><span class="ln">4183 </span></a> 
<a name="l4184"><span class="ln">4184 </span></a>For integer inputs, follows the array-api convention of returning a 
<a name="l4185"><span class="ln">4185 </span></a>copy of the input tensor. 
<a name="l4186"><span class="ln">4186 </span></a> 
<a name="l4187"><span class="ln">4187 </span></a>.. math:: 
<a name="l4188"><span class="ln">4188 </span></a>    \text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor 
<a name="l4189"><span class="ln">4189 </span></a>&quot;&quot;&quot;</span>
<a name="l4190"><span class="ln">4190 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l4191"><span class="ln">4191 </span></a>Args: 
<a name="l4192"><span class="ln">4192 </span></a>    {input} 
<a name="l4193"><span class="ln">4193 </span></a> 
<a name="l4194"><span class="ln">4194 </span></a>Keyword args: 
<a name="l4195"><span class="ln">4195 </span></a>    {out} 
<a name="l4196"><span class="ln">4196 </span></a> 
<a name="l4197"><span class="ln">4197 </span></a>Example:: 
<a name="l4198"><span class="ln">4198 </span></a> 
<a name="l4199"><span class="ln">4199 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l4200"><span class="ln">4200 </span></a>    &gt;&gt;&gt; a 
<a name="l4201"><span class="ln">4201 </span></a>    tensor([-0.8166,  1.5308, -0.2530, -0.2091]) 
<a name="l4202"><span class="ln">4202 </span></a>    &gt;&gt;&gt; torch.floor(a) 
<a name="l4203"><span class="ln">4203 </span></a>    tensor([-1.,  1., -1., -1.]) 
<a name="l4204"><span class="ln">4204 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4205"><span class="ln">4205 </span></a><span class="s3">)</span>
<a name="l4206"><span class="ln">4206 </span></a>
<a name="l4207"><span class="ln">4207 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4208"><span class="ln">4208 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">floor_divide</span><span class="s3">,</span>
<a name="l4209"><span class="ln">4209 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4210"><span class="ln">4210 </span></a>floor_divide(input, other, *, out=None) -&gt; Tensor 
<a name="l4211"><span class="ln">4211 </span></a> 
<a name="l4212"><span class="ln">4212 </span></a>.. note:: 
<a name="l4213"><span class="ln">4213 </span></a> 
<a name="l4214"><span class="ln">4214 </span></a>    Before PyTorch 1.13 :func:`torch.floor_divide` incorrectly performed 
<a name="l4215"><span class="ln">4215 </span></a>    truncation division. To restore the previous behavior use 
<a name="l4216"><span class="ln">4216 </span></a>    :func:`torch.div` with ``rounding_mode='trunc'``. 
<a name="l4217"><span class="ln">4217 </span></a> 
<a name="l4218"><span class="ln">4218 </span></a>Computes :attr:`input` divided by :attr:`other`, elementwise, and floors 
<a name="l4219"><span class="ln">4219 </span></a>the result. 
<a name="l4220"><span class="ln">4220 </span></a> 
<a name="l4221"><span class="ln">4221 </span></a>.. math:: 
<a name="l4222"><span class="ln">4222 </span></a>    \text{{out}}_i = \text{floor} \left( \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right) 
<a name="l4223"><span class="ln">4223 </span></a> 
<a name="l4224"><span class="ln">4224 </span></a>&quot;&quot;&quot;</span>
<a name="l4225"><span class="ln">4225 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l4226"><span class="ln">4226 </span></a> 
<a name="l4227"><span class="ln">4227 </span></a>Supports broadcasting to a common shape, type promotion, and integer and float inputs. 
<a name="l4228"><span class="ln">4228 </span></a> 
<a name="l4229"><span class="ln">4229 </span></a>Args: 
<a name="l4230"><span class="ln">4230 </span></a>    input (Tensor or Number): the dividend 
<a name="l4231"><span class="ln">4231 </span></a>    other (Tensor or Number): the divisor 
<a name="l4232"><span class="ln">4232 </span></a> 
<a name="l4233"><span class="ln">4233 </span></a>Keyword args: 
<a name="l4234"><span class="ln">4234 </span></a>    {out} 
<a name="l4235"><span class="ln">4235 </span></a> 
<a name="l4236"><span class="ln">4236 </span></a>Example:: 
<a name="l4237"><span class="ln">4237 </span></a> 
<a name="l4238"><span class="ln">4238 </span></a>    &gt;&gt;&gt; a = torch.tensor([4.0, 3.0]) 
<a name="l4239"><span class="ln">4239 </span></a>    &gt;&gt;&gt; b = torch.tensor([2.0, 2.0]) 
<a name="l4240"><span class="ln">4240 </span></a>    &gt;&gt;&gt; torch.floor_divide(a, b) 
<a name="l4241"><span class="ln">4241 </span></a>    tensor([2.0, 1.0]) 
<a name="l4242"><span class="ln">4242 </span></a>    &gt;&gt;&gt; torch.floor_divide(a, 1.4) 
<a name="l4243"><span class="ln">4243 </span></a>    tensor([2.0, 2.0]) 
<a name="l4244"><span class="ln">4244 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4245"><span class="ln">4245 </span></a><span class="s3">)</span>
<a name="l4246"><span class="ln">4246 </span></a>
<a name="l4247"><span class="ln">4247 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4248"><span class="ln">4248 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">fmod</span><span class="s3">,</span>
<a name="l4249"><span class="ln">4249 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4250"><span class="ln">4250 </span></a>fmod(input, other, *, out=None) -&gt; Tensor 
<a name="l4251"><span class="ln">4251 </span></a> 
<a name="l4252"><span class="ln">4252 </span></a>Applies C++'s `std::fmod &lt;https://en.cppreference.com/w/cpp/numeric/math/fmod&gt;`_ entrywise. 
<a name="l4253"><span class="ln">4253 </span></a>The result has the same sign as the dividend :attr:`input` and its absolute value 
<a name="l4254"><span class="ln">4254 </span></a>is less than that of :attr:`other`. 
<a name="l4255"><span class="ln">4255 </span></a> 
<a name="l4256"><span class="ln">4256 </span></a>This function may be defined in terms of :func:`torch.div` as 
<a name="l4257"><span class="ln">4257 </span></a> 
<a name="l4258"><span class="ln">4258 </span></a>.. code:: python 
<a name="l4259"><span class="ln">4259 </span></a> 
<a name="l4260"><span class="ln">4260 </span></a>    torch.fmod(a, b) == a - a.div(b, rounding_mode=&quot;trunc&quot;) * b 
<a name="l4261"><span class="ln">4261 </span></a> 
<a name="l4262"><span class="ln">4262 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l4263"><span class="ln">4263 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and float inputs. 
<a name="l4264"><span class="ln">4264 </span></a> 
<a name="l4265"><span class="ln">4265 </span></a>.. note:: 
<a name="l4266"><span class="ln">4266 </span></a> 
<a name="l4267"><span class="ln">4267 </span></a>    When the divisor is zero, returns ``NaN`` for floating point dtypes 
<a name="l4268"><span class="ln">4268 </span></a>    on both CPU and GPU; raises ``RuntimeError`` for integer division by 
<a name="l4269"><span class="ln">4269 </span></a>    zero on CPU; Integer division by zero on GPU may return any value. 
<a name="l4270"><span class="ln">4270 </span></a> 
<a name="l4271"><span class="ln">4271 </span></a>.. note:: 
<a name="l4272"><span class="ln">4272 </span></a> 
<a name="l4273"><span class="ln">4273 </span></a>   Complex inputs are not supported. In some cases, it is not mathematically 
<a name="l4274"><span class="ln">4274 </span></a>   possible to satisfy the definition of a modulo operation with complex numbers. 
<a name="l4275"><span class="ln">4275 </span></a> 
<a name="l4276"><span class="ln">4276 </span></a>.. seealso:: 
<a name="l4277"><span class="ln">4277 </span></a> 
<a name="l4278"><span class="ln">4278 </span></a>    :func:`torch.remainder` which implements Python's modulus operator. 
<a name="l4279"><span class="ln">4279 </span></a>    This one is defined using division rounding down the result. 
<a name="l4280"><span class="ln">4280 </span></a> 
<a name="l4281"><span class="ln">4281 </span></a>Args: 
<a name="l4282"><span class="ln">4282 </span></a>    input (Tensor): the dividend 
<a name="l4283"><span class="ln">4283 </span></a>    other (Tensor or Scalar): the divisor 
<a name="l4284"><span class="ln">4284 </span></a> 
<a name="l4285"><span class="ln">4285 </span></a>Keyword args: 
<a name="l4286"><span class="ln">4286 </span></a>    {out} 
<a name="l4287"><span class="ln">4287 </span></a> 
<a name="l4288"><span class="ln">4288 </span></a>Example:: 
<a name="l4289"><span class="ln">4289 </span></a> 
<a name="l4290"><span class="ln">4290 </span></a>    &gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) 
<a name="l4291"><span class="ln">4291 </span></a>    tensor([-1., -0., -1.,  1.,  0.,  1.]) 
<a name="l4292"><span class="ln">4292 </span></a>    &gt;&gt;&gt; torch.fmod(torch.tensor([1, 2, 3, 4, 5]), -1.5) 
<a name="l4293"><span class="ln">4293 </span></a>    tensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000]) 
<a name="l4294"><span class="ln">4294 </span></a> 
<a name="l4295"><span class="ln">4295 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4296"><span class="ln">4296 </span></a><span class="s3">)</span>
<a name="l4297"><span class="ln">4297 </span></a>
<a name="l4298"><span class="ln">4298 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4299"><span class="ln">4299 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">frac</span><span class="s3">,</span>
<a name="l4300"><span class="ln">4300 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4301"><span class="ln">4301 </span></a>frac(input, *, out=None) -&gt; Tensor 
<a name="l4302"><span class="ln">4302 </span></a> 
<a name="l4303"><span class="ln">4303 </span></a>Computes the fractional portion of each element in :attr:`input`. 
<a name="l4304"><span class="ln">4304 </span></a> 
<a name="l4305"><span class="ln">4305 </span></a>.. math:: 
<a name="l4306"><span class="ln">4306 </span></a>    \text{out}_{i} = \text{input}_{i} - \left\lfloor |\text{input}_{i}| \right\rfloor * \operatorname{sgn}(\text{input}_{i}) 
<a name="l4307"><span class="ln">4307 </span></a> 
<a name="l4308"><span class="ln">4308 </span></a>Example:: 
<a name="l4309"><span class="ln">4309 </span></a> 
<a name="l4310"><span class="ln">4310 </span></a>    &gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2])) 
<a name="l4311"><span class="ln">4311 </span></a>    tensor([ 0.0000,  0.5000, -0.2000]) 
<a name="l4312"><span class="ln">4312 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4313"><span class="ln">4313 </span></a><span class="s3">)</span>
<a name="l4314"><span class="ln">4314 </span></a>
<a name="l4315"><span class="ln">4315 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4316"><span class="ln">4316 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">frexp</span><span class="s3">,</span>
<a name="l4317"><span class="ln">4317 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4318"><span class="ln">4318 </span></a>frexp(input, *, out=None) -&gt; (Tensor mantissa, Tensor exponent) 
<a name="l4319"><span class="ln">4319 </span></a> 
<a name="l4320"><span class="ln">4320 </span></a>Decomposes :attr:`input` into mantissa and exponent tensors 
<a name="l4321"><span class="ln">4321 </span></a>such that :math:`\text{input} = \text{mantissa} \times 2^{\text{exponent}}`. 
<a name="l4322"><span class="ln">4322 </span></a> 
<a name="l4323"><span class="ln">4323 </span></a>The range of mantissa is the open interval (-1, 1). 
<a name="l4324"><span class="ln">4324 </span></a> 
<a name="l4325"><span class="ln">4325 </span></a>Supports float inputs. 
<a name="l4326"><span class="ln">4326 </span></a> 
<a name="l4327"><span class="ln">4327 </span></a>Args: 
<a name="l4328"><span class="ln">4328 </span></a>    input (Tensor): the input tensor 
<a name="l4329"><span class="ln">4329 </span></a> 
<a name="l4330"><span class="ln">4330 </span></a> 
<a name="l4331"><span class="ln">4331 </span></a>Keyword args: 
<a name="l4332"><span class="ln">4332 </span></a>    out (tuple, optional): the output tensors 
<a name="l4333"><span class="ln">4333 </span></a> 
<a name="l4334"><span class="ln">4334 </span></a>Example:: 
<a name="l4335"><span class="ln">4335 </span></a> 
<a name="l4336"><span class="ln">4336 </span></a>    &gt;&gt;&gt; x = torch.arange(9.) 
<a name="l4337"><span class="ln">4337 </span></a>    &gt;&gt;&gt; mantissa, exponent = torch.frexp(x) 
<a name="l4338"><span class="ln">4338 </span></a>    &gt;&gt;&gt; mantissa 
<a name="l4339"><span class="ln">4339 </span></a>    tensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000]) 
<a name="l4340"><span class="ln">4340 </span></a>    &gt;&gt;&gt; exponent 
<a name="l4341"><span class="ln">4341 </span></a>    tensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32) 
<a name="l4342"><span class="ln">4342 </span></a>    &gt;&gt;&gt; torch.ldexp(mantissa, exponent) 
<a name="l4343"><span class="ln">4343 </span></a>    tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.]) 
<a name="l4344"><span class="ln">4344 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4345"><span class="ln">4345 </span></a><span class="s3">)</span>
<a name="l4346"><span class="ln">4346 </span></a>
<a name="l4347"><span class="ln">4347 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4348"><span class="ln">4348 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">from_numpy</span><span class="s3">,</span>
<a name="l4349"><span class="ln">4349 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4350"><span class="ln">4350 </span></a>from_numpy(ndarray) -&gt; Tensor 
<a name="l4351"><span class="ln">4351 </span></a> 
<a name="l4352"><span class="ln">4352 </span></a>Creates a :class:`Tensor` from a :class:`numpy.ndarray`. 
<a name="l4353"><span class="ln">4353 </span></a> 
<a name="l4354"><span class="ln">4354 </span></a>The returned tensor and :attr:`ndarray` share the same memory. Modifications to 
<a name="l4355"><span class="ln">4355 </span></a>the tensor will be reflected in the :attr:`ndarray` and vice versa. The returned 
<a name="l4356"><span class="ln">4356 </span></a>tensor is not resizable. 
<a name="l4357"><span class="ln">4357 </span></a> 
<a name="l4358"><span class="ln">4358 </span></a>It currently accepts :attr:`ndarray` with dtypes of ``numpy.float64``, 
<a name="l4359"><span class="ln">4359 </span></a>``numpy.float32``, ``numpy.float16``, ``numpy.complex64``, ``numpy.complex128``, 
<a name="l4360"><span class="ln">4360 </span></a>``numpy.int64``, ``numpy.int32``, ``numpy.int16``, ``numpy.int8``, ``numpy.uint8``, 
<a name="l4361"><span class="ln">4361 </span></a>and ``bool``. 
<a name="l4362"><span class="ln">4362 </span></a> 
<a name="l4363"><span class="ln">4363 </span></a>.. warning:: 
<a name="l4364"><span class="ln">4364 </span></a>    Writing to a tensor created from a read-only NumPy array is not supported and will result in undefined behavior. 
<a name="l4365"><span class="ln">4365 </span></a> 
<a name="l4366"><span class="ln">4366 </span></a>Example:: 
<a name="l4367"><span class="ln">4367 </span></a> 
<a name="l4368"><span class="ln">4368 </span></a>    &gt;&gt;&gt; a = numpy.array([1, 2, 3]) 
<a name="l4369"><span class="ln">4369 </span></a>    &gt;&gt;&gt; t = torch.from_numpy(a) 
<a name="l4370"><span class="ln">4370 </span></a>    &gt;&gt;&gt; t 
<a name="l4371"><span class="ln">4371 </span></a>    tensor([ 1,  2,  3]) 
<a name="l4372"><span class="ln">4372 </span></a>    &gt;&gt;&gt; t[0] = -1 
<a name="l4373"><span class="ln">4373 </span></a>    &gt;&gt;&gt; a 
<a name="l4374"><span class="ln">4374 </span></a>    array([-1,  2,  3]) 
<a name="l4375"><span class="ln">4375 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4376"><span class="ln">4376 </span></a><span class="s3">)</span>
<a name="l4377"><span class="ln">4377 </span></a>
<a name="l4378"><span class="ln">4378 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4379"><span class="ln">4379 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">frombuffer</span><span class="s3">,</span>
<a name="l4380"><span class="ln">4380 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4381"><span class="ln">4381 </span></a>frombuffer(buffer, *, dtype, count=-1, offset=0, requires_grad=False) -&gt; Tensor 
<a name="l4382"><span class="ln">4382 </span></a> 
<a name="l4383"><span class="ln">4383 </span></a>Creates a 1-dimensional :class:`Tensor` from an object that implements 
<a name="l4384"><span class="ln">4384 </span></a>the Python buffer protocol. 
<a name="l4385"><span class="ln">4385 </span></a> 
<a name="l4386"><span class="ln">4386 </span></a>Skips the first :attr:`offset` bytes in the buffer, and interprets the rest of 
<a name="l4387"><span class="ln">4387 </span></a>the raw bytes as a 1-dimensional tensor of type :attr:`dtype` with :attr:`count` 
<a name="l4388"><span class="ln">4388 </span></a>elements. 
<a name="l4389"><span class="ln">4389 </span></a> 
<a name="l4390"><span class="ln">4390 </span></a>Note that either of the following must be true: 
<a name="l4391"><span class="ln">4391 </span></a> 
<a name="l4392"><span class="ln">4392 </span></a>1. :attr:`count` is a positive non-zero number, and the total number of bytes 
<a name="l4393"><span class="ln">4393 </span></a>in the buffer is more than :attr:`offset` plus :attr:`count` times the size 
<a name="l4394"><span class="ln">4394 </span></a>(in bytes) of :attr:`dtype`. 
<a name="l4395"><span class="ln">4395 </span></a> 
<a name="l4396"><span class="ln">4396 </span></a>2. :attr:`count` is negative, and the length (number of bytes) of the buffer 
<a name="l4397"><span class="ln">4397 </span></a>subtracted by the :attr:`offset` is a multiple of the size (in bytes) of 
<a name="l4398"><span class="ln">4398 </span></a>:attr:`dtype`. 
<a name="l4399"><span class="ln">4399 </span></a> 
<a name="l4400"><span class="ln">4400 </span></a>The returned tensor and buffer share the same memory. Modifications to 
<a name="l4401"><span class="ln">4401 </span></a>the tensor will be reflected in the buffer and vice versa. The returned 
<a name="l4402"><span class="ln">4402 </span></a>tensor is not resizable. 
<a name="l4403"><span class="ln">4403 </span></a> 
<a name="l4404"><span class="ln">4404 </span></a>.. note:: 
<a name="l4405"><span class="ln">4405 </span></a>    This function increments the reference count for the object that 
<a name="l4406"><span class="ln">4406 </span></a>    owns the shared memory. Therefore, such memory will not be deallocated 
<a name="l4407"><span class="ln">4407 </span></a>    before the returned tensor goes out of scope. 
<a name="l4408"><span class="ln">4408 </span></a> 
<a name="l4409"><span class="ln">4409 </span></a>.. warning:: 
<a name="l4410"><span class="ln">4410 </span></a>    This function's behavior is undefined when passed an object implementing 
<a name="l4411"><span class="ln">4411 </span></a>    the buffer protocol whose data is not on the CPU. Doing so is likely to 
<a name="l4412"><span class="ln">4412 </span></a>    cause a segmentation fault. 
<a name="l4413"><span class="ln">4413 </span></a> 
<a name="l4414"><span class="ln">4414 </span></a>.. warning:: 
<a name="l4415"><span class="ln">4415 </span></a>    This function does not try to infer the :attr:`dtype` (hence, it is not 
<a name="l4416"><span class="ln">4416 </span></a>    optional). Passing a different :attr:`dtype` than its source may result 
<a name="l4417"><span class="ln">4417 </span></a>    in unexpected behavior. 
<a name="l4418"><span class="ln">4418 </span></a> 
<a name="l4419"><span class="ln">4419 </span></a>Args: 
<a name="l4420"><span class="ln">4420 </span></a>    buffer (object): a Python object that exposes the buffer interface. 
<a name="l4421"><span class="ln">4421 </span></a> 
<a name="l4422"><span class="ln">4422 </span></a>Keyword args: 
<a name="l4423"><span class="ln">4423 </span></a>    dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l4424"><span class="ln">4424 </span></a>    count (int, optional): the number of desired elements to be read. 
<a name="l4425"><span class="ln">4425 </span></a>        If negative, all the elements (until the end of the buffer) will be 
<a name="l4426"><span class="ln">4426 </span></a>        read. Default: -1. 
<a name="l4427"><span class="ln">4427 </span></a>    offset (int, optional): the number of bytes to skip at the start of 
<a name="l4428"><span class="ln">4428 </span></a>        the buffer. Default: 0. 
<a name="l4429"><span class="ln">4429 </span></a>    {requires_grad} 
<a name="l4430"><span class="ln">4430 </span></a> 
<a name="l4431"><span class="ln">4431 </span></a>Example:: 
<a name="l4432"><span class="ln">4432 </span></a> 
<a name="l4433"><span class="ln">4433 </span></a>    &gt;&gt;&gt; import array 
<a name="l4434"><span class="ln">4434 </span></a>    &gt;&gt;&gt; a = array.array('i', [1, 2, 3]) 
<a name="l4435"><span class="ln">4435 </span></a>    &gt;&gt;&gt; t = torch.frombuffer(a, dtype=torch.int32) 
<a name="l4436"><span class="ln">4436 </span></a>    &gt;&gt;&gt; t 
<a name="l4437"><span class="ln">4437 </span></a>    tensor([ 1,  2,  3]) 
<a name="l4438"><span class="ln">4438 </span></a>    &gt;&gt;&gt; t[0] = -1 
<a name="l4439"><span class="ln">4439 </span></a>    &gt;&gt;&gt; a 
<a name="l4440"><span class="ln">4440 </span></a>    array([-1,  2,  3]) 
<a name="l4441"><span class="ln">4441 </span></a> 
<a name="l4442"><span class="ln">4442 </span></a>    &gt;&gt;&gt; # Interprets the signed char bytes as 32-bit integers. 
<a name="l4443"><span class="ln">4443 </span></a>    &gt;&gt;&gt; # Each 4 signed char elements will be interpreted as 
<a name="l4444"><span class="ln">4444 </span></a>    &gt;&gt;&gt; # 1 signed 32-bit integer. 
<a name="l4445"><span class="ln">4445 </span></a>    &gt;&gt;&gt; import array 
<a name="l4446"><span class="ln">4446 </span></a>    &gt;&gt;&gt; a = array.array('b', [-1, 0, 0, 0]) 
<a name="l4447"><span class="ln">4447 </span></a>    &gt;&gt;&gt; torch.frombuffer(a, dtype=torch.int32) 
<a name="l4448"><span class="ln">4448 </span></a>    tensor([255], dtype=torch.int32) 
<a name="l4449"><span class="ln">4449 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l4450"><span class="ln">4450 </span></a><span class="s3">)</span>
<a name="l4451"><span class="ln">4451 </span></a>
<a name="l4452"><span class="ln">4452 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4453"><span class="ln">4453 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">from_file</span><span class="s3">,</span>
<a name="l4454"><span class="ln">4454 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4455"><span class="ln">4455 </span></a>from_file(filename, shared=None, size=0, *, dtype=None, layout=None, device=None, pin_memory=False) 
<a name="l4456"><span class="ln">4456 </span></a> 
<a name="l4457"><span class="ln">4457 </span></a>Creates a CPU tensor with a storage backed by a memory-mapped file. 
<a name="l4458"><span class="ln">4458 </span></a> 
<a name="l4459"><span class="ln">4459 </span></a>If ``shared`` is True, then memory is shared between processes. All changes are written to the file. 
<a name="l4460"><span class="ln">4460 </span></a>If ``shared`` is False, then changes to the tensor do not affect the file. 
<a name="l4461"><span class="ln">4461 </span></a> 
<a name="l4462"><span class="ln">4462 </span></a>``size`` is the number of elements in the Tensor. If ``shared`` is ``False``, then the file must contain 
<a name="l4463"><span class="ln">4463 </span></a>at least ``size * sizeof(dtype)`` bytes. If ``shared`` is ``True`` the file will be created if needed. 
<a name="l4464"><span class="ln">4464 </span></a> 
<a name="l4465"><span class="ln">4465 </span></a>.. note:: 
<a name="l4466"><span class="ln">4466 </span></a>    Only CPU tensors can be mapped to files. 
<a name="l4467"><span class="ln">4467 </span></a> 
<a name="l4468"><span class="ln">4468 </span></a>.. note:: 
<a name="l4469"><span class="ln">4469 </span></a>    For now, tensors with storages backed by a memory-mapped file cannot be created in pinned memory. 
<a name="l4470"><span class="ln">4470 </span></a> 
<a name="l4471"><span class="ln">4471 </span></a> 
<a name="l4472"><span class="ln">4472 </span></a>Args: 
<a name="l4473"><span class="ln">4473 </span></a>    filename (str): file name to map 
<a name="l4474"><span class="ln">4474 </span></a>    shared (bool): whether to share memory (whether ``MAP_SHARED`` or ``MAP_PRIVATE`` is passed to the 
<a name="l4475"><span class="ln">4475 </span></a>                    underlying `mmap(2) call &lt;https://man7.org/linux/man-pages/man2/mmap.2.html&gt;`_) 
<a name="l4476"><span class="ln">4476 </span></a>    size (int): number of elements in the tensor 
<a name="l4477"><span class="ln">4477 </span></a> 
<a name="l4478"><span class="ln">4478 </span></a>Keyword args: 
<a name="l4479"><span class="ln">4479 </span></a>    {dtype} 
<a name="l4480"><span class="ln">4480 </span></a>    {layout} 
<a name="l4481"><span class="ln">4481 </span></a>    {device} 
<a name="l4482"><span class="ln">4482 </span></a>    {pin_memory} 
<a name="l4483"><span class="ln">4483 </span></a> 
<a name="l4484"><span class="ln">4484 </span></a>Example:: 
<a name="l4485"><span class="ln">4485 </span></a> 
<a name="l4486"><span class="ln">4486 </span></a>    &gt;&gt;&gt; t = torch.randn(2, 5, dtype=torch.float64) 
<a name="l4487"><span class="ln">4487 </span></a>    &gt;&gt;&gt; t.numpy().tofile('storage.pt') 
<a name="l4488"><span class="ln">4488 </span></a>    &gt;&gt;&gt; t_mapped = torch.from_file('storage.pt', shared=False, size=10, dtype=torch.float64) 
<a name="l4489"><span class="ln">4489 </span></a>    &quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l4490"><span class="ln">4490 </span></a><span class="s3">)</span>
<a name="l4491"><span class="ln">4491 </span></a>
<a name="l4492"><span class="ln">4492 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4493"><span class="ln">4493 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">flatten</span><span class="s3">,</span>
<a name="l4494"><span class="ln">4494 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4495"><span class="ln">4495 </span></a>flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor 
<a name="l4496"><span class="ln">4496 </span></a> 
<a name="l4497"><span class="ln">4497 </span></a>Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim` 
<a name="l4498"><span class="ln">4498 </span></a>are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened. 
<a name="l4499"><span class="ln">4499 </span></a>The order of elements in :attr:`input` is unchanged. 
<a name="l4500"><span class="ln">4500 </span></a> 
<a name="l4501"><span class="ln">4501 </span></a>Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view, 
<a name="l4502"><span class="ln">4502 </span></a>or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can 
<a name="l4503"><span class="ln">4503 </span></a>be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the 
<a name="l4504"><span class="ln">4504 </span></a>flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned. 
<a name="l4505"><span class="ln">4505 </span></a> 
<a name="l4506"><span class="ln">4506 </span></a>.. note:: 
<a name="l4507"><span class="ln">4507 </span></a>    Flattening a zero-dimensional tensor will return a one-dimensional view. 
<a name="l4508"><span class="ln">4508 </span></a> 
<a name="l4509"><span class="ln">4509 </span></a>Args: 
<a name="l4510"><span class="ln">4510 </span></a>    {input} 
<a name="l4511"><span class="ln">4511 </span></a>    start_dim (int): the first dim to flatten 
<a name="l4512"><span class="ln">4512 </span></a>    end_dim (int): the last dim to flatten 
<a name="l4513"><span class="ln">4513 </span></a> 
<a name="l4514"><span class="ln">4514 </span></a>Example:: 
<a name="l4515"><span class="ln">4515 </span></a> 
<a name="l4516"><span class="ln">4516 </span></a>    &gt;&gt;&gt; t = torch.tensor([[[1, 2], 
<a name="l4517"><span class="ln">4517 </span></a>    ...                    [3, 4]], 
<a name="l4518"><span class="ln">4518 </span></a>    ...                   [[5, 6], 
<a name="l4519"><span class="ln">4519 </span></a>    ...                    [7, 8]]]) 
<a name="l4520"><span class="ln">4520 </span></a>    &gt;&gt;&gt; torch.flatten(t) 
<a name="l4521"><span class="ln">4521 </span></a>    tensor([1, 2, 3, 4, 5, 6, 7, 8]) 
<a name="l4522"><span class="ln">4522 </span></a>    &gt;&gt;&gt; torch.flatten(t, start_dim=1) 
<a name="l4523"><span class="ln">4523 </span></a>    tensor([[1, 2, 3, 4], 
<a name="l4524"><span class="ln">4524 </span></a>            [5, 6, 7, 8]]) 
<a name="l4525"><span class="ln">4525 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4526"><span class="ln">4526 </span></a><span class="s3">)</span>
<a name="l4527"><span class="ln">4527 </span></a>
<a name="l4528"><span class="ln">4528 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4529"><span class="ln">4529 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unflatten</span><span class="s3">,</span>
<a name="l4530"><span class="ln">4530 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4531"><span class="ln">4531 </span></a>unflatten(input, dim, sizes) -&gt; Tensor 
<a name="l4532"><span class="ln">4532 </span></a> 
<a name="l4533"><span class="ln">4533 </span></a>Expands a dimension of the input tensor over multiple dimensions. 
<a name="l4534"><span class="ln">4534 </span></a> 
<a name="l4535"><span class="ln">4535 </span></a>.. seealso:: 
<a name="l4536"><span class="ln">4536 </span></a> 
<a name="l4537"><span class="ln">4537 </span></a>    :func:`torch.flatten` the inverse of this function. It coalesces several dimensions into one. 
<a name="l4538"><span class="ln">4538 </span></a> 
<a name="l4539"><span class="ln">4539 </span></a>Args: 
<a name="l4540"><span class="ln">4540 </span></a>    {input} 
<a name="l4541"><span class="ln">4541 </span></a>    dim (int): Dimension to be unflattened, specified as an index into 
<a name="l4542"><span class="ln">4542 </span></a>         ``input.shape``. 
<a name="l4543"><span class="ln">4543 </span></a>    sizes (Tuple[int]): New shape of the unflattened dimension. 
<a name="l4544"><span class="ln">4544 </span></a>         One of its elements can be `-1` in which case the corresponding output 
<a name="l4545"><span class="ln">4545 </span></a>         dimension is inferred. Otherwise, the product of ``sizes`` *must* 
<a name="l4546"><span class="ln">4546 </span></a>         equal ``input.shape[dim]``. 
<a name="l4547"><span class="ln">4547 </span></a> 
<a name="l4548"><span class="ln">4548 </span></a>Returns: 
<a name="l4549"><span class="ln">4549 </span></a>    A View of input with the specified dimension unflattened. 
<a name="l4550"><span class="ln">4550 </span></a> 
<a name="l4551"><span class="ln">4551 </span></a>Examples:: 
<a name="l4552"><span class="ln">4552 </span></a>    &gt;&gt;&gt; torch.unflatten(torch.randn(3, 4, 1), 1, (2, 2)).shape 
<a name="l4553"><span class="ln">4553 </span></a>    torch.Size([3, 2, 2, 1]) 
<a name="l4554"><span class="ln">4554 </span></a>    &gt;&gt;&gt; torch.unflatten(torch.randn(3, 4, 1), 1, (-1, 2)).shape 
<a name="l4555"><span class="ln">4555 </span></a>    torch.Size([3, 2, 2, 1]) 
<a name="l4556"><span class="ln">4556 </span></a>    &gt;&gt;&gt; torch.unflatten(torch.randn(5, 12, 3), -2, (2, 2, 3, 1, 1)).shape 
<a name="l4557"><span class="ln">4557 </span></a>    torch.Size([5, 2, 2, 3, 1, 1, 3]) 
<a name="l4558"><span class="ln">4558 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4559"><span class="ln">4559 </span></a><span class="s3">)</span>
<a name="l4560"><span class="ln">4560 </span></a>
<a name="l4561"><span class="ln">4561 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4562"><span class="ln">4562 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">gather</span><span class="s3">,</span>
<a name="l4563"><span class="ln">4563 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4564"><span class="ln">4564 </span></a>gather(input, dim, index, *, sparse_grad=False, out=None) -&gt; Tensor 
<a name="l4565"><span class="ln">4565 </span></a> 
<a name="l4566"><span class="ln">4566 </span></a>Gathers values along an axis specified by `dim`. 
<a name="l4567"><span class="ln">4567 </span></a> 
<a name="l4568"><span class="ln">4568 </span></a>For a 3-D tensor the output is specified by:: 
<a name="l4569"><span class="ln">4569 </span></a> 
<a name="l4570"><span class="ln">4570 </span></a>    out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0 
<a name="l4571"><span class="ln">4571 </span></a>    out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1 
<a name="l4572"><span class="ln">4572 </span></a>    out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2 
<a name="l4573"><span class="ln">4573 </span></a> 
<a name="l4574"><span class="ln">4574 </span></a>:attr:`input` and :attr:`index` must have the same number of dimensions. 
<a name="l4575"><span class="ln">4575 </span></a>It is also required that ``index.size(d) &lt;= input.size(d)`` for all 
<a name="l4576"><span class="ln">4576 </span></a>dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`. 
<a name="l4577"><span class="ln">4577 </span></a>Note that ``input`` and ``index`` do not broadcast against each other. 
<a name="l4578"><span class="ln">4578 </span></a> 
<a name="l4579"><span class="ln">4579 </span></a>Args: 
<a name="l4580"><span class="ln">4580 </span></a>    input (Tensor): the source tensor 
<a name="l4581"><span class="ln">4581 </span></a>    dim (int): the axis along which to index 
<a name="l4582"><span class="ln">4582 </span></a>    index (LongTensor): the indices of elements to gather 
<a name="l4583"><span class="ln">4583 </span></a> 
<a name="l4584"><span class="ln">4584 </span></a>Keyword arguments: 
<a name="l4585"><span class="ln">4585 </span></a>    sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor. 
<a name="l4586"><span class="ln">4586 </span></a>    out (Tensor, optional): the destination tensor 
<a name="l4587"><span class="ln">4587 </span></a> 
<a name="l4588"><span class="ln">4588 </span></a>Example:: 
<a name="l4589"><span class="ln">4589 </span></a> 
<a name="l4590"><span class="ln">4590 </span></a>    &gt;&gt;&gt; t = torch.tensor([[1, 2], [3, 4]]) 
<a name="l4591"><span class="ln">4591 </span></a>    &gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]])) 
<a name="l4592"><span class="ln">4592 </span></a>    tensor([[ 1,  1], 
<a name="l4593"><span class="ln">4593 </span></a>            [ 4,  3]]) 
<a name="l4594"><span class="ln">4594 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4595"><span class="ln">4595 </span></a><span class="s3">)</span>
<a name="l4596"><span class="ln">4596 </span></a>
<a name="l4597"><span class="ln">4597 </span></a>
<a name="l4598"><span class="ln">4598 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4599"><span class="ln">4599 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">gcd</span><span class="s3">,</span>
<a name="l4600"><span class="ln">4600 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4601"><span class="ln">4601 </span></a>gcd(input, other, *, out=None) -&gt; Tensor 
<a name="l4602"><span class="ln">4602 </span></a> 
<a name="l4603"><span class="ln">4603 </span></a>Computes the element-wise greatest common divisor (GCD) of :attr:`input` and :attr:`other`. 
<a name="l4604"><span class="ln">4604 </span></a> 
<a name="l4605"><span class="ln">4605 </span></a>Both :attr:`input` and :attr:`other` must have integer types. 
<a name="l4606"><span class="ln">4606 </span></a> 
<a name="l4607"><span class="ln">4607 </span></a>.. note:: 
<a name="l4608"><span class="ln">4608 </span></a>    This defines :math:`gcd(0, 0) = 0`. 
<a name="l4609"><span class="ln">4609 </span></a> 
<a name="l4610"><span class="ln">4610 </span></a>Args: 
<a name="l4611"><span class="ln">4611 </span></a>    {input} 
<a name="l4612"><span class="ln">4612 </span></a>    other (Tensor): the second input tensor 
<a name="l4613"><span class="ln">4613 </span></a> 
<a name="l4614"><span class="ln">4614 </span></a>Keyword arguments: 
<a name="l4615"><span class="ln">4615 </span></a>    {out} 
<a name="l4616"><span class="ln">4616 </span></a> 
<a name="l4617"><span class="ln">4617 </span></a>Example:: 
<a name="l4618"><span class="ln">4618 </span></a> 
<a name="l4619"><span class="ln">4619 </span></a>    &gt;&gt;&gt; a = torch.tensor([5, 10, 15]) 
<a name="l4620"><span class="ln">4620 </span></a>    &gt;&gt;&gt; b = torch.tensor([3, 4, 5]) 
<a name="l4621"><span class="ln">4621 </span></a>    &gt;&gt;&gt; torch.gcd(a, b) 
<a name="l4622"><span class="ln">4622 </span></a>    tensor([1, 2, 5]) 
<a name="l4623"><span class="ln">4623 </span></a>    &gt;&gt;&gt; c = torch.tensor([3]) 
<a name="l4624"><span class="ln">4624 </span></a>    &gt;&gt;&gt; torch.gcd(a, c) 
<a name="l4625"><span class="ln">4625 </span></a>    tensor([1, 1, 3]) 
<a name="l4626"><span class="ln">4626 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4627"><span class="ln">4627 </span></a><span class="s3">)</span>
<a name="l4628"><span class="ln">4628 </span></a>
<a name="l4629"><span class="ln">4629 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4630"><span class="ln">4630 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ge</span><span class="s3">,</span>
<a name="l4631"><span class="ln">4631 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4632"><span class="ln">4632 </span></a>ge(input, other, *, out=None) -&gt; Tensor 
<a name="l4633"><span class="ln">4633 </span></a> 
<a name="l4634"><span class="ln">4634 </span></a>Computes :math:`\text{input} \geq \text{other}` element-wise. 
<a name="l4635"><span class="ln">4635 </span></a>&quot;&quot;&quot;</span>
<a name="l4636"><span class="ln">4636 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l4637"><span class="ln">4637 </span></a> 
<a name="l4638"><span class="ln">4638 </span></a>The second argument can be a number or a tensor whose shape is 
<a name="l4639"><span class="ln">4639 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l4640"><span class="ln">4640 </span></a> 
<a name="l4641"><span class="ln">4641 </span></a>Args: 
<a name="l4642"><span class="ln">4642 </span></a>    input (Tensor): the tensor to compare 
<a name="l4643"><span class="ln">4643 </span></a>    other (Tensor or float): the tensor or value to compare 
<a name="l4644"><span class="ln">4644 </span></a> 
<a name="l4645"><span class="ln">4645 </span></a>Keyword args: 
<a name="l4646"><span class="ln">4646 </span></a>    {out} 
<a name="l4647"><span class="ln">4647 </span></a> 
<a name="l4648"><span class="ln">4648 </span></a>Returns: 
<a name="l4649"><span class="ln">4649 </span></a>    A boolean tensor that is True where :attr:`input` is greater than or equal to :attr:`other` and False elsewhere 
<a name="l4650"><span class="ln">4650 </span></a> 
<a name="l4651"><span class="ln">4651 </span></a>Example:: 
<a name="l4652"><span class="ln">4652 </span></a> 
<a name="l4653"><span class="ln">4653 </span></a>    &gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l4654"><span class="ln">4654 </span></a>    tensor([[True, True], [False, True]]) 
<a name="l4655"><span class="ln">4655 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4656"><span class="ln">4656 </span></a><span class="s3">)</span>
<a name="l4657"><span class="ln">4657 </span></a>
<a name="l4658"><span class="ln">4658 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4659"><span class="ln">4659 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">greater_equal</span><span class="s3">,</span>
<a name="l4660"><span class="ln">4660 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4661"><span class="ln">4661 </span></a>greater_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l4662"><span class="ln">4662 </span></a> 
<a name="l4663"><span class="ln">4663 </span></a>Alias for :func:`torch.ge`. 
<a name="l4664"><span class="ln">4664 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4665"><span class="ln">4665 </span></a><span class="s3">)</span>
<a name="l4666"><span class="ln">4666 </span></a>
<a name="l4667"><span class="ln">4667 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4668"><span class="ln">4668 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">gradient</span><span class="s3">,</span>
<a name="l4669"><span class="ln">4669 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4670"><span class="ln">4670 </span></a>gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l4671"><span class="ln">4671 </span></a> 
<a name="l4672"><span class="ln">4672 </span></a>Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l4673"><span class="ln">4673 </span></a>one or more dimensions using the `second-order accurate central differences method 
<a name="l4674"><span class="ln">4674 </span></a>&lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l4675"><span class="ln">4675 </span></a>either first or second order estimates at the boundaries. 
<a name="l4676"><span class="ln">4676 </span></a> 
<a name="l4677"><span class="ln">4677 </span></a>The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l4678"><span class="ln">4678 </span></a>specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l4679"><span class="ln">4679 </span></a>to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l4680"><span class="ln">4680 </span></a>:attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l4681"><span class="ln">4681 </span></a>:math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l4682"><span class="ln">4682 </span></a> 
<a name="l4683"><span class="ln">4683 </span></a>When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l4684"><span class="ln">4684 </span></a>This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l4685"><span class="ln">4685 </span></a> 
<a name="l4686"><span class="ln">4686 </span></a>The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l4687"><span class="ln">4687 </span></a>accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l4688"><span class="ln">4688 </span></a>improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l4689"><span class="ln">4689 </span></a>is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l4690"><span class="ln">4690 </span></a>Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l4691"><span class="ln">4691 </span></a>it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l4692"><span class="ln">4692 </span></a> 
<a name="l4693"><span class="ln">4693 </span></a>.. math:: 
<a name="l4694"><span class="ln">4694 </span></a>    \begin{aligned} 
<a name="l4695"><span class="ln">4695 </span></a>        f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l4696"><span class="ln">4696 </span></a>        f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l4697"><span class="ln">4697 </span></a>    \end{aligned} 
<a name="l4698"><span class="ln">4698 </span></a> 
<a name="l4699"><span class="ln">4699 </span></a>Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l4700"><span class="ln">4700 </span></a> 
<a name="l4701"><span class="ln">4701 </span></a>.. math:: 
<a name="l4702"><span class="ln">4702 </span></a>    f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l4703"><span class="ln">4703 </span></a>          + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l4704"><span class="ln">4704 </span></a> 
<a name="l4705"><span class="ln">4705 </span></a>.. note:: 
<a name="l4706"><span class="ln">4706 </span></a>    We estimate the gradient of functions in complex domain 
<a name="l4707"><span class="ln">4707 </span></a>    :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l4708"><span class="ln">4708 </span></a> 
<a name="l4709"><span class="ln">4709 </span></a>The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l4710"><span class="ln">4710 </span></a> 
<a name="l4711"><span class="ln">4711 </span></a>Args: 
<a name="l4712"><span class="ln">4712 </span></a>    input (``Tensor``): the tensor that represents the values of the function 
<a name="l4713"><span class="ln">4713 </span></a> 
<a name="l4714"><span class="ln">4714 </span></a>Keyword args: 
<a name="l4715"><span class="ln">4715 </span></a>    spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l4716"><span class="ln">4716 </span></a>        how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l4717"><span class="ln">4717 </span></a>        the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l4718"><span class="ln">4718 </span></a>        indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l4719"><span class="ln">4719 </span></a>        indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l4720"><span class="ln">4720 </span></a>        Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l4721"><span class="ln">4721 </span></a>        the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l4722"><span class="ln">4722 </span></a>        the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l4723"><span class="ln">4723 </span></a> 
<a name="l4724"><span class="ln">4724 </span></a>    dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l4725"><span class="ln">4725 </span></a>        the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l4726"><span class="ln">4726 </span></a>        the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l4727"><span class="ln">4727 </span></a> 
<a name="l4728"><span class="ln">4728 </span></a>    edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l4729"><span class="ln">4729 </span></a>        &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l4730"><span class="ln">4730 </span></a>        `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l4731"><span class="ln">4731 </span></a>        estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l4732"><span class="ln">4732 </span></a> 
<a name="l4733"><span class="ln">4733 </span></a>Examples:: 
<a name="l4734"><span class="ln">4734 </span></a> 
<a name="l4735"><span class="ln">4735 </span></a>    &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l4736"><span class="ln">4736 </span></a>    &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l4737"><span class="ln">4737 </span></a>    &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l4738"><span class="ln">4738 </span></a>    &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l4739"><span class="ln">4739 </span></a>    (tensor([-3., -2., 2., 5.]),) 
<a name="l4740"><span class="ln">4740 </span></a> 
<a name="l4741"><span class="ln">4741 </span></a>    &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l4742"><span class="ln">4742 </span></a>    &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l4743"><span class="ln">4743 </span></a>    &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l4744"><span class="ln">4744 </span></a>    &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l4745"><span class="ln">4745 </span></a>    &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l4746"><span class="ln">4746 </span></a>    &gt;&gt;&gt; torch.gradient(t) 
<a name="l4747"><span class="ln">4747 </span></a>    (tensor([[ 9., 18., 36., 72.], 
<a name="l4748"><span class="ln">4748 </span></a>             [ 9., 18., 36., 72.]]), 
<a name="l4749"><span class="ln">4749 </span></a>     tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l4750"><span class="ln">4750 </span></a>             [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l4751"><span class="ln">4751 </span></a> 
<a name="l4752"><span class="ln">4752 </span></a>    &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l4753"><span class="ln">4753 </span></a>    &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l4754"><span class="ln">4754 </span></a>    &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l4755"><span class="ln">4755 </span></a>    &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l4756"><span class="ln">4756 </span></a>    &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l4757"><span class="ln">4757 </span></a>    &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l4758"><span class="ln">4758 </span></a>    (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l4759"><span class="ln">4759 </span></a>              [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l4760"><span class="ln">4760 </span></a>     tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l4761"><span class="ln">4761 </span></a>              [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l4762"><span class="ln">4762 </span></a>    &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l4763"><span class="ln">4763 </span></a> 
<a name="l4764"><span class="ln">4764 </span></a>    &gt;&gt;&gt; 
<a name="l4765"><span class="ln">4765 </span></a>    &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l4766"><span class="ln">4766 </span></a>    &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l4767"><span class="ln">4767 </span></a>    (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l4768"><span class="ln">4768 </span></a>             [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l4769"><span class="ln">4769 </span></a> 
<a name="l4770"><span class="ln">4770 </span></a>    &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l4771"><span class="ln">4771 </span></a>    &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l4772"><span class="ln">4772 </span></a>    &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l4773"><span class="ln">4773 </span></a>    &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l4774"><span class="ln">4774 </span></a>    &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l4775"><span class="ln">4775 </span></a>    &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l4776"><span class="ln">4776 </span></a>    (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l4777"><span class="ln">4777 </span></a>             [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l4778"><span class="ln">4778 </span></a>     tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l4779"><span class="ln">4779 </span></a>             [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l4780"><span class="ln">4780 </span></a> 
<a name="l4781"><span class="ln">4781 </span></a>    &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l4782"><span class="ln">4782 </span></a>    &gt;&gt;&gt; # coordinates. 
<a name="l4783"><span class="ln">4783 </span></a>    &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l4784"><span class="ln">4784 </span></a>    &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l4785"><span class="ln">4785 </span></a>    (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l4786"><span class="ln">4786 </span></a>             [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l4787"><span class="ln">4787 </span></a>     tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l4788"><span class="ln">4788 </span></a>             [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l4789"><span class="ln">4789 </span></a> 
<a name="l4790"><span class="ln">4790 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4791"><span class="ln">4791 </span></a><span class="s3">)</span>
<a name="l4792"><span class="ln">4792 </span></a>
<a name="l4793"><span class="ln">4793 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4794"><span class="ln">4794 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">geqrf</span><span class="s3">,</span>
<a name="l4795"><span class="ln">4795 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4796"><span class="ln">4796 </span></a>geqrf(input, *, out=None) -&gt; (Tensor, Tensor) 
<a name="l4797"><span class="ln">4797 </span></a> 
<a name="l4798"><span class="ln">4798 </span></a>This is a low-level function for calling LAPACK's geqrf directly. This function 
<a name="l4799"><span class="ln">4799 </span></a>returns a namedtuple (a, tau) as defined in `LAPACK documentation for geqrf`_ . 
<a name="l4800"><span class="ln">4800 </span></a> 
<a name="l4801"><span class="ln">4801 </span></a>Computes a QR decomposition of :attr:`input`. 
<a name="l4802"><span class="ln">4802 </span></a>Both `Q` and `R` matrices are stored in the same output tensor `a`. 
<a name="l4803"><span class="ln">4803 </span></a>The elements of `R` are stored on and above the diagonal. 
<a name="l4804"><span class="ln">4804 </span></a>Elementary reflectors (or Householder vectors) implicitly defining matrix `Q` 
<a name="l4805"><span class="ln">4805 </span></a>are stored below the diagonal. 
<a name="l4806"><span class="ln">4806 </span></a>The results of this function can be used together with :func:`torch.linalg.householder_product` 
<a name="l4807"><span class="ln">4807 </span></a>to obtain the `Q` matrix or 
<a name="l4808"><span class="ln">4808 </span></a>with :func:`torch.ormqr`, which uses an implicit representation of the `Q` matrix, 
<a name="l4809"><span class="ln">4809 </span></a>for an efficient matrix-matrix multiplication. 
<a name="l4810"><span class="ln">4810 </span></a> 
<a name="l4811"><span class="ln">4811 </span></a>See `LAPACK documentation for geqrf`_ for further details. 
<a name="l4812"><span class="ln">4812 </span></a> 
<a name="l4813"><span class="ln">4813 </span></a>.. note:: 
<a name="l4814"><span class="ln">4814 </span></a>    See also :func:`torch.linalg.qr`, which computes Q and R matrices, and :func:`torch.linalg.lstsq` 
<a name="l4815"><span class="ln">4815 </span></a>    with the ``driver=&quot;gels&quot;`` option for a function that can solve matrix equations using a QR decomposition. 
<a name="l4816"><span class="ln">4816 </span></a> 
<a name="l4817"><span class="ln">4817 </span></a>Args: 
<a name="l4818"><span class="ln">4818 </span></a>    input (Tensor): the input matrix 
<a name="l4819"><span class="ln">4819 </span></a> 
<a name="l4820"><span class="ln">4820 </span></a>Keyword args: 
<a name="l4821"><span class="ln">4821 </span></a>    out (tuple, optional): the output tuple of (Tensor, Tensor). Ignored if `None`. Default: `None`. 
<a name="l4822"><span class="ln">4822 </span></a> 
<a name="l4823"><span class="ln">4823 </span></a>.. _LAPACK documentation for geqrf: 
<a name="l4824"><span class="ln">4824 </span></a>    http://www.netlib.org/lapack/explore-html/df/dc5/group__variants_g_ecomputational_ga3766ea903391b5cf9008132f7440ec7b.html 
<a name="l4825"><span class="ln">4825 </span></a> 
<a name="l4826"><span class="ln">4826 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4827"><span class="ln">4827 </span></a><span class="s3">)</span>
<a name="l4828"><span class="ln">4828 </span></a>
<a name="l4829"><span class="ln">4829 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4830"><span class="ln">4830 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">inner</span><span class="s3">,</span>
<a name="l4831"><span class="ln">4831 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4832"><span class="ln">4832 </span></a>inner(input, other, *, out=None) -&gt; Tensor 
<a name="l4833"><span class="ln">4833 </span></a> 
<a name="l4834"><span class="ln">4834 </span></a>Computes the dot product for 1D tensors. For higher dimensions, sums the product 
<a name="l4835"><span class="ln">4835 </span></a>of elements from :attr:`input` and :attr:`other` along their last dimension. 
<a name="l4836"><span class="ln">4836 </span></a> 
<a name="l4837"><span class="ln">4837 </span></a>.. note:: 
<a name="l4838"><span class="ln">4838 </span></a> 
<a name="l4839"><span class="ln">4839 </span></a>    If either :attr:`input` or :attr:`other` is a scalar, the result is equivalent 
<a name="l4840"><span class="ln">4840 </span></a>    to `torch.mul(input, other)`. 
<a name="l4841"><span class="ln">4841 </span></a> 
<a name="l4842"><span class="ln">4842 </span></a>    If both :attr:`input` and :attr:`other` are non-scalars, the size of their last 
<a name="l4843"><span class="ln">4843 </span></a>    dimension must match and the result is equivalent to `torch.tensordot(input, 
<a name="l4844"><span class="ln">4844 </span></a>    other, dims=([-1], [-1]))` 
<a name="l4845"><span class="ln">4845 </span></a> 
<a name="l4846"><span class="ln">4846 </span></a>Args: 
<a name="l4847"><span class="ln">4847 </span></a>    input (Tensor): First input tensor 
<a name="l4848"><span class="ln">4848 </span></a>    other (Tensor): Second input tensor 
<a name="l4849"><span class="ln">4849 </span></a> 
<a name="l4850"><span class="ln">4850 </span></a>Keyword args: 
<a name="l4851"><span class="ln">4851 </span></a>    out (Tensor, optional): Optional output tensor to write result into. The output 
<a name="l4852"><span class="ln">4852 </span></a>                            shape is `input.shape[:-1] + other.shape[:-1]`. 
<a name="l4853"><span class="ln">4853 </span></a> 
<a name="l4854"><span class="ln">4854 </span></a>Example:: 
<a name="l4855"><span class="ln">4855 </span></a> 
<a name="l4856"><span class="ln">4856 </span></a>    # Dot product 
<a name="l4857"><span class="ln">4857 </span></a>    &gt;&gt;&gt; torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1])) 
<a name="l4858"><span class="ln">4858 </span></a>    tensor(7) 
<a name="l4859"><span class="ln">4859 </span></a> 
<a name="l4860"><span class="ln">4860 </span></a>    # Multidimensional input tensors 
<a name="l4861"><span class="ln">4861 </span></a>    &gt;&gt;&gt; a = torch.randn(2, 3) 
<a name="l4862"><span class="ln">4862 </span></a>    &gt;&gt;&gt; a 
<a name="l4863"><span class="ln">4863 </span></a>    tensor([[0.8173, 1.0874, 1.1784], 
<a name="l4864"><span class="ln">4864 </span></a>            [0.3279, 0.1234, 2.7894]]) 
<a name="l4865"><span class="ln">4865 </span></a>    &gt;&gt;&gt; b = torch.randn(2, 4, 3) 
<a name="l4866"><span class="ln">4866 </span></a>    &gt;&gt;&gt; b 
<a name="l4867"><span class="ln">4867 </span></a>    tensor([[[-0.4682, -0.7159,  0.1506], 
<a name="l4868"><span class="ln">4868 </span></a>            [ 0.4034, -0.3657,  1.0387], 
<a name="l4869"><span class="ln">4869 </span></a>            [ 0.9892, -0.6684,  0.1774], 
<a name="l4870"><span class="ln">4870 </span></a>            [ 0.9482,  1.3261,  0.3917]], 
<a name="l4871"><span class="ln">4871 </span></a> 
<a name="l4872"><span class="ln">4872 </span></a>            [[ 0.4537,  0.7493,  1.1724], 
<a name="l4873"><span class="ln">4873 </span></a>            [ 0.2291,  0.5749, -0.2267], 
<a name="l4874"><span class="ln">4874 </span></a>            [-0.7920,  0.3607, -0.3701], 
<a name="l4875"><span class="ln">4875 </span></a>            [ 1.3666, -0.5850, -1.7242]]]) 
<a name="l4876"><span class="ln">4876 </span></a>    &gt;&gt;&gt; torch.inner(a, b) 
<a name="l4877"><span class="ln">4877 </span></a>    tensor([[[-0.9837,  1.1560,  0.2907,  2.6785], 
<a name="l4878"><span class="ln">4878 </span></a>            [ 2.5671,  0.5452, -0.6912, -1.5509]], 
<a name="l4879"><span class="ln">4879 </span></a> 
<a name="l4880"><span class="ln">4880 </span></a>            [[ 0.1782,  2.9843,  0.7366,  1.5672], 
<a name="l4881"><span class="ln">4881 </span></a>            [ 3.5115, -0.4864, -1.2476, -4.4337]]]) 
<a name="l4882"><span class="ln">4882 </span></a> 
<a name="l4883"><span class="ln">4883 </span></a>    # Scalar input 
<a name="l4884"><span class="ln">4884 </span></a>    &gt;&gt;&gt; torch.inner(a, torch.tensor(2)) 
<a name="l4885"><span class="ln">4885 </span></a>    tensor([[1.6347, 2.1748, 2.3567], 
<a name="l4886"><span class="ln">4886 </span></a>            [0.6558, 0.2469, 5.5787]]) 
<a name="l4887"><span class="ln">4887 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4888"><span class="ln">4888 </span></a><span class="s3">)</span>
<a name="l4889"><span class="ln">4889 </span></a>
<a name="l4890"><span class="ln">4890 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4891"><span class="ln">4891 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">outer</span><span class="s3">,</span>
<a name="l4892"><span class="ln">4892 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4893"><span class="ln">4893 </span></a>outer(input, vec2, *, out=None) -&gt; Tensor 
<a name="l4894"><span class="ln">4894 </span></a> 
<a name="l4895"><span class="ln">4895 </span></a>Outer product of :attr:`input` and :attr:`vec2`. 
<a name="l4896"><span class="ln">4896 </span></a>If :attr:`input` is a vector of size :math:`n` and :attr:`vec2` is a vector of 
<a name="l4897"><span class="ln">4897 </span></a>size :math:`m`, then :attr:`out` must be a matrix of size :math:`(n \times m)`. 
<a name="l4898"><span class="ln">4898 </span></a> 
<a name="l4899"><span class="ln">4899 </span></a>.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l4900"><span class="ln">4900 </span></a> 
<a name="l4901"><span class="ln">4901 </span></a>Args: 
<a name="l4902"><span class="ln">4902 </span></a>    input (Tensor): 1-D input vector 
<a name="l4903"><span class="ln">4903 </span></a>    vec2 (Tensor): 1-D input vector 
<a name="l4904"><span class="ln">4904 </span></a> 
<a name="l4905"><span class="ln">4905 </span></a>Keyword args: 
<a name="l4906"><span class="ln">4906 </span></a>    out (Tensor, optional): optional output matrix 
<a name="l4907"><span class="ln">4907 </span></a> 
<a name="l4908"><span class="ln">4908 </span></a>Example:: 
<a name="l4909"><span class="ln">4909 </span></a> 
<a name="l4910"><span class="ln">4910 </span></a>    &gt;&gt;&gt; v1 = torch.arange(1., 5.) 
<a name="l4911"><span class="ln">4911 </span></a>    &gt;&gt;&gt; v2 = torch.arange(1., 4.) 
<a name="l4912"><span class="ln">4912 </span></a>    &gt;&gt;&gt; torch.outer(v1, v2) 
<a name="l4913"><span class="ln">4913 </span></a>    tensor([[  1.,   2.,   3.], 
<a name="l4914"><span class="ln">4914 </span></a>            [  2.,   4.,   6.], 
<a name="l4915"><span class="ln">4915 </span></a>            [  3.,   6.,   9.], 
<a name="l4916"><span class="ln">4916 </span></a>            [  4.,   8.,  12.]]) 
<a name="l4917"><span class="ln">4917 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4918"><span class="ln">4918 </span></a><span class="s3">)</span>
<a name="l4919"><span class="ln">4919 </span></a>
<a name="l4920"><span class="ln">4920 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4921"><span class="ln">4921 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ger</span><span class="s3">,</span>
<a name="l4922"><span class="ln">4922 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4923"><span class="ln">4923 </span></a>ger(input, vec2, *, out=None) -&gt; Tensor 
<a name="l4924"><span class="ln">4924 </span></a> 
<a name="l4925"><span class="ln">4925 </span></a>Alias of :func:`torch.outer`. 
<a name="l4926"><span class="ln">4926 </span></a> 
<a name="l4927"><span class="ln">4927 </span></a>.. warning:: 
<a name="l4928"><span class="ln">4928 </span></a>    This function is deprecated and will be removed in a future PyTorch release. 
<a name="l4929"><span class="ln">4929 </span></a>    Use :func:`torch.outer` instead. 
<a name="l4930"><span class="ln">4930 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4931"><span class="ln">4931 </span></a><span class="s3">)</span>
<a name="l4932"><span class="ln">4932 </span></a>
<a name="l4933"><span class="ln">4933 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4934"><span class="ln">4934 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">get_default_dtype</span><span class="s3">,</span>
<a name="l4935"><span class="ln">4935 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4936"><span class="ln">4936 </span></a>get_default_dtype() -&gt; torch.dtype 
<a name="l4937"><span class="ln">4937 </span></a> 
<a name="l4938"><span class="ln">4938 </span></a>Get the current default floating point :class:`torch.dtype`. 
<a name="l4939"><span class="ln">4939 </span></a> 
<a name="l4940"><span class="ln">4940 </span></a>Example:: 
<a name="l4941"><span class="ln">4941 </span></a> 
<a name="l4942"><span class="ln">4942 </span></a>    &gt;&gt;&gt; torch.get_default_dtype()  # initial default for floating point is torch.float32 
<a name="l4943"><span class="ln">4943 </span></a>    torch.float32 
<a name="l4944"><span class="ln">4944 </span></a>    &gt;&gt;&gt; torch.set_default_dtype(torch.float64) 
<a name="l4945"><span class="ln">4945 </span></a>    &gt;&gt;&gt; torch.get_default_dtype()  # default is now changed to torch.float64 
<a name="l4946"><span class="ln">4946 </span></a>    torch.float64 
<a name="l4947"><span class="ln">4947 </span></a> 
<a name="l4948"><span class="ln">4948 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4949"><span class="ln">4949 </span></a><span class="s3">)</span>
<a name="l4950"><span class="ln">4950 </span></a>
<a name="l4951"><span class="ln">4951 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4952"><span class="ln">4952 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">get_num_threads</span><span class="s3">,</span>
<a name="l4953"><span class="ln">4953 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4954"><span class="ln">4954 </span></a>get_num_threads() -&gt; int 
<a name="l4955"><span class="ln">4955 </span></a> 
<a name="l4956"><span class="ln">4956 </span></a>Returns the number of threads used for parallelizing CPU operations 
<a name="l4957"><span class="ln">4957 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4958"><span class="ln">4958 </span></a><span class="s3">)</span>
<a name="l4959"><span class="ln">4959 </span></a>
<a name="l4960"><span class="ln">4960 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4961"><span class="ln">4961 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">get_num_interop_threads</span><span class="s3">,</span>
<a name="l4962"><span class="ln">4962 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4963"><span class="ln">4963 </span></a>get_num_interop_threads() -&gt; int 
<a name="l4964"><span class="ln">4964 </span></a> 
<a name="l4965"><span class="ln">4965 </span></a>Returns the number of threads used for inter-op parallelism on CPU 
<a name="l4966"><span class="ln">4966 </span></a>(e.g. in JIT interpreter) 
<a name="l4967"><span class="ln">4967 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4968"><span class="ln">4968 </span></a><span class="s3">)</span>
<a name="l4969"><span class="ln">4969 </span></a>
<a name="l4970"><span class="ln">4970 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l4971"><span class="ln">4971 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">gt</span><span class="s3">,</span>
<a name="l4972"><span class="ln">4972 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4973"><span class="ln">4973 </span></a>gt(input, other, *, out=None) -&gt; Tensor 
<a name="l4974"><span class="ln">4974 </span></a> 
<a name="l4975"><span class="ln">4975 </span></a>Computes :math:`\text{input} &gt; \text{other}` element-wise. 
<a name="l4976"><span class="ln">4976 </span></a>&quot;&quot;&quot;</span>
<a name="l4977"><span class="ln">4977 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l4978"><span class="ln">4978 </span></a> 
<a name="l4979"><span class="ln">4979 </span></a>The second argument can be a number or a tensor whose shape is 
<a name="l4980"><span class="ln">4980 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l4981"><span class="ln">4981 </span></a> 
<a name="l4982"><span class="ln">4982 </span></a>Args: 
<a name="l4983"><span class="ln">4983 </span></a>    input (Tensor): the tensor to compare 
<a name="l4984"><span class="ln">4984 </span></a>    other (Tensor or float): the tensor or value to compare 
<a name="l4985"><span class="ln">4985 </span></a> 
<a name="l4986"><span class="ln">4986 </span></a>Keyword args: 
<a name="l4987"><span class="ln">4987 </span></a>    {out} 
<a name="l4988"><span class="ln">4988 </span></a> 
<a name="l4989"><span class="ln">4989 </span></a>Returns: 
<a name="l4990"><span class="ln">4990 </span></a>    A boolean tensor that is True where :attr:`input` is greater than :attr:`other` and False elsewhere 
<a name="l4991"><span class="ln">4991 </span></a> 
<a name="l4992"><span class="ln">4992 </span></a>Example:: 
<a name="l4993"><span class="ln">4993 </span></a> 
<a name="l4994"><span class="ln">4994 </span></a>    &gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l4995"><span class="ln">4995 </span></a>    tensor([[False, True], [False, False]]) 
<a name="l4996"><span class="ln">4996 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l4997"><span class="ln">4997 </span></a><span class="s3">)</span>
<a name="l4998"><span class="ln">4998 </span></a>
<a name="l4999"><span class="ln">4999 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5000"><span class="ln">5000 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">greater</span><span class="s3">,</span>
<a name="l5001"><span class="ln">5001 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5002"><span class="ln">5002 </span></a>greater(input, other, *, out=None) -&gt; Tensor 
<a name="l5003"><span class="ln">5003 </span></a> 
<a name="l5004"><span class="ln">5004 </span></a>Alias for :func:`torch.gt`. 
<a name="l5005"><span class="ln">5005 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5006"><span class="ln">5006 </span></a><span class="s3">)</span>
<a name="l5007"><span class="ln">5007 </span></a>
<a name="l5008"><span class="ln">5008 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5009"><span class="ln">5009 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">histc</span><span class="s3">,</span>
<a name="l5010"><span class="ln">5010 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5011"><span class="ln">5011 </span></a>histc(input, bins=100, min=0, max=0, *, out=None) -&gt; Tensor 
<a name="l5012"><span class="ln">5012 </span></a> 
<a name="l5013"><span class="ln">5013 </span></a>Computes the histogram of a tensor. 
<a name="l5014"><span class="ln">5014 </span></a> 
<a name="l5015"><span class="ln">5015 </span></a>The elements are sorted into equal width bins between :attr:`min` and 
<a name="l5016"><span class="ln">5016 </span></a>:attr:`max`. If :attr:`min` and :attr:`max` are both zero, the minimum and 
<a name="l5017"><span class="ln">5017 </span></a>maximum values of the data are used. 
<a name="l5018"><span class="ln">5018 </span></a> 
<a name="l5019"><span class="ln">5019 </span></a>Elements lower than min and higher than max and ``NaN`` elements are ignored. 
<a name="l5020"><span class="ln">5020 </span></a> 
<a name="l5021"><span class="ln">5021 </span></a>Args: 
<a name="l5022"><span class="ln">5022 </span></a>    {input} 
<a name="l5023"><span class="ln">5023 </span></a>    bins (int): number of histogram bins 
<a name="l5024"><span class="ln">5024 </span></a>    min (Scalar): lower end of the range (inclusive) 
<a name="l5025"><span class="ln">5025 </span></a>    max (Scalar): upper end of the range (inclusive) 
<a name="l5026"><span class="ln">5026 </span></a> 
<a name="l5027"><span class="ln">5027 </span></a>Keyword args: 
<a name="l5028"><span class="ln">5028 </span></a>    {out} 
<a name="l5029"><span class="ln">5029 </span></a> 
<a name="l5030"><span class="ln">5030 </span></a>Returns: 
<a name="l5031"><span class="ln">5031 </span></a>    Tensor: Histogram represented as a tensor 
<a name="l5032"><span class="ln">5032 </span></a> 
<a name="l5033"><span class="ln">5033 </span></a>Example:: 
<a name="l5034"><span class="ln">5034 </span></a> 
<a name="l5035"><span class="ln">5035 </span></a>    &gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3) 
<a name="l5036"><span class="ln">5036 </span></a>    tensor([ 0.,  2.,  1.,  0.]) 
<a name="l5037"><span class="ln">5037 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5038"><span class="ln">5038 </span></a><span class="s3">)</span>
<a name="l5039"><span class="ln">5039 </span></a>
<a name="l5040"><span class="ln">5040 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5041"><span class="ln">5041 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">histogram</span><span class="s3">,</span>
<a name="l5042"><span class="ln">5042 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5043"><span class="ln">5043 </span></a>histogram(input, bins, *, range=None, weight=None, density=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l5044"><span class="ln">5044 </span></a> 
<a name="l5045"><span class="ln">5045 </span></a>Computes a histogram of the values in a tensor. 
<a name="l5046"><span class="ln">5046 </span></a> 
<a name="l5047"><span class="ln">5047 </span></a>:attr:`bins` can be an integer or a 1D tensor. 
<a name="l5048"><span class="ln">5048 </span></a> 
<a name="l5049"><span class="ln">5049 </span></a>If :attr:`bins` is an int, it specifies the number of equal-width bins. 
<a name="l5050"><span class="ln">5050 </span></a>By default, the lower and upper range of the bins is determined by the 
<a name="l5051"><span class="ln">5051 </span></a>minimum and maximum elements of the input tensor. The :attr:`range` 
<a name="l5052"><span class="ln">5052 </span></a>argument can be provided to specify a range for the bins. 
<a name="l5053"><span class="ln">5053 </span></a> 
<a name="l5054"><span class="ln">5054 </span></a>If :attr:`bins` is a 1D tensor, it specifies the sequence of bin edges 
<a name="l5055"><span class="ln">5055 </span></a>including the rightmost edge. It should contain at least 2 elements 
<a name="l5056"><span class="ln">5056 </span></a>and its elements should be increasing. 
<a name="l5057"><span class="ln">5057 </span></a> 
<a name="l5058"><span class="ln">5058 </span></a>Args: 
<a name="l5059"><span class="ln">5059 </span></a>    {input} 
<a name="l5060"><span class="ln">5060 </span></a>    bins: int or 1D Tensor. If int, defines the number of equal-width bins. If tensor, 
<a name="l5061"><span class="ln">5061 </span></a>          defines the sequence of bin edges including the rightmost edge. 
<a name="l5062"><span class="ln">5062 </span></a> 
<a name="l5063"><span class="ln">5063 </span></a>Keyword args: 
<a name="l5064"><span class="ln">5064 </span></a>    range (tuple of float): Defines the range of the bins. 
<a name="l5065"><span class="ln">5065 </span></a>    weight (Tensor): If provided, weight should have the same shape as input. Each value in 
<a name="l5066"><span class="ln">5066 </span></a>                     input contributes its associated weight towards its bin's result. 
<a name="l5067"><span class="ln">5067 </span></a>    density (bool): If False, the result will contain the count (or total weight) in each bin. 
<a name="l5068"><span class="ln">5068 </span></a>                    If True, the result is the value of the probability density function over the bins, 
<a name="l5069"><span class="ln">5069 </span></a>                    normalized such that the integral over the range of the bins is 1. 
<a name="l5070"><span class="ln">5070 </span></a>    {out} (tuple, optional): The result tuple of two output tensors (hist, bin_edges). 
<a name="l5071"><span class="ln">5071 </span></a> 
<a name="l5072"><span class="ln">5072 </span></a>Returns: 
<a name="l5073"><span class="ln">5073 </span></a>    hist (Tensor): 1D Tensor containing the values of the histogram. 
<a name="l5074"><span class="ln">5074 </span></a>    bin_edges(Tensor): 1D Tensor containing the edges of the histogram bins. 
<a name="l5075"><span class="ln">5075 </span></a> 
<a name="l5076"><span class="ln">5076 </span></a>Example:: 
<a name="l5077"><span class="ln">5077 </span></a> 
<a name="l5078"><span class="ln">5078 </span></a>    &gt;&gt;&gt; torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.])) 
<a name="l5079"><span class="ln">5079 </span></a>    (tensor([ 0.,  5.,  2.,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.])) 
<a name="l5080"><span class="ln">5080 </span></a>    &gt;&gt;&gt; torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.]), density=True) 
<a name="l5081"><span class="ln">5081 </span></a>    (tensor([ 0.,  0.9524,  0.3810,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.])) 
<a name="l5082"><span class="ln">5082 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5083"><span class="ln">5083 </span></a><span class="s3">)</span>
<a name="l5084"><span class="ln">5084 </span></a>
<a name="l5085"><span class="ln">5085 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5086"><span class="ln">5086 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">histogramdd</span><span class="s3">,</span>
<a name="l5087"><span class="ln">5087 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5088"><span class="ln">5088 </span></a>histogramdd(input, bins, *, range=None, weight=None, density=False, out=None) -&gt; (Tensor, Tensor[]) 
<a name="l5089"><span class="ln">5089 </span></a> 
<a name="l5090"><span class="ln">5090 </span></a>Computes a multi-dimensional histogram of the values in a tensor. 
<a name="l5091"><span class="ln">5091 </span></a> 
<a name="l5092"><span class="ln">5092 </span></a>Interprets the elements of an input tensor whose innermost dimension has size N 
<a name="l5093"><span class="ln">5093 </span></a>as a collection of N-dimensional points. Maps each of the points into a set of 
<a name="l5094"><span class="ln">5094 </span></a>N-dimensional bins and returns the number of points (or total weight) in each bin. 
<a name="l5095"><span class="ln">5095 </span></a> 
<a name="l5096"><span class="ln">5096 </span></a>:attr:`input` must be a tensor with at least 2 dimensions. 
<a name="l5097"><span class="ln">5097 </span></a>If input has shape (M, N), each of its M rows defines a point in N-dimensional space. 
<a name="l5098"><span class="ln">5098 </span></a>If input has three or more dimensions, all but the last dimension are flattened. 
<a name="l5099"><span class="ln">5099 </span></a> 
<a name="l5100"><span class="ln">5100 </span></a>Each dimension is independently associated with its own strictly increasing sequence 
<a name="l5101"><span class="ln">5101 </span></a>of bin edges. Bin edges may be specified explicitly by passing a sequence of 1D 
<a name="l5102"><span class="ln">5102 </span></a>tensors. Alternatively, bin edges may be constructed automatically by passing a 
<a name="l5103"><span class="ln">5103 </span></a>sequence of integers specifying the number of equal-width bins in each dimension. 
<a name="l5104"><span class="ln">5104 </span></a> 
<a name="l5105"><span class="ln">5105 </span></a>For each N-dimensional point in input: 
<a name="l5106"><span class="ln">5106 </span></a>    - Each of its coordinates is binned independently among the bin edges 
<a name="l5107"><span class="ln">5107 </span></a>        corresponding to its dimension 
<a name="l5108"><span class="ln">5108 </span></a>    - Binning results are combined to identify the N-dimensional bin (if any) 
<a name="l5109"><span class="ln">5109 </span></a>        into which the point falls 
<a name="l5110"><span class="ln">5110 </span></a>    - If the point falls into a bin, the bin's count (or total weight) is incremented 
<a name="l5111"><span class="ln">5111 </span></a>    - Points which do not fall into any bin do not contribute to the output 
<a name="l5112"><span class="ln">5112 </span></a> 
<a name="l5113"><span class="ln">5113 </span></a>:attr:`bins` can be a sequence of N 1D tensors, a sequence of N ints, or a single int. 
<a name="l5114"><span class="ln">5114 </span></a> 
<a name="l5115"><span class="ln">5115 </span></a>If :attr:`bins` is a sequence of N 1D tensors, it explicitly specifies the N sequences 
<a name="l5116"><span class="ln">5116 </span></a>of bin edges. Each 1D tensor should contain a strictly increasing sequence with at 
<a name="l5117"><span class="ln">5117 </span></a>least one element. A sequence of K bin edges defines K-1 bins, explicitly specifying 
<a name="l5118"><span class="ln">5118 </span></a>the left and right edges of all bins. Every bin is exclusive of its left edge. Only 
<a name="l5119"><span class="ln">5119 </span></a>the rightmost bin is inclusive of its right edge. 
<a name="l5120"><span class="ln">5120 </span></a> 
<a name="l5121"><span class="ln">5121 </span></a>If :attr:`bins` is a sequence of N ints, it specifies the number of equal-width bins 
<a name="l5122"><span class="ln">5122 </span></a>in each dimension. By default, the leftmost and rightmost bin edges in each dimension 
<a name="l5123"><span class="ln">5123 </span></a>are determined by the minimum and maximum elements of the input tensor in the 
<a name="l5124"><span class="ln">5124 </span></a>corresponding dimension. The :attr:`range` argument can be provided to manually 
<a name="l5125"><span class="ln">5125 </span></a>specify the leftmost and rightmost bin edges in each dimension. 
<a name="l5126"><span class="ln">5126 </span></a> 
<a name="l5127"><span class="ln">5127 </span></a>If :attr:`bins` is an int, it specifies the number of equal-width bins for all dimensions. 
<a name="l5128"><span class="ln">5128 </span></a> 
<a name="l5129"><span class="ln">5129 </span></a>.. note:: 
<a name="l5130"><span class="ln">5130 </span></a>    See also :func:`torch.histogram`, which specifically computes 1D histograms. 
<a name="l5131"><span class="ln">5131 </span></a>    While :func:`torch.histogramdd` infers the dimensionality of its bins and 
<a name="l5132"><span class="ln">5132 </span></a>    binned values from the shape of :attr:`input`, :func:`torch.histogram` 
<a name="l5133"><span class="ln">5133 </span></a>    accepts and flattens :attr:`input` of any shape. 
<a name="l5134"><span class="ln">5134 </span></a> 
<a name="l5135"><span class="ln">5135 </span></a>Args: 
<a name="l5136"><span class="ln">5136 </span></a>    {input} 
<a name="l5137"><span class="ln">5137 </span></a>    bins: Tensor[], int[], or int. 
<a name="l5138"><span class="ln">5138 </span></a>            If Tensor[], defines the sequences of bin edges. 
<a name="l5139"><span class="ln">5139 </span></a>            If int[], defines the number of equal-width bins in each dimension. 
<a name="l5140"><span class="ln">5140 </span></a>            If int, defines the number of equal-width bins for all dimensions. 
<a name="l5141"><span class="ln">5141 </span></a>Keyword args: 
<a name="l5142"><span class="ln">5142 </span></a>    range (sequence of float): Defines the leftmost and rightmost bin edges 
<a name="l5143"><span class="ln">5143 </span></a>                                in each dimension. 
<a name="l5144"><span class="ln">5144 </span></a>    weight (Tensor): By default, each value in the input has weight 1. If a weight 
<a name="l5145"><span class="ln">5145 </span></a>                        tensor is passed, each N-dimensional coordinate in input 
<a name="l5146"><span class="ln">5146 </span></a>                        contributes its associated weight towards its bin's result. 
<a name="l5147"><span class="ln">5147 </span></a>                        The weight tensor should have the same shape as the :attr:`input` 
<a name="l5148"><span class="ln">5148 </span></a>                        tensor excluding its innermost dimension N. 
<a name="l5149"><span class="ln">5149 </span></a>    density (bool): If False (default), the result will contain the count (or total weight) 
<a name="l5150"><span class="ln">5150 </span></a>                    in each bin. If True, each count (weight) is divided by the total count 
<a name="l5151"><span class="ln">5151 </span></a>                    (total weight), then divided by the volume of its associated bin. 
<a name="l5152"><span class="ln">5152 </span></a>Returns: 
<a name="l5153"><span class="ln">5153 </span></a>    hist (Tensor): N-dimensional Tensor containing the values of the histogram. 
<a name="l5154"><span class="ln">5154 </span></a>    bin_edges(Tensor[]): sequence of N 1D Tensors containing the bin edges. 
<a name="l5155"><span class="ln">5155 </span></a> 
<a name="l5156"><span class="ln">5156 </span></a>Example:: 
<a name="l5157"><span class="ln">5157 </span></a> 
<a name="l5158"><span class="ln">5158 </span></a>    &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 1.], [1., 0.], [2., 0.], [2., 2.]]), bins=[3, 3], 
<a name="l5159"><span class="ln">5159 </span></a>    ...                   weight=torch.tensor([1., 2., 4., 8.])) 
<a name="l5160"><span class="ln">5160 </span></a>        torch.return_types.histogramdd( 
<a name="l5161"><span class="ln">5161 </span></a>            hist=tensor([[0., 1., 0.], 
<a name="l5162"><span class="ln">5162 </span></a>                         [2., 0., 0.], 
<a name="l5163"><span class="ln">5163 </span></a>                         [4., 0., 8.]]), 
<a name="l5164"><span class="ln">5164 </span></a>            bin_edges=(tensor([0.0000, 0.6667, 1.3333, 2.0000]), 
<a name="l5165"><span class="ln">5165 </span></a>                       tensor([0.0000, 0.6667, 1.3333, 2.0000]))) 
<a name="l5166"><span class="ln">5166 </span></a> 
<a name="l5167"><span class="ln">5167 </span></a>    &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 0.], [1., 1.], [2., 2.]]), bins=[2, 2], 
<a name="l5168"><span class="ln">5168 </span></a>    ...                   range=[0., 1., 0., 1.], density=True) 
<a name="l5169"><span class="ln">5169 </span></a>        torch.return_types.histogramdd( 
<a name="l5170"><span class="ln">5170 </span></a>           hist=tensor([[2., 0.], 
<a name="l5171"><span class="ln">5171 </span></a>                        [0., 2.]]), 
<a name="l5172"><span class="ln">5172 </span></a>           bin_edges=(tensor([0.0000, 0.5000, 1.0000]), 
<a name="l5173"><span class="ln">5173 </span></a>                      tensor([0.0000, 0.5000, 1.0000]))) 
<a name="l5174"><span class="ln">5174 </span></a> 
<a name="l5175"><span class="ln">5175 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5176"><span class="ln">5176 </span></a><span class="s3">)</span>
<a name="l5177"><span class="ln">5177 </span></a><span class="s0"># TODO: Fix via https://github.com/pytorch/pytorch/issues/75798</span>
<a name="l5178"><span class="ln">5178 </span></a><span class="s1">torch</span><span class="s3">.</span><span class="s1">histogramdd</span><span class="s3">.</span><span class="s1">__module__ </span><span class="s2">= </span><span class="s4">&quot;torch&quot;</span>
<a name="l5179"><span class="ln">5179 </span></a>
<a name="l5180"><span class="ln">5180 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5181"><span class="ln">5181 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">hypot</span><span class="s3">,</span>
<a name="l5182"><span class="ln">5182 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5183"><span class="ln">5183 </span></a>hypot(input, other, *, out=None) -&gt; Tensor 
<a name="l5184"><span class="ln">5184 </span></a> 
<a name="l5185"><span class="ln">5185 </span></a>Given the legs of a right triangle, return its hypotenuse. 
<a name="l5186"><span class="ln">5186 </span></a> 
<a name="l5187"><span class="ln">5187 </span></a>.. math:: 
<a name="l5188"><span class="ln">5188 </span></a>    \text{out}_{i} = \sqrt{\text{input}_{i}^{2} + \text{other}_{i}^{2}} 
<a name="l5189"><span class="ln">5189 </span></a> 
<a name="l5190"><span class="ln">5190 </span></a>The shapes of ``input`` and ``other`` must be 
<a name="l5191"><span class="ln">5191 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l5192"><span class="ln">5192 </span></a>&quot;&quot;&quot;</span>
<a name="l5193"><span class="ln">5193 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5194"><span class="ln">5194 </span></a>Args: 
<a name="l5195"><span class="ln">5195 </span></a>    input (Tensor): the first input tensor 
<a name="l5196"><span class="ln">5196 </span></a>    other (Tensor): the second input tensor 
<a name="l5197"><span class="ln">5197 </span></a> 
<a name="l5198"><span class="ln">5198 </span></a>Keyword args: 
<a name="l5199"><span class="ln">5199 </span></a>    {out} 
<a name="l5200"><span class="ln">5200 </span></a> 
<a name="l5201"><span class="ln">5201 </span></a>Example:: 
<a name="l5202"><span class="ln">5202 </span></a> 
<a name="l5203"><span class="ln">5203 </span></a>    &gt;&gt;&gt; a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0])) 
<a name="l5204"><span class="ln">5204 </span></a>    tensor([5.0000, 5.6569, 6.4031]) 
<a name="l5205"><span class="ln">5205 </span></a> 
<a name="l5206"><span class="ln">5206 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5207"><span class="ln">5207 </span></a><span class="s3">)</span>
<a name="l5208"><span class="ln">5208 </span></a>
<a name="l5209"><span class="ln">5209 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5210"><span class="ln">5210 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">i0</span><span class="s3">,</span>
<a name="l5211"><span class="ln">5211 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5212"><span class="ln">5212 </span></a>i0(input, *, out=None) -&gt; Tensor 
<a name="l5213"><span class="ln">5213 </span></a> 
<a name="l5214"><span class="ln">5214 </span></a>Alias for :func:`torch.special.i0`. 
<a name="l5215"><span class="ln">5215 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5216"><span class="ln">5216 </span></a><span class="s3">)</span>
<a name="l5217"><span class="ln">5217 </span></a>
<a name="l5218"><span class="ln">5218 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5219"><span class="ln">5219 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">igamma</span><span class="s3">,</span>
<a name="l5220"><span class="ln">5220 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5221"><span class="ln">5221 </span></a>igamma(input, other, *, out=None) -&gt; Tensor 
<a name="l5222"><span class="ln">5222 </span></a> 
<a name="l5223"><span class="ln">5223 </span></a>Alias for :func:`torch.special.gammainc`. 
<a name="l5224"><span class="ln">5224 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5225"><span class="ln">5225 </span></a><span class="s3">)</span>
<a name="l5226"><span class="ln">5226 </span></a>
<a name="l5227"><span class="ln">5227 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5228"><span class="ln">5228 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">igammac</span><span class="s3">,</span>
<a name="l5229"><span class="ln">5229 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5230"><span class="ln">5230 </span></a>igammac(input, other, *, out=None) -&gt; Tensor 
<a name="l5231"><span class="ln">5231 </span></a> 
<a name="l5232"><span class="ln">5232 </span></a>Alias for :func:`torch.special.gammaincc`. 
<a name="l5233"><span class="ln">5233 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5234"><span class="ln">5234 </span></a><span class="s3">)</span>
<a name="l5235"><span class="ln">5235 </span></a>
<a name="l5236"><span class="ln">5236 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5237"><span class="ln">5237 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">index_select</span><span class="s3">,</span>
<a name="l5238"><span class="ln">5238 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5239"><span class="ln">5239 </span></a>index_select(input, dim, index, *, out=None) -&gt; Tensor 
<a name="l5240"><span class="ln">5240 </span></a> 
<a name="l5241"><span class="ln">5241 </span></a>Returns a new tensor which indexes the :attr:`input` tensor along dimension 
<a name="l5242"><span class="ln">5242 </span></a>:attr:`dim` using the entries in :attr:`index` which is a `LongTensor`. 
<a name="l5243"><span class="ln">5243 </span></a> 
<a name="l5244"><span class="ln">5244 </span></a>The returned tensor has the same number of dimensions as the original tensor 
<a name="l5245"><span class="ln">5245 </span></a>(:attr:`input`).  The :attr:`dim`\ th dimension has the same size as the length 
<a name="l5246"><span class="ln">5246 </span></a>of :attr:`index`; other dimensions have the same size as in the original tensor. 
<a name="l5247"><span class="ln">5247 </span></a> 
<a name="l5248"><span class="ln">5248 </span></a>.. note:: The returned tensor does **not** use the same storage as the original 
<a name="l5249"><span class="ln">5249 </span></a>          tensor.  If :attr:`out` has a different shape than expected, we 
<a name="l5250"><span class="ln">5250 </span></a>          silently change it to the correct shape, reallocating the underlying 
<a name="l5251"><span class="ln">5251 </span></a>          storage if necessary. 
<a name="l5252"><span class="ln">5252 </span></a> 
<a name="l5253"><span class="ln">5253 </span></a>Args: 
<a name="l5254"><span class="ln">5254 </span></a>    {input} 
<a name="l5255"><span class="ln">5255 </span></a>    dim (int): the dimension in which we index 
<a name="l5256"><span class="ln">5256 </span></a>    index (IntTensor or LongTensor): the 1-D tensor containing the indices to index 
<a name="l5257"><span class="ln">5257 </span></a> 
<a name="l5258"><span class="ln">5258 </span></a>Keyword args: 
<a name="l5259"><span class="ln">5259 </span></a>    {out} 
<a name="l5260"><span class="ln">5260 </span></a> 
<a name="l5261"><span class="ln">5261 </span></a>Example:: 
<a name="l5262"><span class="ln">5262 </span></a> 
<a name="l5263"><span class="ln">5263 </span></a>    &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l5264"><span class="ln">5264 </span></a>    &gt;&gt;&gt; x 
<a name="l5265"><span class="ln">5265 </span></a>    tensor([[ 0.1427,  0.0231, -0.5414, -1.0009], 
<a name="l5266"><span class="ln">5266 </span></a>            [-0.4664,  0.2647, -0.1228, -1.1068], 
<a name="l5267"><span class="ln">5267 </span></a>            [-1.1734, -0.6571,  0.7230, -0.6004]]) 
<a name="l5268"><span class="ln">5268 </span></a>    &gt;&gt;&gt; indices = torch.tensor([0, 2]) 
<a name="l5269"><span class="ln">5269 </span></a>    &gt;&gt;&gt; torch.index_select(x, 0, indices) 
<a name="l5270"><span class="ln">5270 </span></a>    tensor([[ 0.1427,  0.0231, -0.5414, -1.0009], 
<a name="l5271"><span class="ln">5271 </span></a>            [-1.1734, -0.6571,  0.7230, -0.6004]]) 
<a name="l5272"><span class="ln">5272 </span></a>    &gt;&gt;&gt; torch.index_select(x, 1, indices) 
<a name="l5273"><span class="ln">5273 </span></a>    tensor([[ 0.1427, -0.5414], 
<a name="l5274"><span class="ln">5274 </span></a>            [-0.4664, -0.1228], 
<a name="l5275"><span class="ln">5275 </span></a>            [-1.1734,  0.7230]]) 
<a name="l5276"><span class="ln">5276 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5277"><span class="ln">5277 </span></a><span class="s3">)</span>
<a name="l5278"><span class="ln">5278 </span></a>
<a name="l5279"><span class="ln">5279 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5280"><span class="ln">5280 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">inverse</span><span class="s3">,</span>
<a name="l5281"><span class="ln">5281 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5282"><span class="ln">5282 </span></a>inverse(input, *, out=None) -&gt; Tensor 
<a name="l5283"><span class="ln">5283 </span></a> 
<a name="l5284"><span class="ln">5284 </span></a>Alias for :func:`torch.linalg.inv` 
<a name="l5285"><span class="ln">5285 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5286"><span class="ln">5286 </span></a><span class="s3">)</span>
<a name="l5287"><span class="ln">5287 </span></a>
<a name="l5288"><span class="ln">5288 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5289"><span class="ln">5289 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isin</span><span class="s3">,</span>
<a name="l5290"><span class="ln">5290 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5291"><span class="ln">5291 </span></a>isin(elements, test_elements, *, assume_unique=False, invert=False) -&gt; Tensor 
<a name="l5292"><span class="ln">5292 </span></a> 
<a name="l5293"><span class="ln">5293 </span></a>Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns 
<a name="l5294"><span class="ln">5294 </span></a>a boolean tensor of the same shape as :attr:`elements` that is True for elements 
<a name="l5295"><span class="ln">5295 </span></a>in :attr:`test_elements` and False otherwise. 
<a name="l5296"><span class="ln">5296 </span></a> 
<a name="l5297"><span class="ln">5297 </span></a>.. note:: 
<a name="l5298"><span class="ln">5298 </span></a>    One of :attr:`elements` or :attr:`test_elements` can be a scalar, but not both. 
<a name="l5299"><span class="ln">5299 </span></a> 
<a name="l5300"><span class="ln">5300 </span></a>Args: 
<a name="l5301"><span class="ln">5301 </span></a>    elements (Tensor or Scalar): Input elements 
<a name="l5302"><span class="ln">5302 </span></a>    test_elements (Tensor or Scalar): Values against which to test for each input element 
<a name="l5303"><span class="ln">5303 </span></a>    assume_unique (bool, optional): If True, assumes both :attr:`elements` and 
<a name="l5304"><span class="ln">5304 </span></a>        :attr:`test_elements` contain unique elements, which can speed up the 
<a name="l5305"><span class="ln">5305 </span></a>        calculation. Default: False 
<a name="l5306"><span class="ln">5306 </span></a>    invert (bool, optional): If True, inverts the boolean return tensor, resulting in True 
<a name="l5307"><span class="ln">5307 </span></a>        values for elements *not* in :attr:`test_elements`. Default: False 
<a name="l5308"><span class="ln">5308 </span></a> 
<a name="l5309"><span class="ln">5309 </span></a>Returns: 
<a name="l5310"><span class="ln">5310 </span></a>    A boolean tensor of the same shape as :attr:`elements` that is True for elements in 
<a name="l5311"><span class="ln">5311 </span></a>    :attr:`test_elements` and False otherwise 
<a name="l5312"><span class="ln">5312 </span></a> 
<a name="l5313"><span class="ln">5313 </span></a>Example: 
<a name="l5314"><span class="ln">5314 </span></a>    &gt;&gt;&gt; torch.isin(torch.tensor([[1, 2], [3, 4]]), torch.tensor([2, 3])) 
<a name="l5315"><span class="ln">5315 </span></a>    tensor([[False,  True], 
<a name="l5316"><span class="ln">5316 </span></a>            [ True, False]]) 
<a name="l5317"><span class="ln">5317 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5318"><span class="ln">5318 </span></a><span class="s3">)</span>
<a name="l5319"><span class="ln">5319 </span></a>
<a name="l5320"><span class="ln">5320 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5321"><span class="ln">5321 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isinf</span><span class="s3">,</span>
<a name="l5322"><span class="ln">5322 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5323"><span class="ln">5323 </span></a>isinf(input) -&gt; Tensor 
<a name="l5324"><span class="ln">5324 </span></a> 
<a name="l5325"><span class="ln">5325 </span></a>Tests if each element of :attr:`input` is infinite 
<a name="l5326"><span class="ln">5326 </span></a>(positive or negative infinity) or not. 
<a name="l5327"><span class="ln">5327 </span></a> 
<a name="l5328"><span class="ln">5328 </span></a>.. note:: 
<a name="l5329"><span class="ln">5329 </span></a>    Complex values are infinite when their real or imaginary part is 
<a name="l5330"><span class="ln">5330 </span></a>    infinite. 
<a name="l5331"><span class="ln">5331 </span></a> 
<a name="l5332"><span class="ln">5332 </span></a>Args: 
<a name="l5333"><span class="ln">5333 </span></a>    {input} 
<a name="l5334"><span class="ln">5334 </span></a> 
<a name="l5335"><span class="ln">5335 </span></a>Returns: 
<a name="l5336"><span class="ln">5336 </span></a>    A boolean tensor that is True where :attr:`input` is infinite and False elsewhere 
<a name="l5337"><span class="ln">5337 </span></a> 
<a name="l5338"><span class="ln">5338 </span></a>Example:: 
<a name="l5339"><span class="ln">5339 </span></a> 
<a name="l5340"><span class="ln">5340 </span></a>    &gt;&gt;&gt; torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) 
<a name="l5341"><span class="ln">5341 </span></a>    tensor([False,  True,  False,  True,  False]) 
<a name="l5342"><span class="ln">5342 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5343"><span class="ln">5343 </span></a><span class="s3">)</span>
<a name="l5344"><span class="ln">5344 </span></a>
<a name="l5345"><span class="ln">5345 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5346"><span class="ln">5346 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isposinf</span><span class="s3">,</span>
<a name="l5347"><span class="ln">5347 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5348"><span class="ln">5348 </span></a>isposinf(input, *, out=None) -&gt; Tensor 
<a name="l5349"><span class="ln">5349 </span></a>Tests if each element of :attr:`input` is positive infinity or not. 
<a name="l5350"><span class="ln">5350 </span></a> 
<a name="l5351"><span class="ln">5351 </span></a>Args: 
<a name="l5352"><span class="ln">5352 </span></a>  {input} 
<a name="l5353"><span class="ln">5353 </span></a> 
<a name="l5354"><span class="ln">5354 </span></a>Keyword args: 
<a name="l5355"><span class="ln">5355 </span></a>  {out} 
<a name="l5356"><span class="ln">5356 </span></a> 
<a name="l5357"><span class="ln">5357 </span></a>Example:: 
<a name="l5358"><span class="ln">5358 </span></a> 
<a name="l5359"><span class="ln">5359 </span></a>    &gt;&gt;&gt; a = torch.tensor([-float('inf'), float('inf'), 1.2]) 
<a name="l5360"><span class="ln">5360 </span></a>    &gt;&gt;&gt; torch.isposinf(a) 
<a name="l5361"><span class="ln">5361 </span></a>    tensor([False,  True, False]) 
<a name="l5362"><span class="ln">5362 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5363"><span class="ln">5363 </span></a><span class="s3">)</span>
<a name="l5364"><span class="ln">5364 </span></a>
<a name="l5365"><span class="ln">5365 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5366"><span class="ln">5366 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isneginf</span><span class="s3">,</span>
<a name="l5367"><span class="ln">5367 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5368"><span class="ln">5368 </span></a>isneginf(input, *, out=None) -&gt; Tensor 
<a name="l5369"><span class="ln">5369 </span></a>Tests if each element of :attr:`input` is negative infinity or not. 
<a name="l5370"><span class="ln">5370 </span></a> 
<a name="l5371"><span class="ln">5371 </span></a>Args: 
<a name="l5372"><span class="ln">5372 </span></a>  {input} 
<a name="l5373"><span class="ln">5373 </span></a> 
<a name="l5374"><span class="ln">5374 </span></a>Keyword args: 
<a name="l5375"><span class="ln">5375 </span></a>  {out} 
<a name="l5376"><span class="ln">5376 </span></a> 
<a name="l5377"><span class="ln">5377 </span></a>Example:: 
<a name="l5378"><span class="ln">5378 </span></a> 
<a name="l5379"><span class="ln">5379 </span></a>    &gt;&gt;&gt; a = torch.tensor([-float('inf'), float('inf'), 1.2]) 
<a name="l5380"><span class="ln">5380 </span></a>    &gt;&gt;&gt; torch.isneginf(a) 
<a name="l5381"><span class="ln">5381 </span></a>    tensor([ True, False, False]) 
<a name="l5382"><span class="ln">5382 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5383"><span class="ln">5383 </span></a><span class="s3">)</span>
<a name="l5384"><span class="ln">5384 </span></a>
<a name="l5385"><span class="ln">5385 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5386"><span class="ln">5386 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isclose</span><span class="s3">,</span>
<a name="l5387"><span class="ln">5387 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5388"><span class="ln">5388 </span></a>isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) -&gt; Tensor 
<a name="l5389"><span class="ln">5389 </span></a> 
<a name="l5390"><span class="ln">5390 </span></a>Returns a new tensor with boolean elements representing if each element of 
<a name="l5391"><span class="ln">5391 </span></a>:attr:`input` is &quot;close&quot; to the corresponding element of :attr:`other`. 
<a name="l5392"><span class="ln">5392 </span></a>Closeness is defined as: 
<a name="l5393"><span class="ln">5393 </span></a> 
<a name="l5394"><span class="ln">5394 </span></a>.. math:: 
<a name="l5395"><span class="ln">5395 </span></a>    \lvert \text{input}_i - \text{other}_i \rvert \leq \texttt{rtol} \times \lvert \text{other}_i \rvert + \texttt{atol} 
<a name="l5396"><span class="ln">5396 </span></a>&quot;&quot;&quot;</span>
<a name="l5397"><span class="ln">5397 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5398"><span class="ln">5398 </span></a> 
<a name="l5399"><span class="ln">5399 </span></a>where :attr:`input` and :attr:`other` are finite. Where :attr:`input` 
<a name="l5400"><span class="ln">5400 </span></a>and/or :attr:`other` are nonfinite they are close if and only if 
<a name="l5401"><span class="ln">5401 </span></a>they are equal, with NaNs being considered equal to each other when 
<a name="l5402"><span class="ln">5402 </span></a>:attr:`equal_nan` is True. 
<a name="l5403"><span class="ln">5403 </span></a> 
<a name="l5404"><span class="ln">5404 </span></a>Args: 
<a name="l5405"><span class="ln">5405 </span></a>    input (Tensor): first tensor to compare 
<a name="l5406"><span class="ln">5406 </span></a>    other (Tensor): second tensor to compare 
<a name="l5407"><span class="ln">5407 </span></a>    rtol (float, optional): relative tolerance. Default: 1e-05 
<a name="l5408"><span class="ln">5408 </span></a>    atol (float, optional): absolute tolerance. Default: 1e-08 
<a name="l5409"><span class="ln">5409 </span></a>    equal_nan (bool, optional): if ``True``, then two ``NaN`` s will be considered equal. Default: ``False`` 
<a name="l5410"><span class="ln">5410 </span></a> 
<a name="l5411"><span class="ln">5411 </span></a>Examples:: 
<a name="l5412"><span class="ln">5412 </span></a> 
<a name="l5413"><span class="ln">5413 </span></a>    &gt;&gt;&gt; torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4))) 
<a name="l5414"><span class="ln">5414 </span></a>    tensor([ True, False, False]) 
<a name="l5415"><span class="ln">5415 </span></a>    &gt;&gt;&gt; torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5) 
<a name="l5416"><span class="ln">5416 </span></a>    tensor([True, True]) 
<a name="l5417"><span class="ln">5417 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5418"><span class="ln">5418 </span></a><span class="s3">)</span>
<a name="l5419"><span class="ln">5419 </span></a>
<a name="l5420"><span class="ln">5420 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5421"><span class="ln">5421 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isfinite</span><span class="s3">,</span>
<a name="l5422"><span class="ln">5422 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5423"><span class="ln">5423 </span></a>isfinite(input) -&gt; Tensor 
<a name="l5424"><span class="ln">5424 </span></a> 
<a name="l5425"><span class="ln">5425 </span></a>Returns a new tensor with boolean elements representing if each element is `finite` or not. 
<a name="l5426"><span class="ln">5426 </span></a> 
<a name="l5427"><span class="ln">5427 </span></a>Real values are finite when they are not NaN, negative infinity, or infinity. 
<a name="l5428"><span class="ln">5428 </span></a>Complex values are finite when both their real and imaginary parts are finite. 
<a name="l5429"><span class="ln">5429 </span></a> 
<a name="l5430"><span class="ln">5430 </span></a>Args: 
<a name="l5431"><span class="ln">5431 </span></a>    {input} 
<a name="l5432"><span class="ln">5432 </span></a> 
<a name="l5433"><span class="ln">5433 </span></a>Returns: 
<a name="l5434"><span class="ln">5434 </span></a>    A boolean tensor that is True where :attr:`input` is finite and False elsewhere 
<a name="l5435"><span class="ln">5435 </span></a> 
<a name="l5436"><span class="ln">5436 </span></a>Example:: 
<a name="l5437"><span class="ln">5437 </span></a> 
<a name="l5438"><span class="ln">5438 </span></a>    &gt;&gt;&gt; torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) 
<a name="l5439"><span class="ln">5439 </span></a>    tensor([True,  False,  True,  False,  False]) 
<a name="l5440"><span class="ln">5440 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5441"><span class="ln">5441 </span></a><span class="s3">)</span>
<a name="l5442"><span class="ln">5442 </span></a>
<a name="l5443"><span class="ln">5443 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5444"><span class="ln">5444 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isnan</span><span class="s3">,</span>
<a name="l5445"><span class="ln">5445 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5446"><span class="ln">5446 </span></a>isnan(input) -&gt; Tensor 
<a name="l5447"><span class="ln">5447 </span></a> 
<a name="l5448"><span class="ln">5448 </span></a>Returns a new tensor with boolean elements representing if each element of :attr:`input` 
<a name="l5449"><span class="ln">5449 </span></a>is NaN or not. Complex values are considered NaN when either their real 
<a name="l5450"><span class="ln">5450 </span></a>and/or imaginary part is NaN. 
<a name="l5451"><span class="ln">5451 </span></a> 
<a name="l5452"><span class="ln">5452 </span></a>Arguments: 
<a name="l5453"><span class="ln">5453 </span></a>    {input} 
<a name="l5454"><span class="ln">5454 </span></a> 
<a name="l5455"><span class="ln">5455 </span></a>Returns: 
<a name="l5456"><span class="ln">5456 </span></a>    A boolean tensor that is True where :attr:`input` is NaN and False elsewhere 
<a name="l5457"><span class="ln">5457 </span></a> 
<a name="l5458"><span class="ln">5458 </span></a>Example:: 
<a name="l5459"><span class="ln">5459 </span></a> 
<a name="l5460"><span class="ln">5460 </span></a>    &gt;&gt;&gt; torch.isnan(torch.tensor([1, float('nan'), 2])) 
<a name="l5461"><span class="ln">5461 </span></a>    tensor([False, True, False]) 
<a name="l5462"><span class="ln">5462 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5463"><span class="ln">5463 </span></a><span class="s3">)</span>
<a name="l5464"><span class="ln">5464 </span></a>
<a name="l5465"><span class="ln">5465 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5466"><span class="ln">5466 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">isreal</span><span class="s3">,</span>
<a name="l5467"><span class="ln">5467 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5468"><span class="ln">5468 </span></a>isreal(input) -&gt; Tensor 
<a name="l5469"><span class="ln">5469 </span></a> 
<a name="l5470"><span class="ln">5470 </span></a>Returns a new tensor with boolean elements representing if each element of :attr:`input` is real-valued or not. 
<a name="l5471"><span class="ln">5471 </span></a>All real-valued types are considered real. Complex values are considered real when their imaginary part is 0. 
<a name="l5472"><span class="ln">5472 </span></a> 
<a name="l5473"><span class="ln">5473 </span></a>Arguments: 
<a name="l5474"><span class="ln">5474 </span></a>    {input} 
<a name="l5475"><span class="ln">5475 </span></a> 
<a name="l5476"><span class="ln">5476 </span></a>Returns: 
<a name="l5477"><span class="ln">5477 </span></a>    A boolean tensor that is True where :attr:`input` is real and False elsewhere 
<a name="l5478"><span class="ln">5478 </span></a> 
<a name="l5479"><span class="ln">5479 </span></a>Example:: 
<a name="l5480"><span class="ln">5480 </span></a> 
<a name="l5481"><span class="ln">5481 </span></a>    &gt;&gt;&gt; torch.isreal(torch.tensor([1, 1+1j, 2+0j])) 
<a name="l5482"><span class="ln">5482 </span></a>    tensor([True, False, True]) 
<a name="l5483"><span class="ln">5483 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5484"><span class="ln">5484 </span></a><span class="s3">)</span>
<a name="l5485"><span class="ln">5485 </span></a>
<a name="l5486"><span class="ln">5486 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5487"><span class="ln">5487 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">is_floating_point</span><span class="s3">,</span>
<a name="l5488"><span class="ln">5488 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5489"><span class="ln">5489 </span></a>is_floating_point(input) -&gt; (bool) 
<a name="l5490"><span class="ln">5490 </span></a> 
<a name="l5491"><span class="ln">5491 </span></a>Returns True if the data type of :attr:`input` is a floating point data type i.e., 
<a name="l5492"><span class="ln">5492 </span></a>one of ``torch.float64``, ``torch.float32``, ``torch.float16``, and ``torch.bfloat16``. 
<a name="l5493"><span class="ln">5493 </span></a> 
<a name="l5494"><span class="ln">5494 </span></a>Args: 
<a name="l5495"><span class="ln">5495 </span></a>    {input} 
<a name="l5496"><span class="ln">5496 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5497"><span class="ln">5497 </span></a><span class="s3">)</span>
<a name="l5498"><span class="ln">5498 </span></a>
<a name="l5499"><span class="ln">5499 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5500"><span class="ln">5500 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">is_complex</span><span class="s3">,</span>
<a name="l5501"><span class="ln">5501 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5502"><span class="ln">5502 </span></a>is_complex(input) -&gt; (bool) 
<a name="l5503"><span class="ln">5503 </span></a> 
<a name="l5504"><span class="ln">5504 </span></a>Returns True if the data type of :attr:`input` is a complex data type i.e., 
<a name="l5505"><span class="ln">5505 </span></a>one of ``torch.complex64``, and ``torch.complex128``. 
<a name="l5506"><span class="ln">5506 </span></a> 
<a name="l5507"><span class="ln">5507 </span></a>Args: 
<a name="l5508"><span class="ln">5508 </span></a>    {input} 
<a name="l5509"><span class="ln">5509 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5510"><span class="ln">5510 </span></a><span class="s3">)</span>
<a name="l5511"><span class="ln">5511 </span></a>
<a name="l5512"><span class="ln">5512 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5513"><span class="ln">5513 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">is_grad_enabled</span><span class="s3">,</span>
<a name="l5514"><span class="ln">5514 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5515"><span class="ln">5515 </span></a>is_grad_enabled() -&gt; (bool) 
<a name="l5516"><span class="ln">5516 </span></a> 
<a name="l5517"><span class="ln">5517 </span></a>Returns True if grad mode is currently enabled. 
<a name="l5518"><span class="ln">5518 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5519"><span class="ln">5519 </span></a><span class="s3">)</span>
<a name="l5520"><span class="ln">5520 </span></a>
<a name="l5521"><span class="ln">5521 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5522"><span class="ln">5522 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">is_inference_mode_enabled</span><span class="s3">,</span>
<a name="l5523"><span class="ln">5523 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5524"><span class="ln">5524 </span></a>is_inference_mode_enabled() -&gt; (bool) 
<a name="l5525"><span class="ln">5525 </span></a> 
<a name="l5526"><span class="ln">5526 </span></a>Returns True if inference mode is currently enabled. 
<a name="l5527"><span class="ln">5527 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5528"><span class="ln">5528 </span></a><span class="s3">)</span>
<a name="l5529"><span class="ln">5529 </span></a>
<a name="l5530"><span class="ln">5530 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5531"><span class="ln">5531 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">is_inference</span><span class="s3">,</span>
<a name="l5532"><span class="ln">5532 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5533"><span class="ln">5533 </span></a>is_inference(input) -&gt; (bool) 
<a name="l5534"><span class="ln">5534 </span></a> 
<a name="l5535"><span class="ln">5535 </span></a>Returns True if :attr:`input` is an inference tensor. 
<a name="l5536"><span class="ln">5536 </span></a> 
<a name="l5537"><span class="ln">5537 </span></a>A non-view tensor is an inference tensor if and only if it was 
<a name="l5538"><span class="ln">5538 </span></a>allocated during inference mode. A view tensor is an inference 
<a name="l5539"><span class="ln">5539 </span></a>tensor if and only if the tensor it is a view of is an inference tensor. 
<a name="l5540"><span class="ln">5540 </span></a> 
<a name="l5541"><span class="ln">5541 </span></a>For details on inference mode please see 
<a name="l5542"><span class="ln">5542 </span></a>`Inference Mode &lt;https://pytorch.org/cppdocs/notes/inference_mode.html&gt;`_. 
<a name="l5543"><span class="ln">5543 </span></a> 
<a name="l5544"><span class="ln">5544 </span></a>Args: 
<a name="l5545"><span class="ln">5545 </span></a>    {input} 
<a name="l5546"><span class="ln">5546 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5547"><span class="ln">5547 </span></a><span class="s3">)</span>
<a name="l5548"><span class="ln">5548 </span></a>
<a name="l5549"><span class="ln">5549 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5550"><span class="ln">5550 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">is_conj</span><span class="s3">,</span>
<a name="l5551"><span class="ln">5551 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5552"><span class="ln">5552 </span></a>is_conj(input) -&gt; (bool) 
<a name="l5553"><span class="ln">5553 </span></a> 
<a name="l5554"><span class="ln">5554 </span></a>Returns True if the :attr:`input` is a conjugated tensor, i.e. its conjugate bit is set to `True`. 
<a name="l5555"><span class="ln">5555 </span></a> 
<a name="l5556"><span class="ln">5556 </span></a>Args: 
<a name="l5557"><span class="ln">5557 </span></a>    {input} 
<a name="l5558"><span class="ln">5558 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5559"><span class="ln">5559 </span></a><span class="s3">)</span>
<a name="l5560"><span class="ln">5560 </span></a>
<a name="l5561"><span class="ln">5561 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5562"><span class="ln">5562 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">is_nonzero</span><span class="s3">,</span>
<a name="l5563"><span class="ln">5563 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5564"><span class="ln">5564 </span></a>is_nonzero(input) -&gt; (bool) 
<a name="l5565"><span class="ln">5565 </span></a> 
<a name="l5566"><span class="ln">5566 </span></a>Returns True if the :attr:`input` is a single element tensor which is not equal to zero 
<a name="l5567"><span class="ln">5567 </span></a>after type conversions. 
<a name="l5568"><span class="ln">5568 </span></a>i.e. not equal to ``torch.tensor([0.])`` or ``torch.tensor([0])`` or 
<a name="l5569"><span class="ln">5569 </span></a>``torch.tensor([False])``. 
<a name="l5570"><span class="ln">5570 </span></a>Throws a ``RuntimeError`` if ``torch.numel() != 1`` (even in case 
<a name="l5571"><span class="ln">5571 </span></a>of sparse tensors). 
<a name="l5572"><span class="ln">5572 </span></a> 
<a name="l5573"><span class="ln">5573 </span></a>Args: 
<a name="l5574"><span class="ln">5574 </span></a>    {input} 
<a name="l5575"><span class="ln">5575 </span></a> 
<a name="l5576"><span class="ln">5576 </span></a>Examples:: 
<a name="l5577"><span class="ln">5577 </span></a> 
<a name="l5578"><span class="ln">5578 </span></a>    &gt;&gt;&gt; torch.is_nonzero(torch.tensor([0.])) 
<a name="l5579"><span class="ln">5579 </span></a>    False 
<a name="l5580"><span class="ln">5580 </span></a>    &gt;&gt;&gt; torch.is_nonzero(torch.tensor([1.5])) 
<a name="l5581"><span class="ln">5581 </span></a>    True 
<a name="l5582"><span class="ln">5582 </span></a>    &gt;&gt;&gt; torch.is_nonzero(torch.tensor([False])) 
<a name="l5583"><span class="ln">5583 </span></a>    False 
<a name="l5584"><span class="ln">5584 </span></a>    &gt;&gt;&gt; torch.is_nonzero(torch.tensor([3])) 
<a name="l5585"><span class="ln">5585 </span></a>    True 
<a name="l5586"><span class="ln">5586 </span></a>    &gt;&gt;&gt; torch.is_nonzero(torch.tensor([1, 3, 5])) 
<a name="l5587"><span class="ln">5587 </span></a>    Traceback (most recent call last): 
<a name="l5588"><span class="ln">5588 </span></a>    ... 
<a name="l5589"><span class="ln">5589 </span></a>    RuntimeError: bool value of Tensor with more than one value is ambiguous 
<a name="l5590"><span class="ln">5590 </span></a>    &gt;&gt;&gt; torch.is_nonzero(torch.tensor([])) 
<a name="l5591"><span class="ln">5591 </span></a>    Traceback (most recent call last): 
<a name="l5592"><span class="ln">5592 </span></a>    ... 
<a name="l5593"><span class="ln">5593 </span></a>    RuntimeError: bool value of Tensor with no values is ambiguous 
<a name="l5594"><span class="ln">5594 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5595"><span class="ln">5595 </span></a><span class="s3">)</span>
<a name="l5596"><span class="ln">5596 </span></a>
<a name="l5597"><span class="ln">5597 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5598"><span class="ln">5598 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">kron</span><span class="s3">,</span>
<a name="l5599"><span class="ln">5599 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5600"><span class="ln">5600 </span></a>kron(input, other, *, out=None) -&gt; Tensor 
<a name="l5601"><span class="ln">5601 </span></a> 
<a name="l5602"><span class="ln">5602 </span></a>Computes the Kronecker product, denoted by :math:`\otimes`, of :attr:`input` and :attr:`other`. 
<a name="l5603"><span class="ln">5603 </span></a> 
<a name="l5604"><span class="ln">5604 </span></a>If :attr:`input` is a :math:`(a_0 \times a_1 \times \dots \times a_n)` tensor and :attr:`other` is a 
<a name="l5605"><span class="ln">5605 </span></a>:math:`(b_0 \times b_1 \times \dots \times b_n)` tensor, the result will be a 
<a name="l5606"><span class="ln">5606 </span></a>:math:`(a_0*b_0 \times a_1*b_1 \times \dots \times a_n*b_n)` tensor with the following entries: 
<a name="l5607"><span class="ln">5607 </span></a> 
<a name="l5608"><span class="ln">5608 </span></a>.. math:: 
<a name="l5609"><span class="ln">5609 </span></a>    (\text{input} \otimes \text{other})_{k_0, k_1, \dots, k_n} = 
<a name="l5610"><span class="ln">5610 </span></a>        \text{input}_{i_0, i_1, \dots, i_n} * \text{other}_{j_0, j_1, \dots, j_n}, 
<a name="l5611"><span class="ln">5611 </span></a> 
<a name="l5612"><span class="ln">5612 </span></a>where :math:`k_t = i_t * b_t + j_t` for :math:`0 \leq t \leq n`. 
<a name="l5613"><span class="ln">5613 </span></a>If one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions. 
<a name="l5614"><span class="ln">5614 </span></a> 
<a name="l5615"><span class="ln">5615 </span></a>Supports real-valued and complex-valued inputs. 
<a name="l5616"><span class="ln">5616 </span></a> 
<a name="l5617"><span class="ln">5617 </span></a>.. note:: 
<a name="l5618"><span class="ln">5618 </span></a>    This function generalizes the typical definition of the Kronecker product for two matrices to two tensors, 
<a name="l5619"><span class="ln">5619 </span></a>    as described above. When :attr:`input` is a :math:`(m \times n)` matrix and :attr:`other` is a 
<a name="l5620"><span class="ln">5620 </span></a>    :math:`(p \times q)` matrix, the result will be a :math:`(p*m \times q*n)` block matrix: 
<a name="l5621"><span class="ln">5621 </span></a> 
<a name="l5622"><span class="ln">5622 </span></a>    .. math:: 
<a name="l5623"><span class="ln">5623 </span></a>        \mathbf{A} \otimes \mathbf{B}=\begin{bmatrix} 
<a name="l5624"><span class="ln">5624 </span></a>        a_{11} \mathbf{B} &amp; \cdots &amp; a_{1 n} \mathbf{B} \\ 
<a name="l5625"><span class="ln">5625 </span></a>        \vdots &amp; \ddots &amp; \vdots \\ 
<a name="l5626"><span class="ln">5626 </span></a>        a_{m 1} \mathbf{B} &amp; \cdots &amp; a_{m n} \mathbf{B} \end{bmatrix} 
<a name="l5627"><span class="ln">5627 </span></a> 
<a name="l5628"><span class="ln">5628 </span></a>    where :attr:`input` is :math:`\mathbf{A}` and :attr:`other` is :math:`\mathbf{B}`. 
<a name="l5629"><span class="ln">5629 </span></a> 
<a name="l5630"><span class="ln">5630 </span></a>Arguments: 
<a name="l5631"><span class="ln">5631 </span></a>    input (Tensor) 
<a name="l5632"><span class="ln">5632 </span></a>    other (Tensor) 
<a name="l5633"><span class="ln">5633 </span></a> 
<a name="l5634"><span class="ln">5634 </span></a>Keyword args: 
<a name="l5635"><span class="ln">5635 </span></a>    out (Tensor, optional): The output tensor. Ignored if ``None``. Default: ``None`` 
<a name="l5636"><span class="ln">5636 </span></a> 
<a name="l5637"><span class="ln">5637 </span></a>Examples:: 
<a name="l5638"><span class="ln">5638 </span></a> 
<a name="l5639"><span class="ln">5639 </span></a>    &gt;&gt;&gt; mat1 = torch.eye(2) 
<a name="l5640"><span class="ln">5640 </span></a>    &gt;&gt;&gt; mat2 = torch.ones(2, 2) 
<a name="l5641"><span class="ln">5641 </span></a>    &gt;&gt;&gt; torch.kron(mat1, mat2) 
<a name="l5642"><span class="ln">5642 </span></a>    tensor([[1., 1., 0., 0.], 
<a name="l5643"><span class="ln">5643 </span></a>            [1., 1., 0., 0.], 
<a name="l5644"><span class="ln">5644 </span></a>            [0., 0., 1., 1.], 
<a name="l5645"><span class="ln">5645 </span></a>            [0., 0., 1., 1.]]) 
<a name="l5646"><span class="ln">5646 </span></a> 
<a name="l5647"><span class="ln">5647 </span></a>    &gt;&gt;&gt; mat1 = torch.eye(2) 
<a name="l5648"><span class="ln">5648 </span></a>    &gt;&gt;&gt; mat2 = torch.arange(1, 5).reshape(2, 2) 
<a name="l5649"><span class="ln">5649 </span></a>    &gt;&gt;&gt; torch.kron(mat1, mat2) 
<a name="l5650"><span class="ln">5650 </span></a>    tensor([[1., 2., 0., 0.], 
<a name="l5651"><span class="ln">5651 </span></a>            [3., 4., 0., 0.], 
<a name="l5652"><span class="ln">5652 </span></a>            [0., 0., 1., 2.], 
<a name="l5653"><span class="ln">5653 </span></a>            [0., 0., 3., 4.]]) 
<a name="l5654"><span class="ln">5654 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5655"><span class="ln">5655 </span></a><span class="s3">)</span>
<a name="l5656"><span class="ln">5656 </span></a>
<a name="l5657"><span class="ln">5657 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5658"><span class="ln">5658 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">kthvalue</span><span class="s3">,</span>
<a name="l5659"><span class="ln">5659 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5660"><span class="ln">5660 </span></a>kthvalue(input, k, dim=None, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l5661"><span class="ln">5661 </span></a> 
<a name="l5662"><span class="ln">5662 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` is the :attr:`k` th 
<a name="l5663"><span class="ln">5663 </span></a>smallest element of each row of the :attr:`input` tensor in the given dimension 
<a name="l5664"><span class="ln">5664 </span></a>:attr:`dim`. And ``indices`` is the index location of each element found. 
<a name="l5665"><span class="ln">5665 </span></a> 
<a name="l5666"><span class="ln">5666 </span></a>If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l5667"><span class="ln">5667 </span></a> 
<a name="l5668"><span class="ln">5668 </span></a>If :attr:`keepdim` is ``True``, both the :attr:`values` and :attr:`indices` tensors 
<a name="l5669"><span class="ln">5669 </span></a>are the same size as :attr:`input`, except in the dimension :attr:`dim` where 
<a name="l5670"><span class="ln">5670 </span></a>they are of size 1. Otherwise, :attr:`dim` is squeezed 
<a name="l5671"><span class="ln">5671 </span></a>(see :func:`torch.squeeze`), resulting in both the :attr:`values` and 
<a name="l5672"><span class="ln">5672 </span></a>:attr:`indices` tensors having 1 fewer dimension than the :attr:`input` tensor. 
<a name="l5673"><span class="ln">5673 </span></a> 
<a name="l5674"><span class="ln">5674 </span></a>.. note:: 
<a name="l5675"><span class="ln">5675 </span></a>    When :attr:`input` is a CUDA tensor and there are multiple valid 
<a name="l5676"><span class="ln">5676 </span></a>    :attr:`k` th values, this function may nondeterministically return 
<a name="l5677"><span class="ln">5677 </span></a>    :attr:`indices` for any of them. 
<a name="l5678"><span class="ln">5678 </span></a> 
<a name="l5679"><span class="ln">5679 </span></a>Args: 
<a name="l5680"><span class="ln">5680 </span></a>    {input} 
<a name="l5681"><span class="ln">5681 </span></a>    k (int): k for the k-th smallest element 
<a name="l5682"><span class="ln">5682 </span></a>    dim (int, optional): the dimension to find the kth value along 
<a name="l5683"><span class="ln">5683 </span></a>    {opt_keepdim} 
<a name="l5684"><span class="ln">5684 </span></a> 
<a name="l5685"><span class="ln">5685 </span></a>Keyword args: 
<a name="l5686"><span class="ln">5686 </span></a>    out (tuple, optional): the output tuple of (Tensor, LongTensor) 
<a name="l5687"><span class="ln">5687 </span></a>                           can be optionally given to be used as output buffers 
<a name="l5688"><span class="ln">5688 </span></a> 
<a name="l5689"><span class="ln">5689 </span></a>Example:: 
<a name="l5690"><span class="ln">5690 </span></a> 
<a name="l5691"><span class="ln">5691 </span></a>    &gt;&gt;&gt; x = torch.arange(1., 6.) 
<a name="l5692"><span class="ln">5692 </span></a>    &gt;&gt;&gt; x 
<a name="l5693"><span class="ln">5693 </span></a>    tensor([ 1.,  2.,  3.,  4.,  5.]) 
<a name="l5694"><span class="ln">5694 </span></a>    &gt;&gt;&gt; torch.kthvalue(x, 4) 
<a name="l5695"><span class="ln">5695 </span></a>    torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3)) 
<a name="l5696"><span class="ln">5696 </span></a> 
<a name="l5697"><span class="ln">5697 </span></a>    &gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3) 
<a name="l5698"><span class="ln">5698 </span></a>    &gt;&gt;&gt; x 
<a name="l5699"><span class="ln">5699 </span></a>    tensor([[ 1.,  2.,  3.], 
<a name="l5700"><span class="ln">5700 </span></a>            [ 4.,  5.,  6.]]) 
<a name="l5701"><span class="ln">5701 </span></a>    &gt;&gt;&gt; torch.kthvalue(x, 2, 0, True) 
<a name="l5702"><span class="ln">5702 </span></a>    torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]])) 
<a name="l5703"><span class="ln">5703 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l5704"><span class="ln">5704 </span></a><span class="s3">)</span>
<a name="l5705"><span class="ln">5705 </span></a>
<a name="l5706"><span class="ln">5706 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5707"><span class="ln">5707 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">lcm</span><span class="s3">,</span>
<a name="l5708"><span class="ln">5708 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5709"><span class="ln">5709 </span></a>lcm(input, other, *, out=None) -&gt; Tensor 
<a name="l5710"><span class="ln">5710 </span></a> 
<a name="l5711"><span class="ln">5711 </span></a>Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`. 
<a name="l5712"><span class="ln">5712 </span></a> 
<a name="l5713"><span class="ln">5713 </span></a>Both :attr:`input` and :attr:`other` must have integer types. 
<a name="l5714"><span class="ln">5714 </span></a> 
<a name="l5715"><span class="ln">5715 </span></a>.. note:: 
<a name="l5716"><span class="ln">5716 </span></a>    This defines :math:`lcm(0, 0) = 0` and :math:`lcm(0, a) = 0`. 
<a name="l5717"><span class="ln">5717 </span></a> 
<a name="l5718"><span class="ln">5718 </span></a>Args: 
<a name="l5719"><span class="ln">5719 </span></a>    {input} 
<a name="l5720"><span class="ln">5720 </span></a>    other (Tensor): the second input tensor 
<a name="l5721"><span class="ln">5721 </span></a> 
<a name="l5722"><span class="ln">5722 </span></a>Keyword arguments: 
<a name="l5723"><span class="ln">5723 </span></a>    {out} 
<a name="l5724"><span class="ln">5724 </span></a> 
<a name="l5725"><span class="ln">5725 </span></a>Example:: 
<a name="l5726"><span class="ln">5726 </span></a> 
<a name="l5727"><span class="ln">5727 </span></a>    &gt;&gt;&gt; a = torch.tensor([5, 10, 15]) 
<a name="l5728"><span class="ln">5728 </span></a>    &gt;&gt;&gt; b = torch.tensor([3, 4, 5]) 
<a name="l5729"><span class="ln">5729 </span></a>    &gt;&gt;&gt; torch.lcm(a, b) 
<a name="l5730"><span class="ln">5730 </span></a>    tensor([15, 20, 15]) 
<a name="l5731"><span class="ln">5731 </span></a>    &gt;&gt;&gt; c = torch.tensor([3]) 
<a name="l5732"><span class="ln">5732 </span></a>    &gt;&gt;&gt; torch.lcm(a, c) 
<a name="l5733"><span class="ln">5733 </span></a>    tensor([15, 30, 15]) 
<a name="l5734"><span class="ln">5734 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5735"><span class="ln">5735 </span></a><span class="s3">)</span>
<a name="l5736"><span class="ln">5736 </span></a>
<a name="l5737"><span class="ln">5737 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5738"><span class="ln">5738 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ldexp</span><span class="s3">,</span>
<a name="l5739"><span class="ln">5739 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5740"><span class="ln">5740 </span></a>ldexp(input, other, *, out=None) -&gt; Tensor 
<a name="l5741"><span class="ln">5741 </span></a> 
<a name="l5742"><span class="ln">5742 </span></a>Multiplies :attr:`input` by 2 ** :attr:`other`. 
<a name="l5743"><span class="ln">5743 </span></a> 
<a name="l5744"><span class="ln">5744 </span></a>.. math:: 
<a name="l5745"><span class="ln">5745 </span></a>    \text{{out}}_i = \text{{input}}_i * 2^\text{{other}}_i 
<a name="l5746"><span class="ln">5746 </span></a>&quot;&quot;&quot;</span>
<a name="l5747"><span class="ln">5747 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5748"><span class="ln">5748 </span></a> 
<a name="l5749"><span class="ln">5749 </span></a>Typically this function is used to construct floating point numbers by multiplying 
<a name="l5750"><span class="ln">5750 </span></a>mantissas in :attr:`input` with integral powers of two created from the exponents 
<a name="l5751"><span class="ln">5751 </span></a>in :attr:`other`. 
<a name="l5752"><span class="ln">5752 </span></a> 
<a name="l5753"><span class="ln">5753 </span></a>Args: 
<a name="l5754"><span class="ln">5754 </span></a>    {input} 
<a name="l5755"><span class="ln">5755 </span></a>    other (Tensor): a tensor of exponents, typically integers. 
<a name="l5756"><span class="ln">5756 </span></a> 
<a name="l5757"><span class="ln">5757 </span></a>Keyword args: 
<a name="l5758"><span class="ln">5758 </span></a>    {out} 
<a name="l5759"><span class="ln">5759 </span></a> 
<a name="l5760"><span class="ln">5760 </span></a>Example:: 
<a name="l5761"><span class="ln">5761 </span></a> 
<a name="l5762"><span class="ln">5762 </span></a>    &gt;&gt;&gt; torch.ldexp(torch.tensor([1.]), torch.tensor([1])) 
<a name="l5763"><span class="ln">5763 </span></a>    tensor([2.]) 
<a name="l5764"><span class="ln">5764 </span></a>    &gt;&gt;&gt; torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4])) 
<a name="l5765"><span class="ln">5765 </span></a>    tensor([ 2.,  4.,  8., 16.]) 
<a name="l5766"><span class="ln">5766 </span></a> 
<a name="l5767"><span class="ln">5767 </span></a> 
<a name="l5768"><span class="ln">5768 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5769"><span class="ln">5769 </span></a><span class="s3">)</span>
<a name="l5770"><span class="ln">5770 </span></a>
<a name="l5771"><span class="ln">5771 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5772"><span class="ln">5772 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">le</span><span class="s3">,</span>
<a name="l5773"><span class="ln">5773 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5774"><span class="ln">5774 </span></a>le(input, other, *, out=None) -&gt; Tensor 
<a name="l5775"><span class="ln">5775 </span></a> 
<a name="l5776"><span class="ln">5776 </span></a>Computes :math:`\text{input} \leq \text{other}` element-wise. 
<a name="l5777"><span class="ln">5777 </span></a>&quot;&quot;&quot;</span>
<a name="l5778"><span class="ln">5778 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5779"><span class="ln">5779 </span></a> 
<a name="l5780"><span class="ln">5780 </span></a>The second argument can be a number or a tensor whose shape is 
<a name="l5781"><span class="ln">5781 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l5782"><span class="ln">5782 </span></a> 
<a name="l5783"><span class="ln">5783 </span></a>Args: 
<a name="l5784"><span class="ln">5784 </span></a>    input (Tensor): the tensor to compare 
<a name="l5785"><span class="ln">5785 </span></a>    other (Tensor or Scalar): the tensor or value to compare 
<a name="l5786"><span class="ln">5786 </span></a> 
<a name="l5787"><span class="ln">5787 </span></a>Keyword args: 
<a name="l5788"><span class="ln">5788 </span></a>    {out} 
<a name="l5789"><span class="ln">5789 </span></a> 
<a name="l5790"><span class="ln">5790 </span></a>Returns: 
<a name="l5791"><span class="ln">5791 </span></a>    A boolean tensor that is True where :attr:`input` is less than or equal to 
<a name="l5792"><span class="ln">5792 </span></a>    :attr:`other` and False elsewhere 
<a name="l5793"><span class="ln">5793 </span></a> 
<a name="l5794"><span class="ln">5794 </span></a>Example:: 
<a name="l5795"><span class="ln">5795 </span></a> 
<a name="l5796"><span class="ln">5796 </span></a>    &gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l5797"><span class="ln">5797 </span></a>    tensor([[True, False], [True, True]]) 
<a name="l5798"><span class="ln">5798 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5799"><span class="ln">5799 </span></a><span class="s3">)</span>
<a name="l5800"><span class="ln">5800 </span></a>
<a name="l5801"><span class="ln">5801 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5802"><span class="ln">5802 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">less_equal</span><span class="s3">,</span>
<a name="l5803"><span class="ln">5803 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5804"><span class="ln">5804 </span></a>less_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l5805"><span class="ln">5805 </span></a> 
<a name="l5806"><span class="ln">5806 </span></a>Alias for :func:`torch.le`. 
<a name="l5807"><span class="ln">5807 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5808"><span class="ln">5808 </span></a><span class="s3">)</span>
<a name="l5809"><span class="ln">5809 </span></a>
<a name="l5810"><span class="ln">5810 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5811"><span class="ln">5811 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">lerp</span><span class="s3">,</span>
<a name="l5812"><span class="ln">5812 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5813"><span class="ln">5813 </span></a>lerp(input, end, weight, *, out=None) 
<a name="l5814"><span class="ln">5814 </span></a> 
<a name="l5815"><span class="ln">5815 </span></a>Does a linear interpolation of two tensors :attr:`start` (given by :attr:`input`) and :attr:`end` based 
<a name="l5816"><span class="ln">5816 </span></a>on a scalar or tensor :attr:`weight` and returns the resulting :attr:`out` tensor. 
<a name="l5817"><span class="ln">5817 </span></a> 
<a name="l5818"><span class="ln">5818 </span></a>.. math:: 
<a name="l5819"><span class="ln">5819 </span></a>    \text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i) 
<a name="l5820"><span class="ln">5820 </span></a>&quot;&quot;&quot;</span>
<a name="l5821"><span class="ln">5821 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5822"><span class="ln">5822 </span></a>The shapes of :attr:`start` and :attr:`end` must be 
<a name="l5823"><span class="ln">5823 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;`. If :attr:`weight` is a tensor, then 
<a name="l5824"><span class="ln">5824 </span></a>the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l5825"><span class="ln">5825 </span></a> 
<a name="l5826"><span class="ln">5826 </span></a>Args: 
<a name="l5827"><span class="ln">5827 </span></a>    input (Tensor): the tensor with the starting points 
<a name="l5828"><span class="ln">5828 </span></a>    end (Tensor): the tensor with the ending points 
<a name="l5829"><span class="ln">5829 </span></a>    weight (float or tensor): the weight for the interpolation formula 
<a name="l5830"><span class="ln">5830 </span></a> 
<a name="l5831"><span class="ln">5831 </span></a>Keyword args: 
<a name="l5832"><span class="ln">5832 </span></a>    {out} 
<a name="l5833"><span class="ln">5833 </span></a> 
<a name="l5834"><span class="ln">5834 </span></a>Example:: 
<a name="l5835"><span class="ln">5835 </span></a> 
<a name="l5836"><span class="ln">5836 </span></a>    &gt;&gt;&gt; start = torch.arange(1., 5.) 
<a name="l5837"><span class="ln">5837 </span></a>    &gt;&gt;&gt; end = torch.empty(4).fill_(10) 
<a name="l5838"><span class="ln">5838 </span></a>    &gt;&gt;&gt; start 
<a name="l5839"><span class="ln">5839 </span></a>    tensor([ 1.,  2.,  3.,  4.]) 
<a name="l5840"><span class="ln">5840 </span></a>    &gt;&gt;&gt; end 
<a name="l5841"><span class="ln">5841 </span></a>    tensor([ 10.,  10.,  10.,  10.]) 
<a name="l5842"><span class="ln">5842 </span></a>    &gt;&gt;&gt; torch.lerp(start, end, 0.5) 
<a name="l5843"><span class="ln">5843 </span></a>    tensor([ 5.5000,  6.0000,  6.5000,  7.0000]) 
<a name="l5844"><span class="ln">5844 </span></a>    &gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5)) 
<a name="l5845"><span class="ln">5845 </span></a>    tensor([ 5.5000,  6.0000,  6.5000,  7.0000]) 
<a name="l5846"><span class="ln">5846 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5847"><span class="ln">5847 </span></a><span class="s3">)</span>
<a name="l5848"><span class="ln">5848 </span></a>
<a name="l5849"><span class="ln">5849 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5850"><span class="ln">5850 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">lgamma</span><span class="s3">,</span>
<a name="l5851"><span class="ln">5851 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5852"><span class="ln">5852 </span></a>lgamma(input, *, out=None) -&gt; Tensor 
<a name="l5853"><span class="ln">5853 </span></a> 
<a name="l5854"><span class="ln">5854 </span></a>Computes the natural logarithm of the absolute value of the gamma function on :attr:`input`. 
<a name="l5855"><span class="ln">5855 </span></a> 
<a name="l5856"><span class="ln">5856 </span></a>.. math:: 
<a name="l5857"><span class="ln">5857 </span></a>    \text{out}_{i} = \ln |\Gamma(\text{input}_{i})| 
<a name="l5858"><span class="ln">5858 </span></a>&quot;&quot;&quot;</span>
<a name="l5859"><span class="ln">5859 </span></a>    <span class="s2">+ </span><span class="s4">&quot;&quot;&quot; 
<a name="l5860"><span class="ln">5860 </span></a>Args: 
<a name="l5861"><span class="ln">5861 </span></a>    {input} 
<a name="l5862"><span class="ln">5862 </span></a> 
<a name="l5863"><span class="ln">5863 </span></a>Keyword args: 
<a name="l5864"><span class="ln">5864 </span></a>    {out} 
<a name="l5865"><span class="ln">5865 </span></a> 
<a name="l5866"><span class="ln">5866 </span></a>Example:: 
<a name="l5867"><span class="ln">5867 </span></a> 
<a name="l5868"><span class="ln">5868 </span></a>    &gt;&gt;&gt; a = torch.arange(0.5, 2, 0.5) 
<a name="l5869"><span class="ln">5869 </span></a>    &gt;&gt;&gt; torch.lgamma(a) 
<a name="l5870"><span class="ln">5870 </span></a>    tensor([ 0.5724,  0.0000, -0.1208]) 
<a name="l5871"><span class="ln">5871 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5872"><span class="ln">5872 </span></a><span class="s3">)</span>
<a name="l5873"><span class="ln">5873 </span></a>
<a name="l5874"><span class="ln">5874 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5875"><span class="ln">5875 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">linspace</span><span class="s3">,</span>
<a name="l5876"><span class="ln">5876 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5877"><span class="ln">5877 </span></a>linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l5878"><span class="ln">5878 </span></a> 
<a name="l5879"><span class="ln">5879 </span></a>Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l5880"><span class="ln">5880 </span></a>spaced from :attr:`start` to :attr:`end`, inclusive. That is, the value are: 
<a name="l5881"><span class="ln">5881 </span></a> 
<a name="l5882"><span class="ln">5882 </span></a>.. math:: 
<a name="l5883"><span class="ln">5883 </span></a>    (\text{start}, 
<a name="l5884"><span class="ln">5884 </span></a>    \text{start} + \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l5885"><span class="ln">5885 </span></a>    \ldots, 
<a name="l5886"><span class="ln">5886 </span></a>    \text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l5887"><span class="ln">5887 </span></a>    \text{end}) 
<a name="l5888"><span class="ln">5888 </span></a>&quot;&quot;&quot;</span>
<a name="l5889"><span class="ln">5889 </span></a>    <span class="s2">+ </span><span class="s4">&quot;&quot;&quot; 
<a name="l5890"><span class="ln">5890 </span></a> 
<a name="l5891"><span class="ln">5891 </span></a>From PyTorch 1.11 linspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l5892"><span class="ln">5892 </span></a> 
<a name="l5893"><span class="ln">5893 </span></a>Args: 
<a name="l5894"><span class="ln">5894 </span></a>    start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l5895"><span class="ln">5895 </span></a>    end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l5896"><span class="ln">5896 </span></a>    steps (int): size of the constructed tensor 
<a name="l5897"><span class="ln">5897 </span></a> 
<a name="l5898"><span class="ln">5898 </span></a>Keyword arguments: 
<a name="l5899"><span class="ln">5899 </span></a>    {out} 
<a name="l5900"><span class="ln">5900 </span></a>    dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l5901"><span class="ln">5901 </span></a>        Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l5902"><span class="ln">5902 </span></a>        when both :attr:`start` and :attr:`end` are real, 
<a name="l5903"><span class="ln">5903 </span></a>        and corresponding complex dtype when either is complex. 
<a name="l5904"><span class="ln">5904 </span></a>    {layout} 
<a name="l5905"><span class="ln">5905 </span></a>    {device} 
<a name="l5906"><span class="ln">5906 </span></a>    {requires_grad} 
<a name="l5907"><span class="ln">5907 </span></a> 
<a name="l5908"><span class="ln">5908 </span></a> 
<a name="l5909"><span class="ln">5909 </span></a>Example:: 
<a name="l5910"><span class="ln">5910 </span></a> 
<a name="l5911"><span class="ln">5911 </span></a>    &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 
<a name="l5912"><span class="ln">5912 </span></a>    tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000]) 
<a name="l5913"><span class="ln">5913 </span></a>    &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) 
<a name="l5914"><span class="ln">5914 </span></a>    tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l5915"><span class="ln">5915 </span></a>    &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) 
<a name="l5916"><span class="ln">5916 </span></a>    tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l5917"><span class="ln">5917 </span></a>    &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1) 
<a name="l5918"><span class="ln">5918 </span></a>    tensor([-10.]) 
<a name="l5919"><span class="ln">5919 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l5920"><span class="ln">5920 </span></a><span class="s3">)</span>
<a name="l5921"><span class="ln">5921 </span></a>
<a name="l5922"><span class="ln">5922 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5923"><span class="ln">5923 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">log</span><span class="s3">,</span>
<a name="l5924"><span class="ln">5924 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5925"><span class="ln">5925 </span></a>log(input, *, out=None) -&gt; Tensor 
<a name="l5926"><span class="ln">5926 </span></a> 
<a name="l5927"><span class="ln">5927 </span></a>Returns a new tensor with the natural logarithm of the elements 
<a name="l5928"><span class="ln">5928 </span></a>of :attr:`input`. 
<a name="l5929"><span class="ln">5929 </span></a> 
<a name="l5930"><span class="ln">5930 </span></a>.. math:: 
<a name="l5931"><span class="ln">5931 </span></a>    y_{i} = \log_{e} (x_{i}) 
<a name="l5932"><span class="ln">5932 </span></a>&quot;&quot;&quot;</span>
<a name="l5933"><span class="ln">5933 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5934"><span class="ln">5934 </span></a> 
<a name="l5935"><span class="ln">5935 </span></a>Args: 
<a name="l5936"><span class="ln">5936 </span></a>    {input} 
<a name="l5937"><span class="ln">5937 </span></a> 
<a name="l5938"><span class="ln">5938 </span></a>Keyword args: 
<a name="l5939"><span class="ln">5939 </span></a>    {out} 
<a name="l5940"><span class="ln">5940 </span></a> 
<a name="l5941"><span class="ln">5941 </span></a>Example:: 
<a name="l5942"><span class="ln">5942 </span></a> 
<a name="l5943"><span class="ln">5943 </span></a>    &gt;&gt;&gt; a = torch.rand(5) * 5 
<a name="l5944"><span class="ln">5944 </span></a>    &gt;&gt;&gt; a 
<a name="l5945"><span class="ln">5945 </span></a>    tensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739]) 
<a name="l5946"><span class="ln">5946 </span></a>    &gt;&gt;&gt; torch.log(a) 
<a name="l5947"><span class="ln">5947 </span></a>    tensor([ 1.5637,  1.4640,  0.1952, -1.4226,  1.5204]) 
<a name="l5948"><span class="ln">5948 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5949"><span class="ln">5949 </span></a><span class="s3">)</span>
<a name="l5950"><span class="ln">5950 </span></a>
<a name="l5951"><span class="ln">5951 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5952"><span class="ln">5952 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">log10</span><span class="s3">,</span>
<a name="l5953"><span class="ln">5953 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5954"><span class="ln">5954 </span></a>log10(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l5955"><span class="ln">5955 </span></a> 
<a name="l5956"><span class="ln">5956 </span></a>Returns a new tensor with the logarithm to the base 10 of the elements 
<a name="l5957"><span class="ln">5957 </span></a>of :attr:`input`. 
<a name="l5958"><span class="ln">5958 </span></a> 
<a name="l5959"><span class="ln">5959 </span></a>.. math:: 
<a name="l5960"><span class="ln">5960 </span></a>    y_{i} = \log_{10} (x_{i}) 
<a name="l5961"><span class="ln">5961 </span></a>&quot;&quot;&quot;</span>
<a name="l5962"><span class="ln">5962 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5963"><span class="ln">5963 </span></a> 
<a name="l5964"><span class="ln">5964 </span></a>Args: 
<a name="l5965"><span class="ln">5965 </span></a>    {input} 
<a name="l5966"><span class="ln">5966 </span></a> 
<a name="l5967"><span class="ln">5967 </span></a>Keyword args: 
<a name="l5968"><span class="ln">5968 </span></a>    {out} 
<a name="l5969"><span class="ln">5969 </span></a> 
<a name="l5970"><span class="ln">5970 </span></a>Example:: 
<a name="l5971"><span class="ln">5971 </span></a> 
<a name="l5972"><span class="ln">5972 </span></a>    &gt;&gt;&gt; a = torch.rand(5) 
<a name="l5973"><span class="ln">5973 </span></a>    &gt;&gt;&gt; a 
<a name="l5974"><span class="ln">5974 </span></a>    tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251]) 
<a name="l5975"><span class="ln">5975 </span></a> 
<a name="l5976"><span class="ln">5976 </span></a> 
<a name="l5977"><span class="ln">5977 </span></a>    &gt;&gt;&gt; torch.log10(a) 
<a name="l5978"><span class="ln">5978 </span></a>    tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476]) 
<a name="l5979"><span class="ln">5979 </span></a> 
<a name="l5980"><span class="ln">5980 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5981"><span class="ln">5981 </span></a><span class="s3">)</span>
<a name="l5982"><span class="ln">5982 </span></a>
<a name="l5983"><span class="ln">5983 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l5984"><span class="ln">5984 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">log1p</span><span class="s3">,</span>
<a name="l5985"><span class="ln">5985 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5986"><span class="ln">5986 </span></a>log1p(input, *, out=None) -&gt; Tensor 
<a name="l5987"><span class="ln">5987 </span></a> 
<a name="l5988"><span class="ln">5988 </span></a>Returns a new tensor with the natural logarithm of (1 + :attr:`input`). 
<a name="l5989"><span class="ln">5989 </span></a> 
<a name="l5990"><span class="ln">5990 </span></a>.. math:: 
<a name="l5991"><span class="ln">5991 </span></a>    y_i = \log_{e} (x_i + 1) 
<a name="l5992"><span class="ln">5992 </span></a>&quot;&quot;&quot;</span>
<a name="l5993"><span class="ln">5993 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l5994"><span class="ln">5994 </span></a>.. note:: This function is more accurate than :func:`torch.log` for small 
<a name="l5995"><span class="ln">5995 </span></a>          values of :attr:`input` 
<a name="l5996"><span class="ln">5996 </span></a> 
<a name="l5997"><span class="ln">5997 </span></a>Args: 
<a name="l5998"><span class="ln">5998 </span></a>    {input} 
<a name="l5999"><span class="ln">5999 </span></a> 
<a name="l6000"><span class="ln">6000 </span></a>Keyword args: 
<a name="l6001"><span class="ln">6001 </span></a>    {out} 
<a name="l6002"><span class="ln">6002 </span></a> 
<a name="l6003"><span class="ln">6003 </span></a>Example:: 
<a name="l6004"><span class="ln">6004 </span></a> 
<a name="l6005"><span class="ln">6005 </span></a>    &gt;&gt;&gt; a = torch.randn(5) 
<a name="l6006"><span class="ln">6006 </span></a>    &gt;&gt;&gt; a 
<a name="l6007"><span class="ln">6007 </span></a>    tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492]) 
<a name="l6008"><span class="ln">6008 </span></a>    &gt;&gt;&gt; torch.log1p(a) 
<a name="l6009"><span class="ln">6009 </span></a>    tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225]) 
<a name="l6010"><span class="ln">6010 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6011"><span class="ln">6011 </span></a><span class="s3">)</span>
<a name="l6012"><span class="ln">6012 </span></a>
<a name="l6013"><span class="ln">6013 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6014"><span class="ln">6014 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">log2</span><span class="s3">,</span>
<a name="l6015"><span class="ln">6015 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6016"><span class="ln">6016 </span></a>log2(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6017"><span class="ln">6017 </span></a> 
<a name="l6018"><span class="ln">6018 </span></a>Returns a new tensor with the logarithm to the base 2 of the elements 
<a name="l6019"><span class="ln">6019 </span></a>of :attr:`input`. 
<a name="l6020"><span class="ln">6020 </span></a> 
<a name="l6021"><span class="ln">6021 </span></a>.. math:: 
<a name="l6022"><span class="ln">6022 </span></a>    y_{i} = \log_{2} (x_{i}) 
<a name="l6023"><span class="ln">6023 </span></a>&quot;&quot;&quot;</span>
<a name="l6024"><span class="ln">6024 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l6025"><span class="ln">6025 </span></a> 
<a name="l6026"><span class="ln">6026 </span></a>Args: 
<a name="l6027"><span class="ln">6027 </span></a>    {input} 
<a name="l6028"><span class="ln">6028 </span></a> 
<a name="l6029"><span class="ln">6029 </span></a>Keyword args: 
<a name="l6030"><span class="ln">6030 </span></a>    {out} 
<a name="l6031"><span class="ln">6031 </span></a> 
<a name="l6032"><span class="ln">6032 </span></a>Example:: 
<a name="l6033"><span class="ln">6033 </span></a> 
<a name="l6034"><span class="ln">6034 </span></a>    &gt;&gt;&gt; a = torch.rand(5) 
<a name="l6035"><span class="ln">6035 </span></a>    &gt;&gt;&gt; a 
<a name="l6036"><span class="ln">6036 </span></a>    tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490]) 
<a name="l6037"><span class="ln">6037 </span></a> 
<a name="l6038"><span class="ln">6038 </span></a> 
<a name="l6039"><span class="ln">6039 </span></a>    &gt;&gt;&gt; torch.log2(a) 
<a name="l6040"><span class="ln">6040 </span></a>    tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504]) 
<a name="l6041"><span class="ln">6041 </span></a> 
<a name="l6042"><span class="ln">6042 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6043"><span class="ln">6043 </span></a><span class="s3">)</span>
<a name="l6044"><span class="ln">6044 </span></a>
<a name="l6045"><span class="ln">6045 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6046"><span class="ln">6046 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logaddexp</span><span class="s3">,</span>
<a name="l6047"><span class="ln">6047 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6048"><span class="ln">6048 </span></a>logaddexp(input, other, *, out=None) -&gt; Tensor 
<a name="l6049"><span class="ln">6049 </span></a> 
<a name="l6050"><span class="ln">6050 </span></a>Logarithm of the sum of exponentiations of the inputs. 
<a name="l6051"><span class="ln">6051 </span></a> 
<a name="l6052"><span class="ln">6052 </span></a>Calculates pointwise :math:`\log\left(e^x + e^y\right)`. This function is useful 
<a name="l6053"><span class="ln">6053 </span></a>in statistics where the calculated probabilities of events may be so small as to 
<a name="l6054"><span class="ln">6054 </span></a>exceed the range of normal floating point numbers. In such cases the logarithm 
<a name="l6055"><span class="ln">6055 </span></a>of the calculated probability is stored. This function allows adding 
<a name="l6056"><span class="ln">6056 </span></a>probabilities stored in such a fashion. 
<a name="l6057"><span class="ln">6057 </span></a> 
<a name="l6058"><span class="ln">6058 </span></a>This op should be disambiguated with :func:`torch.logsumexp` which performs a 
<a name="l6059"><span class="ln">6059 </span></a>reduction on a single tensor. 
<a name="l6060"><span class="ln">6060 </span></a> 
<a name="l6061"><span class="ln">6061 </span></a>Args: 
<a name="l6062"><span class="ln">6062 </span></a>    {input} 
<a name="l6063"><span class="ln">6063 </span></a>    other (Tensor): the second input tensor 
<a name="l6064"><span class="ln">6064 </span></a> 
<a name="l6065"><span class="ln">6065 </span></a>Keyword arguments: 
<a name="l6066"><span class="ln">6066 </span></a>    {out} 
<a name="l6067"><span class="ln">6067 </span></a> 
<a name="l6068"><span class="ln">6068 </span></a>Example:: 
<a name="l6069"><span class="ln">6069 </span></a> 
<a name="l6070"><span class="ln">6070 </span></a>    &gt;&gt;&gt; torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3])) 
<a name="l6071"><span class="ln">6071 </span></a>    tensor([-0.3069, -0.6867, -0.8731]) 
<a name="l6072"><span class="ln">6072 </span></a>    &gt;&gt;&gt; torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3])) 
<a name="l6073"><span class="ln">6073 </span></a>    tensor([-1., -2., -3.]) 
<a name="l6074"><span class="ln">6074 </span></a>    &gt;&gt;&gt; torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3])) 
<a name="l6075"><span class="ln">6075 </span></a>    tensor([1.1269e+00, 2.0000e+03, 3.0000e+04]) 
<a name="l6076"><span class="ln">6076 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6077"><span class="ln">6077 </span></a><span class="s3">)</span>
<a name="l6078"><span class="ln">6078 </span></a>
<a name="l6079"><span class="ln">6079 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6080"><span class="ln">6080 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logaddexp2</span><span class="s3">,</span>
<a name="l6081"><span class="ln">6081 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6082"><span class="ln">6082 </span></a>logaddexp2(input, other, *, out=None) -&gt; Tensor 
<a name="l6083"><span class="ln">6083 </span></a> 
<a name="l6084"><span class="ln">6084 </span></a>Logarithm of the sum of exponentiations of the inputs in base-2. 
<a name="l6085"><span class="ln">6085 </span></a> 
<a name="l6086"><span class="ln">6086 </span></a>Calculates pointwise :math:`\log_2\left(2^x + 2^y\right)`. See 
<a name="l6087"><span class="ln">6087 </span></a>:func:`torch.logaddexp` for more details. 
<a name="l6088"><span class="ln">6088 </span></a> 
<a name="l6089"><span class="ln">6089 </span></a>Args: 
<a name="l6090"><span class="ln">6090 </span></a>    {input} 
<a name="l6091"><span class="ln">6091 </span></a>    other (Tensor): the second input tensor 
<a name="l6092"><span class="ln">6092 </span></a> 
<a name="l6093"><span class="ln">6093 </span></a>Keyword arguments: 
<a name="l6094"><span class="ln">6094 </span></a>    {out} 
<a name="l6095"><span class="ln">6095 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6096"><span class="ln">6096 </span></a><span class="s3">)</span>
<a name="l6097"><span class="ln">6097 </span></a>
<a name="l6098"><span class="ln">6098 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6099"><span class="ln">6099 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">xlogy</span><span class="s3">,</span>
<a name="l6100"><span class="ln">6100 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6101"><span class="ln">6101 </span></a>xlogy(input, other, *, out=None) -&gt; Tensor 
<a name="l6102"><span class="ln">6102 </span></a> 
<a name="l6103"><span class="ln">6103 </span></a>Alias for :func:`torch.special.xlogy`. 
<a name="l6104"><span class="ln">6104 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6105"><span class="ln">6105 </span></a><span class="s3">)</span>
<a name="l6106"><span class="ln">6106 </span></a>
<a name="l6107"><span class="ln">6107 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6108"><span class="ln">6108 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logical_and</span><span class="s3">,</span>
<a name="l6109"><span class="ln">6109 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6110"><span class="ln">6110 </span></a>logical_and(input, other, *, out=None) -&gt; Tensor 
<a name="l6111"><span class="ln">6111 </span></a> 
<a name="l6112"><span class="ln">6112 </span></a>Computes the element-wise logical AND of the given input tensors. Zeros are treated as ``False`` and nonzeros are 
<a name="l6113"><span class="ln">6113 </span></a>treated as ``True``. 
<a name="l6114"><span class="ln">6114 </span></a> 
<a name="l6115"><span class="ln">6115 </span></a>Args: 
<a name="l6116"><span class="ln">6116 </span></a>    {input} 
<a name="l6117"><span class="ln">6117 </span></a>    other (Tensor): the tensor to compute AND with 
<a name="l6118"><span class="ln">6118 </span></a> 
<a name="l6119"><span class="ln">6119 </span></a>Keyword args: 
<a name="l6120"><span class="ln">6120 </span></a>    {out} 
<a name="l6121"><span class="ln">6121 </span></a> 
<a name="l6122"><span class="ln">6122 </span></a>Example:: 
<a name="l6123"><span class="ln">6123 </span></a> 
<a name="l6124"><span class="ln">6124 </span></a>    &gt;&gt;&gt; torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False])) 
<a name="l6125"><span class="ln">6125 </span></a>    tensor([ True, False, False]) 
<a name="l6126"><span class="ln">6126 </span></a>    &gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8) 
<a name="l6127"><span class="ln">6127 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8) 
<a name="l6128"><span class="ln">6128 </span></a>    &gt;&gt;&gt; torch.logical_and(a, b) 
<a name="l6129"><span class="ln">6129 </span></a>    tensor([False, False,  True, False]) 
<a name="l6130"><span class="ln">6130 </span></a>    &gt;&gt;&gt; torch.logical_and(a.double(), b.double()) 
<a name="l6131"><span class="ln">6131 </span></a>    tensor([False, False,  True, False]) 
<a name="l6132"><span class="ln">6132 </span></a>    &gt;&gt;&gt; torch.logical_and(a.double(), b) 
<a name="l6133"><span class="ln">6133 </span></a>    tensor([False, False,  True, False]) 
<a name="l6134"><span class="ln">6134 </span></a>    &gt;&gt;&gt; torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool)) 
<a name="l6135"><span class="ln">6135 </span></a>    tensor([False, False,  True, False]) 
<a name="l6136"><span class="ln">6136 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6137"><span class="ln">6137 </span></a><span class="s3">)</span>
<a name="l6138"><span class="ln">6138 </span></a>
<a name="l6139"><span class="ln">6139 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6140"><span class="ln">6140 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logical_not</span><span class="s3">,</span>
<a name="l6141"><span class="ln">6141 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6142"><span class="ln">6142 </span></a>logical_not(input, *, out=None) -&gt; Tensor 
<a name="l6143"><span class="ln">6143 </span></a> 
<a name="l6144"><span class="ln">6144 </span></a>Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool 
<a name="l6145"><span class="ln">6145 </span></a>dtype. If the input tensor is not a bool tensor, zeros are treated as ``False`` and non-zeros are treated as ``True``. 
<a name="l6146"><span class="ln">6146 </span></a> 
<a name="l6147"><span class="ln">6147 </span></a>Args: 
<a name="l6148"><span class="ln">6148 </span></a>    {input} 
<a name="l6149"><span class="ln">6149 </span></a> 
<a name="l6150"><span class="ln">6150 </span></a>Keyword args: 
<a name="l6151"><span class="ln">6151 </span></a>    {out} 
<a name="l6152"><span class="ln">6152 </span></a> 
<a name="l6153"><span class="ln">6153 </span></a>Example:: 
<a name="l6154"><span class="ln">6154 </span></a> 
<a name="l6155"><span class="ln">6155 </span></a>    &gt;&gt;&gt; torch.logical_not(torch.tensor([True, False])) 
<a name="l6156"><span class="ln">6156 </span></a>    tensor([False,  True]) 
<a name="l6157"><span class="ln">6157 </span></a>    &gt;&gt;&gt; torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8)) 
<a name="l6158"><span class="ln">6158 </span></a>    tensor([ True, False, False]) 
<a name="l6159"><span class="ln">6159 </span></a>    &gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double)) 
<a name="l6160"><span class="ln">6160 </span></a>    tensor([ True, False, False]) 
<a name="l6161"><span class="ln">6161 </span></a>    &gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16)) 
<a name="l6162"><span class="ln">6162 </span></a>    tensor([1, 0, 0], dtype=torch.int16) 
<a name="l6163"><span class="ln">6163 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6164"><span class="ln">6164 </span></a><span class="s3">)</span>
<a name="l6165"><span class="ln">6165 </span></a>
<a name="l6166"><span class="ln">6166 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6167"><span class="ln">6167 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logical_or</span><span class="s3">,</span>
<a name="l6168"><span class="ln">6168 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6169"><span class="ln">6169 </span></a>logical_or(input, other, *, out=None) -&gt; Tensor 
<a name="l6170"><span class="ln">6170 </span></a> 
<a name="l6171"><span class="ln">6171 </span></a>Computes the element-wise logical OR of the given input tensors. Zeros are treated as ``False`` and nonzeros are 
<a name="l6172"><span class="ln">6172 </span></a>treated as ``True``. 
<a name="l6173"><span class="ln">6173 </span></a> 
<a name="l6174"><span class="ln">6174 </span></a>Args: 
<a name="l6175"><span class="ln">6175 </span></a>    {input} 
<a name="l6176"><span class="ln">6176 </span></a>    other (Tensor): the tensor to compute OR with 
<a name="l6177"><span class="ln">6177 </span></a> 
<a name="l6178"><span class="ln">6178 </span></a>Keyword args: 
<a name="l6179"><span class="ln">6179 </span></a>    {out} 
<a name="l6180"><span class="ln">6180 </span></a> 
<a name="l6181"><span class="ln">6181 </span></a>Example:: 
<a name="l6182"><span class="ln">6182 </span></a> 
<a name="l6183"><span class="ln">6183 </span></a>    &gt;&gt;&gt; torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False])) 
<a name="l6184"><span class="ln">6184 </span></a>    tensor([ True, False,  True]) 
<a name="l6185"><span class="ln">6185 </span></a>    &gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8) 
<a name="l6186"><span class="ln">6186 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8) 
<a name="l6187"><span class="ln">6187 </span></a>    &gt;&gt;&gt; torch.logical_or(a, b) 
<a name="l6188"><span class="ln">6188 </span></a>    tensor([ True,  True,  True, False]) 
<a name="l6189"><span class="ln">6189 </span></a>    &gt;&gt;&gt; torch.logical_or(a.double(), b.double()) 
<a name="l6190"><span class="ln">6190 </span></a>    tensor([ True,  True,  True, False]) 
<a name="l6191"><span class="ln">6191 </span></a>    &gt;&gt;&gt; torch.logical_or(a.double(), b) 
<a name="l6192"><span class="ln">6192 </span></a>    tensor([ True,  True,  True, False]) 
<a name="l6193"><span class="ln">6193 </span></a>    &gt;&gt;&gt; torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool)) 
<a name="l6194"><span class="ln">6194 </span></a>    tensor([ True,  True,  True, False]) 
<a name="l6195"><span class="ln">6195 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6196"><span class="ln">6196 </span></a><span class="s3">)</span>
<a name="l6197"><span class="ln">6197 </span></a>
<a name="l6198"><span class="ln">6198 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6199"><span class="ln">6199 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logical_xor</span><span class="s3">,</span>
<a name="l6200"><span class="ln">6200 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6201"><span class="ln">6201 </span></a>logical_xor(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6202"><span class="ln">6202 </span></a> 
<a name="l6203"><span class="ln">6203 </span></a>Computes the element-wise logical XOR of the given input tensors. Zeros are treated as ``False`` and nonzeros are 
<a name="l6204"><span class="ln">6204 </span></a>treated as ``True``. 
<a name="l6205"><span class="ln">6205 </span></a> 
<a name="l6206"><span class="ln">6206 </span></a>Args: 
<a name="l6207"><span class="ln">6207 </span></a>    {input} 
<a name="l6208"><span class="ln">6208 </span></a>    other (Tensor): the tensor to compute XOR with 
<a name="l6209"><span class="ln">6209 </span></a> 
<a name="l6210"><span class="ln">6210 </span></a>Keyword args: 
<a name="l6211"><span class="ln">6211 </span></a>    {out} 
<a name="l6212"><span class="ln">6212 </span></a> 
<a name="l6213"><span class="ln">6213 </span></a>Example:: 
<a name="l6214"><span class="ln">6214 </span></a> 
<a name="l6215"><span class="ln">6215 </span></a>    &gt;&gt;&gt; torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False])) 
<a name="l6216"><span class="ln">6216 </span></a>    tensor([False, False,  True]) 
<a name="l6217"><span class="ln">6217 </span></a>    &gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8) 
<a name="l6218"><span class="ln">6218 </span></a>    &gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8) 
<a name="l6219"><span class="ln">6219 </span></a>    &gt;&gt;&gt; torch.logical_xor(a, b) 
<a name="l6220"><span class="ln">6220 </span></a>    tensor([ True,  True, False, False]) 
<a name="l6221"><span class="ln">6221 </span></a>    &gt;&gt;&gt; torch.logical_xor(a.double(), b.double()) 
<a name="l6222"><span class="ln">6222 </span></a>    tensor([ True,  True, False, False]) 
<a name="l6223"><span class="ln">6223 </span></a>    &gt;&gt;&gt; torch.logical_xor(a.double(), b) 
<a name="l6224"><span class="ln">6224 </span></a>    tensor([ True,  True, False, False]) 
<a name="l6225"><span class="ln">6225 </span></a>    &gt;&gt;&gt; torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool)) 
<a name="l6226"><span class="ln">6226 </span></a>    tensor([ True,  True, False, False]) 
<a name="l6227"><span class="ln">6227 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6228"><span class="ln">6228 </span></a><span class="s3">)</span>
<a name="l6229"><span class="ln">6229 </span></a>
<a name="l6230"><span class="ln">6230 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6231"><span class="ln">6231 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logspace</span><span class="s3">,</span>
<a name="l6232"><span class="ln">6232 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l6233"><span class="ln">6233 </span></a>logspace(start, end, steps, base=10.0, *, \ 
<a name="l6234"><span class="ln">6234 </span></a>         out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l6235"><span class="ln">6235 </span></a>&quot;&quot;&quot;</span>
<a name="l6236"><span class="ln">6236 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l6237"><span class="ln">6237 </span></a> 
<a name="l6238"><span class="ln">6238 </span></a>Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l6239"><span class="ln">6239 </span></a>spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to 
<a name="l6240"><span class="ln">6240 </span></a>:math:`{{\text{{base}}}}^{{\text{{end}}}}`, inclusive, on a logarithmic scale 
<a name="l6241"><span class="ln">6241 </span></a>with base :attr:`base`. That is, the values are: 
<a name="l6242"><span class="ln">6242 </span></a> 
<a name="l6243"><span class="ln">6243 </span></a>.. math:: 
<a name="l6244"><span class="ln">6244 </span></a>    (\text{base}^{\text{start}}, 
<a name="l6245"><span class="ln">6245 </span></a>    \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l6246"><span class="ln">6246 </span></a>    \ldots, 
<a name="l6247"><span class="ln">6247 </span></a>    \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l6248"><span class="ln">6248 </span></a>    \text{base}^{\text{end}}) 
<a name="l6249"><span class="ln">6249 </span></a>&quot;&quot;&quot;</span>
<a name="l6250"><span class="ln">6250 </span></a>    <span class="s2">+ </span><span class="s4">&quot;&quot;&quot; 
<a name="l6251"><span class="ln">6251 </span></a> 
<a name="l6252"><span class="ln">6252 </span></a> 
<a name="l6253"><span class="ln">6253 </span></a>From PyTorch 1.11 logspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l6254"><span class="ln">6254 </span></a> 
<a name="l6255"><span class="ln">6255 </span></a>Args: 
<a name="l6256"><span class="ln">6256 </span></a>    start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l6257"><span class="ln">6257 </span></a>    end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l6258"><span class="ln">6258 </span></a>    steps (int): size of the constructed tensor 
<a name="l6259"><span class="ln">6259 </span></a>    base (float, optional): base of the logarithm function. Default: ``10.0``. 
<a name="l6260"><span class="ln">6260 </span></a> 
<a name="l6261"><span class="ln">6261 </span></a>Keyword arguments: 
<a name="l6262"><span class="ln">6262 </span></a>    {out} 
<a name="l6263"><span class="ln">6263 </span></a>    dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l6264"><span class="ln">6264 </span></a>        Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l6265"><span class="ln">6265 </span></a>        when both :attr:`start` and :attr:`end` are real, 
<a name="l6266"><span class="ln">6266 </span></a>        and corresponding complex dtype when either is complex. 
<a name="l6267"><span class="ln">6267 </span></a>    {layout} 
<a name="l6268"><span class="ln">6268 </span></a>    {device} 
<a name="l6269"><span class="ln">6269 </span></a>    {requires_grad} 
<a name="l6270"><span class="ln">6270 </span></a> 
<a name="l6271"><span class="ln">6271 </span></a>Example:: 
<a name="l6272"><span class="ln">6272 </span></a> 
<a name="l6273"><span class="ln">6273 </span></a>    &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 
<a name="l6274"><span class="ln">6274 </span></a>    tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) 
<a name="l6275"><span class="ln">6275 </span></a>    &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 
<a name="l6276"><span class="ln">6276 </span></a>    tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000]) 
<a name="l6277"><span class="ln">6277 </span></a>    &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1) 
<a name="l6278"><span class="ln">6278 </span></a>    tensor([1.2589]) 
<a name="l6279"><span class="ln">6279 </span></a>    &gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2) 
<a name="l6280"><span class="ln">6280 </span></a>    tensor([4.0]) 
<a name="l6281"><span class="ln">6281 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l6282"><span class="ln">6282 </span></a><span class="s3">)</span>
<a name="l6283"><span class="ln">6283 </span></a>
<a name="l6284"><span class="ln">6284 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6285"><span class="ln">6285 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logsumexp</span><span class="s3">,</span>
<a name="l6286"><span class="ln">6286 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6287"><span class="ln">6287 </span></a>logsumexp(input, dim, keepdim=False, *, out=None) 
<a name="l6288"><span class="ln">6288 </span></a> 
<a name="l6289"><span class="ln">6289 </span></a>Returns the log of summed exponentials of each row of the :attr:`input` 
<a name="l6290"><span class="ln">6290 </span></a>tensor in the given dimension :attr:`dim`. The computation is numerically 
<a name="l6291"><span class="ln">6291 </span></a>stabilized. 
<a name="l6292"><span class="ln">6292 </span></a> 
<a name="l6293"><span class="ln">6293 </span></a>For summation index :math:`j` given by `dim` and other indices :math:`i`, the result is 
<a name="l6294"><span class="ln">6294 </span></a> 
<a name="l6295"><span class="ln">6295 </span></a>    .. math:: 
<a name="l6296"><span class="ln">6296 </span></a>        \text{{logsumexp}}(x)_{{i}} = \log \sum_j \exp(x_{{ij}}) 
<a name="l6297"><span class="ln">6297 </span></a> 
<a name="l6298"><span class="ln">6298 </span></a>{keepdim_details} 
<a name="l6299"><span class="ln">6299 </span></a> 
<a name="l6300"><span class="ln">6300 </span></a>Args: 
<a name="l6301"><span class="ln">6301 </span></a>    {input} 
<a name="l6302"><span class="ln">6302 </span></a>    {dim} 
<a name="l6303"><span class="ln">6303 </span></a>    {opt_keepdim} 
<a name="l6304"><span class="ln">6304 </span></a> 
<a name="l6305"><span class="ln">6305 </span></a>Keyword args: 
<a name="l6306"><span class="ln">6306 </span></a>    {out} 
<a name="l6307"><span class="ln">6307 </span></a> 
<a name="l6308"><span class="ln">6308 </span></a>Example:: 
<a name="l6309"><span class="ln">6309 </span></a> 
<a name="l6310"><span class="ln">6310 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l6311"><span class="ln">6311 </span></a>    &gt;&gt;&gt; torch.logsumexp(a, 1) 
<a name="l6312"><span class="ln">6312 </span></a>    tensor([1.4907, 1.0593, 1.5696]) 
<a name="l6313"><span class="ln">6313 </span></a>    &gt;&gt;&gt; torch.dist(torch.logsumexp(a, 1), torch.log(torch.sum(torch.exp(a), 1))) 
<a name="l6314"><span class="ln">6314 </span></a>    tensor(1.6859e-07) 
<a name="l6315"><span class="ln">6315 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l6316"><span class="ln">6316 </span></a><span class="s3">)</span>
<a name="l6317"><span class="ln">6317 </span></a>
<a name="l6318"><span class="ln">6318 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6319"><span class="ln">6319 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">lt</span><span class="s3">,</span>
<a name="l6320"><span class="ln">6320 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6321"><span class="ln">6321 </span></a>lt(input, other, *, out=None) -&gt; Tensor 
<a name="l6322"><span class="ln">6322 </span></a> 
<a name="l6323"><span class="ln">6323 </span></a>Computes :math:`\text{input} &lt; \text{other}` element-wise. 
<a name="l6324"><span class="ln">6324 </span></a>&quot;&quot;&quot;</span>
<a name="l6325"><span class="ln">6325 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l6326"><span class="ln">6326 </span></a> 
<a name="l6327"><span class="ln">6327 </span></a>The second argument can be a number or a tensor whose shape is 
<a name="l6328"><span class="ln">6328 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l6329"><span class="ln">6329 </span></a> 
<a name="l6330"><span class="ln">6330 </span></a>Args: 
<a name="l6331"><span class="ln">6331 </span></a>    input (Tensor): the tensor to compare 
<a name="l6332"><span class="ln">6332 </span></a>    other (Tensor or float): the tensor or value to compare 
<a name="l6333"><span class="ln">6333 </span></a> 
<a name="l6334"><span class="ln">6334 </span></a>Keyword args: 
<a name="l6335"><span class="ln">6335 </span></a>    {out} 
<a name="l6336"><span class="ln">6336 </span></a> 
<a name="l6337"><span class="ln">6337 </span></a>Returns: 
<a name="l6338"><span class="ln">6338 </span></a>    A boolean tensor that is True where :attr:`input` is less than :attr:`other` and False elsewhere 
<a name="l6339"><span class="ln">6339 </span></a> 
<a name="l6340"><span class="ln">6340 </span></a>Example:: 
<a name="l6341"><span class="ln">6341 </span></a> 
<a name="l6342"><span class="ln">6342 </span></a>    &gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l6343"><span class="ln">6343 </span></a>    tensor([[False, False], [True, False]]) 
<a name="l6344"><span class="ln">6344 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6345"><span class="ln">6345 </span></a><span class="s3">)</span>
<a name="l6346"><span class="ln">6346 </span></a>
<a name="l6347"><span class="ln">6347 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6348"><span class="ln">6348 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">lu_unpack</span><span class="s3">,</span>
<a name="l6349"><span class="ln">6349 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6350"><span class="ln">6350 </span></a>lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None) -&gt; (Tensor, Tensor, Tensor) 
<a name="l6351"><span class="ln">6351 </span></a> 
<a name="l6352"><span class="ln">6352 </span></a>Unpacks the LU decomposition returned by :func:`~linalg.lu_factor` into the `P, L, U` matrices. 
<a name="l6353"><span class="ln">6353 </span></a> 
<a name="l6354"><span class="ln">6354 </span></a>.. seealso:: 
<a name="l6355"><span class="ln">6355 </span></a> 
<a name="l6356"><span class="ln">6356 </span></a>    :func:`~linalg.lu` returns the matrices from the LU decomposition. Its gradient formula is more efficient 
<a name="l6357"><span class="ln">6357 </span></a>    than that of doing :func:`~linalg.lu_factor` followed by :func:`~linalg.lu_unpack`. 
<a name="l6358"><span class="ln">6358 </span></a> 
<a name="l6359"><span class="ln">6359 </span></a>Args: 
<a name="l6360"><span class="ln">6360 </span></a>    LU_data (Tensor): the packed LU factorization data 
<a name="l6361"><span class="ln">6361 </span></a>    LU_pivots (Tensor): the packed LU factorization pivots 
<a name="l6362"><span class="ln">6362 </span></a>    unpack_data (bool): flag indicating if the data should be unpacked. 
<a name="l6363"><span class="ln">6363 </span></a>                        If ``False``, then the returned ``L`` and ``U`` are empty tensors. 
<a name="l6364"><span class="ln">6364 </span></a>                        Default: ``True`` 
<a name="l6365"><span class="ln">6365 </span></a>    unpack_pivots (bool): flag indicating if the pivots should be unpacked into a permutation matrix ``P``. 
<a name="l6366"><span class="ln">6366 </span></a>                          If ``False``, then the returned ``P`` is  an empty tensor. 
<a name="l6367"><span class="ln">6367 </span></a>                          Default: ``True`` 
<a name="l6368"><span class="ln">6368 </span></a> 
<a name="l6369"><span class="ln">6369 </span></a>Keyword args: 
<a name="l6370"><span class="ln">6370 </span></a>    out (tuple, optional): output tuple of three tensors. Ignored if `None`. 
<a name="l6371"><span class="ln">6371 </span></a> 
<a name="l6372"><span class="ln">6372 </span></a>Returns: 
<a name="l6373"><span class="ln">6373 </span></a>    A namedtuple ``(P, L, U)`` 
<a name="l6374"><span class="ln">6374 </span></a> 
<a name="l6375"><span class="ln">6375 </span></a>Examples:: 
<a name="l6376"><span class="ln">6376 </span></a> 
<a name="l6377"><span class="ln">6377 </span></a>    &gt;&gt;&gt; A = torch.randn(2, 3, 3) 
<a name="l6378"><span class="ln">6378 </span></a>    &gt;&gt;&gt; LU, pivots = torch.linalg.lu_factor(A) 
<a name="l6379"><span class="ln">6379 </span></a>    &gt;&gt;&gt; P, L, U = torch.lu_unpack(LU, pivots) 
<a name="l6380"><span class="ln">6380 </span></a>    &gt;&gt;&gt; # We can recover A from the factorization 
<a name="l6381"><span class="ln">6381 </span></a>    &gt;&gt;&gt; A_ = P @ L @ U 
<a name="l6382"><span class="ln">6382 </span></a>    &gt;&gt;&gt; torch.allclose(A, A_) 
<a name="l6383"><span class="ln">6383 </span></a>    True 
<a name="l6384"><span class="ln">6384 </span></a> 
<a name="l6385"><span class="ln">6385 </span></a>    &gt;&gt;&gt; # LU factorization of a rectangular matrix: 
<a name="l6386"><span class="ln">6386 </span></a>    &gt;&gt;&gt; A = torch.randn(2, 3, 2) 
<a name="l6387"><span class="ln">6387 </span></a>    &gt;&gt;&gt; LU, pivots = torch.linalg.lu_factor(A) 
<a name="l6388"><span class="ln">6388 </span></a>    &gt;&gt;&gt; P, L, U = torch.lu_unpack(LU, pivots) 
<a name="l6389"><span class="ln">6389 </span></a>    &gt;&gt;&gt; # P, L, U are the same as returned by linalg.lu 
<a name="l6390"><span class="ln">6390 </span></a>    &gt;&gt;&gt; P_, L_, U_ = torch.linalg.lu(A) 
<a name="l6391"><span class="ln">6391 </span></a>    &gt;&gt;&gt; torch.allclose(P, P_) and torch.allclose(L, L_) and torch.allclose(U, U_) 
<a name="l6392"><span class="ln">6392 </span></a>    True 
<a name="l6393"><span class="ln">6393 </span></a> 
<a name="l6394"><span class="ln">6394 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6395"><span class="ln">6395 </span></a><span class="s3">)</span>
<a name="l6396"><span class="ln">6396 </span></a>
<a name="l6397"><span class="ln">6397 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6398"><span class="ln">6398 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">less</span><span class="s3">,</span>
<a name="l6399"><span class="ln">6399 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6400"><span class="ln">6400 </span></a>less(input, other, *, out=None) -&gt; Tensor 
<a name="l6401"><span class="ln">6401 </span></a> 
<a name="l6402"><span class="ln">6402 </span></a>Alias for :func:`torch.lt`. 
<a name="l6403"><span class="ln">6403 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6404"><span class="ln">6404 </span></a><span class="s3">)</span>
<a name="l6405"><span class="ln">6405 </span></a>
<a name="l6406"><span class="ln">6406 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6407"><span class="ln">6407 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">lu_solve</span><span class="s3">,</span>
<a name="l6408"><span class="ln">6408 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6409"><span class="ln">6409 </span></a>lu_solve(b, LU_data, LU_pivots, *, out=None) -&gt; Tensor 
<a name="l6410"><span class="ln">6410 </span></a> 
<a name="l6411"><span class="ln">6411 </span></a>Returns the LU solve of the linear system :math:`Ax = b` using the partially pivoted 
<a name="l6412"><span class="ln">6412 </span></a>LU factorization of A from :func:`~linalg.lu_factor`. 
<a name="l6413"><span class="ln">6413 </span></a> 
<a name="l6414"><span class="ln">6414 </span></a>This function supports ``float``, ``double``, ``cfloat`` and ``cdouble`` dtypes for :attr:`input`. 
<a name="l6415"><span class="ln">6415 </span></a> 
<a name="l6416"><span class="ln">6416 </span></a>.. warning:: 
<a name="l6417"><span class="ln">6417 </span></a> 
<a name="l6418"><span class="ln">6418 </span></a>    :func:`torch.lu_solve` is deprecated in favor of :func:`torch.linalg.lu_solve`. 
<a name="l6419"><span class="ln">6419 </span></a>    :func:`torch.lu_solve` will be removed in a future PyTorch release. 
<a name="l6420"><span class="ln">6420 </span></a>    ``X = torch.lu_solve(B, LU, pivots)`` should be replaced with 
<a name="l6421"><span class="ln">6421 </span></a> 
<a name="l6422"><span class="ln">6422 </span></a>    .. code:: python 
<a name="l6423"><span class="ln">6423 </span></a> 
<a name="l6424"><span class="ln">6424 </span></a>        X = linalg.lu_solve(LU, pivots, B) 
<a name="l6425"><span class="ln">6425 </span></a> 
<a name="l6426"><span class="ln">6426 </span></a>Arguments: 
<a name="l6427"><span class="ln">6427 </span></a>    b (Tensor): the RHS tensor of size :math:`(*, m, k)`, where :math:`*` 
<a name="l6428"><span class="ln">6428 </span></a>                is zero or more batch dimensions. 
<a name="l6429"><span class="ln">6429 </span></a>    LU_data (Tensor): the pivoted LU factorization of A from :meth:`~linalg.lu_factor` of size :math:`(*, m, m)`, 
<a name="l6430"><span class="ln">6430 </span></a>                       where :math:`*` is zero or more batch dimensions. 
<a name="l6431"><span class="ln">6431 </span></a>    LU_pivots (IntTensor): the pivots of the LU factorization from :meth:`~linalg.lu_factor` of size :math:`(*, m)`, 
<a name="l6432"><span class="ln">6432 </span></a>                           where :math:`*` is zero or more batch dimensions. 
<a name="l6433"><span class="ln">6433 </span></a>                           The batch dimensions of :attr:`LU_pivots` must be equal to the batch dimensions of 
<a name="l6434"><span class="ln">6434 </span></a>                           :attr:`LU_data`. 
<a name="l6435"><span class="ln">6435 </span></a> 
<a name="l6436"><span class="ln">6436 </span></a>Keyword args: 
<a name="l6437"><span class="ln">6437 </span></a>    {out} 
<a name="l6438"><span class="ln">6438 </span></a> 
<a name="l6439"><span class="ln">6439 </span></a>Example:: 
<a name="l6440"><span class="ln">6440 </span></a> 
<a name="l6441"><span class="ln">6441 </span></a>    &gt;&gt;&gt; A = torch.randn(2, 3, 3) 
<a name="l6442"><span class="ln">6442 </span></a>    &gt;&gt;&gt; b = torch.randn(2, 3, 1) 
<a name="l6443"><span class="ln">6443 </span></a>    &gt;&gt;&gt; LU, pivots = torch.linalg.lu_factor(A) 
<a name="l6444"><span class="ln">6444 </span></a>    &gt;&gt;&gt; x = torch.lu_solve(b, LU, pivots) 
<a name="l6445"><span class="ln">6445 </span></a>    &gt;&gt;&gt; torch.dist(A @ x, b) 
<a name="l6446"><span class="ln">6446 </span></a>    tensor(1.00000e-07 * 
<a name="l6447"><span class="ln">6447 </span></a>           2.8312) 
<a name="l6448"><span class="ln">6448 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6449"><span class="ln">6449 </span></a><span class="s3">)</span>
<a name="l6450"><span class="ln">6450 </span></a>
<a name="l6451"><span class="ln">6451 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6452"><span class="ln">6452 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">masked_select</span><span class="s3">,</span>
<a name="l6453"><span class="ln">6453 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6454"><span class="ln">6454 </span></a>masked_select(input, mask, *, out=None) -&gt; Tensor 
<a name="l6455"><span class="ln">6455 </span></a> 
<a name="l6456"><span class="ln">6456 </span></a>Returns a new 1-D tensor which indexes the :attr:`input` tensor according to 
<a name="l6457"><span class="ln">6457 </span></a>the boolean mask :attr:`mask` which is a `BoolTensor`. 
<a name="l6458"><span class="ln">6458 </span></a> 
<a name="l6459"><span class="ln">6459 </span></a>The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need 
<a name="l6460"><span class="ln">6460 </span></a>to match, but they must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l6461"><span class="ln">6461 </span></a> 
<a name="l6462"><span class="ln">6462 </span></a>.. note:: The returned tensor does **not** use the same storage 
<a name="l6463"><span class="ln">6463 </span></a>          as the original tensor 
<a name="l6464"><span class="ln">6464 </span></a> 
<a name="l6465"><span class="ln">6465 </span></a>Args: 
<a name="l6466"><span class="ln">6466 </span></a>    {input} 
<a name="l6467"><span class="ln">6467 </span></a>    mask  (BoolTensor): the tensor containing the binary mask to index with 
<a name="l6468"><span class="ln">6468 </span></a> 
<a name="l6469"><span class="ln">6469 </span></a>Keyword args: 
<a name="l6470"><span class="ln">6470 </span></a>    {out} 
<a name="l6471"><span class="ln">6471 </span></a> 
<a name="l6472"><span class="ln">6472 </span></a>Example:: 
<a name="l6473"><span class="ln">6473 </span></a> 
<a name="l6474"><span class="ln">6474 </span></a>    &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l6475"><span class="ln">6475 </span></a>    &gt;&gt;&gt; x 
<a name="l6476"><span class="ln">6476 </span></a>    tensor([[ 0.3552, -2.3825, -0.8297,  0.3477], 
<a name="l6477"><span class="ln">6477 </span></a>            [-1.2035,  1.2252,  0.5002,  0.6248], 
<a name="l6478"><span class="ln">6478 </span></a>            [ 0.1307, -2.0608,  0.1244,  2.0139]]) 
<a name="l6479"><span class="ln">6479 </span></a>    &gt;&gt;&gt; mask = x.ge(0.5) 
<a name="l6480"><span class="ln">6480 </span></a>    &gt;&gt;&gt; mask 
<a name="l6481"><span class="ln">6481 </span></a>    tensor([[False, False, False, False], 
<a name="l6482"><span class="ln">6482 </span></a>            [False, True, True, True], 
<a name="l6483"><span class="ln">6483 </span></a>            [False, False, False, True]]) 
<a name="l6484"><span class="ln">6484 </span></a>    &gt;&gt;&gt; torch.masked_select(x, mask) 
<a name="l6485"><span class="ln">6485 </span></a>    tensor([ 1.2252,  0.5002,  0.6248,  2.0139]) 
<a name="l6486"><span class="ln">6486 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6487"><span class="ln">6487 </span></a><span class="s3">)</span>
<a name="l6488"><span class="ln">6488 </span></a>
<a name="l6489"><span class="ln">6489 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6490"><span class="ln">6490 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">matrix_power</span><span class="s3">,</span>
<a name="l6491"><span class="ln">6491 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6492"><span class="ln">6492 </span></a>matrix_power(input, n, *, out=None) -&gt; Tensor 
<a name="l6493"><span class="ln">6493 </span></a> 
<a name="l6494"><span class="ln">6494 </span></a>Alias for :func:`torch.linalg.matrix_power` 
<a name="l6495"><span class="ln">6495 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6496"><span class="ln">6496 </span></a><span class="s3">)</span>
<a name="l6497"><span class="ln">6497 </span></a>
<a name="l6498"><span class="ln">6498 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6499"><span class="ln">6499 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">matrix_exp</span><span class="s3">,</span>
<a name="l6500"><span class="ln">6500 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6501"><span class="ln">6501 </span></a>matrix_exp(A) -&gt; Tensor 
<a name="l6502"><span class="ln">6502 </span></a> 
<a name="l6503"><span class="ln">6503 </span></a>Alias for :func:`torch.linalg.matrix_exp`. 
<a name="l6504"><span class="ln">6504 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6505"><span class="ln">6505 </span></a><span class="s3">)</span>
<a name="l6506"><span class="ln">6506 </span></a>
<a name="l6507"><span class="ln">6507 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6508"><span class="ln">6508 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">max</span><span class="s3">,</span>
<a name="l6509"><span class="ln">6509 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6510"><span class="ln">6510 </span></a>max(input, *, out=None) -&gt; Tensor 
<a name="l6511"><span class="ln">6511 </span></a> 
<a name="l6512"><span class="ln">6512 </span></a>Returns the maximum value of all elements in the ``input`` tensor. 
<a name="l6513"><span class="ln">6513 </span></a> 
<a name="l6514"><span class="ln">6514 </span></a>Args: 
<a name="l6515"><span class="ln">6515 </span></a>    {input} 
<a name="l6516"><span class="ln">6516 </span></a> 
<a name="l6517"><span class="ln">6517 </span></a>Keyword args: 
<a name="l6518"><span class="ln">6518 </span></a>    {out} 
<a name="l6519"><span class="ln">6519 </span></a> 
<a name="l6520"><span class="ln">6520 </span></a>Example:: 
<a name="l6521"><span class="ln">6521 </span></a> 
<a name="l6522"><span class="ln">6522 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l6523"><span class="ln">6523 </span></a>    &gt;&gt;&gt; a 
<a name="l6524"><span class="ln">6524 </span></a>    tensor([[ 0.6763,  0.7445, -2.2369]]) 
<a name="l6525"><span class="ln">6525 </span></a>    &gt;&gt;&gt; torch.max(a) 
<a name="l6526"><span class="ln">6526 </span></a>    tensor(0.7445) 
<a name="l6527"><span class="ln">6527 </span></a> 
<a name="l6528"><span class="ln">6528 </span></a>.. function:: max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l6529"><span class="ln">6529 </span></a>   :noindex: 
<a name="l6530"><span class="ln">6530 </span></a> 
<a name="l6531"><span class="ln">6531 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum 
<a name="l6532"><span class="ln">6532 </span></a>value of each row of the :attr:`input` tensor in the given dimension 
<a name="l6533"><span class="ln">6533 </span></a>:attr:`dim`. And ``indices`` is the index location of each maximum value found 
<a name="l6534"><span class="ln">6534 </span></a>(argmax). 
<a name="l6535"><span class="ln">6535 </span></a> 
<a name="l6536"><span class="ln">6536 </span></a>If ``keepdim`` is ``True``, the output tensors are of the same size 
<a name="l6537"><span class="ln">6537 </span></a>as ``input`` except in the dimension ``dim`` where they are of size 1. 
<a name="l6538"><span class="ln">6538 </span></a>Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l6539"><span class="ln">6539 </span></a>in the output tensors having 1 fewer dimension than ``input``. 
<a name="l6540"><span class="ln">6540 </span></a> 
<a name="l6541"><span class="ln">6541 </span></a>.. note:: If there are multiple maximal values in a reduced row then 
<a name="l6542"><span class="ln">6542 </span></a>          the indices of the first maximal value are returned. 
<a name="l6543"><span class="ln">6543 </span></a> 
<a name="l6544"><span class="ln">6544 </span></a>Args: 
<a name="l6545"><span class="ln">6545 </span></a>    {input} 
<a name="l6546"><span class="ln">6546 </span></a>    {opt_dim_all_reduce} 
<a name="l6547"><span class="ln">6547 </span></a>    {opt_keepdim} 
<a name="l6548"><span class="ln">6548 </span></a> 
<a name="l6549"><span class="ln">6549 </span></a>Keyword args: 
<a name="l6550"><span class="ln">6550 </span></a>    out (tuple, optional): the result tuple of two output tensors (max, max_indices) 
<a name="l6551"><span class="ln">6551 </span></a> 
<a name="l6552"><span class="ln">6552 </span></a>Example:: 
<a name="l6553"><span class="ln">6553 </span></a> 
<a name="l6554"><span class="ln">6554 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6555"><span class="ln">6555 </span></a>    &gt;&gt;&gt; a 
<a name="l6556"><span class="ln">6556 </span></a>    tensor([[-1.2360, -0.2942, -0.1222,  0.8475], 
<a name="l6557"><span class="ln">6557 </span></a>            [ 1.1949, -1.1127, -2.2379, -0.6702], 
<a name="l6558"><span class="ln">6558 </span></a>            [ 1.5717, -0.9207,  0.1297, -1.8768], 
<a name="l6559"><span class="ln">6559 </span></a>            [-0.6172,  1.0036, -0.6060, -0.2432]]) 
<a name="l6560"><span class="ln">6560 </span></a>    &gt;&gt;&gt; torch.max(a, 1) 
<a name="l6561"><span class="ln">6561 </span></a>    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1])) 
<a name="l6562"><span class="ln">6562 </span></a>    &gt;&gt;&gt; a = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) 
<a name="l6563"><span class="ln">6563 </span></a>    &gt;&gt;&gt; a.max(dim=1, keepdim=True) 
<a name="l6564"><span class="ln">6564 </span></a>    torch.return_types.max( 
<a name="l6565"><span class="ln">6565 </span></a>    values=tensor([[2.], [4.]]), 
<a name="l6566"><span class="ln">6566 </span></a>    indices=tensor([[1], [1]])) 
<a name="l6567"><span class="ln">6567 </span></a>    &gt;&gt;&gt; a.max(dim=1, keepdim=False) 
<a name="l6568"><span class="ln">6568 </span></a>    torch.return_types.max( 
<a name="l6569"><span class="ln">6569 </span></a>    values=tensor([2., 4.]), 
<a name="l6570"><span class="ln">6570 </span></a>    indices=tensor([1, 1])) 
<a name="l6571"><span class="ln">6571 </span></a> 
<a name="l6572"><span class="ln">6572 </span></a>.. function:: max(input, other, *, out=None) -&gt; Tensor 
<a name="l6573"><span class="ln">6573 </span></a>   :noindex: 
<a name="l6574"><span class="ln">6574 </span></a> 
<a name="l6575"><span class="ln">6575 </span></a>See :func:`torch.maximum`. 
<a name="l6576"><span class="ln">6576 </span></a> 
<a name="l6577"><span class="ln">6577 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l6578"><span class="ln">6578 </span></a><span class="s3">)</span>
<a name="l6579"><span class="ln">6579 </span></a>
<a name="l6580"><span class="ln">6580 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6581"><span class="ln">6581 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">maximum</span><span class="s3">,</span>
<a name="l6582"><span class="ln">6582 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6583"><span class="ln">6583 </span></a>maximum(input, other, *, out=None) -&gt; Tensor 
<a name="l6584"><span class="ln">6584 </span></a> 
<a name="l6585"><span class="ln">6585 </span></a>Computes the element-wise maximum of :attr:`input` and :attr:`other`. 
<a name="l6586"><span class="ln">6586 </span></a> 
<a name="l6587"><span class="ln">6587 </span></a>.. note:: 
<a name="l6588"><span class="ln">6588 </span></a>    If one of the elements being compared is a NaN, then that element is returned. 
<a name="l6589"><span class="ln">6589 </span></a>    :func:`maximum` is not supported for tensors with complex dtypes. 
<a name="l6590"><span class="ln">6590 </span></a> 
<a name="l6591"><span class="ln">6591 </span></a>Args: 
<a name="l6592"><span class="ln">6592 </span></a>    {input} 
<a name="l6593"><span class="ln">6593 </span></a>    other (Tensor): the second input tensor 
<a name="l6594"><span class="ln">6594 </span></a> 
<a name="l6595"><span class="ln">6595 </span></a>Keyword args: 
<a name="l6596"><span class="ln">6596 </span></a>    {out} 
<a name="l6597"><span class="ln">6597 </span></a> 
<a name="l6598"><span class="ln">6598 </span></a>Example:: 
<a name="l6599"><span class="ln">6599 </span></a> 
<a name="l6600"><span class="ln">6600 </span></a>    &gt;&gt;&gt; a = torch.tensor((1, 2, -1)) 
<a name="l6601"><span class="ln">6601 </span></a>    &gt;&gt;&gt; b = torch.tensor((3, 0, 4)) 
<a name="l6602"><span class="ln">6602 </span></a>    &gt;&gt;&gt; torch.maximum(a, b) 
<a name="l6603"><span class="ln">6603 </span></a>    tensor([3, 2, 4]) 
<a name="l6604"><span class="ln">6604 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6605"><span class="ln">6605 </span></a><span class="s3">)</span>
<a name="l6606"><span class="ln">6606 </span></a>
<a name="l6607"><span class="ln">6607 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6608"><span class="ln">6608 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">fmax</span><span class="s3">,</span>
<a name="l6609"><span class="ln">6609 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6610"><span class="ln">6610 </span></a>fmax(input, other, *, out=None) -&gt; Tensor 
<a name="l6611"><span class="ln">6611 </span></a> 
<a name="l6612"><span class="ln">6612 </span></a>Computes the element-wise maximum of :attr:`input` and :attr:`other`. 
<a name="l6613"><span class="ln">6613 </span></a> 
<a name="l6614"><span class="ln">6614 </span></a>This is like :func:`torch.maximum` except it handles NaNs differently: 
<a name="l6615"><span class="ln">6615 </span></a>if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. 
<a name="l6616"><span class="ln">6616 </span></a>Only if both elements are NaN is NaN propagated. 
<a name="l6617"><span class="ln">6617 </span></a> 
<a name="l6618"><span class="ln">6618 </span></a>This function is a wrapper around C++'s ``std::fmax`` and is similar to NumPy's ``fmax`` function. 
<a name="l6619"><span class="ln">6619 </span></a> 
<a name="l6620"><span class="ln">6620 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l6621"><span class="ln">6621 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and floating-point inputs. 
<a name="l6622"><span class="ln">6622 </span></a> 
<a name="l6623"><span class="ln">6623 </span></a>Args: 
<a name="l6624"><span class="ln">6624 </span></a>    {input} 
<a name="l6625"><span class="ln">6625 </span></a>    other (Tensor): the second input tensor 
<a name="l6626"><span class="ln">6626 </span></a> 
<a name="l6627"><span class="ln">6627 </span></a>Keyword args: 
<a name="l6628"><span class="ln">6628 </span></a>    {out} 
<a name="l6629"><span class="ln">6629 </span></a> 
<a name="l6630"><span class="ln">6630 </span></a>Example:: 
<a name="l6631"><span class="ln">6631 </span></a> 
<a name="l6632"><span class="ln">6632 </span></a>    &gt;&gt;&gt; a = torch.tensor([9.7, float('nan'), 3.1, float('nan')]) 
<a name="l6633"><span class="ln">6633 </span></a>    &gt;&gt;&gt; b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')]) 
<a name="l6634"><span class="ln">6634 </span></a>    &gt;&gt;&gt; torch.fmax(a, b) 
<a name="l6635"><span class="ln">6635 </span></a>    tensor([9.7000, 0.5000, 3.1000,    nan]) 
<a name="l6636"><span class="ln">6636 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l6637"><span class="ln">6637 </span></a><span class="s3">)</span>
<a name="l6638"><span class="ln">6638 </span></a>
<a name="l6639"><span class="ln">6639 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6640"><span class="ln">6640 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">amax</span><span class="s3">,</span>
<a name="l6641"><span class="ln">6641 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6642"><span class="ln">6642 </span></a>amax(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l6643"><span class="ln">6643 </span></a> 
<a name="l6644"><span class="ln">6644 </span></a>Returns the maximum value of each slice of the :attr:`input` tensor in the given 
<a name="l6645"><span class="ln">6645 </span></a>dimension(s) :attr:`dim`. 
<a name="l6646"><span class="ln">6646 </span></a> 
<a name="l6647"><span class="ln">6647 </span></a>.. note:: 
<a name="l6648"><span class="ln">6648 </span></a>    The difference between ``max``/``min`` and ``amax``/``amin`` is: 
<a name="l6649"><span class="ln">6649 </span></a>        - ``amax``/``amin`` supports reducing on multiple dimensions, 
<a name="l6650"><span class="ln">6650 </span></a>        - ``amax``/``amin`` does not return indices. 
<a name="l6651"><span class="ln">6651 </span></a> 
<a name="l6652"><span class="ln">6652 </span></a>    Both ``max``/``min`` and ``amax``/``amin`` evenly distribute gradients between equal values 
<a name="l6653"><span class="ln">6653 </span></a>    when there are multiple input elements with the same minimum or maximum value. 
<a name="l6654"><span class="ln">6654 </span></a> 
<a name="l6655"><span class="ln">6655 </span></a>{keepdim_details} 
<a name="l6656"><span class="ln">6656 </span></a> 
<a name="l6657"><span class="ln">6657 </span></a>Args: 
<a name="l6658"><span class="ln">6658 </span></a>    {input} 
<a name="l6659"><span class="ln">6659 </span></a>    {opt_dim_all_reduce} 
<a name="l6660"><span class="ln">6660 </span></a>    {opt_keepdim} 
<a name="l6661"><span class="ln">6661 </span></a> 
<a name="l6662"><span class="ln">6662 </span></a>Keyword args: 
<a name="l6663"><span class="ln">6663 </span></a>  {out} 
<a name="l6664"><span class="ln">6664 </span></a> 
<a name="l6665"><span class="ln">6665 </span></a>Example:: 
<a name="l6666"><span class="ln">6666 </span></a> 
<a name="l6667"><span class="ln">6667 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6668"><span class="ln">6668 </span></a>    &gt;&gt;&gt; a 
<a name="l6669"><span class="ln">6669 </span></a>    tensor([[ 0.8177,  1.4878, -0.2491,  0.9130], 
<a name="l6670"><span class="ln">6670 </span></a>            [-0.7158,  1.1775,  2.0992,  0.4817], 
<a name="l6671"><span class="ln">6671 </span></a>            [-0.0053,  0.0164, -1.3738, -0.0507], 
<a name="l6672"><span class="ln">6672 </span></a>            [ 1.9700,  1.1106, -1.0318, -1.0816]]) 
<a name="l6673"><span class="ln">6673 </span></a>    &gt;&gt;&gt; torch.amax(a, 1) 
<a name="l6674"><span class="ln">6674 </span></a>    tensor([1.4878, 2.0992, 0.0164, 1.9700]) 
<a name="l6675"><span class="ln">6675 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l6676"><span class="ln">6676 </span></a><span class="s3">)</span>
<a name="l6677"><span class="ln">6677 </span></a>
<a name="l6678"><span class="ln">6678 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6679"><span class="ln">6679 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">argmax</span><span class="s3">,</span>
<a name="l6680"><span class="ln">6680 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6681"><span class="ln">6681 </span></a>argmax(input) -&gt; LongTensor 
<a name="l6682"><span class="ln">6682 </span></a> 
<a name="l6683"><span class="ln">6683 </span></a>Returns the indices of the maximum value of all elements in the :attr:`input` tensor. 
<a name="l6684"><span class="ln">6684 </span></a> 
<a name="l6685"><span class="ln">6685 </span></a>This is the second value returned by :meth:`torch.max`. See its 
<a name="l6686"><span class="ln">6686 </span></a>documentation for the exact semantics of this method. 
<a name="l6687"><span class="ln">6687 </span></a> 
<a name="l6688"><span class="ln">6688 </span></a>.. note:: If there are multiple maximal values then the indices of the first maximal value are returned. 
<a name="l6689"><span class="ln">6689 </span></a> 
<a name="l6690"><span class="ln">6690 </span></a>Args: 
<a name="l6691"><span class="ln">6691 </span></a>    {input} 
<a name="l6692"><span class="ln">6692 </span></a> 
<a name="l6693"><span class="ln">6693 </span></a>Example:: 
<a name="l6694"><span class="ln">6694 </span></a> 
<a name="l6695"><span class="ln">6695 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6696"><span class="ln">6696 </span></a>    &gt;&gt;&gt; a 
<a name="l6697"><span class="ln">6697 </span></a>    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450], 
<a name="l6698"><span class="ln">6698 </span></a>            [-0.7401, -0.8805, -0.3402, -1.1936], 
<a name="l6699"><span class="ln">6699 </span></a>            [ 0.4907, -1.3948, -1.0691, -0.3132], 
<a name="l6700"><span class="ln">6700 </span></a>            [-1.6092,  0.5419, -0.2993,  0.3195]]) 
<a name="l6701"><span class="ln">6701 </span></a>    &gt;&gt;&gt; torch.argmax(a) 
<a name="l6702"><span class="ln">6702 </span></a>    tensor(0) 
<a name="l6703"><span class="ln">6703 </span></a> 
<a name="l6704"><span class="ln">6704 </span></a>.. function:: argmax(input, dim, keepdim=False) -&gt; LongTensor 
<a name="l6705"><span class="ln">6705 </span></a>   :noindex: 
<a name="l6706"><span class="ln">6706 </span></a> 
<a name="l6707"><span class="ln">6707 </span></a>Returns the indices of the maximum values of a tensor across a dimension. 
<a name="l6708"><span class="ln">6708 </span></a> 
<a name="l6709"><span class="ln">6709 </span></a>This is the second value returned by :meth:`torch.max`. See its 
<a name="l6710"><span class="ln">6710 </span></a>documentation for the exact semantics of this method. 
<a name="l6711"><span class="ln">6711 </span></a> 
<a name="l6712"><span class="ln">6712 </span></a>Args: 
<a name="l6713"><span class="ln">6713 </span></a>    {input} 
<a name="l6714"><span class="ln">6714 </span></a>    {opt_dim} If ``None``, the argmax of the flattened input is returned. 
<a name="l6715"><span class="ln">6715 </span></a>    {opt_keepdim} 
<a name="l6716"><span class="ln">6716 </span></a> 
<a name="l6717"><span class="ln">6717 </span></a>Example:: 
<a name="l6718"><span class="ln">6718 </span></a> 
<a name="l6719"><span class="ln">6719 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6720"><span class="ln">6720 </span></a>    &gt;&gt;&gt; a 
<a name="l6721"><span class="ln">6721 </span></a>    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450], 
<a name="l6722"><span class="ln">6722 </span></a>            [-0.7401, -0.8805, -0.3402, -1.1936], 
<a name="l6723"><span class="ln">6723 </span></a>            [ 0.4907, -1.3948, -1.0691, -0.3132], 
<a name="l6724"><span class="ln">6724 </span></a>            [-1.6092,  0.5419, -0.2993,  0.3195]]) 
<a name="l6725"><span class="ln">6725 </span></a>    &gt;&gt;&gt; torch.argmax(a, dim=1) 
<a name="l6726"><span class="ln">6726 </span></a>    tensor([ 0,  2,  0,  1]) 
<a name="l6727"><span class="ln">6727 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l6728"><span class="ln">6728 </span></a><span class="s3">)</span>
<a name="l6729"><span class="ln">6729 </span></a>
<a name="l6730"><span class="ln">6730 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6731"><span class="ln">6731 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">argwhere</span><span class="s3">,</span>
<a name="l6732"><span class="ln">6732 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6733"><span class="ln">6733 </span></a>argwhere(input) -&gt; Tensor 
<a name="l6734"><span class="ln">6734 </span></a> 
<a name="l6735"><span class="ln">6735 </span></a>Returns a tensor containing the indices of all non-zero elements of 
<a name="l6736"><span class="ln">6736 </span></a>:attr:`input`.  Each row in the result contains the indices of a non-zero 
<a name="l6737"><span class="ln">6737 </span></a>element in :attr:`input`. The result is sorted lexicographically, with 
<a name="l6738"><span class="ln">6738 </span></a>the last index changing the fastest (C-style). 
<a name="l6739"><span class="ln">6739 </span></a> 
<a name="l6740"><span class="ln">6740 </span></a>If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor 
<a name="l6741"><span class="ln">6741 </span></a>:attr:`out` is of size :math:`(z \times n)`, where :math:`z` is the total number of 
<a name="l6742"><span class="ln">6742 </span></a>non-zero elements in the :attr:`input` tensor. 
<a name="l6743"><span class="ln">6743 </span></a> 
<a name="l6744"><span class="ln">6744 </span></a>.. note:: 
<a name="l6745"><span class="ln">6745 </span></a>    This function is similar to NumPy's `argwhere`. 
<a name="l6746"><span class="ln">6746 </span></a> 
<a name="l6747"><span class="ln">6747 </span></a>    When :attr:`input` is on CUDA, this function causes host-device synchronization. 
<a name="l6748"><span class="ln">6748 </span></a> 
<a name="l6749"><span class="ln">6749 </span></a>Args: 
<a name="l6750"><span class="ln">6750 </span></a>    {input} 
<a name="l6751"><span class="ln">6751 </span></a> 
<a name="l6752"><span class="ln">6752 </span></a>Example:: 
<a name="l6753"><span class="ln">6753 </span></a> 
<a name="l6754"><span class="ln">6754 </span></a>    &gt;&gt;&gt; t = torch.tensor([1, 0, 1]) 
<a name="l6755"><span class="ln">6755 </span></a>    &gt;&gt;&gt; torch.argwhere(t) 
<a name="l6756"><span class="ln">6756 </span></a>    tensor([[0], 
<a name="l6757"><span class="ln">6757 </span></a>            [2]]) 
<a name="l6758"><span class="ln">6758 </span></a>    &gt;&gt;&gt; t = torch.tensor([[1, 0, 1], [0, 1, 1]]) 
<a name="l6759"><span class="ln">6759 </span></a>    &gt;&gt;&gt; torch.argwhere(t) 
<a name="l6760"><span class="ln">6760 </span></a>    tensor([[0, 0], 
<a name="l6761"><span class="ln">6761 </span></a>            [0, 2], 
<a name="l6762"><span class="ln">6762 </span></a>            [1, 1], 
<a name="l6763"><span class="ln">6763 </span></a>            [1, 2]]) 
<a name="l6764"><span class="ln">6764 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6765"><span class="ln">6765 </span></a><span class="s3">)</span>
<a name="l6766"><span class="ln">6766 </span></a>
<a name="l6767"><span class="ln">6767 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6768"><span class="ln">6768 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">mean</span><span class="s3">,</span>
<a name="l6769"><span class="ln">6769 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6770"><span class="ln">6770 </span></a>mean(input, *, dtype=None) -&gt; Tensor 
<a name="l6771"><span class="ln">6771 </span></a> 
<a name="l6772"><span class="ln">6772 </span></a>.. note:: 
<a name="l6773"><span class="ln">6773 </span></a>    If the `input` tensor is empty, ``torch.mean()`` returns ``nan``. 
<a name="l6774"><span class="ln">6774 </span></a>    This behavior is consistent with NumPy and follows the definition 
<a name="l6775"><span class="ln">6775 </span></a>    that the mean over an empty set is undefined. 
<a name="l6776"><span class="ln">6776 </span></a> 
<a name="l6777"><span class="ln">6777 </span></a> 
<a name="l6778"><span class="ln">6778 </span></a>Returns the mean value of all elements in the :attr:`input` tensor. Input must be floating point or complex. 
<a name="l6779"><span class="ln">6779 </span></a> 
<a name="l6780"><span class="ln">6780 </span></a>Args: 
<a name="l6781"><span class="ln">6781 </span></a>    input (Tensor): 
<a name="l6782"><span class="ln">6782 </span></a>      the input tensor, either of floating point or complex dtype 
<a name="l6783"><span class="ln">6783 </span></a> 
<a name="l6784"><span class="ln">6784 </span></a>Keyword args: 
<a name="l6785"><span class="ln">6785 </span></a>    {dtype} 
<a name="l6786"><span class="ln">6786 </span></a> 
<a name="l6787"><span class="ln">6787 </span></a>Example:: 
<a name="l6788"><span class="ln">6788 </span></a> 
<a name="l6789"><span class="ln">6789 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l6790"><span class="ln">6790 </span></a>    &gt;&gt;&gt; a 
<a name="l6791"><span class="ln">6791 </span></a>    tensor([[ 0.2294, -0.5481,  1.3288]]) 
<a name="l6792"><span class="ln">6792 </span></a>    &gt;&gt;&gt; torch.mean(a) 
<a name="l6793"><span class="ln">6793 </span></a>    tensor(0.3367) 
<a name="l6794"><span class="ln">6794 </span></a> 
<a name="l6795"><span class="ln">6795 </span></a>.. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -&gt; Tensor 
<a name="l6796"><span class="ln">6796 </span></a>   :noindex: 
<a name="l6797"><span class="ln">6797 </span></a> 
<a name="l6798"><span class="ln">6798 </span></a>Returns the mean value of each row of the :attr:`input` tensor in the given 
<a name="l6799"><span class="ln">6799 </span></a>dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l6800"><span class="ln">6800 </span></a>reduce over all of them. 
<a name="l6801"><span class="ln">6801 </span></a> 
<a name="l6802"><span class="ln">6802 </span></a>{keepdim_details} 
<a name="l6803"><span class="ln">6803 </span></a> 
<a name="l6804"><span class="ln">6804 </span></a>Args: 
<a name="l6805"><span class="ln">6805 </span></a>    {input} 
<a name="l6806"><span class="ln">6806 </span></a>    {opt_dim_all_reduce} 
<a name="l6807"><span class="ln">6807 </span></a>    {opt_keepdim} 
<a name="l6808"><span class="ln">6808 </span></a> 
<a name="l6809"><span class="ln">6809 </span></a>Keyword args: 
<a name="l6810"><span class="ln">6810 </span></a>    {dtype} 
<a name="l6811"><span class="ln">6811 </span></a>    {out} 
<a name="l6812"><span class="ln">6812 </span></a> 
<a name="l6813"><span class="ln">6813 </span></a>.. seealso:: 
<a name="l6814"><span class="ln">6814 </span></a> 
<a name="l6815"><span class="ln">6815 </span></a>    :func:`torch.nanmean` computes the mean value of `non-NaN` elements. 
<a name="l6816"><span class="ln">6816 </span></a> 
<a name="l6817"><span class="ln">6817 </span></a>Example:: 
<a name="l6818"><span class="ln">6818 </span></a> 
<a name="l6819"><span class="ln">6819 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6820"><span class="ln">6820 </span></a>    &gt;&gt;&gt; a 
<a name="l6821"><span class="ln">6821 </span></a>    tensor([[-0.3841,  0.6320,  0.4254, -0.7384], 
<a name="l6822"><span class="ln">6822 </span></a>            [-0.9644,  1.0131, -0.6549, -1.4279], 
<a name="l6823"><span class="ln">6823 </span></a>            [-0.2951, -1.3350, -0.7694,  0.5600], 
<a name="l6824"><span class="ln">6824 </span></a>            [ 1.0842, -0.9580,  0.3623,  0.2343]]) 
<a name="l6825"><span class="ln">6825 </span></a>    &gt;&gt;&gt; torch.mean(a, 1) 
<a name="l6826"><span class="ln">6826 </span></a>    tensor([-0.0163, -0.5085, -0.4599,  0.1807]) 
<a name="l6827"><span class="ln">6827 </span></a>    &gt;&gt;&gt; torch.mean(a, 1, True) 
<a name="l6828"><span class="ln">6828 </span></a>    tensor([[-0.0163], 
<a name="l6829"><span class="ln">6829 </span></a>            [-0.5085], 
<a name="l6830"><span class="ln">6830 </span></a>            [-0.4599], 
<a name="l6831"><span class="ln">6831 </span></a>            [ 0.1807]]) 
<a name="l6832"><span class="ln">6832 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l6833"><span class="ln">6833 </span></a><span class="s3">)</span>
<a name="l6834"><span class="ln">6834 </span></a>
<a name="l6835"><span class="ln">6835 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6836"><span class="ln">6836 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">nanmean</span><span class="s3">,</span>
<a name="l6837"><span class="ln">6837 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6838"><span class="ln">6838 </span></a>nanmean(input, dim=None, keepdim=False, *, dtype=None, out=None) -&gt; Tensor 
<a name="l6839"><span class="ln">6839 </span></a> 
<a name="l6840"><span class="ln">6840 </span></a>Computes the mean of all `non-NaN` elements along the specified dimensions. 
<a name="l6841"><span class="ln">6841 </span></a>Input must be floating point or complex. 
<a name="l6842"><span class="ln">6842 </span></a> 
<a name="l6843"><span class="ln">6843 </span></a>This function is identical to :func:`torch.mean` when there are no `NaN` values 
<a name="l6844"><span class="ln">6844 </span></a>in the :attr:`input` tensor. In the presence of `NaN`, :func:`torch.mean` will 
<a name="l6845"><span class="ln">6845 </span></a>propagate the `NaN` to the output whereas :func:`torch.nanmean` will ignore the 
<a name="l6846"><span class="ln">6846 </span></a>`NaN` values (`torch.nanmean(a)` is equivalent to `torch.mean(a[~a.isnan()])`). 
<a name="l6847"><span class="ln">6847 </span></a> 
<a name="l6848"><span class="ln">6848 </span></a>{keepdim_details} 
<a name="l6849"><span class="ln">6849 </span></a> 
<a name="l6850"><span class="ln">6850 </span></a>Args: 
<a name="l6851"><span class="ln">6851 </span></a>    input (Tensor): the input tensor, either of floating point or complex dtype 
<a name="l6852"><span class="ln">6852 </span></a>    {opt_dim_all_reduce} 
<a name="l6853"><span class="ln">6853 </span></a>    {opt_keepdim} 
<a name="l6854"><span class="ln">6854 </span></a> 
<a name="l6855"><span class="ln">6855 </span></a>Keyword args: 
<a name="l6856"><span class="ln">6856 </span></a>    {dtype} 
<a name="l6857"><span class="ln">6857 </span></a>    {out} 
<a name="l6858"><span class="ln">6858 </span></a> 
<a name="l6859"><span class="ln">6859 </span></a>.. seealso:: 
<a name="l6860"><span class="ln">6860 </span></a> 
<a name="l6861"><span class="ln">6861 </span></a>    :func:`torch.mean` computes the mean value, propagating `NaN`. 
<a name="l6862"><span class="ln">6862 </span></a> 
<a name="l6863"><span class="ln">6863 </span></a>Example:: 
<a name="l6864"><span class="ln">6864 </span></a> 
<a name="l6865"><span class="ln">6865 </span></a>    &gt;&gt;&gt; x = torch.tensor([[torch.nan, 1, 2], [1, 2, 3]]) 
<a name="l6866"><span class="ln">6866 </span></a>    &gt;&gt;&gt; x.mean() 
<a name="l6867"><span class="ln">6867 </span></a>    tensor(nan) 
<a name="l6868"><span class="ln">6868 </span></a>    &gt;&gt;&gt; x.nanmean() 
<a name="l6869"><span class="ln">6869 </span></a>    tensor(1.8000) 
<a name="l6870"><span class="ln">6870 </span></a>    &gt;&gt;&gt; x.mean(dim=0) 
<a name="l6871"><span class="ln">6871 </span></a>    tensor([   nan, 1.5000, 2.5000]) 
<a name="l6872"><span class="ln">6872 </span></a>    &gt;&gt;&gt; x.nanmean(dim=0) 
<a name="l6873"><span class="ln">6873 </span></a>    tensor([1.0000, 1.5000, 2.5000]) 
<a name="l6874"><span class="ln">6874 </span></a> 
<a name="l6875"><span class="ln">6875 </span></a>    # If all elements in the reduced dimensions are NaN then the result is NaN 
<a name="l6876"><span class="ln">6876 </span></a>    &gt;&gt;&gt; torch.tensor([torch.nan]).nanmean() 
<a name="l6877"><span class="ln">6877 </span></a>    tensor(nan) 
<a name="l6878"><span class="ln">6878 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l6879"><span class="ln">6879 </span></a><span class="s3">)</span>
<a name="l6880"><span class="ln">6880 </span></a>
<a name="l6881"><span class="ln">6881 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6882"><span class="ln">6882 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">median</span><span class="s3">,</span>
<a name="l6883"><span class="ln">6883 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6884"><span class="ln">6884 </span></a>median(input) -&gt; Tensor 
<a name="l6885"><span class="ln">6885 </span></a> 
<a name="l6886"><span class="ln">6886 </span></a>Returns the median of the values in :attr:`input`. 
<a name="l6887"><span class="ln">6887 </span></a> 
<a name="l6888"><span class="ln">6888 </span></a>.. note:: 
<a name="l6889"><span class="ln">6889 </span></a>    The median is not unique for :attr:`input` tensors with an even number 
<a name="l6890"><span class="ln">6890 </span></a>    of elements. In this case the lower of the two medians is returned. To 
<a name="l6891"><span class="ln">6891 </span></a>    compute the mean of both medians, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l6892"><span class="ln">6892 </span></a> 
<a name="l6893"><span class="ln">6893 </span></a>.. warning:: 
<a name="l6894"><span class="ln">6894 </span></a>    This function produces deterministic (sub)gradients unlike ``median(dim=0)`` 
<a name="l6895"><span class="ln">6895 </span></a> 
<a name="l6896"><span class="ln">6896 </span></a>Args: 
<a name="l6897"><span class="ln">6897 </span></a>    {input} 
<a name="l6898"><span class="ln">6898 </span></a> 
<a name="l6899"><span class="ln">6899 </span></a>Example:: 
<a name="l6900"><span class="ln">6900 </span></a> 
<a name="l6901"><span class="ln">6901 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l6902"><span class="ln">6902 </span></a>    &gt;&gt;&gt; a 
<a name="l6903"><span class="ln">6903 </span></a>    tensor([[ 1.5219, -1.5212,  0.2202]]) 
<a name="l6904"><span class="ln">6904 </span></a>    &gt;&gt;&gt; torch.median(a) 
<a name="l6905"><span class="ln">6905 </span></a>    tensor(0.2202) 
<a name="l6906"><span class="ln">6906 </span></a> 
<a name="l6907"><span class="ln">6907 </span></a>.. function:: median(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l6908"><span class="ln">6908 </span></a>   :noindex: 
<a name="l6909"><span class="ln">6909 </span></a> 
<a name="l6910"><span class="ln">6910 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l6911"><span class="ln">6911 </span></a>in the dimension :attr:`dim`, and ``indices`` contains the index of the median values found in the dimension :attr:`dim`. 
<a name="l6912"><span class="ln">6912 </span></a> 
<a name="l6913"><span class="ln">6913 </span></a>By default, :attr:`dim` is the last dimension of the :attr:`input` tensor. 
<a name="l6914"><span class="ln">6914 </span></a> 
<a name="l6915"><span class="ln">6915 </span></a>If :attr:`keepdim` is ``True``, the output tensors are of the same size 
<a name="l6916"><span class="ln">6916 </span></a>as :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l6917"><span class="ln">6917 </span></a>Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l6918"><span class="ln">6918 </span></a>the outputs tensor having 1 fewer dimension than :attr:`input`. 
<a name="l6919"><span class="ln">6919 </span></a> 
<a name="l6920"><span class="ln">6920 </span></a>.. note:: 
<a name="l6921"><span class="ln">6921 </span></a>    The median is not unique for :attr:`input` tensors with an even number 
<a name="l6922"><span class="ln">6922 </span></a>    of elements in the dimension :attr:`dim`. In this case the lower of the 
<a name="l6923"><span class="ln">6923 </span></a>    two medians is returned. To compute the mean of both medians in 
<a name="l6924"><span class="ln">6924 </span></a>    :attr:`input`, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l6925"><span class="ln">6925 </span></a> 
<a name="l6926"><span class="ln">6926 </span></a>.. warning:: 
<a name="l6927"><span class="ln">6927 </span></a>    ``indices`` does not necessarily contain the first occurrence of each 
<a name="l6928"><span class="ln">6928 </span></a>    median value found, unless it is unique. 
<a name="l6929"><span class="ln">6929 </span></a>    The exact implementation details are device-specific. 
<a name="l6930"><span class="ln">6930 </span></a>    Do not expect the same result when run on CPU and GPU in general. 
<a name="l6931"><span class="ln">6931 </span></a>    For the same reason do not expect the gradients to be deterministic. 
<a name="l6932"><span class="ln">6932 </span></a> 
<a name="l6933"><span class="ln">6933 </span></a>Args: 
<a name="l6934"><span class="ln">6934 </span></a>    {input} 
<a name="l6935"><span class="ln">6935 </span></a>    {opt_dim_all_reduce} 
<a name="l6936"><span class="ln">6936 </span></a>    {opt_keepdim} 
<a name="l6937"><span class="ln">6937 </span></a> 
<a name="l6938"><span class="ln">6938 </span></a>Keyword args: 
<a name="l6939"><span class="ln">6939 </span></a>    out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l6940"><span class="ln">6940 </span></a>                                      tensor, which must have dtype long, with their indices in the dimension 
<a name="l6941"><span class="ln">6941 </span></a>                                      :attr:`dim` of :attr:`input`. 
<a name="l6942"><span class="ln">6942 </span></a> 
<a name="l6943"><span class="ln">6943 </span></a>Example:: 
<a name="l6944"><span class="ln">6944 </span></a> 
<a name="l6945"><span class="ln">6945 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 5) 
<a name="l6946"><span class="ln">6946 </span></a>    &gt;&gt;&gt; a 
<a name="l6947"><span class="ln">6947 </span></a>    tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131], 
<a name="l6948"><span class="ln">6948 </span></a>            [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270], 
<a name="l6949"><span class="ln">6949 </span></a>            [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488], 
<a name="l6950"><span class="ln">6950 </span></a>            [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]]) 
<a name="l6951"><span class="ln">6951 </span></a>    &gt;&gt;&gt; torch.median(a, 1) 
<a name="l6952"><span class="ln">6952 </span></a>    torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3])) 
<a name="l6953"><span class="ln">6953 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l6954"><span class="ln">6954 </span></a><span class="s3">)</span>
<a name="l6955"><span class="ln">6955 </span></a>
<a name="l6956"><span class="ln">6956 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l6957"><span class="ln">6957 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">nanmedian</span><span class="s3">,</span>
<a name="l6958"><span class="ln">6958 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6959"><span class="ln">6959 </span></a>nanmedian(input) -&gt; Tensor 
<a name="l6960"><span class="ln">6960 </span></a> 
<a name="l6961"><span class="ln">6961 </span></a>Returns the median of the values in :attr:`input`, ignoring ``NaN`` values. 
<a name="l6962"><span class="ln">6962 </span></a> 
<a name="l6963"><span class="ln">6963 </span></a>This function is identical to :func:`torch.median` when there are no ``NaN`` values in :attr:`input`. 
<a name="l6964"><span class="ln">6964 </span></a>When :attr:`input` has one or more ``NaN`` values, :func:`torch.median` will always return ``NaN``, 
<a name="l6965"><span class="ln">6965 </span></a>while this function will return the median of the non-``NaN`` elements in :attr:`input`. 
<a name="l6966"><span class="ln">6966 </span></a>If all the elements in :attr:`input` are ``NaN`` it will also return ``NaN``. 
<a name="l6967"><span class="ln">6967 </span></a> 
<a name="l6968"><span class="ln">6968 </span></a>Args: 
<a name="l6969"><span class="ln">6969 </span></a>    {input} 
<a name="l6970"><span class="ln">6970 </span></a> 
<a name="l6971"><span class="ln">6971 </span></a>Example:: 
<a name="l6972"><span class="ln">6972 </span></a> 
<a name="l6973"><span class="ln">6973 </span></a>    &gt;&gt;&gt; a = torch.tensor([1, float('nan'), 3, 2]) 
<a name="l6974"><span class="ln">6974 </span></a>    &gt;&gt;&gt; a.median() 
<a name="l6975"><span class="ln">6975 </span></a>    tensor(nan) 
<a name="l6976"><span class="ln">6976 </span></a>    &gt;&gt;&gt; a.nanmedian() 
<a name="l6977"><span class="ln">6977 </span></a>    tensor(2.) 
<a name="l6978"><span class="ln">6978 </span></a> 
<a name="l6979"><span class="ln">6979 </span></a>.. function:: nanmedian(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l6980"><span class="ln">6980 </span></a>   :noindex: 
<a name="l6981"><span class="ln">6981 </span></a> 
<a name="l6982"><span class="ln">6982 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l6983"><span class="ln">6983 </span></a>in the dimension :attr:`dim`, ignoring ``NaN`` values, and ``indices`` contains the index of the median values 
<a name="l6984"><span class="ln">6984 </span></a>found in the dimension :attr:`dim`. 
<a name="l6985"><span class="ln">6985 </span></a> 
<a name="l6986"><span class="ln">6986 </span></a>This function is identical to :func:`torch.median` when there are no ``NaN`` values in a reduced row. When a reduced row has 
<a name="l6987"><span class="ln">6987 </span></a>one or more ``NaN`` values, :func:`torch.median` will always reduce it to ``NaN``, while this function will reduce it to the 
<a name="l6988"><span class="ln">6988 </span></a>median of the non-``NaN`` elements. If all the elements in a reduced row are ``NaN`` then it will be reduced to ``NaN``, too. 
<a name="l6989"><span class="ln">6989 </span></a> 
<a name="l6990"><span class="ln">6990 </span></a>Args: 
<a name="l6991"><span class="ln">6991 </span></a>    {input} 
<a name="l6992"><span class="ln">6992 </span></a>    {opt_dim_all_reduce} 
<a name="l6993"><span class="ln">6993 </span></a>    {opt_keepdim} 
<a name="l6994"><span class="ln">6994 </span></a> 
<a name="l6995"><span class="ln">6995 </span></a>Keyword args: 
<a name="l6996"><span class="ln">6996 </span></a>    out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l6997"><span class="ln">6997 </span></a>                                      tensor, which must have dtype long, with their indices in the dimension 
<a name="l6998"><span class="ln">6998 </span></a>                                      :attr:`dim` of :attr:`input`. 
<a name="l6999"><span class="ln">6999 </span></a> 
<a name="l7000"><span class="ln">7000 </span></a>Example:: 
<a name="l7001"><span class="ln">7001 </span></a> 
<a name="l7002"><span class="ln">7002 </span></a>    &gt;&gt;&gt; a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]]) 
<a name="l7003"><span class="ln">7003 </span></a>    &gt;&gt;&gt; a 
<a name="l7004"><span class="ln">7004 </span></a>    tensor([[2., 3., 1.], 
<a name="l7005"><span class="ln">7005 </span></a>            [nan, 1., nan]]) 
<a name="l7006"><span class="ln">7006 </span></a>    &gt;&gt;&gt; a.median(0) 
<a name="l7007"><span class="ln">7007 </span></a>    torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1])) 
<a name="l7008"><span class="ln">7008 </span></a>    &gt;&gt;&gt; a.nanmedian(0) 
<a name="l7009"><span class="ln">7009 </span></a>    torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0])) 
<a name="l7010"><span class="ln">7010 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l7011"><span class="ln">7011 </span></a><span class="s3">)</span>
<a name="l7012"><span class="ln">7012 </span></a>
<a name="l7013"><span class="ln">7013 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7014"><span class="ln">7014 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">quantile</span><span class="s3">,</span>
<a name="l7015"><span class="ln">7015 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7016"><span class="ln">7016 </span></a>quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -&gt; Tensor 
<a name="l7017"><span class="ln">7017 </span></a> 
<a name="l7018"><span class="ln">7018 </span></a>Computes the q-th quantiles of each row of the :attr:`input` tensor along the dimension :attr:`dim`. 
<a name="l7019"><span class="ln">7019 </span></a> 
<a name="l7020"><span class="ln">7020 </span></a>To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location 
<a name="l7021"><span class="ln">7021 </span></a>of the quantile in the sorted input. If the quantile lies between two data points ``a &lt; b`` with 
<a name="l7022"><span class="ln">7022 </span></a>indices ``i`` and ``j`` in the sorted order, result is computed according to the given 
<a name="l7023"><span class="ln">7023 </span></a>:attr:`interpolation` method as follows: 
<a name="l7024"><span class="ln">7024 </span></a> 
<a name="l7025"><span class="ln">7025 </span></a>- ``linear``: ``a + (b - a) * fraction``, where ``fraction`` is the fractional part of the computed quantile index. 
<a name="l7026"><span class="ln">7026 </span></a>- ``lower``: ``a``. 
<a name="l7027"><span class="ln">7027 </span></a>- ``higher``: ``b``. 
<a name="l7028"><span class="ln">7028 </span></a>- ``nearest``: ``a`` or ``b``, whichever's index is closer to the computed quantile index (rounding down for .5 fractions). 
<a name="l7029"><span class="ln">7029 </span></a>- ``midpoint``: ``(a + b) / 2``. 
<a name="l7030"><span class="ln">7030 </span></a> 
<a name="l7031"><span class="ln">7031 </span></a>If :attr:`q` is a 1D tensor, the first dimension of the output represents the quantiles and has size 
<a name="l7032"><span class="ln">7032 </span></a>equal to the size of :attr:`q`, the remaining dimensions are what remains from the reduction. 
<a name="l7033"><span class="ln">7033 </span></a> 
<a name="l7034"><span class="ln">7034 </span></a>.. note:: 
<a name="l7035"><span class="ln">7035 </span></a>    By default :attr:`dim` is ``None`` resulting in the :attr:`input` tensor being flattened before computation. 
<a name="l7036"><span class="ln">7036 </span></a> 
<a name="l7037"><span class="ln">7037 </span></a>Args: 
<a name="l7038"><span class="ln">7038 </span></a>    {input} 
<a name="l7039"><span class="ln">7039 </span></a>    q (float or Tensor): a scalar or 1D tensor of values in the range [0, 1]. 
<a name="l7040"><span class="ln">7040 </span></a>    {opt_dim} 
<a name="l7041"><span class="ln">7041 </span></a>    {opt_keepdim} 
<a name="l7042"><span class="ln">7042 </span></a> 
<a name="l7043"><span class="ln">7043 </span></a>Keyword arguments: 
<a name="l7044"><span class="ln">7044 </span></a>    interpolation (str): interpolation method to use when the desired quantile lies between two data points. 
<a name="l7045"><span class="ln">7045 </span></a>                            Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``. 
<a name="l7046"><span class="ln">7046 </span></a>                            Default is ``linear``. 
<a name="l7047"><span class="ln">7047 </span></a>    {out} 
<a name="l7048"><span class="ln">7048 </span></a> 
<a name="l7049"><span class="ln">7049 </span></a>Example:: 
<a name="l7050"><span class="ln">7050 </span></a> 
<a name="l7051"><span class="ln">7051 </span></a>    &gt;&gt;&gt; a = torch.randn(2, 3) 
<a name="l7052"><span class="ln">7052 </span></a>    &gt;&gt;&gt; a 
<a name="l7053"><span class="ln">7053 </span></a>    tensor([[ 0.0795, -1.2117,  0.9765], 
<a name="l7054"><span class="ln">7054 </span></a>            [ 1.1707,  0.6706,  0.4884]]) 
<a name="l7055"><span class="ln">7055 </span></a>    &gt;&gt;&gt; q = torch.tensor([0.25, 0.5, 0.75]) 
<a name="l7056"><span class="ln">7056 </span></a>    &gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True) 
<a name="l7057"><span class="ln">7057 </span></a>    tensor([[[-0.5661], 
<a name="l7058"><span class="ln">7058 </span></a>            [ 0.5795]], 
<a name="l7059"><span class="ln">7059 </span></a> 
<a name="l7060"><span class="ln">7060 </span></a>            [[ 0.0795], 
<a name="l7061"><span class="ln">7061 </span></a>            [ 0.6706]], 
<a name="l7062"><span class="ln">7062 </span></a> 
<a name="l7063"><span class="ln">7063 </span></a>            [[ 0.5280], 
<a name="l7064"><span class="ln">7064 </span></a>            [ 0.9206]]]) 
<a name="l7065"><span class="ln">7065 </span></a>    &gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True).shape 
<a name="l7066"><span class="ln">7066 </span></a>    torch.Size([3, 2, 1]) 
<a name="l7067"><span class="ln">7067 </span></a>    &gt;&gt;&gt; a = torch.arange(4.) 
<a name="l7068"><span class="ln">7068 </span></a>    &gt;&gt;&gt; a 
<a name="l7069"><span class="ln">7069 </span></a>    tensor([0., 1., 2., 3.]) 
<a name="l7070"><span class="ln">7070 </span></a>    &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='linear') 
<a name="l7071"><span class="ln">7071 </span></a>    tensor(1.8000) 
<a name="l7072"><span class="ln">7072 </span></a>    &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='lower') 
<a name="l7073"><span class="ln">7073 </span></a>    tensor(1.) 
<a name="l7074"><span class="ln">7074 </span></a>    &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='higher') 
<a name="l7075"><span class="ln">7075 </span></a>    tensor(2.) 
<a name="l7076"><span class="ln">7076 </span></a>    &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='midpoint') 
<a name="l7077"><span class="ln">7077 </span></a>    tensor(1.5000) 
<a name="l7078"><span class="ln">7078 </span></a>    &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='nearest') 
<a name="l7079"><span class="ln">7079 </span></a>    tensor(2.) 
<a name="l7080"><span class="ln">7080 </span></a>    &gt;&gt;&gt; torch.quantile(a, 0.4, interpolation='nearest') 
<a name="l7081"><span class="ln">7081 </span></a>    tensor(1.) 
<a name="l7082"><span class="ln">7082 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l7083"><span class="ln">7083 </span></a><span class="s3">)</span>
<a name="l7084"><span class="ln">7084 </span></a>
<a name="l7085"><span class="ln">7085 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7086"><span class="ln">7086 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">nanquantile</span><span class="s3">,</span>
<a name="l7087"><span class="ln">7087 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7088"><span class="ln">7088 </span></a>nanquantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -&gt; Tensor 
<a name="l7089"><span class="ln">7089 </span></a> 
<a name="l7090"><span class="ln">7090 </span></a>This is a variant of :func:`torch.quantile` that &quot;ignores&quot; ``NaN`` values, 
<a name="l7091"><span class="ln">7091 </span></a>computing the quantiles :attr:`q` as if ``NaN`` values in :attr:`input` did 
<a name="l7092"><span class="ln">7092 </span></a>not exist. If all values in a reduced row are ``NaN`` then the quantiles for 
<a name="l7093"><span class="ln">7093 </span></a>that reduction will be ``NaN``. See the documentation for :func:`torch.quantile`. 
<a name="l7094"><span class="ln">7094 </span></a> 
<a name="l7095"><span class="ln">7095 </span></a>Args: 
<a name="l7096"><span class="ln">7096 </span></a>    {input} 
<a name="l7097"><span class="ln">7097 </span></a>    q (float or Tensor): a scalar or 1D tensor of quantile values in the range [0, 1] 
<a name="l7098"><span class="ln">7098 </span></a>    {opt_dim_all_reduce} 
<a name="l7099"><span class="ln">7099 </span></a>    {opt_keepdim} 
<a name="l7100"><span class="ln">7100 </span></a> 
<a name="l7101"><span class="ln">7101 </span></a>Keyword arguments: 
<a name="l7102"><span class="ln">7102 </span></a>    interpolation (str): interpolation method to use when the desired quantile lies between two data points. 
<a name="l7103"><span class="ln">7103 </span></a>                            Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``. 
<a name="l7104"><span class="ln">7104 </span></a>                            Default is ``linear``. 
<a name="l7105"><span class="ln">7105 </span></a>    {out} 
<a name="l7106"><span class="ln">7106 </span></a> 
<a name="l7107"><span class="ln">7107 </span></a>Example:: 
<a name="l7108"><span class="ln">7108 </span></a> 
<a name="l7109"><span class="ln">7109 </span></a>    &gt;&gt;&gt; t = torch.tensor([float('nan'), 1, 2]) 
<a name="l7110"><span class="ln">7110 </span></a>    &gt;&gt;&gt; t.quantile(0.5) 
<a name="l7111"><span class="ln">7111 </span></a>    tensor(nan) 
<a name="l7112"><span class="ln">7112 </span></a>    &gt;&gt;&gt; t.nanquantile(0.5) 
<a name="l7113"><span class="ln">7113 </span></a>    tensor(1.5000) 
<a name="l7114"><span class="ln">7114 </span></a>    &gt;&gt;&gt; t = torch.tensor([[float('nan'), float('nan')], [1, 2]]) 
<a name="l7115"><span class="ln">7115 </span></a>    &gt;&gt;&gt; t 
<a name="l7116"><span class="ln">7116 </span></a>    tensor([[nan, nan], 
<a name="l7117"><span class="ln">7117 </span></a>            [1., 2.]]) 
<a name="l7118"><span class="ln">7118 </span></a>    &gt;&gt;&gt; t.nanquantile(0.5, dim=0) 
<a name="l7119"><span class="ln">7119 </span></a>    tensor([1., 2.]) 
<a name="l7120"><span class="ln">7120 </span></a>    &gt;&gt;&gt; t.nanquantile(0.5, dim=1) 
<a name="l7121"><span class="ln">7121 </span></a>    tensor([   nan, 1.5000]) 
<a name="l7122"><span class="ln">7122 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l7123"><span class="ln">7123 </span></a><span class="s3">)</span>
<a name="l7124"><span class="ln">7124 </span></a>
<a name="l7125"><span class="ln">7125 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7126"><span class="ln">7126 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">min</span><span class="s3">,</span>
<a name="l7127"><span class="ln">7127 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7128"><span class="ln">7128 </span></a>min(input, *, out=None) -&gt; Tensor 
<a name="l7129"><span class="ln">7129 </span></a> 
<a name="l7130"><span class="ln">7130 </span></a>Returns the minimum value of all elements in the :attr:`input` tensor. 
<a name="l7131"><span class="ln">7131 </span></a> 
<a name="l7132"><span class="ln">7132 </span></a>Args: 
<a name="l7133"><span class="ln">7133 </span></a>    {input} 
<a name="l7134"><span class="ln">7134 </span></a> 
<a name="l7135"><span class="ln">7135 </span></a>Keyword args: 
<a name="l7136"><span class="ln">7136 </span></a>    {out} 
<a name="l7137"><span class="ln">7137 </span></a> 
<a name="l7138"><span class="ln">7138 </span></a>Example:: 
<a name="l7139"><span class="ln">7139 </span></a> 
<a name="l7140"><span class="ln">7140 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l7141"><span class="ln">7141 </span></a>    &gt;&gt;&gt; a 
<a name="l7142"><span class="ln">7142 </span></a>    tensor([[ 0.6750,  1.0857,  1.7197]]) 
<a name="l7143"><span class="ln">7143 </span></a>    &gt;&gt;&gt; torch.min(a) 
<a name="l7144"><span class="ln">7144 </span></a>    tensor(0.6750) 
<a name="l7145"><span class="ln">7145 </span></a> 
<a name="l7146"><span class="ln">7146 </span></a>.. function:: min(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l7147"><span class="ln">7147 </span></a>   :noindex: 
<a name="l7148"><span class="ln">7148 </span></a> 
<a name="l7149"><span class="ln">7149 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum 
<a name="l7150"><span class="ln">7150 </span></a>value of each row of the :attr:`input` tensor in the given dimension 
<a name="l7151"><span class="ln">7151 </span></a>:attr:`dim`. And ``indices`` is the index location of each minimum value found 
<a name="l7152"><span class="ln">7152 </span></a>(argmin). 
<a name="l7153"><span class="ln">7153 </span></a> 
<a name="l7154"><span class="ln">7154 </span></a>If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l7155"><span class="ln">7155 </span></a>:attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l7156"><span class="ln">7156 </span></a>Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l7157"><span class="ln">7157 </span></a>the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l7158"><span class="ln">7158 </span></a> 
<a name="l7159"><span class="ln">7159 </span></a>.. note:: If there are multiple minimal values in a reduced row then 
<a name="l7160"><span class="ln">7160 </span></a>          the indices of the first minimal value are returned. 
<a name="l7161"><span class="ln">7161 </span></a> 
<a name="l7162"><span class="ln">7162 </span></a>Args: 
<a name="l7163"><span class="ln">7163 </span></a>    {input} 
<a name="l7164"><span class="ln">7164 </span></a>    {opt_dim_all_reduce} 
<a name="l7165"><span class="ln">7165 </span></a>    {opt_keepdim} 
<a name="l7166"><span class="ln">7166 </span></a> 
<a name="l7167"><span class="ln">7167 </span></a>Keyword args: 
<a name="l7168"><span class="ln">7168 </span></a>    out (tuple, optional): the tuple of two output tensors (min, min_indices) 
<a name="l7169"><span class="ln">7169 </span></a> 
<a name="l7170"><span class="ln">7170 </span></a>Example:: 
<a name="l7171"><span class="ln">7171 </span></a> 
<a name="l7172"><span class="ln">7172 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l7173"><span class="ln">7173 </span></a>    &gt;&gt;&gt; a 
<a name="l7174"><span class="ln">7174 </span></a>    tensor([[-0.6248,  1.1334, -1.1899, -0.2803], 
<a name="l7175"><span class="ln">7175 </span></a>            [-1.4644, -0.2635, -0.3651,  0.6134], 
<a name="l7176"><span class="ln">7176 </span></a>            [ 0.2457,  0.0384,  1.0128,  0.7015], 
<a name="l7177"><span class="ln">7177 </span></a>            [-0.1153,  2.9849,  2.1458,  0.5788]]) 
<a name="l7178"><span class="ln">7178 </span></a>    &gt;&gt;&gt; torch.min(a, 1) 
<a name="l7179"><span class="ln">7179 </span></a>    torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0])) 
<a name="l7180"><span class="ln">7180 </span></a> 
<a name="l7181"><span class="ln">7181 </span></a>.. function:: min(input, other, *, out=None) -&gt; Tensor 
<a name="l7182"><span class="ln">7182 </span></a>   :noindex: 
<a name="l7183"><span class="ln">7183 </span></a> 
<a name="l7184"><span class="ln">7184 </span></a>See :func:`torch.minimum`. 
<a name="l7185"><span class="ln">7185 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l7186"><span class="ln">7186 </span></a><span class="s3">)</span>
<a name="l7187"><span class="ln">7187 </span></a>
<a name="l7188"><span class="ln">7188 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7189"><span class="ln">7189 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">minimum</span><span class="s3">,</span>
<a name="l7190"><span class="ln">7190 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7191"><span class="ln">7191 </span></a>minimum(input, other, *, out=None) -&gt; Tensor 
<a name="l7192"><span class="ln">7192 </span></a> 
<a name="l7193"><span class="ln">7193 </span></a>Computes the element-wise minimum of :attr:`input` and :attr:`other`. 
<a name="l7194"><span class="ln">7194 </span></a> 
<a name="l7195"><span class="ln">7195 </span></a>.. note:: 
<a name="l7196"><span class="ln">7196 </span></a>    If one of the elements being compared is a NaN, then that element is returned. 
<a name="l7197"><span class="ln">7197 </span></a>    :func:`minimum` is not supported for tensors with complex dtypes. 
<a name="l7198"><span class="ln">7198 </span></a> 
<a name="l7199"><span class="ln">7199 </span></a>Args: 
<a name="l7200"><span class="ln">7200 </span></a>    {input} 
<a name="l7201"><span class="ln">7201 </span></a>    other (Tensor): the second input tensor 
<a name="l7202"><span class="ln">7202 </span></a> 
<a name="l7203"><span class="ln">7203 </span></a>Keyword args: 
<a name="l7204"><span class="ln">7204 </span></a>    {out} 
<a name="l7205"><span class="ln">7205 </span></a> 
<a name="l7206"><span class="ln">7206 </span></a>Example:: 
<a name="l7207"><span class="ln">7207 </span></a> 
<a name="l7208"><span class="ln">7208 </span></a>    &gt;&gt;&gt; a = torch.tensor((1, 2, -1)) 
<a name="l7209"><span class="ln">7209 </span></a>    &gt;&gt;&gt; b = torch.tensor((3, 0, 4)) 
<a name="l7210"><span class="ln">7210 </span></a>    &gt;&gt;&gt; torch.minimum(a, b) 
<a name="l7211"><span class="ln">7211 </span></a>    tensor([1, 0, -1]) 
<a name="l7212"><span class="ln">7212 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7213"><span class="ln">7213 </span></a><span class="s3">)</span>
<a name="l7214"><span class="ln">7214 </span></a>
<a name="l7215"><span class="ln">7215 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7216"><span class="ln">7216 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">fmin</span><span class="s3">,</span>
<a name="l7217"><span class="ln">7217 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7218"><span class="ln">7218 </span></a>fmin(input, other, *, out=None) -&gt; Tensor 
<a name="l7219"><span class="ln">7219 </span></a> 
<a name="l7220"><span class="ln">7220 </span></a>Computes the element-wise minimum of :attr:`input` and :attr:`other`. 
<a name="l7221"><span class="ln">7221 </span></a> 
<a name="l7222"><span class="ln">7222 </span></a>This is like :func:`torch.minimum` except it handles NaNs differently: 
<a name="l7223"><span class="ln">7223 </span></a>if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum. 
<a name="l7224"><span class="ln">7224 </span></a>Only if both elements are NaN is NaN propagated. 
<a name="l7225"><span class="ln">7225 </span></a> 
<a name="l7226"><span class="ln">7226 </span></a>This function is a wrapper around C++'s ``std::fmin`` and is similar to NumPy's ``fmin`` function. 
<a name="l7227"><span class="ln">7227 </span></a> 
<a name="l7228"><span class="ln">7228 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l7229"><span class="ln">7229 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and floating-point inputs. 
<a name="l7230"><span class="ln">7230 </span></a> 
<a name="l7231"><span class="ln">7231 </span></a>Args: 
<a name="l7232"><span class="ln">7232 </span></a>    {input} 
<a name="l7233"><span class="ln">7233 </span></a>    other (Tensor): the second input tensor 
<a name="l7234"><span class="ln">7234 </span></a> 
<a name="l7235"><span class="ln">7235 </span></a>Keyword args: 
<a name="l7236"><span class="ln">7236 </span></a>    {out} 
<a name="l7237"><span class="ln">7237 </span></a> 
<a name="l7238"><span class="ln">7238 </span></a>Example:: 
<a name="l7239"><span class="ln">7239 </span></a> 
<a name="l7240"><span class="ln">7240 </span></a>    &gt;&gt;&gt; a = torch.tensor([2.2, float('nan'), 2.1, float('nan')]) 
<a name="l7241"><span class="ln">7241 </span></a>    &gt;&gt;&gt; b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')]) 
<a name="l7242"><span class="ln">7242 </span></a>    &gt;&gt;&gt; torch.fmin(a, b) 
<a name="l7243"><span class="ln">7243 </span></a>    tensor([-9.3000, 0.1000, 2.1000,    nan]) 
<a name="l7244"><span class="ln">7244 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7245"><span class="ln">7245 </span></a><span class="s3">)</span>
<a name="l7246"><span class="ln">7246 </span></a>
<a name="l7247"><span class="ln">7247 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7248"><span class="ln">7248 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">amin</span><span class="s3">,</span>
<a name="l7249"><span class="ln">7249 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7250"><span class="ln">7250 </span></a>amin(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l7251"><span class="ln">7251 </span></a> 
<a name="l7252"><span class="ln">7252 </span></a>Returns the minimum value of each slice of the :attr:`input` tensor in the given 
<a name="l7253"><span class="ln">7253 </span></a>dimension(s) :attr:`dim`. 
<a name="l7254"><span class="ln">7254 </span></a> 
<a name="l7255"><span class="ln">7255 </span></a>.. note:: 
<a name="l7256"><span class="ln">7256 </span></a>    The difference between ``max``/``min`` and ``amax``/``amin`` is: 
<a name="l7257"><span class="ln">7257 </span></a>        - ``amax``/``amin`` supports reducing on multiple dimensions, 
<a name="l7258"><span class="ln">7258 </span></a>        - ``amax``/``amin`` does not return indices. 
<a name="l7259"><span class="ln">7259 </span></a> 
<a name="l7260"><span class="ln">7260 </span></a>    Both ``max``/``min`` and ``amax``/``amin`` evenly distribute gradients between equal values 
<a name="l7261"><span class="ln">7261 </span></a>    when there are multiple input elements with the same minimum or maximum value. 
<a name="l7262"><span class="ln">7262 </span></a> 
<a name="l7263"><span class="ln">7263 </span></a>{keepdim_details} 
<a name="l7264"><span class="ln">7264 </span></a> 
<a name="l7265"><span class="ln">7265 </span></a>Args: 
<a name="l7266"><span class="ln">7266 </span></a>    {input} 
<a name="l7267"><span class="ln">7267 </span></a>    {opt_dim_all_reduce} 
<a name="l7268"><span class="ln">7268 </span></a>    {opt_keepdim} 
<a name="l7269"><span class="ln">7269 </span></a> 
<a name="l7270"><span class="ln">7270 </span></a>Keyword args: 
<a name="l7271"><span class="ln">7271 </span></a>  {out} 
<a name="l7272"><span class="ln">7272 </span></a> 
<a name="l7273"><span class="ln">7273 </span></a>Example:: 
<a name="l7274"><span class="ln">7274 </span></a> 
<a name="l7275"><span class="ln">7275 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l7276"><span class="ln">7276 </span></a>    &gt;&gt;&gt; a 
<a name="l7277"><span class="ln">7277 </span></a>    tensor([[ 0.6451, -0.4866,  0.2987, -1.3312], 
<a name="l7278"><span class="ln">7278 </span></a>            [-0.5744,  1.2980,  1.8397, -0.2713], 
<a name="l7279"><span class="ln">7279 </span></a>            [ 0.9128,  0.9214, -1.7268, -0.2995], 
<a name="l7280"><span class="ln">7280 </span></a>            [ 0.9023,  0.4853,  0.9075, -1.6165]]) 
<a name="l7281"><span class="ln">7281 </span></a>    &gt;&gt;&gt; torch.amin(a, 1) 
<a name="l7282"><span class="ln">7282 </span></a>    tensor([-1.3312, -0.5744, -1.7268, -1.6165]) 
<a name="l7283"><span class="ln">7283 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l7284"><span class="ln">7284 </span></a><span class="s3">)</span>
<a name="l7285"><span class="ln">7285 </span></a>
<a name="l7286"><span class="ln">7286 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7287"><span class="ln">7287 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">aminmax</span><span class="s3">,</span>
<a name="l7288"><span class="ln">7288 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7289"><span class="ln">7289 </span></a>aminmax(input, *, dim=None, keepdim=False, out=None) -&gt; (Tensor min, Tensor max) 
<a name="l7290"><span class="ln">7290 </span></a> 
<a name="l7291"><span class="ln">7291 </span></a>Computes the minimum and maximum values of the :attr:`input` tensor. 
<a name="l7292"><span class="ln">7292 </span></a> 
<a name="l7293"><span class="ln">7293 </span></a>Args: 
<a name="l7294"><span class="ln">7294 </span></a>    input (Tensor): 
<a name="l7295"><span class="ln">7295 </span></a>        The input tensor 
<a name="l7296"><span class="ln">7296 </span></a> 
<a name="l7297"><span class="ln">7297 </span></a>Keyword Args: 
<a name="l7298"><span class="ln">7298 </span></a>    dim (Optional[int]): 
<a name="l7299"><span class="ln">7299 </span></a>        The dimension along which to compute the values. If `None`, 
<a name="l7300"><span class="ln">7300 </span></a>        computes the values over the entire :attr:`input` tensor. 
<a name="l7301"><span class="ln">7301 </span></a>        Default is `None`. 
<a name="l7302"><span class="ln">7302 </span></a>    keepdim (bool): 
<a name="l7303"><span class="ln">7303 </span></a>        If `True`, the reduced dimensions will be kept in the output 
<a name="l7304"><span class="ln">7304 </span></a>        tensor as dimensions with size 1 for broadcasting, otherwise 
<a name="l7305"><span class="ln">7305 </span></a>        they will be removed, as if calling (:func:`torch.squeeze`). 
<a name="l7306"><span class="ln">7306 </span></a>        Default is `False`. 
<a name="l7307"><span class="ln">7307 </span></a>    out (Optional[Tuple[Tensor, Tensor]]): 
<a name="l7308"><span class="ln">7308 </span></a>        Optional tensors on which to write the result. Must have the same 
<a name="l7309"><span class="ln">7309 </span></a>        shape and dtype as the expected output. 
<a name="l7310"><span class="ln">7310 </span></a>        Default is `None`. 
<a name="l7311"><span class="ln">7311 </span></a> 
<a name="l7312"><span class="ln">7312 </span></a>Returns: 
<a name="l7313"><span class="ln">7313 </span></a>    A named tuple `(min, max)` containing the minimum and maximum values. 
<a name="l7314"><span class="ln">7314 </span></a> 
<a name="l7315"><span class="ln">7315 </span></a>Raises: 
<a name="l7316"><span class="ln">7316 </span></a>    RuntimeError 
<a name="l7317"><span class="ln">7317 </span></a>        If any of the dimensions to compute the values over has size 0. 
<a name="l7318"><span class="ln">7318 </span></a> 
<a name="l7319"><span class="ln">7319 </span></a>.. note:: 
<a name="l7320"><span class="ln">7320 </span></a>    NaN values are propagated to the output if at least one value is NaN. 
<a name="l7321"><span class="ln">7321 </span></a> 
<a name="l7322"><span class="ln">7322 </span></a>.. seealso:: 
<a name="l7323"><span class="ln">7323 </span></a>    :func:`torch.amin` computes just the minimum value 
<a name="l7324"><span class="ln">7324 </span></a>    :func:`torch.amax` computes just the maximum value 
<a name="l7325"><span class="ln">7325 </span></a> 
<a name="l7326"><span class="ln">7326 </span></a>Example:: 
<a name="l7327"><span class="ln">7327 </span></a> 
<a name="l7328"><span class="ln">7328 </span></a>    &gt;&gt;&gt; torch.aminmax(torch.tensor([1, -3, 5])) 
<a name="l7329"><span class="ln">7329 </span></a>    torch.return_types.aminmax( 
<a name="l7330"><span class="ln">7330 </span></a>    min=tensor(-3), 
<a name="l7331"><span class="ln">7331 </span></a>    max=tensor(5)) 
<a name="l7332"><span class="ln">7332 </span></a> 
<a name="l7333"><span class="ln">7333 </span></a>    &gt;&gt;&gt; # aminmax propagates NaNs 
<a name="l7334"><span class="ln">7334 </span></a>    &gt;&gt;&gt; torch.aminmax(torch.tensor([1, -3, 5, torch.nan])) 
<a name="l7335"><span class="ln">7335 </span></a>    torch.return_types.aminmax( 
<a name="l7336"><span class="ln">7336 </span></a>    min=tensor(nan), 
<a name="l7337"><span class="ln">7337 </span></a>    max=tensor(nan)) 
<a name="l7338"><span class="ln">7338 </span></a> 
<a name="l7339"><span class="ln">7339 </span></a>    &gt;&gt;&gt; t = torch.arange(10).view(2, 5) 
<a name="l7340"><span class="ln">7340 </span></a>    &gt;&gt;&gt; t 
<a name="l7341"><span class="ln">7341 </span></a>    tensor([[0, 1, 2, 3, 4], 
<a name="l7342"><span class="ln">7342 </span></a>            [5, 6, 7, 8, 9]]) 
<a name="l7343"><span class="ln">7343 </span></a>    &gt;&gt;&gt; t.aminmax(dim=0, keepdim=True) 
<a name="l7344"><span class="ln">7344 </span></a>    torch.return_types.aminmax( 
<a name="l7345"><span class="ln">7345 </span></a>    min=tensor([[0, 1, 2, 3, 4]]), 
<a name="l7346"><span class="ln">7346 </span></a>    max=tensor([[5, 6, 7, 8, 9]])) 
<a name="l7347"><span class="ln">7347 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l7348"><span class="ln">7348 </span></a><span class="s3">)</span>
<a name="l7349"><span class="ln">7349 </span></a>
<a name="l7350"><span class="ln">7350 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7351"><span class="ln">7351 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">argmin</span><span class="s3">,</span>
<a name="l7352"><span class="ln">7352 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7353"><span class="ln">7353 </span></a>argmin(input, dim=None, keepdim=False) -&gt; LongTensor 
<a name="l7354"><span class="ln">7354 </span></a> 
<a name="l7355"><span class="ln">7355 </span></a>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension 
<a name="l7356"><span class="ln">7356 </span></a> 
<a name="l7357"><span class="ln">7357 </span></a>This is the second value returned by :meth:`torch.min`. See its 
<a name="l7358"><span class="ln">7358 </span></a>documentation for the exact semantics of this method. 
<a name="l7359"><span class="ln">7359 </span></a> 
<a name="l7360"><span class="ln">7360 </span></a>.. note:: If there are multiple minimal values then the indices of the first minimal value are returned. 
<a name="l7361"><span class="ln">7361 </span></a> 
<a name="l7362"><span class="ln">7362 </span></a>Args: 
<a name="l7363"><span class="ln">7363 </span></a>    {input} 
<a name="l7364"><span class="ln">7364 </span></a>    {opt_dim} If ``None``, the argmin of the flattened input is returned. 
<a name="l7365"><span class="ln">7365 </span></a>    {opt_keepdim} 
<a name="l7366"><span class="ln">7366 </span></a> 
<a name="l7367"><span class="ln">7367 </span></a>Example:: 
<a name="l7368"><span class="ln">7368 </span></a> 
<a name="l7369"><span class="ln">7369 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l7370"><span class="ln">7370 </span></a>    &gt;&gt;&gt; a 
<a name="l7371"><span class="ln">7371 </span></a>    tensor([[ 0.1139,  0.2254, -0.1381,  0.3687], 
<a name="l7372"><span class="ln">7372 </span></a>            [ 1.0100, -1.1975, -0.0102, -0.4732], 
<a name="l7373"><span class="ln">7373 </span></a>            [-0.9240,  0.1207, -0.7506, -1.0213], 
<a name="l7374"><span class="ln">7374 </span></a>            [ 1.7809, -1.2960,  0.9384,  0.1438]]) 
<a name="l7375"><span class="ln">7375 </span></a>    &gt;&gt;&gt; torch.argmin(a) 
<a name="l7376"><span class="ln">7376 </span></a>    tensor(13) 
<a name="l7377"><span class="ln">7377 </span></a>    &gt;&gt;&gt; torch.argmin(a, dim=1) 
<a name="l7378"><span class="ln">7378 </span></a>    tensor([ 2,  1,  3,  1]) 
<a name="l7379"><span class="ln">7379 </span></a>    &gt;&gt;&gt; torch.argmin(a, dim=1, keepdim=True) 
<a name="l7380"><span class="ln">7380 </span></a>    tensor([[2], 
<a name="l7381"><span class="ln">7381 </span></a>            [1], 
<a name="l7382"><span class="ln">7382 </span></a>            [3], 
<a name="l7383"><span class="ln">7383 </span></a>            [1]]) 
<a name="l7384"><span class="ln">7384 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l7385"><span class="ln">7385 </span></a><span class="s3">)</span>
<a name="l7386"><span class="ln">7386 </span></a>
<a name="l7387"><span class="ln">7387 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7388"><span class="ln">7388 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">mm</span><span class="s3">,</span>
<a name="l7389"><span class="ln">7389 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7390"><span class="ln">7390 </span></a>mm(input, mat2, out_dtype=None, *, out=None) -&gt; Tensor 
<a name="l7391"><span class="ln">7391 </span></a> 
<a name="l7392"><span class="ln">7392 </span></a>Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`. 
<a name="l7393"><span class="ln">7393 </span></a> 
<a name="l7394"><span class="ln">7394 </span></a>If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l7395"><span class="ln">7395 </span></a>:math:`(m \times p)` tensor, :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l7396"><span class="ln">7396 </span></a> 
<a name="l7397"><span class="ln">7397 </span></a>.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l7398"><span class="ln">7398 </span></a>          For broadcasting matrix products, see :func:`torch.matmul`. 
<a name="l7399"><span class="ln">7399 </span></a> 
<a name="l7400"><span class="ln">7400 </span></a>Supports strided and sparse 2-D tensors as inputs, autograd with 
<a name="l7401"><span class="ln">7401 </span></a>respect to strided inputs. 
<a name="l7402"><span class="ln">7402 </span></a> 
<a name="l7403"><span class="ln">7403 </span></a>This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. 
<a name="l7404"><span class="ln">7404 </span></a>If :attr:`out` is provided its layout will be used. Otherwise, the result 
<a name="l7405"><span class="ln">7405 </span></a>layout will be deduced from that of :attr:`input`. 
<a name="l7406"><span class="ln">7406 </span></a> 
<a name="l7407"><span class="ln">7407 </span></a>{sparse_beta_warning} 
<a name="l7408"><span class="ln">7408 </span></a> 
<a name="l7409"><span class="ln">7409 </span></a>{tf32_note} 
<a name="l7410"><span class="ln">7410 </span></a> 
<a name="l7411"><span class="ln">7411 </span></a>{rocm_fp16_note} 
<a name="l7412"><span class="ln">7412 </span></a> 
<a name="l7413"><span class="ln">7413 </span></a>Args: 
<a name="l7414"><span class="ln">7414 </span></a>    input (Tensor): the first matrix to be matrix multiplied 
<a name="l7415"><span class="ln">7415 </span></a>    mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l7416"><span class="ln">7416 </span></a>    out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l7417"><span class="ln">7417 </span></a>        Supported only on CUDA and for torch.float32 given 
<a name="l7418"><span class="ln">7418 </span></a>        torch.float16/torch.bfloat16 input dtypes 
<a name="l7419"><span class="ln">7419 </span></a> 
<a name="l7420"><span class="ln">7420 </span></a>Keyword args: 
<a name="l7421"><span class="ln">7421 </span></a>    {out} 
<a name="l7422"><span class="ln">7422 </span></a> 
<a name="l7423"><span class="ln">7423 </span></a>Example:: 
<a name="l7424"><span class="ln">7424 </span></a> 
<a name="l7425"><span class="ln">7425 </span></a>    &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l7426"><span class="ln">7426 </span></a>    &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l7427"><span class="ln">7427 </span></a>    &gt;&gt;&gt; torch.mm(mat1, mat2) 
<a name="l7428"><span class="ln">7428 </span></a>    tensor([[ 0.4851,  0.5037, -0.3633], 
<a name="l7429"><span class="ln">7429 </span></a>            [-0.0760, -3.6705,  2.4784]]) 
<a name="l7430"><span class="ln">7430 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">, </span><span class="s2">**</span><span class="s1">tf32_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">rocm_fp16_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">sparse_support_notes</span><span class="s3">),</span>
<a name="l7431"><span class="ln">7431 </span></a><span class="s3">)</span>
<a name="l7432"><span class="ln">7432 </span></a>
<a name="l7433"><span class="ln">7433 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7434"><span class="ln">7434 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">hspmm</span><span class="s3">,</span>
<a name="l7435"><span class="ln">7435 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7436"><span class="ln">7436 </span></a>hspmm(mat1, mat2, *, out=None) -&gt; Tensor 
<a name="l7437"><span class="ln">7437 </span></a> 
<a name="l7438"><span class="ln">7438 </span></a>Performs a matrix multiplication of a :ref:`sparse COO matrix 
<a name="l7439"><span class="ln">7439 </span></a>&lt;sparse-coo-docs&gt;` :attr:`mat1` and a strided matrix :attr:`mat2`. The 
<a name="l7440"><span class="ln">7440 </span></a>result is a (1 + 1)-dimensional :ref:`hybrid COO matrix 
<a name="l7441"><span class="ln">7441 </span></a>&lt;sparse-hybrid-coo-docs&gt;`. 
<a name="l7442"><span class="ln">7442 </span></a> 
<a name="l7443"><span class="ln">7443 </span></a>Args: 
<a name="l7444"><span class="ln">7444 </span></a>    mat1 (Tensor): the first sparse matrix to be matrix multiplied 
<a name="l7445"><span class="ln">7445 </span></a>    mat2 (Tensor): the second strided matrix to be matrix multiplied 
<a name="l7446"><span class="ln">7446 </span></a> 
<a name="l7447"><span class="ln">7447 </span></a>Keyword args: 
<a name="l7448"><span class="ln">7448 </span></a>    {out} 
<a name="l7449"><span class="ln">7449 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7450"><span class="ln">7450 </span></a><span class="s3">)</span>
<a name="l7451"><span class="ln">7451 </span></a>
<a name="l7452"><span class="ln">7452 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7453"><span class="ln">7453 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">matmul</span><span class="s3">,</span>
<a name="l7454"><span class="ln">7454 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7455"><span class="ln">7455 </span></a>matmul(input, other, *, out=None) -&gt; Tensor 
<a name="l7456"><span class="ln">7456 </span></a> 
<a name="l7457"><span class="ln">7457 </span></a>Matrix product of two tensors. 
<a name="l7458"><span class="ln">7458 </span></a> 
<a name="l7459"><span class="ln">7459 </span></a>The behavior depends on the dimensionality of the tensors as follows: 
<a name="l7460"><span class="ln">7460 </span></a> 
<a name="l7461"><span class="ln">7461 </span></a>- If both tensors are 1-dimensional, the dot product (scalar) is returned. 
<a name="l7462"><span class="ln">7462 </span></a>- If both arguments are 2-dimensional, the matrix-matrix product is returned. 
<a name="l7463"><span class="ln">7463 </span></a>- If the first argument is 1-dimensional and the second argument is 2-dimensional, 
<a name="l7464"><span class="ln">7464 </span></a>  a 1 is prepended to its dimension for the purpose of the matrix multiply. 
<a name="l7465"><span class="ln">7465 </span></a>  After the matrix multiply, the prepended dimension is removed. 
<a name="l7466"><span class="ln">7466 </span></a>- If the first argument is 2-dimensional and the second argument is 1-dimensional, 
<a name="l7467"><span class="ln">7467 </span></a>  the matrix-vector product is returned. 
<a name="l7468"><span class="ln">7468 </span></a>- If both arguments are at least 1-dimensional and at least one argument is 
<a name="l7469"><span class="ln">7469 </span></a>  N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first 
<a name="l7470"><span class="ln">7470 </span></a>  argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the 
<a name="l7471"><span class="ln">7471 </span></a>  batched matrix multiply and removed after.  If the second argument is 1-dimensional, a 
<a name="l7472"><span class="ln">7472 </span></a>  1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. 
<a name="l7473"><span class="ln">7473 </span></a>  The non-matrix (i.e. batch) dimensions are :ref:`broadcasted &lt;broadcasting-semantics&gt;` (and thus 
<a name="l7474"><span class="ln">7474 </span></a>  must be broadcastable).  For example, if :attr:`input` is a 
<a name="l7475"><span class="ln">7475 </span></a>  :math:`(j \times 1 \times n \times n)` tensor and :attr:`other` is a :math:`(k \times n \times n)` 
<a name="l7476"><span class="ln">7476 </span></a>  tensor, :attr:`out` will be a :math:`(j \times k \times n \times n)` tensor. 
<a name="l7477"><span class="ln">7477 </span></a> 
<a name="l7478"><span class="ln">7478 </span></a>  Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs 
<a name="l7479"><span class="ln">7479 </span></a>  are broadcastable, and not the matrix dimensions. For example, if :attr:`input` is a 
<a name="l7480"><span class="ln">7480 </span></a>  :math:`(j \times 1 \times n \times m)` tensor and :attr:`other` is a :math:`(k \times m \times p)` 
<a name="l7481"><span class="ln">7481 </span></a>  tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the 
<a name="l7482"><span class="ln">7482 </span></a>  matrix dimensions) are different. :attr:`out` will be a :math:`(j \times k \times n \times p)` tensor. 
<a name="l7483"><span class="ln">7483 </span></a> 
<a name="l7484"><span class="ln">7484 </span></a>This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. In particular the 
<a name="l7485"><span class="ln">7485 </span></a>matrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions 
<a name="l7486"><span class="ln">7486 </span></a>as :func:`torch.mm` 
<a name="l7487"><span class="ln">7487 </span></a> 
<a name="l7488"><span class="ln">7488 </span></a>{sparse_beta_warning} 
<a name="l7489"><span class="ln">7489 </span></a> 
<a name="l7490"><span class="ln">7490 </span></a>{tf32_note} 
<a name="l7491"><span class="ln">7491 </span></a> 
<a name="l7492"><span class="ln">7492 </span></a>{rocm_fp16_note} 
<a name="l7493"><span class="ln">7493 </span></a> 
<a name="l7494"><span class="ln">7494 </span></a>.. note:: 
<a name="l7495"><span class="ln">7495 </span></a> 
<a name="l7496"><span class="ln">7496 </span></a>    The 1-dimensional dot product version of this function does not support an :attr:`out` parameter. 
<a name="l7497"><span class="ln">7497 </span></a> 
<a name="l7498"><span class="ln">7498 </span></a>Arguments: 
<a name="l7499"><span class="ln">7499 </span></a>    input (Tensor): the first tensor to be multiplied 
<a name="l7500"><span class="ln">7500 </span></a>    other (Tensor): the second tensor to be multiplied 
<a name="l7501"><span class="ln">7501 </span></a> 
<a name="l7502"><span class="ln">7502 </span></a>Keyword args: 
<a name="l7503"><span class="ln">7503 </span></a>    {out} 
<a name="l7504"><span class="ln">7504 </span></a> 
<a name="l7505"><span class="ln">7505 </span></a>Example:: 
<a name="l7506"><span class="ln">7506 </span></a> 
<a name="l7507"><span class="ln">7507 </span></a>    &gt;&gt;&gt; # vector x vector 
<a name="l7508"><span class="ln">7508 </span></a>    &gt;&gt;&gt; tensor1 = torch.randn(3) 
<a name="l7509"><span class="ln">7509 </span></a>    &gt;&gt;&gt; tensor2 = torch.randn(3) 
<a name="l7510"><span class="ln">7510 </span></a>    &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l7511"><span class="ln">7511 </span></a>    torch.Size([]) 
<a name="l7512"><span class="ln">7512 </span></a>    &gt;&gt;&gt; # matrix x vector 
<a name="l7513"><span class="ln">7513 </span></a>    &gt;&gt;&gt; tensor1 = torch.randn(3, 4) 
<a name="l7514"><span class="ln">7514 </span></a>    &gt;&gt;&gt; tensor2 = torch.randn(4) 
<a name="l7515"><span class="ln">7515 </span></a>    &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l7516"><span class="ln">7516 </span></a>    torch.Size([3]) 
<a name="l7517"><span class="ln">7517 </span></a>    &gt;&gt;&gt; # batched matrix x broadcasted vector 
<a name="l7518"><span class="ln">7518 </span></a>    &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) 
<a name="l7519"><span class="ln">7519 </span></a>    &gt;&gt;&gt; tensor2 = torch.randn(4) 
<a name="l7520"><span class="ln">7520 </span></a>    &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l7521"><span class="ln">7521 </span></a>    torch.Size([10, 3]) 
<a name="l7522"><span class="ln">7522 </span></a>    &gt;&gt;&gt; # batched matrix x batched matrix 
<a name="l7523"><span class="ln">7523 </span></a>    &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) 
<a name="l7524"><span class="ln">7524 </span></a>    &gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5) 
<a name="l7525"><span class="ln">7525 </span></a>    &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l7526"><span class="ln">7526 </span></a>    torch.Size([10, 3, 5]) 
<a name="l7527"><span class="ln">7527 </span></a>    &gt;&gt;&gt; # batched matrix x broadcasted matrix 
<a name="l7528"><span class="ln">7528 </span></a>    &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) 
<a name="l7529"><span class="ln">7529 </span></a>    &gt;&gt;&gt; tensor2 = torch.randn(4, 5) 
<a name="l7530"><span class="ln">7530 </span></a>    &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l7531"><span class="ln">7531 </span></a>    torch.Size([10, 3, 5]) 
<a name="l7532"><span class="ln">7532 </span></a> 
<a name="l7533"><span class="ln">7533 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">, </span><span class="s2">**</span><span class="s1">tf32_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">rocm_fp16_notes</span><span class="s3">, </span><span class="s2">**</span><span class="s1">sparse_support_notes</span><span class="s3">),</span>
<a name="l7534"><span class="ln">7534 </span></a><span class="s3">)</span>
<a name="l7535"><span class="ln">7535 </span></a>
<a name="l7536"><span class="ln">7536 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7537"><span class="ln">7537 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">mode</span><span class="s3">,</span>
<a name="l7538"><span class="ln">7538 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7539"><span class="ln">7539 </span></a>mode(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l7540"><span class="ln">7540 </span></a> 
<a name="l7541"><span class="ln">7541 </span></a>Returns a namedtuple ``(values, indices)`` where ``values`` is the mode 
<a name="l7542"><span class="ln">7542 </span></a>value of each row of the :attr:`input` tensor in the given dimension 
<a name="l7543"><span class="ln">7543 </span></a>:attr:`dim`, i.e. a value which appears most often 
<a name="l7544"><span class="ln">7544 </span></a>in that row, and ``indices`` is the index location of each mode value found. 
<a name="l7545"><span class="ln">7545 </span></a> 
<a name="l7546"><span class="ln">7546 </span></a>By default, :attr:`dim` is the last dimension of the :attr:`input` tensor. 
<a name="l7547"><span class="ln">7547 </span></a> 
<a name="l7548"><span class="ln">7548 </span></a>If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l7549"><span class="ln">7549 </span></a>:attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l7550"><span class="ln">7550 </span></a>Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l7551"><span class="ln">7551 </span></a>in the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l7552"><span class="ln">7552 </span></a> 
<a name="l7553"><span class="ln">7553 </span></a>.. note:: This function is not defined for ``torch.cuda.Tensor`` yet. 
<a name="l7554"><span class="ln">7554 </span></a> 
<a name="l7555"><span class="ln">7555 </span></a>Args: 
<a name="l7556"><span class="ln">7556 </span></a>    {input} 
<a name="l7557"><span class="ln">7557 </span></a>    {opt_dim} 
<a name="l7558"><span class="ln">7558 </span></a>    {opt_keepdim} 
<a name="l7559"><span class="ln">7559 </span></a> 
<a name="l7560"><span class="ln">7560 </span></a>Keyword args: 
<a name="l7561"><span class="ln">7561 </span></a>    out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l7562"><span class="ln">7562 </span></a> 
<a name="l7563"><span class="ln">7563 </span></a>Example:: 
<a name="l7564"><span class="ln">7564 </span></a> 
<a name="l7565"><span class="ln">7565 </span></a>    &gt;&gt;&gt; b = torch.tensor([[0, 0, 0, 2, 0, 0, 2], 
<a name="l7566"><span class="ln">7566 </span></a>    ...                   [0, 3, 0, 0, 2, 0, 1], 
<a name="l7567"><span class="ln">7567 </span></a>    ...                   [2, 2, 2, 0, 0, 0, 3], 
<a name="l7568"><span class="ln">7568 </span></a>    ...                   [2, 2, 3, 0, 1, 1, 0], 
<a name="l7569"><span class="ln">7569 </span></a>    ...                   [1, 1, 0, 0, 2, 0, 2]]) 
<a name="l7570"><span class="ln">7570 </span></a>    &gt;&gt;&gt; torch.mode(b, 0) 
<a name="l7571"><span class="ln">7571 </span></a>    torch.return_types.mode( 
<a name="l7572"><span class="ln">7572 </span></a>    values=tensor([0, 2, 0, 0, 0, 0, 2]), 
<a name="l7573"><span class="ln">7573 </span></a>    indices=tensor([1, 3, 4, 4, 2, 4, 4])) 
<a name="l7574"><span class="ln">7574 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l7575"><span class="ln">7575 </span></a><span class="s3">)</span>
<a name="l7576"><span class="ln">7576 </span></a>
<a name="l7577"><span class="ln">7577 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7578"><span class="ln">7578 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">mul</span><span class="s3">,</span>
<a name="l7579"><span class="ln">7579 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7580"><span class="ln">7580 </span></a>mul(input, other, *, out=None) -&gt; Tensor 
<a name="l7581"><span class="ln">7581 </span></a> 
<a name="l7582"><span class="ln">7582 </span></a>Multiplies :attr:`input` by :attr:`other`. 
<a name="l7583"><span class="ln">7583 </span></a> 
<a name="l7584"><span class="ln">7584 </span></a> 
<a name="l7585"><span class="ln">7585 </span></a>.. math:: 
<a name="l7586"><span class="ln">7586 </span></a>    \text{out}_i = \text{input}_i \times \text{other}_i 
<a name="l7587"><span class="ln">7587 </span></a>&quot;&quot;&quot;</span>
<a name="l7588"><span class="ln">7588 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l7589"><span class="ln">7589 </span></a> 
<a name="l7590"><span class="ln">7590 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l7591"><span class="ln">7591 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l7592"><span class="ln">7592 </span></a> 
<a name="l7593"><span class="ln">7593 </span></a>Args: 
<a name="l7594"><span class="ln">7594 </span></a>    {input} 
<a name="l7595"><span class="ln">7595 </span></a>    other (Tensor or Number) - the tensor or number to multiply input by. 
<a name="l7596"><span class="ln">7596 </span></a> 
<a name="l7597"><span class="ln">7597 </span></a>Keyword args: 
<a name="l7598"><span class="ln">7598 </span></a>    {out} 
<a name="l7599"><span class="ln">7599 </span></a> 
<a name="l7600"><span class="ln">7600 </span></a>Examples:: 
<a name="l7601"><span class="ln">7601 </span></a> 
<a name="l7602"><span class="ln">7602 </span></a>    &gt;&gt;&gt; a = torch.randn(3) 
<a name="l7603"><span class="ln">7603 </span></a>    &gt;&gt;&gt; a 
<a name="l7604"><span class="ln">7604 </span></a>    tensor([ 0.2015, -0.4255,  2.6087]) 
<a name="l7605"><span class="ln">7605 </span></a>    &gt;&gt;&gt; torch.mul(a, 100) 
<a name="l7606"><span class="ln">7606 </span></a>    tensor([  20.1494,  -42.5491,  260.8663]) 
<a name="l7607"><span class="ln">7607 </span></a> 
<a name="l7608"><span class="ln">7608 </span></a>    &gt;&gt;&gt; b = torch.randn(4, 1) 
<a name="l7609"><span class="ln">7609 </span></a>    &gt;&gt;&gt; b 
<a name="l7610"><span class="ln">7610 </span></a>    tensor([[ 1.1207], 
<a name="l7611"><span class="ln">7611 </span></a>            [-0.3137], 
<a name="l7612"><span class="ln">7612 </span></a>            [ 0.0700], 
<a name="l7613"><span class="ln">7613 </span></a>            [ 0.8378]]) 
<a name="l7614"><span class="ln">7614 </span></a>    &gt;&gt;&gt; c = torch.randn(1, 4) 
<a name="l7615"><span class="ln">7615 </span></a>    &gt;&gt;&gt; c 
<a name="l7616"><span class="ln">7616 </span></a>    tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]]) 
<a name="l7617"><span class="ln">7617 </span></a>    &gt;&gt;&gt; torch.mul(b, c) 
<a name="l7618"><span class="ln">7618 </span></a>    tensor([[ 0.5767,  0.1363, -0.5877,  2.5083], 
<a name="l7619"><span class="ln">7619 </span></a>            [-0.1614, -0.0382,  0.1645, -0.7021], 
<a name="l7620"><span class="ln">7620 </span></a>            [ 0.0360,  0.0085, -0.0367,  0.1567], 
<a name="l7621"><span class="ln">7621 </span></a>            [ 0.4312,  0.1019, -0.4394,  1.8753]]) 
<a name="l7622"><span class="ln">7622 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7623"><span class="ln">7623 </span></a><span class="s3">)</span>
<a name="l7624"><span class="ln">7624 </span></a>
<a name="l7625"><span class="ln">7625 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7626"><span class="ln">7626 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">multiply</span><span class="s3">,</span>
<a name="l7627"><span class="ln">7627 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7628"><span class="ln">7628 </span></a>multiply(input, other, *, out=None) 
<a name="l7629"><span class="ln">7629 </span></a> 
<a name="l7630"><span class="ln">7630 </span></a>Alias for :func:`torch.mul`. 
<a name="l7631"><span class="ln">7631 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l7632"><span class="ln">7632 </span></a><span class="s3">)</span>
<a name="l7633"><span class="ln">7633 </span></a>
<a name="l7634"><span class="ln">7634 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7635"><span class="ln">7635 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">multinomial</span><span class="s3">,</span>
<a name="l7636"><span class="ln">7636 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7637"><span class="ln">7637 </span></a>multinomial(input, num_samples, replacement=False, *, generator=None, out=None) -&gt; LongTensor 
<a name="l7638"><span class="ln">7638 </span></a> 
<a name="l7639"><span class="ln">7639 </span></a>Returns a tensor where each row contains :attr:`num_samples` indices sampled 
<a name="l7640"><span class="ln">7640 </span></a>from the multinomial (a stricter definition would be multivariate, 
<a name="l7641"><span class="ln">7641 </span></a>refer to :class:`torch.distributions.multinomial.Multinomial` for more details) 
<a name="l7642"><span class="ln">7642 </span></a>probability distribution located in the corresponding row 
<a name="l7643"><span class="ln">7643 </span></a>of tensor :attr:`input`. 
<a name="l7644"><span class="ln">7644 </span></a> 
<a name="l7645"><span class="ln">7645 </span></a>.. note:: 
<a name="l7646"><span class="ln">7646 </span></a>    The rows of :attr:`input` do not need to sum to one (in which case we use 
<a name="l7647"><span class="ln">7647 </span></a>    the values as weights), but must be non-negative, finite and have 
<a name="l7648"><span class="ln">7648 </span></a>    a non-zero sum. 
<a name="l7649"><span class="ln">7649 </span></a> 
<a name="l7650"><span class="ln">7650 </span></a>Indices are ordered from left to right according to when each was sampled 
<a name="l7651"><span class="ln">7651 </span></a>(first samples are placed in first column). 
<a name="l7652"><span class="ln">7652 </span></a> 
<a name="l7653"><span class="ln">7653 </span></a>If :attr:`input` is a vector, :attr:`out` is a vector of size :attr:`num_samples`. 
<a name="l7654"><span class="ln">7654 </span></a> 
<a name="l7655"><span class="ln">7655 </span></a>If :attr:`input` is a matrix with `m` rows, :attr:`out` is an matrix of shape 
<a name="l7656"><span class="ln">7656 </span></a>:math:`(m \times \text{{num\_samples}})`. 
<a name="l7657"><span class="ln">7657 </span></a> 
<a name="l7658"><span class="ln">7658 </span></a>If replacement is ``True``, samples are drawn with replacement. 
<a name="l7659"><span class="ln">7659 </span></a> 
<a name="l7660"><span class="ln">7660 </span></a>If not, they are drawn without replacement, which means that when a 
<a name="l7661"><span class="ln">7661 </span></a>sample index is drawn for a row, it cannot be drawn again for that row. 
<a name="l7662"><span class="ln">7662 </span></a> 
<a name="l7663"><span class="ln">7663 </span></a>.. note:: 
<a name="l7664"><span class="ln">7664 </span></a>    When drawn without replacement, :attr:`num_samples` must be lower than 
<a name="l7665"><span class="ln">7665 </span></a>    number of non-zero elements in :attr:`input` (or the min number of non-zero 
<a name="l7666"><span class="ln">7666 </span></a>    elements in each row of :attr:`input` if it is a matrix). 
<a name="l7667"><span class="ln">7667 </span></a> 
<a name="l7668"><span class="ln">7668 </span></a>Args: 
<a name="l7669"><span class="ln">7669 </span></a>    input (Tensor): the input tensor containing probabilities 
<a name="l7670"><span class="ln">7670 </span></a>    num_samples (int): number of samples to draw 
<a name="l7671"><span class="ln">7671 </span></a>    replacement (bool, optional): whether to draw with replacement or not 
<a name="l7672"><span class="ln">7672 </span></a> 
<a name="l7673"><span class="ln">7673 </span></a>Keyword args: 
<a name="l7674"><span class="ln">7674 </span></a>    {generator} 
<a name="l7675"><span class="ln">7675 </span></a>    {out} 
<a name="l7676"><span class="ln">7676 </span></a> 
<a name="l7677"><span class="ln">7677 </span></a>Example:: 
<a name="l7678"><span class="ln">7678 </span></a> 
<a name="l7679"><span class="ln">7679 </span></a>    &gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights 
<a name="l7680"><span class="ln">7680 </span></a>    &gt;&gt;&gt; torch.multinomial(weights, 2) 
<a name="l7681"><span class="ln">7681 </span></a>    tensor([1, 2]) 
<a name="l7682"><span class="ln">7682 </span></a>    &gt;&gt;&gt; torch.multinomial(weights, 5) # ERROR! 
<a name="l7683"><span class="ln">7683 </span></a>    RuntimeError: cannot sample n_sample &gt; prob_dist.size(-1) samples without replacement 
<a name="l7684"><span class="ln">7684 </span></a>    &gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True) 
<a name="l7685"><span class="ln">7685 </span></a>    tensor([ 2,  1,  1,  1]) 
<a name="l7686"><span class="ln">7686 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7687"><span class="ln">7687 </span></a><span class="s3">)</span>
<a name="l7688"><span class="ln">7688 </span></a>
<a name="l7689"><span class="ln">7689 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7690"><span class="ln">7690 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">mv</span><span class="s3">,</span>
<a name="l7691"><span class="ln">7691 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7692"><span class="ln">7692 </span></a>mv(input, vec, *, out=None) -&gt; Tensor 
<a name="l7693"><span class="ln">7693 </span></a> 
<a name="l7694"><span class="ln">7694 </span></a>Performs a matrix-vector product of the matrix :attr:`input` and the vector 
<a name="l7695"><span class="ln">7695 </span></a>:attr:`vec`. 
<a name="l7696"><span class="ln">7696 </span></a> 
<a name="l7697"><span class="ln">7697 </span></a>If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l7698"><span class="ln">7698 </span></a>size :math:`m`, :attr:`out` will be 1-D of size :math:`n`. 
<a name="l7699"><span class="ln">7699 </span></a> 
<a name="l7700"><span class="ln">7700 </span></a>.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l7701"><span class="ln">7701 </span></a> 
<a name="l7702"><span class="ln">7702 </span></a>Args: 
<a name="l7703"><span class="ln">7703 </span></a>    input (Tensor): matrix to be multiplied 
<a name="l7704"><span class="ln">7704 </span></a>    vec (Tensor): vector to be multiplied 
<a name="l7705"><span class="ln">7705 </span></a> 
<a name="l7706"><span class="ln">7706 </span></a>Keyword args: 
<a name="l7707"><span class="ln">7707 </span></a>    {out} 
<a name="l7708"><span class="ln">7708 </span></a> 
<a name="l7709"><span class="ln">7709 </span></a>Example:: 
<a name="l7710"><span class="ln">7710 </span></a> 
<a name="l7711"><span class="ln">7711 </span></a>    &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l7712"><span class="ln">7712 </span></a>    &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l7713"><span class="ln">7713 </span></a>    &gt;&gt;&gt; torch.mv(mat, vec) 
<a name="l7714"><span class="ln">7714 </span></a>    tensor([ 1.0404, -0.6361]) 
<a name="l7715"><span class="ln">7715 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7716"><span class="ln">7716 </span></a><span class="s3">)</span>
<a name="l7717"><span class="ln">7717 </span></a>
<a name="l7718"><span class="ln">7718 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7719"><span class="ln">7719 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">mvlgamma</span><span class="s3">,</span>
<a name="l7720"><span class="ln">7720 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7721"><span class="ln">7721 </span></a>mvlgamma(input, p, *, out=None) -&gt; Tensor 
<a name="l7722"><span class="ln">7722 </span></a> 
<a name="l7723"><span class="ln">7723 </span></a>Alias for :func:`torch.special.multigammaln`. 
<a name="l7724"><span class="ln">7724 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l7725"><span class="ln">7725 </span></a><span class="s3">)</span>
<a name="l7726"><span class="ln">7726 </span></a>
<a name="l7727"><span class="ln">7727 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7728"><span class="ln">7728 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">movedim</span><span class="s3">,</span>
<a name="l7729"><span class="ln">7729 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7730"><span class="ln">7730 </span></a>movedim(input, source, destination) -&gt; Tensor 
<a name="l7731"><span class="ln">7731 </span></a> 
<a name="l7732"><span class="ln">7732 </span></a>Moves the dimension(s) of :attr:`input` at the position(s) in :attr:`source` 
<a name="l7733"><span class="ln">7733 </span></a>to the position(s) in :attr:`destination`. 
<a name="l7734"><span class="ln">7734 </span></a> 
<a name="l7735"><span class="ln">7735 </span></a>Other dimensions of :attr:`input` that are not explicitly moved remain in 
<a name="l7736"><span class="ln">7736 </span></a>their original order and appear at the positions not specified in :attr:`destination`. 
<a name="l7737"><span class="ln">7737 </span></a> 
<a name="l7738"><span class="ln">7738 </span></a>Args: 
<a name="l7739"><span class="ln">7739 </span></a>    {input} 
<a name="l7740"><span class="ln">7740 </span></a>    source (int or tuple of ints): Original positions of the dims to move. These must be unique. 
<a name="l7741"><span class="ln">7741 </span></a>    destination (int or tuple of ints): Destination positions for each of the original dims. These must also be unique. 
<a name="l7742"><span class="ln">7742 </span></a> 
<a name="l7743"><span class="ln">7743 </span></a>Examples:: 
<a name="l7744"><span class="ln">7744 </span></a> 
<a name="l7745"><span class="ln">7745 </span></a>    &gt;&gt;&gt; t = torch.randn(3,2,1) 
<a name="l7746"><span class="ln">7746 </span></a>    &gt;&gt;&gt; t 
<a name="l7747"><span class="ln">7747 </span></a>    tensor([[[-0.3362], 
<a name="l7748"><span class="ln">7748 </span></a>            [-0.8437]], 
<a name="l7749"><span class="ln">7749 </span></a> 
<a name="l7750"><span class="ln">7750 </span></a>            [[-0.9627], 
<a name="l7751"><span class="ln">7751 </span></a>            [ 0.1727]], 
<a name="l7752"><span class="ln">7752 </span></a> 
<a name="l7753"><span class="ln">7753 </span></a>            [[ 0.5173], 
<a name="l7754"><span class="ln">7754 </span></a>            [-0.1398]]]) 
<a name="l7755"><span class="ln">7755 </span></a>    &gt;&gt;&gt; torch.movedim(t, 1, 0).shape 
<a name="l7756"><span class="ln">7756 </span></a>    torch.Size([2, 3, 1]) 
<a name="l7757"><span class="ln">7757 </span></a>    &gt;&gt;&gt; torch.movedim(t, 1, 0) 
<a name="l7758"><span class="ln">7758 </span></a>    tensor([[[-0.3362], 
<a name="l7759"><span class="ln">7759 </span></a>            [-0.9627], 
<a name="l7760"><span class="ln">7760 </span></a>            [ 0.5173]], 
<a name="l7761"><span class="ln">7761 </span></a> 
<a name="l7762"><span class="ln">7762 </span></a>            [[-0.8437], 
<a name="l7763"><span class="ln">7763 </span></a>            [ 0.1727], 
<a name="l7764"><span class="ln">7764 </span></a>            [-0.1398]]]) 
<a name="l7765"><span class="ln">7765 </span></a>    &gt;&gt;&gt; torch.movedim(t, (1, 2), (0, 1)).shape 
<a name="l7766"><span class="ln">7766 </span></a>    torch.Size([2, 1, 3]) 
<a name="l7767"><span class="ln">7767 </span></a>    &gt;&gt;&gt; torch.movedim(t, (1, 2), (0, 1)) 
<a name="l7768"><span class="ln">7768 </span></a>    tensor([[[-0.3362, -0.9627,  0.5173]], 
<a name="l7769"><span class="ln">7769 </span></a> 
<a name="l7770"><span class="ln">7770 </span></a>            [[-0.8437,  0.1727, -0.1398]]]) 
<a name="l7771"><span class="ln">7771 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7772"><span class="ln">7772 </span></a><span class="s3">)</span>
<a name="l7773"><span class="ln">7773 </span></a>
<a name="l7774"><span class="ln">7774 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7775"><span class="ln">7775 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">moveaxis</span><span class="s3">,</span>
<a name="l7776"><span class="ln">7776 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7777"><span class="ln">7777 </span></a>moveaxis(input, source, destination) -&gt; Tensor 
<a name="l7778"><span class="ln">7778 </span></a> 
<a name="l7779"><span class="ln">7779 </span></a>Alias for :func:`torch.movedim`. 
<a name="l7780"><span class="ln">7780 </span></a> 
<a name="l7781"><span class="ln">7781 </span></a>This function is equivalent to NumPy's moveaxis function. 
<a name="l7782"><span class="ln">7782 </span></a> 
<a name="l7783"><span class="ln">7783 </span></a>Examples:: 
<a name="l7784"><span class="ln">7784 </span></a> 
<a name="l7785"><span class="ln">7785 </span></a>    &gt;&gt;&gt; t = torch.randn(3,2,1) 
<a name="l7786"><span class="ln">7786 </span></a>    &gt;&gt;&gt; t 
<a name="l7787"><span class="ln">7787 </span></a>    tensor([[[-0.3362], 
<a name="l7788"><span class="ln">7788 </span></a>            [-0.8437]], 
<a name="l7789"><span class="ln">7789 </span></a> 
<a name="l7790"><span class="ln">7790 </span></a>            [[-0.9627], 
<a name="l7791"><span class="ln">7791 </span></a>            [ 0.1727]], 
<a name="l7792"><span class="ln">7792 </span></a> 
<a name="l7793"><span class="ln">7793 </span></a>            [[ 0.5173], 
<a name="l7794"><span class="ln">7794 </span></a>            [-0.1398]]]) 
<a name="l7795"><span class="ln">7795 </span></a>    &gt;&gt;&gt; torch.moveaxis(t, 1, 0).shape 
<a name="l7796"><span class="ln">7796 </span></a>    torch.Size([2, 3, 1]) 
<a name="l7797"><span class="ln">7797 </span></a>    &gt;&gt;&gt; torch.moveaxis(t, 1, 0) 
<a name="l7798"><span class="ln">7798 </span></a>    tensor([[[-0.3362], 
<a name="l7799"><span class="ln">7799 </span></a>            [-0.9627], 
<a name="l7800"><span class="ln">7800 </span></a>            [ 0.5173]], 
<a name="l7801"><span class="ln">7801 </span></a> 
<a name="l7802"><span class="ln">7802 </span></a>            [[-0.8437], 
<a name="l7803"><span class="ln">7803 </span></a>            [ 0.1727], 
<a name="l7804"><span class="ln">7804 </span></a>            [-0.1398]]]) 
<a name="l7805"><span class="ln">7805 </span></a>    &gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1)).shape 
<a name="l7806"><span class="ln">7806 </span></a>    torch.Size([2, 1, 3]) 
<a name="l7807"><span class="ln">7807 </span></a>    &gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1)) 
<a name="l7808"><span class="ln">7808 </span></a>    tensor([[[-0.3362, -0.9627,  0.5173]], 
<a name="l7809"><span class="ln">7809 </span></a> 
<a name="l7810"><span class="ln">7810 </span></a>            [[-0.8437,  0.1727, -0.1398]]]) 
<a name="l7811"><span class="ln">7811 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7812"><span class="ln">7812 </span></a><span class="s3">)</span>
<a name="l7813"><span class="ln">7813 </span></a>
<a name="l7814"><span class="ln">7814 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7815"><span class="ln">7815 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">swapdims</span><span class="s3">,</span>
<a name="l7816"><span class="ln">7816 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7817"><span class="ln">7817 </span></a>swapdims(input, dim0, dim1) -&gt; Tensor 
<a name="l7818"><span class="ln">7818 </span></a> 
<a name="l7819"><span class="ln">7819 </span></a>Alias for :func:`torch.transpose`. 
<a name="l7820"><span class="ln">7820 </span></a> 
<a name="l7821"><span class="ln">7821 </span></a>This function is equivalent to NumPy's swapaxes function. 
<a name="l7822"><span class="ln">7822 </span></a> 
<a name="l7823"><span class="ln">7823 </span></a>Examples:: 
<a name="l7824"><span class="ln">7824 </span></a> 
<a name="l7825"><span class="ln">7825 </span></a>    &gt;&gt;&gt; x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]]) 
<a name="l7826"><span class="ln">7826 </span></a>    &gt;&gt;&gt; x 
<a name="l7827"><span class="ln">7827 </span></a>    tensor([[[0, 1], 
<a name="l7828"><span class="ln">7828 </span></a>            [2, 3]], 
<a name="l7829"><span class="ln">7829 </span></a> 
<a name="l7830"><span class="ln">7830 </span></a>            [[4, 5], 
<a name="l7831"><span class="ln">7831 </span></a>            [6, 7]]]) 
<a name="l7832"><span class="ln">7832 </span></a>    &gt;&gt;&gt; torch.swapdims(x, 0, 1) 
<a name="l7833"><span class="ln">7833 </span></a>    tensor([[[0, 1], 
<a name="l7834"><span class="ln">7834 </span></a>            [4, 5]], 
<a name="l7835"><span class="ln">7835 </span></a> 
<a name="l7836"><span class="ln">7836 </span></a>            [[2, 3], 
<a name="l7837"><span class="ln">7837 </span></a>            [6, 7]]]) 
<a name="l7838"><span class="ln">7838 </span></a>    &gt;&gt;&gt; torch.swapdims(x, 0, 2) 
<a name="l7839"><span class="ln">7839 </span></a>    tensor([[[0, 4], 
<a name="l7840"><span class="ln">7840 </span></a>            [2, 6]], 
<a name="l7841"><span class="ln">7841 </span></a> 
<a name="l7842"><span class="ln">7842 </span></a>            [[1, 5], 
<a name="l7843"><span class="ln">7843 </span></a>            [3, 7]]]) 
<a name="l7844"><span class="ln">7844 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7845"><span class="ln">7845 </span></a><span class="s3">)</span>
<a name="l7846"><span class="ln">7846 </span></a>
<a name="l7847"><span class="ln">7847 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7848"><span class="ln">7848 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">swapaxes</span><span class="s3">,</span>
<a name="l7849"><span class="ln">7849 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7850"><span class="ln">7850 </span></a>swapaxes(input, axis0, axis1) -&gt; Tensor 
<a name="l7851"><span class="ln">7851 </span></a> 
<a name="l7852"><span class="ln">7852 </span></a>Alias for :func:`torch.transpose`. 
<a name="l7853"><span class="ln">7853 </span></a> 
<a name="l7854"><span class="ln">7854 </span></a>This function is equivalent to NumPy's swapaxes function. 
<a name="l7855"><span class="ln">7855 </span></a> 
<a name="l7856"><span class="ln">7856 </span></a>Examples:: 
<a name="l7857"><span class="ln">7857 </span></a> 
<a name="l7858"><span class="ln">7858 </span></a>    &gt;&gt;&gt; x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]]) 
<a name="l7859"><span class="ln">7859 </span></a>    &gt;&gt;&gt; x 
<a name="l7860"><span class="ln">7860 </span></a>    tensor([[[0, 1], 
<a name="l7861"><span class="ln">7861 </span></a>            [2, 3]], 
<a name="l7862"><span class="ln">7862 </span></a> 
<a name="l7863"><span class="ln">7863 </span></a>            [[4, 5], 
<a name="l7864"><span class="ln">7864 </span></a>            [6, 7]]]) 
<a name="l7865"><span class="ln">7865 </span></a>    &gt;&gt;&gt; torch.swapaxes(x, 0, 1) 
<a name="l7866"><span class="ln">7866 </span></a>    tensor([[[0, 1], 
<a name="l7867"><span class="ln">7867 </span></a>            [4, 5]], 
<a name="l7868"><span class="ln">7868 </span></a> 
<a name="l7869"><span class="ln">7869 </span></a>            [[2, 3], 
<a name="l7870"><span class="ln">7870 </span></a>            [6, 7]]]) 
<a name="l7871"><span class="ln">7871 </span></a>    &gt;&gt;&gt; torch.swapaxes(x, 0, 2) 
<a name="l7872"><span class="ln">7872 </span></a>    tensor([[[0, 4], 
<a name="l7873"><span class="ln">7873 </span></a>            [2, 6]], 
<a name="l7874"><span class="ln">7874 </span></a> 
<a name="l7875"><span class="ln">7875 </span></a>            [[1, 5], 
<a name="l7876"><span class="ln">7876 </span></a>            [3, 7]]]) 
<a name="l7877"><span class="ln">7877 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7878"><span class="ln">7878 </span></a><span class="s3">)</span>
<a name="l7879"><span class="ln">7879 </span></a>
<a name="l7880"><span class="ln">7880 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7881"><span class="ln">7881 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">narrow</span><span class="s3">,</span>
<a name="l7882"><span class="ln">7882 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7883"><span class="ln">7883 </span></a>narrow(input, dim, start, length) -&gt; Tensor 
<a name="l7884"><span class="ln">7884 </span></a> 
<a name="l7885"><span class="ln">7885 </span></a>Returns a new tensor that is a narrowed version of :attr:`input` tensor. The 
<a name="l7886"><span class="ln">7886 </span></a>dimension :attr:`dim` is input from :attr:`start` to ``start + length``. The 
<a name="l7887"><span class="ln">7887 </span></a>returned tensor and :attr:`input` tensor share the same underlying storage. 
<a name="l7888"><span class="ln">7888 </span></a> 
<a name="l7889"><span class="ln">7889 </span></a>Args: 
<a name="l7890"><span class="ln">7890 </span></a>    input (Tensor): the tensor to narrow 
<a name="l7891"><span class="ln">7891 </span></a>    dim (int): the dimension along which to narrow 
<a name="l7892"><span class="ln">7892 </span></a>    start (int or Tensor): index of the element to start the narrowed dimension 
<a name="l7893"><span class="ln">7893 </span></a>        from. Can be negative, which means indexing from the end of `dim`. If 
<a name="l7894"><span class="ln">7894 </span></a>        `Tensor`, it must be an 0-dim integral `Tensor` (bools not allowed) 
<a name="l7895"><span class="ln">7895 </span></a>    length (int): length of the narrowed dimension, must be weakly positive 
<a name="l7896"><span class="ln">7896 </span></a> 
<a name="l7897"><span class="ln">7897 </span></a>Example:: 
<a name="l7898"><span class="ln">7898 </span></a> 
<a name="l7899"><span class="ln">7899 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
<a name="l7900"><span class="ln">7900 </span></a>    &gt;&gt;&gt; torch.narrow(x, 0, 0, 2) 
<a name="l7901"><span class="ln">7901 </span></a>    tensor([[ 1,  2,  3], 
<a name="l7902"><span class="ln">7902 </span></a>            [ 4,  5,  6]]) 
<a name="l7903"><span class="ln">7903 </span></a>    &gt;&gt;&gt; torch.narrow(x, 1, 1, 2) 
<a name="l7904"><span class="ln">7904 </span></a>    tensor([[ 2,  3], 
<a name="l7905"><span class="ln">7905 </span></a>            [ 5,  6], 
<a name="l7906"><span class="ln">7906 </span></a>            [ 8,  9]]) 
<a name="l7907"><span class="ln">7907 </span></a>    &gt;&gt;&gt; torch.narrow(x, -1, torch.tensor(-1), 1) 
<a name="l7908"><span class="ln">7908 </span></a>    tensor([[3], 
<a name="l7909"><span class="ln">7909 </span></a>            [6], 
<a name="l7910"><span class="ln">7910 </span></a>            [9]]) 
<a name="l7911"><span class="ln">7911 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l7912"><span class="ln">7912 </span></a><span class="s3">)</span>
<a name="l7913"><span class="ln">7913 </span></a>
<a name="l7914"><span class="ln">7914 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7915"><span class="ln">7915 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">narrow_copy</span><span class="s3">,</span>
<a name="l7916"><span class="ln">7916 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7917"><span class="ln">7917 </span></a>narrow_copy(input, dim, start, length, *, out=None) -&gt; Tensor 
<a name="l7918"><span class="ln">7918 </span></a> 
<a name="l7919"><span class="ln">7919 </span></a>Same as :meth:`Tensor.narrow` except this returns a copy rather 
<a name="l7920"><span class="ln">7920 </span></a>than shared storage. This is primarily for sparse tensors, which 
<a name="l7921"><span class="ln">7921 </span></a>do not have a shared-storage narrow method. 
<a name="l7922"><span class="ln">7922 </span></a> 
<a name="l7923"><span class="ln">7923 </span></a>Args: 
<a name="l7924"><span class="ln">7924 </span></a>    input (Tensor): the tensor to narrow 
<a name="l7925"><span class="ln">7925 </span></a>    dim (int): the dimension along which to narrow 
<a name="l7926"><span class="ln">7926 </span></a>    start (int): index of the element to start the narrowed dimension from. Can 
<a name="l7927"><span class="ln">7927 </span></a>        be negative, which means indexing from the end of `dim` 
<a name="l7928"><span class="ln">7928 </span></a>    length (int): length of the narrowed dimension, must be weakly positive 
<a name="l7929"><span class="ln">7929 </span></a> 
<a name="l7930"><span class="ln">7930 </span></a>Keyword args: 
<a name="l7931"><span class="ln">7931 </span></a>    {out} 
<a name="l7932"><span class="ln">7932 </span></a> 
<a name="l7933"><span class="ln">7933 </span></a>Example:: 
<a name="l7934"><span class="ln">7934 </span></a> 
<a name="l7935"><span class="ln">7935 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
<a name="l7936"><span class="ln">7936 </span></a>    &gt;&gt;&gt; torch.narrow_copy(x, 0, 0, 2) 
<a name="l7937"><span class="ln">7937 </span></a>    tensor([[ 1,  2,  3], 
<a name="l7938"><span class="ln">7938 </span></a>            [ 4,  5,  6]]) 
<a name="l7939"><span class="ln">7939 </span></a>    &gt;&gt;&gt; torch.narrow_copy(x, 1, 1, 2) 
<a name="l7940"><span class="ln">7940 </span></a>    tensor([[ 2,  3], 
<a name="l7941"><span class="ln">7941 </span></a>            [ 5,  6], 
<a name="l7942"><span class="ln">7942 </span></a>            [ 8,  9]]) 
<a name="l7943"><span class="ln">7943 </span></a>    &gt;&gt;&gt; s = torch.arange(16).reshape(2, 2, 2, 2).to_sparse(2) 
<a name="l7944"><span class="ln">7944 </span></a>    &gt;&gt;&gt; torch.narrow_copy(s, 0, 0, 1) 
<a name="l7945"><span class="ln">7945 </span></a>    tensor(indices=tensor([[0, 0], 
<a name="l7946"><span class="ln">7946 </span></a>                           [0, 1]]), 
<a name="l7947"><span class="ln">7947 </span></a>           values=tensor([[[0, 1], 
<a name="l7948"><span class="ln">7948 </span></a>                           [2, 3]], 
<a name="l7949"><span class="ln">7949 </span></a> 
<a name="l7950"><span class="ln">7950 </span></a>                          [[4, 5], 
<a name="l7951"><span class="ln">7951 </span></a>                           [6, 7]]]), 
<a name="l7952"><span class="ln">7952 </span></a>           size=(1, 2, 2, 2), nnz=2, layout=torch.sparse_coo) 
<a name="l7953"><span class="ln">7953 </span></a> 
<a name="l7954"><span class="ln">7954 </span></a>.. seealso:: 
<a name="l7955"><span class="ln">7955 </span></a> 
<a name="l7956"><span class="ln">7956 </span></a>        :func:`torch.narrow` for a non copy variant 
<a name="l7957"><span class="ln">7957 </span></a> 
<a name="l7958"><span class="ln">7958 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7959"><span class="ln">7959 </span></a><span class="s3">)</span>
<a name="l7960"><span class="ln">7960 </span></a>
<a name="l7961"><span class="ln">7961 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7962"><span class="ln">7962 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">nan_to_num</span><span class="s3">,</span>
<a name="l7963"><span class="ln">7963 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l7964"><span class="ln">7964 </span></a>nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -&gt; Tensor 
<a name="l7965"><span class="ln">7965 </span></a> 
<a name="l7966"><span class="ln">7966 </span></a>Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input` 
<a name="l7967"><span class="ln">7967 </span></a>with the values specified by :attr:`nan`, :attr:`posinf`, and :attr:`neginf`, respectively. 
<a name="l7968"><span class="ln">7968 </span></a>By default, :literal:`NaN`\ s are replaced with zero, positive infinity is replaced with the 
<a name="l7969"><span class="ln">7969 </span></a>greatest finite value representable by :attr:`input`'s dtype, and negative infinity 
<a name="l7970"><span class="ln">7970 </span></a>is replaced with the least finite value representable by :attr:`input`'s dtype. 
<a name="l7971"><span class="ln">7971 </span></a> 
<a name="l7972"><span class="ln">7972 </span></a>Args: 
<a name="l7973"><span class="ln">7973 </span></a>    {input} 
<a name="l7974"><span class="ln">7974 </span></a>    nan (Number, optional): the value to replace :literal:`NaN`\s with. Default is zero. 
<a name="l7975"><span class="ln">7975 </span></a>    posinf (Number, optional): if a Number, the value to replace positive infinity values with. 
<a name="l7976"><span class="ln">7976 </span></a>        If None, positive infinity values are replaced with the greatest finite value representable by :attr:`input`'s dtype. 
<a name="l7977"><span class="ln">7977 </span></a>        Default is None. 
<a name="l7978"><span class="ln">7978 </span></a>    neginf (Number, optional): if a Number, the value to replace negative infinity values with. 
<a name="l7979"><span class="ln">7979 </span></a>        If None, negative infinity values are replaced with the lowest finite value representable by :attr:`input`'s dtype. 
<a name="l7980"><span class="ln">7980 </span></a>        Default is None. 
<a name="l7981"><span class="ln">7981 </span></a> 
<a name="l7982"><span class="ln">7982 </span></a>Keyword args: 
<a name="l7983"><span class="ln">7983 </span></a>    {out} 
<a name="l7984"><span class="ln">7984 </span></a> 
<a name="l7985"><span class="ln">7985 </span></a>Example:: 
<a name="l7986"><span class="ln">7986 </span></a> 
<a name="l7987"><span class="ln">7987 </span></a>    &gt;&gt;&gt; x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14]) 
<a name="l7988"><span class="ln">7988 </span></a>    &gt;&gt;&gt; torch.nan_to_num(x) 
<a name="l7989"><span class="ln">7989 </span></a>    tensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00]) 
<a name="l7990"><span class="ln">7990 </span></a>    &gt;&gt;&gt; torch.nan_to_num(x, nan=2.0) 
<a name="l7991"><span class="ln">7991 </span></a>    tensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00]) 
<a name="l7992"><span class="ln">7992 </span></a>    &gt;&gt;&gt; torch.nan_to_num(x, nan=2.0, posinf=1.0) 
<a name="l7993"><span class="ln">7993 </span></a>    tensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00]) 
<a name="l7994"><span class="ln">7994 </span></a> 
<a name="l7995"><span class="ln">7995 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l7996"><span class="ln">7996 </span></a><span class="s3">)</span>
<a name="l7997"><span class="ln">7997 </span></a>
<a name="l7998"><span class="ln">7998 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l7999"><span class="ln">7999 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ne</span><span class="s3">,</span>
<a name="l8000"><span class="ln">8000 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8001"><span class="ln">8001 </span></a>ne(input, other, *, out=None) -&gt; Tensor 
<a name="l8002"><span class="ln">8002 </span></a> 
<a name="l8003"><span class="ln">8003 </span></a>Computes :math:`\text{input} \neq \text{other}` element-wise. 
<a name="l8004"><span class="ln">8004 </span></a>&quot;&quot;&quot;</span>
<a name="l8005"><span class="ln">8005 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l8006"><span class="ln">8006 </span></a> 
<a name="l8007"><span class="ln">8007 </span></a>The second argument can be a number or a tensor whose shape is 
<a name="l8008"><span class="ln">8008 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l8009"><span class="ln">8009 </span></a> 
<a name="l8010"><span class="ln">8010 </span></a>Args: 
<a name="l8011"><span class="ln">8011 </span></a>    input (Tensor): the tensor to compare 
<a name="l8012"><span class="ln">8012 </span></a>    other (Tensor or float): the tensor or value to compare 
<a name="l8013"><span class="ln">8013 </span></a> 
<a name="l8014"><span class="ln">8014 </span></a>Keyword args: 
<a name="l8015"><span class="ln">8015 </span></a>    {out} 
<a name="l8016"><span class="ln">8016 </span></a> 
<a name="l8017"><span class="ln">8017 </span></a>Returns: 
<a name="l8018"><span class="ln">8018 </span></a>    A boolean tensor that is True where :attr:`input` is not equal to :attr:`other` and False elsewhere 
<a name="l8019"><span class="ln">8019 </span></a> 
<a name="l8020"><span class="ln">8020 </span></a>Example:: 
<a name="l8021"><span class="ln">8021 </span></a> 
<a name="l8022"><span class="ln">8022 </span></a>    &gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l8023"><span class="ln">8023 </span></a>    tensor([[False, True], [True, False]]) 
<a name="l8024"><span class="ln">8024 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8025"><span class="ln">8025 </span></a><span class="s3">)</span>
<a name="l8026"><span class="ln">8026 </span></a>
<a name="l8027"><span class="ln">8027 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8028"><span class="ln">8028 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">not_equal</span><span class="s3">,</span>
<a name="l8029"><span class="ln">8029 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8030"><span class="ln">8030 </span></a>not_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l8031"><span class="ln">8031 </span></a> 
<a name="l8032"><span class="ln">8032 </span></a>Alias for :func:`torch.ne`. 
<a name="l8033"><span class="ln">8033 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l8034"><span class="ln">8034 </span></a><span class="s3">)</span>
<a name="l8035"><span class="ln">8035 </span></a>
<a name="l8036"><span class="ln">8036 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8037"><span class="ln">8037 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">neg</span><span class="s3">,</span>
<a name="l8038"><span class="ln">8038 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8039"><span class="ln">8039 </span></a>neg(input, *, out=None) -&gt; Tensor 
<a name="l8040"><span class="ln">8040 </span></a> 
<a name="l8041"><span class="ln">8041 </span></a>Returns a new tensor with the negative of the elements of :attr:`input`. 
<a name="l8042"><span class="ln">8042 </span></a> 
<a name="l8043"><span class="ln">8043 </span></a>.. math:: 
<a name="l8044"><span class="ln">8044 </span></a>    \text{out} = -1 \times \text{input} 
<a name="l8045"><span class="ln">8045 </span></a>&quot;&quot;&quot;</span>
<a name="l8046"><span class="ln">8046 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l8047"><span class="ln">8047 </span></a>Args: 
<a name="l8048"><span class="ln">8048 </span></a>    {input} 
<a name="l8049"><span class="ln">8049 </span></a> 
<a name="l8050"><span class="ln">8050 </span></a>Keyword args: 
<a name="l8051"><span class="ln">8051 </span></a>    {out} 
<a name="l8052"><span class="ln">8052 </span></a> 
<a name="l8053"><span class="ln">8053 </span></a>Example:: 
<a name="l8054"><span class="ln">8054 </span></a> 
<a name="l8055"><span class="ln">8055 </span></a>    &gt;&gt;&gt; a = torch.randn(5) 
<a name="l8056"><span class="ln">8056 </span></a>    &gt;&gt;&gt; a 
<a name="l8057"><span class="ln">8057 </span></a>    tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940]) 
<a name="l8058"><span class="ln">8058 </span></a>    &gt;&gt;&gt; torch.neg(a) 
<a name="l8059"><span class="ln">8059 </span></a>    tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940]) 
<a name="l8060"><span class="ln">8060 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8061"><span class="ln">8061 </span></a><span class="s3">)</span>
<a name="l8062"><span class="ln">8062 </span></a>
<a name="l8063"><span class="ln">8063 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8064"><span class="ln">8064 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">negative</span><span class="s3">,</span>
<a name="l8065"><span class="ln">8065 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8066"><span class="ln">8066 </span></a>negative(input, *, out=None) -&gt; Tensor 
<a name="l8067"><span class="ln">8067 </span></a> 
<a name="l8068"><span class="ln">8068 </span></a>Alias for :func:`torch.neg` 
<a name="l8069"><span class="ln">8069 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l8070"><span class="ln">8070 </span></a><span class="s3">)</span>
<a name="l8071"><span class="ln">8071 </span></a>
<a name="l8072"><span class="ln">8072 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8073"><span class="ln">8073 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">nextafter</span><span class="s3">,</span>
<a name="l8074"><span class="ln">8074 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8075"><span class="ln">8075 </span></a>nextafter(input, other, *, out=None) -&gt; Tensor 
<a name="l8076"><span class="ln">8076 </span></a> 
<a name="l8077"><span class="ln">8077 </span></a>Return the next floating-point value after :attr:`input` towards :attr:`other`, elementwise. 
<a name="l8078"><span class="ln">8078 </span></a> 
<a name="l8079"><span class="ln">8079 </span></a>The shapes of ``input`` and ``other`` must be 
<a name="l8080"><span class="ln">8080 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l8081"><span class="ln">8081 </span></a> 
<a name="l8082"><span class="ln">8082 </span></a>Args: 
<a name="l8083"><span class="ln">8083 </span></a>    input (Tensor): the first input tensor 
<a name="l8084"><span class="ln">8084 </span></a>    other (Tensor): the second input tensor 
<a name="l8085"><span class="ln">8085 </span></a> 
<a name="l8086"><span class="ln">8086 </span></a>Keyword args: 
<a name="l8087"><span class="ln">8087 </span></a>    {out} 
<a name="l8088"><span class="ln">8088 </span></a> 
<a name="l8089"><span class="ln">8089 </span></a>Example:: 
<a name="l8090"><span class="ln">8090 </span></a> 
<a name="l8091"><span class="ln">8091 </span></a>    &gt;&gt;&gt; eps = torch.finfo(torch.float32).eps 
<a name="l8092"><span class="ln">8092 </span></a>    &gt;&gt;&gt; torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps]) 
<a name="l8093"><span class="ln">8093 </span></a>    tensor([True, True]) 
<a name="l8094"><span class="ln">8094 </span></a> 
<a name="l8095"><span class="ln">8095 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8096"><span class="ln">8096 </span></a><span class="s3">)</span>
<a name="l8097"><span class="ln">8097 </span></a>
<a name="l8098"><span class="ln">8098 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8099"><span class="ln">8099 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">nonzero</span><span class="s3">,</span>
<a name="l8100"><span class="ln">8100 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8101"><span class="ln">8101 </span></a>nonzero(input, *, out=None, as_tuple=False) -&gt; LongTensor or tuple of LongTensors 
<a name="l8102"><span class="ln">8102 </span></a> 
<a name="l8103"><span class="ln">8103 </span></a>.. note:: 
<a name="l8104"><span class="ln">8104 </span></a>    :func:`torch.nonzero(..., as_tuple=False) &lt;torch.nonzero&gt;` (default) returns a 
<a name="l8105"><span class="ln">8105 </span></a>    2-D tensor where each row is the index for a nonzero value. 
<a name="l8106"><span class="ln">8106 </span></a> 
<a name="l8107"><span class="ln">8107 </span></a>    :func:`torch.nonzero(..., as_tuple=True) &lt;torch.nonzero&gt;` returns a tuple of 1-D 
<a name="l8108"><span class="ln">8108 </span></a>    index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]`` 
<a name="l8109"><span class="ln">8109 </span></a>    gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor 
<a name="l8110"><span class="ln">8110 </span></a>    contains nonzero indices for a certain dimension. 
<a name="l8111"><span class="ln">8111 </span></a> 
<a name="l8112"><span class="ln">8112 </span></a>    See below for more details on the two behaviors. 
<a name="l8113"><span class="ln">8113 </span></a> 
<a name="l8114"><span class="ln">8114 </span></a>    When :attr:`input` is on CUDA, :func:`torch.nonzero() &lt;torch.nonzero&gt;` causes 
<a name="l8115"><span class="ln">8115 </span></a>    host-device synchronization. 
<a name="l8116"><span class="ln">8116 </span></a> 
<a name="l8117"><span class="ln">8117 </span></a>**When** :attr:`as_tuple` **is** ``False`` **(default)**: 
<a name="l8118"><span class="ln">8118 </span></a> 
<a name="l8119"><span class="ln">8119 </span></a>Returns a tensor containing the indices of all non-zero elements of 
<a name="l8120"><span class="ln">8120 </span></a>:attr:`input`.  Each row in the result contains the indices of a non-zero 
<a name="l8121"><span class="ln">8121 </span></a>element in :attr:`input`. The result is sorted lexicographically, with 
<a name="l8122"><span class="ln">8122 </span></a>the last index changing the fastest (C-style). 
<a name="l8123"><span class="ln">8123 </span></a> 
<a name="l8124"><span class="ln">8124 </span></a>If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor 
<a name="l8125"><span class="ln">8125 </span></a>:attr:`out` is of size :math:`(z \times n)`, where :math:`z` is the total number of 
<a name="l8126"><span class="ln">8126 </span></a>non-zero elements in the :attr:`input` tensor. 
<a name="l8127"><span class="ln">8127 </span></a> 
<a name="l8128"><span class="ln">8128 </span></a>**When** :attr:`as_tuple` **is** ``True``: 
<a name="l8129"><span class="ln">8129 </span></a> 
<a name="l8130"><span class="ln">8130 </span></a>Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`, 
<a name="l8131"><span class="ln">8131 </span></a>each containing the indices (in that dimension) of all non-zero elements of 
<a name="l8132"><span class="ln">8132 </span></a>:attr:`input` . 
<a name="l8133"><span class="ln">8133 </span></a> 
<a name="l8134"><span class="ln">8134 </span></a>If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n` 
<a name="l8135"><span class="ln">8135 </span></a>tensors of size :math:`z`, where :math:`z` is the total number of 
<a name="l8136"><span class="ln">8136 </span></a>non-zero elements in the :attr:`input` tensor. 
<a name="l8137"><span class="ln">8137 </span></a> 
<a name="l8138"><span class="ln">8138 </span></a>As a special case, when :attr:`input` has zero dimensions and a nonzero scalar 
<a name="l8139"><span class="ln">8139 </span></a>value, it is treated as a one-dimensional tensor with one element. 
<a name="l8140"><span class="ln">8140 </span></a> 
<a name="l8141"><span class="ln">8141 </span></a>Args: 
<a name="l8142"><span class="ln">8142 </span></a>    {input} 
<a name="l8143"><span class="ln">8143 </span></a> 
<a name="l8144"><span class="ln">8144 </span></a>Keyword args: 
<a name="l8145"><span class="ln">8145 </span></a>    out (LongTensor, optional): the output tensor containing indices 
<a name="l8146"><span class="ln">8146 </span></a> 
<a name="l8147"><span class="ln">8147 </span></a>Returns: 
<a name="l8148"><span class="ln">8148 </span></a>    LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output 
<a name="l8149"><span class="ln">8149 </span></a>    tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for 
<a name="l8150"><span class="ln">8150 </span></a>    each dimension, containing the indices of each nonzero element along that 
<a name="l8151"><span class="ln">8151 </span></a>    dimension. 
<a name="l8152"><span class="ln">8152 </span></a> 
<a name="l8153"><span class="ln">8153 </span></a>Example:: 
<a name="l8154"><span class="ln">8154 </span></a> 
<a name="l8155"><span class="ln">8155 </span></a>    &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1])) 
<a name="l8156"><span class="ln">8156 </span></a>    tensor([[ 0], 
<a name="l8157"><span class="ln">8157 </span></a>            [ 1], 
<a name="l8158"><span class="ln">8158 </span></a>            [ 2], 
<a name="l8159"><span class="ln">8159 </span></a>            [ 4]]) 
<a name="l8160"><span class="ln">8160 </span></a>    &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], 
<a name="l8161"><span class="ln">8161 </span></a>    ...                             [0.0, 0.4, 0.0, 0.0], 
<a name="l8162"><span class="ln">8162 </span></a>    ...                             [0.0, 0.0, 1.2, 0.0], 
<a name="l8163"><span class="ln">8163 </span></a>    ...                             [0.0, 0.0, 0.0,-0.4]])) 
<a name="l8164"><span class="ln">8164 </span></a>    tensor([[ 0,  0], 
<a name="l8165"><span class="ln">8165 </span></a>            [ 1,  1], 
<a name="l8166"><span class="ln">8166 </span></a>            [ 2,  2], 
<a name="l8167"><span class="ln">8167 </span></a>            [ 3,  3]]) 
<a name="l8168"><span class="ln">8168 </span></a>    &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True) 
<a name="l8169"><span class="ln">8169 </span></a>    (tensor([0, 1, 2, 4]),) 
<a name="l8170"><span class="ln">8170 </span></a>    &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], 
<a name="l8171"><span class="ln">8171 </span></a>    ...                             [0.0, 0.4, 0.0, 0.0], 
<a name="l8172"><span class="ln">8172 </span></a>    ...                             [0.0, 0.0, 1.2, 0.0], 
<a name="l8173"><span class="ln">8173 </span></a>    ...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True) 
<a name="l8174"><span class="ln">8174 </span></a>    (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])) 
<a name="l8175"><span class="ln">8175 </span></a>    &gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True) 
<a name="l8176"><span class="ln">8176 </span></a>    (tensor([0]),) 
<a name="l8177"><span class="ln">8177 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8178"><span class="ln">8178 </span></a><span class="s3">)</span>
<a name="l8179"><span class="ln">8179 </span></a>
<a name="l8180"><span class="ln">8180 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8181"><span class="ln">8181 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">normal</span><span class="s3">,</span>
<a name="l8182"><span class="ln">8182 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8183"><span class="ln">8183 </span></a>normal(mean, std, *, generator=None, out=None) -&gt; Tensor 
<a name="l8184"><span class="ln">8184 </span></a> 
<a name="l8185"><span class="ln">8185 </span></a>Returns a tensor of random numbers drawn from separate normal distributions 
<a name="l8186"><span class="ln">8186 </span></a>whose mean and standard deviation are given. 
<a name="l8187"><span class="ln">8187 </span></a> 
<a name="l8188"><span class="ln">8188 </span></a>The :attr:`mean` is a tensor with the mean of 
<a name="l8189"><span class="ln">8189 </span></a>each output element's normal distribution 
<a name="l8190"><span class="ln">8190 </span></a> 
<a name="l8191"><span class="ln">8191 </span></a>The :attr:`std` is a tensor with the standard deviation of 
<a name="l8192"><span class="ln">8192 </span></a>each output element's normal distribution 
<a name="l8193"><span class="ln">8193 </span></a> 
<a name="l8194"><span class="ln">8194 </span></a>The shapes of :attr:`mean` and :attr:`std` don't need to match, but the 
<a name="l8195"><span class="ln">8195 </span></a>total number of elements in each tensor need to be the same. 
<a name="l8196"><span class="ln">8196 </span></a> 
<a name="l8197"><span class="ln">8197 </span></a>.. note:: When the shapes do not match, the shape of :attr:`mean` 
<a name="l8198"><span class="ln">8198 </span></a>          is used as the shape for the returned output tensor 
<a name="l8199"><span class="ln">8199 </span></a> 
<a name="l8200"><span class="ln">8200 </span></a>.. note:: When :attr:`std` is a CUDA tensor, this function synchronizes 
<a name="l8201"><span class="ln">8201 </span></a>          its device with the CPU. 
<a name="l8202"><span class="ln">8202 </span></a> 
<a name="l8203"><span class="ln">8203 </span></a>Args: 
<a name="l8204"><span class="ln">8204 </span></a>    mean (Tensor): the tensor of per-element means 
<a name="l8205"><span class="ln">8205 </span></a>    std (Tensor): the tensor of per-element standard deviations 
<a name="l8206"><span class="ln">8206 </span></a> 
<a name="l8207"><span class="ln">8207 </span></a>Keyword args: 
<a name="l8208"><span class="ln">8208 </span></a>    {generator} 
<a name="l8209"><span class="ln">8209 </span></a>    {out} 
<a name="l8210"><span class="ln">8210 </span></a> 
<a name="l8211"><span class="ln">8211 </span></a>Example:: 
<a name="l8212"><span class="ln">8212 </span></a> 
<a name="l8213"><span class="ln">8213 </span></a>    &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) 
<a name="l8214"><span class="ln">8214 </span></a>    tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134, 
<a name="l8215"><span class="ln">8215 </span></a>              8.0505,   8.1408,   9.0563,  10.0566]) 
<a name="l8216"><span class="ln">8216 </span></a> 
<a name="l8217"><span class="ln">8217 </span></a>.. function:: normal(mean=0.0, std, *, out=None) -&gt; Tensor 
<a name="l8218"><span class="ln">8218 </span></a>   :noindex: 
<a name="l8219"><span class="ln">8219 </span></a> 
<a name="l8220"><span class="ln">8220 </span></a>Similar to the function above, but the means are shared among all drawn 
<a name="l8221"><span class="ln">8221 </span></a>elements. 
<a name="l8222"><span class="ln">8222 </span></a> 
<a name="l8223"><span class="ln">8223 </span></a>Args: 
<a name="l8224"><span class="ln">8224 </span></a>    mean (float, optional): the mean for all distributions 
<a name="l8225"><span class="ln">8225 </span></a>    std (Tensor): the tensor of per-element standard deviations 
<a name="l8226"><span class="ln">8226 </span></a> 
<a name="l8227"><span class="ln">8227 </span></a>Keyword args: 
<a name="l8228"><span class="ln">8228 </span></a>    {out} 
<a name="l8229"><span class="ln">8229 </span></a> 
<a name="l8230"><span class="ln">8230 </span></a>Example:: 
<a name="l8231"><span class="ln">8231 </span></a> 
<a name="l8232"><span class="ln">8232 </span></a>    &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.)) 
<a name="l8233"><span class="ln">8233 </span></a>    tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303]) 
<a name="l8234"><span class="ln">8234 </span></a> 
<a name="l8235"><span class="ln">8235 </span></a>.. function:: normal(mean, std=1.0, *, out=None) -&gt; Tensor 
<a name="l8236"><span class="ln">8236 </span></a>   :noindex: 
<a name="l8237"><span class="ln">8237 </span></a> 
<a name="l8238"><span class="ln">8238 </span></a>Similar to the function above, but the standard deviations are shared among 
<a name="l8239"><span class="ln">8239 </span></a>all drawn elements. 
<a name="l8240"><span class="ln">8240 </span></a> 
<a name="l8241"><span class="ln">8241 </span></a>Args: 
<a name="l8242"><span class="ln">8242 </span></a>    mean (Tensor): the tensor of per-element means 
<a name="l8243"><span class="ln">8243 </span></a>    std (float, optional): the standard deviation for all distributions 
<a name="l8244"><span class="ln">8244 </span></a> 
<a name="l8245"><span class="ln">8245 </span></a>Keyword args: 
<a name="l8246"><span class="ln">8246 </span></a>    out (Tensor, optional): the output tensor 
<a name="l8247"><span class="ln">8247 </span></a> 
<a name="l8248"><span class="ln">8248 </span></a>Example:: 
<a name="l8249"><span class="ln">8249 </span></a> 
<a name="l8250"><span class="ln">8250 </span></a>    &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.)) 
<a name="l8251"><span class="ln">8251 </span></a>    tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361]) 
<a name="l8252"><span class="ln">8252 </span></a> 
<a name="l8253"><span class="ln">8253 </span></a>.. function:: normal(mean, std, size, *, out=None) -&gt; Tensor 
<a name="l8254"><span class="ln">8254 </span></a>   :noindex: 
<a name="l8255"><span class="ln">8255 </span></a> 
<a name="l8256"><span class="ln">8256 </span></a>Similar to the function above, but the means and standard deviations are shared 
<a name="l8257"><span class="ln">8257 </span></a>among all drawn elements. The resulting tensor has size given by :attr:`size`. 
<a name="l8258"><span class="ln">8258 </span></a> 
<a name="l8259"><span class="ln">8259 </span></a>Args: 
<a name="l8260"><span class="ln">8260 </span></a>    mean (float): the mean for all distributions 
<a name="l8261"><span class="ln">8261 </span></a>    std (float): the standard deviation for all distributions 
<a name="l8262"><span class="ln">8262 </span></a>    size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l8263"><span class="ln">8263 </span></a> 
<a name="l8264"><span class="ln">8264 </span></a>Keyword args: 
<a name="l8265"><span class="ln">8265 </span></a>    {out} 
<a name="l8266"><span class="ln">8266 </span></a> 
<a name="l8267"><span class="ln">8267 </span></a>Example:: 
<a name="l8268"><span class="ln">8268 </span></a> 
<a name="l8269"><span class="ln">8269 </span></a>    &gt;&gt;&gt; torch.normal(2, 3, size=(1, 4)) 
<a name="l8270"><span class="ln">8270 </span></a>    tensor([[-1.3987, -1.9544,  3.6048,  0.7909]]) 
<a name="l8271"><span class="ln">8271 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8272"><span class="ln">8272 </span></a><span class="s3">)</span>
<a name="l8273"><span class="ln">8273 </span></a>
<a name="l8274"><span class="ln">8274 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8275"><span class="ln">8275 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">numel</span><span class="s3">,</span>
<a name="l8276"><span class="ln">8276 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8277"><span class="ln">8277 </span></a>numel(input: Tensor) -&gt; int 
<a name="l8278"><span class="ln">8278 </span></a> 
<a name="l8279"><span class="ln">8279 </span></a>Returns the total number of elements in the :attr:`input` tensor. 
<a name="l8280"><span class="ln">8280 </span></a> 
<a name="l8281"><span class="ln">8281 </span></a>Args: 
<a name="l8282"><span class="ln">8282 </span></a>    {input} 
<a name="l8283"><span class="ln">8283 </span></a> 
<a name="l8284"><span class="ln">8284 </span></a>Example:: 
<a name="l8285"><span class="ln">8285 </span></a> 
<a name="l8286"><span class="ln">8286 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5) 
<a name="l8287"><span class="ln">8287 </span></a>    &gt;&gt;&gt; torch.numel(a) 
<a name="l8288"><span class="ln">8288 </span></a>    120 
<a name="l8289"><span class="ln">8289 </span></a>    &gt;&gt;&gt; a = torch.zeros(4,4) 
<a name="l8290"><span class="ln">8290 </span></a>    &gt;&gt;&gt; torch.numel(a) 
<a name="l8291"><span class="ln">8291 </span></a>    16 
<a name="l8292"><span class="ln">8292 </span></a> 
<a name="l8293"><span class="ln">8293 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8294"><span class="ln">8294 </span></a><span class="s3">)</span>
<a name="l8295"><span class="ln">8295 </span></a>
<a name="l8296"><span class="ln">8296 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8297"><span class="ln">8297 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ones</span><span class="s3">,</span>
<a name="l8298"><span class="ln">8298 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8299"><span class="ln">8299 </span></a>ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l8300"><span class="ln">8300 </span></a> 
<a name="l8301"><span class="ln">8301 </span></a>Returns a tensor filled with the scalar value `1`, with the shape defined 
<a name="l8302"><span class="ln">8302 </span></a>by the variable argument :attr:`size`. 
<a name="l8303"><span class="ln">8303 </span></a> 
<a name="l8304"><span class="ln">8304 </span></a>Args: 
<a name="l8305"><span class="ln">8305 </span></a>    size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l8306"><span class="ln">8306 </span></a>        Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l8307"><span class="ln">8307 </span></a> 
<a name="l8308"><span class="ln">8308 </span></a>Keyword arguments: 
<a name="l8309"><span class="ln">8309 </span></a>    {out} 
<a name="l8310"><span class="ln">8310 </span></a>    {dtype} 
<a name="l8311"><span class="ln">8311 </span></a>    {layout} 
<a name="l8312"><span class="ln">8312 </span></a>    {device} 
<a name="l8313"><span class="ln">8313 </span></a>    {requires_grad} 
<a name="l8314"><span class="ln">8314 </span></a> 
<a name="l8315"><span class="ln">8315 </span></a>Example:: 
<a name="l8316"><span class="ln">8316 </span></a> 
<a name="l8317"><span class="ln">8317 </span></a>    &gt;&gt;&gt; torch.ones(2, 3) 
<a name="l8318"><span class="ln">8318 </span></a>    tensor([[ 1.,  1.,  1.], 
<a name="l8319"><span class="ln">8319 </span></a>            [ 1.,  1.,  1.]]) 
<a name="l8320"><span class="ln">8320 </span></a> 
<a name="l8321"><span class="ln">8321 </span></a>    &gt;&gt;&gt; torch.ones(5) 
<a name="l8322"><span class="ln">8322 </span></a>    tensor([ 1.,  1.,  1.,  1.,  1.]) 
<a name="l8323"><span class="ln">8323 </span></a> 
<a name="l8324"><span class="ln">8324 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l8325"><span class="ln">8325 </span></a><span class="s3">)</span>
<a name="l8326"><span class="ln">8326 </span></a>
<a name="l8327"><span class="ln">8327 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8328"><span class="ln">8328 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ones_like</span><span class="s3">,</span>
<a name="l8329"><span class="ln">8329 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8330"><span class="ln">8330 </span></a>ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l8331"><span class="ln">8331 </span></a> 
<a name="l8332"><span class="ln">8332 </span></a>Returns a tensor filled with the scalar value `1`, with the same size as 
<a name="l8333"><span class="ln">8333 </span></a>:attr:`input`. ``torch.ones_like(input)`` is equivalent to 
<a name="l8334"><span class="ln">8334 </span></a>``torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l8335"><span class="ln">8335 </span></a> 
<a name="l8336"><span class="ln">8336 </span></a>.. warning:: 
<a name="l8337"><span class="ln">8337 </span></a>    As of 0.4, this function does not support an :attr:`out` keyword. As an alternative, 
<a name="l8338"><span class="ln">8338 </span></a>    the old ``torch.ones_like(input, out=output)`` is equivalent to 
<a name="l8339"><span class="ln">8339 </span></a>    ``torch.ones(input.size(), out=output)``. 
<a name="l8340"><span class="ln">8340 </span></a> 
<a name="l8341"><span class="ln">8341 </span></a>Args: 
<a name="l8342"><span class="ln">8342 </span></a>    {input} 
<a name="l8343"><span class="ln">8343 </span></a> 
<a name="l8344"><span class="ln">8344 </span></a>Keyword arguments: 
<a name="l8345"><span class="ln">8345 </span></a>    {dtype} 
<a name="l8346"><span class="ln">8346 </span></a>    {layout} 
<a name="l8347"><span class="ln">8347 </span></a>    {device} 
<a name="l8348"><span class="ln">8348 </span></a>    {requires_grad} 
<a name="l8349"><span class="ln">8349 </span></a>    {memory_format} 
<a name="l8350"><span class="ln">8350 </span></a> 
<a name="l8351"><span class="ln">8351 </span></a>Example:: 
<a name="l8352"><span class="ln">8352 </span></a> 
<a name="l8353"><span class="ln">8353 </span></a>    &gt;&gt;&gt; input = torch.empty(2, 3) 
<a name="l8354"><span class="ln">8354 </span></a>    &gt;&gt;&gt; torch.ones_like(input) 
<a name="l8355"><span class="ln">8355 </span></a>    tensor([[ 1.,  1.,  1.], 
<a name="l8356"><span class="ln">8356 </span></a>            [ 1.,  1.,  1.]]) 
<a name="l8357"><span class="ln">8357 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_like_common_args</span><span class="s3">),</span>
<a name="l8358"><span class="ln">8358 </span></a><span class="s3">)</span>
<a name="l8359"><span class="ln">8359 </span></a>
<a name="l8360"><span class="ln">8360 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8361"><span class="ln">8361 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">orgqr</span><span class="s3">,</span>
<a name="l8362"><span class="ln">8362 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8363"><span class="ln">8363 </span></a>orgqr(input, tau) -&gt; Tensor 
<a name="l8364"><span class="ln">8364 </span></a> 
<a name="l8365"><span class="ln">8365 </span></a>Alias for :func:`torch.linalg.householder_product`. 
<a name="l8366"><span class="ln">8366 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l8367"><span class="ln">8367 </span></a><span class="s3">)</span>
<a name="l8368"><span class="ln">8368 </span></a>
<a name="l8369"><span class="ln">8369 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8370"><span class="ln">8370 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ormqr</span><span class="s3">,</span>
<a name="l8371"><span class="ln">8371 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8372"><span class="ln">8372 </span></a>ormqr(input, tau, other, left=True, transpose=False, *, out=None) -&gt; Tensor 
<a name="l8373"><span class="ln">8373 </span></a> 
<a name="l8374"><span class="ln">8374 </span></a>Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. 
<a name="l8375"><span class="ln">8375 </span></a> 
<a name="l8376"><span class="ln">8376 </span></a>Multiplies a :math:`m \times n` matrix `C` (given by :attr:`other`) with a matrix `Q`, 
<a name="l8377"><span class="ln">8377 </span></a>where `Q` is represented using Householder reflectors `(input, tau)`. 
<a name="l8378"><span class="ln">8378 </span></a>See `Representation of Orthogonal or Unitary Matrices`_ for further details. 
<a name="l8379"><span class="ln">8379 </span></a> 
<a name="l8380"><span class="ln">8380 </span></a>If :attr:`left` is `True` then `op(Q)` times `C` is computed, otherwise the result is `C` times `op(Q)`. 
<a name="l8381"><span class="ln">8381 </span></a>When :attr:`left` is `True`, the implicit matrix `Q` has size :math:`m \times m`. 
<a name="l8382"><span class="ln">8382 </span></a>It has size :math:`n \times n` otherwise. 
<a name="l8383"><span class="ln">8383 </span></a>If :attr:`transpose` is `True` then `op` is the conjugate transpose operation, otherwise it's a no-op. 
<a name="l8384"><span class="ln">8384 </span></a> 
<a name="l8385"><span class="ln">8385 </span></a>Supports inputs of float, double, cfloat and cdouble dtypes. 
<a name="l8386"><span class="ln">8386 </span></a>Also supports batched inputs, and, if the input is batched, the output is batched with the same dimensions. 
<a name="l8387"><span class="ln">8387 </span></a> 
<a name="l8388"><span class="ln">8388 </span></a>.. seealso:: 
<a name="l8389"><span class="ln">8389 </span></a>        :func:`torch.geqrf` can be used to form the Householder representation `(input, tau)` of matrix `Q` 
<a name="l8390"><span class="ln">8390 </span></a>        from the QR decomposition. 
<a name="l8391"><span class="ln">8391 </span></a> 
<a name="l8392"><span class="ln">8392 </span></a>.. note:: 
<a name="l8393"><span class="ln">8393 </span></a>        This function supports backward but it is only fast when ``(input, tau)`` do not require gradients 
<a name="l8394"><span class="ln">8394 </span></a>        and/or ``tau.size(-1)`` is very small. 
<a name="l8395"><span class="ln">8395 </span></a>        `` 
<a name="l8396"><span class="ln">8396 </span></a> 
<a name="l8397"><span class="ln">8397 </span></a>Args: 
<a name="l8398"><span class="ln">8398 </span></a>    input (Tensor): tensor of shape `(*, mn, k)` where `*` is zero or more batch dimensions 
<a name="l8399"><span class="ln">8399 </span></a>                    and `mn` equals to `m` or `n` depending on the :attr:`left`. 
<a name="l8400"><span class="ln">8400 </span></a>    tau (Tensor): tensor of shape `(*, min(mn, k))` where `*` is zero or more batch dimensions. 
<a name="l8401"><span class="ln">8401 </span></a>    other (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions. 
<a name="l8402"><span class="ln">8402 </span></a>    left (bool): controls the order of multiplication. 
<a name="l8403"><span class="ln">8403 </span></a>    transpose (bool): controls whether the matrix `Q` is conjugate transposed or not. 
<a name="l8404"><span class="ln">8404 </span></a> 
<a name="l8405"><span class="ln">8405 </span></a>Keyword args: 
<a name="l8406"><span class="ln">8406 </span></a>    out (Tensor, optional): the output Tensor. Ignored if `None`. Default: `None`. 
<a name="l8407"><span class="ln">8407 </span></a> 
<a name="l8408"><span class="ln">8408 </span></a>.. _Representation of Orthogonal or Unitary Matrices: 
<a name="l8409"><span class="ln">8409 </span></a>    https://www.netlib.org/lapack/lug/node128.html 
<a name="l8410"><span class="ln">8410 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l8411"><span class="ln">8411 </span></a><span class="s3">)</span>
<a name="l8412"><span class="ln">8412 </span></a>
<a name="l8413"><span class="ln">8413 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8414"><span class="ln">8414 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">permute</span><span class="s3">,</span>
<a name="l8415"><span class="ln">8415 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8416"><span class="ln">8416 </span></a>permute(input, dims) -&gt; Tensor 
<a name="l8417"><span class="ln">8417 </span></a> 
<a name="l8418"><span class="ln">8418 </span></a>Returns a view of the original tensor :attr:`input` with its dimensions permuted. 
<a name="l8419"><span class="ln">8419 </span></a> 
<a name="l8420"><span class="ln">8420 </span></a>Args: 
<a name="l8421"><span class="ln">8421 </span></a>    {input} 
<a name="l8422"><span class="ln">8422 </span></a>    dims (tuple of int): The desired ordering of dimensions 
<a name="l8423"><span class="ln">8423 </span></a> 
<a name="l8424"><span class="ln">8424 </span></a>Example: 
<a name="l8425"><span class="ln">8425 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 3, 5) 
<a name="l8426"><span class="ln">8426 </span></a>    &gt;&gt;&gt; x.size() 
<a name="l8427"><span class="ln">8427 </span></a>    torch.Size([2, 3, 5]) 
<a name="l8428"><span class="ln">8428 </span></a>    &gt;&gt;&gt; torch.permute(x, (2, 0, 1)).size() 
<a name="l8429"><span class="ln">8429 </span></a>    torch.Size([5, 2, 3]) 
<a name="l8430"><span class="ln">8430 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8431"><span class="ln">8431 </span></a><span class="s3">)</span>
<a name="l8432"><span class="ln">8432 </span></a>
<a name="l8433"><span class="ln">8433 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8434"><span class="ln">8434 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">poisson</span><span class="s3">,</span>
<a name="l8435"><span class="ln">8435 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8436"><span class="ln">8436 </span></a>poisson(input, generator=None) -&gt; Tensor 
<a name="l8437"><span class="ln">8437 </span></a> 
<a name="l8438"><span class="ln">8438 </span></a>Returns a tensor of the same size as :attr:`input` with each element 
<a name="l8439"><span class="ln">8439 </span></a>sampled from a Poisson distribution with rate parameter given by the corresponding 
<a name="l8440"><span class="ln">8440 </span></a>element in :attr:`input` i.e., 
<a name="l8441"><span class="ln">8441 </span></a> 
<a name="l8442"><span class="ln">8442 </span></a>.. math:: 
<a name="l8443"><span class="ln">8443 </span></a>    \text{{out}}_i \sim \text{{Poisson}}(\text{{input}}_i) 
<a name="l8444"><span class="ln">8444 </span></a> 
<a name="l8445"><span class="ln">8445 </span></a>:attr:`input` must be non-negative. 
<a name="l8446"><span class="ln">8446 </span></a> 
<a name="l8447"><span class="ln">8447 </span></a>Args: 
<a name="l8448"><span class="ln">8448 </span></a>    input (Tensor): the input tensor containing the rates of the Poisson distribution 
<a name="l8449"><span class="ln">8449 </span></a> 
<a name="l8450"><span class="ln">8450 </span></a>Keyword args: 
<a name="l8451"><span class="ln">8451 </span></a>    {generator} 
<a name="l8452"><span class="ln">8452 </span></a> 
<a name="l8453"><span class="ln">8453 </span></a>Example:: 
<a name="l8454"><span class="ln">8454 </span></a> 
<a name="l8455"><span class="ln">8455 </span></a>    &gt;&gt;&gt; rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5 
<a name="l8456"><span class="ln">8456 </span></a>    &gt;&gt;&gt; torch.poisson(rates) 
<a name="l8457"><span class="ln">8457 </span></a>    tensor([[9., 1., 3., 5.], 
<a name="l8458"><span class="ln">8458 </span></a>            [8., 6., 6., 0.], 
<a name="l8459"><span class="ln">8459 </span></a>            [0., 4., 5., 3.], 
<a name="l8460"><span class="ln">8460 </span></a>            [2., 1., 4., 2.]]) 
<a name="l8461"><span class="ln">8461 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8462"><span class="ln">8462 </span></a><span class="s3">)</span>
<a name="l8463"><span class="ln">8463 </span></a>
<a name="l8464"><span class="ln">8464 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8465"><span class="ln">8465 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">polygamma</span><span class="s3">,</span>
<a name="l8466"><span class="ln">8466 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8467"><span class="ln">8467 </span></a>polygamma(n, input, *, out=None) -&gt; Tensor 
<a name="l8468"><span class="ln">8468 </span></a> 
<a name="l8469"><span class="ln">8469 </span></a>Alias for :func:`torch.special.polygamma`. 
<a name="l8470"><span class="ln">8470 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l8471"><span class="ln">8471 </span></a><span class="s3">)</span>
<a name="l8472"><span class="ln">8472 </span></a>
<a name="l8473"><span class="ln">8473 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8474"><span class="ln">8474 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">positive</span><span class="s3">,</span>
<a name="l8475"><span class="ln">8475 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8476"><span class="ln">8476 </span></a>positive(input) -&gt; Tensor 
<a name="l8477"><span class="ln">8477 </span></a> 
<a name="l8478"><span class="ln">8478 </span></a>Returns :attr:`input`. 
<a name="l8479"><span class="ln">8479 </span></a>Throws a runtime error if :attr:`input` is a bool tensor. 
<a name="l8480"><span class="ln">8480 </span></a>&quot;&quot;&quot;</span>
<a name="l8481"><span class="ln">8481 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l8482"><span class="ln">8482 </span></a>Args: 
<a name="l8483"><span class="ln">8483 </span></a>    {input} 
<a name="l8484"><span class="ln">8484 </span></a> 
<a name="l8485"><span class="ln">8485 </span></a>Example:: 
<a name="l8486"><span class="ln">8486 </span></a> 
<a name="l8487"><span class="ln">8487 </span></a>    &gt;&gt;&gt; t = torch.randn(5) 
<a name="l8488"><span class="ln">8488 </span></a>    &gt;&gt;&gt; t 
<a name="l8489"><span class="ln">8489 </span></a>    tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940]) 
<a name="l8490"><span class="ln">8490 </span></a>    &gt;&gt;&gt; torch.positive(t) 
<a name="l8491"><span class="ln">8491 </span></a>    tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940]) 
<a name="l8492"><span class="ln">8492 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8493"><span class="ln">8493 </span></a><span class="s3">)</span>
<a name="l8494"><span class="ln">8494 </span></a>
<a name="l8495"><span class="ln">8495 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8496"><span class="ln">8496 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">pow</span><span class="s3">,</span>
<a name="l8497"><span class="ln">8497 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8498"><span class="ln">8498 </span></a>pow(input, exponent, *, out=None) -&gt; Tensor 
<a name="l8499"><span class="ln">8499 </span></a> 
<a name="l8500"><span class="ln">8500 </span></a>Takes the power of each element in :attr:`input` with :attr:`exponent` and 
<a name="l8501"><span class="ln">8501 </span></a>returns a tensor with the result. 
<a name="l8502"><span class="ln">8502 </span></a> 
<a name="l8503"><span class="ln">8503 </span></a>:attr:`exponent` can be either a single ``float`` number or a `Tensor` 
<a name="l8504"><span class="ln">8504 </span></a>with the same number of elements as :attr:`input`. 
<a name="l8505"><span class="ln">8505 </span></a> 
<a name="l8506"><span class="ln">8506 </span></a>When :attr:`exponent` is a scalar value, the operation applied is: 
<a name="l8507"><span class="ln">8507 </span></a> 
<a name="l8508"><span class="ln">8508 </span></a>.. math:: 
<a name="l8509"><span class="ln">8509 </span></a>    \text{out}_i = x_i ^ \text{exponent} 
<a name="l8510"><span class="ln">8510 </span></a> 
<a name="l8511"><span class="ln">8511 </span></a>When :attr:`exponent` is a tensor, the operation applied is: 
<a name="l8512"><span class="ln">8512 </span></a> 
<a name="l8513"><span class="ln">8513 </span></a>.. math:: 
<a name="l8514"><span class="ln">8514 </span></a>    \text{out}_i = x_i ^ {\text{exponent}_i} 
<a name="l8515"><span class="ln">8515 </span></a>&quot;&quot;&quot;</span>
<a name="l8516"><span class="ln">8516 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l8517"><span class="ln">8517 </span></a>When :attr:`exponent` is a tensor, the shapes of :attr:`input` 
<a name="l8518"><span class="ln">8518 </span></a>and :attr:`exponent` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l8519"><span class="ln">8519 </span></a> 
<a name="l8520"><span class="ln">8520 </span></a>Args: 
<a name="l8521"><span class="ln">8521 </span></a>    {input} 
<a name="l8522"><span class="ln">8522 </span></a>    exponent (float or tensor): the exponent value 
<a name="l8523"><span class="ln">8523 </span></a> 
<a name="l8524"><span class="ln">8524 </span></a>Keyword args: 
<a name="l8525"><span class="ln">8525 </span></a>    {out} 
<a name="l8526"><span class="ln">8526 </span></a> 
<a name="l8527"><span class="ln">8527 </span></a>Example:: 
<a name="l8528"><span class="ln">8528 </span></a> 
<a name="l8529"><span class="ln">8529 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l8530"><span class="ln">8530 </span></a>    &gt;&gt;&gt; a 
<a name="l8531"><span class="ln">8531 </span></a>    tensor([ 0.4331,  1.2475,  0.6834, -0.2791]) 
<a name="l8532"><span class="ln">8532 </span></a>    &gt;&gt;&gt; torch.pow(a, 2) 
<a name="l8533"><span class="ln">8533 </span></a>    tensor([ 0.1875,  1.5561,  0.4670,  0.0779]) 
<a name="l8534"><span class="ln">8534 </span></a>    &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l8535"><span class="ln">8535 </span></a> 
<a name="l8536"><span class="ln">8536 </span></a>    &gt;&gt;&gt; a = torch.arange(1., 5.) 
<a name="l8537"><span class="ln">8537 </span></a>    &gt;&gt;&gt; a 
<a name="l8538"><span class="ln">8538 </span></a>    tensor([ 1.,  2.,  3.,  4.]) 
<a name="l8539"><span class="ln">8539 </span></a>    &gt;&gt;&gt; exp 
<a name="l8540"><span class="ln">8540 </span></a>    tensor([ 1.,  2.,  3.,  4.]) 
<a name="l8541"><span class="ln">8541 </span></a>    &gt;&gt;&gt; torch.pow(a, exp) 
<a name="l8542"><span class="ln">8542 </span></a>    tensor([   1.,    4.,   27.,  256.]) 
<a name="l8543"><span class="ln">8543 </span></a> 
<a name="l8544"><span class="ln">8544 </span></a>.. function:: pow(self, exponent, *, out=None) -&gt; Tensor 
<a name="l8545"><span class="ln">8545 </span></a>   :noindex: 
<a name="l8546"><span class="ln">8546 </span></a> 
<a name="l8547"><span class="ln">8547 </span></a>:attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor. 
<a name="l8548"><span class="ln">8548 </span></a>The returned tensor :attr:`out` is of the same shape as :attr:`exponent` 
<a name="l8549"><span class="ln">8549 </span></a> 
<a name="l8550"><span class="ln">8550 </span></a>The operation applied is: 
<a name="l8551"><span class="ln">8551 </span></a> 
<a name="l8552"><span class="ln">8552 </span></a>.. math:: 
<a name="l8553"><span class="ln">8553 </span></a>    \text{{out}}_i = \text{{self}} ^ {{\text{{exponent}}_i}} 
<a name="l8554"><span class="ln">8554 </span></a> 
<a name="l8555"><span class="ln">8555 </span></a>Args: 
<a name="l8556"><span class="ln">8556 </span></a>    self (float): the scalar base value for the power operation 
<a name="l8557"><span class="ln">8557 </span></a>    exponent (Tensor): the exponent tensor 
<a name="l8558"><span class="ln">8558 </span></a> 
<a name="l8559"><span class="ln">8559 </span></a>Keyword args: 
<a name="l8560"><span class="ln">8560 </span></a>    {out} 
<a name="l8561"><span class="ln">8561 </span></a> 
<a name="l8562"><span class="ln">8562 </span></a>Example:: 
<a name="l8563"><span class="ln">8563 </span></a> 
<a name="l8564"><span class="ln">8564 </span></a>    &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l8565"><span class="ln">8565 </span></a>    &gt;&gt;&gt; base = 2 
<a name="l8566"><span class="ln">8566 </span></a>    &gt;&gt;&gt; torch.pow(base, exp) 
<a name="l8567"><span class="ln">8567 </span></a>    tensor([  2.,   4.,   8.,  16.]) 
<a name="l8568"><span class="ln">8568 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8569"><span class="ln">8569 </span></a><span class="s3">)</span>
<a name="l8570"><span class="ln">8570 </span></a>
<a name="l8571"><span class="ln">8571 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8572"><span class="ln">8572 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">float_power</span><span class="s3">,</span>
<a name="l8573"><span class="ln">8573 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8574"><span class="ln">8574 </span></a>float_power(input, exponent, *, out=None) -&gt; Tensor 
<a name="l8575"><span class="ln">8575 </span></a> 
<a name="l8576"><span class="ln">8576 </span></a>Raises :attr:`input` to the power of :attr:`exponent`, elementwise, in double precision. 
<a name="l8577"><span class="ln">8577 </span></a>If neither input is complex returns a ``torch.float64`` tensor, 
<a name="l8578"><span class="ln">8578 </span></a>and if one or more inputs is complex returns a ``torch.complex128`` tensor. 
<a name="l8579"><span class="ln">8579 </span></a> 
<a name="l8580"><span class="ln">8580 </span></a>.. note:: 
<a name="l8581"><span class="ln">8581 </span></a>    This function always computes in double precision, unlike :func:`torch.pow`, 
<a name="l8582"><span class="ln">8582 </span></a>    which implements more typical :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l8583"><span class="ln">8583 </span></a>    This is useful when the computation needs to be performed in a wider or more precise dtype, 
<a name="l8584"><span class="ln">8584 </span></a>    or the results of the computation may contain fractional values not representable in the input dtypes, 
<a name="l8585"><span class="ln">8585 </span></a>    like when an integer base is raised to a negative integer exponent. 
<a name="l8586"><span class="ln">8586 </span></a> 
<a name="l8587"><span class="ln">8587 </span></a>Args: 
<a name="l8588"><span class="ln">8588 </span></a>    input (Tensor or Number): the base value(s) 
<a name="l8589"><span class="ln">8589 </span></a>    exponent (Tensor or Number): the exponent value(s) 
<a name="l8590"><span class="ln">8590 </span></a> 
<a name="l8591"><span class="ln">8591 </span></a>Keyword args: 
<a name="l8592"><span class="ln">8592 </span></a>    {out} 
<a name="l8593"><span class="ln">8593 </span></a> 
<a name="l8594"><span class="ln">8594 </span></a>Example:: 
<a name="l8595"><span class="ln">8595 </span></a> 
<a name="l8596"><span class="ln">8596 </span></a>    &gt;&gt;&gt; a = torch.randint(10, (4,)) 
<a name="l8597"><span class="ln">8597 </span></a>    &gt;&gt;&gt; a 
<a name="l8598"><span class="ln">8598 </span></a>    tensor([6, 4, 7, 1]) 
<a name="l8599"><span class="ln">8599 </span></a>    &gt;&gt;&gt; torch.float_power(a, 2) 
<a name="l8600"><span class="ln">8600 </span></a>    tensor([36., 16., 49.,  1.], dtype=torch.float64) 
<a name="l8601"><span class="ln">8601 </span></a> 
<a name="l8602"><span class="ln">8602 </span></a>    &gt;&gt;&gt; a = torch.arange(1, 5) 
<a name="l8603"><span class="ln">8603 </span></a>    &gt;&gt;&gt; a 
<a name="l8604"><span class="ln">8604 </span></a>    tensor([ 1,  2,  3,  4]) 
<a name="l8605"><span class="ln">8605 </span></a>    &gt;&gt;&gt; exp = torch.tensor([2, -3, 4, -5]) 
<a name="l8606"><span class="ln">8606 </span></a>    &gt;&gt;&gt; exp 
<a name="l8607"><span class="ln">8607 </span></a>    tensor([ 2, -3,  4, -5]) 
<a name="l8608"><span class="ln">8608 </span></a>    &gt;&gt;&gt; torch.float_power(a, exp) 
<a name="l8609"><span class="ln">8609 </span></a>    tensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64) 
<a name="l8610"><span class="ln">8610 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8611"><span class="ln">8611 </span></a><span class="s3">)</span>
<a name="l8612"><span class="ln">8612 </span></a>
<a name="l8613"><span class="ln">8613 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8614"><span class="ln">8614 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">prod</span><span class="s3">,</span>
<a name="l8615"><span class="ln">8615 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8616"><span class="ln">8616 </span></a>prod(input: Tensor, *, dtype: Optional[_dtype]) -&gt; Tensor 
<a name="l8617"><span class="ln">8617 </span></a> 
<a name="l8618"><span class="ln">8618 </span></a>Returns the product of all elements in the :attr:`input` tensor. 
<a name="l8619"><span class="ln">8619 </span></a> 
<a name="l8620"><span class="ln">8620 </span></a>Args: 
<a name="l8621"><span class="ln">8621 </span></a>    {input} 
<a name="l8622"><span class="ln">8622 </span></a> 
<a name="l8623"><span class="ln">8623 </span></a>Keyword args: 
<a name="l8624"><span class="ln">8624 </span></a>    {dtype} 
<a name="l8625"><span class="ln">8625 </span></a> 
<a name="l8626"><span class="ln">8626 </span></a>Example:: 
<a name="l8627"><span class="ln">8627 </span></a> 
<a name="l8628"><span class="ln">8628 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l8629"><span class="ln">8629 </span></a>    &gt;&gt;&gt; a 
<a name="l8630"><span class="ln">8630 </span></a>    tensor([[-0.8020,  0.5428, -1.5854]]) 
<a name="l8631"><span class="ln">8631 </span></a>    &gt;&gt;&gt; torch.prod(a) 
<a name="l8632"><span class="ln">8632 </span></a>    tensor(0.6902) 
<a name="l8633"><span class="ln">8633 </span></a> 
<a name="l8634"><span class="ln">8634 </span></a>.. function:: prod(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l8635"><span class="ln">8635 </span></a>   :noindex: 
<a name="l8636"><span class="ln">8636 </span></a> 
<a name="l8637"><span class="ln">8637 </span></a>Returns the product of each row of the :attr:`input` tensor in the given 
<a name="l8638"><span class="ln">8638 </span></a>dimension :attr:`dim`. 
<a name="l8639"><span class="ln">8639 </span></a> 
<a name="l8640"><span class="ln">8640 </span></a>{keepdim_details} 
<a name="l8641"><span class="ln">8641 </span></a> 
<a name="l8642"><span class="ln">8642 </span></a>Args: 
<a name="l8643"><span class="ln">8643 </span></a>    {input} 
<a name="l8644"><span class="ln">8644 </span></a>    {opt_dim_all_reduce} 
<a name="l8645"><span class="ln">8645 </span></a>    {opt_keepdim} 
<a name="l8646"><span class="ln">8646 </span></a> 
<a name="l8647"><span class="ln">8647 </span></a>Keyword args: 
<a name="l8648"><span class="ln">8648 </span></a>    {dtype} 
<a name="l8649"><span class="ln">8649 </span></a> 
<a name="l8650"><span class="ln">8650 </span></a>Example:: 
<a name="l8651"><span class="ln">8651 </span></a> 
<a name="l8652"><span class="ln">8652 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 2) 
<a name="l8653"><span class="ln">8653 </span></a>    &gt;&gt;&gt; a 
<a name="l8654"><span class="ln">8654 </span></a>    tensor([[ 0.5261, -0.3837], 
<a name="l8655"><span class="ln">8655 </span></a>            [ 1.1857, -0.2498], 
<a name="l8656"><span class="ln">8656 </span></a>            [-1.1646,  0.0705], 
<a name="l8657"><span class="ln">8657 </span></a>            [ 1.1131, -1.0629]]) 
<a name="l8658"><span class="ln">8658 </span></a>    &gt;&gt;&gt; torch.prod(a, 1) 
<a name="l8659"><span class="ln">8659 </span></a>    tensor([-0.2018, -0.2962, -0.0821, -1.1831]) 
<a name="l8660"><span class="ln">8660 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">single_dim_common</span><span class="s3">),</span>
<a name="l8661"><span class="ln">8661 </span></a><span class="s3">)</span>
<a name="l8662"><span class="ln">8662 </span></a>
<a name="l8663"><span class="ln">8663 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8664"><span class="ln">8664 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">promote_types</span><span class="s3">,</span>
<a name="l8665"><span class="ln">8665 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8666"><span class="ln">8666 </span></a>promote_types(type1, type2) -&gt; dtype 
<a name="l8667"><span class="ln">8667 </span></a> 
<a name="l8668"><span class="ln">8668 </span></a>Returns the :class:`torch.dtype` with the smallest size and scalar kind that is 
<a name="l8669"><span class="ln">8669 </span></a>not smaller nor of lower kind than either `type1` or `type2`. See type promotion 
<a name="l8670"><span class="ln">8670 </span></a>:ref:`documentation &lt;type-promotion-doc&gt;` for more information on the type 
<a name="l8671"><span class="ln">8671 </span></a>promotion logic. 
<a name="l8672"><span class="ln">8672 </span></a> 
<a name="l8673"><span class="ln">8673 </span></a>Args: 
<a name="l8674"><span class="ln">8674 </span></a>    type1 (:class:`torch.dtype`) 
<a name="l8675"><span class="ln">8675 </span></a>    type2 (:class:`torch.dtype`) 
<a name="l8676"><span class="ln">8676 </span></a> 
<a name="l8677"><span class="ln">8677 </span></a>Example:: 
<a name="l8678"><span class="ln">8678 </span></a> 
<a name="l8679"><span class="ln">8679 </span></a>    &gt;&gt;&gt; torch.promote_types(torch.int32, torch.float32) 
<a name="l8680"><span class="ln">8680 </span></a>    torch.float32 
<a name="l8681"><span class="ln">8681 </span></a>    &gt;&gt;&gt; torch.promote_types(torch.uint8, torch.long) 
<a name="l8682"><span class="ln">8682 </span></a>    torch.long 
<a name="l8683"><span class="ln">8683 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l8684"><span class="ln">8684 </span></a><span class="s3">)</span>
<a name="l8685"><span class="ln">8685 </span></a>
<a name="l8686"><span class="ln">8686 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8687"><span class="ln">8687 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">qr</span><span class="s3">,</span>
<a name="l8688"><span class="ln">8688 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8689"><span class="ln">8689 </span></a>qr(input: Tensor, some: bool = True, *, out: Union[Tensor, Tuple[Tensor, ...], List[Tensor], None]) -&gt; (Tensor, Tensor) 
<a name="l8690"><span class="ln">8690 </span></a> 
<a name="l8691"><span class="ln">8691 </span></a>Computes the QR decomposition of a matrix or a batch of matrices :attr:`input`, 
<a name="l8692"><span class="ln">8692 </span></a>and returns a namedtuple (Q, R) of tensors such that :math:`\text{input} = Q R` 
<a name="l8693"><span class="ln">8693 </span></a>with :math:`Q` being an orthogonal matrix or batch of orthogonal matrices and 
<a name="l8694"><span class="ln">8694 </span></a>:math:`R` being an upper triangular matrix or batch of upper triangular matrices. 
<a name="l8695"><span class="ln">8695 </span></a> 
<a name="l8696"><span class="ln">8696 </span></a>If :attr:`some` is ``True``, then this function returns the thin (reduced) QR factorization. 
<a name="l8697"><span class="ln">8697 </span></a>Otherwise, if :attr:`some` is ``False``, this function returns the complete QR factorization. 
<a name="l8698"><span class="ln">8698 </span></a> 
<a name="l8699"><span class="ln">8699 </span></a>.. warning:: 
<a name="l8700"><span class="ln">8700 </span></a> 
<a name="l8701"><span class="ln">8701 </span></a>    :func:`torch.qr` is deprecated in favor of :func:`torch.linalg.qr` 
<a name="l8702"><span class="ln">8702 </span></a>    and will be removed in a future PyTorch release. The boolean parameter :attr:`some` has been 
<a name="l8703"><span class="ln">8703 </span></a>    replaced with a string parameter :attr:`mode`. 
<a name="l8704"><span class="ln">8704 </span></a> 
<a name="l8705"><span class="ln">8705 </span></a>    ``Q, R = torch.qr(A)`` should be replaced with 
<a name="l8706"><span class="ln">8706 </span></a> 
<a name="l8707"><span class="ln">8707 </span></a>    .. code:: python 
<a name="l8708"><span class="ln">8708 </span></a> 
<a name="l8709"><span class="ln">8709 </span></a>        Q, R = torch.linalg.qr(A) 
<a name="l8710"><span class="ln">8710 </span></a> 
<a name="l8711"><span class="ln">8711 </span></a>    ``Q, R = torch.qr(A, some=False)`` should be replaced with 
<a name="l8712"><span class="ln">8712 </span></a> 
<a name="l8713"><span class="ln">8713 </span></a>    .. code:: python 
<a name="l8714"><span class="ln">8714 </span></a> 
<a name="l8715"><span class="ln">8715 </span></a>        Q, R = torch.linalg.qr(A, mode=&quot;complete&quot;) 
<a name="l8716"><span class="ln">8716 </span></a> 
<a name="l8717"><span class="ln">8717 </span></a>.. warning:: 
<a name="l8718"><span class="ln">8718 </span></a>          If you plan to backpropagate through QR, note that the current backward implementation 
<a name="l8719"><span class="ln">8719 </span></a>          is only well-defined when the first :math:`\min(input.size(-1), input.size(-2))` 
<a name="l8720"><span class="ln">8720 </span></a>          columns of :attr:`input` are linearly independent. 
<a name="l8721"><span class="ln">8721 </span></a>          This behavior will probably change once QR supports pivoting. 
<a name="l8722"><span class="ln">8722 </span></a> 
<a name="l8723"><span class="ln">8723 </span></a>.. note:: This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, 
<a name="l8724"><span class="ln">8724 </span></a>          and may produce different (valid) decompositions on different device types 
<a name="l8725"><span class="ln">8725 </span></a>          or different platforms. 
<a name="l8726"><span class="ln">8726 </span></a> 
<a name="l8727"><span class="ln">8727 </span></a>Args: 
<a name="l8728"><span class="ln">8728 </span></a>    input (Tensor): the input tensor of size :math:`(*, m, n)` where `*` is zero or more 
<a name="l8729"><span class="ln">8729 </span></a>                batch dimensions consisting of matrices of dimension :math:`m \times n`. 
<a name="l8730"><span class="ln">8730 </span></a>    some (bool, optional): Set to ``True`` for reduced QR decomposition and ``False`` for 
<a name="l8731"><span class="ln">8731 </span></a>                complete QR decomposition. If `k = min(m, n)` then: 
<a name="l8732"><span class="ln">8732 </span></a> 
<a name="l8733"><span class="ln">8733 </span></a>                  * ``some=True`` : returns `(Q, R)` with dimensions (m, k), (k, n) (default) 
<a name="l8734"><span class="ln">8734 </span></a> 
<a name="l8735"><span class="ln">8735 </span></a>                  * ``'some=False'``: returns `(Q, R)` with dimensions (m, m), (m, n) 
<a name="l8736"><span class="ln">8736 </span></a> 
<a name="l8737"><span class="ln">8737 </span></a>Keyword args: 
<a name="l8738"><span class="ln">8738 </span></a>    out (tuple, optional): tuple of `Q` and `R` tensors. 
<a name="l8739"><span class="ln">8739 </span></a>                The dimensions of `Q` and `R` are detailed in the description of :attr:`some` above. 
<a name="l8740"><span class="ln">8740 </span></a> 
<a name="l8741"><span class="ln">8741 </span></a>Example:: 
<a name="l8742"><span class="ln">8742 </span></a> 
<a name="l8743"><span class="ln">8743 </span></a>    &gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]]) 
<a name="l8744"><span class="ln">8744 </span></a>    &gt;&gt;&gt; q, r = torch.qr(a) 
<a name="l8745"><span class="ln">8745 </span></a>    &gt;&gt;&gt; q 
<a name="l8746"><span class="ln">8746 </span></a>    tensor([[-0.8571,  0.3943,  0.3314], 
<a name="l8747"><span class="ln">8747 </span></a>            [-0.4286, -0.9029, -0.0343], 
<a name="l8748"><span class="ln">8748 </span></a>            [ 0.2857, -0.1714,  0.9429]]) 
<a name="l8749"><span class="ln">8749 </span></a>    &gt;&gt;&gt; r 
<a name="l8750"><span class="ln">8750 </span></a>    tensor([[ -14.0000,  -21.0000,   14.0000], 
<a name="l8751"><span class="ln">8751 </span></a>            [   0.0000, -175.0000,   70.0000], 
<a name="l8752"><span class="ln">8752 </span></a>            [   0.0000,    0.0000,  -35.0000]]) 
<a name="l8753"><span class="ln">8753 </span></a>    &gt;&gt;&gt; torch.mm(q, r).round() 
<a name="l8754"><span class="ln">8754 </span></a>    tensor([[  12.,  -51.,    4.], 
<a name="l8755"><span class="ln">8755 </span></a>            [   6.,  167.,  -68.], 
<a name="l8756"><span class="ln">8756 </span></a>            [  -4.,   24.,  -41.]]) 
<a name="l8757"><span class="ln">8757 </span></a>    &gt;&gt;&gt; torch.mm(q.t(), q).round() 
<a name="l8758"><span class="ln">8758 </span></a>    tensor([[ 1.,  0.,  0.], 
<a name="l8759"><span class="ln">8759 </span></a>            [ 0.,  1., -0.], 
<a name="l8760"><span class="ln">8760 </span></a>            [ 0., -0.,  1.]]) 
<a name="l8761"><span class="ln">8761 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 4, 5) 
<a name="l8762"><span class="ln">8762 </span></a>    &gt;&gt;&gt; q, r = torch.qr(a, some=False) 
<a name="l8763"><span class="ln">8763 </span></a>    &gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a) 
<a name="l8764"><span class="ln">8764 </span></a>    True 
<a name="l8765"><span class="ln">8765 </span></a>    &gt;&gt;&gt; torch.allclose(torch.matmul(q.mT, q), torch.eye(5)) 
<a name="l8766"><span class="ln">8766 </span></a>    True 
<a name="l8767"><span class="ln">8767 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l8768"><span class="ln">8768 </span></a><span class="s3">)</span>
<a name="l8769"><span class="ln">8769 </span></a>
<a name="l8770"><span class="ln">8770 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8771"><span class="ln">8771 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">rad2deg</span><span class="s3">,</span>
<a name="l8772"><span class="ln">8772 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8773"><span class="ln">8773 </span></a>rad2deg(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l8774"><span class="ln">8774 </span></a> 
<a name="l8775"><span class="ln">8775 </span></a>Returns a new tensor with each of the elements of :attr:`input` 
<a name="l8776"><span class="ln">8776 </span></a>converted from angles in radians to degrees. 
<a name="l8777"><span class="ln">8777 </span></a> 
<a name="l8778"><span class="ln">8778 </span></a>Args: 
<a name="l8779"><span class="ln">8779 </span></a>    {input} 
<a name="l8780"><span class="ln">8780 </span></a> 
<a name="l8781"><span class="ln">8781 </span></a>Keyword arguments: 
<a name="l8782"><span class="ln">8782 </span></a>    {out} 
<a name="l8783"><span class="ln">8783 </span></a> 
<a name="l8784"><span class="ln">8784 </span></a>Example:: 
<a name="l8785"><span class="ln">8785 </span></a> 
<a name="l8786"><span class="ln">8786 </span></a>    &gt;&gt;&gt; a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]]) 
<a name="l8787"><span class="ln">8787 </span></a>    &gt;&gt;&gt; torch.rad2deg(a) 
<a name="l8788"><span class="ln">8788 </span></a>    tensor([[ 180.0233, -180.0233], 
<a name="l8789"><span class="ln">8789 </span></a>            [ 359.9894, -359.9894], 
<a name="l8790"><span class="ln">8790 </span></a>            [  89.9544,  -89.9544]]) 
<a name="l8791"><span class="ln">8791 </span></a> 
<a name="l8792"><span class="ln">8792 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8793"><span class="ln">8793 </span></a><span class="s3">)</span>
<a name="l8794"><span class="ln">8794 </span></a>
<a name="l8795"><span class="ln">8795 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8796"><span class="ln">8796 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">deg2rad</span><span class="s3">,</span>
<a name="l8797"><span class="ln">8797 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8798"><span class="ln">8798 </span></a>deg2rad(input, *, out=None) -&gt; Tensor 
<a name="l8799"><span class="ln">8799 </span></a> 
<a name="l8800"><span class="ln">8800 </span></a>Returns a new tensor with each of the elements of :attr:`input` 
<a name="l8801"><span class="ln">8801 </span></a>converted from angles in degrees to radians. 
<a name="l8802"><span class="ln">8802 </span></a> 
<a name="l8803"><span class="ln">8803 </span></a>Args: 
<a name="l8804"><span class="ln">8804 </span></a>    {input} 
<a name="l8805"><span class="ln">8805 </span></a> 
<a name="l8806"><span class="ln">8806 </span></a>Keyword arguments: 
<a name="l8807"><span class="ln">8807 </span></a>    {out} 
<a name="l8808"><span class="ln">8808 </span></a> 
<a name="l8809"><span class="ln">8809 </span></a>Example:: 
<a name="l8810"><span class="ln">8810 </span></a> 
<a name="l8811"><span class="ln">8811 </span></a>    &gt;&gt;&gt; a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]]) 
<a name="l8812"><span class="ln">8812 </span></a>    &gt;&gt;&gt; torch.deg2rad(a) 
<a name="l8813"><span class="ln">8813 </span></a>    tensor([[ 3.1416, -3.1416], 
<a name="l8814"><span class="ln">8814 </span></a>            [ 6.2832, -6.2832], 
<a name="l8815"><span class="ln">8815 </span></a>            [ 1.5708, -1.5708]]) 
<a name="l8816"><span class="ln">8816 </span></a> 
<a name="l8817"><span class="ln">8817 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8818"><span class="ln">8818 </span></a><span class="s3">)</span>
<a name="l8819"><span class="ln">8819 </span></a>
<a name="l8820"><span class="ln">8820 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8821"><span class="ln">8821 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">heaviside</span><span class="s3">,</span>
<a name="l8822"><span class="ln">8822 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8823"><span class="ln">8823 </span></a>heaviside(input, values, *, out=None) -&gt; Tensor 
<a name="l8824"><span class="ln">8824 </span></a> 
<a name="l8825"><span class="ln">8825 </span></a>Computes the Heaviside step function for each element in :attr:`input`. 
<a name="l8826"><span class="ln">8826 </span></a>The Heaviside step function is defined as: 
<a name="l8827"><span class="ln">8827 </span></a> 
<a name="l8828"><span class="ln">8828 </span></a>.. math:: 
<a name="l8829"><span class="ln">8829 </span></a>    \text{{heaviside}}(input, values) = \begin{cases} 
<a name="l8830"><span class="ln">8830 </span></a>        0, &amp; \text{if input &lt; 0}\\ 
<a name="l8831"><span class="ln">8831 </span></a>        values, &amp; \text{if input == 0}\\ 
<a name="l8832"><span class="ln">8832 </span></a>        1, &amp; \text{if input &gt; 0} 
<a name="l8833"><span class="ln">8833 </span></a>    \end{cases} 
<a name="l8834"><span class="ln">8834 </span></a>&quot;&quot;&quot;</span>
<a name="l8835"><span class="ln">8835 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l8836"><span class="ln">8836 </span></a> 
<a name="l8837"><span class="ln">8837 </span></a>Args: 
<a name="l8838"><span class="ln">8838 </span></a>    {input} 
<a name="l8839"><span class="ln">8839 </span></a>    values (Tensor): The values to use where :attr:`input` is zero. 
<a name="l8840"><span class="ln">8840 </span></a> 
<a name="l8841"><span class="ln">8841 </span></a>Keyword arguments: 
<a name="l8842"><span class="ln">8842 </span></a>    {out} 
<a name="l8843"><span class="ln">8843 </span></a> 
<a name="l8844"><span class="ln">8844 </span></a>Example:: 
<a name="l8845"><span class="ln">8845 </span></a> 
<a name="l8846"><span class="ln">8846 </span></a>    &gt;&gt;&gt; input = torch.tensor([-1.5, 0, 2.0]) 
<a name="l8847"><span class="ln">8847 </span></a>    &gt;&gt;&gt; values = torch.tensor([0.5]) 
<a name="l8848"><span class="ln">8848 </span></a>    &gt;&gt;&gt; torch.heaviside(input, values) 
<a name="l8849"><span class="ln">8849 </span></a>    tensor([0.0000, 0.5000, 1.0000]) 
<a name="l8850"><span class="ln">8850 </span></a>    &gt;&gt;&gt; values = torch.tensor([1.2, -2.0, 3.5]) 
<a name="l8851"><span class="ln">8851 </span></a>    &gt;&gt;&gt; torch.heaviside(input, values) 
<a name="l8852"><span class="ln">8852 </span></a>    tensor([0., -2., 1.]) 
<a name="l8853"><span class="ln">8853 </span></a> 
<a name="l8854"><span class="ln">8854 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l8855"><span class="ln">8855 </span></a><span class="s3">)</span>
<a name="l8856"><span class="ln">8856 </span></a>
<a name="l8857"><span class="ln">8857 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8858"><span class="ln">8858 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">rand</span><span class="s3">,</span>
<a name="l8859"><span class="ln">8859 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l8860"><span class="ln">8860 </span></a>rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, \ 
<a name="l8861"><span class="ln">8861 </span></a>requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l8862"><span class="ln">8862 </span></a>&quot;&quot;&quot;</span>
<a name="l8863"><span class="ln">8863 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l8864"><span class="ln">8864 </span></a>Returns a tensor filled with random numbers from a uniform distribution 
<a name="l8865"><span class="ln">8865 </span></a>on the interval :math:`[0, 1)` 
<a name="l8866"><span class="ln">8866 </span></a> 
<a name="l8867"><span class="ln">8867 </span></a>The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l8868"><span class="ln">8868 </span></a> 
<a name="l8869"><span class="ln">8869 </span></a>Args: 
<a name="l8870"><span class="ln">8870 </span></a>    size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l8871"><span class="ln">8871 </span></a>        Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l8872"><span class="ln">8872 </span></a> 
<a name="l8873"><span class="ln">8873 </span></a>Keyword args: 
<a name="l8874"><span class="ln">8874 </span></a>    {generator} 
<a name="l8875"><span class="ln">8875 </span></a>    {out} 
<a name="l8876"><span class="ln">8876 </span></a>    {dtype} 
<a name="l8877"><span class="ln">8877 </span></a>    {layout} 
<a name="l8878"><span class="ln">8878 </span></a>    {device} 
<a name="l8879"><span class="ln">8879 </span></a>    {requires_grad} 
<a name="l8880"><span class="ln">8880 </span></a>    {pin_memory} 
<a name="l8881"><span class="ln">8881 </span></a> 
<a name="l8882"><span class="ln">8882 </span></a>Example:: 
<a name="l8883"><span class="ln">8883 </span></a> 
<a name="l8884"><span class="ln">8884 </span></a>    &gt;&gt;&gt; torch.rand(4) 
<a name="l8885"><span class="ln">8885 </span></a>    tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l8886"><span class="ln">8886 </span></a>    &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l8887"><span class="ln">8887 </span></a>    tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l8888"><span class="ln">8888 </span></a>            [ 0.3816,  0.7249,  0.0998]]) 
<a name="l8889"><span class="ln">8889 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l8890"><span class="ln">8890 </span></a><span class="s3">)</span>
<a name="l8891"><span class="ln">8891 </span></a>
<a name="l8892"><span class="ln">8892 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8893"><span class="ln">8893 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">rand_like</span><span class="s3">,</span>
<a name="l8894"><span class="ln">8894 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l8895"><span class="ln">8895 </span></a>rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l8896"><span class="ln">8896 </span></a> 
<a name="l8897"><span class="ln">8897 </span></a>Returns a tensor with the same size as :attr:`input` that is filled with 
<a name="l8898"><span class="ln">8898 </span></a>random numbers from a uniform distribution on the interval :math:`[0, 1)`. 
<a name="l8899"><span class="ln">8899 </span></a>``torch.rand_like(input)`` is equivalent to 
<a name="l8900"><span class="ln">8900 </span></a>``torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l8901"><span class="ln">8901 </span></a> 
<a name="l8902"><span class="ln">8902 </span></a>Args: 
<a name="l8903"><span class="ln">8903 </span></a>    {input} 
<a name="l8904"><span class="ln">8904 </span></a> 
<a name="l8905"><span class="ln">8905 </span></a>Keyword args: 
<a name="l8906"><span class="ln">8906 </span></a>    {dtype} 
<a name="l8907"><span class="ln">8907 </span></a>    {layout} 
<a name="l8908"><span class="ln">8908 </span></a>    {device} 
<a name="l8909"><span class="ln">8909 </span></a>    {requires_grad} 
<a name="l8910"><span class="ln">8910 </span></a>    {memory_format} 
<a name="l8911"><span class="ln">8911 </span></a> 
<a name="l8912"><span class="ln">8912 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_like_common_args</span><span class="s3">),</span>
<a name="l8913"><span class="ln">8913 </span></a><span class="s3">)</span>
<a name="l8914"><span class="ln">8914 </span></a>
<a name="l8915"><span class="ln">8915 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8916"><span class="ln">8916 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">randint</span><span class="s3">,</span>
<a name="l8917"><span class="ln">8917 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l8918"><span class="ln">8918 </span></a>randint(low=0, high, size, \\*, generator=None, out=None, \ 
<a name="l8919"><span class="ln">8919 </span></a>dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l8920"><span class="ln">8920 </span></a> 
<a name="l8921"><span class="ln">8921 </span></a>Returns a tensor filled with random integers generated uniformly 
<a name="l8922"><span class="ln">8922 </span></a>between :attr:`low` (inclusive) and :attr:`high` (exclusive). 
<a name="l8923"><span class="ln">8923 </span></a> 
<a name="l8924"><span class="ln">8924 </span></a>The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l8925"><span class="ln">8925 </span></a> 
<a name="l8926"><span class="ln">8926 </span></a>.. note:: 
<a name="l8927"><span class="ln">8927 </span></a>    With the global dtype default (``torch.float32``), this function returns 
<a name="l8928"><span class="ln">8928 </span></a>    a tensor with dtype ``torch.int64``. 
<a name="l8929"><span class="ln">8929 </span></a> 
<a name="l8930"><span class="ln">8930 </span></a>Args: 
<a name="l8931"><span class="ln">8931 </span></a>    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l8932"><span class="ln">8932 </span></a>    high (int): One above the highest integer to be drawn from the distribution. 
<a name="l8933"><span class="ln">8933 </span></a>    size (tuple): a tuple defining the shape of the output tensor. 
<a name="l8934"><span class="ln">8934 </span></a> 
<a name="l8935"><span class="ln">8935 </span></a>Keyword args: 
<a name="l8936"><span class="ln">8936 </span></a>    {generator} 
<a name="l8937"><span class="ln">8937 </span></a>    {out} 
<a name="l8938"><span class="ln">8938 </span></a>    dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``, 
<a name="l8939"><span class="ln">8939 </span></a>        this function returns a tensor with dtype ``torch.int64``. 
<a name="l8940"><span class="ln">8940 </span></a>    {layout} 
<a name="l8941"><span class="ln">8941 </span></a>    {device} 
<a name="l8942"><span class="ln">8942 </span></a>    {requires_grad} 
<a name="l8943"><span class="ln">8943 </span></a> 
<a name="l8944"><span class="ln">8944 </span></a>Example:: 
<a name="l8945"><span class="ln">8945 </span></a> 
<a name="l8946"><span class="ln">8946 </span></a>    &gt;&gt;&gt; torch.randint(3, 5, (3,)) 
<a name="l8947"><span class="ln">8947 </span></a>    tensor([4, 3, 4]) 
<a name="l8948"><span class="ln">8948 </span></a> 
<a name="l8949"><span class="ln">8949 </span></a> 
<a name="l8950"><span class="ln">8950 </span></a>    &gt;&gt;&gt; torch.randint(10, (2, 2)) 
<a name="l8951"><span class="ln">8951 </span></a>    tensor([[0, 2], 
<a name="l8952"><span class="ln">8952 </span></a>            [5, 5]]) 
<a name="l8953"><span class="ln">8953 </span></a> 
<a name="l8954"><span class="ln">8954 </span></a> 
<a name="l8955"><span class="ln">8955 </span></a>    &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) 
<a name="l8956"><span class="ln">8956 </span></a>    tensor([[4, 5], 
<a name="l8957"><span class="ln">8957 </span></a>            [6, 7]]) 
<a name="l8958"><span class="ln">8958 </span></a> 
<a name="l8959"><span class="ln">8959 </span></a> 
<a name="l8960"><span class="ln">8960 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l8961"><span class="ln">8961 </span></a><span class="s3">)</span>
<a name="l8962"><span class="ln">8962 </span></a>
<a name="l8963"><span class="ln">8963 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8964"><span class="ln">8964 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">randint_like</span><span class="s3">,</span>
<a name="l8965"><span class="ln">8965 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l8966"><span class="ln">8966 </span></a>randint_like(input, low=0, high, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \ 
<a name="l8967"><span class="ln">8967 </span></a>memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l8968"><span class="ln">8968 </span></a> 
<a name="l8969"><span class="ln">8969 </span></a>Returns a tensor with the same shape as Tensor :attr:`input` filled with 
<a name="l8970"><span class="ln">8970 </span></a>random integers generated uniformly between :attr:`low` (inclusive) and 
<a name="l8971"><span class="ln">8971 </span></a>:attr:`high` (exclusive). 
<a name="l8972"><span class="ln">8972 </span></a> 
<a name="l8973"><span class="ln">8973 </span></a>.. note: 
<a name="l8974"><span class="ln">8974 </span></a>    With the global dtype default (``torch.float32``), this function returns 
<a name="l8975"><span class="ln">8975 </span></a>    a tensor with dtype ``torch.int64``. 
<a name="l8976"><span class="ln">8976 </span></a> 
<a name="l8977"><span class="ln">8977 </span></a>Args: 
<a name="l8978"><span class="ln">8978 </span></a>    {input} 
<a name="l8979"><span class="ln">8979 </span></a>    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l8980"><span class="ln">8980 </span></a>    high (int): One above the highest integer to be drawn from the distribution. 
<a name="l8981"><span class="ln">8981 </span></a> 
<a name="l8982"><span class="ln">8982 </span></a>Keyword args: 
<a name="l8983"><span class="ln">8983 </span></a>    {dtype} 
<a name="l8984"><span class="ln">8984 </span></a>    {layout} 
<a name="l8985"><span class="ln">8985 </span></a>    {device} 
<a name="l8986"><span class="ln">8986 </span></a>    {requires_grad} 
<a name="l8987"><span class="ln">8987 </span></a>    {memory_format} 
<a name="l8988"><span class="ln">8988 </span></a> 
<a name="l8989"><span class="ln">8989 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_like_common_args</span><span class="s3">),</span>
<a name="l8990"><span class="ln">8990 </span></a><span class="s3">)</span>
<a name="l8991"><span class="ln">8991 </span></a>
<a name="l8992"><span class="ln">8992 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l8993"><span class="ln">8993 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">randn</span><span class="s3">,</span>
<a name="l8994"><span class="ln">8994 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l8995"><span class="ln">8995 </span></a>randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, \ 
<a name="l8996"><span class="ln">8996 </span></a>pin_memory=False) -&gt; Tensor 
<a name="l8997"><span class="ln">8997 </span></a>&quot;&quot;&quot;</span>
<a name="l8998"><span class="ln">8998 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l8999"><span class="ln">8999 </span></a> 
<a name="l9000"><span class="ln">9000 </span></a>Returns a tensor filled with random numbers from a normal distribution 
<a name="l9001"><span class="ln">9001 </span></a>with mean `0` and variance `1` (also called the standard normal 
<a name="l9002"><span class="ln">9002 </span></a>distribution). 
<a name="l9003"><span class="ln">9003 </span></a> 
<a name="l9004"><span class="ln">9004 </span></a>.. math:: 
<a name="l9005"><span class="ln">9005 </span></a>    \text{{out}}_{{i}} \sim \mathcal{{N}}(0, 1) 
<a name="l9006"><span class="ln">9006 </span></a> 
<a name="l9007"><span class="ln">9007 </span></a>For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l9008"><span class="ln">9008 </span></a>unit variance as 
<a name="l9009"><span class="ln">9009 </span></a> 
<a name="l9010"><span class="ln">9010 </span></a>.. math:: 
<a name="l9011"><span class="ln">9011 </span></a>    \text{{out}}_{{i}} \sim \mathcal{{CN}}(0, 1) 
<a name="l9012"><span class="ln">9012 </span></a> 
<a name="l9013"><span class="ln">9013 </span></a>This is equivalent to separately sampling the real :math:`(\operatorname{{Re}})` and imaginary 
<a name="l9014"><span class="ln">9014 </span></a>:math:`(\operatorname{{Im}})` part of :math:`\text{{out}}_i` as 
<a name="l9015"><span class="ln">9015 </span></a> 
<a name="l9016"><span class="ln">9016 </span></a>.. math:: 
<a name="l9017"><span class="ln">9017 </span></a>    \operatorname{{Re}}(\text{{out}}_{{i}}) \sim \mathcal{{N}}(0, \frac{{1}}{{2}}),\quad 
<a name="l9018"><span class="ln">9018 </span></a>    \operatorname{{Im}}(\text{{out}}_{{i}}) \sim \mathcal{{N}}(0, \frac{{1}}{{2}}) 
<a name="l9019"><span class="ln">9019 </span></a> 
<a name="l9020"><span class="ln">9020 </span></a>The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l9021"><span class="ln">9021 </span></a> 
<a name="l9022"><span class="ln">9022 </span></a> 
<a name="l9023"><span class="ln">9023 </span></a>Args: 
<a name="l9024"><span class="ln">9024 </span></a>    size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l9025"><span class="ln">9025 </span></a>        Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l9026"><span class="ln">9026 </span></a> 
<a name="l9027"><span class="ln">9027 </span></a>Keyword args: 
<a name="l9028"><span class="ln">9028 </span></a>    {generator} 
<a name="l9029"><span class="ln">9029 </span></a>    {out} 
<a name="l9030"><span class="ln">9030 </span></a>    {dtype} 
<a name="l9031"><span class="ln">9031 </span></a>    {layout} 
<a name="l9032"><span class="ln">9032 </span></a>    {device} 
<a name="l9033"><span class="ln">9033 </span></a>    {requires_grad} 
<a name="l9034"><span class="ln">9034 </span></a>    {pin_memory} 
<a name="l9035"><span class="ln">9035 </span></a> 
<a name="l9036"><span class="ln">9036 </span></a>Example:: 
<a name="l9037"><span class="ln">9037 </span></a> 
<a name="l9038"><span class="ln">9038 </span></a>    &gt;&gt;&gt; torch.randn(4) 
<a name="l9039"><span class="ln">9039 </span></a>    tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l9040"><span class="ln">9040 </span></a>    &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l9041"><span class="ln">9041 </span></a>    tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l9042"><span class="ln">9042 </span></a>            [ 1.1719, -0.4709, -0.1996]]) 
<a name="l9043"><span class="ln">9043 </span></a> 
<a name="l9044"><span class="ln">9044 </span></a>.. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l9045"><span class="ln">9045 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l9046"><span class="ln">9046 </span></a><span class="s3">)</span>
<a name="l9047"><span class="ln">9047 </span></a>
<a name="l9048"><span class="ln">9048 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9049"><span class="ln">9049 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">randn_like</span><span class="s3">,</span>
<a name="l9050"><span class="ln">9050 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9051"><span class="ln">9051 </span></a>randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l9052"><span class="ln">9052 </span></a> 
<a name="l9053"><span class="ln">9053 </span></a>Returns a tensor with the same size as :attr:`input` that is filled with 
<a name="l9054"><span class="ln">9054 </span></a>random numbers from a normal distribution with mean 0 and variance 1. Please refer to :func:`torch.randn` for the 
<a name="l9055"><span class="ln">9055 </span></a>sampling process of complex dtypes. ``torch.randn_like(input)`` is equivalent to 
<a name="l9056"><span class="ln">9056 </span></a>``torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l9057"><span class="ln">9057 </span></a> 
<a name="l9058"><span class="ln">9058 </span></a>Args: 
<a name="l9059"><span class="ln">9059 </span></a>    {input} 
<a name="l9060"><span class="ln">9060 </span></a> 
<a name="l9061"><span class="ln">9061 </span></a>Keyword args: 
<a name="l9062"><span class="ln">9062 </span></a>    {dtype} 
<a name="l9063"><span class="ln">9063 </span></a>    {layout} 
<a name="l9064"><span class="ln">9064 </span></a>    {device} 
<a name="l9065"><span class="ln">9065 </span></a>    {requires_grad} 
<a name="l9066"><span class="ln">9066 </span></a>    {memory_format} 
<a name="l9067"><span class="ln">9067 </span></a> 
<a name="l9068"><span class="ln">9068 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_like_common_args</span><span class="s3">),</span>
<a name="l9069"><span class="ln">9069 </span></a><span class="s3">)</span>
<a name="l9070"><span class="ln">9070 </span></a>
<a name="l9071"><span class="ln">9071 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9072"><span class="ln">9072 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">randperm</span><span class="s3">,</span>
<a name="l9073"><span class="ln">9073 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l9074"><span class="ln">9074 </span></a>randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, \ 
<a name="l9075"><span class="ln">9075 </span></a>device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l9076"><span class="ln">9076 </span></a>&quot;&quot;&quot;</span>
<a name="l9077"><span class="ln">9077 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9078"><span class="ln">9078 </span></a>Returns a random permutation of integers from ``0`` to ``n - 1``. 
<a name="l9079"><span class="ln">9079 </span></a> 
<a name="l9080"><span class="ln">9080 </span></a>Args: 
<a name="l9081"><span class="ln">9081 </span></a>    n (int): the upper bound (exclusive) 
<a name="l9082"><span class="ln">9082 </span></a> 
<a name="l9083"><span class="ln">9083 </span></a>Keyword args: 
<a name="l9084"><span class="ln">9084 </span></a>    {generator} 
<a name="l9085"><span class="ln">9085 </span></a>    {out} 
<a name="l9086"><span class="ln">9086 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l9087"><span class="ln">9087 </span></a>        Default: ``torch.int64``. 
<a name="l9088"><span class="ln">9088 </span></a>    {layout} 
<a name="l9089"><span class="ln">9089 </span></a>    {device} 
<a name="l9090"><span class="ln">9090 </span></a>    {requires_grad} 
<a name="l9091"><span class="ln">9091 </span></a>    {pin_memory} 
<a name="l9092"><span class="ln">9092 </span></a> 
<a name="l9093"><span class="ln">9093 </span></a>Example:: 
<a name="l9094"><span class="ln">9094 </span></a> 
<a name="l9095"><span class="ln">9095 </span></a>    &gt;&gt;&gt; torch.randperm(4) 
<a name="l9096"><span class="ln">9096 </span></a>    tensor([2, 1, 0, 3]) 
<a name="l9097"><span class="ln">9097 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l9098"><span class="ln">9098 </span></a><span class="s3">)</span>
<a name="l9099"><span class="ln">9099 </span></a>
<a name="l9100"><span class="ln">9100 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9101"><span class="ln">9101 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">tensor</span><span class="s3">,</span>
<a name="l9102"><span class="ln">9102 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9103"><span class="ln">9103 </span></a>tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l9104"><span class="ln">9104 </span></a> 
<a name="l9105"><span class="ln">9105 </span></a>Constructs a tensor with no autograd history (also known as a &quot;leaf tensor&quot;, see :doc:`/notes/autograd`) by copying :attr:`data`. 
<a name="l9106"><span class="ln">9106 </span></a> 
<a name="l9107"><span class="ln">9107 </span></a>.. warning:: 
<a name="l9108"><span class="ln">9108 </span></a> 
<a name="l9109"><span class="ln">9109 </span></a>    When working with tensors prefer using :func:`torch.Tensor.clone`, 
<a name="l9110"><span class="ln">9110 </span></a>    :func:`torch.Tensor.detach`, and :func:`torch.Tensor.requires_grad_` for 
<a name="l9111"><span class="ln">9111 </span></a>    readability. Letting `t` be a tensor, ``torch.tensor(t)`` is equivalent to 
<a name="l9112"><span class="ln">9112 </span></a>    ``t.detach().clone()``, and ``torch.tensor(t, requires_grad=True)`` 
<a name="l9113"><span class="ln">9113 </span></a>    is equivalent to ``t.detach().clone().requires_grad_(True)``. 
<a name="l9114"><span class="ln">9114 </span></a> 
<a name="l9115"><span class="ln">9115 </span></a>.. seealso:: 
<a name="l9116"><span class="ln">9116 </span></a> 
<a name="l9117"><span class="ln">9117 </span></a>    :func:`torch.as_tensor` preserves autograd history and avoids copies where possible. 
<a name="l9118"><span class="ln">9118 </span></a>    :func:`torch.from_numpy` creates a tensor that shares storage with a NumPy array. 
<a name="l9119"><span class="ln">9119 </span></a> 
<a name="l9120"><span class="ln">9120 </span></a>Args: 
<a name="l9121"><span class="ln">9121 </span></a>    {data} 
<a name="l9122"><span class="ln">9122 </span></a> 
<a name="l9123"><span class="ln">9123 </span></a>Keyword args: 
<a name="l9124"><span class="ln">9124 </span></a>    {dtype} 
<a name="l9125"><span class="ln">9125 </span></a>    device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor 
<a name="l9126"><span class="ln">9126 </span></a>        then the device of data is used. If None and data is not a tensor then 
<a name="l9127"><span class="ln">9127 </span></a>        the result tensor is constructed on the current device. 
<a name="l9128"><span class="ln">9128 </span></a>    {requires_grad} 
<a name="l9129"><span class="ln">9129 </span></a>    {pin_memory} 
<a name="l9130"><span class="ln">9130 </span></a> 
<a name="l9131"><span class="ln">9131 </span></a> 
<a name="l9132"><span class="ln">9132 </span></a>Example:: 
<a name="l9133"><span class="ln">9133 </span></a> 
<a name="l9134"><span class="ln">9134 </span></a>    &gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]]) 
<a name="l9135"><span class="ln">9135 </span></a>    tensor([[ 0.1000,  1.2000], 
<a name="l9136"><span class="ln">9136 </span></a>            [ 2.2000,  3.1000], 
<a name="l9137"><span class="ln">9137 </span></a>            [ 4.9000,  5.2000]]) 
<a name="l9138"><span class="ln">9138 </span></a> 
<a name="l9139"><span class="ln">9139 </span></a>    &gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data 
<a name="l9140"><span class="ln">9140 </span></a>    tensor([ 0,  1]) 
<a name="l9141"><span class="ln">9141 </span></a> 
<a name="l9142"><span class="ln">9142 </span></a>    &gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]], 
<a name="l9143"><span class="ln">9143 </span></a>    ...              dtype=torch.float64, 
<a name="l9144"><span class="ln">9144 </span></a>    ...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device 
<a name="l9145"><span class="ln">9145 </span></a>    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0') 
<a name="l9146"><span class="ln">9146 </span></a> 
<a name="l9147"><span class="ln">9147 </span></a>    &gt;&gt;&gt; torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor 
<a name="l9148"><span class="ln">9148 </span></a>    tensor(3.1416) 
<a name="l9149"><span class="ln">9149 </span></a> 
<a name="l9150"><span class="ln">9150 </span></a>    &gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,)) 
<a name="l9151"><span class="ln">9151 </span></a>    tensor([]) 
<a name="l9152"><span class="ln">9152 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_data_common_args</span><span class="s3">),</span>
<a name="l9153"><span class="ln">9153 </span></a><span class="s3">)</span>
<a name="l9154"><span class="ln">9154 </span></a>
<a name="l9155"><span class="ln">9155 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9156"><span class="ln">9156 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">range</span><span class="s3">,</span>
<a name="l9157"><span class="ln">9157 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9158"><span class="ln">9158 </span></a>range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l9159"><span class="ln">9159 </span></a> 
<a name="l9160"><span class="ln">9160 </span></a>Returns a 1-D tensor of size :math:`\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1` 
<a name="l9161"><span class="ln">9161 </span></a>with values from :attr:`start` to :attr:`end` with step :attr:`step`. Step is 
<a name="l9162"><span class="ln">9162 </span></a>the gap between two values in the tensor. 
<a name="l9163"><span class="ln">9163 </span></a> 
<a name="l9164"><span class="ln">9164 </span></a>.. math:: 
<a name="l9165"><span class="ln">9165 </span></a>    \text{out}_{i+1} = \text{out}_i + \text{step}. 
<a name="l9166"><span class="ln">9166 </span></a>&quot;&quot;&quot;</span>
<a name="l9167"><span class="ln">9167 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9168"><span class="ln">9168 </span></a>.. warning:: 
<a name="l9169"><span class="ln">9169 </span></a>    This function is deprecated and will be removed in a future release because its behavior is inconsistent with 
<a name="l9170"><span class="ln">9170 </span></a>    Python's range builtin. Instead, use :func:`torch.arange`, which produces values in [start, end). 
<a name="l9171"><span class="ln">9171 </span></a> 
<a name="l9172"><span class="ln">9172 </span></a>Args: 
<a name="l9173"><span class="ln">9173 </span></a>    start (float, optional): the starting value for the set of points. Default: ``0``. 
<a name="l9174"><span class="ln">9174 </span></a>    end (float): the ending value for the set of points 
<a name="l9175"><span class="ln">9175 </span></a>    step (float, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l9176"><span class="ln">9176 </span></a> 
<a name="l9177"><span class="ln">9177 </span></a>Keyword args: 
<a name="l9178"><span class="ln">9178 </span></a>    {out} 
<a name="l9179"><span class="ln">9179 </span></a>    {dtype} If `dtype` is not given, infer the data type from the other input 
<a name="l9180"><span class="ln">9180 </span></a>        arguments. If any of `start`, `end`, or `step` are floating-point, the 
<a name="l9181"><span class="ln">9181 </span></a>        `dtype` is inferred to be the default dtype, see 
<a name="l9182"><span class="ln">9182 </span></a>        :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l9183"><span class="ln">9183 </span></a>        be `torch.int64`. 
<a name="l9184"><span class="ln">9184 </span></a>    {layout} 
<a name="l9185"><span class="ln">9185 </span></a>    {device} 
<a name="l9186"><span class="ln">9186 </span></a>    {requires_grad} 
<a name="l9187"><span class="ln">9187 </span></a> 
<a name="l9188"><span class="ln">9188 </span></a>Example:: 
<a name="l9189"><span class="ln">9189 </span></a> 
<a name="l9190"><span class="ln">9190 </span></a>    &gt;&gt;&gt; torch.range(1, 4) 
<a name="l9191"><span class="ln">9191 </span></a>    tensor([ 1.,  2.,  3.,  4.]) 
<a name="l9192"><span class="ln">9192 </span></a>    &gt;&gt;&gt; torch.range(1, 4, 0.5) 
<a name="l9193"><span class="ln">9193 </span></a>    tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000]) 
<a name="l9194"><span class="ln">9194 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l9195"><span class="ln">9195 </span></a><span class="s3">)</span>
<a name="l9196"><span class="ln">9196 </span></a>
<a name="l9197"><span class="ln">9197 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9198"><span class="ln">9198 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">arange</span><span class="s3">,</span>
<a name="l9199"><span class="ln">9199 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9200"><span class="ln">9200 </span></a>arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l9201"><span class="ln">9201 </span></a> 
<a name="l9202"><span class="ln">9202 </span></a>Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil` 
<a name="l9203"><span class="ln">9203 </span></a>with values from the interval ``[start, end)`` taken with common difference 
<a name="l9204"><span class="ln">9204 </span></a>:attr:`step` beginning from `start`. 
<a name="l9205"><span class="ln">9205 </span></a> 
<a name="l9206"><span class="ln">9206 </span></a>Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``), 
<a name="l9207"><span class="ln">9207 </span></a>the results may be affected by floating-point rounding behavior. Some values in the sequence 
<a name="l9208"><span class="ln">9208 </span></a>might not be exactly representable in certain floating-point formats, which can lead to 
<a name="l9209"><span class="ln">9209 </span></a>repeated values or unexpected rounding. For precise sequences, it is recommended to use 
<a name="l9210"><span class="ln">9210 </span></a>integer dtypes instead of floating-point dtypes. 
<a name="l9211"><span class="ln">9211 </span></a> 
<a name="l9212"><span class="ln">9212 </span></a>Note that non-integer :attr:`step` is subject to floating point rounding errors when 
<a name="l9213"><span class="ln">9213 </span></a>comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end` 
<a name="l9214"><span class="ln">9214 </span></a>in such cases. 
<a name="l9215"><span class="ln">9215 </span></a> 
<a name="l9216"><span class="ln">9216 </span></a>.. math:: 
<a name="l9217"><span class="ln">9217 </span></a>    \text{out}_{{i+1}} = \text{out}_{i} + \text{step} 
<a name="l9218"><span class="ln">9218 </span></a>&quot;&quot;&quot;</span>
<a name="l9219"><span class="ln">9219 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9220"><span class="ln">9220 </span></a>Args: 
<a name="l9221"><span class="ln">9221 </span></a>    start (Number, optional): the starting value for the set of points. Default: ``0``. 
<a name="l9222"><span class="ln">9222 </span></a>    end (Number): the ending value for the set of points 
<a name="l9223"><span class="ln">9223 </span></a>    step (Number, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l9224"><span class="ln">9224 </span></a> 
<a name="l9225"><span class="ln">9225 </span></a>Keyword args: 
<a name="l9226"><span class="ln">9226 </span></a>    {out} 
<a name="l9227"><span class="ln">9227 </span></a>    {dtype} If `dtype` is not given, infer the data type from the other input 
<a name="l9228"><span class="ln">9228 </span></a>        arguments. If any of `start`, `end`, or `stop` are floating-point, the 
<a name="l9229"><span class="ln">9229 </span></a>        `dtype` is inferred to be the default dtype, see 
<a name="l9230"><span class="ln">9230 </span></a>        :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l9231"><span class="ln">9231 </span></a>        be `torch.int64`. 
<a name="l9232"><span class="ln">9232 </span></a>    {layout} 
<a name="l9233"><span class="ln">9233 </span></a>    {device} 
<a name="l9234"><span class="ln">9234 </span></a>    {requires_grad} 
<a name="l9235"><span class="ln">9235 </span></a> 
<a name="l9236"><span class="ln">9236 </span></a>Example:: 
<a name="l9237"><span class="ln">9237 </span></a> 
<a name="l9238"><span class="ln">9238 </span></a>    &gt;&gt;&gt; torch.arange(5) 
<a name="l9239"><span class="ln">9239 </span></a>    tensor([ 0,  1,  2,  3,  4]) 
<a name="l9240"><span class="ln">9240 </span></a>    &gt;&gt;&gt; torch.arange(1, 4) 
<a name="l9241"><span class="ln">9241 </span></a>    tensor([ 1,  2,  3]) 
<a name="l9242"><span class="ln">9242 </span></a>    &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 
<a name="l9243"><span class="ln">9243 </span></a>    tensor([ 1.0000,  1.5000,  2.0000]) 
<a name="l9244"><span class="ln">9244 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l9245"><span class="ln">9245 </span></a><span class="s3">)</span>
<a name="l9246"><span class="ln">9246 </span></a>
<a name="l9247"><span class="ln">9247 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9248"><span class="ln">9248 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">ravel</span><span class="s3">,</span>
<a name="l9249"><span class="ln">9249 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9250"><span class="ln">9250 </span></a>ravel(input) -&gt; Tensor 
<a name="l9251"><span class="ln">9251 </span></a> 
<a name="l9252"><span class="ln">9252 </span></a>Return a contiguous flattened tensor. A copy is made only if needed. 
<a name="l9253"><span class="ln">9253 </span></a> 
<a name="l9254"><span class="ln">9254 </span></a>Args: 
<a name="l9255"><span class="ln">9255 </span></a>    {input} 
<a name="l9256"><span class="ln">9256 </span></a> 
<a name="l9257"><span class="ln">9257 </span></a>Example:: 
<a name="l9258"><span class="ln">9258 </span></a> 
<a name="l9259"><span class="ln">9259 </span></a>    &gt;&gt;&gt; t = torch.tensor([[[1, 2], 
<a name="l9260"><span class="ln">9260 </span></a>    ...                    [3, 4]], 
<a name="l9261"><span class="ln">9261 </span></a>    ...                   [[5, 6], 
<a name="l9262"><span class="ln">9262 </span></a>    ...                    [7, 8]]]) 
<a name="l9263"><span class="ln">9263 </span></a>    &gt;&gt;&gt; torch.ravel(t) 
<a name="l9264"><span class="ln">9264 </span></a>    tensor([1, 2, 3, 4, 5, 6, 7, 8]) 
<a name="l9265"><span class="ln">9265 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9266"><span class="ln">9266 </span></a><span class="s3">)</span>
<a name="l9267"><span class="ln">9267 </span></a>
<a name="l9268"><span class="ln">9268 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9269"><span class="ln">9269 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">remainder</span><span class="s3">,</span>
<a name="l9270"><span class="ln">9270 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9271"><span class="ln">9271 </span></a>remainder(input, other, *, out=None) -&gt; Tensor 
<a name="l9272"><span class="ln">9272 </span></a> 
<a name="l9273"><span class="ln">9273 </span></a>Computes 
<a name="l9274"><span class="ln">9274 </span></a>`Python's modulus operation &lt;https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations&gt;`_ 
<a name="l9275"><span class="ln">9275 </span></a>entrywise.  The result has the same sign as the divisor :attr:`other` and its absolute value 
<a name="l9276"><span class="ln">9276 </span></a>is less than that of :attr:`other`. 
<a name="l9277"><span class="ln">9277 </span></a> 
<a name="l9278"><span class="ln">9278 </span></a>It may also be defined in terms of :func:`torch.div` as 
<a name="l9279"><span class="ln">9279 </span></a> 
<a name="l9280"><span class="ln">9280 </span></a>.. code:: python 
<a name="l9281"><span class="ln">9281 </span></a> 
<a name="l9282"><span class="ln">9282 </span></a>    torch.remainder(a, b) == a - a.div(b, rounding_mode=&quot;floor&quot;) * b 
<a name="l9283"><span class="ln">9283 </span></a> 
<a name="l9284"><span class="ln">9284 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l9285"><span class="ln">9285 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and float inputs. 
<a name="l9286"><span class="ln">9286 </span></a> 
<a name="l9287"><span class="ln">9287 </span></a>.. note:: 
<a name="l9288"><span class="ln">9288 </span></a>    Complex inputs are not supported. In some cases, it is not mathematically 
<a name="l9289"><span class="ln">9289 </span></a>    possible to satisfy the definition of a modulo operation with complex numbers. 
<a name="l9290"><span class="ln">9290 </span></a>    See :func:`torch.fmod` for how division by zero is handled. 
<a name="l9291"><span class="ln">9291 </span></a> 
<a name="l9292"><span class="ln">9292 </span></a>.. seealso:: 
<a name="l9293"><span class="ln">9293 </span></a> 
<a name="l9294"><span class="ln">9294 </span></a>    :func:`torch.fmod` which implements C++'s `std::fmod &lt;https://en.cppreference.com/w/cpp/numeric/math/fmod&gt;`_. 
<a name="l9295"><span class="ln">9295 </span></a>    This one is defined in terms of division rounding towards zero. 
<a name="l9296"><span class="ln">9296 </span></a> 
<a name="l9297"><span class="ln">9297 </span></a>Args: 
<a name="l9298"><span class="ln">9298 </span></a>    input (Tensor or Scalar): the dividend 
<a name="l9299"><span class="ln">9299 </span></a>    other (Tensor or Scalar): the divisor 
<a name="l9300"><span class="ln">9300 </span></a> 
<a name="l9301"><span class="ln">9301 </span></a>Keyword args: 
<a name="l9302"><span class="ln">9302 </span></a>    {out} 
<a name="l9303"><span class="ln">9303 </span></a> 
<a name="l9304"><span class="ln">9304 </span></a>Example:: 
<a name="l9305"><span class="ln">9305 </span></a> 
<a name="l9306"><span class="ln">9306 </span></a>    &gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) 
<a name="l9307"><span class="ln">9307 </span></a>    tensor([ 1.,  0.,  1.,  1.,  0.,  1.]) 
<a name="l9308"><span class="ln">9308 </span></a>    &gt;&gt;&gt; torch.remainder(torch.tensor([1, 2, 3, 4, 5]), -1.5) 
<a name="l9309"><span class="ln">9309 </span></a>    tensor([ -0.5000, -1.0000,  0.0000, -0.5000, -1.0000 ]) 
<a name="l9310"><span class="ln">9310 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9311"><span class="ln">9311 </span></a><span class="s3">)</span>
<a name="l9312"><span class="ln">9312 </span></a>
<a name="l9313"><span class="ln">9313 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9314"><span class="ln">9314 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">renorm</span><span class="s3">,</span>
<a name="l9315"><span class="ln">9315 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9316"><span class="ln">9316 </span></a>renorm(input, p, dim, maxnorm, *, out=None) -&gt; Tensor 
<a name="l9317"><span class="ln">9317 </span></a> 
<a name="l9318"><span class="ln">9318 </span></a>Returns a tensor where each sub-tensor of :attr:`input` along dimension 
<a name="l9319"><span class="ln">9319 </span></a>:attr:`dim` is normalized such that the `p`-norm of the sub-tensor is lower 
<a name="l9320"><span class="ln">9320 </span></a>than the value :attr:`maxnorm` 
<a name="l9321"><span class="ln">9321 </span></a> 
<a name="l9322"><span class="ln">9322 </span></a>.. note:: If the norm of a row is lower than `maxnorm`, the row is unchanged 
<a name="l9323"><span class="ln">9323 </span></a> 
<a name="l9324"><span class="ln">9324 </span></a>Args: 
<a name="l9325"><span class="ln">9325 </span></a>    {input} 
<a name="l9326"><span class="ln">9326 </span></a>    p (float): the power for the norm computation 
<a name="l9327"><span class="ln">9327 </span></a>    dim (int): the dimension to slice over to get the sub-tensors 
<a name="l9328"><span class="ln">9328 </span></a>    maxnorm (float): the maximum norm to keep each sub-tensor under 
<a name="l9329"><span class="ln">9329 </span></a> 
<a name="l9330"><span class="ln">9330 </span></a>Keyword args: 
<a name="l9331"><span class="ln">9331 </span></a>    {out} 
<a name="l9332"><span class="ln">9332 </span></a> 
<a name="l9333"><span class="ln">9333 </span></a>Example:: 
<a name="l9334"><span class="ln">9334 </span></a> 
<a name="l9335"><span class="ln">9335 </span></a>    &gt;&gt;&gt; x = torch.ones(3, 3) 
<a name="l9336"><span class="ln">9336 </span></a>    &gt;&gt;&gt; x[1].fill_(2) 
<a name="l9337"><span class="ln">9337 </span></a>    tensor([ 2.,  2.,  2.]) 
<a name="l9338"><span class="ln">9338 </span></a>    &gt;&gt;&gt; x[2].fill_(3) 
<a name="l9339"><span class="ln">9339 </span></a>    tensor([ 3.,  3.,  3.]) 
<a name="l9340"><span class="ln">9340 </span></a>    &gt;&gt;&gt; x 
<a name="l9341"><span class="ln">9341 </span></a>    tensor([[ 1.,  1.,  1.], 
<a name="l9342"><span class="ln">9342 </span></a>            [ 2.,  2.,  2.], 
<a name="l9343"><span class="ln">9343 </span></a>            [ 3.,  3.,  3.]]) 
<a name="l9344"><span class="ln">9344 </span></a>    &gt;&gt;&gt; torch.renorm(x, 1, 0, 5) 
<a name="l9345"><span class="ln">9345 </span></a>    tensor([[ 1.0000,  1.0000,  1.0000], 
<a name="l9346"><span class="ln">9346 </span></a>            [ 1.6667,  1.6667,  1.6667], 
<a name="l9347"><span class="ln">9347 </span></a>            [ 1.6667,  1.6667,  1.6667]]) 
<a name="l9348"><span class="ln">9348 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9349"><span class="ln">9349 </span></a><span class="s3">)</span>
<a name="l9350"><span class="ln">9350 </span></a>
<a name="l9351"><span class="ln">9351 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9352"><span class="ln">9352 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">reshape</span><span class="s3">,</span>
<a name="l9353"><span class="ln">9353 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9354"><span class="ln">9354 </span></a>reshape(input, shape) -&gt; Tensor 
<a name="l9355"><span class="ln">9355 </span></a> 
<a name="l9356"><span class="ln">9356 </span></a>Returns a tensor with the same data and number of elements as :attr:`input`, 
<a name="l9357"><span class="ln">9357 </span></a>but with the specified shape. When possible, the returned tensor will be a view 
<a name="l9358"><span class="ln">9358 </span></a>of :attr:`input`. Otherwise, it will be a copy. Contiguous inputs and inputs 
<a name="l9359"><span class="ln">9359 </span></a>with compatible strides can be reshaped without copying, but you should not 
<a name="l9360"><span class="ln">9360 </span></a>depend on the copying vs. viewing behavior. 
<a name="l9361"><span class="ln">9361 </span></a> 
<a name="l9362"><span class="ln">9362 </span></a>See :meth:`torch.Tensor.view` on when it is possible to return a view. 
<a name="l9363"><span class="ln">9363 </span></a> 
<a name="l9364"><span class="ln">9364 </span></a>A single dimension may be -1, in which case it's inferred from the remaining 
<a name="l9365"><span class="ln">9365 </span></a>dimensions and the number of elements in :attr:`input`. 
<a name="l9366"><span class="ln">9366 </span></a> 
<a name="l9367"><span class="ln">9367 </span></a>Args: 
<a name="l9368"><span class="ln">9368 </span></a>    input (Tensor): the tensor to be reshaped 
<a name="l9369"><span class="ln">9369 </span></a>    shape (tuple of int): the new shape 
<a name="l9370"><span class="ln">9370 </span></a> 
<a name="l9371"><span class="ln">9371 </span></a>Example:: 
<a name="l9372"><span class="ln">9372 </span></a> 
<a name="l9373"><span class="ln">9373 </span></a>    &gt;&gt;&gt; a = torch.arange(4.) 
<a name="l9374"><span class="ln">9374 </span></a>    &gt;&gt;&gt; torch.reshape(a, (2, 2)) 
<a name="l9375"><span class="ln">9375 </span></a>    tensor([[ 0.,  1.], 
<a name="l9376"><span class="ln">9376 </span></a>            [ 2.,  3.]]) 
<a name="l9377"><span class="ln">9377 </span></a>    &gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]]) 
<a name="l9378"><span class="ln">9378 </span></a>    &gt;&gt;&gt; torch.reshape(b, (-1,)) 
<a name="l9379"><span class="ln">9379 </span></a>    tensor([ 0,  1,  2,  3]) 
<a name="l9380"><span class="ln">9380 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9381"><span class="ln">9381 </span></a><span class="s3">)</span>
<a name="l9382"><span class="ln">9382 </span></a>
<a name="l9383"><span class="ln">9383 </span></a>
<a name="l9384"><span class="ln">9384 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9385"><span class="ln">9385 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">result_type</span><span class="s3">,</span>
<a name="l9386"><span class="ln">9386 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9387"><span class="ln">9387 </span></a>result_type(tensor1, tensor2) -&gt; dtype 
<a name="l9388"><span class="ln">9388 </span></a> 
<a name="l9389"><span class="ln">9389 </span></a>Returns the :class:`torch.dtype` that would result from performing an arithmetic 
<a name="l9390"><span class="ln">9390 </span></a>operation on the provided input tensors. See type promotion :ref:`documentation &lt;type-promotion-doc&gt;` 
<a name="l9391"><span class="ln">9391 </span></a>for more information on the type promotion logic. 
<a name="l9392"><span class="ln">9392 </span></a> 
<a name="l9393"><span class="ln">9393 </span></a>Args: 
<a name="l9394"><span class="ln">9394 </span></a>    tensor1 (Tensor or Number): an input tensor or number 
<a name="l9395"><span class="ln">9395 </span></a>    tensor2 (Tensor or Number): an input tensor or number 
<a name="l9396"><span class="ln">9396 </span></a> 
<a name="l9397"><span class="ln">9397 </span></a>Example:: 
<a name="l9398"><span class="ln">9398 </span></a> 
<a name="l9399"><span class="ln">9399 </span></a>    &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0) 
<a name="l9400"><span class="ln">9400 </span></a>    torch.float32 
<a name="l9401"><span class="ln">9401 </span></a>    &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1)) 
<a name="l9402"><span class="ln">9402 </span></a>    torch.uint8 
<a name="l9403"><span class="ln">9403 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9404"><span class="ln">9404 </span></a><span class="s3">)</span>
<a name="l9405"><span class="ln">9405 </span></a>
<a name="l9406"><span class="ln">9406 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9407"><span class="ln">9407 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">row_stack</span><span class="s3">,</span>
<a name="l9408"><span class="ln">9408 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9409"><span class="ln">9409 </span></a>row_stack(tensors, *, out=None) -&gt; Tensor 
<a name="l9410"><span class="ln">9410 </span></a> 
<a name="l9411"><span class="ln">9411 </span></a>Alias of :func:`torch.vstack`. 
<a name="l9412"><span class="ln">9412 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9413"><span class="ln">9413 </span></a><span class="s3">)</span>
<a name="l9414"><span class="ln">9414 </span></a>
<a name="l9415"><span class="ln">9415 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9416"><span class="ln">9416 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">round</span><span class="s3">,</span>
<a name="l9417"><span class="ln">9417 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9418"><span class="ln">9418 </span></a>round(input, *, decimals=0, out=None) -&gt; Tensor 
<a name="l9419"><span class="ln">9419 </span></a> 
<a name="l9420"><span class="ln">9420 </span></a>Rounds elements of :attr:`input` to the nearest integer. 
<a name="l9421"><span class="ln">9421 </span></a> 
<a name="l9422"><span class="ln">9422 </span></a>For integer inputs, follows the array-api convention of returning a 
<a name="l9423"><span class="ln">9423 </span></a>copy of the input tensor. 
<a name="l9424"><span class="ln">9424 </span></a>The return type of output is same as that of input's dtype. 
<a name="l9425"><span class="ln">9425 </span></a> 
<a name="l9426"><span class="ln">9426 </span></a>.. note:: 
<a name="l9427"><span class="ln">9427 </span></a>    This function implements the &quot;round half to even&quot; to 
<a name="l9428"><span class="ln">9428 </span></a>    break ties when a number is equidistant from two 
<a name="l9429"><span class="ln">9429 </span></a>    integers (e.g. `round(2.5)` is 2). 
<a name="l9430"><span class="ln">9430 </span></a> 
<a name="l9431"><span class="ln">9431 </span></a>    When the :attr:\`decimals\` argument is specified the 
<a name="l9432"><span class="ln">9432 </span></a>    algorithm used is similar to NumPy's `around`. This 
<a name="l9433"><span class="ln">9433 </span></a>    algorithm is fast but inexact and it can easily 
<a name="l9434"><span class="ln">9434 </span></a>    overflow for low precision dtypes. 
<a name="l9435"><span class="ln">9435 </span></a>    Eg. `round(tensor([10000], dtype=torch.float16), decimals=3)` is `inf`. 
<a name="l9436"><span class="ln">9436 </span></a> 
<a name="l9437"><span class="ln">9437 </span></a>.. seealso:: 
<a name="l9438"><span class="ln">9438 </span></a>    :func:`torch.ceil`, which rounds up. 
<a name="l9439"><span class="ln">9439 </span></a>    :func:`torch.floor`, which rounds down. 
<a name="l9440"><span class="ln">9440 </span></a>    :func:`torch.trunc`, which rounds towards zero. 
<a name="l9441"><span class="ln">9441 </span></a> 
<a name="l9442"><span class="ln">9442 </span></a>Args: 
<a name="l9443"><span class="ln">9443 </span></a>    {input} 
<a name="l9444"><span class="ln">9444 </span></a>    decimals (int): Number of decimal places to round to (default: 0). 
<a name="l9445"><span class="ln">9445 </span></a>        If decimals is negative, it specifies the number of positions 
<a name="l9446"><span class="ln">9446 </span></a>        to the left of the decimal point. 
<a name="l9447"><span class="ln">9447 </span></a> 
<a name="l9448"><span class="ln">9448 </span></a>Keyword args: 
<a name="l9449"><span class="ln">9449 </span></a>    {out} 
<a name="l9450"><span class="ln">9450 </span></a> 
<a name="l9451"><span class="ln">9451 </span></a>Example:: 
<a name="l9452"><span class="ln">9452 </span></a> 
<a name="l9453"><span class="ln">9453 </span></a>    &gt;&gt;&gt; torch.round(torch.tensor((4.7, -2.3, 9.1, -7.7))) 
<a name="l9454"><span class="ln">9454 </span></a>    tensor([ 5.,  -2.,  9., -8.]) 
<a name="l9455"><span class="ln">9455 </span></a> 
<a name="l9456"><span class="ln">9456 </span></a>    &gt;&gt;&gt; # Values equidistant from two integers are rounded towards the 
<a name="l9457"><span class="ln">9457 </span></a>    &gt;&gt;&gt; #   the nearest even value (zero is treated as even) 
<a name="l9458"><span class="ln">9458 </span></a>    &gt;&gt;&gt; torch.round(torch.tensor([-0.5, 0.5, 1.5, 2.5])) 
<a name="l9459"><span class="ln">9459 </span></a>    tensor([-0., 0., 2., 2.]) 
<a name="l9460"><span class="ln">9460 </span></a> 
<a name="l9461"><span class="ln">9461 </span></a>    &gt;&gt;&gt; # A positive decimals argument rounds to the to that decimal place 
<a name="l9462"><span class="ln">9462 </span></a>    &gt;&gt;&gt; torch.round(torch.tensor([0.1234567]), decimals=3) 
<a name="l9463"><span class="ln">9463 </span></a>    tensor([0.1230]) 
<a name="l9464"><span class="ln">9464 </span></a> 
<a name="l9465"><span class="ln">9465 </span></a>    &gt;&gt;&gt; # A negative decimals argument rounds to the left of the decimal 
<a name="l9466"><span class="ln">9466 </span></a>    &gt;&gt;&gt; torch.round(torch.tensor([1200.1234567]), decimals=-3) 
<a name="l9467"><span class="ln">9467 </span></a>    tensor([1000.]) 
<a name="l9468"><span class="ln">9468 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9469"><span class="ln">9469 </span></a><span class="s3">)</span>
<a name="l9470"><span class="ln">9470 </span></a>
<a name="l9471"><span class="ln">9471 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9472"><span class="ln">9472 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">rsqrt</span><span class="s3">,</span>
<a name="l9473"><span class="ln">9473 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9474"><span class="ln">9474 </span></a>rsqrt(input, *, out=None) -&gt; Tensor 
<a name="l9475"><span class="ln">9475 </span></a> 
<a name="l9476"><span class="ln">9476 </span></a>Returns a new tensor with the reciprocal of the square-root of each of 
<a name="l9477"><span class="ln">9477 </span></a>the elements of :attr:`input`. 
<a name="l9478"><span class="ln">9478 </span></a> 
<a name="l9479"><span class="ln">9479 </span></a>.. math:: 
<a name="l9480"><span class="ln">9480 </span></a>    \text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}} 
<a name="l9481"><span class="ln">9481 </span></a>&quot;&quot;&quot;</span>
<a name="l9482"><span class="ln">9482 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9483"><span class="ln">9483 </span></a>Args: 
<a name="l9484"><span class="ln">9484 </span></a>    {input} 
<a name="l9485"><span class="ln">9485 </span></a> 
<a name="l9486"><span class="ln">9486 </span></a>Keyword args: 
<a name="l9487"><span class="ln">9487 </span></a>    {out} 
<a name="l9488"><span class="ln">9488 </span></a> 
<a name="l9489"><span class="ln">9489 </span></a>Example:: 
<a name="l9490"><span class="ln">9490 </span></a> 
<a name="l9491"><span class="ln">9491 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l9492"><span class="ln">9492 </span></a>    &gt;&gt;&gt; a 
<a name="l9493"><span class="ln">9493 </span></a>    tensor([-0.0370,  0.2970,  1.5420, -0.9105]) 
<a name="l9494"><span class="ln">9494 </span></a>    &gt;&gt;&gt; torch.rsqrt(a) 
<a name="l9495"><span class="ln">9495 </span></a>    tensor([    nan,  1.8351,  0.8053,     nan]) 
<a name="l9496"><span class="ln">9496 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9497"><span class="ln">9497 </span></a><span class="s3">)</span>
<a name="l9498"><span class="ln">9498 </span></a>
<a name="l9499"><span class="ln">9499 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9500"><span class="ln">9500 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">scatter</span><span class="s3">,</span>
<a name="l9501"><span class="ln">9501 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9502"><span class="ln">9502 </span></a>scatter(input, dim, index, src) -&gt; Tensor 
<a name="l9503"><span class="ln">9503 </span></a> 
<a name="l9504"><span class="ln">9504 </span></a>Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l9505"><span class="ln">9505 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9506"><span class="ln">9506 </span></a><span class="s3">)</span>
<a name="l9507"><span class="ln">9507 </span></a>
<a name="l9508"><span class="ln">9508 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9509"><span class="ln">9509 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">scatter_add</span><span class="s3">,</span>
<a name="l9510"><span class="ln">9510 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9511"><span class="ln">9511 </span></a>scatter_add(input, dim, index, src) -&gt; Tensor 
<a name="l9512"><span class="ln">9512 </span></a> 
<a name="l9513"><span class="ln">9513 </span></a>Out-of-place version of :meth:`torch.Tensor.scatter_add_` 
<a name="l9514"><span class="ln">9514 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9515"><span class="ln">9515 </span></a><span class="s3">)</span>
<a name="l9516"><span class="ln">9516 </span></a>
<a name="l9517"><span class="ln">9517 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9518"><span class="ln">9518 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">scatter_reduce</span><span class="s3">,</span>
<a name="l9519"><span class="ln">9519 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9520"><span class="ln">9520 </span></a>scatter_reduce(input, dim, index, src, reduce, *, include_self=True) -&gt; Tensor 
<a name="l9521"><span class="ln">9521 </span></a> 
<a name="l9522"><span class="ln">9522 </span></a>Out-of-place version of :meth:`torch.Tensor.scatter_reduce_` 
<a name="l9523"><span class="ln">9523 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9524"><span class="ln">9524 </span></a><span class="s3">)</span>
<a name="l9525"><span class="ln">9525 </span></a>
<a name="l9526"><span class="ln">9526 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9527"><span class="ln">9527 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">select</span><span class="s3">,</span>
<a name="l9528"><span class="ln">9528 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9529"><span class="ln">9529 </span></a>select(input, dim, index) -&gt; Tensor 
<a name="l9530"><span class="ln">9530 </span></a> 
<a name="l9531"><span class="ln">9531 </span></a>Slices the :attr:`input` tensor along the selected dimension at the given index. 
<a name="l9532"><span class="ln">9532 </span></a>This function returns a view of the original tensor with the given dimension removed. 
<a name="l9533"><span class="ln">9533 </span></a> 
<a name="l9534"><span class="ln">9534 </span></a>.. note:: If :attr:`input` is a sparse tensor and returning a view of 
<a name="l9535"><span class="ln">9535 </span></a>          the tensor is not possible, a RuntimeError exception is 
<a name="l9536"><span class="ln">9536 </span></a>          raised. In this is the case, consider using 
<a name="l9537"><span class="ln">9537 </span></a>          :func:`torch.select_copy` function. 
<a name="l9538"><span class="ln">9538 </span></a> 
<a name="l9539"><span class="ln">9539 </span></a>Args: 
<a name="l9540"><span class="ln">9540 </span></a>    {input} 
<a name="l9541"><span class="ln">9541 </span></a>    dim (int): the dimension to slice 
<a name="l9542"><span class="ln">9542 </span></a>    index (int): the index to select with 
<a name="l9543"><span class="ln">9543 </span></a> 
<a name="l9544"><span class="ln">9544 </span></a>.. note:: 
<a name="l9545"><span class="ln">9545 </span></a> 
<a name="l9546"><span class="ln">9546 </span></a>    :meth:`select` is equivalent to slicing. For example, 
<a name="l9547"><span class="ln">9547 </span></a>    ``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and 
<a name="l9548"><span class="ln">9548 </span></a>    ``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``. 
<a name="l9549"><span class="ln">9549 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9550"><span class="ln">9550 </span></a><span class="s3">)</span>
<a name="l9551"><span class="ln">9551 </span></a>
<a name="l9552"><span class="ln">9552 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9553"><span class="ln">9553 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">select_scatter</span><span class="s3">,</span>
<a name="l9554"><span class="ln">9554 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9555"><span class="ln">9555 </span></a>select_scatter(input, src, dim, index) -&gt; Tensor 
<a name="l9556"><span class="ln">9556 </span></a> 
<a name="l9557"><span class="ln">9557 </span></a>Embeds the values of the :attr:`src` tensor into :attr:`input` at the given index. 
<a name="l9558"><span class="ln">9558 </span></a>This function returns a tensor with fresh storage; it does not create a view. 
<a name="l9559"><span class="ln">9559 </span></a> 
<a name="l9560"><span class="ln">9560 </span></a> 
<a name="l9561"><span class="ln">9561 </span></a>Args: 
<a name="l9562"><span class="ln">9562 </span></a>    {input} 
<a name="l9563"><span class="ln">9563 </span></a>    src (Tensor): The tensor to embed into :attr:`input` 
<a name="l9564"><span class="ln">9564 </span></a>    dim (int): the dimension to insert the slice into. 
<a name="l9565"><span class="ln">9565 </span></a>    index (int): the index to select with 
<a name="l9566"><span class="ln">9566 </span></a> 
<a name="l9567"><span class="ln">9567 </span></a>.. note:: 
<a name="l9568"><span class="ln">9568 </span></a> 
<a name="l9569"><span class="ln">9569 </span></a>    :attr:`src` must be of the proper size in order to be embedded 
<a name="l9570"><span class="ln">9570 </span></a>    into :attr:`input`. Specifically, it should have the same shape as 
<a name="l9571"><span class="ln">9571 </span></a>    ``torch.select(input, dim, index)`` 
<a name="l9572"><span class="ln">9572 </span></a> 
<a name="l9573"><span class="ln">9573 </span></a>Example:: 
<a name="l9574"><span class="ln">9574 </span></a> 
<a name="l9575"><span class="ln">9575 </span></a>    &gt;&gt;&gt; a = torch.zeros(2, 2) 
<a name="l9576"><span class="ln">9576 </span></a>    &gt;&gt;&gt; b = torch.ones(2) 
<a name="l9577"><span class="ln">9577 </span></a>    &gt;&gt;&gt; a.select_scatter(b, 0, 0) 
<a name="l9578"><span class="ln">9578 </span></a>    tensor([[1., 1.], 
<a name="l9579"><span class="ln">9579 </span></a>            [0., 0.]]) 
<a name="l9580"><span class="ln">9580 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9581"><span class="ln">9581 </span></a><span class="s3">)</span>
<a name="l9582"><span class="ln">9582 </span></a>
<a name="l9583"><span class="ln">9583 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9584"><span class="ln">9584 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">slice_scatter</span><span class="s3">,</span>
<a name="l9585"><span class="ln">9585 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9586"><span class="ln">9586 </span></a>slice_scatter(input, src, dim=0, start=None, end=None, step=1) -&gt; Tensor 
<a name="l9587"><span class="ln">9587 </span></a> 
<a name="l9588"><span class="ln">9588 </span></a>Embeds the values of the :attr:`src` tensor into :attr:`input` at the given 
<a name="l9589"><span class="ln">9589 </span></a>dimension. 
<a name="l9590"><span class="ln">9590 </span></a>This function returns a tensor with fresh storage; it does not create a view. 
<a name="l9591"><span class="ln">9591 </span></a> 
<a name="l9592"><span class="ln">9592 </span></a> 
<a name="l9593"><span class="ln">9593 </span></a>Args: 
<a name="l9594"><span class="ln">9594 </span></a>    {input} 
<a name="l9595"><span class="ln">9595 </span></a>    src (Tensor): The tensor to embed into :attr:`input` 
<a name="l9596"><span class="ln">9596 </span></a>    dim (int): the dimension to insert the slice into 
<a name="l9597"><span class="ln">9597 </span></a>    start (Optional[int]): the start index of where to insert the slice 
<a name="l9598"><span class="ln">9598 </span></a>    end (Optional[int]): the end index of where to insert the slice 
<a name="l9599"><span class="ln">9599 </span></a>    step (int): the how many elements to skip in 
<a name="l9600"><span class="ln">9600 </span></a> 
<a name="l9601"><span class="ln">9601 </span></a>Example:: 
<a name="l9602"><span class="ln">9602 </span></a> 
<a name="l9603"><span class="ln">9603 </span></a>    &gt;&gt;&gt; a = torch.zeros(8, 8) 
<a name="l9604"><span class="ln">9604 </span></a>    &gt;&gt;&gt; b = torch.ones(2, 8) 
<a name="l9605"><span class="ln">9605 </span></a>    &gt;&gt;&gt; a.slice_scatter(b, start=6) 
<a name="l9606"><span class="ln">9606 </span></a>    tensor([[0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l9607"><span class="ln">9607 </span></a>            [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l9608"><span class="ln">9608 </span></a>            [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l9609"><span class="ln">9609 </span></a>            [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l9610"><span class="ln">9610 </span></a>            [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l9611"><span class="ln">9611 </span></a>            [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l9612"><span class="ln">9612 </span></a>            [1., 1., 1., 1., 1., 1., 1., 1.], 
<a name="l9613"><span class="ln">9613 </span></a>            [1., 1., 1., 1., 1., 1., 1., 1.]]) 
<a name="l9614"><span class="ln">9614 </span></a> 
<a name="l9615"><span class="ln">9615 </span></a>    &gt;&gt;&gt; b = torch.ones(8, 2) 
<a name="l9616"><span class="ln">9616 </span></a>    &gt;&gt;&gt; a.slice_scatter(b, dim=1, start=2, end=6, step=2) 
<a name="l9617"><span class="ln">9617 </span></a>    tensor([[0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l9618"><span class="ln">9618 </span></a>            [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l9619"><span class="ln">9619 </span></a>            [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l9620"><span class="ln">9620 </span></a>            [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l9621"><span class="ln">9621 </span></a>            [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l9622"><span class="ln">9622 </span></a>            [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l9623"><span class="ln">9623 </span></a>            [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l9624"><span class="ln">9624 </span></a>            [0., 0., 1., 0., 1., 0., 0., 0.]]) 
<a name="l9625"><span class="ln">9625 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9626"><span class="ln">9626 </span></a><span class="s3">)</span>
<a name="l9627"><span class="ln">9627 </span></a>
<a name="l9628"><span class="ln">9628 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9629"><span class="ln">9629 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">set_flush_denormal</span><span class="s3">,</span>
<a name="l9630"><span class="ln">9630 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9631"><span class="ln">9631 </span></a>set_flush_denormal(mode) -&gt; bool 
<a name="l9632"><span class="ln">9632 </span></a> 
<a name="l9633"><span class="ln">9633 </span></a>Disables denormal floating numbers on CPU. 
<a name="l9634"><span class="ln">9634 </span></a> 
<a name="l9635"><span class="ln">9635 </span></a>Returns ``True`` if your system supports flushing denormal numbers and it 
<a name="l9636"><span class="ln">9636 </span></a>successfully configures flush denormal mode.  :meth:`~torch.set_flush_denormal` 
<a name="l9637"><span class="ln">9637 </span></a>is supported on x86 architectures supporting SSE3 and AArch64 architecture. 
<a name="l9638"><span class="ln">9638 </span></a> 
<a name="l9639"><span class="ln">9639 </span></a>Args: 
<a name="l9640"><span class="ln">9640 </span></a>    mode (bool): Controls whether to enable flush denormal mode or not 
<a name="l9641"><span class="ln">9641 </span></a> 
<a name="l9642"><span class="ln">9642 </span></a>Example:: 
<a name="l9643"><span class="ln">9643 </span></a> 
<a name="l9644"><span class="ln">9644 </span></a>    &gt;&gt;&gt; torch.set_flush_denormal(True) 
<a name="l9645"><span class="ln">9645 </span></a>    True 
<a name="l9646"><span class="ln">9646 </span></a>    &gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64) 
<a name="l9647"><span class="ln">9647 </span></a>    tensor([ 0.], dtype=torch.float64) 
<a name="l9648"><span class="ln">9648 </span></a>    &gt;&gt;&gt; torch.set_flush_denormal(False) 
<a name="l9649"><span class="ln">9649 </span></a>    True 
<a name="l9650"><span class="ln">9650 </span></a>    &gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64) 
<a name="l9651"><span class="ln">9651 </span></a>    tensor(9.88131e-324 * 
<a name="l9652"><span class="ln">9652 </span></a>           [ 1.0000], dtype=torch.float64) 
<a name="l9653"><span class="ln">9653 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9654"><span class="ln">9654 </span></a><span class="s3">)</span>
<a name="l9655"><span class="ln">9655 </span></a>
<a name="l9656"><span class="ln">9656 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9657"><span class="ln">9657 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">set_num_threads</span><span class="s3">,</span>
<a name="l9658"><span class="ln">9658 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9659"><span class="ln">9659 </span></a>set_num_threads(int) 
<a name="l9660"><span class="ln">9660 </span></a> 
<a name="l9661"><span class="ln">9661 </span></a>Sets the number of threads used for intraop parallelism on CPU. 
<a name="l9662"><span class="ln">9662 </span></a> 
<a name="l9663"><span class="ln">9663 </span></a>.. warning:: 
<a name="l9664"><span class="ln">9664 </span></a>    To ensure that the correct number of threads is used, set_num_threads 
<a name="l9665"><span class="ln">9665 </span></a>    must be called before running eager, JIT or autograd code. 
<a name="l9666"><span class="ln">9666 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9667"><span class="ln">9667 </span></a><span class="s3">)</span>
<a name="l9668"><span class="ln">9668 </span></a>
<a name="l9669"><span class="ln">9669 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9670"><span class="ln">9670 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">set_num_interop_threads</span><span class="s3">,</span>
<a name="l9671"><span class="ln">9671 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9672"><span class="ln">9672 </span></a>set_num_interop_threads(int) 
<a name="l9673"><span class="ln">9673 </span></a> 
<a name="l9674"><span class="ln">9674 </span></a>Sets the number of threads used for interop parallelism 
<a name="l9675"><span class="ln">9675 </span></a>(e.g. in JIT interpreter) on CPU. 
<a name="l9676"><span class="ln">9676 </span></a> 
<a name="l9677"><span class="ln">9677 </span></a>.. warning:: 
<a name="l9678"><span class="ln">9678 </span></a>    Can only be called once and before any inter-op parallel work 
<a name="l9679"><span class="ln">9679 </span></a>    is started (e.g. JIT execution). 
<a name="l9680"><span class="ln">9680 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9681"><span class="ln">9681 </span></a><span class="s3">)</span>
<a name="l9682"><span class="ln">9682 </span></a>
<a name="l9683"><span class="ln">9683 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9684"><span class="ln">9684 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sigmoid</span><span class="s3">,</span>
<a name="l9685"><span class="ln">9685 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9686"><span class="ln">9686 </span></a>sigmoid(input, *, out=None) -&gt; Tensor 
<a name="l9687"><span class="ln">9687 </span></a> 
<a name="l9688"><span class="ln">9688 </span></a>Alias for :func:`torch.special.expit`. 
<a name="l9689"><span class="ln">9689 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9690"><span class="ln">9690 </span></a><span class="s3">)</span>
<a name="l9691"><span class="ln">9691 </span></a>
<a name="l9692"><span class="ln">9692 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9693"><span class="ln">9693 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logit</span><span class="s3">,</span>
<a name="l9694"><span class="ln">9694 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9695"><span class="ln">9695 </span></a>logit(input, eps=None, *, out=None) -&gt; Tensor 
<a name="l9696"><span class="ln">9696 </span></a> 
<a name="l9697"><span class="ln">9697 </span></a>Alias for :func:`torch.special.logit`. 
<a name="l9698"><span class="ln">9698 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9699"><span class="ln">9699 </span></a><span class="s3">)</span>
<a name="l9700"><span class="ln">9700 </span></a>
<a name="l9701"><span class="ln">9701 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9702"><span class="ln">9702 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sign</span><span class="s3">,</span>
<a name="l9703"><span class="ln">9703 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9704"><span class="ln">9704 </span></a>sign(input, *, out=None) -&gt; Tensor 
<a name="l9705"><span class="ln">9705 </span></a> 
<a name="l9706"><span class="ln">9706 </span></a>Returns a new tensor with the signs of the elements of :attr:`input`. 
<a name="l9707"><span class="ln">9707 </span></a> 
<a name="l9708"><span class="ln">9708 </span></a>.. math:: 
<a name="l9709"><span class="ln">9709 </span></a>    \text{out}_{i} = \operatorname{sgn}(\text{input}_{i}) 
<a name="l9710"><span class="ln">9710 </span></a>&quot;&quot;&quot;</span>
<a name="l9711"><span class="ln">9711 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9712"><span class="ln">9712 </span></a>Args: 
<a name="l9713"><span class="ln">9713 </span></a>    {input} 
<a name="l9714"><span class="ln">9714 </span></a> 
<a name="l9715"><span class="ln">9715 </span></a>Keyword args: 
<a name="l9716"><span class="ln">9716 </span></a>    {out} 
<a name="l9717"><span class="ln">9717 </span></a> 
<a name="l9718"><span class="ln">9718 </span></a>Example:: 
<a name="l9719"><span class="ln">9719 </span></a> 
<a name="l9720"><span class="ln">9720 </span></a>    &gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3]) 
<a name="l9721"><span class="ln">9721 </span></a>    &gt;&gt;&gt; a 
<a name="l9722"><span class="ln">9722 </span></a>    tensor([ 0.7000, -1.2000,  0.0000,  2.3000]) 
<a name="l9723"><span class="ln">9723 </span></a>    &gt;&gt;&gt; torch.sign(a) 
<a name="l9724"><span class="ln">9724 </span></a>    tensor([ 1., -1.,  0.,  1.]) 
<a name="l9725"><span class="ln">9725 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9726"><span class="ln">9726 </span></a><span class="s3">)</span>
<a name="l9727"><span class="ln">9727 </span></a>
<a name="l9728"><span class="ln">9728 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9729"><span class="ln">9729 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">signbit</span><span class="s3">,</span>
<a name="l9730"><span class="ln">9730 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9731"><span class="ln">9731 </span></a>signbit(input, *, out=None) -&gt; Tensor 
<a name="l9732"><span class="ln">9732 </span></a> 
<a name="l9733"><span class="ln">9733 </span></a>Tests if each element of :attr:`input` has its sign bit set or not. 
<a name="l9734"><span class="ln">9734 </span></a> 
<a name="l9735"><span class="ln">9735 </span></a>Args: 
<a name="l9736"><span class="ln">9736 </span></a>  {input} 
<a name="l9737"><span class="ln">9737 </span></a> 
<a name="l9738"><span class="ln">9738 </span></a>Keyword args: 
<a name="l9739"><span class="ln">9739 </span></a>  {out} 
<a name="l9740"><span class="ln">9740 </span></a> 
<a name="l9741"><span class="ln">9741 </span></a>Example:: 
<a name="l9742"><span class="ln">9742 </span></a> 
<a name="l9743"><span class="ln">9743 </span></a>    &gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3]) 
<a name="l9744"><span class="ln">9744 </span></a>    &gt;&gt;&gt; torch.signbit(a) 
<a name="l9745"><span class="ln">9745 </span></a>    tensor([ False, True,  False,  False]) 
<a name="l9746"><span class="ln">9746 </span></a>    &gt;&gt;&gt; a = torch.tensor([-0.0, 0.0]) 
<a name="l9747"><span class="ln">9747 </span></a>    &gt;&gt;&gt; torch.signbit(a) 
<a name="l9748"><span class="ln">9748 </span></a>    tensor([ True,  False]) 
<a name="l9749"><span class="ln">9749 </span></a> 
<a name="l9750"><span class="ln">9750 </span></a>.. note:: 
<a name="l9751"><span class="ln">9751 </span></a>    signbit handles signed zeros, so negative zero (-0) returns True. 
<a name="l9752"><span class="ln">9752 </span></a> 
<a name="l9753"><span class="ln">9753 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9754"><span class="ln">9754 </span></a><span class="s3">)</span>
<a name="l9755"><span class="ln">9755 </span></a>
<a name="l9756"><span class="ln">9756 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9757"><span class="ln">9757 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sgn</span><span class="s3">,</span>
<a name="l9758"><span class="ln">9758 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9759"><span class="ln">9759 </span></a>sgn(input, *, out=None) -&gt; Tensor 
<a name="l9760"><span class="ln">9760 </span></a> 
<a name="l9761"><span class="ln">9761 </span></a>This function is an extension of torch.sign() to complex tensors. 
<a name="l9762"><span class="ln">9762 </span></a>It computes a new tensor whose elements have 
<a name="l9763"><span class="ln">9763 </span></a>the same angles as the corresponding elements of :attr:`input` and 
<a name="l9764"><span class="ln">9764 </span></a>absolute values (i.e. magnitudes) of one for complex tensors and 
<a name="l9765"><span class="ln">9765 </span></a>is equivalent to torch.sign() for non-complex tensors. 
<a name="l9766"><span class="ln">9766 </span></a> 
<a name="l9767"><span class="ln">9767 </span></a>.. math:: 
<a name="l9768"><span class="ln">9768 </span></a>    \text{out}_{i} = \begin{cases} 
<a name="l9769"><span class="ln">9769 </span></a>                    0 &amp; |\text{{input}}_i| == 0 \\ 
<a name="l9770"><span class="ln">9770 </span></a>                    \frac{{\text{{input}}_i}}{|{\text{{input}}_i}|} &amp; \text{otherwise} 
<a name="l9771"><span class="ln">9771 </span></a>                    \end{cases} 
<a name="l9772"><span class="ln">9772 </span></a> 
<a name="l9773"><span class="ln">9773 </span></a>&quot;&quot;&quot;</span>
<a name="l9774"><span class="ln">9774 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9775"><span class="ln">9775 </span></a>Args: 
<a name="l9776"><span class="ln">9776 </span></a>    {input} 
<a name="l9777"><span class="ln">9777 </span></a> 
<a name="l9778"><span class="ln">9778 </span></a>Keyword args: 
<a name="l9779"><span class="ln">9779 </span></a>  {out} 
<a name="l9780"><span class="ln">9780 </span></a> 
<a name="l9781"><span class="ln">9781 </span></a>Example:: 
<a name="l9782"><span class="ln">9782 </span></a> 
<a name="l9783"><span class="ln">9783 </span></a>    &gt;&gt;&gt; t = torch.tensor([3+4j, 7-24j, 0, 1+2j]) 
<a name="l9784"><span class="ln">9784 </span></a>    &gt;&gt;&gt; t.sgn() 
<a name="l9785"><span class="ln">9785 </span></a>    tensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j]) 
<a name="l9786"><span class="ln">9786 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9787"><span class="ln">9787 </span></a><span class="s3">)</span>
<a name="l9788"><span class="ln">9788 </span></a>
<a name="l9789"><span class="ln">9789 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9790"><span class="ln">9790 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sin</span><span class="s3">,</span>
<a name="l9791"><span class="ln">9791 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9792"><span class="ln">9792 </span></a>sin(input, *, out=None) -&gt; Tensor 
<a name="l9793"><span class="ln">9793 </span></a> 
<a name="l9794"><span class="ln">9794 </span></a>Returns a new tensor with the sine of the elements of :attr:`input`. 
<a name="l9795"><span class="ln">9795 </span></a> 
<a name="l9796"><span class="ln">9796 </span></a>.. math:: 
<a name="l9797"><span class="ln">9797 </span></a>    \text{out}_{i} = \sin(\text{input}_{i}) 
<a name="l9798"><span class="ln">9798 </span></a>&quot;&quot;&quot;</span>
<a name="l9799"><span class="ln">9799 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9800"><span class="ln">9800 </span></a>Args: 
<a name="l9801"><span class="ln">9801 </span></a>    {input} 
<a name="l9802"><span class="ln">9802 </span></a> 
<a name="l9803"><span class="ln">9803 </span></a>Keyword args: 
<a name="l9804"><span class="ln">9804 </span></a>    {out} 
<a name="l9805"><span class="ln">9805 </span></a> 
<a name="l9806"><span class="ln">9806 </span></a>Example:: 
<a name="l9807"><span class="ln">9807 </span></a> 
<a name="l9808"><span class="ln">9808 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l9809"><span class="ln">9809 </span></a>    &gt;&gt;&gt; a 
<a name="l9810"><span class="ln">9810 </span></a>    tensor([-0.5461,  0.1347, -2.7266, -0.2746]) 
<a name="l9811"><span class="ln">9811 </span></a>    &gt;&gt;&gt; torch.sin(a) 
<a name="l9812"><span class="ln">9812 </span></a>    tensor([-0.5194,  0.1343, -0.4032, -0.2711]) 
<a name="l9813"><span class="ln">9813 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9814"><span class="ln">9814 </span></a><span class="s3">)</span>
<a name="l9815"><span class="ln">9815 </span></a>
<a name="l9816"><span class="ln">9816 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9817"><span class="ln">9817 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sinc</span><span class="s3">,</span>
<a name="l9818"><span class="ln">9818 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9819"><span class="ln">9819 </span></a>sinc(input, *, out=None) -&gt; Tensor 
<a name="l9820"><span class="ln">9820 </span></a> 
<a name="l9821"><span class="ln">9821 </span></a>Alias for :func:`torch.special.sinc`. 
<a name="l9822"><span class="ln">9822 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l9823"><span class="ln">9823 </span></a><span class="s3">)</span>
<a name="l9824"><span class="ln">9824 </span></a>
<a name="l9825"><span class="ln">9825 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9826"><span class="ln">9826 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sinh</span><span class="s3">,</span>
<a name="l9827"><span class="ln">9827 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9828"><span class="ln">9828 </span></a>sinh(input, *, out=None) -&gt; Tensor 
<a name="l9829"><span class="ln">9829 </span></a> 
<a name="l9830"><span class="ln">9830 </span></a>Returns a new tensor with the hyperbolic sine of the elements of 
<a name="l9831"><span class="ln">9831 </span></a>:attr:`input`. 
<a name="l9832"><span class="ln">9832 </span></a> 
<a name="l9833"><span class="ln">9833 </span></a>.. math:: 
<a name="l9834"><span class="ln">9834 </span></a>    \text{out}_{i} = \sinh(\text{input}_{i}) 
<a name="l9835"><span class="ln">9835 </span></a>&quot;&quot;&quot;</span>
<a name="l9836"><span class="ln">9836 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l9837"><span class="ln">9837 </span></a>Args: 
<a name="l9838"><span class="ln">9838 </span></a>    {input} 
<a name="l9839"><span class="ln">9839 </span></a> 
<a name="l9840"><span class="ln">9840 </span></a>Keyword args: 
<a name="l9841"><span class="ln">9841 </span></a>    {out} 
<a name="l9842"><span class="ln">9842 </span></a> 
<a name="l9843"><span class="ln">9843 </span></a>Example:: 
<a name="l9844"><span class="ln">9844 </span></a> 
<a name="l9845"><span class="ln">9845 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l9846"><span class="ln">9846 </span></a>    &gt;&gt;&gt; a 
<a name="l9847"><span class="ln">9847 </span></a>    tensor([ 0.5380, -0.8632, -0.1265,  0.9399]) 
<a name="l9848"><span class="ln">9848 </span></a>    &gt;&gt;&gt; torch.sinh(a) 
<a name="l9849"><span class="ln">9849 </span></a>    tensor([ 0.5644, -0.9744, -0.1268,  1.0845]) 
<a name="l9850"><span class="ln">9850 </span></a> 
<a name="l9851"><span class="ln">9851 </span></a>.. note:: 
<a name="l9852"><span class="ln">9852 </span></a>   When :attr:`input` is on the CPU, the implementation of torch.sinh may use 
<a name="l9853"><span class="ln">9853 </span></a>   the Sleef library, which rounds very large results to infinity or negative 
<a name="l9854"><span class="ln">9854 </span></a>   infinity. See `here &lt;https://sleef.org/purec.xhtml&gt;`_ for details. 
<a name="l9855"><span class="ln">9855 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9856"><span class="ln">9856 </span></a><span class="s3">)</span>
<a name="l9857"><span class="ln">9857 </span></a>
<a name="l9858"><span class="ln">9858 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9859"><span class="ln">9859 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sort</span><span class="s3">,</span>
<a name="l9860"><span class="ln">9860 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9861"><span class="ln">9861 </span></a>sort(input, dim=-1, descending=False, stable=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l9862"><span class="ln">9862 </span></a> 
<a name="l9863"><span class="ln">9863 </span></a>Sorts the elements of the :attr:`input` tensor along a given dimension 
<a name="l9864"><span class="ln">9864 </span></a>in ascending order by value. 
<a name="l9865"><span class="ln">9865 </span></a> 
<a name="l9866"><span class="ln">9866 </span></a>If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l9867"><span class="ln">9867 </span></a> 
<a name="l9868"><span class="ln">9868 </span></a>If :attr:`descending` is ``True`` then the elements are sorted in descending 
<a name="l9869"><span class="ln">9869 </span></a>order by value. 
<a name="l9870"><span class="ln">9870 </span></a> 
<a name="l9871"><span class="ln">9871 </span></a>If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l9872"><span class="ln">9872 </span></a>the order of equivalent elements. 
<a name="l9873"><span class="ln">9873 </span></a> 
<a name="l9874"><span class="ln">9874 </span></a>A namedtuple of (values, indices) is returned, where the `values` are the 
<a name="l9875"><span class="ln">9875 </span></a>sorted values and `indices` are the indices of the elements in the original 
<a name="l9876"><span class="ln">9876 </span></a>`input` tensor. 
<a name="l9877"><span class="ln">9877 </span></a> 
<a name="l9878"><span class="ln">9878 </span></a>Args: 
<a name="l9879"><span class="ln">9879 </span></a>    {input} 
<a name="l9880"><span class="ln">9880 </span></a>    dim (int, optional): the dimension to sort along 
<a name="l9881"><span class="ln">9881 </span></a>    descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l9882"><span class="ln">9882 </span></a>    stable (bool, optional): makes the sorting routine stable, which guarantees that the order 
<a name="l9883"><span class="ln">9883 </span></a>       of equivalent elements is preserved. 
<a name="l9884"><span class="ln">9884 </span></a> 
<a name="l9885"><span class="ln">9885 </span></a>Keyword args: 
<a name="l9886"><span class="ln">9886 </span></a>    out (tuple, optional): the output tuple of (`Tensor`, `LongTensor`) that can 
<a name="l9887"><span class="ln">9887 </span></a>        be optionally given to be used as output buffers 
<a name="l9888"><span class="ln">9888 </span></a> 
<a name="l9889"><span class="ln">9889 </span></a>Example:: 
<a name="l9890"><span class="ln">9890 </span></a> 
<a name="l9891"><span class="ln">9891 </span></a>    &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l9892"><span class="ln">9892 </span></a>    &gt;&gt;&gt; sorted, indices = torch.sort(x) 
<a name="l9893"><span class="ln">9893 </span></a>    &gt;&gt;&gt; sorted 
<a name="l9894"><span class="ln">9894 </span></a>    tensor([[-0.2162,  0.0608,  0.6719,  2.3332], 
<a name="l9895"><span class="ln">9895 </span></a>            [-0.5793,  0.0061,  0.6058,  0.9497], 
<a name="l9896"><span class="ln">9896 </span></a>            [-0.5071,  0.3343,  0.9553,  1.0960]]) 
<a name="l9897"><span class="ln">9897 </span></a>    &gt;&gt;&gt; indices 
<a name="l9898"><span class="ln">9898 </span></a>    tensor([[ 1,  0,  2,  3], 
<a name="l9899"><span class="ln">9899 </span></a>            [ 3,  1,  0,  2], 
<a name="l9900"><span class="ln">9900 </span></a>            [ 0,  3,  1,  2]]) 
<a name="l9901"><span class="ln">9901 </span></a> 
<a name="l9902"><span class="ln">9902 </span></a>    &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) 
<a name="l9903"><span class="ln">9903 </span></a>    &gt;&gt;&gt; sorted 
<a name="l9904"><span class="ln">9904 </span></a>    tensor([[-0.5071, -0.2162,  0.6719, -0.5793], 
<a name="l9905"><span class="ln">9905 </span></a>            [ 0.0608,  0.0061,  0.9497,  0.3343], 
<a name="l9906"><span class="ln">9906 </span></a>            [ 0.6058,  0.9553,  1.0960,  2.3332]]) 
<a name="l9907"><span class="ln">9907 </span></a>    &gt;&gt;&gt; indices 
<a name="l9908"><span class="ln">9908 </span></a>    tensor([[ 2,  0,  0,  1], 
<a name="l9909"><span class="ln">9909 </span></a>            [ 0,  1,  1,  2], 
<a name="l9910"><span class="ln">9910 </span></a>            [ 1,  2,  2,  0]]) 
<a name="l9911"><span class="ln">9911 </span></a>    &gt;&gt;&gt; x = torch.tensor([0, 1] * 9) 
<a name="l9912"><span class="ln">9912 </span></a>    &gt;&gt;&gt; x.sort() 
<a name="l9913"><span class="ln">9913 </span></a>    torch.return_types.sort( 
<a name="l9914"><span class="ln">9914 </span></a>        values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l9915"><span class="ln">9915 </span></a>        indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1])) 
<a name="l9916"><span class="ln">9916 </span></a>    &gt;&gt;&gt; x.sort(stable=True) 
<a name="l9917"><span class="ln">9917 </span></a>    torch.return_types.sort( 
<a name="l9918"><span class="ln">9918 </span></a>        values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l9919"><span class="ln">9919 </span></a>        indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17])) 
<a name="l9920"><span class="ln">9920 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9921"><span class="ln">9921 </span></a><span class="s3">)</span>
<a name="l9922"><span class="ln">9922 </span></a>
<a name="l9923"><span class="ln">9923 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9924"><span class="ln">9924 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">argsort</span><span class="s3">,</span>
<a name="l9925"><span class="ln">9925 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9926"><span class="ln">9926 </span></a>argsort(input, dim=-1, descending=False, stable=False) -&gt; Tensor 
<a name="l9927"><span class="ln">9927 </span></a> 
<a name="l9928"><span class="ln">9928 </span></a>Returns the indices that sort a tensor along a given dimension in ascending 
<a name="l9929"><span class="ln">9929 </span></a>order by value. 
<a name="l9930"><span class="ln">9930 </span></a> 
<a name="l9931"><span class="ln">9931 </span></a>This is the second value returned by :meth:`torch.sort`.  See its documentation 
<a name="l9932"><span class="ln">9932 </span></a>for the exact semantics of this method. 
<a name="l9933"><span class="ln">9933 </span></a> 
<a name="l9934"><span class="ln">9934 </span></a>If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l9935"><span class="ln">9935 </span></a>the order of equivalent elements. If ``False``, the relative order of values 
<a name="l9936"><span class="ln">9936 </span></a>which compare equal is not guaranteed. ``True`` is slower. 
<a name="l9937"><span class="ln">9937 </span></a> 
<a name="l9938"><span class="ln">9938 </span></a>Args: 
<a name="l9939"><span class="ln">9939 </span></a>    {input} 
<a name="l9940"><span class="ln">9940 </span></a>    dim (int, optional): the dimension to sort along 
<a name="l9941"><span class="ln">9941 </span></a>    descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l9942"><span class="ln">9942 </span></a>    stable (bool, optional): controls the relative order of equivalent elements 
<a name="l9943"><span class="ln">9943 </span></a> 
<a name="l9944"><span class="ln">9944 </span></a>Example:: 
<a name="l9945"><span class="ln">9945 </span></a> 
<a name="l9946"><span class="ln">9946 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l9947"><span class="ln">9947 </span></a>    &gt;&gt;&gt; a 
<a name="l9948"><span class="ln">9948 </span></a>    tensor([[ 0.0785,  1.5267, -0.8521,  0.4065], 
<a name="l9949"><span class="ln">9949 </span></a>            [ 0.1598,  0.0788, -0.0745, -1.2700], 
<a name="l9950"><span class="ln">9950 </span></a>            [ 1.2208,  1.0722, -0.7064,  1.2564], 
<a name="l9951"><span class="ln">9951 </span></a>            [ 0.0669, -0.2318, -0.8229, -0.9280]]) 
<a name="l9952"><span class="ln">9952 </span></a> 
<a name="l9953"><span class="ln">9953 </span></a> 
<a name="l9954"><span class="ln">9954 </span></a>    &gt;&gt;&gt; torch.argsort(a, dim=1) 
<a name="l9955"><span class="ln">9955 </span></a>    tensor([[2, 0, 3, 1], 
<a name="l9956"><span class="ln">9956 </span></a>            [3, 2, 1, 0], 
<a name="l9957"><span class="ln">9957 </span></a>            [2, 1, 0, 3], 
<a name="l9958"><span class="ln">9958 </span></a>            [3, 2, 1, 0]]) 
<a name="l9959"><span class="ln">9959 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9960"><span class="ln">9960 </span></a><span class="s3">)</span>
<a name="l9961"><span class="ln">9961 </span></a>
<a name="l9962"><span class="ln">9962 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9963"><span class="ln">9963 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">msort</span><span class="s3">,</span>
<a name="l9964"><span class="ln">9964 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l9965"><span class="ln">9965 </span></a>msort(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l9966"><span class="ln">9966 </span></a> 
<a name="l9967"><span class="ln">9967 </span></a>Sorts the elements of the :attr:`input` tensor along its first dimension 
<a name="l9968"><span class="ln">9968 </span></a>in ascending order by value. 
<a name="l9969"><span class="ln">9969 </span></a> 
<a name="l9970"><span class="ln">9970 </span></a>.. note:: `torch.msort(t)` is equivalent to `torch.sort(t, dim=0)[0]`. 
<a name="l9971"><span class="ln">9971 </span></a>          See also :func:`torch.sort`. 
<a name="l9972"><span class="ln">9972 </span></a> 
<a name="l9973"><span class="ln">9973 </span></a>Args: 
<a name="l9974"><span class="ln">9974 </span></a>    {input} 
<a name="l9975"><span class="ln">9975 </span></a> 
<a name="l9976"><span class="ln">9976 </span></a>Keyword args: 
<a name="l9977"><span class="ln">9977 </span></a>    {out} 
<a name="l9978"><span class="ln">9978 </span></a> 
<a name="l9979"><span class="ln">9979 </span></a>Example:: 
<a name="l9980"><span class="ln">9980 </span></a> 
<a name="l9981"><span class="ln">9981 </span></a>    &gt;&gt;&gt; t = torch.randn(3, 4) 
<a name="l9982"><span class="ln">9982 </span></a>    &gt;&gt;&gt; t 
<a name="l9983"><span class="ln">9983 </span></a>    tensor([[-0.1321,  0.4370, -1.2631, -1.1289], 
<a name="l9984"><span class="ln">9984 </span></a>            [-2.0527, -1.1250,  0.2275,  0.3077], 
<a name="l9985"><span class="ln">9985 </span></a>            [-0.0881, -0.1259, -0.5495,  1.0284]]) 
<a name="l9986"><span class="ln">9986 </span></a>    &gt;&gt;&gt; torch.msort(t) 
<a name="l9987"><span class="ln">9987 </span></a>    tensor([[-2.0527, -1.1250, -1.2631, -1.1289], 
<a name="l9988"><span class="ln">9988 </span></a>            [-0.1321, -0.1259, -0.5495,  0.3077], 
<a name="l9989"><span class="ln">9989 </span></a>            [-0.0881,  0.4370,  0.2275,  1.0284]]) 
<a name="l9990"><span class="ln">9990 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l9991"><span class="ln">9991 </span></a><span class="s3">)</span>
<a name="l9992"><span class="ln">9992 </span></a>
<a name="l9993"><span class="ln">9993 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l9994"><span class="ln">9994 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sparse_compressed_tensor</span><span class="s3">,</span>
<a name="l9995"><span class="ln">9995 </span></a>    <span class="s4">r&quot;&quot;&quot;sparse_compressed_tensor(compressed_indices, plain_indices, values, size=None, &quot;&quot;&quot;</span>
<a name="l9996"><span class="ln">9996 </span></a>    <span class="s4">r&quot;&quot;&quot;*, dtype=None, layout=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l9997"><span class="ln">9997 </span></a> 
<a name="l9998"><span class="ln">9998 </span></a>Constructs a :ref:`sparse tensor in Compressed Sparse format - CSR, 
<a name="l9999"><span class="ln">9999 </span></a>CSC, BSR, or BSC - &lt;sparse-compressed-docs&gt;` with specified values at 
<a name="l10000"><span class="ln">10000 </span></a>the given :attr:`compressed_indices` and :attr:`plain_indices`. Sparse 
<a name="l10001"><span class="ln">10001 </span></a>matrix multiplication operations in Compressed Sparse format are 
<a name="l10002"><span class="ln">10002 </span></a>typically faster than that for sparse tensors in COO format. Make you 
<a name="l10003"><span class="ln">10003 </span></a>have a look at :ref:`the note on the data type of the indices 
<a name="l10004"><span class="ln">10004 </span></a>&lt;sparse-compressed-docs&gt;`. 
<a name="l10005"><span class="ln">10005 </span></a> 
<a name="l10006"><span class="ln">10006 </span></a>{sparse_factory_device_note} 
<a name="l10007"><span class="ln">10007 </span></a> 
<a name="l10008"><span class="ln">10008 </span></a>Args: 
<a name="l10009"><span class="ln">10009 </span></a>    compressed_indices (array_like): (B+1)-dimensional array of size 
<a name="l10010"><span class="ln">10010 </span></a>        ``(*batchsize, compressed_dim_size + 1)``.  The last element of 
<a name="l10011"><span class="ln">10011 </span></a>        each batch is the number of non-zero elements or blocks. This 
<a name="l10012"><span class="ln">10012 </span></a>        tensor encodes the index in ``values`` and ``plain_indices`` 
<a name="l10013"><span class="ln">10013 </span></a>        depending on where the given compressed dimension (row or 
<a name="l10014"><span class="ln">10014 </span></a>        column) starts. Each successive number in the tensor 
<a name="l10015"><span class="ln">10015 </span></a>        subtracted by the number before it denotes the number of 
<a name="l10016"><span class="ln">10016 </span></a>        elements or blocks in a given compressed dimension. 
<a name="l10017"><span class="ln">10017 </span></a>    plain_indices (array_like): Plain dimension (column or row) 
<a name="l10018"><span class="ln">10018 </span></a>        co-ordinates of each element or block in values. (B+1)-dimensional 
<a name="l10019"><span class="ln">10019 </span></a>        tensor with the same length as values. 
<a name="l10020"><span class="ln">10020 </span></a> 
<a name="l10021"><span class="ln">10021 </span></a>    values (array_list): Initial values for the tensor. Can be a list, 
<a name="l10022"><span class="ln">10022 </span></a>        tuple, NumPy ``ndarray``, scalar, and other types.  that 
<a name="l10023"><span class="ln">10023 </span></a>        represents a (1+K)-dimensional (for CSR and CSC layouts) or 
<a name="l10024"><span class="ln">10024 </span></a>        (1+2+K)-dimensional tensor (for BSR and BSC layouts) where 
<a name="l10025"><span class="ln">10025 </span></a>        ``K`` is the number of dense dimensions. 
<a name="l10026"><span class="ln">10026 </span></a>    size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l10027"><span class="ln">10027 </span></a>        sparse tensor: ``(*batchsize, nrows * blocksize[0], ncols * 
<a name="l10028"><span class="ln">10028 </span></a>        blocksize[1], *densesize)`` where ``blocksize[0] == 
<a name="l10029"><span class="ln">10029 </span></a>        blocksize[1] == 1`` for CSR and CSC formats. If not provided, 
<a name="l10030"><span class="ln">10030 </span></a>        the size will be inferred as the minimum size big enough to 
<a name="l10031"><span class="ln">10031 </span></a>        hold all non-zero elements or blocks. 
<a name="l10032"><span class="ln">10032 </span></a> 
<a name="l10033"><span class="ln">10033 </span></a>Keyword args: 
<a name="l10034"><span class="ln">10034 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l10035"><span class="ln">10035 </span></a>        returned tensor.  Default: if None, infers data type from 
<a name="l10036"><span class="ln">10036 </span></a>        :attr:`values`. 
<a name="l10037"><span class="ln">10037 </span></a>    layout (:class:`torch.layout`, required): the desired layout of 
<a name="l10038"><span class="ln">10038 </span></a>        returned tensor: :attr:`torch.sparse_csr`, 
<a name="l10039"><span class="ln">10039 </span></a>        :attr:`torch.sparse_csc`, :attr:`torch.sparse_bsr`, or 
<a name="l10040"><span class="ln">10040 </span></a>        :attr:`torch.sparse_bsc`. 
<a name="l10041"><span class="ln">10041 </span></a>    device (:class:`torch.device`, optional): the desired device of 
<a name="l10042"><span class="ln">10042 </span></a>        returned tensor.  Default: if None, uses the current device 
<a name="l10043"><span class="ln">10043 </span></a>        for the default tensor type (see 
<a name="l10044"><span class="ln">10044 </span></a>        :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l10045"><span class="ln">10045 </span></a>        the CPU for CPU tensor types and the current CUDA device for 
<a name="l10046"><span class="ln">10046 </span></a>        CUDA tensor types. 
<a name="l10047"><span class="ln">10047 </span></a>    {pin_memory} 
<a name="l10048"><span class="ln">10048 </span></a>    {requires_grad} 
<a name="l10049"><span class="ln">10049 </span></a>    {check_invariants} 
<a name="l10050"><span class="ln">10050 </span></a> 
<a name="l10051"><span class="ln">10051 </span></a>Example:: 
<a name="l10052"><span class="ln">10052 </span></a> 
<a name="l10053"><span class="ln">10053 </span></a>    &gt;&gt;&gt; compressed_indices = [0, 2, 4] 
<a name="l10054"><span class="ln">10054 </span></a>    &gt;&gt;&gt; plain_indices = [0, 1, 0, 1] 
<a name="l10055"><span class="ln">10055 </span></a>    &gt;&gt;&gt; values = [1, 2, 3, 4] 
<a name="l10056"><span class="ln">10056 </span></a>    &gt;&gt;&gt; torch.sparse_compressed_tensor(torch.tensor(compressed_indices, dtype=torch.int64), 
<a name="l10057"><span class="ln">10057 </span></a>    ...                                torch.tensor(plain_indices, dtype=torch.int64), 
<a name="l10058"><span class="ln">10058 </span></a>    ...                                torch.tensor(values), dtype=torch.double, layout=torch.sparse_csr) 
<a name="l10059"><span class="ln">10059 </span></a>    tensor(crow_indices=tensor([0, 2, 4]), 
<a name="l10060"><span class="ln">10060 </span></a>           col_indices=tensor([0, 1, 0, 1]), 
<a name="l10061"><span class="ln">10061 </span></a>           values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, 
<a name="l10062"><span class="ln">10062 </span></a>           dtype=torch.float64, layout=torch.sparse_csr) 
<a name="l10063"><span class="ln">10063 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l10064"><span class="ln">10064 </span></a><span class="s3">)</span>
<a name="l10065"><span class="ln">10065 </span></a>
<a name="l10066"><span class="ln">10066 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10067"><span class="ln">10067 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sparse_csr_tensor</span><span class="s3">,</span>
<a name="l10068"><span class="ln">10068 </span></a>    <span class="s4">r&quot;&quot;&quot;sparse_csr_tensor(crow_indices, col_indices, values, size=None, &quot;&quot;&quot;</span>
<a name="l10069"><span class="ln">10069 </span></a>    <span class="s4">r&quot;&quot;&quot;*, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l10070"><span class="ln">10070 </span></a> 
<a name="l10071"><span class="ln">10071 </span></a>Constructs a :ref:`sparse tensor in CSR (Compressed Sparse Row) &lt;sparse-csr-docs&gt;` with specified 
<a name="l10072"><span class="ln">10072 </span></a>values at the given :attr:`crow_indices` and :attr:`col_indices`. Sparse matrix multiplication operations 
<a name="l10073"><span class="ln">10073 </span></a>in CSR format are typically faster than that for sparse tensors in COO format. Make you have a look 
<a name="l10074"><span class="ln">10074 </span></a>at :ref:`the note on the data type of the indices &lt;sparse-csr-docs&gt;`. 
<a name="l10075"><span class="ln">10075 </span></a> 
<a name="l10076"><span class="ln">10076 </span></a>{sparse_factory_device_note} 
<a name="l10077"><span class="ln">10077 </span></a> 
<a name="l10078"><span class="ln">10078 </span></a>Args: 
<a name="l10079"><span class="ln">10079 </span></a>    crow_indices (array_like): (B+1)-dimensional array of size 
<a name="l10080"><span class="ln">10080 </span></a>        ``(*batchsize, nrows + 1)``.  The last element of each batch 
<a name="l10081"><span class="ln">10081 </span></a>        is the number of non-zeros. This tensor encodes the index in 
<a name="l10082"><span class="ln">10082 </span></a>        values and col_indices depending on where the given row 
<a name="l10083"><span class="ln">10083 </span></a>        starts. Each successive number in the tensor subtracted by the 
<a name="l10084"><span class="ln">10084 </span></a>        number before it denotes the number of elements in a given 
<a name="l10085"><span class="ln">10085 </span></a>        row. 
<a name="l10086"><span class="ln">10086 </span></a>    col_indices (array_like): Column co-ordinates of each element in 
<a name="l10087"><span class="ln">10087 </span></a>        values. (B+1)-dimensional tensor with the same length 
<a name="l10088"><span class="ln">10088 </span></a>        as values. 
<a name="l10089"><span class="ln">10089 </span></a>    values (array_list): Initial values for the tensor. Can be a list, 
<a name="l10090"><span class="ln">10090 </span></a>        tuple, NumPy ``ndarray``, scalar, and other types that 
<a name="l10091"><span class="ln">10091 </span></a>        represents a (1+K)-dimensional tensor where ``K`` is the number 
<a name="l10092"><span class="ln">10092 </span></a>        of dense dimensions. 
<a name="l10093"><span class="ln">10093 </span></a>    size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l10094"><span class="ln">10094 </span></a>        sparse tensor: ``(*batchsize, nrows, ncols, *densesize)``. If 
<a name="l10095"><span class="ln">10095 </span></a>        not provided, the size will be inferred as the minimum size 
<a name="l10096"><span class="ln">10096 </span></a>        big enough to hold all non-zero elements. 
<a name="l10097"><span class="ln">10097 </span></a> 
<a name="l10098"><span class="ln">10098 </span></a>Keyword args: 
<a name="l10099"><span class="ln">10099 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l10100"><span class="ln">10100 </span></a>        returned tensor.  Default: if None, infers data type from 
<a name="l10101"><span class="ln">10101 </span></a>        :attr:`values`. 
<a name="l10102"><span class="ln">10102 </span></a>    device (:class:`torch.device`, optional): the desired device of 
<a name="l10103"><span class="ln">10103 </span></a>        returned tensor.  Default: if None, uses the current device 
<a name="l10104"><span class="ln">10104 </span></a>        for the default tensor type (see 
<a name="l10105"><span class="ln">10105 </span></a>        :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l10106"><span class="ln">10106 </span></a>        the CPU for CPU tensor types and the current CUDA device for 
<a name="l10107"><span class="ln">10107 </span></a>        CUDA tensor types. 
<a name="l10108"><span class="ln">10108 </span></a>    {pin_memory} 
<a name="l10109"><span class="ln">10109 </span></a>    {requires_grad} 
<a name="l10110"><span class="ln">10110 </span></a>    {check_invariants} 
<a name="l10111"><span class="ln">10111 </span></a> 
<a name="l10112"><span class="ln">10112 </span></a>Example:: 
<a name="l10113"><span class="ln">10113 </span></a> 
<a name="l10114"><span class="ln">10114 </span></a>    &gt;&gt;&gt; crow_indices = [0, 2, 4] 
<a name="l10115"><span class="ln">10115 </span></a>    &gt;&gt;&gt; col_indices = [0, 1, 0, 1] 
<a name="l10116"><span class="ln">10116 </span></a>    &gt;&gt;&gt; values = [1, 2, 3, 4] 
<a name="l10117"><span class="ln">10117 </span></a>    &gt;&gt;&gt; torch.sparse_csr_tensor(torch.tensor(crow_indices, dtype=torch.int64), 
<a name="l10118"><span class="ln">10118 </span></a>    ...                         torch.tensor(col_indices, dtype=torch.int64), 
<a name="l10119"><span class="ln">10119 </span></a>    ...                         torch.tensor(values), dtype=torch.double) 
<a name="l10120"><span class="ln">10120 </span></a>    tensor(crow_indices=tensor([0, 2, 4]), 
<a name="l10121"><span class="ln">10121 </span></a>           col_indices=tensor([0, 1, 0, 1]), 
<a name="l10122"><span class="ln">10122 </span></a>           values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, 
<a name="l10123"><span class="ln">10123 </span></a>           dtype=torch.float64, layout=torch.sparse_csr) 
<a name="l10124"><span class="ln">10124 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l10125"><span class="ln">10125 </span></a><span class="s3">)</span>
<a name="l10126"><span class="ln">10126 </span></a>
<a name="l10127"><span class="ln">10127 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10128"><span class="ln">10128 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sparse_csc_tensor</span><span class="s3">,</span>
<a name="l10129"><span class="ln">10129 </span></a>    <span class="s4">r&quot;&quot;&quot;sparse_csc_tensor(ccol_indices, row_indices, values, size=None, &quot;&quot;&quot;</span>
<a name="l10130"><span class="ln">10130 </span></a>    <span class="s4">r&quot;&quot;&quot;*, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l10131"><span class="ln">10131 </span></a> 
<a name="l10132"><span class="ln">10132 </span></a>Constructs a :ref:`sparse tensor in CSC (Compressed Sparse Column) 
<a name="l10133"><span class="ln">10133 </span></a>&lt;sparse-csc-docs&gt;` with specified values at the given 
<a name="l10134"><span class="ln">10134 </span></a>:attr:`ccol_indices` and :attr:`row_indices`. Sparse matrix 
<a name="l10135"><span class="ln">10135 </span></a>multiplication operations in CSC format are typically faster than that 
<a name="l10136"><span class="ln">10136 </span></a>for sparse tensors in COO format. Make you have a look at :ref:`the 
<a name="l10137"><span class="ln">10137 </span></a>note on the data type of the indices &lt;sparse-csc-docs&gt;`. 
<a name="l10138"><span class="ln">10138 </span></a> 
<a name="l10139"><span class="ln">10139 </span></a>{sparse_factory_device_note} 
<a name="l10140"><span class="ln">10140 </span></a> 
<a name="l10141"><span class="ln">10141 </span></a>Args: 
<a name="l10142"><span class="ln">10142 </span></a>    ccol_indices (array_like): (B+1)-dimensional array of size 
<a name="l10143"><span class="ln">10143 </span></a>        ``(*batchsize, ncols + 1)``.  The last element of each batch 
<a name="l10144"><span class="ln">10144 </span></a>        is the number of non-zeros. This tensor encodes the index in 
<a name="l10145"><span class="ln">10145 </span></a>        values and row_indices depending on where the given column 
<a name="l10146"><span class="ln">10146 </span></a>        starts. Each successive number in the tensor subtracted by the 
<a name="l10147"><span class="ln">10147 </span></a>        number before it denotes the number of elements in a given 
<a name="l10148"><span class="ln">10148 </span></a>        column. 
<a name="l10149"><span class="ln">10149 </span></a>    row_indices (array_like): Row co-ordinates of each element in 
<a name="l10150"><span class="ln">10150 </span></a>        values. (B+1)-dimensional tensor with the same length as 
<a name="l10151"><span class="ln">10151 </span></a>        values. 
<a name="l10152"><span class="ln">10152 </span></a>    values (array_list): Initial values for the tensor. Can be a list, 
<a name="l10153"><span class="ln">10153 </span></a>        tuple, NumPy ``ndarray``, scalar, and other types that 
<a name="l10154"><span class="ln">10154 </span></a>        represents a (1+K)-dimensional tensor where ``K`` is the number 
<a name="l10155"><span class="ln">10155 </span></a>        of dense dimensions. 
<a name="l10156"><span class="ln">10156 </span></a>    size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l10157"><span class="ln">10157 </span></a>        sparse tensor: ``(*batchsize, nrows, ncols, *densesize)``. If 
<a name="l10158"><span class="ln">10158 </span></a>        not provided, the size will be inferred as the minimum size 
<a name="l10159"><span class="ln">10159 </span></a>        big enough to hold all non-zero elements. 
<a name="l10160"><span class="ln">10160 </span></a> 
<a name="l10161"><span class="ln">10161 </span></a>Keyword args: 
<a name="l10162"><span class="ln">10162 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l10163"><span class="ln">10163 </span></a>        returned tensor.  Default: if None, infers data type from 
<a name="l10164"><span class="ln">10164 </span></a>        :attr:`values`. 
<a name="l10165"><span class="ln">10165 </span></a>    device (:class:`torch.device`, optional): the desired device of 
<a name="l10166"><span class="ln">10166 </span></a>        returned tensor.  Default: if None, uses the current device 
<a name="l10167"><span class="ln">10167 </span></a>        for the default tensor type (see 
<a name="l10168"><span class="ln">10168 </span></a>        :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l10169"><span class="ln">10169 </span></a>        the CPU for CPU tensor types and the current CUDA device for 
<a name="l10170"><span class="ln">10170 </span></a>        CUDA tensor types. 
<a name="l10171"><span class="ln">10171 </span></a>    {pin_memory} 
<a name="l10172"><span class="ln">10172 </span></a>    {requires_grad} 
<a name="l10173"><span class="ln">10173 </span></a>    {check_invariants} 
<a name="l10174"><span class="ln">10174 </span></a> 
<a name="l10175"><span class="ln">10175 </span></a>Example:: 
<a name="l10176"><span class="ln">10176 </span></a> 
<a name="l10177"><span class="ln">10177 </span></a>    &gt;&gt;&gt; ccol_indices = [0, 2, 4] 
<a name="l10178"><span class="ln">10178 </span></a>    &gt;&gt;&gt; row_indices = [0, 1, 0, 1] 
<a name="l10179"><span class="ln">10179 </span></a>    &gt;&gt;&gt; values = [1, 2, 3, 4] 
<a name="l10180"><span class="ln">10180 </span></a>    &gt;&gt;&gt; torch.sparse_csc_tensor(torch.tensor(ccol_indices, dtype=torch.int64), 
<a name="l10181"><span class="ln">10181 </span></a>    ...                         torch.tensor(row_indices, dtype=torch.int64), 
<a name="l10182"><span class="ln">10182 </span></a>    ...                         torch.tensor(values), dtype=torch.double) 
<a name="l10183"><span class="ln">10183 </span></a>    tensor(ccol_indices=tensor([0, 2, 4]), 
<a name="l10184"><span class="ln">10184 </span></a>           row_indices=tensor([0, 1, 0, 1]), 
<a name="l10185"><span class="ln">10185 </span></a>           values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, 
<a name="l10186"><span class="ln">10186 </span></a>           dtype=torch.float64, layout=torch.sparse_csc) 
<a name="l10187"><span class="ln">10187 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l10188"><span class="ln">10188 </span></a><span class="s3">)</span>
<a name="l10189"><span class="ln">10189 </span></a>
<a name="l10190"><span class="ln">10190 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10191"><span class="ln">10191 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sparse_bsr_tensor</span><span class="s3">,</span>
<a name="l10192"><span class="ln">10192 </span></a>    <span class="s4">r&quot;&quot;&quot;sparse_bsr_tensor(crow_indices, col_indices, values, size=None, &quot;&quot;&quot;</span>
<a name="l10193"><span class="ln">10193 </span></a>    <span class="s4">r&quot;&quot;&quot;*, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l10194"><span class="ln">10194 </span></a> 
<a name="l10195"><span class="ln">10195 </span></a>Constructs a :ref:`sparse tensor in BSR (Block Compressed Sparse Row)) 
<a name="l10196"><span class="ln">10196 </span></a>&lt;sparse-bsr-docs&gt;` with specified 2-dimensional blocks at the given 
<a name="l10197"><span class="ln">10197 </span></a>:attr:`crow_indices` and :attr:`col_indices`. Sparse matrix 
<a name="l10198"><span class="ln">10198 </span></a>multiplication operations in BSR format are typically faster than that 
<a name="l10199"><span class="ln">10199 </span></a>for sparse tensors in COO format. Make you have a look at :ref:`the 
<a name="l10200"><span class="ln">10200 </span></a>note on the data type of the indices &lt;sparse-bsr-docs&gt;`. 
<a name="l10201"><span class="ln">10201 </span></a> 
<a name="l10202"><span class="ln">10202 </span></a>{sparse_factory_device_note} 
<a name="l10203"><span class="ln">10203 </span></a> 
<a name="l10204"><span class="ln">10204 </span></a>Args: 
<a name="l10205"><span class="ln">10205 </span></a>    crow_indices (array_like): (B+1)-dimensional array of size 
<a name="l10206"><span class="ln">10206 </span></a>        ``(*batchsize, nrowblocks + 1)``.  The last element of each 
<a name="l10207"><span class="ln">10207 </span></a>        batch is the number of non-zeros. This tensor encodes the 
<a name="l10208"><span class="ln">10208 </span></a>        block index in values and col_indices depending on where the 
<a name="l10209"><span class="ln">10209 </span></a>        given row block starts. Each successive number in the tensor 
<a name="l10210"><span class="ln">10210 </span></a>        subtracted by the number before it denotes the number of 
<a name="l10211"><span class="ln">10211 </span></a>        blocks in a given row. 
<a name="l10212"><span class="ln">10212 </span></a>    col_indices (array_like): Column block co-ordinates of each block 
<a name="l10213"><span class="ln">10213 </span></a>        in values. (B+1)-dimensional tensor with the same length as 
<a name="l10214"><span class="ln">10214 </span></a>        values. 
<a name="l10215"><span class="ln">10215 </span></a>    values (array_list): Initial values for the tensor. Can be a list, 
<a name="l10216"><span class="ln">10216 </span></a>        tuple, NumPy ``ndarray``, scalar, and other types that 
<a name="l10217"><span class="ln">10217 </span></a>        represents a (1 + 2 + K)-dimensional tensor where ``K`` is the 
<a name="l10218"><span class="ln">10218 </span></a>        number of dense dimensions. 
<a name="l10219"><span class="ln">10219 </span></a>    size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l10220"><span class="ln">10220 </span></a>        sparse tensor: ``(*batchsize, nrows * blocksize[0], ncols * 
<a name="l10221"><span class="ln">10221 </span></a>        blocksize[1], *densesize)`` where ``blocksize == 
<a name="l10222"><span class="ln">10222 </span></a>        values.shape[1:3]``. If not provided, the size will be 
<a name="l10223"><span class="ln">10223 </span></a>        inferred as the minimum size big enough to hold all non-zero 
<a name="l10224"><span class="ln">10224 </span></a>        blocks. 
<a name="l10225"><span class="ln">10225 </span></a> 
<a name="l10226"><span class="ln">10226 </span></a>Keyword args: 
<a name="l10227"><span class="ln">10227 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l10228"><span class="ln">10228 </span></a>        returned tensor.  Default: if None, infers data type from 
<a name="l10229"><span class="ln">10229 </span></a>        :attr:`values`. 
<a name="l10230"><span class="ln">10230 </span></a>    device (:class:`torch.device`, optional): the desired device of 
<a name="l10231"><span class="ln">10231 </span></a>        returned tensor.  Default: if None, uses the current device 
<a name="l10232"><span class="ln">10232 </span></a>        for the default tensor type (see 
<a name="l10233"><span class="ln">10233 </span></a>        :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l10234"><span class="ln">10234 </span></a>        the CPU for CPU tensor types and the current CUDA device for 
<a name="l10235"><span class="ln">10235 </span></a>        CUDA tensor types. 
<a name="l10236"><span class="ln">10236 </span></a>    {pin_memory} 
<a name="l10237"><span class="ln">10237 </span></a>    {requires_grad} 
<a name="l10238"><span class="ln">10238 </span></a>    {check_invariants} 
<a name="l10239"><span class="ln">10239 </span></a> 
<a name="l10240"><span class="ln">10240 </span></a>Example:: 
<a name="l10241"><span class="ln">10241 </span></a> 
<a name="l10242"><span class="ln">10242 </span></a>    &gt;&gt;&gt; crow_indices = [0, 1, 2] 
<a name="l10243"><span class="ln">10243 </span></a>    &gt;&gt;&gt; col_indices = [0, 1] 
<a name="l10244"><span class="ln">10244 </span></a>    &gt;&gt;&gt; values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] 
<a name="l10245"><span class="ln">10245 </span></a>    &gt;&gt;&gt; torch.sparse_bsr_tensor(torch.tensor(crow_indices, dtype=torch.int64), 
<a name="l10246"><span class="ln">10246 </span></a>    ...                         torch.tensor(col_indices, dtype=torch.int64), 
<a name="l10247"><span class="ln">10247 </span></a>    ...                         torch.tensor(values), dtype=torch.double) 
<a name="l10248"><span class="ln">10248 </span></a>    tensor(crow_indices=tensor([0, 1, 2]), 
<a name="l10249"><span class="ln">10249 </span></a>           col_indices=tensor([0, 1]), 
<a name="l10250"><span class="ln">10250 </span></a>           values=tensor([[[1., 2.], 
<a name="l10251"><span class="ln">10251 </span></a>                           [3., 4.]], 
<a name="l10252"><span class="ln">10252 </span></a>                          [[5., 6.], 
<a name="l10253"><span class="ln">10253 </span></a>                           [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64, 
<a name="l10254"><span class="ln">10254 </span></a>           layout=torch.sparse_bsr) 
<a name="l10255"><span class="ln">10255 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l10256"><span class="ln">10256 </span></a><span class="s3">)</span>
<a name="l10257"><span class="ln">10257 </span></a>
<a name="l10258"><span class="ln">10258 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10259"><span class="ln">10259 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sparse_bsc_tensor</span><span class="s3">,</span>
<a name="l10260"><span class="ln">10260 </span></a>    <span class="s4">r&quot;&quot;&quot;sparse_bsc_tensor(ccol_indices, row_indices, values, size=None, &quot;&quot;&quot;</span>
<a name="l10261"><span class="ln">10261 </span></a>    <span class="s4">r&quot;&quot;&quot;*, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l10262"><span class="ln">10262 </span></a> 
<a name="l10263"><span class="ln">10263 </span></a>Constructs a :ref:`sparse tensor in BSC (Block Compressed Sparse 
<a name="l10264"><span class="ln">10264 </span></a>Column)) &lt;sparse-bsc-docs&gt;` with specified 2-dimensional blocks at the 
<a name="l10265"><span class="ln">10265 </span></a>given :attr:`ccol_indices` and :attr:`row_indices`. Sparse matrix 
<a name="l10266"><span class="ln">10266 </span></a>multiplication operations in BSC format are typically faster than that 
<a name="l10267"><span class="ln">10267 </span></a>for sparse tensors in COO format. Make you have a look at :ref:`the 
<a name="l10268"><span class="ln">10268 </span></a>note on the data type of the indices &lt;sparse-bsc-docs&gt;`. 
<a name="l10269"><span class="ln">10269 </span></a> 
<a name="l10270"><span class="ln">10270 </span></a>{sparse_factory_device_note} 
<a name="l10271"><span class="ln">10271 </span></a> 
<a name="l10272"><span class="ln">10272 </span></a>Args: 
<a name="l10273"><span class="ln">10273 </span></a>    ccol_indices (array_like): (B+1)-dimensional array of size 
<a name="l10274"><span class="ln">10274 </span></a>        ``(*batchsize, ncolblocks + 1)``. The last element of each 
<a name="l10275"><span class="ln">10275 </span></a>        batch is the number of non-zeros. This tensor encodes the 
<a name="l10276"><span class="ln">10276 </span></a>        index in values and row_indices depending on where the given 
<a name="l10277"><span class="ln">10277 </span></a>        column starts. Each successive number in the tensor subtracted 
<a name="l10278"><span class="ln">10278 </span></a>        by the number before it denotes the number of elements in a 
<a name="l10279"><span class="ln">10279 </span></a>        given column. 
<a name="l10280"><span class="ln">10280 </span></a>    row_indices (array_like): Row block co-ordinates of each block in 
<a name="l10281"><span class="ln">10281 </span></a>        values. (B+1)-dimensional tensor with the same length 
<a name="l10282"><span class="ln">10282 </span></a>        as values. 
<a name="l10283"><span class="ln">10283 </span></a>    values (array_list): Initial blocks for the tensor. Can be a list, 
<a name="l10284"><span class="ln">10284 </span></a>        tuple, NumPy ``ndarray``, and other types that 
<a name="l10285"><span class="ln">10285 </span></a>        represents a (1 + 2 + K)-dimensional tensor where ``K`` is the 
<a name="l10286"><span class="ln">10286 </span></a>        number of dense dimensions. 
<a name="l10287"><span class="ln">10287 </span></a>    size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l10288"><span class="ln">10288 </span></a>        sparse tensor: ``(*batchsize, nrows * blocksize[0], ncols * 
<a name="l10289"><span class="ln">10289 </span></a>        blocksize[1], *densesize)`` If not provided, the size will be 
<a name="l10290"><span class="ln">10290 </span></a>        inferred as the minimum size big enough to hold all non-zero 
<a name="l10291"><span class="ln">10291 </span></a>        blocks. 
<a name="l10292"><span class="ln">10292 </span></a> 
<a name="l10293"><span class="ln">10293 </span></a>Keyword args: 
<a name="l10294"><span class="ln">10294 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l10295"><span class="ln">10295 </span></a>        returned tensor.  Default: if None, infers data type from 
<a name="l10296"><span class="ln">10296 </span></a>        :attr:`values`. 
<a name="l10297"><span class="ln">10297 </span></a>    device (:class:`torch.device`, optional): the desired device of 
<a name="l10298"><span class="ln">10298 </span></a>        returned tensor.  Default: if None, uses the current device 
<a name="l10299"><span class="ln">10299 </span></a>        for the default tensor type (see 
<a name="l10300"><span class="ln">10300 </span></a>        :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l10301"><span class="ln">10301 </span></a>        the CPU for CPU tensor types and the current CUDA device for 
<a name="l10302"><span class="ln">10302 </span></a>        CUDA tensor types. 
<a name="l10303"><span class="ln">10303 </span></a>    {pin_memory} 
<a name="l10304"><span class="ln">10304 </span></a>    {requires_grad} 
<a name="l10305"><span class="ln">10305 </span></a>    {check_invariants} 
<a name="l10306"><span class="ln">10306 </span></a> 
<a name="l10307"><span class="ln">10307 </span></a>Example:: 
<a name="l10308"><span class="ln">10308 </span></a> 
<a name="l10309"><span class="ln">10309 </span></a>    &gt;&gt;&gt; ccol_indices = [0, 1, 2] 
<a name="l10310"><span class="ln">10310 </span></a>    &gt;&gt;&gt; row_indices = [0, 1] 
<a name="l10311"><span class="ln">10311 </span></a>    &gt;&gt;&gt; values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] 
<a name="l10312"><span class="ln">10312 </span></a>    &gt;&gt;&gt; torch.sparse_bsc_tensor(torch.tensor(ccol_indices, dtype=torch.int64), 
<a name="l10313"><span class="ln">10313 </span></a>    ...                         torch.tensor(row_indices, dtype=torch.int64), 
<a name="l10314"><span class="ln">10314 </span></a>    ...                         torch.tensor(values), dtype=torch.double) 
<a name="l10315"><span class="ln">10315 </span></a>    tensor(ccol_indices=tensor([0, 1, 2]), 
<a name="l10316"><span class="ln">10316 </span></a>           row_indices=tensor([0, 1]), 
<a name="l10317"><span class="ln">10317 </span></a>           values=tensor([[[1., 2.], 
<a name="l10318"><span class="ln">10318 </span></a>                           [3., 4.]], 
<a name="l10319"><span class="ln">10319 </span></a>                          [[5., 6.], 
<a name="l10320"><span class="ln">10320 </span></a>                           [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64, 
<a name="l10321"><span class="ln">10321 </span></a>           layout=torch.sparse_bsc) 
<a name="l10322"><span class="ln">10322 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l10323"><span class="ln">10323 </span></a><span class="s3">)</span>
<a name="l10324"><span class="ln">10324 </span></a>
<a name="l10325"><span class="ln">10325 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10326"><span class="ln">10326 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sparse_coo_tensor</span><span class="s3">,</span>
<a name="l10327"><span class="ln">10327 </span></a>    <span class="s4">r&quot;&quot;&quot;sparse_coo_tensor(indices, values, size=None, &quot;&quot;&quot;</span>
<a name="l10328"><span class="ln">10328 </span></a>    <span class="s4">r&quot;&quot;&quot;*, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None, is_coalesced=None) -&gt; Tensor 
<a name="l10329"><span class="ln">10329 </span></a> 
<a name="l10330"><span class="ln">10330 </span></a>Constructs a :ref:`sparse tensor in COO(rdinate) format 
<a name="l10331"><span class="ln">10331 </span></a>&lt;sparse-coo-docs&gt;` with specified values at the given 
<a name="l10332"><span class="ln">10332 </span></a>:attr:`indices`. 
<a name="l10333"><span class="ln">10333 </span></a> 
<a name="l10334"><span class="ln">10334 </span></a>.. note:: 
<a name="l10335"><span class="ln">10335 </span></a> 
<a name="l10336"><span class="ln">10336 </span></a>   This function returns an :ref:`uncoalesced tensor 
<a name="l10337"><span class="ln">10337 </span></a>   &lt;sparse-uncoalesced-coo-docs&gt;` when :attr:`is_coalesced` is 
<a name="l10338"><span class="ln">10338 </span></a>   unspecified or ``None``. 
<a name="l10339"><span class="ln">10339 </span></a> 
<a name="l10340"><span class="ln">10340 </span></a>{sparse_factory_device_note} 
<a name="l10341"><span class="ln">10341 </span></a> 
<a name="l10342"><span class="ln">10342 </span></a>Args: 
<a name="l10343"><span class="ln">10343 </span></a>    indices (array_like): Initial data for the tensor. Can be a list, tuple, 
<a name="l10344"><span class="ln">10344 </span></a>        NumPy ``ndarray``, scalar, and other types. Will be cast to a :class:`torch.LongTensor` 
<a name="l10345"><span class="ln">10345 </span></a>        internally. The indices are the coordinates of the non-zero values in the matrix, and thus 
<a name="l10346"><span class="ln">10346 </span></a>        should be two-dimensional where the first dimension is the number of tensor dimensions and 
<a name="l10347"><span class="ln">10347 </span></a>        the second dimension is the number of non-zero values. 
<a name="l10348"><span class="ln">10348 </span></a>    values (array_like): Initial values for the tensor. Can be a list, tuple, 
<a name="l10349"><span class="ln">10349 </span></a>        NumPy ``ndarray``, scalar, and other types. 
<a name="l10350"><span class="ln">10350 </span></a>    size (list, tuple, or :class:`torch.Size`, optional): Size of the sparse tensor. If not 
<a name="l10351"><span class="ln">10351 </span></a>        provided the size will be inferred as the minimum size big enough to hold all non-zero 
<a name="l10352"><span class="ln">10352 </span></a>        elements. 
<a name="l10353"><span class="ln">10353 </span></a> 
<a name="l10354"><span class="ln">10354 </span></a>Keyword args: 
<a name="l10355"><span class="ln">10355 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l10356"><span class="ln">10356 </span></a>        Default: if None, infers data type from :attr:`values`. 
<a name="l10357"><span class="ln">10357 </span></a>    device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l10358"><span class="ln">10358 </span></a>        Default: if None, uses the current device for the default tensor type 
<a name="l10359"><span class="ln">10359 </span></a>        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l10360"><span class="ln">10360 </span></a>        for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l10361"><span class="ln">10361 </span></a>    {pin_memory} 
<a name="l10362"><span class="ln">10362 </span></a>    {requires_grad} 
<a name="l10363"><span class="ln">10363 </span></a>    {check_invariants} 
<a name="l10364"><span class="ln">10364 </span></a>    is_coalesced (bool, optional): When``True``, the caller is 
<a name="l10365"><span class="ln">10365 </span></a>        responsible for providing tensor indices that correspond to a 
<a name="l10366"><span class="ln">10366 </span></a>        coalesced tensor.  If the :attr:`check_invariants` flag is 
<a name="l10367"><span class="ln">10367 </span></a>        False, no error will be raised if the prerequisites are not 
<a name="l10368"><span class="ln">10368 </span></a>        met and this will lead to silently incorrect results. To force 
<a name="l10369"><span class="ln">10369 </span></a>        coalescion please use :meth:`coalesce` on the resulting 
<a name="l10370"><span class="ln">10370 </span></a>        Tensor. 
<a name="l10371"><span class="ln">10371 </span></a>        Default: None: except for trivial cases (e.g. nnz &lt; 2) the 
<a name="l10372"><span class="ln">10372 </span></a>        resulting Tensor has is_coalesced set to ``False```. 
<a name="l10373"><span class="ln">10373 </span></a> 
<a name="l10374"><span class="ln">10374 </span></a>Example:: 
<a name="l10375"><span class="ln">10375 </span></a> 
<a name="l10376"><span class="ln">10376 </span></a>    &gt;&gt;&gt; i = torch.tensor([[0, 1, 1], 
<a name="l10377"><span class="ln">10377 </span></a>    ...                   [2, 0, 2]]) 
<a name="l10378"><span class="ln">10378 </span></a>    &gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32) 
<a name="l10379"><span class="ln">10379 </span></a>    &gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4]) 
<a name="l10380"><span class="ln">10380 </span></a>    tensor(indices=tensor([[0, 1, 1], 
<a name="l10381"><span class="ln">10381 </span></a>                           [2, 0, 2]]), 
<a name="l10382"><span class="ln">10382 </span></a>           values=tensor([3., 4., 5.]), 
<a name="l10383"><span class="ln">10383 </span></a>           size=(2, 4), nnz=3, layout=torch.sparse_coo) 
<a name="l10384"><span class="ln">10384 </span></a> 
<a name="l10385"><span class="ln">10385 </span></a>    &gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference 
<a name="l10386"><span class="ln">10386 </span></a>    tensor(indices=tensor([[0, 1, 1], 
<a name="l10387"><span class="ln">10387 </span></a>                           [2, 0, 2]]), 
<a name="l10388"><span class="ln">10388 </span></a>           values=tensor([3., 4., 5.]), 
<a name="l10389"><span class="ln">10389 </span></a>           size=(2, 3), nnz=3, layout=torch.sparse_coo) 
<a name="l10390"><span class="ln">10390 </span></a> 
<a name="l10391"><span class="ln">10391 </span></a>    &gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4], 
<a name="l10392"><span class="ln">10392 </span></a>    ...                         dtype=torch.float64, 
<a name="l10393"><span class="ln">10393 </span></a>    ...                         device=torch.device('cuda:0')) 
<a name="l10394"><span class="ln">10394 </span></a>    tensor(indices=tensor([[0, 1, 1], 
<a name="l10395"><span class="ln">10395 </span></a>                           [2, 0, 2]]), 
<a name="l10396"><span class="ln">10396 </span></a>           values=tensor([3., 4., 5.]), 
<a name="l10397"><span class="ln">10397 </span></a>           device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64, 
<a name="l10398"><span class="ln">10398 </span></a>           layout=torch.sparse_coo) 
<a name="l10399"><span class="ln">10399 </span></a> 
<a name="l10400"><span class="ln">10400 </span></a>    # Create an empty sparse tensor with the following invariants: 
<a name="l10401"><span class="ln">10401 </span></a>    #   1. sparse_dim + dense_dim = len(SparseTensor.shape) 
<a name="l10402"><span class="ln">10402 </span></a>    #   2. SparseTensor._indices().shape = (sparse_dim, nnz) 
<a name="l10403"><span class="ln">10403 </span></a>    #   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:]) 
<a name="l10404"><span class="ln">10404 </span></a>    # 
<a name="l10405"><span class="ln">10405 </span></a>    # For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and 
<a name="l10406"><span class="ln">10406 </span></a>    # sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0)) 
<a name="l10407"><span class="ln">10407 </span></a>    &gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1]) 
<a name="l10408"><span class="ln">10408 </span></a>    tensor(indices=tensor([], size=(1, 0)), 
<a name="l10409"><span class="ln">10409 </span></a>           values=tensor([], size=(0,)), 
<a name="l10410"><span class="ln">10410 </span></a>           size=(1,), nnz=0, layout=torch.sparse_coo) 
<a name="l10411"><span class="ln">10411 </span></a> 
<a name="l10412"><span class="ln">10412 </span></a>    # and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and 
<a name="l10413"><span class="ln">10413 </span></a>    # sparse_dim = 1 
<a name="l10414"><span class="ln">10414 </span></a>    &gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2]) 
<a name="l10415"><span class="ln">10415 </span></a>    tensor(indices=tensor([], size=(1, 0)), 
<a name="l10416"><span class="ln">10416 </span></a>           values=tensor([], size=(0, 2)), 
<a name="l10417"><span class="ln">10417 </span></a>           size=(1, 2), nnz=0, layout=torch.sparse_coo) 
<a name="l10418"><span class="ln">10418 </span></a> 
<a name="l10419"><span class="ln">10419 </span></a>.. _torch.sparse: https://pytorch.org/docs/stable/sparse.html 
<a name="l10420"><span class="ln">10420 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l10421"><span class="ln">10421 </span></a><span class="s3">)</span>
<a name="l10422"><span class="ln">10422 </span></a>
<a name="l10423"><span class="ln">10423 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10424"><span class="ln">10424 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sqrt</span><span class="s3">,</span>
<a name="l10425"><span class="ln">10425 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10426"><span class="ln">10426 </span></a>sqrt(input, *, out=None) -&gt; Tensor 
<a name="l10427"><span class="ln">10427 </span></a> 
<a name="l10428"><span class="ln">10428 </span></a>Returns a new tensor with the square-root of the elements of :attr:`input`. 
<a name="l10429"><span class="ln">10429 </span></a> 
<a name="l10430"><span class="ln">10430 </span></a>.. math:: 
<a name="l10431"><span class="ln">10431 </span></a>    \text{out}_{i} = \sqrt{\text{input}_{i}} 
<a name="l10432"><span class="ln">10432 </span></a>&quot;&quot;&quot;</span>
<a name="l10433"><span class="ln">10433 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l10434"><span class="ln">10434 </span></a>Args: 
<a name="l10435"><span class="ln">10435 </span></a>    {input} 
<a name="l10436"><span class="ln">10436 </span></a> 
<a name="l10437"><span class="ln">10437 </span></a>Keyword args: 
<a name="l10438"><span class="ln">10438 </span></a>    {out} 
<a name="l10439"><span class="ln">10439 </span></a> 
<a name="l10440"><span class="ln">10440 </span></a>Example:: 
<a name="l10441"><span class="ln">10441 </span></a> 
<a name="l10442"><span class="ln">10442 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l10443"><span class="ln">10443 </span></a>    &gt;&gt;&gt; a 
<a name="l10444"><span class="ln">10444 </span></a>    tensor([-2.0755,  1.0226,  0.0831,  0.4806]) 
<a name="l10445"><span class="ln">10445 </span></a>    &gt;&gt;&gt; torch.sqrt(a) 
<a name="l10446"><span class="ln">10446 </span></a>    tensor([    nan,  1.0112,  0.2883,  0.6933]) 
<a name="l10447"><span class="ln">10447 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l10448"><span class="ln">10448 </span></a><span class="s3">)</span>
<a name="l10449"><span class="ln">10449 </span></a>
<a name="l10450"><span class="ln">10450 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10451"><span class="ln">10451 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">square</span><span class="s3">,</span>
<a name="l10452"><span class="ln">10452 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10453"><span class="ln">10453 </span></a>square(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l10454"><span class="ln">10454 </span></a> 
<a name="l10455"><span class="ln">10455 </span></a>Returns a new tensor with the square of the elements of :attr:`input`. 
<a name="l10456"><span class="ln">10456 </span></a> 
<a name="l10457"><span class="ln">10457 </span></a>Args: 
<a name="l10458"><span class="ln">10458 </span></a>    {input} 
<a name="l10459"><span class="ln">10459 </span></a> 
<a name="l10460"><span class="ln">10460 </span></a>Keyword args: 
<a name="l10461"><span class="ln">10461 </span></a>    {out} 
<a name="l10462"><span class="ln">10462 </span></a> 
<a name="l10463"><span class="ln">10463 </span></a>Example:: 
<a name="l10464"><span class="ln">10464 </span></a> 
<a name="l10465"><span class="ln">10465 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l10466"><span class="ln">10466 </span></a>    &gt;&gt;&gt; a 
<a name="l10467"><span class="ln">10467 </span></a>    tensor([-2.0755,  1.0226,  0.0831,  0.4806]) 
<a name="l10468"><span class="ln">10468 </span></a>    &gt;&gt;&gt; torch.square(a) 
<a name="l10469"><span class="ln">10469 </span></a>    tensor([ 4.3077,  1.0457,  0.0069,  0.2310]) 
<a name="l10470"><span class="ln">10470 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l10471"><span class="ln">10471 </span></a><span class="s3">)</span>
<a name="l10472"><span class="ln">10472 </span></a>
<a name="l10473"><span class="ln">10473 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10474"><span class="ln">10474 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">squeeze</span><span class="s3">,</span>
<a name="l10475"><span class="ln">10475 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10476"><span class="ln">10476 </span></a>squeeze(input: Tensor, dim: Optional[Union[int, List[int]]]) -&gt; Tensor 
<a name="l10477"><span class="ln">10477 </span></a> 
<a name="l10478"><span class="ln">10478 </span></a>Returns a tensor with all specified dimensions of :attr:`input` of size `1` removed. 
<a name="l10479"><span class="ln">10479 </span></a> 
<a name="l10480"><span class="ln">10480 </span></a>For example, if `input` is of shape: 
<a name="l10481"><span class="ln">10481 </span></a>:math:`(A \times 1 \times B \times C \times 1 \times D)` then the `input.squeeze()` 
<a name="l10482"><span class="ln">10482 </span></a>will be of shape: :math:`(A \times B \times C \times D)`. 
<a name="l10483"><span class="ln">10483 </span></a> 
<a name="l10484"><span class="ln">10484 </span></a>When :attr:`dim` is given, a squeeze operation is done only in the given 
<a name="l10485"><span class="ln">10485 </span></a>dimension(s). If `input` is of shape: :math:`(A \times 1 \times B)`, 
<a name="l10486"><span class="ln">10486 </span></a>``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)`` 
<a name="l10487"><span class="ln">10487 </span></a>will squeeze the tensor to the shape :math:`(A \times B)`. 
<a name="l10488"><span class="ln">10488 </span></a> 
<a name="l10489"><span class="ln">10489 </span></a>.. note:: The returned tensor shares the storage with the input tensor, 
<a name="l10490"><span class="ln">10490 </span></a>          so changing the contents of one will change the contents of the other. 
<a name="l10491"><span class="ln">10491 </span></a> 
<a name="l10492"><span class="ln">10492 </span></a>.. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)` 
<a name="l10493"><span class="ln">10493 </span></a>          will also remove the batch dimension, which can lead to unexpected 
<a name="l10494"><span class="ln">10494 </span></a>          errors. Consider specifying only the dims you wish to be squeezed. 
<a name="l10495"><span class="ln">10495 </span></a> 
<a name="l10496"><span class="ln">10496 </span></a>Args: 
<a name="l10497"><span class="ln">10497 </span></a>    {input} 
<a name="l10498"><span class="ln">10498 </span></a>    dim (int or tuple of ints, optional): if given, the input will be squeezed 
<a name="l10499"><span class="ln">10499 </span></a>           only in the specified dimensions. 
<a name="l10500"><span class="ln">10500 </span></a> 
<a name="l10501"><span class="ln">10501 </span></a>        .. versionchanged:: 2.0 
<a name="l10502"><span class="ln">10502 </span></a>           :attr:`dim` now accepts tuples of dimensions. 
<a name="l10503"><span class="ln">10503 </span></a> 
<a name="l10504"><span class="ln">10504 </span></a>Example:: 
<a name="l10505"><span class="ln">10505 </span></a> 
<a name="l10506"><span class="ln">10506 </span></a>    &gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) 
<a name="l10507"><span class="ln">10507 </span></a>    &gt;&gt;&gt; x.size() 
<a name="l10508"><span class="ln">10508 </span></a>    torch.Size([2, 1, 2, 1, 2]) 
<a name="l10509"><span class="ln">10509 </span></a>    &gt;&gt;&gt; y = torch.squeeze(x) 
<a name="l10510"><span class="ln">10510 </span></a>    &gt;&gt;&gt; y.size() 
<a name="l10511"><span class="ln">10511 </span></a>    torch.Size([2, 2, 2]) 
<a name="l10512"><span class="ln">10512 </span></a>    &gt;&gt;&gt; y = torch.squeeze(x, 0) 
<a name="l10513"><span class="ln">10513 </span></a>    &gt;&gt;&gt; y.size() 
<a name="l10514"><span class="ln">10514 </span></a>    torch.Size([2, 1, 2, 1, 2]) 
<a name="l10515"><span class="ln">10515 </span></a>    &gt;&gt;&gt; y = torch.squeeze(x, 1) 
<a name="l10516"><span class="ln">10516 </span></a>    &gt;&gt;&gt; y.size() 
<a name="l10517"><span class="ln">10517 </span></a>    torch.Size([2, 2, 1, 2]) 
<a name="l10518"><span class="ln">10518 </span></a>    &gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) 
<a name="l10519"><span class="ln">10519 </span></a>    torch.Size([2, 2, 2]) 
<a name="l10520"><span class="ln">10520 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l10521"><span class="ln">10521 </span></a><span class="s3">)</span>
<a name="l10522"><span class="ln">10522 </span></a>
<a name="l10523"><span class="ln">10523 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10524"><span class="ln">10524 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">std</span><span class="s3">,</span>
<a name="l10525"><span class="ln">10525 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10526"><span class="ln">10526 </span></a>std(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l10527"><span class="ln">10527 </span></a> 
<a name="l10528"><span class="ln">10528 </span></a>Calculates the standard deviation over the dimensions specified by :attr:`dim`. 
<a name="l10529"><span class="ln">10529 </span></a>:attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l10530"><span class="ln">10530 </span></a>reduce over all dimensions. 
<a name="l10531"><span class="ln">10531 </span></a> 
<a name="l10532"><span class="ln">10532 </span></a>The standard deviation (:math:`\sigma`) is calculated as 
<a name="l10533"><span class="ln">10533 </span></a> 
<a name="l10534"><span class="ln">10534 </span></a>.. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l10535"><span class="ln">10535 </span></a> 
<a name="l10536"><span class="ln">10536 </span></a>where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l10537"><span class="ln">10537 </span></a>sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l10538"><span class="ln">10538 </span></a>the :attr:`correction`. 
<a name="l10539"><span class="ln">10539 </span></a>&quot;&quot;&quot;</span>
<a name="l10540"><span class="ln">10540 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l10541"><span class="ln">10541 </span></a> 
<a name="l10542"><span class="ln">10542 </span></a>{keepdim_details} 
<a name="l10543"><span class="ln">10543 </span></a> 
<a name="l10544"><span class="ln">10544 </span></a>Args: 
<a name="l10545"><span class="ln">10545 </span></a>    {input} 
<a name="l10546"><span class="ln">10546 </span></a>    {opt_dim_all_reduce} 
<a name="l10547"><span class="ln">10547 </span></a> 
<a name="l10548"><span class="ln">10548 </span></a>Keyword args: 
<a name="l10549"><span class="ln">10549 </span></a>    correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l10550"><span class="ln">10550 </span></a>        Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l10551"><span class="ln">10551 </span></a> 
<a name="l10552"><span class="ln">10552 </span></a>        .. versionchanged:: 2.0 
<a name="l10553"><span class="ln">10553 </span></a>            Previously this argument was called ``unbiased`` and was a boolean 
<a name="l10554"><span class="ln">10554 </span></a>            with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l10555"><span class="ln">10555 </span></a>            ``correction=0``. 
<a name="l10556"><span class="ln">10556 </span></a>    {opt_keepdim} 
<a name="l10557"><span class="ln">10557 </span></a>    {out} 
<a name="l10558"><span class="ln">10558 </span></a> 
<a name="l10559"><span class="ln">10559 </span></a>Example: 
<a name="l10560"><span class="ln">10560 </span></a> 
<a name="l10561"><span class="ln">10561 </span></a>    &gt;&gt;&gt; a = torch.tensor( 
<a name="l10562"><span class="ln">10562 </span></a>    ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l10563"><span class="ln">10563 </span></a>    ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l10564"><span class="ln">10564 </span></a>    ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l10565"><span class="ln">10565 </span></a>    ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l10566"><span class="ln">10566 </span></a>    ... )  # fmt: skip 
<a name="l10567"><span class="ln">10567 </span></a>    &gt;&gt;&gt; torch.std(a, dim=1, keepdim=True) 
<a name="l10568"><span class="ln">10568 </span></a>    tensor([[1.0311], 
<a name="l10569"><span class="ln">10569 </span></a>            [0.7477], 
<a name="l10570"><span class="ln">10570 </span></a>            [1.2204], 
<a name="l10571"><span class="ln">10571 </span></a>            [0.9087]]) 
<a name="l10572"><span class="ln">10572 </span></a> 
<a name="l10573"><span class="ln">10573 </span></a>.. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l10574"><span class="ln">10574 </span></a> 
<a name="l10575"><span class="ln">10575 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l10576"><span class="ln">10576 </span></a><span class="s3">)</span>
<a name="l10577"><span class="ln">10577 </span></a>
<a name="l10578"><span class="ln">10578 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10579"><span class="ln">10579 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">std_mean</span><span class="s3">,</span>
<a name="l10580"><span class="ln">10580 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10581"><span class="ln">10581 </span></a>std_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l10582"><span class="ln">10582 </span></a> 
<a name="l10583"><span class="ln">10583 </span></a>Calculates the standard deviation and mean over the dimensions specified by 
<a name="l10584"><span class="ln">10584 </span></a>:attr:`dim`. :attr:`dim` can be a single dimension, list of dimensions, or 
<a name="l10585"><span class="ln">10585 </span></a>``None`` to reduce over all dimensions. 
<a name="l10586"><span class="ln">10586 </span></a> 
<a name="l10587"><span class="ln">10587 </span></a>The standard deviation (:math:`\sigma`) is calculated as 
<a name="l10588"><span class="ln">10588 </span></a> 
<a name="l10589"><span class="ln">10589 </span></a>.. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l10590"><span class="ln">10590 </span></a> 
<a name="l10591"><span class="ln">10591 </span></a>where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l10592"><span class="ln">10592 </span></a>sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l10593"><span class="ln">10593 </span></a>the :attr:`correction`. 
<a name="l10594"><span class="ln">10594 </span></a> 
<a name="l10595"><span class="ln">10595 </span></a>&quot;&quot;&quot;</span>
<a name="l10596"><span class="ln">10596 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l10597"><span class="ln">10597 </span></a> 
<a name="l10598"><span class="ln">10598 </span></a>{keepdim_details} 
<a name="l10599"><span class="ln">10599 </span></a> 
<a name="l10600"><span class="ln">10600 </span></a>Args: 
<a name="l10601"><span class="ln">10601 </span></a>    {input} 
<a name="l10602"><span class="ln">10602 </span></a>    {opt_dim_all_reduce} 
<a name="l10603"><span class="ln">10603 </span></a> 
<a name="l10604"><span class="ln">10604 </span></a>Keyword args: 
<a name="l10605"><span class="ln">10605 </span></a>    correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l10606"><span class="ln">10606 </span></a>        Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l10607"><span class="ln">10607 </span></a> 
<a name="l10608"><span class="ln">10608 </span></a>        .. versionchanged:: 2.0 
<a name="l10609"><span class="ln">10609 </span></a>            Previously this argument was called ``unbiased`` and was a boolean 
<a name="l10610"><span class="ln">10610 </span></a>            with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l10611"><span class="ln">10611 </span></a>            ``correction=0``. 
<a name="l10612"><span class="ln">10612 </span></a>    {opt_keepdim} 
<a name="l10613"><span class="ln">10613 </span></a>    {out} 
<a name="l10614"><span class="ln">10614 </span></a> 
<a name="l10615"><span class="ln">10615 </span></a>Returns: 
<a name="l10616"><span class="ln">10616 </span></a>    A tuple (std, mean) containing the standard deviation and mean. 
<a name="l10617"><span class="ln">10617 </span></a> 
<a name="l10618"><span class="ln">10618 </span></a>Example: 
<a name="l10619"><span class="ln">10619 </span></a> 
<a name="l10620"><span class="ln">10620 </span></a>    &gt;&gt;&gt; a = torch.tensor( 
<a name="l10621"><span class="ln">10621 </span></a>    ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l10622"><span class="ln">10622 </span></a>    ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l10623"><span class="ln">10623 </span></a>    ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l10624"><span class="ln">10624 </span></a>    ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l10625"><span class="ln">10625 </span></a>    ... )  # fmt: skip 
<a name="l10626"><span class="ln">10626 </span></a>    &gt;&gt;&gt; torch.std_mean(a, dim=0, keepdim=True) 
<a name="l10627"><span class="ln">10627 </span></a>    (tensor([[1.2620, 1.0028, 1.0957, 0.6038]]), 
<a name="l10628"><span class="ln">10628 </span></a>     tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l10629"><span class="ln">10629 </span></a> 
<a name="l10630"><span class="ln">10630 </span></a>.. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l10631"><span class="ln">10631 </span></a> 
<a name="l10632"><span class="ln">10632 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l10633"><span class="ln">10633 </span></a><span class="s3">)</span>
<a name="l10634"><span class="ln">10634 </span></a>
<a name="l10635"><span class="ln">10635 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10636"><span class="ln">10636 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sub</span><span class="s3">,</span>
<a name="l10637"><span class="ln">10637 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10638"><span class="ln">10638 </span></a>sub(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l10639"><span class="ln">10639 </span></a> 
<a name="l10640"><span class="ln">10640 </span></a>Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`. 
<a name="l10641"><span class="ln">10641 </span></a> 
<a name="l10642"><span class="ln">10642 </span></a>.. math:: 
<a name="l10643"><span class="ln">10643 </span></a>    \text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i 
<a name="l10644"><span class="ln">10644 </span></a>&quot;&quot;&quot;</span>
<a name="l10645"><span class="ln">10645 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l10646"><span class="ln">10646 </span></a> 
<a name="l10647"><span class="ln">10647 </span></a>Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l10648"><span class="ln">10648 </span></a>:ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l10649"><span class="ln">10649 </span></a> 
<a name="l10650"><span class="ln">10650 </span></a>Args: 
<a name="l10651"><span class="ln">10651 </span></a>    {input} 
<a name="l10652"><span class="ln">10652 </span></a>    other (Tensor or Number): the tensor or number to subtract from :attr:`input`. 
<a name="l10653"><span class="ln">10653 </span></a> 
<a name="l10654"><span class="ln">10654 </span></a>Keyword args: 
<a name="l10655"><span class="ln">10655 </span></a>    alpha (Number): the multiplier for :attr:`other`. 
<a name="l10656"><span class="ln">10656 </span></a>    {out} 
<a name="l10657"><span class="ln">10657 </span></a> 
<a name="l10658"><span class="ln">10658 </span></a>Example:: 
<a name="l10659"><span class="ln">10659 </span></a> 
<a name="l10660"><span class="ln">10660 </span></a>    &gt;&gt;&gt; a = torch.tensor((1, 2)) 
<a name="l10661"><span class="ln">10661 </span></a>    &gt;&gt;&gt; b = torch.tensor((0, 1)) 
<a name="l10662"><span class="ln">10662 </span></a>    &gt;&gt;&gt; torch.sub(a, b, alpha=2) 
<a name="l10663"><span class="ln">10663 </span></a>    tensor([1, 0]) 
<a name="l10664"><span class="ln">10664 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l10665"><span class="ln">10665 </span></a><span class="s3">)</span>
<a name="l10666"><span class="ln">10666 </span></a>
<a name="l10667"><span class="ln">10667 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10668"><span class="ln">10668 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">subtract</span><span class="s3">,</span>
<a name="l10669"><span class="ln">10669 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10670"><span class="ln">10670 </span></a>subtract(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l10671"><span class="ln">10671 </span></a> 
<a name="l10672"><span class="ln">10672 </span></a>Alias for :func:`torch.sub`. 
<a name="l10673"><span class="ln">10673 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l10674"><span class="ln">10674 </span></a><span class="s3">)</span>
<a name="l10675"><span class="ln">10675 </span></a>
<a name="l10676"><span class="ln">10676 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10677"><span class="ln">10677 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">,</span>
<a name="l10678"><span class="ln">10678 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10679"><span class="ln">10679 </span></a>sum(input, *, dtype=None) -&gt; Tensor 
<a name="l10680"><span class="ln">10680 </span></a> 
<a name="l10681"><span class="ln">10681 </span></a>Returns the sum of all elements in the :attr:`input` tensor. 
<a name="l10682"><span class="ln">10682 </span></a> 
<a name="l10683"><span class="ln">10683 </span></a>Args: 
<a name="l10684"><span class="ln">10684 </span></a>    {input} 
<a name="l10685"><span class="ln">10685 </span></a> 
<a name="l10686"><span class="ln">10686 </span></a>Keyword args: 
<a name="l10687"><span class="ln">10687 </span></a>    {dtype} 
<a name="l10688"><span class="ln">10688 </span></a> 
<a name="l10689"><span class="ln">10689 </span></a>.. note:: Use the `dtype` argument if you need the result in a specific tensor type. 
<a name="l10690"><span class="ln">10690 </span></a>          Otherwise, the result type may be automatically promoted (e.g., from `torch.int32` to `torch.int64`). 
<a name="l10691"><span class="ln">10691 </span></a> 
<a name="l10692"><span class="ln">10692 </span></a>Example:: 
<a name="l10693"><span class="ln">10693 </span></a> 
<a name="l10694"><span class="ln">10694 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l10695"><span class="ln">10695 </span></a>    &gt;&gt;&gt; a 
<a name="l10696"><span class="ln">10696 </span></a>    tensor([[ 0.1133, -0.9567,  0.2958]]) 
<a name="l10697"><span class="ln">10697 </span></a>    &gt;&gt;&gt; torch.sum(a) 
<a name="l10698"><span class="ln">10698 </span></a>    tensor(-0.5475) 
<a name="l10699"><span class="ln">10699 </span></a> 
<a name="l10700"><span class="ln">10700 </span></a>.. function:: sum(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l10701"><span class="ln">10701 </span></a>   :noindex: 
<a name="l10702"><span class="ln">10702 </span></a> 
<a name="l10703"><span class="ln">10703 </span></a>Returns the sum of each row of the :attr:`input` tensor in the given 
<a name="l10704"><span class="ln">10704 </span></a>dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l10705"><span class="ln">10705 </span></a>reduce over all of them. 
<a name="l10706"><span class="ln">10706 </span></a> 
<a name="l10707"><span class="ln">10707 </span></a>{keepdim_details} 
<a name="l10708"><span class="ln">10708 </span></a> 
<a name="l10709"><span class="ln">10709 </span></a>Args: 
<a name="l10710"><span class="ln">10710 </span></a>    {input} 
<a name="l10711"><span class="ln">10711 </span></a>    {opt_dim_all_reduce} 
<a name="l10712"><span class="ln">10712 </span></a>    {opt_keepdim} 
<a name="l10713"><span class="ln">10713 </span></a> 
<a name="l10714"><span class="ln">10714 </span></a>Keyword args: 
<a name="l10715"><span class="ln">10715 </span></a>    {dtype} 
<a name="l10716"><span class="ln">10716 </span></a> 
<a name="l10717"><span class="ln">10717 </span></a>Example:: 
<a name="l10718"><span class="ln">10718 </span></a> 
<a name="l10719"><span class="ln">10719 </span></a>    &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l10720"><span class="ln">10720 </span></a>    &gt;&gt;&gt; a 
<a name="l10721"><span class="ln">10721 </span></a>    tensor([[ 0.0569, -0.2475,  0.0737, -0.3429], 
<a name="l10722"><span class="ln">10722 </span></a>            [-0.2993,  0.9138,  0.9337, -1.6864], 
<a name="l10723"><span class="ln">10723 </span></a>            [ 0.1132,  0.7892, -0.1003,  0.5688], 
<a name="l10724"><span class="ln">10724 </span></a>            [ 0.3637, -0.9906, -0.4752, -1.5197]]) 
<a name="l10725"><span class="ln">10725 </span></a>    &gt;&gt;&gt; torch.sum(a, 1) 
<a name="l10726"><span class="ln">10726 </span></a>    tensor([-0.4598, -0.1381,  1.3708, -2.6217]) 
<a name="l10727"><span class="ln">10727 </span></a>    &gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6) 
<a name="l10728"><span class="ln">10728 </span></a>    &gt;&gt;&gt; torch.sum(b, (2, 1)) 
<a name="l10729"><span class="ln">10729 </span></a>    tensor([  435.,  1335.,  2235.,  3135.]) 
<a name="l10730"><span class="ln">10730 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l10731"><span class="ln">10731 </span></a><span class="s3">)</span>
<a name="l10732"><span class="ln">10732 </span></a>
<a name="l10733"><span class="ln">10733 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10734"><span class="ln">10734 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">nansum</span><span class="s3">,</span>
<a name="l10735"><span class="ln">10735 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10736"><span class="ln">10736 </span></a>nansum(input, *, dtype=None) -&gt; Tensor 
<a name="l10737"><span class="ln">10737 </span></a> 
<a name="l10738"><span class="ln">10738 </span></a>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero. 
<a name="l10739"><span class="ln">10739 </span></a> 
<a name="l10740"><span class="ln">10740 </span></a>Args: 
<a name="l10741"><span class="ln">10741 </span></a>    {input} 
<a name="l10742"><span class="ln">10742 </span></a> 
<a name="l10743"><span class="ln">10743 </span></a>Keyword args: 
<a name="l10744"><span class="ln">10744 </span></a>    {dtype} 
<a name="l10745"><span class="ln">10745 </span></a> 
<a name="l10746"><span class="ln">10746 </span></a>Example:: 
<a name="l10747"><span class="ln">10747 </span></a> 
<a name="l10748"><span class="ln">10748 </span></a>    &gt;&gt;&gt; a = torch.tensor([1., 2., float('nan'), 4.]) 
<a name="l10749"><span class="ln">10749 </span></a>    &gt;&gt;&gt; torch.nansum(a) 
<a name="l10750"><span class="ln">10750 </span></a>    tensor(7.) 
<a name="l10751"><span class="ln">10751 </span></a> 
<a name="l10752"><span class="ln">10752 </span></a>.. function:: nansum(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l10753"><span class="ln">10753 </span></a>   :noindex: 
<a name="l10754"><span class="ln">10754 </span></a> 
<a name="l10755"><span class="ln">10755 </span></a>Returns the sum of each row of the :attr:`input` tensor in the given 
<a name="l10756"><span class="ln">10756 </span></a>dimension :attr:`dim`, treating Not a Numbers (NaNs) as zero. 
<a name="l10757"><span class="ln">10757 </span></a>If :attr:`dim` is a list of dimensions, reduce over all of them. 
<a name="l10758"><span class="ln">10758 </span></a> 
<a name="l10759"><span class="ln">10759 </span></a>{keepdim_details} 
<a name="l10760"><span class="ln">10760 </span></a> 
<a name="l10761"><span class="ln">10761 </span></a>Args: 
<a name="l10762"><span class="ln">10762 </span></a>    {input} 
<a name="l10763"><span class="ln">10763 </span></a>    {opt_dim_all_reduce} 
<a name="l10764"><span class="ln">10764 </span></a>    {opt_keepdim} 
<a name="l10765"><span class="ln">10765 </span></a> 
<a name="l10766"><span class="ln">10766 </span></a>Keyword args: 
<a name="l10767"><span class="ln">10767 </span></a>    {dtype} 
<a name="l10768"><span class="ln">10768 </span></a> 
<a name="l10769"><span class="ln">10769 </span></a>Example:: 
<a name="l10770"><span class="ln">10770 </span></a> 
<a name="l10771"><span class="ln">10771 </span></a>    &gt;&gt;&gt; torch.nansum(torch.tensor([1., float(&quot;nan&quot;)])) 
<a name="l10772"><span class="ln">10772 </span></a>    tensor(1.) 
<a name="l10773"><span class="ln">10773 </span></a>    &gt;&gt;&gt; a = torch.tensor([[1, 2], [3., float(&quot;nan&quot;)]]) 
<a name="l10774"><span class="ln">10774 </span></a>    &gt;&gt;&gt; torch.nansum(a) 
<a name="l10775"><span class="ln">10775 </span></a>    tensor(6.) 
<a name="l10776"><span class="ln">10776 </span></a>    &gt;&gt;&gt; torch.nansum(a, dim=0) 
<a name="l10777"><span class="ln">10777 </span></a>    tensor([4., 2.]) 
<a name="l10778"><span class="ln">10778 </span></a>    &gt;&gt;&gt; torch.nansum(a, dim=1) 
<a name="l10779"><span class="ln">10779 </span></a>    tensor([3., 3.]) 
<a name="l10780"><span class="ln">10780 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l10781"><span class="ln">10781 </span></a><span class="s3">)</span>
<a name="l10782"><span class="ln">10782 </span></a>
<a name="l10783"><span class="ln">10783 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10784"><span class="ln">10784 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">svd</span><span class="s3">,</span>
<a name="l10785"><span class="ln">10785 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10786"><span class="ln">10786 </span></a>svd(input, some=True, compute_uv=True, *, out=None) -&gt; (Tensor, Tensor, Tensor) 
<a name="l10787"><span class="ln">10787 </span></a> 
<a name="l10788"><span class="ln">10788 </span></a>Computes the singular value decomposition of either a matrix or batch of 
<a name="l10789"><span class="ln">10789 </span></a>matrices :attr:`input`. The singular value decomposition is represented as a 
<a name="l10790"><span class="ln">10790 </span></a>namedtuple `(U, S, V)`, such that :attr:`input` :math:`= U \text{diag}(S) V^{\text{H}}`. 
<a name="l10791"><span class="ln">10791 </span></a>where :math:`V^{\text{H}}` is the transpose of `V` for real inputs, 
<a name="l10792"><span class="ln">10792 </span></a>and the conjugate transpose of `V` for complex inputs. 
<a name="l10793"><span class="ln">10793 </span></a>If :attr:`input` is a batch of matrices, then `U`, `S`, and `V` are also 
<a name="l10794"><span class="ln">10794 </span></a>batched with the same batch dimensions as :attr:`input`. 
<a name="l10795"><span class="ln">10795 </span></a> 
<a name="l10796"><span class="ln">10796 </span></a>If :attr:`some` is `True` (default), the method returns the reduced singular 
<a name="l10797"><span class="ln">10797 </span></a>value decomposition. In this case, if the last two dimensions of :attr:`input` are 
<a name="l10798"><span class="ln">10798 </span></a>`m` and `n`, then the returned `U` and `V` matrices will contain only 
<a name="l10799"><span class="ln">10799 </span></a>`min(n, m)` orthonormal columns. 
<a name="l10800"><span class="ln">10800 </span></a> 
<a name="l10801"><span class="ln">10801 </span></a>If :attr:`compute_uv` is `False`, the returned `U` and `V` will be 
<a name="l10802"><span class="ln">10802 </span></a>zero-filled matrices of shape `(m, m)` and `(n, n)` 
<a name="l10803"><span class="ln">10803 </span></a>respectively, and the same device as :attr:`input`. The argument :attr:`some` 
<a name="l10804"><span class="ln">10804 </span></a>has no effect when :attr:`compute_uv` is `False`. 
<a name="l10805"><span class="ln">10805 </span></a> 
<a name="l10806"><span class="ln">10806 </span></a>Supports :attr:`input` of float, double, cfloat and cdouble data types. 
<a name="l10807"><span class="ln">10807 </span></a>The dtypes of `U` and `V` are the same as :attr:`input`'s. `S` will 
<a name="l10808"><span class="ln">10808 </span></a>always be real-valued, even if :attr:`input` is complex. 
<a name="l10809"><span class="ln">10809 </span></a> 
<a name="l10810"><span class="ln">10810 </span></a>.. warning:: 
<a name="l10811"><span class="ln">10811 </span></a> 
<a name="l10812"><span class="ln">10812 </span></a>    :func:`torch.svd` is deprecated in favor of :func:`torch.linalg.svd` 
<a name="l10813"><span class="ln">10813 </span></a>    and will be removed in a future PyTorch release. 
<a name="l10814"><span class="ln">10814 </span></a> 
<a name="l10815"><span class="ln">10815 </span></a>    ``U, S, V = torch.svd(A, some=some, compute_uv=True)`` (default) should be replaced with 
<a name="l10816"><span class="ln">10816 </span></a> 
<a name="l10817"><span class="ln">10817 </span></a>    .. code:: python 
<a name="l10818"><span class="ln">10818 </span></a> 
<a name="l10819"><span class="ln">10819 </span></a>        U, S, Vh = torch.linalg.svd(A, full_matrices=not some) 
<a name="l10820"><span class="ln">10820 </span></a>        V = Vh.mH 
<a name="l10821"><span class="ln">10821 </span></a> 
<a name="l10822"><span class="ln">10822 </span></a>    ``_, S, _ = torch.svd(A, some=some, compute_uv=False)`` should be replaced with 
<a name="l10823"><span class="ln">10823 </span></a> 
<a name="l10824"><span class="ln">10824 </span></a>    .. code:: python 
<a name="l10825"><span class="ln">10825 </span></a> 
<a name="l10826"><span class="ln">10826 </span></a>        S = torch.linalg.svdvals(A) 
<a name="l10827"><span class="ln">10827 </span></a> 
<a name="l10828"><span class="ln">10828 </span></a>.. note:: Differences with :func:`torch.linalg.svd`: 
<a name="l10829"><span class="ln">10829 </span></a> 
<a name="l10830"><span class="ln">10830 </span></a>             * :attr:`some` is the opposite of 
<a name="l10831"><span class="ln">10831 </span></a>               :func:`torch.linalg.svd`'s :attr:`full_matrices`. Note that 
<a name="l10832"><span class="ln">10832 </span></a>               default value for both is `True`, so the default behavior is 
<a name="l10833"><span class="ln">10833 </span></a>               effectively the opposite. 
<a name="l10834"><span class="ln">10834 </span></a>             * :func:`torch.svd` returns `V`, whereas :func:`torch.linalg.svd` returns 
<a name="l10835"><span class="ln">10835 </span></a>               `Vh`, that is, :math:`V^{\text{H}}`. 
<a name="l10836"><span class="ln">10836 </span></a>             * If :attr:`compute_uv` is `False`, :func:`torch.svd` returns zero-filled 
<a name="l10837"><span class="ln">10837 </span></a>               tensors for `U` and `Vh`, whereas :func:`torch.linalg.svd` returns 
<a name="l10838"><span class="ln">10838 </span></a>               empty tensors. 
<a name="l10839"><span class="ln">10839 </span></a> 
<a name="l10840"><span class="ln">10840 </span></a>.. note:: The singular values are returned in descending order. If :attr:`input` is a batch of matrices, 
<a name="l10841"><span class="ln">10841 </span></a>          then the singular values of each matrix in the batch are returned in descending order. 
<a name="l10842"><span class="ln">10842 </span></a> 
<a name="l10843"><span class="ln">10843 </span></a>.. note:: The `S` tensor can only be used to compute gradients if :attr:`compute_uv` is `True`. 
<a name="l10844"><span class="ln">10844 </span></a> 
<a name="l10845"><span class="ln">10845 </span></a>.. note:: When :attr:`some` is `False`, the gradients on `U[..., :, min(m, n):]` 
<a name="l10846"><span class="ln">10846 </span></a>          and `V[..., :, min(m, n):]` will be ignored in the backward pass, as those vectors 
<a name="l10847"><span class="ln">10847 </span></a>          can be arbitrary bases of the corresponding subspaces. 
<a name="l10848"><span class="ln">10848 </span></a> 
<a name="l10849"><span class="ln">10849 </span></a>.. note:: The implementation of :func:`torch.linalg.svd` on CPU uses LAPACK's routine `?gesdd` 
<a name="l10850"><span class="ln">10850 </span></a>          (a divide-and-conquer algorithm) instead of `?gesvd` for speed. Analogously, 
<a name="l10851"><span class="ln">10851 </span></a>          on GPU, it uses cuSOLVER's routines `gesvdj` and `gesvdjBatched` on CUDA 10.1.243 
<a name="l10852"><span class="ln">10852 </span></a>          and later, and MAGMA's routine `gesdd` on earlier versions of CUDA. 
<a name="l10853"><span class="ln">10853 </span></a> 
<a name="l10854"><span class="ln">10854 </span></a>.. note:: The returned `U` will not be contiguous. The matrix (or batch of matrices) will 
<a name="l10855"><span class="ln">10855 </span></a>          be represented as a column-major matrix (i.e. Fortran-contiguous). 
<a name="l10856"><span class="ln">10856 </span></a> 
<a name="l10857"><span class="ln">10857 </span></a>.. warning:: The gradients with respect to `U` and `V` will only be finite when the input does not 
<a name="l10858"><span class="ln">10858 </span></a>             have zero nor repeated singular values. 
<a name="l10859"><span class="ln">10859 </span></a> 
<a name="l10860"><span class="ln">10860 </span></a>.. warning:: If the distance between any two singular values is close to zero, the gradients with respect to 
<a name="l10861"><span class="ln">10861 </span></a>             `U` and `V` will be numerically unstable, as they depends on 
<a name="l10862"><span class="ln">10862 </span></a>             :math:`\frac{1}{\min_{i \neq j} \sigma_i^2 - \sigma_j^2}`. The same happens when the matrix 
<a name="l10863"><span class="ln">10863 </span></a>             has small singular values, as these gradients also depend on `S^{-1}`. 
<a name="l10864"><span class="ln">10864 </span></a> 
<a name="l10865"><span class="ln">10865 </span></a>.. warning:: For complex-valued :attr:`input` the singular value decomposition is not unique, 
<a name="l10866"><span class="ln">10866 </span></a>             as `U` and `V` may be multiplied by an arbitrary phase factor :math:`e^{i \phi}` on every column. 
<a name="l10867"><span class="ln">10867 </span></a>             The same happens when :attr:`input` has repeated singular values, where one may multiply 
<a name="l10868"><span class="ln">10868 </span></a>             the columns of the spanning subspace in `U` and `V` by a rotation matrix 
<a name="l10869"><span class="ln">10869 </span></a>             and `the resulting vectors will span the same subspace`_. 
<a name="l10870"><span class="ln">10870 </span></a>             Different platforms, like NumPy, or inputs on different device types, 
<a name="l10871"><span class="ln">10871 </span></a>             may produce different `U` and `V` tensors. 
<a name="l10872"><span class="ln">10872 </span></a> 
<a name="l10873"><span class="ln">10873 </span></a>Args: 
<a name="l10874"><span class="ln">10874 </span></a>    input (Tensor): the input tensor of size `(*, m, n)` where `*` is zero or more 
<a name="l10875"><span class="ln">10875 </span></a>                    batch dimensions consisting of `(m, n)` matrices. 
<a name="l10876"><span class="ln">10876 </span></a>    some (bool, optional): controls whether to compute the reduced or full decomposition, and 
<a name="l10877"><span class="ln">10877 </span></a>                           consequently, the shape of returned `U` and `V`. Default: `True`. 
<a name="l10878"><span class="ln">10878 </span></a>    compute_uv (bool, optional): controls whether to compute `U` and `V`. Default: `True`. 
<a name="l10879"><span class="ln">10879 </span></a> 
<a name="l10880"><span class="ln">10880 </span></a>Keyword args: 
<a name="l10881"><span class="ln">10881 </span></a>    out (tuple, optional): the output tuple of tensors 
<a name="l10882"><span class="ln">10882 </span></a> 
<a name="l10883"><span class="ln">10883 </span></a>Example:: 
<a name="l10884"><span class="ln">10884 </span></a> 
<a name="l10885"><span class="ln">10885 </span></a>    &gt;&gt;&gt; a = torch.randn(5, 3) 
<a name="l10886"><span class="ln">10886 </span></a>    &gt;&gt;&gt; a 
<a name="l10887"><span class="ln">10887 </span></a>    tensor([[ 0.2364, -0.7752,  0.6372], 
<a name="l10888"><span class="ln">10888 </span></a>            [ 1.7201,  0.7394, -0.0504], 
<a name="l10889"><span class="ln">10889 </span></a>            [-0.3371, -1.0584,  0.5296], 
<a name="l10890"><span class="ln">10890 </span></a>            [ 0.3550, -0.4022,  1.5569], 
<a name="l10891"><span class="ln">10891 </span></a>            [ 0.2445, -0.0158,  1.1414]]) 
<a name="l10892"><span class="ln">10892 </span></a>    &gt;&gt;&gt; u, s, v = torch.svd(a) 
<a name="l10893"><span class="ln">10893 </span></a>    &gt;&gt;&gt; u 
<a name="l10894"><span class="ln">10894 </span></a>    tensor([[ 0.4027,  0.0287,  0.5434], 
<a name="l10895"><span class="ln">10895 </span></a>            [-0.1946,  0.8833,  0.3679], 
<a name="l10896"><span class="ln">10896 </span></a>            [ 0.4296, -0.2890,  0.5261], 
<a name="l10897"><span class="ln">10897 </span></a>            [ 0.6604,  0.2717, -0.2618], 
<a name="l10898"><span class="ln">10898 </span></a>            [ 0.4234,  0.2481, -0.4733]]) 
<a name="l10899"><span class="ln">10899 </span></a>    &gt;&gt;&gt; s 
<a name="l10900"><span class="ln">10900 </span></a>    tensor([2.3289, 2.0315, 0.7806]) 
<a name="l10901"><span class="ln">10901 </span></a>    &gt;&gt;&gt; v 
<a name="l10902"><span class="ln">10902 </span></a>    tensor([[-0.0199,  0.8766,  0.4809], 
<a name="l10903"><span class="ln">10903 </span></a>            [-0.5080,  0.4054, -0.7600], 
<a name="l10904"><span class="ln">10904 </span></a>            [ 0.8611,  0.2594, -0.4373]]) 
<a name="l10905"><span class="ln">10905 </span></a>    &gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) 
<a name="l10906"><span class="ln">10906 </span></a>    tensor(8.6531e-07) 
<a name="l10907"><span class="ln">10907 </span></a>    &gt;&gt;&gt; a_big = torch.randn(7, 5, 3) 
<a name="l10908"><span class="ln">10908 </span></a>    &gt;&gt;&gt; u, s, v = torch.svd(a_big) 
<a name="l10909"><span class="ln">10909 </span></a>    &gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.mT)) 
<a name="l10910"><span class="ln">10910 </span></a>    tensor(2.6503e-06) 
<a name="l10911"><span class="ln">10911 </span></a> 
<a name="l10912"><span class="ln">10912 </span></a>.. _the resulting vectors will span the same subspace: 
<a name="l10913"><span class="ln">10913 </span></a>       (https://en.wikipedia.org/wiki/Singular_value_decomposition#Singular_values,_singular_vectors,_and_their_relation_to_the_SVD) 
<a name="l10914"><span class="ln">10914 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l10915"><span class="ln">10915 </span></a><span class="s3">)</span>
<a name="l10916"><span class="ln">10916 </span></a>
<a name="l10917"><span class="ln">10917 </span></a>
<a name="l10918"><span class="ln">10918 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10919"><span class="ln">10919 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">t</span><span class="s3">,</span>
<a name="l10920"><span class="ln">10920 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10921"><span class="ln">10921 </span></a>t(input) -&gt; Tensor 
<a name="l10922"><span class="ln">10922 </span></a> 
<a name="l10923"><span class="ln">10923 </span></a>Expects :attr:`input` to be &lt;= 2-D tensor and transposes dimensions 0 
<a name="l10924"><span class="ln">10924 </span></a>and 1. 
<a name="l10925"><span class="ln">10925 </span></a> 
<a name="l10926"><span class="ln">10926 </span></a>0-D and 1-D tensors are returned as is. When input is a 2-D tensor this 
<a name="l10927"><span class="ln">10927 </span></a>is equivalent to ``transpose(input, 0, 1)``. 
<a name="l10928"><span class="ln">10928 </span></a> 
<a name="l10929"><span class="ln">10929 </span></a>Args: 
<a name="l10930"><span class="ln">10930 </span></a>    {input} 
<a name="l10931"><span class="ln">10931 </span></a> 
<a name="l10932"><span class="ln">10932 </span></a>Example:: 
<a name="l10933"><span class="ln">10933 </span></a> 
<a name="l10934"><span class="ln">10934 </span></a>    &gt;&gt;&gt; x = torch.randn(()) 
<a name="l10935"><span class="ln">10935 </span></a>    &gt;&gt;&gt; x 
<a name="l10936"><span class="ln">10936 </span></a>    tensor(0.1995) 
<a name="l10937"><span class="ln">10937 </span></a>    &gt;&gt;&gt; torch.t(x) 
<a name="l10938"><span class="ln">10938 </span></a>    tensor(0.1995) 
<a name="l10939"><span class="ln">10939 </span></a>    &gt;&gt;&gt; x = torch.randn(3) 
<a name="l10940"><span class="ln">10940 </span></a>    &gt;&gt;&gt; x 
<a name="l10941"><span class="ln">10941 </span></a>    tensor([ 2.4320, -0.4608,  0.7702]) 
<a name="l10942"><span class="ln">10942 </span></a>    &gt;&gt;&gt; torch.t(x) 
<a name="l10943"><span class="ln">10943 </span></a>    tensor([ 2.4320, -0.4608,  0.7702]) 
<a name="l10944"><span class="ln">10944 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l10945"><span class="ln">10945 </span></a>    &gt;&gt;&gt; x 
<a name="l10946"><span class="ln">10946 </span></a>    tensor([[ 0.4875,  0.9158, -0.5872], 
<a name="l10947"><span class="ln">10947 </span></a>            [ 0.3938, -0.6929,  0.6932]]) 
<a name="l10948"><span class="ln">10948 </span></a>    &gt;&gt;&gt; torch.t(x) 
<a name="l10949"><span class="ln">10949 </span></a>    tensor([[ 0.4875,  0.3938], 
<a name="l10950"><span class="ln">10950 </span></a>            [ 0.9158, -0.6929], 
<a name="l10951"><span class="ln">10951 </span></a>            [-0.5872,  0.6932]]) 
<a name="l10952"><span class="ln">10952 </span></a> 
<a name="l10953"><span class="ln">10953 </span></a>See also :func:`torch.transpose`. 
<a name="l10954"><span class="ln">10954 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l10955"><span class="ln">10955 </span></a><span class="s3">)</span>
<a name="l10956"><span class="ln">10956 </span></a>
<a name="l10957"><span class="ln">10957 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10958"><span class="ln">10958 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">flip</span><span class="s3">,</span>
<a name="l10959"><span class="ln">10959 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10960"><span class="ln">10960 </span></a>flip(input, dims) -&gt; Tensor 
<a name="l10961"><span class="ln">10961 </span></a> 
<a name="l10962"><span class="ln">10962 </span></a>Reverse the order of an n-D tensor along given axis in dims. 
<a name="l10963"><span class="ln">10963 </span></a> 
<a name="l10964"><span class="ln">10964 </span></a>.. note:: 
<a name="l10965"><span class="ln">10965 </span></a>    `torch.flip` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.flip`, 
<a name="l10966"><span class="ln">10966 </span></a>    which returns a view in constant time. Since copying a tensor's data is more work than viewing that data, 
<a name="l10967"><span class="ln">10967 </span></a>    `torch.flip` is expected to be slower than `np.flip`. 
<a name="l10968"><span class="ln">10968 </span></a> 
<a name="l10969"><span class="ln">10969 </span></a>Args: 
<a name="l10970"><span class="ln">10970 </span></a>    {input} 
<a name="l10971"><span class="ln">10971 </span></a>    dims (a list or tuple): axis to flip on 
<a name="l10972"><span class="ln">10972 </span></a> 
<a name="l10973"><span class="ln">10973 </span></a>Example:: 
<a name="l10974"><span class="ln">10974 </span></a> 
<a name="l10975"><span class="ln">10975 </span></a>    &gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2) 
<a name="l10976"><span class="ln">10976 </span></a>    &gt;&gt;&gt; x 
<a name="l10977"><span class="ln">10977 </span></a>    tensor([[[ 0,  1], 
<a name="l10978"><span class="ln">10978 </span></a>             [ 2,  3]], 
<a name="l10979"><span class="ln">10979 </span></a> 
<a name="l10980"><span class="ln">10980 </span></a>            [[ 4,  5], 
<a name="l10981"><span class="ln">10981 </span></a>             [ 6,  7]]]) 
<a name="l10982"><span class="ln">10982 </span></a>    &gt;&gt;&gt; torch.flip(x, [0, 1]) 
<a name="l10983"><span class="ln">10983 </span></a>    tensor([[[ 6,  7], 
<a name="l10984"><span class="ln">10984 </span></a>             [ 4,  5]], 
<a name="l10985"><span class="ln">10985 </span></a> 
<a name="l10986"><span class="ln">10986 </span></a>            [[ 2,  3], 
<a name="l10987"><span class="ln">10987 </span></a>             [ 0,  1]]]) 
<a name="l10988"><span class="ln">10988 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l10989"><span class="ln">10989 </span></a><span class="s3">)</span>
<a name="l10990"><span class="ln">10990 </span></a>
<a name="l10991"><span class="ln">10991 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l10992"><span class="ln">10992 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">fliplr</span><span class="s3">,</span>
<a name="l10993"><span class="ln">10993 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l10994"><span class="ln">10994 </span></a>fliplr(input) -&gt; Tensor 
<a name="l10995"><span class="ln">10995 </span></a> 
<a name="l10996"><span class="ln">10996 </span></a>Flip tensor in the left/right direction, returning a new tensor. 
<a name="l10997"><span class="ln">10997 </span></a> 
<a name="l10998"><span class="ln">10998 </span></a>Flip the entries in each row in the left/right direction. 
<a name="l10999"><span class="ln">10999 </span></a>Columns are preserved, but appear in a different order than before. 
<a name="l11000"><span class="ln">11000 </span></a> 
<a name="l11001"><span class="ln">11001 </span></a>Note: 
<a name="l11002"><span class="ln">11002 </span></a>    Requires the tensor to be at least 2-D. 
<a name="l11003"><span class="ln">11003 </span></a> 
<a name="l11004"><span class="ln">11004 </span></a>.. note:: 
<a name="l11005"><span class="ln">11005 </span></a>    `torch.fliplr` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.fliplr`, 
<a name="l11006"><span class="ln">11006 </span></a>    which returns a view in constant time. Since copying a tensor's data is more work than viewing that data, 
<a name="l11007"><span class="ln">11007 </span></a>    `torch.fliplr` is expected to be slower than `np.fliplr`. 
<a name="l11008"><span class="ln">11008 </span></a> 
<a name="l11009"><span class="ln">11009 </span></a>Args: 
<a name="l11010"><span class="ln">11010 </span></a>    input (Tensor): Must be at least 2-dimensional. 
<a name="l11011"><span class="ln">11011 </span></a> 
<a name="l11012"><span class="ln">11012 </span></a>Example:: 
<a name="l11013"><span class="ln">11013 </span></a> 
<a name="l11014"><span class="ln">11014 </span></a>    &gt;&gt;&gt; x = torch.arange(4).view(2, 2) 
<a name="l11015"><span class="ln">11015 </span></a>    &gt;&gt;&gt; x 
<a name="l11016"><span class="ln">11016 </span></a>    tensor([[0, 1], 
<a name="l11017"><span class="ln">11017 </span></a>            [2, 3]]) 
<a name="l11018"><span class="ln">11018 </span></a>    &gt;&gt;&gt; torch.fliplr(x) 
<a name="l11019"><span class="ln">11019 </span></a>    tensor([[1, 0], 
<a name="l11020"><span class="ln">11020 </span></a>            [3, 2]]) 
<a name="l11021"><span class="ln">11021 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11022"><span class="ln">11022 </span></a><span class="s3">)</span>
<a name="l11023"><span class="ln">11023 </span></a>
<a name="l11024"><span class="ln">11024 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11025"><span class="ln">11025 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">flipud</span><span class="s3">,</span>
<a name="l11026"><span class="ln">11026 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11027"><span class="ln">11027 </span></a>flipud(input) -&gt; Tensor 
<a name="l11028"><span class="ln">11028 </span></a> 
<a name="l11029"><span class="ln">11029 </span></a>Flip tensor in the up/down direction, returning a new tensor. 
<a name="l11030"><span class="ln">11030 </span></a> 
<a name="l11031"><span class="ln">11031 </span></a>Flip the entries in each column in the up/down direction. 
<a name="l11032"><span class="ln">11032 </span></a>Rows are preserved, but appear in a different order than before. 
<a name="l11033"><span class="ln">11033 </span></a> 
<a name="l11034"><span class="ln">11034 </span></a>Note: 
<a name="l11035"><span class="ln">11035 </span></a>    Requires the tensor to be at least 1-D. 
<a name="l11036"><span class="ln">11036 </span></a> 
<a name="l11037"><span class="ln">11037 </span></a>.. note:: 
<a name="l11038"><span class="ln">11038 </span></a>    `torch.flipud` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.flipud`, 
<a name="l11039"><span class="ln">11039 </span></a>    which returns a view in constant time. Since copying a tensor's data is more work than viewing that data, 
<a name="l11040"><span class="ln">11040 </span></a>    `torch.flipud` is expected to be slower than `np.flipud`. 
<a name="l11041"><span class="ln">11041 </span></a> 
<a name="l11042"><span class="ln">11042 </span></a>Args: 
<a name="l11043"><span class="ln">11043 </span></a>    input (Tensor): Must be at least 1-dimensional. 
<a name="l11044"><span class="ln">11044 </span></a> 
<a name="l11045"><span class="ln">11045 </span></a>Example:: 
<a name="l11046"><span class="ln">11046 </span></a> 
<a name="l11047"><span class="ln">11047 </span></a>    &gt;&gt;&gt; x = torch.arange(4).view(2, 2) 
<a name="l11048"><span class="ln">11048 </span></a>    &gt;&gt;&gt; x 
<a name="l11049"><span class="ln">11049 </span></a>    tensor([[0, 1], 
<a name="l11050"><span class="ln">11050 </span></a>            [2, 3]]) 
<a name="l11051"><span class="ln">11051 </span></a>    &gt;&gt;&gt; torch.flipud(x) 
<a name="l11052"><span class="ln">11052 </span></a>    tensor([[2, 3], 
<a name="l11053"><span class="ln">11053 </span></a>            [0, 1]]) 
<a name="l11054"><span class="ln">11054 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11055"><span class="ln">11055 </span></a><span class="s3">)</span>
<a name="l11056"><span class="ln">11056 </span></a>
<a name="l11057"><span class="ln">11057 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11058"><span class="ln">11058 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">roll</span><span class="s3">,</span>
<a name="l11059"><span class="ln">11059 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11060"><span class="ln">11060 </span></a>roll(input, shifts, dims=None) -&gt; Tensor 
<a name="l11061"><span class="ln">11061 </span></a> 
<a name="l11062"><span class="ln">11062 </span></a>Roll the tensor :attr:`input` along the given dimension(s). Elements that are 
<a name="l11063"><span class="ln">11063 </span></a>shifted beyond the last position are re-introduced at the first position. If 
<a name="l11064"><span class="ln">11064 </span></a>:attr:`dims` is `None`, the tensor will be flattened before rolling and then 
<a name="l11065"><span class="ln">11065 </span></a>restored to the original shape. 
<a name="l11066"><span class="ln">11066 </span></a> 
<a name="l11067"><span class="ln">11067 </span></a>Args: 
<a name="l11068"><span class="ln">11068 </span></a>    {input} 
<a name="l11069"><span class="ln">11069 </span></a>    shifts (int or tuple of ints): The number of places by which the elements 
<a name="l11070"><span class="ln">11070 </span></a>        of the tensor are shifted. If shifts is a tuple, dims must be a tuple of 
<a name="l11071"><span class="ln">11071 </span></a>        the same size, and each dimension will be rolled by the corresponding 
<a name="l11072"><span class="ln">11072 </span></a>        value 
<a name="l11073"><span class="ln">11073 </span></a>    dims (int or tuple of ints): Axis along which to roll 
<a name="l11074"><span class="ln">11074 </span></a> 
<a name="l11075"><span class="ln">11075 </span></a>Example:: 
<a name="l11076"><span class="ln">11076 </span></a> 
<a name="l11077"><span class="ln">11077 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2) 
<a name="l11078"><span class="ln">11078 </span></a>    &gt;&gt;&gt; x 
<a name="l11079"><span class="ln">11079 </span></a>    tensor([[1, 2], 
<a name="l11080"><span class="ln">11080 </span></a>            [3, 4], 
<a name="l11081"><span class="ln">11081 </span></a>            [5, 6], 
<a name="l11082"><span class="ln">11082 </span></a>            [7, 8]]) 
<a name="l11083"><span class="ln">11083 </span></a>    &gt;&gt;&gt; torch.roll(x, 1) 
<a name="l11084"><span class="ln">11084 </span></a>    tensor([[8, 1], 
<a name="l11085"><span class="ln">11085 </span></a>            [2, 3], 
<a name="l11086"><span class="ln">11086 </span></a>            [4, 5], 
<a name="l11087"><span class="ln">11087 </span></a>            [6, 7]]) 
<a name="l11088"><span class="ln">11088 </span></a>    &gt;&gt;&gt; torch.roll(x, 1, 0) 
<a name="l11089"><span class="ln">11089 </span></a>    tensor([[7, 8], 
<a name="l11090"><span class="ln">11090 </span></a>            [1, 2], 
<a name="l11091"><span class="ln">11091 </span></a>            [3, 4], 
<a name="l11092"><span class="ln">11092 </span></a>            [5, 6]]) 
<a name="l11093"><span class="ln">11093 </span></a>    &gt;&gt;&gt; torch.roll(x, -1, 0) 
<a name="l11094"><span class="ln">11094 </span></a>    tensor([[3, 4], 
<a name="l11095"><span class="ln">11095 </span></a>            [5, 6], 
<a name="l11096"><span class="ln">11096 </span></a>            [7, 8], 
<a name="l11097"><span class="ln">11097 </span></a>            [1, 2]]) 
<a name="l11098"><span class="ln">11098 </span></a>    &gt;&gt;&gt; torch.roll(x, shifts=(2, 1), dims=(0, 1)) 
<a name="l11099"><span class="ln">11099 </span></a>    tensor([[6, 5], 
<a name="l11100"><span class="ln">11100 </span></a>            [8, 7], 
<a name="l11101"><span class="ln">11101 </span></a>            [2, 1], 
<a name="l11102"><span class="ln">11102 </span></a>            [4, 3]]) 
<a name="l11103"><span class="ln">11103 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11104"><span class="ln">11104 </span></a><span class="s3">)</span>
<a name="l11105"><span class="ln">11105 </span></a>
<a name="l11106"><span class="ln">11106 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11107"><span class="ln">11107 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">rot90</span><span class="s3">,</span>
<a name="l11108"><span class="ln">11108 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11109"><span class="ln">11109 </span></a>rot90(input, k=1, dims=(0, 1)) -&gt; Tensor 
<a name="l11110"><span class="ln">11110 </span></a> 
<a name="l11111"><span class="ln">11111 </span></a>Rotate an n-D tensor by 90 degrees in the plane specified by dims axis. 
<a name="l11112"><span class="ln">11112 </span></a>Rotation direction is from the first towards the second axis if k &gt; 0, and from the second towards the first for k &lt; 0. 
<a name="l11113"><span class="ln">11113 </span></a> 
<a name="l11114"><span class="ln">11114 </span></a>Args: 
<a name="l11115"><span class="ln">11115 </span></a>    {input} 
<a name="l11116"><span class="ln">11116 </span></a>    k (int): number of times to rotate. Default value is 1 
<a name="l11117"><span class="ln">11117 </span></a>    dims (a list or tuple): axis to rotate. Default value is [0, 1] 
<a name="l11118"><span class="ln">11118 </span></a> 
<a name="l11119"><span class="ln">11119 </span></a>Example:: 
<a name="l11120"><span class="ln">11120 </span></a> 
<a name="l11121"><span class="ln">11121 </span></a>    &gt;&gt;&gt; x = torch.arange(4).view(2, 2) 
<a name="l11122"><span class="ln">11122 </span></a>    &gt;&gt;&gt; x 
<a name="l11123"><span class="ln">11123 </span></a>    tensor([[0, 1], 
<a name="l11124"><span class="ln">11124 </span></a>            [2, 3]]) 
<a name="l11125"><span class="ln">11125 </span></a>    &gt;&gt;&gt; torch.rot90(x, 1, [0, 1]) 
<a name="l11126"><span class="ln">11126 </span></a>    tensor([[1, 3], 
<a name="l11127"><span class="ln">11127 </span></a>            [0, 2]]) 
<a name="l11128"><span class="ln">11128 </span></a> 
<a name="l11129"><span class="ln">11129 </span></a>    &gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2) 
<a name="l11130"><span class="ln">11130 </span></a>    &gt;&gt;&gt; x 
<a name="l11131"><span class="ln">11131 </span></a>    tensor([[[0, 1], 
<a name="l11132"><span class="ln">11132 </span></a>             [2, 3]], 
<a name="l11133"><span class="ln">11133 </span></a> 
<a name="l11134"><span class="ln">11134 </span></a>            [[4, 5], 
<a name="l11135"><span class="ln">11135 </span></a>             [6, 7]]]) 
<a name="l11136"><span class="ln">11136 </span></a>    &gt;&gt;&gt; torch.rot90(x, 1, [1, 2]) 
<a name="l11137"><span class="ln">11137 </span></a>    tensor([[[1, 3], 
<a name="l11138"><span class="ln">11138 </span></a>             [0, 2]], 
<a name="l11139"><span class="ln">11139 </span></a> 
<a name="l11140"><span class="ln">11140 </span></a>            [[5, 7], 
<a name="l11141"><span class="ln">11141 </span></a>             [4, 6]]]) 
<a name="l11142"><span class="ln">11142 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11143"><span class="ln">11143 </span></a><span class="s3">)</span>
<a name="l11144"><span class="ln">11144 </span></a>
<a name="l11145"><span class="ln">11145 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11146"><span class="ln">11146 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">take</span><span class="s3">,</span>
<a name="l11147"><span class="ln">11147 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11148"><span class="ln">11148 </span></a>take(input, index) -&gt; Tensor 
<a name="l11149"><span class="ln">11149 </span></a> 
<a name="l11150"><span class="ln">11150 </span></a>Returns a new tensor with the elements of :attr:`input` at the given indices. 
<a name="l11151"><span class="ln">11151 </span></a>The input tensor is treated as if it were viewed as a 1-D tensor. The result 
<a name="l11152"><span class="ln">11152 </span></a>takes the same shape as the indices. 
<a name="l11153"><span class="ln">11153 </span></a> 
<a name="l11154"><span class="ln">11154 </span></a>Args: 
<a name="l11155"><span class="ln">11155 </span></a>    {input} 
<a name="l11156"><span class="ln">11156 </span></a>    index (LongTensor): the indices into tensor 
<a name="l11157"><span class="ln">11157 </span></a> 
<a name="l11158"><span class="ln">11158 </span></a>Example:: 
<a name="l11159"><span class="ln">11159 </span></a> 
<a name="l11160"><span class="ln">11160 </span></a>    &gt;&gt;&gt; src = torch.tensor([[4, 3, 5], 
<a name="l11161"><span class="ln">11161 </span></a>    ...                     [6, 7, 8]]) 
<a name="l11162"><span class="ln">11162 </span></a>    &gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5])) 
<a name="l11163"><span class="ln">11163 </span></a>    tensor([ 4,  5,  8]) 
<a name="l11164"><span class="ln">11164 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11165"><span class="ln">11165 </span></a><span class="s3">)</span>
<a name="l11166"><span class="ln">11166 </span></a>
<a name="l11167"><span class="ln">11167 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11168"><span class="ln">11168 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">take_along_dim</span><span class="s3">,</span>
<a name="l11169"><span class="ln">11169 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11170"><span class="ln">11170 </span></a>take_along_dim(input, indices, dim=None, *, out=None) -&gt; Tensor 
<a name="l11171"><span class="ln">11171 </span></a> 
<a name="l11172"><span class="ln">11172 </span></a>Selects values from :attr:`input` at the 1-dimensional indices from :attr:`indices` along the given :attr:`dim`. 
<a name="l11173"><span class="ln">11173 </span></a> 
<a name="l11174"><span class="ln">11174 </span></a>If :attr:`dim` is None, the input array is treated as if it has been flattened to 1d. 
<a name="l11175"><span class="ln">11175 </span></a> 
<a name="l11176"><span class="ln">11176 </span></a>Functions that return indices along a dimension, like :func:`torch.argmax` and :func:`torch.argsort`, 
<a name="l11177"><span class="ln">11177 </span></a>are designed to work with this function. See the examples below. 
<a name="l11178"><span class="ln">11178 </span></a> 
<a name="l11179"><span class="ln">11179 </span></a>.. note:: 
<a name="l11180"><span class="ln">11180 </span></a>    This function is similar to NumPy's `take_along_axis`. 
<a name="l11181"><span class="ln">11181 </span></a>    See also :func:`torch.gather`. 
<a name="l11182"><span class="ln">11182 </span></a> 
<a name="l11183"><span class="ln">11183 </span></a>Args: 
<a name="l11184"><span class="ln">11184 </span></a>    {input} 
<a name="l11185"><span class="ln">11185 </span></a>    indices (LongTensor): the indices into :attr:`input`. Must have long dtype. 
<a name="l11186"><span class="ln">11186 </span></a>    dim (int, optional): dimension to select along. Default: 0 
<a name="l11187"><span class="ln">11187 </span></a> 
<a name="l11188"><span class="ln">11188 </span></a>Keyword args: 
<a name="l11189"><span class="ln">11189 </span></a>    {out} 
<a name="l11190"><span class="ln">11190 </span></a> 
<a name="l11191"><span class="ln">11191 </span></a>Example:: 
<a name="l11192"><span class="ln">11192 </span></a> 
<a name="l11193"><span class="ln">11193 </span></a>    &gt;&gt;&gt; t = torch.tensor([[10, 30, 20], [60, 40, 50]]) 
<a name="l11194"><span class="ln">11194 </span></a>    &gt;&gt;&gt; max_idx = torch.argmax(t) 
<a name="l11195"><span class="ln">11195 </span></a>    &gt;&gt;&gt; torch.take_along_dim(t, max_idx) 
<a name="l11196"><span class="ln">11196 </span></a>    tensor([60]) 
<a name="l11197"><span class="ln">11197 </span></a>    &gt;&gt;&gt; sorted_idx = torch.argsort(t, dim=1) 
<a name="l11198"><span class="ln">11198 </span></a>    &gt;&gt;&gt; torch.take_along_dim(t, sorted_idx, dim=1) 
<a name="l11199"><span class="ln">11199 </span></a>    tensor([[10, 20, 30], 
<a name="l11200"><span class="ln">11200 </span></a>            [40, 50, 60]]) 
<a name="l11201"><span class="ln">11201 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11202"><span class="ln">11202 </span></a><span class="s3">)</span>
<a name="l11203"><span class="ln">11203 </span></a>
<a name="l11204"><span class="ln">11204 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11205"><span class="ln">11205 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">tan</span><span class="s3">,</span>
<a name="l11206"><span class="ln">11206 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11207"><span class="ln">11207 </span></a>tan(input, *, out=None) -&gt; Tensor 
<a name="l11208"><span class="ln">11208 </span></a> 
<a name="l11209"><span class="ln">11209 </span></a>Returns a new tensor with the tangent of the elements of :attr:`input`. 
<a name="l11210"><span class="ln">11210 </span></a> 
<a name="l11211"><span class="ln">11211 </span></a>.. math:: 
<a name="l11212"><span class="ln">11212 </span></a>    \text{out}_{i} = \tan(\text{input}_{i}) 
<a name="l11213"><span class="ln">11213 </span></a>&quot;&quot;&quot;</span>
<a name="l11214"><span class="ln">11214 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11215"><span class="ln">11215 </span></a>Args: 
<a name="l11216"><span class="ln">11216 </span></a>    {input} 
<a name="l11217"><span class="ln">11217 </span></a> 
<a name="l11218"><span class="ln">11218 </span></a>Keyword args: 
<a name="l11219"><span class="ln">11219 </span></a>    {out} 
<a name="l11220"><span class="ln">11220 </span></a> 
<a name="l11221"><span class="ln">11221 </span></a>Example:: 
<a name="l11222"><span class="ln">11222 </span></a> 
<a name="l11223"><span class="ln">11223 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l11224"><span class="ln">11224 </span></a>    &gt;&gt;&gt; a 
<a name="l11225"><span class="ln">11225 </span></a>    tensor([-1.2027, -1.7687,  0.4412, -1.3856]) 
<a name="l11226"><span class="ln">11226 </span></a>    &gt;&gt;&gt; torch.tan(a) 
<a name="l11227"><span class="ln">11227 </span></a>    tensor([-2.5930,  4.9859,  0.4722, -5.3366]) 
<a name="l11228"><span class="ln">11228 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11229"><span class="ln">11229 </span></a><span class="s3">)</span>
<a name="l11230"><span class="ln">11230 </span></a>
<a name="l11231"><span class="ln">11231 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11232"><span class="ln">11232 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">tanh</span><span class="s3">,</span>
<a name="l11233"><span class="ln">11233 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11234"><span class="ln">11234 </span></a>tanh(input, *, out=None) -&gt; Tensor 
<a name="l11235"><span class="ln">11235 </span></a> 
<a name="l11236"><span class="ln">11236 </span></a>Returns a new tensor with the hyperbolic tangent of the elements 
<a name="l11237"><span class="ln">11237 </span></a>of :attr:`input`. 
<a name="l11238"><span class="ln">11238 </span></a> 
<a name="l11239"><span class="ln">11239 </span></a>.. math:: 
<a name="l11240"><span class="ln">11240 </span></a>    \text{out}_{i} = \tanh(\text{input}_{i}) 
<a name="l11241"><span class="ln">11241 </span></a>&quot;&quot;&quot;</span>
<a name="l11242"><span class="ln">11242 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11243"><span class="ln">11243 </span></a>Args: 
<a name="l11244"><span class="ln">11244 </span></a>    {input} 
<a name="l11245"><span class="ln">11245 </span></a> 
<a name="l11246"><span class="ln">11246 </span></a>Keyword args: 
<a name="l11247"><span class="ln">11247 </span></a>    {out} 
<a name="l11248"><span class="ln">11248 </span></a> 
<a name="l11249"><span class="ln">11249 </span></a>Example:: 
<a name="l11250"><span class="ln">11250 </span></a> 
<a name="l11251"><span class="ln">11251 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l11252"><span class="ln">11252 </span></a>    &gt;&gt;&gt; a 
<a name="l11253"><span class="ln">11253 </span></a>    tensor([ 0.8986, -0.7279,  1.1745,  0.2611]) 
<a name="l11254"><span class="ln">11254 </span></a>    &gt;&gt;&gt; torch.tanh(a) 
<a name="l11255"><span class="ln">11255 </span></a>    tensor([ 0.7156, -0.6218,  0.8257,  0.2553]) 
<a name="l11256"><span class="ln">11256 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11257"><span class="ln">11257 </span></a><span class="s3">)</span>
<a name="l11258"><span class="ln">11258 </span></a>
<a name="l11259"><span class="ln">11259 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11260"><span class="ln">11260 </span></a>    <span class="s0"># torch.softmax doc str. Point this to torch.nn.functional.softmax</span>
<a name="l11261"><span class="ln">11261 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">softmax</span><span class="s3">,</span>
<a name="l11262"><span class="ln">11262 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11263"><span class="ln">11263 </span></a>softmax(input, dim, *, dtype=None) -&gt; Tensor 
<a name="l11264"><span class="ln">11264 </span></a> 
<a name="l11265"><span class="ln">11265 </span></a>Alias for :func:`torch.nn.functional.softmax`. 
<a name="l11266"><span class="ln">11266 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l11267"><span class="ln">11267 </span></a><span class="s3">)</span>
<a name="l11268"><span class="ln">11268 </span></a>
<a name="l11269"><span class="ln">11269 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11270"><span class="ln">11270 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">topk</span><span class="s3">,</span>
<a name="l11271"><span class="ln">11271 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11272"><span class="ln">11272 </span></a>topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l11273"><span class="ln">11273 </span></a> 
<a name="l11274"><span class="ln">11274 </span></a>Returns the :attr:`k` largest elements of the given :attr:`input` tensor along 
<a name="l11275"><span class="ln">11275 </span></a>a given dimension. 
<a name="l11276"><span class="ln">11276 </span></a> 
<a name="l11277"><span class="ln">11277 </span></a>If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l11278"><span class="ln">11278 </span></a> 
<a name="l11279"><span class="ln">11279 </span></a>If :attr:`largest` is ``False`` then the `k` smallest elements are returned. 
<a name="l11280"><span class="ln">11280 </span></a> 
<a name="l11281"><span class="ln">11281 </span></a>A namedtuple of `(values, indices)` is returned with the `values` and 
<a name="l11282"><span class="ln">11282 </span></a>`indices` of the largest `k` elements of each row of the `input` tensor in the 
<a name="l11283"><span class="ln">11283 </span></a>given dimension `dim`. 
<a name="l11284"><span class="ln">11284 </span></a> 
<a name="l11285"><span class="ln">11285 </span></a>The boolean option :attr:`sorted` if ``True``, will make sure that the returned 
<a name="l11286"><span class="ln">11286 </span></a>`k` elements are themselves sorted 
<a name="l11287"><span class="ln">11287 </span></a> 
<a name="l11288"><span class="ln">11288 </span></a>.. note:: 
<a name="l11289"><span class="ln">11289 </span></a>    When using `torch.topk`, the indices of tied elements are not guaranteed to be stable 
<a name="l11290"><span class="ln">11290 </span></a>    and may vary across different invocations. 
<a name="l11291"><span class="ln">11291 </span></a> 
<a name="l11292"><span class="ln">11292 </span></a>Args: 
<a name="l11293"><span class="ln">11293 </span></a>    {input} 
<a name="l11294"><span class="ln">11294 </span></a>    k (int): the k in &quot;top-k&quot; 
<a name="l11295"><span class="ln">11295 </span></a>    dim (int, optional): the dimension to sort along 
<a name="l11296"><span class="ln">11296 </span></a>    largest (bool, optional): controls whether to return largest or 
<a name="l11297"><span class="ln">11297 </span></a>           smallest elements 
<a name="l11298"><span class="ln">11298 </span></a>    sorted (bool, optional): controls whether to return the elements 
<a name="l11299"><span class="ln">11299 </span></a>           in sorted order 
<a name="l11300"><span class="ln">11300 </span></a> 
<a name="l11301"><span class="ln">11301 </span></a>Keyword args: 
<a name="l11302"><span class="ln">11302 </span></a>    out (tuple, optional): the output tuple of (Tensor, LongTensor) that can be 
<a name="l11303"><span class="ln">11303 </span></a>        optionally given to be used as output buffers 
<a name="l11304"><span class="ln">11304 </span></a> 
<a name="l11305"><span class="ln">11305 </span></a>Example:: 
<a name="l11306"><span class="ln">11306 </span></a> 
<a name="l11307"><span class="ln">11307 </span></a>    &gt;&gt;&gt; x = torch.arange(1., 6.) 
<a name="l11308"><span class="ln">11308 </span></a>    &gt;&gt;&gt; x 
<a name="l11309"><span class="ln">11309 </span></a>    tensor([ 1.,  2.,  3.,  4.,  5.]) 
<a name="l11310"><span class="ln">11310 </span></a>    &gt;&gt;&gt; torch.topk(x, 3) 
<a name="l11311"><span class="ln">11311 </span></a>    torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2])) 
<a name="l11312"><span class="ln">11312 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11313"><span class="ln">11313 </span></a><span class="s3">)</span>
<a name="l11314"><span class="ln">11314 </span></a>
<a name="l11315"><span class="ln">11315 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11316"><span class="ln">11316 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">trace</span><span class="s3">,</span>
<a name="l11317"><span class="ln">11317 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11318"><span class="ln">11318 </span></a>trace(input) -&gt; Tensor 
<a name="l11319"><span class="ln">11319 </span></a> 
<a name="l11320"><span class="ln">11320 </span></a>Returns the sum of the elements of the diagonal of the input 2-D matrix. 
<a name="l11321"><span class="ln">11321 </span></a> 
<a name="l11322"><span class="ln">11322 </span></a>Example:: 
<a name="l11323"><span class="ln">11323 </span></a> 
<a name="l11324"><span class="ln">11324 </span></a>    &gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3) 
<a name="l11325"><span class="ln">11325 </span></a>    &gt;&gt;&gt; x 
<a name="l11326"><span class="ln">11326 </span></a>    tensor([[ 1.,  2.,  3.], 
<a name="l11327"><span class="ln">11327 </span></a>            [ 4.,  5.,  6.], 
<a name="l11328"><span class="ln">11328 </span></a>            [ 7.,  8.,  9.]]) 
<a name="l11329"><span class="ln">11329 </span></a>    &gt;&gt;&gt; torch.trace(x) 
<a name="l11330"><span class="ln">11330 </span></a>    tensor(15.) 
<a name="l11331"><span class="ln">11331 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l11332"><span class="ln">11332 </span></a><span class="s3">)</span>
<a name="l11333"><span class="ln">11333 </span></a>
<a name="l11334"><span class="ln">11334 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11335"><span class="ln">11335 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">transpose</span><span class="s3">,</span>
<a name="l11336"><span class="ln">11336 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11337"><span class="ln">11337 </span></a>transpose(input, dim0, dim1) -&gt; Tensor 
<a name="l11338"><span class="ln">11338 </span></a> 
<a name="l11339"><span class="ln">11339 </span></a>Returns a tensor that is a transposed version of :attr:`input`. 
<a name="l11340"><span class="ln">11340 </span></a>The given dimensions :attr:`dim0` and :attr:`dim1` are swapped. 
<a name="l11341"><span class="ln">11341 </span></a> 
<a name="l11342"><span class="ln">11342 </span></a>If :attr:`input` is a strided tensor then the resulting :attr:`out` 
<a name="l11343"><span class="ln">11343 </span></a>tensor shares its underlying storage with the :attr:`input` tensor, so 
<a name="l11344"><span class="ln">11344 </span></a>changing the content of one would change the content of the other. 
<a name="l11345"><span class="ln">11345 </span></a> 
<a name="l11346"><span class="ln">11346 </span></a>If :attr:`input` is a :ref:`sparse tensor &lt;sparse-docs&gt;` then the 
<a name="l11347"><span class="ln">11347 </span></a>resulting :attr:`out` tensor *does not* share the underlying storage 
<a name="l11348"><span class="ln">11348 </span></a>with the :attr:`input` tensor. 
<a name="l11349"><span class="ln">11349 </span></a> 
<a name="l11350"><span class="ln">11350 </span></a>If :attr:`input` is a :ref:`sparse tensor &lt;sparse-docs&gt;` with compressed 
<a name="l11351"><span class="ln">11351 </span></a>layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments 
<a name="l11352"><span class="ln">11352 </span></a>:attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must 
<a name="l11353"><span class="ln">11353 </span></a>both be sparse dimensions. The batch dimensions of a sparse tensor are the 
<a name="l11354"><span class="ln">11354 </span></a>dimensions preceding the sparse dimensions. 
<a name="l11355"><span class="ln">11355 </span></a> 
<a name="l11356"><span class="ln">11356 </span></a>.. note:: 
<a name="l11357"><span class="ln">11357 </span></a>    Transpositions which interchange the sparse dimensions of a `SparseCSR` 
<a name="l11358"><span class="ln">11358 </span></a>    or `SparseCSC` layout tensor will result in the layout changing between 
<a name="l11359"><span class="ln">11359 </span></a>    the two options. Transposition of the sparse dimensions of a ` SparseBSR` 
<a name="l11360"><span class="ln">11360 </span></a>    or `SparseBSC` layout tensor will likewise generate a result with the 
<a name="l11361"><span class="ln">11361 </span></a>    opposite layout. 
<a name="l11362"><span class="ln">11362 </span></a> 
<a name="l11363"><span class="ln">11363 </span></a> 
<a name="l11364"><span class="ln">11364 </span></a>Args: 
<a name="l11365"><span class="ln">11365 </span></a>    {input} 
<a name="l11366"><span class="ln">11366 </span></a>    dim0 (int): the first dimension to be transposed 
<a name="l11367"><span class="ln">11367 </span></a>    dim1 (int): the second dimension to be transposed 
<a name="l11368"><span class="ln">11368 </span></a> 
<a name="l11369"><span class="ln">11369 </span></a>Example:: 
<a name="l11370"><span class="ln">11370 </span></a> 
<a name="l11371"><span class="ln">11371 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l11372"><span class="ln">11372 </span></a>    &gt;&gt;&gt; x 
<a name="l11373"><span class="ln">11373 </span></a>    tensor([[ 1.0028, -0.9893,  0.5809], 
<a name="l11374"><span class="ln">11374 </span></a>            [-0.1669,  0.7299,  0.4942]]) 
<a name="l11375"><span class="ln">11375 </span></a>    &gt;&gt;&gt; torch.transpose(x, 0, 1) 
<a name="l11376"><span class="ln">11376 </span></a>    tensor([[ 1.0028, -0.1669], 
<a name="l11377"><span class="ln">11377 </span></a>            [-0.9893,  0.7299], 
<a name="l11378"><span class="ln">11378 </span></a>            [ 0.5809,  0.4942]]) 
<a name="l11379"><span class="ln">11379 </span></a> 
<a name="l11380"><span class="ln">11380 </span></a>See also :func:`torch.t`. 
<a name="l11381"><span class="ln">11381 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11382"><span class="ln">11382 </span></a><span class="s3">)</span>
<a name="l11383"><span class="ln">11383 </span></a>
<a name="l11384"><span class="ln">11384 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11385"><span class="ln">11385 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">triangular_solve</span><span class="s3">,</span>
<a name="l11386"><span class="ln">11386 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11387"><span class="ln">11387 </span></a>triangular_solve(b, A, upper=True, transpose=False, unitriangular=False, *, out=None) -&gt; (Tensor, Tensor) 
<a name="l11388"><span class="ln">11388 </span></a> 
<a name="l11389"><span class="ln">11389 </span></a>Solves a system of equations with a square upper or lower triangular invertible matrix :math:`A` 
<a name="l11390"><span class="ln">11390 </span></a>and multiple right-hand sides :math:`b`. 
<a name="l11391"><span class="ln">11391 </span></a> 
<a name="l11392"><span class="ln">11392 </span></a>In symbols, it solves :math:`AX = b` and assumes :math:`A` is square upper-triangular 
<a name="l11393"><span class="ln">11393 </span></a>(or lower-triangular if :attr:`upper`\ `= False`) and does not have zeros on the diagonal. 
<a name="l11394"><span class="ln">11394 </span></a> 
<a name="l11395"><span class="ln">11395 </span></a>`torch.triangular_solve(b, A)` can take in 2D inputs `b, A` or inputs that are 
<a name="l11396"><span class="ln">11396 </span></a>batches of 2D matrices. If the inputs are batches, then returns 
<a name="l11397"><span class="ln">11397 </span></a>batched outputs `X` 
<a name="l11398"><span class="ln">11398 </span></a> 
<a name="l11399"><span class="ln">11399 </span></a>If the diagonal of :attr:`A` contains zeros or elements that are very close to zero and 
<a name="l11400"><span class="ln">11400 </span></a>:attr:`unitriangular`\ `= False` (default) or if the input matrix is badly conditioned, 
<a name="l11401"><span class="ln">11401 </span></a>the result may contain `NaN` s. 
<a name="l11402"><span class="ln">11402 </span></a> 
<a name="l11403"><span class="ln">11403 </span></a>Supports input of float, double, cfloat and cdouble data types. 
<a name="l11404"><span class="ln">11404 </span></a> 
<a name="l11405"><span class="ln">11405 </span></a>.. warning:: 
<a name="l11406"><span class="ln">11406 </span></a> 
<a name="l11407"><span class="ln">11407 </span></a>    :func:`torch.triangular_solve` is deprecated in favor of :func:`torch.linalg.solve_triangular` 
<a name="l11408"><span class="ln">11408 </span></a>    and will be removed in a future PyTorch release. 
<a name="l11409"><span class="ln">11409 </span></a>    :func:`torch.linalg.solve_triangular` has its arguments reversed and does not return a 
<a name="l11410"><span class="ln">11410 </span></a>    copy of one of the inputs. 
<a name="l11411"><span class="ln">11411 </span></a> 
<a name="l11412"><span class="ln">11412 </span></a>    ``X = torch.triangular_solve(B, A).solution`` should be replaced with 
<a name="l11413"><span class="ln">11413 </span></a> 
<a name="l11414"><span class="ln">11414 </span></a>    .. code:: python 
<a name="l11415"><span class="ln">11415 </span></a> 
<a name="l11416"><span class="ln">11416 </span></a>        X = torch.linalg.solve_triangular(A, B) 
<a name="l11417"><span class="ln">11417 </span></a> 
<a name="l11418"><span class="ln">11418 </span></a>Args: 
<a name="l11419"><span class="ln">11419 </span></a>    b (Tensor): multiple right-hand sides of size :math:`(*, m, k)` where 
<a name="l11420"><span class="ln">11420 </span></a>                :math:`*` is zero of more batch dimensions 
<a name="l11421"><span class="ln">11421 </span></a>    A (Tensor): the input triangular coefficient matrix of size :math:`(*, m, m)` 
<a name="l11422"><span class="ln">11422 </span></a>                where :math:`*` is zero or more batch dimensions 
<a name="l11423"><span class="ln">11423 </span></a>    upper (bool, optional): whether :math:`A` is upper or lower triangular. Default: ``True``. 
<a name="l11424"><span class="ln">11424 </span></a>    transpose (bool, optional): solves `op(A)X = b` where `op(A) = A^T` if this flag is ``True``, 
<a name="l11425"><span class="ln">11425 </span></a>                                and `op(A) = A` if it is ``False``. Default: ``False``. 
<a name="l11426"><span class="ln">11426 </span></a>    unitriangular (bool, optional): whether :math:`A` is unit triangular. 
<a name="l11427"><span class="ln">11427 </span></a>        If True, the diagonal elements of :math:`A` are assumed to be 
<a name="l11428"><span class="ln">11428 </span></a>        1 and not referenced from :math:`A`. Default: ``False``. 
<a name="l11429"><span class="ln">11429 </span></a> 
<a name="l11430"><span class="ln">11430 </span></a>Keyword args: 
<a name="l11431"><span class="ln">11431 </span></a>    out ((Tensor, Tensor), optional): tuple of two tensors to write 
<a name="l11432"><span class="ln">11432 </span></a>        the output to. Ignored if `None`. Default: `None`. 
<a name="l11433"><span class="ln">11433 </span></a> 
<a name="l11434"><span class="ln">11434 </span></a>Returns: 
<a name="l11435"><span class="ln">11435 </span></a>    A namedtuple `(solution, cloned_coefficient)` where `cloned_coefficient` 
<a name="l11436"><span class="ln">11436 </span></a>    is a clone of :math:`A` and `solution` is the solution :math:`X` to :math:`AX = b` 
<a name="l11437"><span class="ln">11437 </span></a>    (or whatever variant of the system of equations, depending on the keyword arguments.) 
<a name="l11438"><span class="ln">11438 </span></a> 
<a name="l11439"><span class="ln">11439 </span></a>Examples:: 
<a name="l11440"><span class="ln">11440 </span></a> 
<a name="l11441"><span class="ln">11441 </span></a>    &gt;&gt;&gt; A = torch.randn(2, 2).triu() 
<a name="l11442"><span class="ln">11442 </span></a>    &gt;&gt;&gt; A 
<a name="l11443"><span class="ln">11443 </span></a>    tensor([[ 1.1527, -1.0753], 
<a name="l11444"><span class="ln">11444 </span></a>            [ 0.0000,  0.7986]]) 
<a name="l11445"><span class="ln">11445 </span></a>    &gt;&gt;&gt; b = torch.randn(2, 3) 
<a name="l11446"><span class="ln">11446 </span></a>    &gt;&gt;&gt; b 
<a name="l11447"><span class="ln">11447 </span></a>    tensor([[-0.0210,  2.3513, -1.5492], 
<a name="l11448"><span class="ln">11448 </span></a>            [ 1.5429,  0.7403, -1.0243]]) 
<a name="l11449"><span class="ln">11449 </span></a>    &gt;&gt;&gt; torch.triangular_solve(b, A) 
<a name="l11450"><span class="ln">11450 </span></a>    torch.return_types.triangular_solve( 
<a name="l11451"><span class="ln">11451 </span></a>    solution=tensor([[ 1.7841,  2.9046, -2.5405], 
<a name="l11452"><span class="ln">11452 </span></a>            [ 1.9320,  0.9270, -1.2826]]), 
<a name="l11453"><span class="ln">11453 </span></a>    cloned_coefficient=tensor([[ 1.1527, -1.0753], 
<a name="l11454"><span class="ln">11454 </span></a>            [ 0.0000,  0.7986]])) 
<a name="l11455"><span class="ln">11455 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l11456"><span class="ln">11456 </span></a><span class="s3">)</span>
<a name="l11457"><span class="ln">11457 </span></a>
<a name="l11458"><span class="ln">11458 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11459"><span class="ln">11459 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">tril</span><span class="s3">,</span>
<a name="l11460"><span class="ln">11460 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11461"><span class="ln">11461 </span></a>tril(input, diagonal=0, *, out=None) -&gt; Tensor 
<a name="l11462"><span class="ln">11462 </span></a> 
<a name="l11463"><span class="ln">11463 </span></a>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices 
<a name="l11464"><span class="ln">11464 </span></a>:attr:`input`, the other elements of the result tensor :attr:`out` are set to 0. 
<a name="l11465"><span class="ln">11465 </span></a> 
<a name="l11466"><span class="ln">11466 </span></a>The lower triangular part of the matrix is defined as the elements on and 
<a name="l11467"><span class="ln">11467 </span></a>below the diagonal. 
<a name="l11468"><span class="ln">11468 </span></a> 
<a name="l11469"><span class="ln">11469 </span></a>The argument :attr:`diagonal` controls which diagonal to consider. If 
<a name="l11470"><span class="ln">11470 </span></a>:attr:`diagonal` = 0, all elements on and below the main diagonal are 
<a name="l11471"><span class="ln">11471 </span></a>retained. A positive value includes just as many diagonals above the main 
<a name="l11472"><span class="ln">11472 </span></a>diagonal, and similarly a negative value excludes just as many diagonals below 
<a name="l11473"><span class="ln">11473 </span></a>the main diagonal. The main diagonal are the set of indices 
<a name="l11474"><span class="ln">11474 </span></a>:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where 
<a name="l11475"><span class="ln">11475 </span></a>:math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l11476"><span class="ln">11476 </span></a>&quot;&quot;&quot;</span>
<a name="l11477"><span class="ln">11477 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11478"><span class="ln">11478 </span></a>Args: 
<a name="l11479"><span class="ln">11479 </span></a>    {input} 
<a name="l11480"><span class="ln">11480 </span></a>    diagonal (int, optional): the diagonal to consider 
<a name="l11481"><span class="ln">11481 </span></a> 
<a name="l11482"><span class="ln">11482 </span></a>Keyword args: 
<a name="l11483"><span class="ln">11483 </span></a>    {out} 
<a name="l11484"><span class="ln">11484 </span></a> 
<a name="l11485"><span class="ln">11485 </span></a>Example:: 
<a name="l11486"><span class="ln">11486 </span></a> 
<a name="l11487"><span class="ln">11487 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l11488"><span class="ln">11488 </span></a>    &gt;&gt;&gt; a 
<a name="l11489"><span class="ln">11489 </span></a>    tensor([[-1.0813, -0.8619,  0.7105], 
<a name="l11490"><span class="ln">11490 </span></a>            [ 0.0935,  0.1380,  2.2112], 
<a name="l11491"><span class="ln">11491 </span></a>            [-0.3409, -0.9828,  0.0289]]) 
<a name="l11492"><span class="ln">11492 </span></a>    &gt;&gt;&gt; torch.tril(a) 
<a name="l11493"><span class="ln">11493 </span></a>    tensor([[-1.0813,  0.0000,  0.0000], 
<a name="l11494"><span class="ln">11494 </span></a>            [ 0.0935,  0.1380,  0.0000], 
<a name="l11495"><span class="ln">11495 </span></a>            [-0.3409, -0.9828,  0.0289]]) 
<a name="l11496"><span class="ln">11496 </span></a> 
<a name="l11497"><span class="ln">11497 </span></a>    &gt;&gt;&gt; b = torch.randn(4, 6) 
<a name="l11498"><span class="ln">11498 </span></a>    &gt;&gt;&gt; b 
<a name="l11499"><span class="ln">11499 </span></a>    tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461], 
<a name="l11500"><span class="ln">11500 </span></a>            [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145], 
<a name="l11501"><span class="ln">11501 </span></a>            [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864], 
<a name="l11502"><span class="ln">11502 </span></a>            [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]]) 
<a name="l11503"><span class="ln">11503 </span></a>    &gt;&gt;&gt; torch.tril(b, diagonal=1) 
<a name="l11504"><span class="ln">11504 </span></a>    tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l11505"><span class="ln">11505 </span></a>            [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000], 
<a name="l11506"><span class="ln">11506 </span></a>            [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000], 
<a name="l11507"><span class="ln">11507 </span></a>            [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]]) 
<a name="l11508"><span class="ln">11508 </span></a>    &gt;&gt;&gt; torch.tril(b, diagonal=-1) 
<a name="l11509"><span class="ln">11509 </span></a>    tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l11510"><span class="ln">11510 </span></a>            [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l11511"><span class="ln">11511 </span></a>            [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l11512"><span class="ln">11512 </span></a>            [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]]) 
<a name="l11513"><span class="ln">11513 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11514"><span class="ln">11514 </span></a><span class="s3">)</span>
<a name="l11515"><span class="ln">11515 </span></a>
<a name="l11516"><span class="ln">11516 </span></a><span class="s0"># docstr is split in two parts to avoid format mis-captureing :math: braces '{}'</span>
<a name="l11517"><span class="ln">11517 </span></a><span class="s0"># as common args.</span>
<a name="l11518"><span class="ln">11518 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11519"><span class="ln">11519 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">tril_indices</span><span class="s3">,</span>
<a name="l11520"><span class="ln">11520 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11521"><span class="ln">11521 </span></a>tril_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -&gt; Tensor 
<a name="l11522"><span class="ln">11522 </span></a> 
<a name="l11523"><span class="ln">11523 </span></a>Returns the indices of the lower triangular part of a :attr:`row`-by- 
<a name="l11524"><span class="ln">11524 </span></a>:attr:`col` matrix in a 2-by-N Tensor, where the first row contains row 
<a name="l11525"><span class="ln">11525 </span></a>coordinates of all indices and the second row contains column coordinates. 
<a name="l11526"><span class="ln">11526 </span></a>Indices are ordered based on rows and then columns. 
<a name="l11527"><span class="ln">11527 </span></a> 
<a name="l11528"><span class="ln">11528 </span></a>The lower triangular part of the matrix is defined as the elements on and 
<a name="l11529"><span class="ln">11529 </span></a>below the diagonal. 
<a name="l11530"><span class="ln">11530 </span></a> 
<a name="l11531"><span class="ln">11531 </span></a>The argument :attr:`offset` controls which diagonal to consider. If 
<a name="l11532"><span class="ln">11532 </span></a>:attr:`offset` = 0, all elements on and below the main diagonal are 
<a name="l11533"><span class="ln">11533 </span></a>retained. A positive value includes just as many diagonals above the main 
<a name="l11534"><span class="ln">11534 </span></a>diagonal, and similarly a negative value excludes just as many diagonals below 
<a name="l11535"><span class="ln">11535 </span></a>the main diagonal. The main diagonal are the set of indices 
<a name="l11536"><span class="ln">11536 </span></a>:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` 
<a name="l11537"><span class="ln">11537 </span></a>where :math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l11538"><span class="ln">11538 </span></a> 
<a name="l11539"><span class="ln">11539 </span></a>.. note:: 
<a name="l11540"><span class="ln">11540 </span></a>    When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to 
<a name="l11541"><span class="ln">11541 </span></a>    prevent overflow during calculation. 
<a name="l11542"><span class="ln">11542 </span></a>&quot;&quot;&quot;</span>
<a name="l11543"><span class="ln">11543 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11544"><span class="ln">11544 </span></a>Args: 
<a name="l11545"><span class="ln">11545 </span></a>    row (``int``): number of rows in the 2-D matrix. 
<a name="l11546"><span class="ln">11546 </span></a>    col (``int``): number of columns in the 2-D matrix. 
<a name="l11547"><span class="ln">11547 </span></a>    offset (``int``): diagonal offset from the main diagonal. 
<a name="l11548"><span class="ln">11548 </span></a>        Default: if not provided, 0. 
<a name="l11549"><span class="ln">11549 </span></a> 
<a name="l11550"><span class="ln">11550 </span></a>Keyword args: 
<a name="l11551"><span class="ln">11551 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor, 
<a name="l11552"><span class="ln">11552 </span></a>        only support ``torch.int``, ``torch.long``. Default: if ``None``, ``torch.long``. 
<a name="l11553"><span class="ln">11553 </span></a>    {device} 
<a name="l11554"><span class="ln">11554 </span></a>    layout (:class:`torch.layout`, optional): currently only support ``torch.strided``. 
<a name="l11555"><span class="ln">11555 </span></a> 
<a name="l11556"><span class="ln">11556 </span></a>Example:: 
<a name="l11557"><span class="ln">11557 </span></a> 
<a name="l11558"><span class="ln">11558 </span></a>    &gt;&gt;&gt; a = torch.tril_indices(3, 3) 
<a name="l11559"><span class="ln">11559 </span></a>    &gt;&gt;&gt; a 
<a name="l11560"><span class="ln">11560 </span></a>    tensor([[0, 1, 1, 2, 2, 2], 
<a name="l11561"><span class="ln">11561 </span></a>            [0, 0, 1, 0, 1, 2]]) 
<a name="l11562"><span class="ln">11562 </span></a> 
<a name="l11563"><span class="ln">11563 </span></a>    &gt;&gt;&gt; a = torch.tril_indices(4, 3, -1) 
<a name="l11564"><span class="ln">11564 </span></a>    &gt;&gt;&gt; a 
<a name="l11565"><span class="ln">11565 </span></a>    tensor([[1, 2, 2, 3, 3, 3], 
<a name="l11566"><span class="ln">11566 </span></a>            [0, 0, 1, 0, 1, 2]]) 
<a name="l11567"><span class="ln">11567 </span></a> 
<a name="l11568"><span class="ln">11568 </span></a>    &gt;&gt;&gt; a = torch.tril_indices(4, 3, 1) 
<a name="l11569"><span class="ln">11569 </span></a>    &gt;&gt;&gt; a 
<a name="l11570"><span class="ln">11570 </span></a>    tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], 
<a name="l11571"><span class="ln">11571 </span></a>            [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]]) 
<a name="l11572"><span class="ln">11572 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l11573"><span class="ln">11573 </span></a><span class="s3">)</span>
<a name="l11574"><span class="ln">11574 </span></a>
<a name="l11575"><span class="ln">11575 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11576"><span class="ln">11576 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">triu</span><span class="s3">,</span>
<a name="l11577"><span class="ln">11577 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11578"><span class="ln">11578 </span></a>triu(input, diagonal=0, *, out=None) -&gt; Tensor 
<a name="l11579"><span class="ln">11579 </span></a> 
<a name="l11580"><span class="ln">11580 </span></a>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices 
<a name="l11581"><span class="ln">11581 </span></a>:attr:`input`, the other elements of the result tensor :attr:`out` are set to 0. 
<a name="l11582"><span class="ln">11582 </span></a> 
<a name="l11583"><span class="ln">11583 </span></a>The upper triangular part of the matrix is defined as the elements on and 
<a name="l11584"><span class="ln">11584 </span></a>above the diagonal. 
<a name="l11585"><span class="ln">11585 </span></a> 
<a name="l11586"><span class="ln">11586 </span></a>The argument :attr:`diagonal` controls which diagonal to consider. If 
<a name="l11587"><span class="ln">11587 </span></a>:attr:`diagonal` = 0, all elements on and above the main diagonal are 
<a name="l11588"><span class="ln">11588 </span></a>retained. A positive value excludes just as many diagonals above the main 
<a name="l11589"><span class="ln">11589 </span></a>diagonal, and similarly a negative value includes just as many diagonals below 
<a name="l11590"><span class="ln">11590 </span></a>the main diagonal. The main diagonal are the set of indices 
<a name="l11591"><span class="ln">11591 </span></a>:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where 
<a name="l11592"><span class="ln">11592 </span></a>:math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l11593"><span class="ln">11593 </span></a>&quot;&quot;&quot;</span>
<a name="l11594"><span class="ln">11594 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11595"><span class="ln">11595 </span></a>Args: 
<a name="l11596"><span class="ln">11596 </span></a>    {input} 
<a name="l11597"><span class="ln">11597 </span></a>    diagonal (int, optional): the diagonal to consider 
<a name="l11598"><span class="ln">11598 </span></a> 
<a name="l11599"><span class="ln">11599 </span></a>Keyword args: 
<a name="l11600"><span class="ln">11600 </span></a>    {out} 
<a name="l11601"><span class="ln">11601 </span></a> 
<a name="l11602"><span class="ln">11602 </span></a>Example:: 
<a name="l11603"><span class="ln">11603 </span></a> 
<a name="l11604"><span class="ln">11604 </span></a>    &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l11605"><span class="ln">11605 </span></a>    &gt;&gt;&gt; a 
<a name="l11606"><span class="ln">11606 </span></a>    tensor([[ 0.2309,  0.5207,  2.0049], 
<a name="l11607"><span class="ln">11607 </span></a>            [ 0.2072, -1.0680,  0.6602], 
<a name="l11608"><span class="ln">11608 </span></a>            [ 0.3480, -0.5211, -0.4573]]) 
<a name="l11609"><span class="ln">11609 </span></a>    &gt;&gt;&gt; torch.triu(a) 
<a name="l11610"><span class="ln">11610 </span></a>    tensor([[ 0.2309,  0.5207,  2.0049], 
<a name="l11611"><span class="ln">11611 </span></a>            [ 0.0000, -1.0680,  0.6602], 
<a name="l11612"><span class="ln">11612 </span></a>            [ 0.0000,  0.0000, -0.4573]]) 
<a name="l11613"><span class="ln">11613 </span></a>    &gt;&gt;&gt; torch.triu(a, diagonal=1) 
<a name="l11614"><span class="ln">11614 </span></a>    tensor([[ 0.0000,  0.5207,  2.0049], 
<a name="l11615"><span class="ln">11615 </span></a>            [ 0.0000,  0.0000,  0.6602], 
<a name="l11616"><span class="ln">11616 </span></a>            [ 0.0000,  0.0000,  0.0000]]) 
<a name="l11617"><span class="ln">11617 </span></a>    &gt;&gt;&gt; torch.triu(a, diagonal=-1) 
<a name="l11618"><span class="ln">11618 </span></a>    tensor([[ 0.2309,  0.5207,  2.0049], 
<a name="l11619"><span class="ln">11619 </span></a>            [ 0.2072, -1.0680,  0.6602], 
<a name="l11620"><span class="ln">11620 </span></a>            [ 0.0000, -0.5211, -0.4573]]) 
<a name="l11621"><span class="ln">11621 </span></a> 
<a name="l11622"><span class="ln">11622 </span></a>    &gt;&gt;&gt; b = torch.randn(4, 6) 
<a name="l11623"><span class="ln">11623 </span></a>    &gt;&gt;&gt; b 
<a name="l11624"><span class="ln">11624 </span></a>    tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235], 
<a name="l11625"><span class="ln">11625 </span></a>            [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857], 
<a name="l11626"><span class="ln">11626 </span></a>            [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410], 
<a name="l11627"><span class="ln">11627 </span></a>            [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]]) 
<a name="l11628"><span class="ln">11628 </span></a>    &gt;&gt;&gt; torch.triu(b, diagonal=1) 
<a name="l11629"><span class="ln">11629 </span></a>    tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235], 
<a name="l11630"><span class="ln">11630 </span></a>            [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857], 
<a name="l11631"><span class="ln">11631 </span></a>            [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410], 
<a name="l11632"><span class="ln">11632 </span></a>            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]]) 
<a name="l11633"><span class="ln">11633 </span></a>    &gt;&gt;&gt; torch.triu(b, diagonal=-1) 
<a name="l11634"><span class="ln">11634 </span></a>    tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235], 
<a name="l11635"><span class="ln">11635 </span></a>            [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857], 
<a name="l11636"><span class="ln">11636 </span></a>            [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410], 
<a name="l11637"><span class="ln">11637 </span></a>            [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]]) 
<a name="l11638"><span class="ln">11638 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11639"><span class="ln">11639 </span></a><span class="s3">)</span>
<a name="l11640"><span class="ln">11640 </span></a>
<a name="l11641"><span class="ln">11641 </span></a><span class="s0"># docstr is split in two parts to avoid format mis-capturing :math: braces '{}'</span>
<a name="l11642"><span class="ln">11642 </span></a><span class="s0"># as common args.</span>
<a name="l11643"><span class="ln">11643 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11644"><span class="ln">11644 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">triu_indices</span><span class="s3">,</span>
<a name="l11645"><span class="ln">11645 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11646"><span class="ln">11646 </span></a>triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -&gt; Tensor 
<a name="l11647"><span class="ln">11647 </span></a> 
<a name="l11648"><span class="ln">11648 </span></a>Returns the indices of the upper triangular part of a :attr:`row` by 
<a name="l11649"><span class="ln">11649 </span></a>:attr:`col` matrix in a 2-by-N Tensor, where the first row contains row 
<a name="l11650"><span class="ln">11650 </span></a>coordinates of all indices and the second row contains column coordinates. 
<a name="l11651"><span class="ln">11651 </span></a>Indices are ordered based on rows and then columns. 
<a name="l11652"><span class="ln">11652 </span></a> 
<a name="l11653"><span class="ln">11653 </span></a>The upper triangular part of the matrix is defined as the elements on and 
<a name="l11654"><span class="ln">11654 </span></a>above the diagonal. 
<a name="l11655"><span class="ln">11655 </span></a> 
<a name="l11656"><span class="ln">11656 </span></a>The argument :attr:`offset` controls which diagonal to consider. If 
<a name="l11657"><span class="ln">11657 </span></a>:attr:`offset` = 0, all elements on and above the main diagonal are 
<a name="l11658"><span class="ln">11658 </span></a>retained. A positive value excludes just as many diagonals above the main 
<a name="l11659"><span class="ln">11659 </span></a>diagonal, and similarly a negative value includes just as many diagonals below 
<a name="l11660"><span class="ln">11660 </span></a>the main diagonal. The main diagonal are the set of indices 
<a name="l11661"><span class="ln">11661 </span></a>:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` 
<a name="l11662"><span class="ln">11662 </span></a>where :math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l11663"><span class="ln">11663 </span></a> 
<a name="l11664"><span class="ln">11664 </span></a>.. note:: 
<a name="l11665"><span class="ln">11665 </span></a>    When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to 
<a name="l11666"><span class="ln">11666 </span></a>    prevent overflow during calculation. 
<a name="l11667"><span class="ln">11667 </span></a>&quot;&quot;&quot;</span>
<a name="l11668"><span class="ln">11668 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11669"><span class="ln">11669 </span></a>Args: 
<a name="l11670"><span class="ln">11670 </span></a>    row (``int``): number of rows in the 2-D matrix. 
<a name="l11671"><span class="ln">11671 </span></a>    col (``int``): number of columns in the 2-D matrix. 
<a name="l11672"><span class="ln">11672 </span></a>    offset (``int``): diagonal offset from the main diagonal. 
<a name="l11673"><span class="ln">11673 </span></a>        Default: if not provided, 0. 
<a name="l11674"><span class="ln">11674 </span></a> 
<a name="l11675"><span class="ln">11675 </span></a>Keyword args: 
<a name="l11676"><span class="ln">11676 </span></a>    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor, 
<a name="l11677"><span class="ln">11677 </span></a>        only support ``torch.int``, ``torch.long``. Default: if ``None``, ``torch.long``. 
<a name="l11678"><span class="ln">11678 </span></a>    {device} 
<a name="l11679"><span class="ln">11679 </span></a>    layout (:class:`torch.layout`, optional): currently only support ``torch.strided``. 
<a name="l11680"><span class="ln">11680 </span></a> 
<a name="l11681"><span class="ln">11681 </span></a>Example:: 
<a name="l11682"><span class="ln">11682 </span></a> 
<a name="l11683"><span class="ln">11683 </span></a>    &gt;&gt;&gt; a = torch.triu_indices(3, 3) 
<a name="l11684"><span class="ln">11684 </span></a>    &gt;&gt;&gt; a 
<a name="l11685"><span class="ln">11685 </span></a>    tensor([[0, 0, 0, 1, 1, 2], 
<a name="l11686"><span class="ln">11686 </span></a>            [0, 1, 2, 1, 2, 2]]) 
<a name="l11687"><span class="ln">11687 </span></a> 
<a name="l11688"><span class="ln">11688 </span></a>    &gt;&gt;&gt; a = torch.triu_indices(4, 3, -1) 
<a name="l11689"><span class="ln">11689 </span></a>    &gt;&gt;&gt; a 
<a name="l11690"><span class="ln">11690 </span></a>    tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3], 
<a name="l11691"><span class="ln">11691 </span></a>            [0, 1, 2, 0, 1, 2, 1, 2, 2]]) 
<a name="l11692"><span class="ln">11692 </span></a> 
<a name="l11693"><span class="ln">11693 </span></a>    &gt;&gt;&gt; a = torch.triu_indices(4, 3, 1) 
<a name="l11694"><span class="ln">11694 </span></a>    &gt;&gt;&gt; a 
<a name="l11695"><span class="ln">11695 </span></a>    tensor([[0, 0, 1], 
<a name="l11696"><span class="ln">11696 </span></a>            [1, 2, 2]]) 
<a name="l11697"><span class="ln">11697 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l11698"><span class="ln">11698 </span></a><span class="s3">)</span>
<a name="l11699"><span class="ln">11699 </span></a>
<a name="l11700"><span class="ln">11700 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11701"><span class="ln">11701 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">true_divide</span><span class="s3">,</span>
<a name="l11702"><span class="ln">11702 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11703"><span class="ln">11703 </span></a>true_divide(dividend, divisor, *, out) -&gt; Tensor 
<a name="l11704"><span class="ln">11704 </span></a> 
<a name="l11705"><span class="ln">11705 </span></a>Alias for :func:`torch.div` with ``rounding_mode=None``. 
<a name="l11706"><span class="ln">11706 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l11707"><span class="ln">11707 </span></a><span class="s3">)</span>
<a name="l11708"><span class="ln">11708 </span></a>
<a name="l11709"><span class="ln">11709 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11710"><span class="ln">11710 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">trunc</span><span class="s3">,</span>
<a name="l11711"><span class="ln">11711 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11712"><span class="ln">11712 </span></a>trunc(input, *, out=None) -&gt; Tensor 
<a name="l11713"><span class="ln">11713 </span></a> 
<a name="l11714"><span class="ln">11714 </span></a>Returns a new tensor with the truncated integer values of 
<a name="l11715"><span class="ln">11715 </span></a>the elements of :attr:`input`. 
<a name="l11716"><span class="ln">11716 </span></a> 
<a name="l11717"><span class="ln">11717 </span></a>For integer inputs, follows the array-api convention of returning a 
<a name="l11718"><span class="ln">11718 </span></a>copy of the input tensor. 
<a name="l11719"><span class="ln">11719 </span></a> 
<a name="l11720"><span class="ln">11720 </span></a>Args: 
<a name="l11721"><span class="ln">11721 </span></a>    {input} 
<a name="l11722"><span class="ln">11722 </span></a> 
<a name="l11723"><span class="ln">11723 </span></a>Keyword args: 
<a name="l11724"><span class="ln">11724 </span></a>    {out} 
<a name="l11725"><span class="ln">11725 </span></a> 
<a name="l11726"><span class="ln">11726 </span></a>Example:: 
<a name="l11727"><span class="ln">11727 </span></a> 
<a name="l11728"><span class="ln">11728 </span></a>    &gt;&gt;&gt; a = torch.randn(4) 
<a name="l11729"><span class="ln">11729 </span></a>    &gt;&gt;&gt; a 
<a name="l11730"><span class="ln">11730 </span></a>    tensor([ 3.4742,  0.5466, -0.8008, -0.9079]) 
<a name="l11731"><span class="ln">11731 </span></a>    &gt;&gt;&gt; torch.trunc(a) 
<a name="l11732"><span class="ln">11732 </span></a>    tensor([ 3.,  0., -0., -0.]) 
<a name="l11733"><span class="ln">11733 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11734"><span class="ln">11734 </span></a><span class="s3">)</span>
<a name="l11735"><span class="ln">11735 </span></a>
<a name="l11736"><span class="ln">11736 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11737"><span class="ln">11737 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">fake_quantize_per_tensor_affine</span><span class="s3">,</span>
<a name="l11738"><span class="ln">11738 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11739"><span class="ln">11739 </span></a>fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max) -&gt; Tensor 
<a name="l11740"><span class="ln">11740 </span></a> 
<a name="l11741"><span class="ln">11741 </span></a>Returns a new tensor with the data in :attr:`input` fake quantized using :attr:`scale`, 
<a name="l11742"><span class="ln">11742 </span></a>:attr:`zero_point`, :attr:`quant_min` and :attr:`quant_max`. 
<a name="l11743"><span class="ln">11743 </span></a> 
<a name="l11744"><span class="ln">11744 </span></a>.. math:: 
<a name="l11745"><span class="ln">11745 </span></a>    \text{output} = ( 
<a name="l11746"><span class="ln">11746 </span></a>        min( 
<a name="l11747"><span class="ln">11747 </span></a>            \text{quant\_max}, 
<a name="l11748"><span class="ln">11748 </span></a>            max( 
<a name="l11749"><span class="ln">11749 </span></a>                \text{quant\_min}, 
<a name="l11750"><span class="ln">11750 </span></a>                \text{std::nearby\_int}(\text{input} / \text{scale}) + \text{zero\_point} 
<a name="l11751"><span class="ln">11751 </span></a>            ) 
<a name="l11752"><span class="ln">11752 </span></a>        ) - \text{zero\_point} 
<a name="l11753"><span class="ln">11753 </span></a>    ) \times \text{scale} 
<a name="l11754"><span class="ln">11754 </span></a> 
<a name="l11755"><span class="ln">11755 </span></a>Args: 
<a name="l11756"><span class="ln">11756 </span></a>    input (Tensor): the input value(s), ``torch.float32`` tensor 
<a name="l11757"><span class="ln">11757 </span></a>    scale (double scalar or ``float32`` Tensor): quantization scale 
<a name="l11758"><span class="ln">11758 </span></a>    zero_point (int64 scalar or ``int32`` Tensor): quantization zero_point 
<a name="l11759"><span class="ln">11759 </span></a>    quant_min (int64): lower bound of the quantized domain 
<a name="l11760"><span class="ln">11760 </span></a>    quant_max (int64): upper bound of the quantized domain 
<a name="l11761"><span class="ln">11761 </span></a> 
<a name="l11762"><span class="ln">11762 </span></a>Returns: 
<a name="l11763"><span class="ln">11763 </span></a>    Tensor: A newly fake_quantized ``torch.float32`` tensor 
<a name="l11764"><span class="ln">11764 </span></a> 
<a name="l11765"><span class="ln">11765 </span></a>Example:: 
<a name="l11766"><span class="ln">11766 </span></a> 
<a name="l11767"><span class="ln">11767 </span></a>    &gt;&gt;&gt; x = torch.randn(4) 
<a name="l11768"><span class="ln">11768 </span></a>    &gt;&gt;&gt; x 
<a name="l11769"><span class="ln">11769 </span></a>    tensor([ 0.0552,  0.9730,  0.3973, -1.0780]) 
<a name="l11770"><span class="ln">11770 </span></a>    &gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255) 
<a name="l11771"><span class="ln">11771 </span></a>    tensor([0.1000, 1.0000, 0.4000, 0.0000]) 
<a name="l11772"><span class="ln">11772 </span></a>    &gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, torch.tensor(0.1), torch.tensor(0), 0, 255) 
<a name="l11773"><span class="ln">11773 </span></a>    tensor([0.1000, 1.0000, 0.4000, 0.0000]) 
<a name="l11774"><span class="ln">11774 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l11775"><span class="ln">11775 </span></a><span class="s3">)</span>
<a name="l11776"><span class="ln">11776 </span></a>
<a name="l11777"><span class="ln">11777 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11778"><span class="ln">11778 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">fake_quantize_per_channel_affine</span><span class="s3">,</span>
<a name="l11779"><span class="ln">11779 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11780"><span class="ln">11780 </span></a>fake_quantize_per_channel_affine(input, scale, zero_point, axis, quant_min, quant_max) -&gt; Tensor 
<a name="l11781"><span class="ln">11781 </span></a> 
<a name="l11782"><span class="ln">11782 </span></a>Returns a new tensor with the data in :attr:`input` fake quantized per channel using :attr:`scale`, 
<a name="l11783"><span class="ln">11783 </span></a>:attr:`zero_point`, :attr:`quant_min` and :attr:`quant_max`, across the channel specified by :attr:`axis`. 
<a name="l11784"><span class="ln">11784 </span></a> 
<a name="l11785"><span class="ln">11785 </span></a>.. math:: 
<a name="l11786"><span class="ln">11786 </span></a>    \text{output} = ( 
<a name="l11787"><span class="ln">11787 </span></a>        min( 
<a name="l11788"><span class="ln">11788 </span></a>            \text{quant\_max}, 
<a name="l11789"><span class="ln">11789 </span></a>            max( 
<a name="l11790"><span class="ln">11790 </span></a>                \text{quant\_min}, 
<a name="l11791"><span class="ln">11791 </span></a>                \text{std::nearby\_int}(\text{input} / \text{scale}) + \text{zero\_point} 
<a name="l11792"><span class="ln">11792 </span></a>            ) 
<a name="l11793"><span class="ln">11793 </span></a>        ) - \text{zero\_point} 
<a name="l11794"><span class="ln">11794 </span></a>    ) \times \text{scale} 
<a name="l11795"><span class="ln">11795 </span></a> 
<a name="l11796"><span class="ln">11796 </span></a>Args: 
<a name="l11797"><span class="ln">11797 </span></a>    input (Tensor): the input value(s), in ``torch.float32`` 
<a name="l11798"><span class="ln">11798 </span></a>    scale (Tensor): quantization scale, per channel in ``torch.float32`` 
<a name="l11799"><span class="ln">11799 </span></a>    zero_point (Tensor): quantization zero_point, per channel in ``torch.int32`` or ``torch.half`` or ``torch.float32`` 
<a name="l11800"><span class="ln">11800 </span></a>    axis (int32): channel axis 
<a name="l11801"><span class="ln">11801 </span></a>    quant_min (int64): lower bound of the quantized domain 
<a name="l11802"><span class="ln">11802 </span></a>    quant_max (int64): upper bound of the quantized domain 
<a name="l11803"><span class="ln">11803 </span></a> 
<a name="l11804"><span class="ln">11804 </span></a>Returns: 
<a name="l11805"><span class="ln">11805 </span></a>    Tensor: A newly fake_quantized per channel ``torch.float32`` tensor 
<a name="l11806"><span class="ln">11806 </span></a> 
<a name="l11807"><span class="ln">11807 </span></a>Example:: 
<a name="l11808"><span class="ln">11808 </span></a> 
<a name="l11809"><span class="ln">11809 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 2, 2) 
<a name="l11810"><span class="ln">11810 </span></a>    &gt;&gt;&gt; x 
<a name="l11811"><span class="ln">11811 </span></a>    tensor([[[-0.2525, -0.0466], 
<a name="l11812"><span class="ln">11812 </span></a>             [ 0.3491, -0.2168]], 
<a name="l11813"><span class="ln">11813 </span></a> 
<a name="l11814"><span class="ln">11814 </span></a>            [[-0.5906,  1.6258], 
<a name="l11815"><span class="ln">11815 </span></a>             [ 0.6444, -0.0542]]]) 
<a name="l11816"><span class="ln">11816 </span></a>    &gt;&gt;&gt; scales = (torch.randn(2) + 1) * 0.05 
<a name="l11817"><span class="ln">11817 </span></a>    &gt;&gt;&gt; scales 
<a name="l11818"><span class="ln">11818 </span></a>    tensor([0.0475, 0.0486]) 
<a name="l11819"><span class="ln">11819 </span></a>    &gt;&gt;&gt; zero_points = torch.zeros(2).to(torch.int32) 
<a name="l11820"><span class="ln">11820 </span></a>    &gt;&gt;&gt; zero_points 
<a name="l11821"><span class="ln">11821 </span></a>    tensor([0, 0]) 
<a name="l11822"><span class="ln">11822 </span></a>    &gt;&gt;&gt; torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255) 
<a name="l11823"><span class="ln">11823 </span></a>    tensor([[[0.0000, 0.0000], 
<a name="l11824"><span class="ln">11824 </span></a>             [0.3405, 0.0000]], 
<a name="l11825"><span class="ln">11825 </span></a> 
<a name="l11826"><span class="ln">11826 </span></a>            [[0.0000, 1.6134], 
<a name="l11827"><span class="ln">11827 </span></a>            [0.6323, 0.0000]]]) 
<a name="l11828"><span class="ln">11828 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l11829"><span class="ln">11829 </span></a><span class="s3">)</span>
<a name="l11830"><span class="ln">11830 </span></a>
<a name="l11831"><span class="ln">11831 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11832"><span class="ln">11832 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">fix</span><span class="s3">,</span>
<a name="l11833"><span class="ln">11833 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11834"><span class="ln">11834 </span></a>fix(input, *, out=None) -&gt; Tensor 
<a name="l11835"><span class="ln">11835 </span></a> 
<a name="l11836"><span class="ln">11836 </span></a>Alias for :func:`torch.trunc` 
<a name="l11837"><span class="ln">11837 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l11838"><span class="ln">11838 </span></a><span class="s3">)</span>
<a name="l11839"><span class="ln">11839 </span></a>
<a name="l11840"><span class="ln">11840 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11841"><span class="ln">11841 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unsqueeze</span><span class="s3">,</span>
<a name="l11842"><span class="ln">11842 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11843"><span class="ln">11843 </span></a>unsqueeze(input, dim) -&gt; Tensor 
<a name="l11844"><span class="ln">11844 </span></a> 
<a name="l11845"><span class="ln">11845 </span></a>Returns a new tensor with a dimension of size one inserted at the 
<a name="l11846"><span class="ln">11846 </span></a>specified position. 
<a name="l11847"><span class="ln">11847 </span></a> 
<a name="l11848"><span class="ln">11848 </span></a>The returned tensor shares the same underlying data with this tensor. 
<a name="l11849"><span class="ln">11849 </span></a> 
<a name="l11850"><span class="ln">11850 </span></a>A :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)`` 
<a name="l11851"><span class="ln">11851 </span></a>can be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze` 
<a name="l11852"><span class="ln">11852 </span></a>applied at :attr:`dim` = ``dim + input.dim() + 1``. 
<a name="l11853"><span class="ln">11853 </span></a> 
<a name="l11854"><span class="ln">11854 </span></a>Args: 
<a name="l11855"><span class="ln">11855 </span></a>    {input} 
<a name="l11856"><span class="ln">11856 </span></a>    dim (int): the index at which to insert the singleton dimension 
<a name="l11857"><span class="ln">11857 </span></a> 
<a name="l11858"><span class="ln">11858 </span></a>Example:: 
<a name="l11859"><span class="ln">11859 </span></a> 
<a name="l11860"><span class="ln">11860 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4]) 
<a name="l11861"><span class="ln">11861 </span></a>    &gt;&gt;&gt; torch.unsqueeze(x, 0) 
<a name="l11862"><span class="ln">11862 </span></a>    tensor([[ 1,  2,  3,  4]]) 
<a name="l11863"><span class="ln">11863 </span></a>    &gt;&gt;&gt; torch.unsqueeze(x, 1) 
<a name="l11864"><span class="ln">11864 </span></a>    tensor([[ 1], 
<a name="l11865"><span class="ln">11865 </span></a>            [ 2], 
<a name="l11866"><span class="ln">11866 </span></a>            [ 3], 
<a name="l11867"><span class="ln">11867 </span></a>            [ 4]]) 
<a name="l11868"><span class="ln">11868 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l11869"><span class="ln">11869 </span></a><span class="s3">)</span>
<a name="l11870"><span class="ln">11870 </span></a>
<a name="l11871"><span class="ln">11871 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11872"><span class="ln">11872 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">var</span><span class="s3">,</span>
<a name="l11873"><span class="ln">11873 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11874"><span class="ln">11874 </span></a>var(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l11875"><span class="ln">11875 </span></a> 
<a name="l11876"><span class="ln">11876 </span></a>Calculates the variance over the dimensions specified by :attr:`dim`. :attr:`dim` 
<a name="l11877"><span class="ln">11877 </span></a>can be a single dimension, list of dimensions, or ``None`` to reduce over all 
<a name="l11878"><span class="ln">11878 </span></a>dimensions. 
<a name="l11879"><span class="ln">11879 </span></a> 
<a name="l11880"><span class="ln">11880 </span></a>The variance (:math:`\sigma^2`) is calculated as 
<a name="l11881"><span class="ln">11881 </span></a> 
<a name="l11882"><span class="ln">11882 </span></a>.. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l11883"><span class="ln">11883 </span></a> 
<a name="l11884"><span class="ln">11884 </span></a>where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l11885"><span class="ln">11885 </span></a>sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l11886"><span class="ln">11886 </span></a>the :attr:`correction`. 
<a name="l11887"><span class="ln">11887 </span></a>&quot;&quot;&quot;</span>
<a name="l11888"><span class="ln">11888 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11889"><span class="ln">11889 </span></a> 
<a name="l11890"><span class="ln">11890 </span></a>{keepdim_details} 
<a name="l11891"><span class="ln">11891 </span></a> 
<a name="l11892"><span class="ln">11892 </span></a>Args: 
<a name="l11893"><span class="ln">11893 </span></a>    {input} 
<a name="l11894"><span class="ln">11894 </span></a>    {opt_dim_all_reduce} 
<a name="l11895"><span class="ln">11895 </span></a> 
<a name="l11896"><span class="ln">11896 </span></a>Keyword args: 
<a name="l11897"><span class="ln">11897 </span></a>    correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l11898"><span class="ln">11898 </span></a>        Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l11899"><span class="ln">11899 </span></a> 
<a name="l11900"><span class="ln">11900 </span></a>        .. versionchanged:: 2.0 
<a name="l11901"><span class="ln">11901 </span></a>            Previously this argument was called ``unbiased`` and was a boolean 
<a name="l11902"><span class="ln">11902 </span></a>            with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l11903"><span class="ln">11903 </span></a>            ``correction=0``. 
<a name="l11904"><span class="ln">11904 </span></a>    {opt_keepdim} 
<a name="l11905"><span class="ln">11905 </span></a>    {out} 
<a name="l11906"><span class="ln">11906 </span></a> 
<a name="l11907"><span class="ln">11907 </span></a>Example: 
<a name="l11908"><span class="ln">11908 </span></a> 
<a name="l11909"><span class="ln">11909 </span></a>    &gt;&gt;&gt; a = torch.tensor( 
<a name="l11910"><span class="ln">11910 </span></a>    ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l11911"><span class="ln">11911 </span></a>    ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l11912"><span class="ln">11912 </span></a>    ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l11913"><span class="ln">11913 </span></a>    ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l11914"><span class="ln">11914 </span></a>    ... )  # fmt: skip 
<a name="l11915"><span class="ln">11915 </span></a>    &gt;&gt;&gt; torch.var(a, dim=1, keepdim=True) 
<a name="l11916"><span class="ln">11916 </span></a>    tensor([[1.0631], 
<a name="l11917"><span class="ln">11917 </span></a>            [0.5590], 
<a name="l11918"><span class="ln">11918 </span></a>            [1.4893], 
<a name="l11919"><span class="ln">11919 </span></a>            [0.8258]]) 
<a name="l11920"><span class="ln">11920 </span></a> 
<a name="l11921"><span class="ln">11921 </span></a>.. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l11922"><span class="ln">11922 </span></a> 
<a name="l11923"><span class="ln">11923 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l11924"><span class="ln">11924 </span></a><span class="s3">)</span>
<a name="l11925"><span class="ln">11925 </span></a>
<a name="l11926"><span class="ln">11926 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11927"><span class="ln">11927 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">var_mean</span><span class="s3">,</span>
<a name="l11928"><span class="ln">11928 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11929"><span class="ln">11929 </span></a>var_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l11930"><span class="ln">11930 </span></a> 
<a name="l11931"><span class="ln">11931 </span></a>Calculates the variance and mean over the dimensions specified by :attr:`dim`. 
<a name="l11932"><span class="ln">11932 </span></a>:attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l11933"><span class="ln">11933 </span></a>reduce over all dimensions. 
<a name="l11934"><span class="ln">11934 </span></a> 
<a name="l11935"><span class="ln">11935 </span></a>The variance (:math:`\sigma^2`) is calculated as 
<a name="l11936"><span class="ln">11936 </span></a> 
<a name="l11937"><span class="ln">11937 </span></a>.. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l11938"><span class="ln">11938 </span></a> 
<a name="l11939"><span class="ln">11939 </span></a>where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l11940"><span class="ln">11940 </span></a>sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l11941"><span class="ln">11941 </span></a>the :attr:`correction`. 
<a name="l11942"><span class="ln">11942 </span></a>&quot;&quot;&quot;</span>
<a name="l11943"><span class="ln">11943 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l11944"><span class="ln">11944 </span></a> 
<a name="l11945"><span class="ln">11945 </span></a>{keepdim_details} 
<a name="l11946"><span class="ln">11946 </span></a> 
<a name="l11947"><span class="ln">11947 </span></a>Args: 
<a name="l11948"><span class="ln">11948 </span></a>    {input} 
<a name="l11949"><span class="ln">11949 </span></a>    {opt_dim_all_reduce} 
<a name="l11950"><span class="ln">11950 </span></a> 
<a name="l11951"><span class="ln">11951 </span></a>Keyword args: 
<a name="l11952"><span class="ln">11952 </span></a>    correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l11953"><span class="ln">11953 </span></a>        Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l11954"><span class="ln">11954 </span></a> 
<a name="l11955"><span class="ln">11955 </span></a>        .. versionchanged:: 2.0 
<a name="l11956"><span class="ln">11956 </span></a>            Previously this argument was called ``unbiased`` and was a boolean 
<a name="l11957"><span class="ln">11957 </span></a>            with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l11958"><span class="ln">11958 </span></a>            ``correction=0``. 
<a name="l11959"><span class="ln">11959 </span></a>    {opt_keepdim} 
<a name="l11960"><span class="ln">11960 </span></a>    {out} 
<a name="l11961"><span class="ln">11961 </span></a> 
<a name="l11962"><span class="ln">11962 </span></a>Returns: 
<a name="l11963"><span class="ln">11963 </span></a>    A tuple (var, mean) containing the variance and mean. 
<a name="l11964"><span class="ln">11964 </span></a> 
<a name="l11965"><span class="ln">11965 </span></a>Example: 
<a name="l11966"><span class="ln">11966 </span></a> 
<a name="l11967"><span class="ln">11967 </span></a>    &gt;&gt;&gt; a = torch.tensor( 
<a name="l11968"><span class="ln">11968 </span></a>    ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l11969"><span class="ln">11969 </span></a>    ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l11970"><span class="ln">11970 </span></a>    ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l11971"><span class="ln">11971 </span></a>    ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l11972"><span class="ln">11972 </span></a>    ... )  # fmt: skip 
<a name="l11973"><span class="ln">11973 </span></a>    &gt;&gt;&gt; torch.var_mean(a, dim=0, keepdim=True) 
<a name="l11974"><span class="ln">11974 </span></a>    (tensor([[1.5926, 1.0056, 1.2005, 0.3646]]), 
<a name="l11975"><span class="ln">11975 </span></a>     tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l11976"><span class="ln">11976 </span></a> 
<a name="l11977"><span class="ln">11977 </span></a>.. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l11978"><span class="ln">11978 </span></a> 
<a name="l11979"><span class="ln">11979 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">multi_dim_common</span><span class="s3">),</span>
<a name="l11980"><span class="ln">11980 </span></a><span class="s3">)</span>
<a name="l11981"><span class="ln">11981 </span></a>
<a name="l11982"><span class="ln">11982 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l11983"><span class="ln">11983 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">,</span>
<a name="l11984"><span class="ln">11984 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l11985"><span class="ln">11985 </span></a>zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l11986"><span class="ln">11986 </span></a> 
<a name="l11987"><span class="ln">11987 </span></a>Returns a tensor filled with the scalar value `0`, with the shape defined 
<a name="l11988"><span class="ln">11988 </span></a>by the variable argument :attr:`size`. 
<a name="l11989"><span class="ln">11989 </span></a> 
<a name="l11990"><span class="ln">11990 </span></a>Args: 
<a name="l11991"><span class="ln">11991 </span></a>    size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l11992"><span class="ln">11992 </span></a>        Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l11993"><span class="ln">11993 </span></a> 
<a name="l11994"><span class="ln">11994 </span></a>Keyword args: 
<a name="l11995"><span class="ln">11995 </span></a>    {out} 
<a name="l11996"><span class="ln">11996 </span></a>    {dtype} 
<a name="l11997"><span class="ln">11997 </span></a>    {layout} 
<a name="l11998"><span class="ln">11998 </span></a>    {device} 
<a name="l11999"><span class="ln">11999 </span></a>    {requires_grad} 
<a name="l12000"><span class="ln">12000 </span></a> 
<a name="l12001"><span class="ln">12001 </span></a>Example:: 
<a name="l12002"><span class="ln">12002 </span></a> 
<a name="l12003"><span class="ln">12003 </span></a>    &gt;&gt;&gt; torch.zeros(2, 3) 
<a name="l12004"><span class="ln">12004 </span></a>    tensor([[ 0.,  0.,  0.], 
<a name="l12005"><span class="ln">12005 </span></a>            [ 0.,  0.,  0.]]) 
<a name="l12006"><span class="ln">12006 </span></a> 
<a name="l12007"><span class="ln">12007 </span></a>    &gt;&gt;&gt; torch.zeros(5) 
<a name="l12008"><span class="ln">12008 </span></a>    tensor([ 0.,  0.,  0.,  0.,  0.]) 
<a name="l12009"><span class="ln">12009 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12010"><span class="ln">12010 </span></a><span class="s3">)</span>
<a name="l12011"><span class="ln">12011 </span></a>
<a name="l12012"><span class="ln">12012 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12013"><span class="ln">12013 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">zeros_like</span><span class="s3">,</span>
<a name="l12014"><span class="ln">12014 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12015"><span class="ln">12015 </span></a>zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l12016"><span class="ln">12016 </span></a> 
<a name="l12017"><span class="ln">12017 </span></a>Returns a tensor filled with the scalar value `0`, with the same size as 
<a name="l12018"><span class="ln">12018 </span></a>:attr:`input`. ``torch.zeros_like(input)`` is equivalent to 
<a name="l12019"><span class="ln">12019 </span></a>``torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l12020"><span class="ln">12020 </span></a> 
<a name="l12021"><span class="ln">12021 </span></a>.. warning:: 
<a name="l12022"><span class="ln">12022 </span></a>    As of 0.4, this function does not support an :attr:`out` keyword. As an alternative, 
<a name="l12023"><span class="ln">12023 </span></a>    the old ``torch.zeros_like(input, out=output)`` is equivalent to 
<a name="l12024"><span class="ln">12024 </span></a>    ``torch.zeros(input.size(), out=output)``. 
<a name="l12025"><span class="ln">12025 </span></a> 
<a name="l12026"><span class="ln">12026 </span></a>Args: 
<a name="l12027"><span class="ln">12027 </span></a>    {input} 
<a name="l12028"><span class="ln">12028 </span></a> 
<a name="l12029"><span class="ln">12029 </span></a>Keyword args: 
<a name="l12030"><span class="ln">12030 </span></a>    {dtype} 
<a name="l12031"><span class="ln">12031 </span></a>    {layout} 
<a name="l12032"><span class="ln">12032 </span></a>    {device} 
<a name="l12033"><span class="ln">12033 </span></a>    {requires_grad} 
<a name="l12034"><span class="ln">12034 </span></a>    {memory_format} 
<a name="l12035"><span class="ln">12035 </span></a> 
<a name="l12036"><span class="ln">12036 </span></a>Example:: 
<a name="l12037"><span class="ln">12037 </span></a> 
<a name="l12038"><span class="ln">12038 </span></a>    &gt;&gt;&gt; input = torch.empty(2, 3) 
<a name="l12039"><span class="ln">12039 </span></a>    &gt;&gt;&gt; torch.zeros_like(input) 
<a name="l12040"><span class="ln">12040 </span></a>    tensor([[ 0.,  0.,  0.], 
<a name="l12041"><span class="ln">12041 </span></a>            [ 0.,  0.,  0.]]) 
<a name="l12042"><span class="ln">12042 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_like_common_args</span><span class="s3">),</span>
<a name="l12043"><span class="ln">12043 </span></a><span class="s3">)</span>
<a name="l12044"><span class="ln">12044 </span></a>
<a name="l12045"><span class="ln">12045 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12046"><span class="ln">12046 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">empty</span><span class="s3">,</span>
<a name="l12047"><span class="ln">12047 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12048"><span class="ln">12048 </span></a>empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, \ 
<a name="l12049"><span class="ln">12049 </span></a>memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l12050"><span class="ln">12050 </span></a> 
<a name="l12051"><span class="ln">12051 </span></a>Returns a tensor filled with uninitialized data. The shape of the tensor is 
<a name="l12052"><span class="ln">12052 </span></a>defined by the variable argument :attr:`size`. 
<a name="l12053"><span class="ln">12053 </span></a> 
<a name="l12054"><span class="ln">12054 </span></a>.. note:: 
<a name="l12055"><span class="ln">12055 </span></a>    If :func:`torch.use_deterministic_algorithms()` and 
<a name="l12056"><span class="ln">12056 </span></a>    :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l12057"><span class="ln">12057 </span></a>    ``True``, the output tensor is initialized to prevent any possible 
<a name="l12058"><span class="ln">12058 </span></a>    nondeterministic behavior from using the data as an input to an operation. 
<a name="l12059"><span class="ln">12059 </span></a>    Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l12060"><span class="ln">12060 </span></a>    are filled with the maximum value. 
<a name="l12061"><span class="ln">12061 </span></a> 
<a name="l12062"><span class="ln">12062 </span></a>Args: 
<a name="l12063"><span class="ln">12063 </span></a>    size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l12064"><span class="ln">12064 </span></a>        Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l12065"><span class="ln">12065 </span></a> 
<a name="l12066"><span class="ln">12066 </span></a>Keyword args: 
<a name="l12067"><span class="ln">12067 </span></a>    {out} 
<a name="l12068"><span class="ln">12068 </span></a>    {dtype} 
<a name="l12069"><span class="ln">12069 </span></a>    {layout} 
<a name="l12070"><span class="ln">12070 </span></a>    {device} 
<a name="l12071"><span class="ln">12071 </span></a>    {requires_grad} 
<a name="l12072"><span class="ln">12072 </span></a>    {pin_memory} 
<a name="l12073"><span class="ln">12073 </span></a>    {memory_format} 
<a name="l12074"><span class="ln">12074 </span></a> 
<a name="l12075"><span class="ln">12075 </span></a>Example:: 
<a name="l12076"><span class="ln">12076 </span></a> 
<a name="l12077"><span class="ln">12077 </span></a>    &gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64) 
<a name="l12078"><span class="ln">12078 </span></a>    tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13], 
<a name="l12079"><span class="ln">12079 </span></a>            [ 7.5751e+18,  7.1428e+18,  7.5955e+18]]) 
<a name="l12080"><span class="ln">12080 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12081"><span class="ln">12081 </span></a><span class="s3">)</span>
<a name="l12082"><span class="ln">12082 </span></a>
<a name="l12083"><span class="ln">12083 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12084"><span class="ln">12084 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">empty_like</span><span class="s3">,</span>
<a name="l12085"><span class="ln">12085 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12086"><span class="ln">12086 </span></a>empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l12087"><span class="ln">12087 </span></a> 
<a name="l12088"><span class="ln">12088 </span></a>Returns an uninitialized tensor with the same size as :attr:`input`. 
<a name="l12089"><span class="ln">12089 </span></a>``torch.empty_like(input)`` is equivalent to 
<a name="l12090"><span class="ln">12090 </span></a>``torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l12091"><span class="ln">12091 </span></a> 
<a name="l12092"><span class="ln">12092 </span></a>.. note:: 
<a name="l12093"><span class="ln">12093 </span></a>    If :func:`torch.use_deterministic_algorithms()` and 
<a name="l12094"><span class="ln">12094 </span></a>    :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l12095"><span class="ln">12095 </span></a>    ``True``, the output tensor is initialized to prevent any possible 
<a name="l12096"><span class="ln">12096 </span></a>    nondeterministic behavior from using the data as an input to an operation. 
<a name="l12097"><span class="ln">12097 </span></a>    Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l12098"><span class="ln">12098 </span></a>    are filled with the maximum value. 
<a name="l12099"><span class="ln">12099 </span></a> 
<a name="l12100"><span class="ln">12100 </span></a>Args: 
<a name="l12101"><span class="ln">12101 </span></a>    {input} 
<a name="l12102"><span class="ln">12102 </span></a> 
<a name="l12103"><span class="ln">12103 </span></a>Keyword args: 
<a name="l12104"><span class="ln">12104 </span></a>    {dtype} 
<a name="l12105"><span class="ln">12105 </span></a>    {layout} 
<a name="l12106"><span class="ln">12106 </span></a>    {device} 
<a name="l12107"><span class="ln">12107 </span></a>    {requires_grad} 
<a name="l12108"><span class="ln">12108 </span></a>    {memory_format} 
<a name="l12109"><span class="ln">12109 </span></a> 
<a name="l12110"><span class="ln">12110 </span></a>Example:: 
<a name="l12111"><span class="ln">12111 </span></a> 
<a name="l12112"><span class="ln">12112 </span></a>    &gt;&gt;&gt; a=torch.empty((2,3), dtype=torch.int32, device = 'cuda') 
<a name="l12113"><span class="ln">12113 </span></a>    &gt;&gt;&gt; torch.empty_like(a) 
<a name="l12114"><span class="ln">12114 </span></a>    tensor([[0, 0, 0], 
<a name="l12115"><span class="ln">12115 </span></a>            [0, 0, 0]], device='cuda:0', dtype=torch.int32) 
<a name="l12116"><span class="ln">12116 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_like_common_args</span><span class="s3">),</span>
<a name="l12117"><span class="ln">12117 </span></a><span class="s3">)</span>
<a name="l12118"><span class="ln">12118 </span></a>
<a name="l12119"><span class="ln">12119 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12120"><span class="ln">12120 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">empty_strided</span><span class="s3">,</span>
<a name="l12121"><span class="ln">12121 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12122"><span class="ln">12122 </span></a>empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l12123"><span class="ln">12123 </span></a> 
<a name="l12124"><span class="ln">12124 </span></a>Creates a tensor with the specified :attr:`size` and :attr:`stride` and filled with undefined data. 
<a name="l12125"><span class="ln">12125 </span></a> 
<a name="l12126"><span class="ln">12126 </span></a>.. warning:: 
<a name="l12127"><span class="ln">12127 </span></a>    If the constructed tensor is &quot;overlapped&quot; (with multiple indices referring to the same element 
<a name="l12128"><span class="ln">12128 </span></a>    in memory) its behavior is undefined. 
<a name="l12129"><span class="ln">12129 </span></a> 
<a name="l12130"><span class="ln">12130 </span></a>.. note:: 
<a name="l12131"><span class="ln">12131 </span></a>    If :func:`torch.use_deterministic_algorithms()` and 
<a name="l12132"><span class="ln">12132 </span></a>    :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l12133"><span class="ln">12133 </span></a>    ``True``, the output tensor is initialized to prevent any possible 
<a name="l12134"><span class="ln">12134 </span></a>    nondeterministic behavior from using the data as an input to an operation. 
<a name="l12135"><span class="ln">12135 </span></a>    Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l12136"><span class="ln">12136 </span></a>    are filled with the maximum value. 
<a name="l12137"><span class="ln">12137 </span></a> 
<a name="l12138"><span class="ln">12138 </span></a>Args: 
<a name="l12139"><span class="ln">12139 </span></a>    size (tuple of int): the shape of the output tensor 
<a name="l12140"><span class="ln">12140 </span></a>    stride (tuple of int): the strides of the output tensor 
<a name="l12141"><span class="ln">12141 </span></a> 
<a name="l12142"><span class="ln">12142 </span></a>Keyword args: 
<a name="l12143"><span class="ln">12143 </span></a>    {dtype} 
<a name="l12144"><span class="ln">12144 </span></a>    {layout} 
<a name="l12145"><span class="ln">12145 </span></a>    {device} 
<a name="l12146"><span class="ln">12146 </span></a>    {requires_grad} 
<a name="l12147"><span class="ln">12147 </span></a>    {pin_memory} 
<a name="l12148"><span class="ln">12148 </span></a> 
<a name="l12149"><span class="ln">12149 </span></a>Example:: 
<a name="l12150"><span class="ln">12150 </span></a> 
<a name="l12151"><span class="ln">12151 </span></a>    &gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2)) 
<a name="l12152"><span class="ln">12152 </span></a>    &gt;&gt;&gt; a 
<a name="l12153"><span class="ln">12153 </span></a>    tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07], 
<a name="l12154"><span class="ln">12154 </span></a>            [0.0000e+00, 0.0000e+00, 3.0705e-41]]) 
<a name="l12155"><span class="ln">12155 </span></a>    &gt;&gt;&gt; a.stride() 
<a name="l12156"><span class="ln">12156 </span></a>    (1, 2) 
<a name="l12157"><span class="ln">12157 </span></a>    &gt;&gt;&gt; a.size() 
<a name="l12158"><span class="ln">12158 </span></a>    torch.Size([2, 3]) 
<a name="l12159"><span class="ln">12159 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12160"><span class="ln">12160 </span></a><span class="s3">)</span>
<a name="l12161"><span class="ln">12161 </span></a>
<a name="l12162"><span class="ln">12162 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12163"><span class="ln">12163 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">empty_permuted</span><span class="s3">,</span>
<a name="l12164"><span class="ln">12164 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12165"><span class="ln">12165 </span></a>empty_permuted(size, physical_layout, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l12166"><span class="ln">12166 </span></a> 
<a name="l12167"><span class="ln">12167 </span></a>Creates an uninitialized, non-overlapping and dense tensor with the 
<a name="l12168"><span class="ln">12168 </span></a>specified :attr:`size`, with :attr:`physical_layout` specifying how the 
<a name="l12169"><span class="ln">12169 </span></a>dimensions are physically laid out in memory (each logical dimension is listed 
<a name="l12170"><span class="ln">12170 </span></a>from outermost to innermost).  :attr:`physical_layout` is a generalization 
<a name="l12171"><span class="ln">12171 </span></a>of NCHW/NHWC notation: if each dimension is assigned a number according to 
<a name="l12172"><span class="ln">12172 </span></a>what order they occur in size (N=0, C=1, H=2, W=3), then NCHW is ``(0, 1, 2, 3)`` 
<a name="l12173"><span class="ln">12173 </span></a>while NHWC is ``(0, 2, 3, 1)``.  Equivalently, the strides of the output 
<a name="l12174"><span class="ln">12174 </span></a>tensor ``t`` are such that ``t.stride(physical_layout[i]) == contiguous_strides[i]`` 
<a name="l12175"><span class="ln">12175 </span></a>(notably, this function is *not* equivalent to ``torch.empty(size).permute(physical_layout)``). 
<a name="l12176"><span class="ln">12176 </span></a> 
<a name="l12177"><span class="ln">12177 </span></a>Unlike :func:`torch.empty_strided`, this is guaranteed to produce a dense 
<a name="l12178"><span class="ln">12178 </span></a>tensor with no overlaps.  If possible, prefer using this function over 
<a name="l12179"><span class="ln">12179 </span></a>:func:`torch.empty_strided` or manual use of :func:`torch.as_strided`. 
<a name="l12180"><span class="ln">12180 </span></a> 
<a name="l12181"><span class="ln">12181 </span></a>.. note:: 
<a name="l12182"><span class="ln">12182 </span></a>    If :func:`torch.use_deterministic_algorithms()` and 
<a name="l12183"><span class="ln">12183 </span></a>    :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l12184"><span class="ln">12184 </span></a>    ``True``, the output tensor is initialized to prevent any possible 
<a name="l12185"><span class="ln">12185 </span></a>    nondeterministic behavior from using the data as an input to an operation. 
<a name="l12186"><span class="ln">12186 </span></a>    Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l12187"><span class="ln">12187 </span></a>    are filled with the maximum value. 
<a name="l12188"><span class="ln">12188 </span></a> 
<a name="l12189"><span class="ln">12189 </span></a>Args: 
<a name="l12190"><span class="ln">12190 </span></a>    size (tuple of int): the shape of the output tensor 
<a name="l12191"><span class="ln">12191 </span></a>    physical_layout (tuple of int): the ordering of dimensions physically in memory 
<a name="l12192"><span class="ln">12192 </span></a> 
<a name="l12193"><span class="ln">12193 </span></a>Keyword args: 
<a name="l12194"><span class="ln">12194 </span></a>    {dtype} 
<a name="l12195"><span class="ln">12195 </span></a>    {layout} 
<a name="l12196"><span class="ln">12196 </span></a>    {device} 
<a name="l12197"><span class="ln">12197 </span></a>    {requires_grad} 
<a name="l12198"><span class="ln">12198 </span></a>    {pin_memory} 
<a name="l12199"><span class="ln">12199 </span></a> 
<a name="l12200"><span class="ln">12200 </span></a>Examples: 
<a name="l12201"><span class="ln">12201 </span></a> 
<a name="l12202"><span class="ln">12202 </span></a>    &gt;&gt;&gt; torch.empty((2, 3, 5, 7)).stride() 
<a name="l12203"><span class="ln">12203 </span></a>    (105, 35, 7, 1) 
<a name="l12204"><span class="ln">12204 </span></a>    &gt;&gt;&gt; torch.empty_permuted((2, 3, 5, 7), (0, 1, 2, 3)).stride() 
<a name="l12205"><span class="ln">12205 </span></a>    (105, 35, 7, 1) 
<a name="l12206"><span class="ln">12206 </span></a>    &gt;&gt;&gt; torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).stride() 
<a name="l12207"><span class="ln">12207 </span></a>    (105, 1, 21, 3) 
<a name="l12208"><span class="ln">12208 </span></a>    &gt;&gt;&gt; torch.empty_permuted((2, 3, 5, 7), (0, 2, 3, 1)).stride() 
<a name="l12209"><span class="ln">12209 </span></a>    (105, 1, 21, 3) 
<a name="l12210"><span class="ln">12210 </span></a>    &gt;&gt;&gt; torch.empty_permuted((2, 3, 5, 7), (0, 2, 3, 1)).dim_order() 
<a name="l12211"><span class="ln">12211 </span></a>    (0, 2, 3, 1) 
<a name="l12212"><span class="ln">12212 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12213"><span class="ln">12213 </span></a><span class="s3">)</span>
<a name="l12214"><span class="ln">12214 </span></a>
<a name="l12215"><span class="ln">12215 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12216"><span class="ln">12216 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">full</span><span class="s3">,</span>
<a name="l12217"><span class="ln">12217 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12218"><span class="ln">12218 </span></a>full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12219"><span class="ln">12219 </span></a> 
<a name="l12220"><span class="ln">12220 </span></a>Creates a tensor of size :attr:`size` filled with :attr:`fill_value`. The 
<a name="l12221"><span class="ln">12221 </span></a>tensor's dtype is inferred from :attr:`fill_value`. 
<a name="l12222"><span class="ln">12222 </span></a> 
<a name="l12223"><span class="ln">12223 </span></a>Args: 
<a name="l12224"><span class="ln">12224 </span></a>    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l12225"><span class="ln">12225 </span></a>        shape of the output tensor. 
<a name="l12226"><span class="ln">12226 </span></a>    fill_value (Scalar): the value to fill the output tensor with. 
<a name="l12227"><span class="ln">12227 </span></a> 
<a name="l12228"><span class="ln">12228 </span></a>Keyword args: 
<a name="l12229"><span class="ln">12229 </span></a>    {out} 
<a name="l12230"><span class="ln">12230 </span></a>    {dtype} 
<a name="l12231"><span class="ln">12231 </span></a>    {layout} 
<a name="l12232"><span class="ln">12232 </span></a>    {device} 
<a name="l12233"><span class="ln">12233 </span></a>    {requires_grad} 
<a name="l12234"><span class="ln">12234 </span></a> 
<a name="l12235"><span class="ln">12235 </span></a>Example:: 
<a name="l12236"><span class="ln">12236 </span></a> 
<a name="l12237"><span class="ln">12237 </span></a>    &gt;&gt;&gt; torch.full((2, 3), 3.141592) 
<a name="l12238"><span class="ln">12238 </span></a>    tensor([[ 3.1416,  3.1416,  3.1416], 
<a name="l12239"><span class="ln">12239 </span></a>            [ 3.1416,  3.1416,  3.1416]]) 
<a name="l12240"><span class="ln">12240 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12241"><span class="ln">12241 </span></a><span class="s3">)</span>
<a name="l12242"><span class="ln">12242 </span></a>
<a name="l12243"><span class="ln">12243 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12244"><span class="ln">12244 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">full_like</span><span class="s3">,</span>
<a name="l12245"><span class="ln">12245 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12246"><span class="ln">12246 </span></a>full_like(input, fill_value, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \ 
<a name="l12247"><span class="ln">12247 </span></a>memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l12248"><span class="ln">12248 </span></a> 
<a name="l12249"><span class="ln">12249 </span></a>Returns a tensor with the same size as :attr:`input` filled with :attr:`fill_value`. 
<a name="l12250"><span class="ln">12250 </span></a>``torch.full_like(input, fill_value)`` is equivalent to 
<a name="l12251"><span class="ln">12251 </span></a>``torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l12252"><span class="ln">12252 </span></a> 
<a name="l12253"><span class="ln">12253 </span></a>Args: 
<a name="l12254"><span class="ln">12254 </span></a>    {input} 
<a name="l12255"><span class="ln">12255 </span></a>    fill_value: the number to fill the output tensor with. 
<a name="l12256"><span class="ln">12256 </span></a> 
<a name="l12257"><span class="ln">12257 </span></a>Keyword args: 
<a name="l12258"><span class="ln">12258 </span></a>    {dtype} 
<a name="l12259"><span class="ln">12259 </span></a>    {layout} 
<a name="l12260"><span class="ln">12260 </span></a>    {device} 
<a name="l12261"><span class="ln">12261 </span></a>    {requires_grad} 
<a name="l12262"><span class="ln">12262 </span></a>    {memory_format} 
<a name="l12263"><span class="ln">12263 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_like_common_args</span><span class="s3">),</span>
<a name="l12264"><span class="ln">12264 </span></a><span class="s3">)</span>
<a name="l12265"><span class="ln">12265 </span></a>
<a name="l12266"><span class="ln">12266 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12267"><span class="ln">12267 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">det</span><span class="s3">,</span>
<a name="l12268"><span class="ln">12268 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12269"><span class="ln">12269 </span></a>det(input) -&gt; Tensor 
<a name="l12270"><span class="ln">12270 </span></a> 
<a name="l12271"><span class="ln">12271 </span></a>Alias for :func:`torch.linalg.det` 
<a name="l12272"><span class="ln">12272 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12273"><span class="ln">12273 </span></a><span class="s3">)</span>
<a name="l12274"><span class="ln">12274 </span></a>
<a name="l12275"><span class="ln">12275 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12276"><span class="ln">12276 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">where</span><span class="s3">,</span>
<a name="l12277"><span class="ln">12277 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12278"><span class="ln">12278 </span></a>where(condition, input, other, *, out=None) -&gt; Tensor 
<a name="l12279"><span class="ln">12279 </span></a> 
<a name="l12280"><span class="ln">12280 </span></a>Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`. 
<a name="l12281"><span class="ln">12281 </span></a> 
<a name="l12282"><span class="ln">12282 </span></a>The operation is defined as: 
<a name="l12283"><span class="ln">12283 </span></a> 
<a name="l12284"><span class="ln">12284 </span></a>.. math:: 
<a name="l12285"><span class="ln">12285 </span></a>    \text{out}_i = \begin{cases} 
<a name="l12286"><span class="ln">12286 </span></a>        \text{input}_i &amp; \text{if } \text{condition}_i \\ 
<a name="l12287"><span class="ln">12287 </span></a>        \text{other}_i &amp; \text{otherwise} \\ 
<a name="l12288"><span class="ln">12288 </span></a>    \end{cases} 
<a name="l12289"><span class="ln">12289 </span></a>&quot;&quot;&quot;</span>
<a name="l12290"><span class="ln">12290 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12291"><span class="ln">12291 </span></a>.. note:: 
<a name="l12292"><span class="ln">12292 </span></a>    The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l12293"><span class="ln">12293 </span></a> 
<a name="l12294"><span class="ln">12294 </span></a>Arguments: 
<a name="l12295"><span class="ln">12295 </span></a>    condition (BoolTensor): When True (nonzero), yield input, otherwise yield other 
<a name="l12296"><span class="ln">12296 </span></a>    input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices 
<a name="l12297"><span class="ln">12297 </span></a>                          where :attr:`condition` is ``True`` 
<a name="l12298"><span class="ln">12298 </span></a>    other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices 
<a name="l12299"><span class="ln">12299 </span></a>                          where :attr:`condition` is ``False`` 
<a name="l12300"><span class="ln">12300 </span></a> 
<a name="l12301"><span class="ln">12301 </span></a>Keyword args: 
<a name="l12302"><span class="ln">12302 </span></a>    {out} 
<a name="l12303"><span class="ln">12303 </span></a> 
<a name="l12304"><span class="ln">12304 </span></a>Returns: 
<a name="l12305"><span class="ln">12305 </span></a>    Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other` 
<a name="l12306"><span class="ln">12306 </span></a> 
<a name="l12307"><span class="ln">12307 </span></a>Example:: 
<a name="l12308"><span class="ln">12308 </span></a> 
<a name="l12309"><span class="ln">12309 </span></a>    &gt;&gt;&gt; x = torch.randn(3, 2) 
<a name="l12310"><span class="ln">12310 </span></a>    &gt;&gt;&gt; y = torch.ones(3, 2) 
<a name="l12311"><span class="ln">12311 </span></a>    &gt;&gt;&gt; x 
<a name="l12312"><span class="ln">12312 </span></a>    tensor([[-0.4620,  0.3139], 
<a name="l12313"><span class="ln">12313 </span></a>            [ 0.3898, -0.7197], 
<a name="l12314"><span class="ln">12314 </span></a>            [ 0.0478, -0.1657]]) 
<a name="l12315"><span class="ln">12315 </span></a>    &gt;&gt;&gt; torch.where(x &gt; 0, 1.0, 0.0) 
<a name="l12316"><span class="ln">12316 </span></a>    tensor([[0., 1.], 
<a name="l12317"><span class="ln">12317 </span></a>            [1., 0.], 
<a name="l12318"><span class="ln">12318 </span></a>            [1., 0.]]) 
<a name="l12319"><span class="ln">12319 </span></a>    &gt;&gt;&gt; torch.where(x &gt; 0, x, y) 
<a name="l12320"><span class="ln">12320 </span></a>    tensor([[ 1.0000,  0.3139], 
<a name="l12321"><span class="ln">12321 </span></a>            [ 0.3898,  1.0000], 
<a name="l12322"><span class="ln">12322 </span></a>            [ 0.0478,  1.0000]]) 
<a name="l12323"><span class="ln">12323 </span></a>    &gt;&gt;&gt; x = torch.randn(2, 2, dtype=torch.double) 
<a name="l12324"><span class="ln">12324 </span></a>    &gt;&gt;&gt; x 
<a name="l12325"><span class="ln">12325 </span></a>    tensor([[ 1.0779,  0.0383], 
<a name="l12326"><span class="ln">12326 </span></a>            [-0.8785, -1.1089]], dtype=torch.float64) 
<a name="l12327"><span class="ln">12327 </span></a>    &gt;&gt;&gt; torch.where(x &gt; 0, x, 0.) 
<a name="l12328"><span class="ln">12328 </span></a>    tensor([[1.0779, 0.0383], 
<a name="l12329"><span class="ln">12329 </span></a>            [0.0000, 0.0000]], dtype=torch.float64) 
<a name="l12330"><span class="ln">12330 </span></a> 
<a name="l12331"><span class="ln">12331 </span></a>.. function:: where(condition) -&gt; tuple of LongTensor 
<a name="l12332"><span class="ln">12332 </span></a>   :noindex: 
<a name="l12333"><span class="ln">12333 </span></a> 
<a name="l12334"><span class="ln">12334 </span></a>``torch.where(condition)`` is identical to 
<a name="l12335"><span class="ln">12335 </span></a>``torch.nonzero(condition, as_tuple=True)``. 
<a name="l12336"><span class="ln">12336 </span></a> 
<a name="l12337"><span class="ln">12337 </span></a>.. note:: 
<a name="l12338"><span class="ln">12338 </span></a>    See also :func:`torch.nonzero`. 
<a name="l12339"><span class="ln">12339 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l12340"><span class="ln">12340 </span></a><span class="s3">)</span>
<a name="l12341"><span class="ln">12341 </span></a>
<a name="l12342"><span class="ln">12342 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12343"><span class="ln">12343 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">logdet</span><span class="s3">,</span>
<a name="l12344"><span class="ln">12344 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12345"><span class="ln">12345 </span></a>logdet(input) -&gt; Tensor 
<a name="l12346"><span class="ln">12346 </span></a> 
<a name="l12347"><span class="ln">12347 </span></a>Calculates log determinant of a square matrix or batches of square matrices. 
<a name="l12348"><span class="ln">12348 </span></a> 
<a name="l12349"><span class="ln">12349 </span></a>It returns ``-inf`` if the input has a determinant of zero, and ``NaN`` if it has 
<a name="l12350"><span class="ln">12350 </span></a>a negative determinant. 
<a name="l12351"><span class="ln">12351 </span></a> 
<a name="l12352"><span class="ln">12352 </span></a>.. note:: 
<a name="l12353"><span class="ln">12353 </span></a>    Backward through :meth:`logdet` internally uses SVD results when :attr:`input` 
<a name="l12354"><span class="ln">12354 </span></a>    is not invertible. In this case, double backward through :meth:`logdet` will 
<a name="l12355"><span class="ln">12355 </span></a>    be unstable in when :attr:`input` doesn't have distinct singular values. See 
<a name="l12356"><span class="ln">12356 </span></a>    :func:`torch.linalg.svd` for details. 
<a name="l12357"><span class="ln">12357 </span></a> 
<a name="l12358"><span class="ln">12358 </span></a>.. seealso:: 
<a name="l12359"><span class="ln">12359 </span></a> 
<a name="l12360"><span class="ln">12360 </span></a>        :func:`torch.linalg.slogdet` computes the sign (resp. angle) and natural logarithm of the 
<a name="l12361"><span class="ln">12361 </span></a>        absolute value of the determinant of real-valued (resp. complex) square matrices. 
<a name="l12362"><span class="ln">12362 </span></a> 
<a name="l12363"><span class="ln">12363 </span></a>Arguments: 
<a name="l12364"><span class="ln">12364 </span></a>    input (Tensor): the input tensor of size ``(*, n, n)`` where ``*`` is zero or more 
<a name="l12365"><span class="ln">12365 </span></a>                batch dimensions. 
<a name="l12366"><span class="ln">12366 </span></a> 
<a name="l12367"><span class="ln">12367 </span></a>Example:: 
<a name="l12368"><span class="ln">12368 </span></a> 
<a name="l12369"><span class="ln">12369 </span></a>    &gt;&gt;&gt; A = torch.randn(3, 3) 
<a name="l12370"><span class="ln">12370 </span></a>    &gt;&gt;&gt; torch.det(A) 
<a name="l12371"><span class="ln">12371 </span></a>    tensor(0.2611) 
<a name="l12372"><span class="ln">12372 </span></a>    &gt;&gt;&gt; torch.logdet(A) 
<a name="l12373"><span class="ln">12373 </span></a>    tensor(-1.3430) 
<a name="l12374"><span class="ln">12374 </span></a>    &gt;&gt;&gt; A 
<a name="l12375"><span class="ln">12375 </span></a>    tensor([[[ 0.9254, -0.6213], 
<a name="l12376"><span class="ln">12376 </span></a>             [-0.5787,  1.6843]], 
<a name="l12377"><span class="ln">12377 </span></a> 
<a name="l12378"><span class="ln">12378 </span></a>            [[ 0.3242, -0.9665], 
<a name="l12379"><span class="ln">12379 </span></a>             [ 0.4539, -0.0887]], 
<a name="l12380"><span class="ln">12380 </span></a> 
<a name="l12381"><span class="ln">12381 </span></a>            [[ 1.1336, -0.4025], 
<a name="l12382"><span class="ln">12382 </span></a>             [-0.7089,  0.9032]]]) 
<a name="l12383"><span class="ln">12383 </span></a>    &gt;&gt;&gt; A.det() 
<a name="l12384"><span class="ln">12384 </span></a>    tensor([1.1990, 0.4099, 0.7386]) 
<a name="l12385"><span class="ln">12385 </span></a>    &gt;&gt;&gt; A.det().log() 
<a name="l12386"><span class="ln">12386 </span></a>    tensor([ 0.1815, -0.8917, -0.3031]) 
<a name="l12387"><span class="ln">12387 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12388"><span class="ln">12388 </span></a><span class="s3">)</span>
<a name="l12389"><span class="ln">12389 </span></a>
<a name="l12390"><span class="ln">12390 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12391"><span class="ln">12391 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">slogdet</span><span class="s3">,</span>
<a name="l12392"><span class="ln">12392 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12393"><span class="ln">12393 </span></a>slogdet(input) -&gt; (Tensor, Tensor) 
<a name="l12394"><span class="ln">12394 </span></a> 
<a name="l12395"><span class="ln">12395 </span></a>Alias for :func:`torch.linalg.slogdet` 
<a name="l12396"><span class="ln">12396 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12397"><span class="ln">12397 </span></a><span class="s3">)</span>
<a name="l12398"><span class="ln">12398 </span></a>
<a name="l12399"><span class="ln">12399 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12400"><span class="ln">12400 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">pinverse</span><span class="s3">,</span>
<a name="l12401"><span class="ln">12401 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12402"><span class="ln">12402 </span></a>pinverse(input, rcond=1e-15) -&gt; Tensor 
<a name="l12403"><span class="ln">12403 </span></a> 
<a name="l12404"><span class="ln">12404 </span></a>Alias for :func:`torch.linalg.pinv` 
<a name="l12405"><span class="ln">12405 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12406"><span class="ln">12406 </span></a><span class="s3">)</span>
<a name="l12407"><span class="ln">12407 </span></a>
<a name="l12408"><span class="ln">12408 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12409"><span class="ln">12409 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">hann_window</span><span class="s3">,</span>
<a name="l12410"><span class="ln">12410 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12411"><span class="ln">12411 </span></a>hann_window(window_length, periodic=True, *, dtype=None, \ 
<a name="l12412"><span class="ln">12412 </span></a>layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12413"><span class="ln">12413 </span></a>&quot;&quot;&quot;</span>
<a name="l12414"><span class="ln">12414 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12415"><span class="ln">12415 </span></a>Hann window function. 
<a name="l12416"><span class="ln">12416 </span></a> 
<a name="l12417"><span class="ln">12417 </span></a>.. math:: 
<a name="l12418"><span class="ln">12418 </span></a>    w[n] = \frac{1}{2}\ \left[1 - \cos \left( \frac{2 \pi n}{N - 1} \right)\right] = 
<a name="l12419"><span class="ln">12419 </span></a>            \sin^2 \left( \frac{\pi n}{N - 1} \right), 
<a name="l12420"><span class="ln">12420 </span></a> 
<a name="l12421"><span class="ln">12421 </span></a>where :math:`N` is the full window size. 
<a name="l12422"><span class="ln">12422 </span></a> 
<a name="l12423"><span class="ln">12423 </span></a>The input :attr:`window_length` is a positive integer controlling the 
<a name="l12424"><span class="ln">12424 </span></a>returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l12425"><span class="ln">12425 </span></a>window trims off the last duplicate value from the symmetric window and is 
<a name="l12426"><span class="ln">12426 </span></a>ready to be used as a periodic window with functions like 
<a name="l12427"><span class="ln">12427 </span></a>:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l12428"><span class="ln">12428 </span></a>above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l12429"><span class="ln">12429 </span></a>``torch.hann_window(L, periodic=True)`` equal to 
<a name="l12430"><span class="ln">12430 </span></a>``torch.hann_window(L + 1, periodic=False)[:-1])``. 
<a name="l12431"><span class="ln">12431 </span></a> 
<a name="l12432"><span class="ln">12432 </span></a>.. note:: 
<a name="l12433"><span class="ln">12433 </span></a>    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l12434"><span class="ln">12434 </span></a>&quot;&quot;&quot;</span>
<a name="l12435"><span class="ln">12435 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12436"><span class="ln">12436 </span></a>Arguments: 
<a name="l12437"><span class="ln">12437 </span></a>    window_length (int): the size of returned window 
<a name="l12438"><span class="ln">12438 </span></a>    periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l12439"><span class="ln">12439 </span></a>        function. If False, return a symmetric window. 
<a name="l12440"><span class="ln">12440 </span></a> 
<a name="l12441"><span class="ln">12441 </span></a>Keyword args: 
<a name="l12442"><span class="ln">12442 </span></a>    {dtype} Only floating point types are supported. 
<a name="l12443"><span class="ln">12443 </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l12444"><span class="ln">12444 </span></a>          ``torch.strided`` (dense layout) is supported. 
<a name="l12445"><span class="ln">12445 </span></a>    {device} 
<a name="l12446"><span class="ln">12446 </span></a>    {requires_grad} 
<a name="l12447"><span class="ln">12447 </span></a> 
<a name="l12448"><span class="ln">12448 </span></a>Returns: 
<a name="l12449"><span class="ln">12449 </span></a>    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window 
<a name="l12450"><span class="ln">12450 </span></a> 
<a name="l12451"><span class="ln">12451 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12452"><span class="ln">12452 </span></a><span class="s3">)</span>
<a name="l12453"><span class="ln">12453 </span></a>
<a name="l12454"><span class="ln">12454 </span></a>
<a name="l12455"><span class="ln">12455 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12456"><span class="ln">12456 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">hamming_window</span><span class="s3">,</span>
<a name="l12457"><span class="ln">12457 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12458"><span class="ln">12458 </span></a>hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, \ 
<a name="l12459"><span class="ln">12459 </span></a>layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12460"><span class="ln">12460 </span></a>&quot;&quot;&quot;</span>
<a name="l12461"><span class="ln">12461 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12462"><span class="ln">12462 </span></a>Hamming window function. 
<a name="l12463"><span class="ln">12463 </span></a> 
<a name="l12464"><span class="ln">12464 </span></a>.. math:: 
<a name="l12465"><span class="ln">12465 </span></a>    w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right), 
<a name="l12466"><span class="ln">12466 </span></a> 
<a name="l12467"><span class="ln">12467 </span></a>where :math:`N` is the full window size. 
<a name="l12468"><span class="ln">12468 </span></a> 
<a name="l12469"><span class="ln">12469 </span></a>The input :attr:`window_length` is a positive integer controlling the 
<a name="l12470"><span class="ln">12470 </span></a>returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l12471"><span class="ln">12471 </span></a>window trims off the last duplicate value from the symmetric window and is 
<a name="l12472"><span class="ln">12472 </span></a>ready to be used as a periodic window with functions like 
<a name="l12473"><span class="ln">12473 </span></a>:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l12474"><span class="ln">12474 </span></a>above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l12475"><span class="ln">12475 </span></a>``torch.hamming_window(L, periodic=True)`` equal to 
<a name="l12476"><span class="ln">12476 </span></a>``torch.hamming_window(L + 1, periodic=False)[:-1])``. 
<a name="l12477"><span class="ln">12477 </span></a> 
<a name="l12478"><span class="ln">12478 </span></a>.. note:: 
<a name="l12479"><span class="ln">12479 </span></a>    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l12480"><span class="ln">12480 </span></a> 
<a name="l12481"><span class="ln">12481 </span></a>.. note:: 
<a name="l12482"><span class="ln">12482 </span></a>    This is a generalized version of :meth:`torch.hann_window`. 
<a name="l12483"><span class="ln">12483 </span></a>&quot;&quot;&quot;</span>
<a name="l12484"><span class="ln">12484 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12485"><span class="ln">12485 </span></a>Arguments: 
<a name="l12486"><span class="ln">12486 </span></a>    window_length (int): the size of returned window 
<a name="l12487"><span class="ln">12487 </span></a>    periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l12488"><span class="ln">12488 </span></a>        function. If False, return a symmetric window. 
<a name="l12489"><span class="ln">12489 </span></a>    alpha (float, optional): The coefficient :math:`\alpha` in the equation above 
<a name="l12490"><span class="ln">12490 </span></a>    beta (float, optional): The coefficient :math:`\beta` in the equation above 
<a name="l12491"><span class="ln">12491 </span></a> 
<a name="l12492"><span class="ln">12492 </span></a>Keyword args: 
<a name="l12493"><span class="ln">12493 </span></a>    {dtype} Only floating point types are supported. 
<a name="l12494"><span class="ln">12494 </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l12495"><span class="ln">12495 </span></a>          ``torch.strided`` (dense layout) is supported. 
<a name="l12496"><span class="ln">12496 </span></a>    {device} 
<a name="l12497"><span class="ln">12497 </span></a>    {requires_grad} 
<a name="l12498"><span class="ln">12498 </span></a> 
<a name="l12499"><span class="ln">12499 </span></a>Returns: 
<a name="l12500"><span class="ln">12500 </span></a>    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window. 
<a name="l12501"><span class="ln">12501 </span></a> 
<a name="l12502"><span class="ln">12502 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12503"><span class="ln">12503 </span></a><span class="s3">)</span>
<a name="l12504"><span class="ln">12504 </span></a>
<a name="l12505"><span class="ln">12505 </span></a>
<a name="l12506"><span class="ln">12506 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12507"><span class="ln">12507 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bartlett_window</span><span class="s3">,</span>
<a name="l12508"><span class="ln">12508 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12509"><span class="ln">12509 </span></a>bartlett_window(window_length, periodic=True, *, dtype=None, \ 
<a name="l12510"><span class="ln">12510 </span></a>layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12511"><span class="ln">12511 </span></a>&quot;&quot;&quot;</span>
<a name="l12512"><span class="ln">12512 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12513"><span class="ln">12513 </span></a>Bartlett window function. 
<a name="l12514"><span class="ln">12514 </span></a> 
<a name="l12515"><span class="ln">12515 </span></a>.. math:: 
<a name="l12516"><span class="ln">12516 </span></a>    w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases} 
<a name="l12517"><span class="ln">12517 </span></a>        \frac{2n}{N - 1} &amp; \text{if } 0 \leq n \leq \frac{N - 1}{2} \\ 
<a name="l12518"><span class="ln">12518 </span></a>        2 - \frac{2n}{N - 1} &amp; \text{if } \frac{N - 1}{2} &lt; n &lt; N \\ 
<a name="l12519"><span class="ln">12519 </span></a>    \end{cases}, 
<a name="l12520"><span class="ln">12520 </span></a> 
<a name="l12521"><span class="ln">12521 </span></a>where :math:`N` is the full window size. 
<a name="l12522"><span class="ln">12522 </span></a> 
<a name="l12523"><span class="ln">12523 </span></a>The input :attr:`window_length` is a positive integer controlling the 
<a name="l12524"><span class="ln">12524 </span></a>returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l12525"><span class="ln">12525 </span></a>window trims off the last duplicate value from the symmetric window and is 
<a name="l12526"><span class="ln">12526 </span></a>ready to be used as a periodic window with functions like 
<a name="l12527"><span class="ln">12527 </span></a>:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l12528"><span class="ln">12528 </span></a>above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l12529"><span class="ln">12529 </span></a>``torch.bartlett_window(L, periodic=True)`` equal to 
<a name="l12530"><span class="ln">12530 </span></a>``torch.bartlett_window(L + 1, periodic=False)[:-1])``. 
<a name="l12531"><span class="ln">12531 </span></a> 
<a name="l12532"><span class="ln">12532 </span></a>.. note:: 
<a name="l12533"><span class="ln">12533 </span></a>    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l12534"><span class="ln">12534 </span></a>&quot;&quot;&quot;</span>
<a name="l12535"><span class="ln">12535 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12536"><span class="ln">12536 </span></a>Arguments: 
<a name="l12537"><span class="ln">12537 </span></a>    window_length (int): the size of returned window 
<a name="l12538"><span class="ln">12538 </span></a>    periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l12539"><span class="ln">12539 </span></a>        function. If False, return a symmetric window. 
<a name="l12540"><span class="ln">12540 </span></a> 
<a name="l12541"><span class="ln">12541 </span></a>Keyword args: 
<a name="l12542"><span class="ln">12542 </span></a>    {dtype} Only floating point types are supported. 
<a name="l12543"><span class="ln">12543 </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l12544"><span class="ln">12544 </span></a>          ``torch.strided`` (dense layout) is supported. 
<a name="l12545"><span class="ln">12545 </span></a>    {device} 
<a name="l12546"><span class="ln">12546 </span></a>    {requires_grad} 
<a name="l12547"><span class="ln">12547 </span></a> 
<a name="l12548"><span class="ln">12548 </span></a>Returns: 
<a name="l12549"><span class="ln">12549 </span></a>    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window 
<a name="l12550"><span class="ln">12550 </span></a> 
<a name="l12551"><span class="ln">12551 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12552"><span class="ln">12552 </span></a><span class="s3">)</span>
<a name="l12553"><span class="ln">12553 </span></a>
<a name="l12554"><span class="ln">12554 </span></a>
<a name="l12555"><span class="ln">12555 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12556"><span class="ln">12556 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">blackman_window</span><span class="s3">,</span>
<a name="l12557"><span class="ln">12557 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12558"><span class="ln">12558 </span></a>blackman_window(window_length, periodic=True, *, dtype=None, \ 
<a name="l12559"><span class="ln">12559 </span></a>layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12560"><span class="ln">12560 </span></a>&quot;&quot;&quot;</span>
<a name="l12561"><span class="ln">12561 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12562"><span class="ln">12562 </span></a>Blackman window function. 
<a name="l12563"><span class="ln">12563 </span></a> 
<a name="l12564"><span class="ln">12564 </span></a>.. math:: 
<a name="l12565"><span class="ln">12565 </span></a>    w[n] = 0.42 - 0.5 \cos \left( \frac{2 \pi n}{N - 1} \right) + 0.08 \cos \left( \frac{4 \pi n}{N - 1} \right) 
<a name="l12566"><span class="ln">12566 </span></a> 
<a name="l12567"><span class="ln">12567 </span></a>where :math:`N` is the full window size. 
<a name="l12568"><span class="ln">12568 </span></a> 
<a name="l12569"><span class="ln">12569 </span></a>The input :attr:`window_length` is a positive integer controlling the 
<a name="l12570"><span class="ln">12570 </span></a>returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l12571"><span class="ln">12571 </span></a>window trims off the last duplicate value from the symmetric window and is 
<a name="l12572"><span class="ln">12572 </span></a>ready to be used as a periodic window with functions like 
<a name="l12573"><span class="ln">12573 </span></a>:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l12574"><span class="ln">12574 </span></a>above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l12575"><span class="ln">12575 </span></a>``torch.blackman_window(L, periodic=True)`` equal to 
<a name="l12576"><span class="ln">12576 </span></a>``torch.blackman_window(L + 1, periodic=False)[:-1]``. 
<a name="l12577"><span class="ln">12577 </span></a> 
<a name="l12578"><span class="ln">12578 </span></a>.. note:: 
<a name="l12579"><span class="ln">12579 </span></a>    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l12580"><span class="ln">12580 </span></a>&quot;&quot;&quot;</span>
<a name="l12581"><span class="ln">12581 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12582"><span class="ln">12582 </span></a>Arguments: 
<a name="l12583"><span class="ln">12583 </span></a>    window_length (int): the size of returned window 
<a name="l12584"><span class="ln">12584 </span></a>    periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l12585"><span class="ln">12585 </span></a>        function. If False, return a symmetric window. 
<a name="l12586"><span class="ln">12586 </span></a> 
<a name="l12587"><span class="ln">12587 </span></a>Keyword args: 
<a name="l12588"><span class="ln">12588 </span></a>    {dtype} Only floating point types are supported. 
<a name="l12589"><span class="ln">12589 </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l12590"><span class="ln">12590 </span></a>          ``torch.strided`` (dense layout) is supported. 
<a name="l12591"><span class="ln">12591 </span></a>    {device} 
<a name="l12592"><span class="ln">12592 </span></a>    {requires_grad} 
<a name="l12593"><span class="ln">12593 </span></a> 
<a name="l12594"><span class="ln">12594 </span></a>Returns: 
<a name="l12595"><span class="ln">12595 </span></a>    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window 
<a name="l12596"><span class="ln">12596 </span></a> 
<a name="l12597"><span class="ln">12597 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12598"><span class="ln">12598 </span></a><span class="s3">)</span>
<a name="l12599"><span class="ln">12599 </span></a>
<a name="l12600"><span class="ln">12600 </span></a>
<a name="l12601"><span class="ln">12601 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12602"><span class="ln">12602 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">kaiser_window</span><span class="s3">,</span>
<a name="l12603"><span class="ln">12603 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12604"><span class="ln">12604 </span></a>kaiser_window(window_length, periodic=True, beta=12.0, *, dtype=None, \ 
<a name="l12605"><span class="ln">12605 </span></a>layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12606"><span class="ln">12606 </span></a>&quot;&quot;&quot;</span>
<a name="l12607"><span class="ln">12607 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12608"><span class="ln">12608 </span></a>Computes the Kaiser window with window length :attr:`window_length` and shape parameter :attr:`beta`. 
<a name="l12609"><span class="ln">12609 </span></a> 
<a name="l12610"><span class="ln">12610 </span></a>Let I_0 be the zeroth order modified Bessel function of the first kind (see :func:`torch.i0`) and 
<a name="l12611"><span class="ln">12611 </span></a>``N = L - 1`` if :attr:`periodic` is False and ``L`` if :attr:`periodic` is True, 
<a name="l12612"><span class="ln">12612 </span></a>where ``L`` is the :attr:`window_length`. This function computes: 
<a name="l12613"><span class="ln">12613 </span></a> 
<a name="l12614"><span class="ln">12614 </span></a>.. math:: 
<a name="l12615"><span class="ln">12615 </span></a>    out_i = I_0 \left( \beta \sqrt{1 - \left( {\frac{i - N/2}{N/2}} \right) ^2 } \right) / I_0( \beta ) 
<a name="l12616"><span class="ln">12616 </span></a> 
<a name="l12617"><span class="ln">12617 </span></a>Calling ``torch.kaiser_window(L, B, periodic=True)`` is equivalent to calling 
<a name="l12618"><span class="ln">12618 </span></a>``torch.kaiser_window(L + 1, B, periodic=False)[:-1])``. 
<a name="l12619"><span class="ln">12619 </span></a>The :attr:`periodic` argument is intended as a helpful shorthand 
<a name="l12620"><span class="ln">12620 </span></a>to produce a periodic window as input to functions like :func:`torch.stft`. 
<a name="l12621"><span class="ln">12621 </span></a> 
<a name="l12622"><span class="ln">12622 </span></a>.. note:: 
<a name="l12623"><span class="ln">12623 </span></a>    If :attr:`window_length` is one, then the returned window is a single element tensor containing a one. 
<a name="l12624"><span class="ln">12624 </span></a> 
<a name="l12625"><span class="ln">12625 </span></a>&quot;&quot;&quot;</span>
<a name="l12626"><span class="ln">12626 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12627"><span class="ln">12627 </span></a>Args: 
<a name="l12628"><span class="ln">12628 </span></a>    window_length (int): length of the window. 
<a name="l12629"><span class="ln">12629 </span></a>    periodic (bool, optional): If True, returns a periodic window suitable for use in spectral analysis. 
<a name="l12630"><span class="ln">12630 </span></a>        If False, returns a symmetric window suitable for use in filter design. 
<a name="l12631"><span class="ln">12631 </span></a>    beta (float, optional): shape parameter for the window. 
<a name="l12632"><span class="ln">12632 </span></a> 
<a name="l12633"><span class="ln">12633 </span></a>Keyword args: 
<a name="l12634"><span class="ln">12634 </span></a>    {dtype} 
<a name="l12635"><span class="ln">12635 </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l12636"><span class="ln">12636 </span></a>          ``torch.strided`` (dense layout) is supported. 
<a name="l12637"><span class="ln">12637 </span></a>    {device} 
<a name="l12638"><span class="ln">12638 </span></a>    {requires_grad} 
<a name="l12639"><span class="ln">12639 </span></a> 
<a name="l12640"><span class="ln">12640 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12641"><span class="ln">12641 </span></a><span class="s3">)</span>
<a name="l12642"><span class="ln">12642 </span></a>
<a name="l12643"><span class="ln">12643 </span></a>
<a name="l12644"><span class="ln">12644 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12645"><span class="ln">12645 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">vander</span><span class="s3">,</span>
<a name="l12646"><span class="ln">12646 </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l12647"><span class="ln">12647 </span></a>vander(x, N=None, increasing=False) -&gt; Tensor 
<a name="l12648"><span class="ln">12648 </span></a>&quot;&quot;&quot;</span>
<a name="l12649"><span class="ln">12649 </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l12650"><span class="ln">12650 </span></a>Generates a Vandermonde matrix. 
<a name="l12651"><span class="ln">12651 </span></a> 
<a name="l12652"><span class="ln">12652 </span></a>The columns of the output matrix are elementwise powers of the input vector :math:`x^{{(N-1)}}, x^{{(N-2)}}, ..., x^0`. 
<a name="l12653"><span class="ln">12653 </span></a>If increasing is True, the order of the columns is reversed :math:`x^0, x^1, ..., x^{{(N-1)}}`. Such a 
<a name="l12654"><span class="ln">12654 </span></a>matrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. 
<a name="l12655"><span class="ln">12655 </span></a> 
<a name="l12656"><span class="ln">12656 </span></a>Arguments: 
<a name="l12657"><span class="ln">12657 </span></a>    x (Tensor): 1-D input tensor. 
<a name="l12658"><span class="ln">12658 </span></a>    N (int, optional): Number of columns in the output. If N is not specified, 
<a name="l12659"><span class="ln">12659 </span></a>        a square array is returned :math:`(N = len(x))`. 
<a name="l12660"><span class="ln">12660 </span></a>    increasing (bool, optional): Order of the powers of the columns. If True, 
<a name="l12661"><span class="ln">12661 </span></a>        the powers increase from left to right, if False (the default) they are reversed. 
<a name="l12662"><span class="ln">12662 </span></a> 
<a name="l12663"><span class="ln">12663 </span></a>Returns: 
<a name="l12664"><span class="ln">12664 </span></a>    Tensor: Vandermonde matrix. If increasing is False, the first column is :math:`x^{{(N-1)}}`, 
<a name="l12665"><span class="ln">12665 </span></a>    the second :math:`x^{{(N-2)}}` and so forth. If increasing is True, the columns 
<a name="l12666"><span class="ln">12666 </span></a>    are :math:`x^0, x^1, ..., x^{{(N-1)}}`. 
<a name="l12667"><span class="ln">12667 </span></a> 
<a name="l12668"><span class="ln">12668 </span></a>Example:: 
<a name="l12669"><span class="ln">12669 </span></a> 
<a name="l12670"><span class="ln">12670 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 5]) 
<a name="l12671"><span class="ln">12671 </span></a>    &gt;&gt;&gt; torch.vander(x) 
<a name="l12672"><span class="ln">12672 </span></a>    tensor([[  1,   1,   1,   1], 
<a name="l12673"><span class="ln">12673 </span></a>            [  8,   4,   2,   1], 
<a name="l12674"><span class="ln">12674 </span></a>            [ 27,   9,   3,   1], 
<a name="l12675"><span class="ln">12675 </span></a>            [125,  25,   5,   1]]) 
<a name="l12676"><span class="ln">12676 </span></a>    &gt;&gt;&gt; torch.vander(x, N=3) 
<a name="l12677"><span class="ln">12677 </span></a>    tensor([[ 1,  1,  1], 
<a name="l12678"><span class="ln">12678 </span></a>            [ 4,  2,  1], 
<a name="l12679"><span class="ln">12679 </span></a>            [ 9,  3,  1], 
<a name="l12680"><span class="ln">12680 </span></a>            [25,  5,  1]]) 
<a name="l12681"><span class="ln">12681 </span></a>    &gt;&gt;&gt; torch.vander(x, N=3, increasing=True) 
<a name="l12682"><span class="ln">12682 </span></a>    tensor([[ 1,  1,  1], 
<a name="l12683"><span class="ln">12683 </span></a>            [ 1,  2,  4], 
<a name="l12684"><span class="ln">12684 </span></a>            [ 1,  3,  9], 
<a name="l12685"><span class="ln">12685 </span></a>            [ 1,  5, 25]]) 
<a name="l12686"><span class="ln">12686 </span></a> 
<a name="l12687"><span class="ln">12687 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">factory_common_args</span><span class="s3">),</span>
<a name="l12688"><span class="ln">12688 </span></a><span class="s3">)</span>
<a name="l12689"><span class="ln">12689 </span></a>
<a name="l12690"><span class="ln">12690 </span></a>
<a name="l12691"><span class="ln">12691 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12692"><span class="ln">12692 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unbind</span><span class="s3">,</span>
<a name="l12693"><span class="ln">12693 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12694"><span class="ln">12694 </span></a>unbind(input, dim=0) -&gt; seq 
<a name="l12695"><span class="ln">12695 </span></a> 
<a name="l12696"><span class="ln">12696 </span></a>Removes a tensor dimension. 
<a name="l12697"><span class="ln">12697 </span></a> 
<a name="l12698"><span class="ln">12698 </span></a>Returns a tuple of all slices along a given dimension, already without it. 
<a name="l12699"><span class="ln">12699 </span></a> 
<a name="l12700"><span class="ln">12700 </span></a>Arguments: 
<a name="l12701"><span class="ln">12701 </span></a>    input (Tensor): the tensor to unbind 
<a name="l12702"><span class="ln">12702 </span></a>    dim (int): dimension to remove 
<a name="l12703"><span class="ln">12703 </span></a> 
<a name="l12704"><span class="ln">12704 </span></a>Example:: 
<a name="l12705"><span class="ln">12705 </span></a> 
<a name="l12706"><span class="ln">12706 </span></a>    &gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3], 
<a name="l12707"><span class="ln">12707 </span></a>    &gt;&gt;&gt;                            [4, 5, 6], 
<a name="l12708"><span class="ln">12708 </span></a>    &gt;&gt;&gt;                            [7, 8, 9]])) 
<a name="l12709"><span class="ln">12709 </span></a>    (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9])) 
<a name="l12710"><span class="ln">12710 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12711"><span class="ln">12711 </span></a><span class="s3">)</span>
<a name="l12712"><span class="ln">12712 </span></a>
<a name="l12713"><span class="ln">12713 </span></a>
<a name="l12714"><span class="ln">12714 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12715"><span class="ln">12715 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">combinations</span><span class="s3">,</span>
<a name="l12716"><span class="ln">12716 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12717"><span class="ln">12717 </span></a>combinations(input: Tensor, r: int = 2, with_replacement: bool = False) -&gt; seq 
<a name="l12718"><span class="ln">12718 </span></a> 
<a name="l12719"><span class="ln">12719 </span></a>Compute combinations of length :math:`r` of the given tensor. The behavior is similar to 
<a name="l12720"><span class="ln">12720 </span></a>python's `itertools.combinations` when `with_replacement` is set to `False`, and 
<a name="l12721"><span class="ln">12721 </span></a>`itertools.combinations_with_replacement` when `with_replacement` is set to `True`. 
<a name="l12722"><span class="ln">12722 </span></a> 
<a name="l12723"><span class="ln">12723 </span></a>Arguments: 
<a name="l12724"><span class="ln">12724 </span></a>    input (Tensor): 1D vector. 
<a name="l12725"><span class="ln">12725 </span></a>    r (int, optional): number of elements to combine 
<a name="l12726"><span class="ln">12726 </span></a>    with_replacement (bool, optional): whether to allow duplication in combination 
<a name="l12727"><span class="ln">12727 </span></a> 
<a name="l12728"><span class="ln">12728 </span></a>Returns: 
<a name="l12729"><span class="ln">12729 </span></a>    Tensor: A tensor equivalent to converting all the input tensors into lists, do 
<a name="l12730"><span class="ln">12730 </span></a>    `itertools.combinations` or `itertools.combinations_with_replacement` on these 
<a name="l12731"><span class="ln">12731 </span></a>    lists, and finally convert the resulting list into tensor. 
<a name="l12732"><span class="ln">12732 </span></a> 
<a name="l12733"><span class="ln">12733 </span></a>Example:: 
<a name="l12734"><span class="ln">12734 </span></a> 
<a name="l12735"><span class="ln">12735 </span></a>    &gt;&gt;&gt; a = [1, 2, 3] 
<a name="l12736"><span class="ln">12736 </span></a>    &gt;&gt;&gt; list(itertools.combinations(a, r=2)) 
<a name="l12737"><span class="ln">12737 </span></a>    [(1, 2), (1, 3), (2, 3)] 
<a name="l12738"><span class="ln">12738 </span></a>    &gt;&gt;&gt; list(itertools.combinations(a, r=3)) 
<a name="l12739"><span class="ln">12739 </span></a>    [(1, 2, 3)] 
<a name="l12740"><span class="ln">12740 </span></a>    &gt;&gt;&gt; list(itertools.combinations_with_replacement(a, r=2)) 
<a name="l12741"><span class="ln">12741 </span></a>    [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)] 
<a name="l12742"><span class="ln">12742 </span></a>    &gt;&gt;&gt; tensor_a = torch.tensor(a) 
<a name="l12743"><span class="ln">12743 </span></a>    &gt;&gt;&gt; torch.combinations(tensor_a) 
<a name="l12744"><span class="ln">12744 </span></a>    tensor([[1, 2], 
<a name="l12745"><span class="ln">12745 </span></a>            [1, 3], 
<a name="l12746"><span class="ln">12746 </span></a>            [2, 3]]) 
<a name="l12747"><span class="ln">12747 </span></a>    &gt;&gt;&gt; torch.combinations(tensor_a, r=3) 
<a name="l12748"><span class="ln">12748 </span></a>    tensor([[1, 2, 3]]) 
<a name="l12749"><span class="ln">12749 </span></a>    &gt;&gt;&gt; torch.combinations(tensor_a, with_replacement=True) 
<a name="l12750"><span class="ln">12750 </span></a>    tensor([[1, 1], 
<a name="l12751"><span class="ln">12751 </span></a>            [1, 2], 
<a name="l12752"><span class="ln">12752 </span></a>            [1, 3], 
<a name="l12753"><span class="ln">12753 </span></a>            [2, 2], 
<a name="l12754"><span class="ln">12754 </span></a>            [2, 3], 
<a name="l12755"><span class="ln">12755 </span></a>            [3, 3]]) 
<a name="l12756"><span class="ln">12756 </span></a> 
<a name="l12757"><span class="ln">12757 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12758"><span class="ln">12758 </span></a><span class="s3">)</span>
<a name="l12759"><span class="ln">12759 </span></a>
<a name="l12760"><span class="ln">12760 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12761"><span class="ln">12761 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">trapezoid</span><span class="s3">,</span>
<a name="l12762"><span class="ln">12762 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12763"><span class="ln">12763 </span></a>trapezoid(y, x=None, *, dx=None, dim=-1) -&gt; Tensor 
<a name="l12764"><span class="ln">12764 </span></a> 
<a name="l12765"><span class="ln">12765 </span></a>Computes the `trapezoidal rule &lt;https://en.wikipedia.org/wiki/Trapezoidal_rule&gt;`_ along 
<a name="l12766"><span class="ln">12766 </span></a>:attr:`dim`. By default the spacing between elements is assumed to be 1, but 
<a name="l12767"><span class="ln">12767 </span></a>:attr:`dx` can be used to specify a different constant spacing, and :attr:`x` can be 
<a name="l12768"><span class="ln">12768 </span></a>used to specify arbitrary spacing along :attr:`dim`. Only one of :attr:`x` or :attr:`dx` should be specified. 
<a name="l12769"><span class="ln">12769 </span></a> 
<a name="l12770"><span class="ln">12770 </span></a> 
<a name="l12771"><span class="ln">12771 </span></a>Assuming :attr:`y` is a one-dimensional tensor with elements :math:`{y_0, y_1, ..., y_n}`, 
<a name="l12772"><span class="ln">12772 </span></a>the default computation is 
<a name="l12773"><span class="ln">12773 </span></a> 
<a name="l12774"><span class="ln">12774 </span></a>.. math:: 
<a name="l12775"><span class="ln">12775 </span></a>    \begin{aligned} 
<a name="l12776"><span class="ln">12776 </span></a>        \sum_{i = 1}^{n} \frac{1}{2} (y_i + y_{i-1}) 
<a name="l12777"><span class="ln">12777 </span></a>    \end{aligned} 
<a name="l12778"><span class="ln">12778 </span></a> 
<a name="l12779"><span class="ln">12779 </span></a>When :attr:`dx` is specified the computation becomes 
<a name="l12780"><span class="ln">12780 </span></a> 
<a name="l12781"><span class="ln">12781 </span></a>.. math:: 
<a name="l12782"><span class="ln">12782 </span></a>    \begin{aligned} 
<a name="l12783"><span class="ln">12783 </span></a>        \sum_{i = 1}^{n} \frac{\Delta x}{2} (y_i + y_{i-1}) 
<a name="l12784"><span class="ln">12784 </span></a>    \end{aligned} 
<a name="l12785"><span class="ln">12785 </span></a> 
<a name="l12786"><span class="ln">12786 </span></a>effectively multiplying the result by :attr:`dx`. When :attr:`x` is specified, 
<a name="l12787"><span class="ln">12787 </span></a>assuming :attr:`x` is also a one-dimensional tensor with 
<a name="l12788"><span class="ln">12788 </span></a>elements :math:`{x_0, x_1, ..., x_n}`, the computation becomes 
<a name="l12789"><span class="ln">12789 </span></a> 
<a name="l12790"><span class="ln">12790 </span></a>.. math:: 
<a name="l12791"><span class="ln">12791 </span></a>    \begin{aligned} 
<a name="l12792"><span class="ln">12792 </span></a>        \sum_{i = 1}^{n} \frac{(x_i - x_{i-1})}{2} (y_i + y_{i-1}) 
<a name="l12793"><span class="ln">12793 </span></a>    \end{aligned} 
<a name="l12794"><span class="ln">12794 </span></a> 
<a name="l12795"><span class="ln">12795 </span></a>When :attr:`x` and :attr:`y` have the same size, the computation is as described above and no broadcasting is needed. 
<a name="l12796"><span class="ln">12796 </span></a>The broadcasting behavior of this function is as follows when their sizes are different. For both :attr:`x` 
<a name="l12797"><span class="ln">12797 </span></a>and :attr:`y`, the function computes the difference between consecutive elements along 
<a name="l12798"><span class="ln">12798 </span></a>dimension :attr:`dim`. This effectively creates two tensors, `x_diff` and `y_diff`, that have 
<a name="l12799"><span class="ln">12799 </span></a>the same shape as the original tensors except their lengths along the dimension :attr:`dim` is reduced by 1. 
<a name="l12800"><span class="ln">12800 </span></a>After that, those two tensors are broadcast together to compute final output as part of the trapezoidal rule. 
<a name="l12801"><span class="ln">12801 </span></a>See the examples below for details. 
<a name="l12802"><span class="ln">12802 </span></a> 
<a name="l12803"><span class="ln">12803 </span></a>.. note:: 
<a name="l12804"><span class="ln">12804 </span></a>    The trapezoidal rule is a technique for approximating the definite integral of a function 
<a name="l12805"><span class="ln">12805 </span></a>    by averaging its left and right Riemann sums. The approximation becomes more accurate as 
<a name="l12806"><span class="ln">12806 </span></a>    the resolution of the partition increases. 
<a name="l12807"><span class="ln">12807 </span></a> 
<a name="l12808"><span class="ln">12808 </span></a>Arguments: 
<a name="l12809"><span class="ln">12809 </span></a>    y (Tensor): Values to use when computing the trapezoidal rule. 
<a name="l12810"><span class="ln">12810 </span></a>    x (Tensor): If specified, defines spacing between values as specified above. 
<a name="l12811"><span class="ln">12811 </span></a> 
<a name="l12812"><span class="ln">12812 </span></a>Keyword arguments: 
<a name="l12813"><span class="ln">12813 </span></a>    dx (float): constant spacing between values. If neither :attr:`x` or :attr:`dx` 
<a name="l12814"><span class="ln">12814 </span></a>        are specified then this defaults to 1. Effectively multiplies the result by its value. 
<a name="l12815"><span class="ln">12815 </span></a>    dim (int): The dimension along which to compute the trapezoidal rule. 
<a name="l12816"><span class="ln">12816 </span></a>        The last (inner-most) dimension by default. 
<a name="l12817"><span class="ln">12817 </span></a> 
<a name="l12818"><span class="ln">12818 </span></a>Examples:: 
<a name="l12819"><span class="ln">12819 </span></a> 
<a name="l12820"><span class="ln">12820 </span></a>    &gt;&gt;&gt; # Computes the trapezoidal rule in 1D, spacing is implicitly 1 
<a name="l12821"><span class="ln">12821 </span></a>    &gt;&gt;&gt; y = torch.tensor([1, 5, 10]) 
<a name="l12822"><span class="ln">12822 </span></a>    &gt;&gt;&gt; torch.trapezoid(y) 
<a name="l12823"><span class="ln">12823 </span></a>    tensor(10.5) 
<a name="l12824"><span class="ln">12824 </span></a> 
<a name="l12825"><span class="ln">12825 </span></a>    &gt;&gt;&gt; # Computes the same trapezoidal rule directly to verify 
<a name="l12826"><span class="ln">12826 </span></a>    &gt;&gt;&gt; (1 + 10 + 10) / 2 
<a name="l12827"><span class="ln">12827 </span></a>    10.5 
<a name="l12828"><span class="ln">12828 </span></a> 
<a name="l12829"><span class="ln">12829 </span></a>    &gt;&gt;&gt; # Computes the trapezoidal rule in 1D with constant spacing of 2 
<a name="l12830"><span class="ln">12830 </span></a>    &gt;&gt;&gt; # NOTE: the result is the same as before, but multiplied by 2 
<a name="l12831"><span class="ln">12831 </span></a>    &gt;&gt;&gt; torch.trapezoid(y, dx=2) 
<a name="l12832"><span class="ln">12832 </span></a>    21.0 
<a name="l12833"><span class="ln">12833 </span></a> 
<a name="l12834"><span class="ln">12834 </span></a>    &gt;&gt;&gt; # Computes the trapezoidal rule in 1D with arbitrary spacing 
<a name="l12835"><span class="ln">12835 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l12836"><span class="ln">12836 </span></a>    &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l12837"><span class="ln">12837 </span></a>    28.5 
<a name="l12838"><span class="ln">12838 </span></a> 
<a name="l12839"><span class="ln">12839 </span></a>    &gt;&gt;&gt; # Computes the same trapezoidal rule directly to verify 
<a name="l12840"><span class="ln">12840 </span></a>    &gt;&gt;&gt; ((3 - 1) * (1 + 5) + (6 - 3) * (5 + 10)) / 2 
<a name="l12841"><span class="ln">12841 </span></a>    28.5 
<a name="l12842"><span class="ln">12842 </span></a> 
<a name="l12843"><span class="ln">12843 </span></a>    &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 matrix 
<a name="l12844"><span class="ln">12844 </span></a>    &gt;&gt;&gt; y = torch.arange(9).reshape(3, 3) 
<a name="l12845"><span class="ln">12845 </span></a>    tensor([[0, 1, 2], 
<a name="l12846"><span class="ln">12846 </span></a>            [3, 4, 5], 
<a name="l12847"><span class="ln">12847 </span></a>            [6, 7, 8]]) 
<a name="l12848"><span class="ln">12848 </span></a>    &gt;&gt;&gt; torch.trapezoid(y) 
<a name="l12849"><span class="ln">12849 </span></a>    tensor([ 2., 8., 14.]) 
<a name="l12850"><span class="ln">12850 </span></a> 
<a name="l12851"><span class="ln">12851 </span></a>    &gt;&gt;&gt; # Computes the trapezoidal rule for each column of the matrix 
<a name="l12852"><span class="ln">12852 </span></a>    &gt;&gt;&gt; torch.trapezoid(y, dim=0) 
<a name="l12853"><span class="ln">12853 </span></a>    tensor([ 6., 8., 10.]) 
<a name="l12854"><span class="ln">12854 </span></a> 
<a name="l12855"><span class="ln">12855 </span></a>    &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l12856"><span class="ln">12856 </span></a>    &gt;&gt;&gt; #   with the same arbitrary spacing 
<a name="l12857"><span class="ln">12857 </span></a>    &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l12858"><span class="ln">12858 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l12859"><span class="ln">12859 </span></a>    &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l12860"><span class="ln">12860 </span></a>    array([5., 5., 5.]) 
<a name="l12861"><span class="ln">12861 </span></a> 
<a name="l12862"><span class="ln">12862 </span></a>    &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l12863"><span class="ln">12863 </span></a>    &gt;&gt;&gt; #   with different arbitrary spacing per row 
<a name="l12864"><span class="ln">12864 </span></a>    &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l12865"><span class="ln">12865 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [1, 3, 5], [1, 4, 7]]) 
<a name="l12866"><span class="ln">12866 </span></a>    &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l12867"><span class="ln">12867 </span></a>    array([2., 4., 6.]) 
<a name="l12868"><span class="ln">12868 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12869"><span class="ln">12869 </span></a><span class="s3">)</span>
<a name="l12870"><span class="ln">12870 </span></a>
<a name="l12871"><span class="ln">12871 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12872"><span class="ln">12872 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">trapz</span><span class="s3">,</span>
<a name="l12873"><span class="ln">12873 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12874"><span class="ln">12874 </span></a>trapz(y, x, *, dim=-1) -&gt; Tensor 
<a name="l12875"><span class="ln">12875 </span></a> 
<a name="l12876"><span class="ln">12876 </span></a>Alias for :func:`torch.trapezoid`. 
<a name="l12877"><span class="ln">12877 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12878"><span class="ln">12878 </span></a><span class="s3">)</span>
<a name="l12879"><span class="ln">12879 </span></a>
<a name="l12880"><span class="ln">12880 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12881"><span class="ln">12881 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">cumulative_trapezoid</span><span class="s3">,</span>
<a name="l12882"><span class="ln">12882 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12883"><span class="ln">12883 </span></a>cumulative_trapezoid(y, x=None, *, dx=None, dim=-1) -&gt; Tensor 
<a name="l12884"><span class="ln">12884 </span></a> 
<a name="l12885"><span class="ln">12885 </span></a>Cumulatively computes the `trapezoidal rule &lt;https://en.wikipedia.org/wiki/Trapezoidal_rule&gt;`_ 
<a name="l12886"><span class="ln">12886 </span></a>along :attr:`dim`. By default the spacing between elements is assumed to be 1, but 
<a name="l12887"><span class="ln">12887 </span></a>:attr:`dx` can be used to specify a different constant spacing, and :attr:`x` can be 
<a name="l12888"><span class="ln">12888 </span></a>used to specify arbitrary spacing along :attr:`dim`. 
<a name="l12889"><span class="ln">12889 </span></a> 
<a name="l12890"><span class="ln">12890 </span></a>For more details, please read :func:`torch.trapezoid`. The difference between :func:`torch.trapezoid` 
<a name="l12891"><span class="ln">12891 </span></a>and this function is that, :func:`torch.trapezoid` returns a value for each integration, 
<a name="l12892"><span class="ln">12892 </span></a>where as this function returns a cumulative value for every spacing within the integration. This 
<a name="l12893"><span class="ln">12893 </span></a>is analogous to how `.sum` returns a value and `.cumsum` returns a cumulative sum. 
<a name="l12894"><span class="ln">12894 </span></a> 
<a name="l12895"><span class="ln">12895 </span></a>Arguments: 
<a name="l12896"><span class="ln">12896 </span></a>    y (Tensor): Values to use when computing the trapezoidal rule. 
<a name="l12897"><span class="ln">12897 </span></a>    x (Tensor): If specified, defines spacing between values as specified above. 
<a name="l12898"><span class="ln">12898 </span></a> 
<a name="l12899"><span class="ln">12899 </span></a>Keyword arguments: 
<a name="l12900"><span class="ln">12900 </span></a>    dx (float): constant spacing between values. If neither :attr:`x` or :attr:`dx` 
<a name="l12901"><span class="ln">12901 </span></a>        are specified then this defaults to 1. Effectively multiplies the result by its value. 
<a name="l12902"><span class="ln">12902 </span></a>    dim (int): The dimension along which to compute the trapezoidal rule. 
<a name="l12903"><span class="ln">12903 </span></a>        The last (inner-most) dimension by default. 
<a name="l12904"><span class="ln">12904 </span></a> 
<a name="l12905"><span class="ln">12905 </span></a>Examples:: 
<a name="l12906"><span class="ln">12906 </span></a> 
<a name="l12907"><span class="ln">12907 </span></a>    &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D, spacing is implicitly 1. 
<a name="l12908"><span class="ln">12908 </span></a>    &gt;&gt;&gt; y = torch.tensor([1, 5, 10]) 
<a name="l12909"><span class="ln">12909 </span></a>    &gt;&gt;&gt; torch.cumulative_trapezoid(y) 
<a name="l12910"><span class="ln">12910 </span></a>    tensor([3., 10.5]) 
<a name="l12911"><span class="ln">12911 </span></a> 
<a name="l12912"><span class="ln">12912 </span></a>    &gt;&gt;&gt; # Computes the same trapezoidal rule directly up to each element to verify 
<a name="l12913"><span class="ln">12913 </span></a>    &gt;&gt;&gt; (1 + 5) / 2 
<a name="l12914"><span class="ln">12914 </span></a>    3.0 
<a name="l12915"><span class="ln">12915 </span></a>    &gt;&gt;&gt; (1 + 10 + 10) / 2 
<a name="l12916"><span class="ln">12916 </span></a>    10.5 
<a name="l12917"><span class="ln">12917 </span></a> 
<a name="l12918"><span class="ln">12918 </span></a>    &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D with constant spacing of 2 
<a name="l12919"><span class="ln">12919 </span></a>    &gt;&gt;&gt; # NOTE: the result is the same as before, but multiplied by 2 
<a name="l12920"><span class="ln">12920 </span></a>    &gt;&gt;&gt; torch.cumulative_trapezoid(y, dx=2) 
<a name="l12921"><span class="ln">12921 </span></a>    tensor([6., 21.]) 
<a name="l12922"><span class="ln">12922 </span></a> 
<a name="l12923"><span class="ln">12923 </span></a>    &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D with arbitrary spacing 
<a name="l12924"><span class="ln">12924 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l12925"><span class="ln">12925 </span></a>    &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l12926"><span class="ln">12926 </span></a>    tensor([6., 28.5]) 
<a name="l12927"><span class="ln">12927 </span></a> 
<a name="l12928"><span class="ln">12928 </span></a>    &gt;&gt;&gt; # Computes the same trapezoidal rule directly up to each element to verify 
<a name="l12929"><span class="ln">12929 </span></a>    &gt;&gt;&gt; ((3 - 1) * (1 + 5)) / 2 
<a name="l12930"><span class="ln">12930 </span></a>    6.0 
<a name="l12931"><span class="ln">12931 </span></a>    &gt;&gt;&gt; ((3 - 1) * (1 + 5) + (6 - 3) * (5 + 10)) / 2 
<a name="l12932"><span class="ln">12932 </span></a>    28.5 
<a name="l12933"><span class="ln">12933 </span></a> 
<a name="l12934"><span class="ln">12934 </span></a>    &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 matrix 
<a name="l12935"><span class="ln">12935 </span></a>    &gt;&gt;&gt; y = torch.arange(9).reshape(3, 3) 
<a name="l12936"><span class="ln">12936 </span></a>    tensor([[0, 1, 2], 
<a name="l12937"><span class="ln">12937 </span></a>            [3, 4, 5], 
<a name="l12938"><span class="ln">12938 </span></a>            [6, 7, 8]]) 
<a name="l12939"><span class="ln">12939 </span></a>    &gt;&gt;&gt; torch.cumulative_trapezoid(y) 
<a name="l12940"><span class="ln">12940 </span></a>    tensor([[ 0.5,  2.], 
<a name="l12941"><span class="ln">12941 </span></a>            [ 3.5,  8.], 
<a name="l12942"><span class="ln">12942 </span></a>            [ 6.5, 14.]]) 
<a name="l12943"><span class="ln">12943 </span></a> 
<a name="l12944"><span class="ln">12944 </span></a>    &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each column of the matrix 
<a name="l12945"><span class="ln">12945 </span></a>    &gt;&gt;&gt; torch.cumulative_trapezoid(y, dim=0) 
<a name="l12946"><span class="ln">12946 </span></a>    tensor([[ 1.5,  2.5,  3.5], 
<a name="l12947"><span class="ln">12947 </span></a>            [ 6.0,  8.0, 10.0]]) 
<a name="l12948"><span class="ln">12948 </span></a> 
<a name="l12949"><span class="ln">12949 </span></a>    &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l12950"><span class="ln">12950 </span></a>    &gt;&gt;&gt; #   with the same arbitrary spacing 
<a name="l12951"><span class="ln">12951 </span></a>    &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l12952"><span class="ln">12952 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l12953"><span class="ln">12953 </span></a>    &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l12954"><span class="ln">12954 </span></a>    tensor([[2., 5.], 
<a name="l12955"><span class="ln">12955 </span></a>            [2., 5.], 
<a name="l12956"><span class="ln">12956 </span></a>            [2., 5.]]) 
<a name="l12957"><span class="ln">12957 </span></a> 
<a name="l12958"><span class="ln">12958 </span></a>    &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l12959"><span class="ln">12959 </span></a>    &gt;&gt;&gt; #   with different arbitrary spacing per row 
<a name="l12960"><span class="ln">12960 </span></a>    &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l12961"><span class="ln">12961 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [1, 3, 5], [1, 4, 7]]) 
<a name="l12962"><span class="ln">12962 </span></a>    &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l12963"><span class="ln">12963 </span></a>    tensor([[1., 2.], 
<a name="l12964"><span class="ln">12964 </span></a>            [2., 4.], 
<a name="l12965"><span class="ln">12965 </span></a>            [3., 6.]]) 
<a name="l12966"><span class="ln">12966 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l12967"><span class="ln">12967 </span></a><span class="s3">)</span>
<a name="l12968"><span class="ln">12968 </span></a>
<a name="l12969"><span class="ln">12969 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l12970"><span class="ln">12970 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">repeat_interleave</span><span class="s3">,</span>
<a name="l12971"><span class="ln">12971 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l12972"><span class="ln">12972 </span></a>repeat_interleave(input, repeats, dim=None, *, output_size=None) -&gt; Tensor 
<a name="l12973"><span class="ln">12973 </span></a> 
<a name="l12974"><span class="ln">12974 </span></a>Repeat elements of a tensor. 
<a name="l12975"><span class="ln">12975 </span></a> 
<a name="l12976"><span class="ln">12976 </span></a>.. warning:: 
<a name="l12977"><span class="ln">12977 </span></a> 
<a name="l12978"><span class="ln">12978 </span></a>    This is different from :meth:`torch.Tensor.repeat` but similar to ``numpy.repeat``. 
<a name="l12979"><span class="ln">12979 </span></a> 
<a name="l12980"><span class="ln">12980 </span></a>Args: 
<a name="l12981"><span class="ln">12981 </span></a>    {input} 
<a name="l12982"><span class="ln">12982 </span></a>    repeats (Tensor or int): The number of repetitions for each element. 
<a name="l12983"><span class="ln">12983 </span></a>        repeats is broadcasted to fit the shape of the given axis. 
<a name="l12984"><span class="ln">12984 </span></a>    dim (int, optional): The dimension along which to repeat values. 
<a name="l12985"><span class="ln">12985 </span></a>        By default, use the flattened input array, and return a flat output 
<a name="l12986"><span class="ln">12986 </span></a>        array. 
<a name="l12987"><span class="ln">12987 </span></a> 
<a name="l12988"><span class="ln">12988 </span></a>Keyword args: 
<a name="l12989"><span class="ln">12989 </span></a>    output_size (int, optional): Total output size for the given axis 
<a name="l12990"><span class="ln">12990 </span></a>        ( e.g. sum of repeats). If given, it will avoid stream synchronization 
<a name="l12991"><span class="ln">12991 </span></a>        needed to calculate output shape of the tensor. 
<a name="l12992"><span class="ln">12992 </span></a> 
<a name="l12993"><span class="ln">12993 </span></a>Returns: 
<a name="l12994"><span class="ln">12994 </span></a>    Tensor: Repeated tensor which has the same shape as input, except along the given axis. 
<a name="l12995"><span class="ln">12995 </span></a> 
<a name="l12996"><span class="ln">12996 </span></a>Example:: 
<a name="l12997"><span class="ln">12997 </span></a> 
<a name="l12998"><span class="ln">12998 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l12999"><span class="ln">12999 </span></a>    &gt;&gt;&gt; x.repeat_interleave(2) 
<a name="l13000"><span class="ln">13000 </span></a>    tensor([1, 1, 2, 2, 3, 3]) 
<a name="l13001"><span class="ln">13001 </span></a>    &gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]]) 
<a name="l13002"><span class="ln">13002 </span></a>    &gt;&gt;&gt; torch.repeat_interleave(y, 2) 
<a name="l13003"><span class="ln">13003 </span></a>    tensor([1, 1, 2, 2, 3, 3, 4, 4]) 
<a name="l13004"><span class="ln">13004 </span></a>    &gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1) 
<a name="l13005"><span class="ln">13005 </span></a>    tensor([[1, 1, 1, 2, 2, 2], 
<a name="l13006"><span class="ln">13006 </span></a>            [3, 3, 3, 4, 4, 4]]) 
<a name="l13007"><span class="ln">13007 </span></a>    &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0) 
<a name="l13008"><span class="ln">13008 </span></a>    tensor([[1, 2], 
<a name="l13009"><span class="ln">13009 </span></a>            [3, 4], 
<a name="l13010"><span class="ln">13010 </span></a>            [3, 4]]) 
<a name="l13011"><span class="ln">13011 </span></a>    &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0, output_size=3) 
<a name="l13012"><span class="ln">13012 </span></a>    tensor([[1, 2], 
<a name="l13013"><span class="ln">13013 </span></a>            [3, 4], 
<a name="l13014"><span class="ln">13014 </span></a>            [3, 4]]) 
<a name="l13015"><span class="ln">13015 </span></a> 
<a name="l13016"><span class="ln">13016 </span></a>If the `repeats` is `tensor([n1, n2, n3, ...])`, then the output will be 
<a name="l13017"><span class="ln">13017 </span></a>`tensor([0, 0, ..., 1, 1, ..., 2, 2, ..., ...])` where `0` appears `n1` times, 
<a name="l13018"><span class="ln">13018 </span></a>`1` appears `n2` times, `2` appears `n3` times, etc. 
<a name="l13019"><span class="ln">13019 </span></a> 
<a name="l13020"><span class="ln">13020 </span></a>.. function:: repeat_interleave(repeats, *) -&gt; Tensor 
<a name="l13021"><span class="ln">13021 </span></a>   :noindex: 
<a name="l13022"><span class="ln">13022 </span></a> 
<a name="l13023"><span class="ln">13023 </span></a>Repeats 0 repeats[0] times, 1 repeats[1] times, 2 repeats[2] times, etc. 
<a name="l13024"><span class="ln">13024 </span></a> 
<a name="l13025"><span class="ln">13025 </span></a>Args: 
<a name="l13026"><span class="ln">13026 </span></a>    repeats (Tensor): The number of repetitions for each element. 
<a name="l13027"><span class="ln">13027 </span></a> 
<a name="l13028"><span class="ln">13028 </span></a>Returns: 
<a name="l13029"><span class="ln">13029 </span></a>    Tensor: Repeated tensor of size `sum(repeats)`. 
<a name="l13030"><span class="ln">13030 </span></a> 
<a name="l13031"><span class="ln">13031 </span></a>Example:: 
<a name="l13032"><span class="ln">13032 </span></a> 
<a name="l13033"><span class="ln">13033 </span></a>    &gt;&gt;&gt; torch.repeat_interleave(torch.tensor([1, 2, 3])) 
<a name="l13034"><span class="ln">13034 </span></a>    tensor([0, 1, 1, 2, 2, 2]) 
<a name="l13035"><span class="ln">13035 </span></a> 
<a name="l13036"><span class="ln">13036 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l13037"><span class="ln">13037 </span></a><span class="s3">)</span>
<a name="l13038"><span class="ln">13038 </span></a>
<a name="l13039"><span class="ln">13039 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13040"><span class="ln">13040 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">tile</span><span class="s3">,</span>
<a name="l13041"><span class="ln">13041 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13042"><span class="ln">13042 </span></a>tile(input, dims) -&gt; Tensor 
<a name="l13043"><span class="ln">13043 </span></a> 
<a name="l13044"><span class="ln">13044 </span></a>Constructs a tensor by repeating the elements of :attr:`input`. 
<a name="l13045"><span class="ln">13045 </span></a>The :attr:`dims` argument specifies the number of repetitions 
<a name="l13046"><span class="ln">13046 </span></a>in each dimension. 
<a name="l13047"><span class="ln">13047 </span></a> 
<a name="l13048"><span class="ln">13048 </span></a>If :attr:`dims` specifies fewer dimensions than :attr:`input` has, then 
<a name="l13049"><span class="ln">13049 </span></a>ones are prepended to :attr:`dims` until all dimensions are specified. 
<a name="l13050"><span class="ln">13050 </span></a>For example, if :attr:`input` has shape (8, 6, 4, 2) and :attr:`dims` 
<a name="l13051"><span class="ln">13051 </span></a>is (2, 2), then :attr:`dims` is treated as (1, 1, 2, 2). 
<a name="l13052"><span class="ln">13052 </span></a> 
<a name="l13053"><span class="ln">13053 </span></a>Analogously, if :attr:`input` has fewer dimensions than :attr:`dims` 
<a name="l13054"><span class="ln">13054 </span></a>specifies, then :attr:`input` is treated as if it were unsqueezed at 
<a name="l13055"><span class="ln">13055 </span></a>dimension zero until it has as many dimensions as :attr:`dims` specifies. 
<a name="l13056"><span class="ln">13056 </span></a>For example, if :attr:`input` has shape (4, 2) and :attr:`dims` 
<a name="l13057"><span class="ln">13057 </span></a>is (3, 3, 2, 2), then :attr:`input` is treated as if it had the 
<a name="l13058"><span class="ln">13058 </span></a>shape (1, 1, 4, 2). 
<a name="l13059"><span class="ln">13059 </span></a> 
<a name="l13060"><span class="ln">13060 </span></a>.. note:: 
<a name="l13061"><span class="ln">13061 </span></a> 
<a name="l13062"><span class="ln">13062 </span></a>    This function is similar to NumPy's tile function. 
<a name="l13063"><span class="ln">13063 </span></a> 
<a name="l13064"><span class="ln">13064 </span></a>Args: 
<a name="l13065"><span class="ln">13065 </span></a>    input (Tensor): the tensor whose elements to repeat. 
<a name="l13066"><span class="ln">13066 </span></a>    dims (tuple): the number of repetitions per dimension. 
<a name="l13067"><span class="ln">13067 </span></a> 
<a name="l13068"><span class="ln">13068 </span></a>Example:: 
<a name="l13069"><span class="ln">13069 </span></a> 
<a name="l13070"><span class="ln">13070 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l13071"><span class="ln">13071 </span></a>    &gt;&gt;&gt; x.tile((2,)) 
<a name="l13072"><span class="ln">13072 </span></a>    tensor([1, 2, 3, 1, 2, 3]) 
<a name="l13073"><span class="ln">13073 </span></a>    &gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]]) 
<a name="l13074"><span class="ln">13074 </span></a>    &gt;&gt;&gt; torch.tile(y, (2, 2)) 
<a name="l13075"><span class="ln">13075 </span></a>    tensor([[1, 2, 1, 2], 
<a name="l13076"><span class="ln">13076 </span></a>            [3, 4, 3, 4], 
<a name="l13077"><span class="ln">13077 </span></a>            [1, 2, 1, 2], 
<a name="l13078"><span class="ln">13078 </span></a>            [3, 4, 3, 4]]) 
<a name="l13079"><span class="ln">13079 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13080"><span class="ln">13080 </span></a><span class="s3">)</span>
<a name="l13081"><span class="ln">13081 </span></a>
<a name="l13082"><span class="ln">13082 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13083"><span class="ln">13083 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">quantize_per_tensor</span><span class="s3">,</span>
<a name="l13084"><span class="ln">13084 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13085"><span class="ln">13085 </span></a>quantize_per_tensor(input, scale, zero_point, dtype) -&gt; Tensor 
<a name="l13086"><span class="ln">13086 </span></a> 
<a name="l13087"><span class="ln">13087 </span></a>Converts a float tensor to a quantized tensor with given scale and zero point. 
<a name="l13088"><span class="ln">13088 </span></a> 
<a name="l13089"><span class="ln">13089 </span></a>Arguments: 
<a name="l13090"><span class="ln">13090 </span></a>    input (Tensor): float tensor or list of tensors to quantize 
<a name="l13091"><span class="ln">13091 </span></a>    scale (float or Tensor): scale to apply in quantization formula 
<a name="l13092"><span class="ln">13092 </span></a>    zero_point (int or Tensor): offset in integer value that maps to float zero 
<a name="l13093"><span class="ln">13093 </span></a>    dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l13094"><span class="ln">13094 </span></a>        Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32`` 
<a name="l13095"><span class="ln">13095 </span></a> 
<a name="l13096"><span class="ln">13096 </span></a>Returns: 
<a name="l13097"><span class="ln">13097 </span></a>    Tensor: A newly quantized tensor or list of quantized tensors. 
<a name="l13098"><span class="ln">13098 </span></a> 
<a name="l13099"><span class="ln">13099 </span></a>Example:: 
<a name="l13100"><span class="ln">13100 </span></a> 
<a name="l13101"><span class="ln">13101 </span></a>    &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8) 
<a name="l13102"><span class="ln">13102 </span></a>    tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l13103"><span class="ln">13103 </span></a>           quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10) 
<a name="l13104"><span class="ln">13104 </span></a>    &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr() 
<a name="l13105"><span class="ln">13105 </span></a>    tensor([ 0, 10, 20, 30], dtype=torch.uint8) 
<a name="l13106"><span class="ln">13106 </span></a>    &gt;&gt;&gt; torch.quantize_per_tensor([torch.tensor([-1.0, 0.0]), torch.tensor([-2.0, 2.0])], 
<a name="l13107"><span class="ln">13107 </span></a>    &gt;&gt;&gt; torch.tensor([0.1, 0.2]), torch.tensor([10, 20]), torch.quint8) 
<a name="l13108"><span class="ln">13108 </span></a>    (tensor([-1.,  0.], size=(2,), dtype=torch.quint8, 
<a name="l13109"><span class="ln">13109 </span></a>        quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10), 
<a name="l13110"><span class="ln">13110 </span></a>        tensor([-2.,  2.], size=(2,), dtype=torch.quint8, 
<a name="l13111"><span class="ln">13111 </span></a>        quantization_scheme=torch.per_tensor_affine, scale=0.2, zero_point=20)) 
<a name="l13112"><span class="ln">13112 </span></a>    &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8) 
<a name="l13113"><span class="ln">13113 </span></a>    tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l13114"><span class="ln">13114 </span></a>       quantization_scheme=torch.per_tensor_affine, scale=0.10, zero_point=10) 
<a name="l13115"><span class="ln">13115 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13116"><span class="ln">13116 </span></a><span class="s3">)</span>
<a name="l13117"><span class="ln">13117 </span></a>
<a name="l13118"><span class="ln">13118 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13119"><span class="ln">13119 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">quantize_per_tensor_dynamic</span><span class="s3">,</span>
<a name="l13120"><span class="ln">13120 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13121"><span class="ln">13121 </span></a>quantize_per_tensor_dynamic(input, dtype, reduce_range) -&gt; Tensor 
<a name="l13122"><span class="ln">13122 </span></a> 
<a name="l13123"><span class="ln">13123 </span></a>Converts a float tensor to a quantized tensor with scale and zero_point calculated 
<a name="l13124"><span class="ln">13124 </span></a>dynamically based on the input. 
<a name="l13125"><span class="ln">13125 </span></a> 
<a name="l13126"><span class="ln">13126 </span></a>Arguments: 
<a name="l13127"><span class="ln">13127 </span></a>    input (Tensor): float tensor or list of tensors to quantize 
<a name="l13128"><span class="ln">13128 </span></a>    dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l13129"><span class="ln">13129 </span></a>        Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8`` 
<a name="l13130"><span class="ln">13130 </span></a>    reduce_range (bool): a flag to indicate whether to reduce the range of quantized 
<a name="l13131"><span class="ln">13131 </span></a>    data by 1 bit, it's required to avoid instruction overflow for some hardwares 
<a name="l13132"><span class="ln">13132 </span></a> 
<a name="l13133"><span class="ln">13133 </span></a>Returns: 
<a name="l13134"><span class="ln">13134 </span></a>    Tensor: A newly (dynamically) quantized tensor 
<a name="l13135"><span class="ln">13135 </span></a> 
<a name="l13136"><span class="ln">13136 </span></a>Example:: 
<a name="l13137"><span class="ln">13137 </span></a> 
<a name="l13138"><span class="ln">13138 </span></a>    &gt;&gt;&gt; t = torch.quantize_per_tensor_dynamic(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.quint8, False) 
<a name="l13139"><span class="ln">13139 </span></a>    &gt;&gt;&gt; print(t) 
<a name="l13140"><span class="ln">13140 </span></a>    tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l13141"><span class="ln">13141 </span></a>           quantization_scheme=torch.per_tensor_affine, scale=0.011764705882352941, 
<a name="l13142"><span class="ln">13142 </span></a>           zero_point=85) 
<a name="l13143"><span class="ln">13143 </span></a>    &gt;&gt;&gt; t.int_repr() 
<a name="l13144"><span class="ln">13144 </span></a>    tensor([  0,  85, 170, 255], dtype=torch.uint8) 
<a name="l13145"><span class="ln">13145 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13146"><span class="ln">13146 </span></a><span class="s3">)</span>
<a name="l13147"><span class="ln">13147 </span></a>
<a name="l13148"><span class="ln">13148 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13149"><span class="ln">13149 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">quantize_per_channel</span><span class="s3">,</span>
<a name="l13150"><span class="ln">13150 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13151"><span class="ln">13151 </span></a>quantize_per_channel(input, scales, zero_points, axis, dtype) -&gt; Tensor 
<a name="l13152"><span class="ln">13152 </span></a> 
<a name="l13153"><span class="ln">13153 </span></a>Converts a float tensor to a per-channel quantized tensor with given scales and zero points. 
<a name="l13154"><span class="ln">13154 </span></a> 
<a name="l13155"><span class="ln">13155 </span></a>Arguments: 
<a name="l13156"><span class="ln">13156 </span></a>    input (Tensor): float tensor to quantize 
<a name="l13157"><span class="ln">13157 </span></a>    scales (Tensor): float 1D tensor of scales to use, size should match ``input.size(axis)`` 
<a name="l13158"><span class="ln">13158 </span></a>    zero_points (int): integer 1D tensor of offset to use, size should match ``input.size(axis)`` 
<a name="l13159"><span class="ln">13159 </span></a>    axis (int): dimension on which apply per-channel quantization 
<a name="l13160"><span class="ln">13160 </span></a>    dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l13161"><span class="ln">13161 </span></a>        Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32`` 
<a name="l13162"><span class="ln">13162 </span></a> 
<a name="l13163"><span class="ln">13163 </span></a>Returns: 
<a name="l13164"><span class="ln">13164 </span></a>    Tensor: A newly quantized tensor 
<a name="l13165"><span class="ln">13165 </span></a> 
<a name="l13166"><span class="ln">13166 </span></a>Example:: 
<a name="l13167"><span class="ln">13167 </span></a> 
<a name="l13168"><span class="ln">13168 </span></a>    &gt;&gt;&gt; x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]]) 
<a name="l13169"><span class="ln">13169 </span></a>    &gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8) 
<a name="l13170"><span class="ln">13170 </span></a>    tensor([[-1.,  0.], 
<a name="l13171"><span class="ln">13171 </span></a>            [ 1.,  2.]], size=(2, 2), dtype=torch.quint8, 
<a name="l13172"><span class="ln">13172 </span></a>           quantization_scheme=torch.per_channel_affine, 
<a name="l13173"><span class="ln">13173 </span></a>           scale=tensor([0.1000, 0.0100], dtype=torch.float64), 
<a name="l13174"><span class="ln">13174 </span></a>           zero_point=tensor([10,  0]), axis=0) 
<a name="l13175"><span class="ln">13175 </span></a>    &gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr() 
<a name="l13176"><span class="ln">13176 </span></a>    tensor([[  0,  10], 
<a name="l13177"><span class="ln">13177 </span></a>            [100, 200]], dtype=torch.uint8) 
<a name="l13178"><span class="ln">13178 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13179"><span class="ln">13179 </span></a><span class="s3">)</span>
<a name="l13180"><span class="ln">13180 </span></a>
<a name="l13181"><span class="ln">13181 </span></a>
<a name="l13182"><span class="ln">13182 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13183"><span class="ln">13183 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">quantized_batch_norm</span><span class="s3">,</span>
<a name="l13184"><span class="ln">13184 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13185"><span class="ln">13185 </span></a>quantized_batch_norm(input, weight=None, bias=None, mean, var, eps, output_scale, output_zero_point) -&gt; Tensor 
<a name="l13186"><span class="ln">13186 </span></a> 
<a name="l13187"><span class="ln">13187 </span></a>Applies batch normalization on a 4D (NCHW) quantized tensor. 
<a name="l13188"><span class="ln">13188 </span></a> 
<a name="l13189"><span class="ln">13189 </span></a>.. math:: 
<a name="l13190"><span class="ln">13190 </span></a> 
<a name="l13191"><span class="ln">13191 </span></a>        y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta 
<a name="l13192"><span class="ln">13192 </span></a> 
<a name="l13193"><span class="ln">13193 </span></a>Arguments: 
<a name="l13194"><span class="ln">13194 </span></a>    input (Tensor): quantized tensor 
<a name="l13195"><span class="ln">13195 </span></a>    weight (Tensor): float tensor that corresponds to the gamma, size C 
<a name="l13196"><span class="ln">13196 </span></a>    bias (Tensor):  float tensor that corresponds to the beta, size C 
<a name="l13197"><span class="ln">13197 </span></a>    mean (Tensor): float mean value in batch normalization, size C 
<a name="l13198"><span class="ln">13198 </span></a>    var (Tensor): float tensor for variance, size C 
<a name="l13199"><span class="ln">13199 </span></a>    eps (float): a value added to the denominator for numerical stability. 
<a name="l13200"><span class="ln">13200 </span></a>    output_scale (float): output quantized tensor scale 
<a name="l13201"><span class="ln">13201 </span></a>    output_zero_point (int): output quantized tensor zero_point 
<a name="l13202"><span class="ln">13202 </span></a> 
<a name="l13203"><span class="ln">13203 </span></a>Returns: 
<a name="l13204"><span class="ln">13204 </span></a>    Tensor: A quantized tensor with batch normalization applied. 
<a name="l13205"><span class="ln">13205 </span></a> 
<a name="l13206"><span class="ln">13206 </span></a>Example:: 
<a name="l13207"><span class="ln">13207 </span></a> 
<a name="l13208"><span class="ln">13208 </span></a>    &gt;&gt;&gt; qx = torch.quantize_per_tensor(torch.rand(2, 2, 2, 2), 1.5, 3, torch.quint8) 
<a name="l13209"><span class="ln">13209 </span></a>    &gt;&gt;&gt; torch.quantized_batch_norm(qx, torch.ones(2), torch.zeros(2), torch.rand(2), torch.rand(2), 0.00001, 0.2, 2) 
<a name="l13210"><span class="ln">13210 </span></a>    tensor([[[[-0.2000, -0.2000], 
<a name="l13211"><span class="ln">13211 </span></a>          [ 1.6000, -0.2000]], 
<a name="l13212"><span class="ln">13212 </span></a> 
<a name="l13213"><span class="ln">13213 </span></a>         [[-0.4000, -0.4000], 
<a name="l13214"><span class="ln">13214 </span></a>          [-0.4000,  0.6000]]], 
<a name="l13215"><span class="ln">13215 </span></a> 
<a name="l13216"><span class="ln">13216 </span></a> 
<a name="l13217"><span class="ln">13217 </span></a>        [[[-0.2000, -0.2000], 
<a name="l13218"><span class="ln">13218 </span></a>          [-0.2000, -0.2000]], 
<a name="l13219"><span class="ln">13219 </span></a> 
<a name="l13220"><span class="ln">13220 </span></a>         [[ 0.6000, -0.4000], 
<a name="l13221"><span class="ln">13221 </span></a>          [ 0.6000, -0.4000]]]], size=(2, 2, 2, 2), dtype=torch.quint8, 
<a name="l13222"><span class="ln">13222 </span></a>       quantization_scheme=torch.per_tensor_affine, scale=0.2, zero_point=2) 
<a name="l13223"><span class="ln">13223 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13224"><span class="ln">13224 </span></a><span class="s3">)</span>
<a name="l13225"><span class="ln">13225 </span></a>
<a name="l13226"><span class="ln">13226 </span></a>
<a name="l13227"><span class="ln">13227 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13228"><span class="ln">13228 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">quantized_max_pool1d</span><span class="s3">,</span>
<a name="l13229"><span class="ln">13229 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13230"><span class="ln">13230 </span></a>quantized_max_pool1d(input, kernel_size, stride=[], padding=0, dilation=1, ceil_mode=False) -&gt; Tensor 
<a name="l13231"><span class="ln">13231 </span></a> 
<a name="l13232"><span class="ln">13232 </span></a>Applies a 1D max pooling over an input quantized tensor composed of several input planes. 
<a name="l13233"><span class="ln">13233 </span></a> 
<a name="l13234"><span class="ln">13234 </span></a>Arguments: 
<a name="l13235"><span class="ln">13235 </span></a>    input (Tensor): quantized tensor 
<a name="l13236"><span class="ln">13236 </span></a>    kernel_size (list of int): the size of the sliding window 
<a name="l13237"><span class="ln">13237 </span></a>    stride (``list of int``, optional): the stride of the sliding window 
<a name="l13238"><span class="ln">13238 </span></a>    padding (``list of int``, optional): padding to be added on both sides, must be &gt;= 0 and &lt;= kernel_size / 2 
<a name="l13239"><span class="ln">13239 </span></a>    dilation (``list of int``, optional): The stride between elements within a sliding window, must be &gt; 0. Default 1 
<a name="l13240"><span class="ln">13240 </span></a>    ceil_mode (bool, optional):  If True, will use ceil instead of floor to compute the output shape. 
<a name="l13241"><span class="ln">13241 </span></a>        Defaults to False. 
<a name="l13242"><span class="ln">13242 </span></a> 
<a name="l13243"><span class="ln">13243 </span></a> 
<a name="l13244"><span class="ln">13244 </span></a>Returns: 
<a name="l13245"><span class="ln">13245 </span></a>    Tensor: A quantized tensor with max_pool1d applied. 
<a name="l13246"><span class="ln">13246 </span></a> 
<a name="l13247"><span class="ln">13247 </span></a>Example:: 
<a name="l13248"><span class="ln">13248 </span></a> 
<a name="l13249"><span class="ln">13249 </span></a>    &gt;&gt;&gt; qx = torch.quantize_per_tensor(torch.rand(2, 2), 1.5, 3, torch.quint8) 
<a name="l13250"><span class="ln">13250 </span></a>    &gt;&gt;&gt; torch.quantized_max_pool1d(qx, [2]) 
<a name="l13251"><span class="ln">13251 </span></a>    tensor([[0.0000], 
<a name="l13252"><span class="ln">13252 </span></a>            [1.5000]], size=(2, 1), dtype=torch.quint8, 
<a name="l13253"><span class="ln">13253 </span></a>        quantization_scheme=torch.per_tensor_affine, scale=1.5, zero_point=3) 
<a name="l13254"><span class="ln">13254 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13255"><span class="ln">13255 </span></a><span class="s3">)</span>
<a name="l13256"><span class="ln">13256 </span></a>
<a name="l13257"><span class="ln">13257 </span></a>
<a name="l13258"><span class="ln">13258 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13259"><span class="ln">13259 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">quantized_max_pool2d</span><span class="s3">,</span>
<a name="l13260"><span class="ln">13260 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13261"><span class="ln">13261 </span></a>quantized_max_pool2d(input, kernel_size, stride=[], padding=0, dilation=1, ceil_mode=False) -&gt; Tensor 
<a name="l13262"><span class="ln">13262 </span></a> 
<a name="l13263"><span class="ln">13263 </span></a>Applies a 2D max pooling over an input quantized tensor composed of several input planes. 
<a name="l13264"><span class="ln">13264 </span></a> 
<a name="l13265"><span class="ln">13265 </span></a>Arguments: 
<a name="l13266"><span class="ln">13266 </span></a>    input (Tensor): quantized tensor 
<a name="l13267"><span class="ln">13267 </span></a>    kernel_size (``list of int``): the size of the sliding window 
<a name="l13268"><span class="ln">13268 </span></a>    stride (``list of int``, optional): the stride of the sliding window 
<a name="l13269"><span class="ln">13269 </span></a>    padding (``list of int``, optional): padding to be added on both sides, must be &gt;= 0 and &lt;= kernel_size / 2 
<a name="l13270"><span class="ln">13270 </span></a>    dilation (``list of int``, optional): The stride between elements within a sliding window, must be &gt; 0. Default 1 
<a name="l13271"><span class="ln">13271 </span></a>    ceil_mode (bool, optional):  If True, will use ceil instead of floor to compute the output shape. 
<a name="l13272"><span class="ln">13272 </span></a>        Defaults to False. 
<a name="l13273"><span class="ln">13273 </span></a> 
<a name="l13274"><span class="ln">13274 </span></a> 
<a name="l13275"><span class="ln">13275 </span></a>Returns: 
<a name="l13276"><span class="ln">13276 </span></a>    Tensor: A quantized tensor with max_pool2d applied. 
<a name="l13277"><span class="ln">13277 </span></a> 
<a name="l13278"><span class="ln">13278 </span></a>Example:: 
<a name="l13279"><span class="ln">13279 </span></a> 
<a name="l13280"><span class="ln">13280 </span></a>    &gt;&gt;&gt; qx = torch.quantize_per_tensor(torch.rand(2, 2, 2, 2), 1.5, 3, torch.quint8) 
<a name="l13281"><span class="ln">13281 </span></a>    &gt;&gt;&gt; torch.quantized_max_pool2d(qx, [2,2]) 
<a name="l13282"><span class="ln">13282 </span></a>    tensor([[[[1.5000]], 
<a name="l13283"><span class="ln">13283 </span></a> 
<a name="l13284"><span class="ln">13284 </span></a>            [[1.5000]]], 
<a name="l13285"><span class="ln">13285 </span></a> 
<a name="l13286"><span class="ln">13286 </span></a> 
<a name="l13287"><span class="ln">13287 </span></a>            [[[0.0000]], 
<a name="l13288"><span class="ln">13288 </span></a> 
<a name="l13289"><span class="ln">13289 </span></a>            [[0.0000]]]], size=(2, 2, 1, 1), dtype=torch.quint8, 
<a name="l13290"><span class="ln">13290 </span></a>        quantization_scheme=torch.per_tensor_affine, scale=1.5, zero_point=3) 
<a name="l13291"><span class="ln">13291 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13292"><span class="ln">13292 </span></a><span class="s3">)</span>
<a name="l13293"><span class="ln">13293 </span></a>
<a name="l13294"><span class="ln">13294 </span></a>
<a name="l13295"><span class="ln">13295 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13296"><span class="ln">13296 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Stream</span><span class="s3">,</span>
<a name="l13297"><span class="ln">13297 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13298"><span class="ln">13298 </span></a>Stream(device, *, priority) -&gt; Stream 
<a name="l13299"><span class="ln">13299 </span></a> 
<a name="l13300"><span class="ln">13300 </span></a>An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order. 
<a name="l13301"><span class="ln">13301 </span></a>It can control or synchronize the execution of other Stream or block the current host thread to ensure 
<a name="l13302"><span class="ln">13302 </span></a>the correct task sequencing. It supports with statement as a context manager to ensure the operators 
<a name="l13303"><span class="ln">13303 </span></a>within the with block are running on the corresponding stream. 
<a name="l13304"><span class="ln">13304 </span></a> 
<a name="l13305"><span class="ln">13305 </span></a>See in-depth description of the CUDA behavior at :ref:`cuda-semantics` for details 
<a name="l13306"><span class="ln">13306 </span></a>on the exact semantic that applies to all devices. 
<a name="l13307"><span class="ln">13307 </span></a> 
<a name="l13308"><span class="ln">13308 </span></a>Arguments: 
<a name="l13309"><span class="ln">13309 </span></a>    device (:class:`torch.device`, optional): the desired device for the Stream. 
<a name="l13310"><span class="ln">13310 </span></a>        If not given, the current :ref:`accelerator&lt;accelerators&gt;` type will be used. 
<a name="l13311"><span class="ln">13311 </span></a>    priority (int, optional): priority of the stream, should be 0 or negative, where negative 
<a name="l13312"><span class="ln">13312 </span></a>        numbers indicate higher priority. By default, streams have priority 0. 
<a name="l13313"><span class="ln">13313 </span></a> 
<a name="l13314"><span class="ln">13314 </span></a>Returns: 
<a name="l13315"><span class="ln">13315 </span></a>    Stream: An torch.Stream object. 
<a name="l13316"><span class="ln">13316 </span></a> 
<a name="l13317"><span class="ln">13317 </span></a>Example:: 
<a name="l13318"><span class="ln">13318 </span></a> 
<a name="l13319"><span class="ln">13319 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13320"><span class="ln">13320 </span></a>    &gt;&gt;&gt; with torch.Stream(device='cuda') as s_cuda: 
<a name="l13321"><span class="ln">13321 </span></a>    &gt;&gt;&gt;     a = torch.randn(10, 5, device='cuda') 
<a name="l13322"><span class="ln">13322 </span></a>    &gt;&gt;&gt;     b = torch.randn(5, 10, device='cuda') 
<a name="l13323"><span class="ln">13323 </span></a>    &gt;&gt;&gt;     c = torch.mm(a, b) 
<a name="l13324"><span class="ln">13324 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13325"><span class="ln">13325 </span></a><span class="s3">)</span>
<a name="l13326"><span class="ln">13326 </span></a>
<a name="l13327"><span class="ln">13327 </span></a>
<a name="l13328"><span class="ln">13328 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13329"><span class="ln">13329 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Stream</span><span class="s3">.</span><span class="s1">query</span><span class="s3">,</span>
<a name="l13330"><span class="ln">13330 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13331"><span class="ln">13331 </span></a>Stream.query() -&gt; bool 
<a name="l13332"><span class="ln">13332 </span></a> 
<a name="l13333"><span class="ln">13333 </span></a>Check if all the work submitted has been completed. 
<a name="l13334"><span class="ln">13334 </span></a> 
<a name="l13335"><span class="ln">13335 </span></a>Returns: 
<a name="l13336"><span class="ln">13336 </span></a>    bool: A boolean indicating if all kernels in this stream are completed. 
<a name="l13337"><span class="ln">13337 </span></a> 
<a name="l13338"><span class="ln">13338 </span></a>Example:: 
<a name="l13339"><span class="ln">13339 </span></a> 
<a name="l13340"><span class="ln">13340 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13341"><span class="ln">13341 </span></a>    &gt;&gt;&gt; s_cuda = torch.Stream(device='cuda') 
<a name="l13342"><span class="ln">13342 </span></a>    &gt;&gt;&gt; s_cuda.query() 
<a name="l13343"><span class="ln">13343 </span></a>    True 
<a name="l13344"><span class="ln">13344 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13345"><span class="ln">13345 </span></a><span class="s3">)</span>
<a name="l13346"><span class="ln">13346 </span></a>
<a name="l13347"><span class="ln">13347 </span></a>
<a name="l13348"><span class="ln">13348 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13349"><span class="ln">13349 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Stream</span><span class="s3">.</span><span class="s1">record_event</span><span class="s3">,</span>
<a name="l13350"><span class="ln">13350 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13351"><span class="ln">13351 </span></a>Stream.record_event(event) -&gt; Event 
<a name="l13352"><span class="ln">13352 </span></a> 
<a name="l13353"><span class="ln">13353 </span></a>Record an event. En-queuing it into the Stream to allow further synchronization from the current point in the FIFO queue. 
<a name="l13354"><span class="ln">13354 </span></a> 
<a name="l13355"><span class="ln">13355 </span></a>Arguments: 
<a name="l13356"><span class="ln">13356 </span></a>    event (:class:`torch.Event`, optional): event to record. If not given, a new one will be allocated. 
<a name="l13357"><span class="ln">13357 </span></a> 
<a name="l13358"><span class="ln">13358 </span></a>Returns: 
<a name="l13359"><span class="ln">13359 </span></a>    Event: Recorded event. 
<a name="l13360"><span class="ln">13360 </span></a> 
<a name="l13361"><span class="ln">13361 </span></a>Example:: 
<a name="l13362"><span class="ln">13362 </span></a> 
<a name="l13363"><span class="ln">13363 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13364"><span class="ln">13364 </span></a>    &gt;&gt;&gt; s_cuda = torch.Stream(device='cuda') 
<a name="l13365"><span class="ln">13365 </span></a>    &gt;&gt;&gt; e_cuda = s_cuda.record_event() 
<a name="l13366"><span class="ln">13366 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13367"><span class="ln">13367 </span></a><span class="s3">)</span>
<a name="l13368"><span class="ln">13368 </span></a>
<a name="l13369"><span class="ln">13369 </span></a>
<a name="l13370"><span class="ln">13370 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13371"><span class="ln">13371 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Stream</span><span class="s3">.</span><span class="s1">synchronize</span><span class="s3">,</span>
<a name="l13372"><span class="ln">13372 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13373"><span class="ln">13373 </span></a>Stream.synchronize() -&gt; None 
<a name="l13374"><span class="ln">13374 </span></a> 
<a name="l13375"><span class="ln">13375 </span></a>Wait for all the kernels in this stream to complete. 
<a name="l13376"><span class="ln">13376 </span></a> 
<a name="l13377"><span class="ln">13377 </span></a>Example:: 
<a name="l13378"><span class="ln">13378 </span></a> 
<a name="l13379"><span class="ln">13379 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13380"><span class="ln">13380 </span></a>    &gt;&gt;&gt; s_cuda = torch.Stream(device='cuda') 
<a name="l13381"><span class="ln">13381 </span></a>    &gt;&gt;&gt; s_cuda.synchronize() 
<a name="l13382"><span class="ln">13382 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13383"><span class="ln">13383 </span></a><span class="s3">)</span>
<a name="l13384"><span class="ln">13384 </span></a>
<a name="l13385"><span class="ln">13385 </span></a>
<a name="l13386"><span class="ln">13386 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13387"><span class="ln">13387 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Stream</span><span class="s3">.</span><span class="s1">wait_event</span><span class="s3">,</span>
<a name="l13388"><span class="ln">13388 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13389"><span class="ln">13389 </span></a>Stream.wait_event(event) -&gt; None 
<a name="l13390"><span class="ln">13390 </span></a> 
<a name="l13391"><span class="ln">13391 </span></a>Make all future work submitted to the stream wait for an event. 
<a name="l13392"><span class="ln">13392 </span></a> 
<a name="l13393"><span class="ln">13393 </span></a>Arguments: 
<a name="l13394"><span class="ln">13394 </span></a>    event (:class:`torch.Event`): an event to wait for. 
<a name="l13395"><span class="ln">13395 </span></a> 
<a name="l13396"><span class="ln">13396 </span></a>Example:: 
<a name="l13397"><span class="ln">13397 </span></a> 
<a name="l13398"><span class="ln">13398 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13399"><span class="ln">13399 </span></a>    &gt;&gt;&gt; s1_cuda = torch.Stream(device='cuda') 
<a name="l13400"><span class="ln">13400 </span></a>    &gt;&gt;&gt; s2_cuda = torch.Stream(device='cuda') 
<a name="l13401"><span class="ln">13401 </span></a>    &gt;&gt;&gt; e_cuda = s1_cuda.record_event() 
<a name="l13402"><span class="ln">13402 </span></a>    &gt;&gt;&gt; s2_cuda.wait_event(e_cuda) 
<a name="l13403"><span class="ln">13403 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13404"><span class="ln">13404 </span></a><span class="s3">)</span>
<a name="l13405"><span class="ln">13405 </span></a>
<a name="l13406"><span class="ln">13406 </span></a>
<a name="l13407"><span class="ln">13407 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13408"><span class="ln">13408 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Stream</span><span class="s3">.</span><span class="s1">wait_stream</span><span class="s3">,</span>
<a name="l13409"><span class="ln">13409 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13410"><span class="ln">13410 </span></a>Stream.wait_stream(stream) -&gt; None 
<a name="l13411"><span class="ln">13411 </span></a> 
<a name="l13412"><span class="ln">13412 </span></a>Synchronize with another stream. All future work submitted to this stream will wait until all kernels 
<a name="l13413"><span class="ln">13413 </span></a>already submitted to the given stream are completed. 
<a name="l13414"><span class="ln">13414 </span></a> 
<a name="l13415"><span class="ln">13415 </span></a>Arguments: 
<a name="l13416"><span class="ln">13416 </span></a>    stream (:class:`torch.Stream`): a stream to synchronize. 
<a name="l13417"><span class="ln">13417 </span></a> 
<a name="l13418"><span class="ln">13418 </span></a>Example:: 
<a name="l13419"><span class="ln">13419 </span></a> 
<a name="l13420"><span class="ln">13420 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13421"><span class="ln">13421 </span></a>    &gt;&gt;&gt; s1_cuda = torch.Stream(device='cuda') 
<a name="l13422"><span class="ln">13422 </span></a>    &gt;&gt;&gt; s2_cuda = torch.Stream(device='cuda') 
<a name="l13423"><span class="ln">13423 </span></a>    &gt;&gt;&gt; s2_cuda.wait_stream(s1_cuda) 
<a name="l13424"><span class="ln">13424 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13425"><span class="ln">13425 </span></a><span class="s3">)</span>
<a name="l13426"><span class="ln">13426 </span></a>
<a name="l13427"><span class="ln">13427 </span></a>
<a name="l13428"><span class="ln">13428 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13429"><span class="ln">13429 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Event</span><span class="s3">,</span>
<a name="l13430"><span class="ln">13430 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13431"><span class="ln">13431 </span></a>Event(device=None, *, enable_timing=False, blocking=False, interprocess=False) 
<a name="l13432"><span class="ln">13432 </span></a> 
<a name="l13433"><span class="ln">13433 </span></a>Query and record Stream status to identify or control dependencies across Stream and measure timing. 
<a name="l13434"><span class="ln">13434 </span></a> 
<a name="l13435"><span class="ln">13435 </span></a>Arguments: 
<a name="l13436"><span class="ln">13436 </span></a>    device (:class:`torch.device`, optional): the desired device for the Event. 
<a name="l13437"><span class="ln">13437 </span></a>        If not given, the current :ref:`accelerator&lt;accelerators&gt;` type will be used. 
<a name="l13438"><span class="ln">13438 </span></a>    enable_timing (bool, optional): indicates if the event should measure time (default: ``False``) 
<a name="l13439"><span class="ln">13439 </span></a>    blocking (bool, optional): if ``True``, :meth:`wait` will be blocking (default: ``False``) 
<a name="l13440"><span class="ln">13440 </span></a>    interprocess (bool): if ``True``, the event can be shared between processes (default: ``False``) 
<a name="l13441"><span class="ln">13441 </span></a> 
<a name="l13442"><span class="ln">13442 </span></a>.. warning:: 
<a name="l13443"><span class="ln">13443 </span></a> 
<a name="l13444"><span class="ln">13444 </span></a>    Both blocking and interprocess are not supported right now and are noops. 
<a name="l13445"><span class="ln">13445 </span></a> 
<a name="l13446"><span class="ln">13446 </span></a>Returns: 
<a name="l13447"><span class="ln">13447 </span></a>    Event: An torch.Event object. 
<a name="l13448"><span class="ln">13448 </span></a> 
<a name="l13449"><span class="ln">13449 </span></a>Example:: 
<a name="l13450"><span class="ln">13450 </span></a> 
<a name="l13451"><span class="ln">13451 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13452"><span class="ln">13452 </span></a>    &gt;&gt;&gt; event = torch.Event() 
<a name="l13453"><span class="ln">13453 </span></a>    &gt;&gt;&gt; e_cuda = torch.Event(device='cuda') 
<a name="l13454"><span class="ln">13454 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13455"><span class="ln">13455 </span></a><span class="s3">)</span>
<a name="l13456"><span class="ln">13456 </span></a>
<a name="l13457"><span class="ln">13457 </span></a>
<a name="l13458"><span class="ln">13458 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13459"><span class="ln">13459 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Event</span><span class="s3">.</span><span class="s1">elapsed_time</span><span class="s3">,</span>
<a name="l13460"><span class="ln">13460 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13461"><span class="ln">13461 </span></a>Event.elapsed_time(end_event) -&gt; float 
<a name="l13462"><span class="ln">13462 </span></a> 
<a name="l13463"><span class="ln">13463 </span></a>Returns the elapsed time in milliseconds between when this event and the :attr:`end_event` are 
<a name="l13464"><span class="ln">13464 </span></a>each recorded via :func:`torch.Stream.record_event`. 
<a name="l13465"><span class="ln">13465 </span></a> 
<a name="l13466"><span class="ln">13466 </span></a>Arguments: 
<a name="l13467"><span class="ln">13467 </span></a>    end_event (:class:`torch.Event`): The ending event has been recorded. 
<a name="l13468"><span class="ln">13468 </span></a> 
<a name="l13469"><span class="ln">13469 </span></a>Returns: 
<a name="l13470"><span class="ln">13470 </span></a>    float: Time between starting and ending event in milliseconds. 
<a name="l13471"><span class="ln">13471 </span></a> 
<a name="l13472"><span class="ln">13472 </span></a>Example:: 
<a name="l13473"><span class="ln">13473 </span></a> 
<a name="l13474"><span class="ln">13474 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13475"><span class="ln">13475 </span></a>    &gt;&gt;&gt; s_cuda = torch.Stream(device='cuda') 
<a name="l13476"><span class="ln">13476 </span></a>    &gt;&gt;&gt; e1_cuda = s_cuda.record_event() 
<a name="l13477"><span class="ln">13477 </span></a>    &gt;&gt;&gt; e2_cuda = s_cuda.record_event() 
<a name="l13478"><span class="ln">13478 </span></a>    &gt;&gt;&gt; ms = e1_cuda.elapsed_time(e2_cuda) 
<a name="l13479"><span class="ln">13479 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13480"><span class="ln">13480 </span></a><span class="s3">)</span>
<a name="l13481"><span class="ln">13481 </span></a>
<a name="l13482"><span class="ln">13482 </span></a>
<a name="l13483"><span class="ln">13483 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13484"><span class="ln">13484 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Event</span><span class="s3">.</span><span class="s1">query</span><span class="s3">,</span>
<a name="l13485"><span class="ln">13485 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13486"><span class="ln">13486 </span></a>Event.query() -&gt; bool 
<a name="l13487"><span class="ln">13487 </span></a> 
<a name="l13488"><span class="ln">13488 </span></a>Check if the stream where this event was recorded already moved past the point where the event was recorded. 
<a name="l13489"><span class="ln">13489 </span></a>Always returns ``True`` if the Event was not recorded. 
<a name="l13490"><span class="ln">13490 </span></a> 
<a name="l13491"><span class="ln">13491 </span></a>Returns: 
<a name="l13492"><span class="ln">13492 </span></a>    bool: A boolean indicating if all work currently captured by event has completed. 
<a name="l13493"><span class="ln">13493 </span></a> 
<a name="l13494"><span class="ln">13494 </span></a>Example:: 
<a name="l13495"><span class="ln">13495 </span></a> 
<a name="l13496"><span class="ln">13496 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13497"><span class="ln">13497 </span></a>    &gt;&gt;&gt; s_cuda = torch.Stream(device='cuda') 
<a name="l13498"><span class="ln">13498 </span></a>    &gt;&gt;&gt; e_cuda = s_cuda.record_event() 
<a name="l13499"><span class="ln">13499 </span></a>    &gt;&gt;&gt; e_cuda.query() 
<a name="l13500"><span class="ln">13500 </span></a>    True 
<a name="l13501"><span class="ln">13501 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13502"><span class="ln">13502 </span></a><span class="s3">)</span>
<a name="l13503"><span class="ln">13503 </span></a>
<a name="l13504"><span class="ln">13504 </span></a>
<a name="l13505"><span class="ln">13505 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13506"><span class="ln">13506 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Event</span><span class="s3">.</span><span class="s1">record</span><span class="s3">,</span>
<a name="l13507"><span class="ln">13507 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13508"><span class="ln">13508 </span></a>Event.record(stream=None) -&gt; None 
<a name="l13509"><span class="ln">13509 </span></a> 
<a name="l13510"><span class="ln">13510 </span></a>Record the event in a given stream. The stream's device must match the event's device. 
<a name="l13511"><span class="ln">13511 </span></a>This function is equivalent to ``stream.record_event(self)``. 
<a name="l13512"><span class="ln">13512 </span></a> 
<a name="l13513"><span class="ln">13513 </span></a>Arguments: 
<a name="l13514"><span class="ln">13514 </span></a>    stream (:class:`torch.Stream`, optional): A stream to be recorded. 
<a name="l13515"><span class="ln">13515 </span></a>        If not given, the current stream will be used. 
<a name="l13516"><span class="ln">13516 </span></a> 
<a name="l13517"><span class="ln">13517 </span></a>Example:: 
<a name="l13518"><span class="ln">13518 </span></a> 
<a name="l13519"><span class="ln">13519 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13520"><span class="ln">13520 </span></a>    &gt;&gt;&gt; e_cuda = torch.Event(device='cuda') 
<a name="l13521"><span class="ln">13521 </span></a>    &gt;&gt;&gt; e_cuda.record() 
<a name="l13522"><span class="ln">13522 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13523"><span class="ln">13523 </span></a><span class="s3">)</span>
<a name="l13524"><span class="ln">13524 </span></a>
<a name="l13525"><span class="ln">13525 </span></a>
<a name="l13526"><span class="ln">13526 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13527"><span class="ln">13527 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Event</span><span class="s3">.</span><span class="s1">synchronize</span><span class="s3">,</span>
<a name="l13528"><span class="ln">13528 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13529"><span class="ln">13529 </span></a>Event.synchronize() -&gt; None 
<a name="l13530"><span class="ln">13530 </span></a> 
<a name="l13531"><span class="ln">13531 </span></a>Wait for the event to complete. This prevents the CPU thread from proceeding until the event completes. 
<a name="l13532"><span class="ln">13532 </span></a> 
<a name="l13533"><span class="ln">13533 </span></a>Example:: 
<a name="l13534"><span class="ln">13534 </span></a> 
<a name="l13535"><span class="ln">13535 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13536"><span class="ln">13536 </span></a>    &gt;&gt;&gt; s_cuda = torch.Stream(device='cuda') 
<a name="l13537"><span class="ln">13537 </span></a>    &gt;&gt;&gt; e_cuda = s_cuda.record_event() 
<a name="l13538"><span class="ln">13538 </span></a>    &gt;&gt;&gt; e_cuda.synchronize() 
<a name="l13539"><span class="ln">13539 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13540"><span class="ln">13540 </span></a><span class="s3">)</span>
<a name="l13541"><span class="ln">13541 </span></a>
<a name="l13542"><span class="ln">13542 </span></a>
<a name="l13543"><span class="ln">13543 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13544"><span class="ln">13544 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Event</span><span class="s3">.</span><span class="s1">wait</span><span class="s3">,</span>
<a name="l13545"><span class="ln">13545 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13546"><span class="ln">13546 </span></a>Event.wait(stream=None) -&gt; None 
<a name="l13547"><span class="ln">13547 </span></a> 
<a name="l13548"><span class="ln">13548 </span></a>Make all future work submitted to the given stream wait for this event. 
<a name="l13549"><span class="ln">13549 </span></a> 
<a name="l13550"><span class="ln">13550 </span></a>Arguments: 
<a name="l13551"><span class="ln">13551 </span></a>    stream (:class:`torch.Stream`, optional): A stream to synchronize. 
<a name="l13552"><span class="ln">13552 </span></a>        If not given, the current stream will be used. 
<a name="l13553"><span class="ln">13553 </span></a> 
<a name="l13554"><span class="ln">13554 </span></a>Example:: 
<a name="l13555"><span class="ln">13555 </span></a> 
<a name="l13556"><span class="ln">13556 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13557"><span class="ln">13557 </span></a>    &gt;&gt;&gt; s1_cuda = torch.Stream(device='cuda') 
<a name="l13558"><span class="ln">13558 </span></a>    &gt;&gt;&gt; s2_cuda = torch.Stream(device='cuda') 
<a name="l13559"><span class="ln">13559 </span></a>    &gt;&gt;&gt; e_cuda = s1_cuda.record() 
<a name="l13560"><span class="ln">13560 </span></a>    &gt;&gt;&gt; e_cuda.wait(s2) 
<a name="l13561"><span class="ln">13561 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13562"><span class="ln">13562 </span></a><span class="s3">)</span>
<a name="l13563"><span class="ln">13563 </span></a>
<a name="l13564"><span class="ln">13564 </span></a>
<a name="l13565"><span class="ln">13565 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13566"><span class="ln">13566 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">,</span>
<a name="l13567"><span class="ln">13567 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13568"><span class="ln">13568 </span></a>Generator(device='cpu') -&gt; Generator 
<a name="l13569"><span class="ln">13569 </span></a> 
<a name="l13570"><span class="ln">13570 </span></a>Creates and returns a generator object that manages the state of the algorithm which 
<a name="l13571"><span class="ln">13571 </span></a>produces pseudo random numbers. Used as a keyword argument in many :ref:`inplace-random-sampling` 
<a name="l13572"><span class="ln">13572 </span></a>functions. 
<a name="l13573"><span class="ln">13573 </span></a> 
<a name="l13574"><span class="ln">13574 </span></a>Arguments: 
<a name="l13575"><span class="ln">13575 </span></a>    device (:class:`torch.device`, optional): the desired device for the generator. 
<a name="l13576"><span class="ln">13576 </span></a> 
<a name="l13577"><span class="ln">13577 </span></a>Returns: 
<a name="l13578"><span class="ln">13578 </span></a>    Generator: An torch.Generator object. 
<a name="l13579"><span class="ln">13579 </span></a> 
<a name="l13580"><span class="ln">13580 </span></a>Example:: 
<a name="l13581"><span class="ln">13581 </span></a> 
<a name="l13582"><span class="ln">13582 </span></a>    &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) 
<a name="l13583"><span class="ln">13583 </span></a>    &gt;&gt;&gt; g_cpu = torch.Generator() 
<a name="l13584"><span class="ln">13584 </span></a>    &gt;&gt;&gt; g_cuda = torch.Generator(device='cuda') 
<a name="l13585"><span class="ln">13585 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13586"><span class="ln">13586 </span></a><span class="s3">)</span>
<a name="l13587"><span class="ln">13587 </span></a>
<a name="l13588"><span class="ln">13588 </span></a>
<a name="l13589"><span class="ln">13589 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13590"><span class="ln">13590 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">set_state</span><span class="s3">,</span>
<a name="l13591"><span class="ln">13591 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13592"><span class="ln">13592 </span></a>Generator.set_state(new_state) -&gt; void 
<a name="l13593"><span class="ln">13593 </span></a> 
<a name="l13594"><span class="ln">13594 </span></a>Sets the Generator state. 
<a name="l13595"><span class="ln">13595 </span></a> 
<a name="l13596"><span class="ln">13596 </span></a>Arguments: 
<a name="l13597"><span class="ln">13597 </span></a>    new_state (torch.ByteTensor): The desired state. 
<a name="l13598"><span class="ln">13598 </span></a> 
<a name="l13599"><span class="ln">13599 </span></a>Example:: 
<a name="l13600"><span class="ln">13600 </span></a> 
<a name="l13601"><span class="ln">13601 </span></a>    &gt;&gt;&gt; g_cpu = torch.Generator() 
<a name="l13602"><span class="ln">13602 </span></a>    &gt;&gt;&gt; g_cpu_other = torch.Generator() 
<a name="l13603"><span class="ln">13603 </span></a>    &gt;&gt;&gt; g_cpu.set_state(g_cpu_other.get_state()) 
<a name="l13604"><span class="ln">13604 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13605"><span class="ln">13605 </span></a><span class="s3">)</span>
<a name="l13606"><span class="ln">13606 </span></a>
<a name="l13607"><span class="ln">13607 </span></a>
<a name="l13608"><span class="ln">13608 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13609"><span class="ln">13609 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">get_state</span><span class="s3">,</span>
<a name="l13610"><span class="ln">13610 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13611"><span class="ln">13611 </span></a>Generator.get_state() -&gt; Tensor 
<a name="l13612"><span class="ln">13612 </span></a> 
<a name="l13613"><span class="ln">13613 </span></a>Returns the Generator state as a ``torch.ByteTensor``. 
<a name="l13614"><span class="ln">13614 </span></a> 
<a name="l13615"><span class="ln">13615 </span></a>Returns: 
<a name="l13616"><span class="ln">13616 </span></a>    Tensor: A ``torch.ByteTensor`` which contains all the necessary bits 
<a name="l13617"><span class="ln">13617 </span></a>    to restore a Generator to a specific point in time. 
<a name="l13618"><span class="ln">13618 </span></a> 
<a name="l13619"><span class="ln">13619 </span></a>Example:: 
<a name="l13620"><span class="ln">13620 </span></a> 
<a name="l13621"><span class="ln">13621 </span></a>    &gt;&gt;&gt; g_cpu = torch.Generator() 
<a name="l13622"><span class="ln">13622 </span></a>    &gt;&gt;&gt; g_cpu.get_state() 
<a name="l13623"><span class="ln">13623 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13624"><span class="ln">13624 </span></a><span class="s3">)</span>
<a name="l13625"><span class="ln">13625 </span></a>
<a name="l13626"><span class="ln">13626 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13627"><span class="ln">13627 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">graphsafe_set_state</span><span class="s3">,</span>
<a name="l13628"><span class="ln">13628 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13629"><span class="ln">13629 </span></a>Generator.graphsafe_set_state(state) -&gt; None 
<a name="l13630"><span class="ln">13630 </span></a> 
<a name="l13631"><span class="ln">13631 </span></a>Sets the state of the generator to the specified state in a manner that is safe for use in graph capture. 
<a name="l13632"><span class="ln">13632 </span></a>This method is crucial for ensuring that the generator's state can be captured in the CUDA graph. 
<a name="l13633"><span class="ln">13633 </span></a> 
<a name="l13634"><span class="ln">13634 </span></a>Arguments: 
<a name="l13635"><span class="ln">13635 </span></a>    state (torch.Generator): A Generator point to the new state for the generator, typically obtained from `graphsafe_get_state`. 
<a name="l13636"><span class="ln">13636 </span></a> 
<a name="l13637"><span class="ln">13637 </span></a>Example: 
<a name="l13638"><span class="ln">13638 </span></a>    &gt;&gt;&gt; g_cuda = torch.Generator(device='cuda') 
<a name="l13639"><span class="ln">13639 </span></a>    &gt;&gt;&gt; g_cuda_other = torch.Generator(device='cuda') 
<a name="l13640"><span class="ln">13640 </span></a>    &gt;&gt;&gt; current_state = g_cuda_other.graphsafe_get_state() 
<a name="l13641"><span class="ln">13641 </span></a>    &gt;&gt;&gt; g_cuda.graphsafe_set_state(current_state) 
<a name="l13642"><span class="ln">13642 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13643"><span class="ln">13643 </span></a><span class="s3">)</span>
<a name="l13644"><span class="ln">13644 </span></a>
<a name="l13645"><span class="ln">13645 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13646"><span class="ln">13646 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">graphsafe_get_state</span><span class="s3">,</span>
<a name="l13647"><span class="ln">13647 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13648"><span class="ln">13648 </span></a>Generator.graphsafe_get_state() -&gt; torch.Generator 
<a name="l13649"><span class="ln">13649 </span></a> 
<a name="l13650"><span class="ln">13650 </span></a>Retrieves the current state of the generator in a manner that is safe for graph capture. 
<a name="l13651"><span class="ln">13651 </span></a>This method is crucial for ensuring that the generator's state can be captured in the CUDA graph. 
<a name="l13652"><span class="ln">13652 </span></a> 
<a name="l13653"><span class="ln">13653 </span></a>Returns: 
<a name="l13654"><span class="ln">13654 </span></a>    torch.Generator: A Generator point to the current state of the generator 
<a name="l13655"><span class="ln">13655 </span></a> 
<a name="l13656"><span class="ln">13656 </span></a>Example: 
<a name="l13657"><span class="ln">13657 </span></a>    &gt;&gt;&gt; g_cuda = torch.Generator(device='cuda') 
<a name="l13658"><span class="ln">13658 </span></a>    &gt;&gt;&gt; current_state = g_cuda.graphsafe_get_state() 
<a name="l13659"><span class="ln">13659 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13660"><span class="ln">13660 </span></a><span class="s3">)</span>
<a name="l13661"><span class="ln">13661 </span></a>
<a name="l13662"><span class="ln">13662 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13663"><span class="ln">13663 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">clone_state</span><span class="s3">,</span>
<a name="l13664"><span class="ln">13664 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13665"><span class="ln">13665 </span></a>Generator.clone_state() -&gt; torch.Generator 
<a name="l13666"><span class="ln">13666 </span></a> 
<a name="l13667"><span class="ln">13667 </span></a>Clones the current state of the generator and returns a new generator pointing to this cloned state. 
<a name="l13668"><span class="ln">13668 </span></a>This method is beneficial for preserving a particular state of a generator to restore at a later point. 
<a name="l13669"><span class="ln">13669 </span></a> 
<a name="l13670"><span class="ln">13670 </span></a>Returns: 
<a name="l13671"><span class="ln">13671 </span></a>    torch.Generator: A Generator pointing to the newly cloned state. 
<a name="l13672"><span class="ln">13672 </span></a> 
<a name="l13673"><span class="ln">13673 </span></a>Example: 
<a name="l13674"><span class="ln">13674 </span></a>    &gt;&gt;&gt; g_cuda = torch.Generator(device='cuda') 
<a name="l13675"><span class="ln">13675 </span></a>    &gt;&gt;&gt; cloned_state = g_cuda.clone_state() 
<a name="l13676"><span class="ln">13676 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13677"><span class="ln">13677 </span></a><span class="s3">)</span>
<a name="l13678"><span class="ln">13678 </span></a>
<a name="l13679"><span class="ln">13679 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13680"><span class="ln">13680 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">manual_seed</span><span class="s3">,</span>
<a name="l13681"><span class="ln">13681 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13682"><span class="ln">13682 </span></a>Generator.manual_seed(seed) -&gt; Generator 
<a name="l13683"><span class="ln">13683 </span></a> 
<a name="l13684"><span class="ln">13684 </span></a>Sets the seed for generating random numbers. Returns a `torch.Generator` object. Any 32-bit integer is a valid seed. 
<a name="l13685"><span class="ln">13685 </span></a> 
<a name="l13686"><span class="ln">13686 </span></a>Arguments: 
<a name="l13687"><span class="ln">13687 </span></a>    seed (int): The desired seed. Value must be within the inclusive range 
<a name="l13688"><span class="ln">13688 </span></a>        `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError 
<a name="l13689"><span class="ln">13689 </span></a>        is raised. Negative inputs are remapped to positive values with the formula 
<a name="l13690"><span class="ln">13690 </span></a>        `0xffff_ffff_ffff_ffff + seed`. 
<a name="l13691"><span class="ln">13691 </span></a> 
<a name="l13692"><span class="ln">13692 </span></a>Returns: 
<a name="l13693"><span class="ln">13693 </span></a>    Generator: An torch.Generator object. 
<a name="l13694"><span class="ln">13694 </span></a> 
<a name="l13695"><span class="ln">13695 </span></a>Example:: 
<a name="l13696"><span class="ln">13696 </span></a> 
<a name="l13697"><span class="ln">13697 </span></a>    &gt;&gt;&gt; g_cpu = torch.Generator() 
<a name="l13698"><span class="ln">13698 </span></a>    &gt;&gt;&gt; g_cpu.manual_seed(2147483647) 
<a name="l13699"><span class="ln">13699 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13700"><span class="ln">13700 </span></a><span class="s3">)</span>
<a name="l13701"><span class="ln">13701 </span></a>
<a name="l13702"><span class="ln">13702 </span></a>
<a name="l13703"><span class="ln">13703 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13704"><span class="ln">13704 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">initial_seed</span><span class="s3">,</span>
<a name="l13705"><span class="ln">13705 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13706"><span class="ln">13706 </span></a>Generator.initial_seed() -&gt; int 
<a name="l13707"><span class="ln">13707 </span></a> 
<a name="l13708"><span class="ln">13708 </span></a>Returns the initial seed for generating random numbers. 
<a name="l13709"><span class="ln">13709 </span></a> 
<a name="l13710"><span class="ln">13710 </span></a>Example:: 
<a name="l13711"><span class="ln">13711 </span></a> 
<a name="l13712"><span class="ln">13712 </span></a>    &gt;&gt;&gt; g_cpu = torch.Generator() 
<a name="l13713"><span class="ln">13713 </span></a>    &gt;&gt;&gt; g_cpu.initial_seed() 
<a name="l13714"><span class="ln">13714 </span></a>    2147483647 
<a name="l13715"><span class="ln">13715 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13716"><span class="ln">13716 </span></a><span class="s3">)</span>
<a name="l13717"><span class="ln">13717 </span></a>
<a name="l13718"><span class="ln">13718 </span></a>
<a name="l13719"><span class="ln">13719 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13720"><span class="ln">13720 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">seed</span><span class="s3">,</span>
<a name="l13721"><span class="ln">13721 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13722"><span class="ln">13722 </span></a>Generator.seed() -&gt; int 
<a name="l13723"><span class="ln">13723 </span></a> 
<a name="l13724"><span class="ln">13724 </span></a>Gets a non-deterministic random number from std::random_device or the current 
<a name="l13725"><span class="ln">13725 </span></a>time and uses it to seed a Generator. 
<a name="l13726"><span class="ln">13726 </span></a> 
<a name="l13727"><span class="ln">13727 </span></a>Example:: 
<a name="l13728"><span class="ln">13728 </span></a> 
<a name="l13729"><span class="ln">13729 </span></a>    &gt;&gt;&gt; g_cpu = torch.Generator() 
<a name="l13730"><span class="ln">13730 </span></a>    &gt;&gt;&gt; g_cpu.seed() 
<a name="l13731"><span class="ln">13731 </span></a>    1516516984916 
<a name="l13732"><span class="ln">13732 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13733"><span class="ln">13733 </span></a><span class="s3">)</span>
<a name="l13734"><span class="ln">13734 </span></a>
<a name="l13735"><span class="ln">13735 </span></a>
<a name="l13736"><span class="ln">13736 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13737"><span class="ln">13737 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">Generator</span><span class="s3">.</span><span class="s1">device</span><span class="s3">,</span>
<a name="l13738"><span class="ln">13738 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13739"><span class="ln">13739 </span></a>Generator.device -&gt; device 
<a name="l13740"><span class="ln">13740 </span></a> 
<a name="l13741"><span class="ln">13741 </span></a>Gets the current device of the generator. 
<a name="l13742"><span class="ln">13742 </span></a> 
<a name="l13743"><span class="ln">13743 </span></a>Example:: 
<a name="l13744"><span class="ln">13744 </span></a> 
<a name="l13745"><span class="ln">13745 </span></a>    &gt;&gt;&gt; g_cpu = torch.Generator() 
<a name="l13746"><span class="ln">13746 </span></a>    &gt;&gt;&gt; g_cpu.device 
<a name="l13747"><span class="ln">13747 </span></a>    device(type='cpu') 
<a name="l13748"><span class="ln">13748 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13749"><span class="ln">13749 </span></a><span class="s3">)</span>
<a name="l13750"><span class="ln">13750 </span></a>
<a name="l13751"><span class="ln">13751 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13752"><span class="ln">13752 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">_assert_async</span><span class="s3">,</span>
<a name="l13753"><span class="ln">13753 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13754"><span class="ln">13754 </span></a>_assert_async(tensor) -&gt; void 
<a name="l13755"><span class="ln">13755 </span></a> 
<a name="l13756"><span class="ln">13756 </span></a>Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors, 
<a name="l13757"><span class="ln">13757 </span></a>this is equivalent to ``assert tensor`` or ``assert tensor.is_nonzero()``; for 
<a name="l13758"><span class="ln">13758 </span></a>CUDA tensors, we DO NOT synchronize and you may only find out the assertion 
<a name="l13759"><span class="ln">13759 </span></a>failed at a later CUDA kernel launch.  Asynchronous assertion can be helpful for 
<a name="l13760"><span class="ln">13760 </span></a>testing invariants in CUDA tensors without giving up performance.  This function 
<a name="l13761"><span class="ln">13761 </span></a>is NOT intended to be used for regular error checking, as it will trash your CUDA 
<a name="l13762"><span class="ln">13762 </span></a>context if the assert fails (forcing you to restart your PyTorch process.) 
<a name="l13763"><span class="ln">13763 </span></a> 
<a name="l13764"><span class="ln">13764 </span></a>Args: 
<a name="l13765"><span class="ln">13765 </span></a>    tensor (Tensor): a one element tensor to test to see if it is nonzero.  Zero 
<a name="l13766"><span class="ln">13766 </span></a>        elements (including False for boolean tensors) cause an assertion failure 
<a name="l13767"><span class="ln">13767 </span></a>        to be raised. 
<a name="l13768"><span class="ln">13768 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13769"><span class="ln">13769 </span></a><span class="s3">)</span>
<a name="l13770"><span class="ln">13770 </span></a>
<a name="l13771"><span class="ln">13771 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13772"><span class="ln">13772 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">searchsorted</span><span class="s3">,</span>
<a name="l13773"><span class="ln">13773 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13774"><span class="ln">13774 </span></a>searchsorted(sorted_sequence, values, *, out_int32=False, right=False, side=None, out=None, sorter=None) -&gt; Tensor 
<a name="l13775"><span class="ln">13775 </span></a> 
<a name="l13776"><span class="ln">13776 </span></a>Find the indices from the *innermost* dimension of :attr:`sorted_sequence` such that, if the 
<a name="l13777"><span class="ln">13777 </span></a>corresponding values in :attr:`values` were inserted before the indices, when sorted, the order 
<a name="l13778"><span class="ln">13778 </span></a>of the corresponding *innermost* dimension within :attr:`sorted_sequence` would be preserved. 
<a name="l13779"><span class="ln">13779 </span></a>Return a new tensor with the same size as :attr:`values`. More formally, 
<a name="l13780"><span class="ln">13780 </span></a>the returned index satisfies the following rules: 
<a name="l13781"><span class="ln">13781 </span></a> 
<a name="l13782"><span class="ln">13782 </span></a>.. list-table:: 
<a name="l13783"><span class="ln">13783 </span></a>   :widths: 12 10 78 
<a name="l13784"><span class="ln">13784 </span></a>   :header-rows: 1 
<a name="l13785"><span class="ln">13785 </span></a> 
<a name="l13786"><span class="ln">13786 </span></a>   * - :attr:`sorted_sequence` 
<a name="l13787"><span class="ln">13787 </span></a>     - :attr:`right` 
<a name="l13788"><span class="ln">13788 </span></a>     - *returned index satisfies* 
<a name="l13789"><span class="ln">13789 </span></a>   * - 1-D 
<a name="l13790"><span class="ln">13790 </span></a>     - False 
<a name="l13791"><span class="ln">13791 </span></a>     - ``sorted_sequence[i-1] &lt; values[m][n]...[l][x] &lt;= sorted_sequence[i]`` 
<a name="l13792"><span class="ln">13792 </span></a>   * - 1-D 
<a name="l13793"><span class="ln">13793 </span></a>     - True 
<a name="l13794"><span class="ln">13794 </span></a>     - ``sorted_sequence[i-1] &lt;= values[m][n]...[l][x] &lt; sorted_sequence[i]`` 
<a name="l13795"><span class="ln">13795 </span></a>   * - N-D 
<a name="l13796"><span class="ln">13796 </span></a>     - False 
<a name="l13797"><span class="ln">13797 </span></a>     - ``sorted_sequence[m][n]...[l][i-1] &lt; values[m][n]...[l][x] &lt;= sorted_sequence[m][n]...[l][i]`` 
<a name="l13798"><span class="ln">13798 </span></a>   * - N-D 
<a name="l13799"><span class="ln">13799 </span></a>     - True 
<a name="l13800"><span class="ln">13800 </span></a>     - ``sorted_sequence[m][n]...[l][i-1] &lt;= values[m][n]...[l][x] &lt; sorted_sequence[m][n]...[l][i]`` 
<a name="l13801"><span class="ln">13801 </span></a> 
<a name="l13802"><span class="ln">13802 </span></a>Args: 
<a name="l13803"><span class="ln">13803 </span></a>    sorted_sequence (Tensor): N-D or 1-D tensor, containing monotonically increasing sequence on the *innermost* 
<a name="l13804"><span class="ln">13804 </span></a>                              dimension unless :attr:`sorter` is provided, in which case the sequence does not 
<a name="l13805"><span class="ln">13805 </span></a>                              need to be sorted 
<a name="l13806"><span class="ln">13806 </span></a>    values (Tensor or Scalar): N-D tensor or a Scalar containing the search value(s). 
<a name="l13807"><span class="ln">13807 </span></a> 
<a name="l13808"><span class="ln">13808 </span></a>Keyword args: 
<a name="l13809"><span class="ln">13809 </span></a>    out_int32 (bool, optional): indicate the output data type. torch.int32 if True, torch.int64 otherwise. 
<a name="l13810"><span class="ln">13810 </span></a>                                Default value is False, i.e. default output data type is torch.int64. 
<a name="l13811"><span class="ln">13811 </span></a>    right (bool, optional): if False, return the first suitable location that is found. If True, return the 
<a name="l13812"><span class="ln">13812 </span></a>                            last such index. If no suitable index found, return 0 for non-numerical value 
<a name="l13813"><span class="ln">13813 </span></a>                            (eg. nan, inf) or the size of *innermost* dimension within :attr:`sorted_sequence` 
<a name="l13814"><span class="ln">13814 </span></a>                            (one pass the last index of the *innermost* dimension). In other words, if False, 
<a name="l13815"><span class="ln">13815 </span></a>                            gets the lower bound index for each value in :attr:`values` on the corresponding 
<a name="l13816"><span class="ln">13816 </span></a>                            *innermost* dimension of the :attr:`sorted_sequence`. If True, gets the upper 
<a name="l13817"><span class="ln">13817 </span></a>                            bound index instead. Default value is False. :attr:`side` does the same and is 
<a name="l13818"><span class="ln">13818 </span></a>                            preferred. It will error if :attr:`side` is set to &quot;left&quot; while this is True. 
<a name="l13819"><span class="ln">13819 </span></a>    side (str, optional): the same as :attr:`right` but preferred. &quot;left&quot; corresponds to False for :attr:`right` 
<a name="l13820"><span class="ln">13820 </span></a>                            and &quot;right&quot; corresponds to True for :attr:`right`. It will error if this is set to 
<a name="l13821"><span class="ln">13821 </span></a>                            &quot;left&quot; while :attr:`right` is True. Default value is None. 
<a name="l13822"><span class="ln">13822 </span></a>    out (Tensor, optional): the output tensor, must be the same size as :attr:`values` if provided. 
<a name="l13823"><span class="ln">13823 </span></a>    sorter (LongTensor, optional): if provided, a tensor matching the shape of the unsorted 
<a name="l13824"><span class="ln">13824 </span></a>                            :attr:`sorted_sequence` containing a sequence of indices that sort it in the 
<a name="l13825"><span class="ln">13825 </span></a>                            ascending order on the innermost dimension 
<a name="l13826"><span class="ln">13826 </span></a> 
<a name="l13827"><span class="ln">13827 </span></a> 
<a name="l13828"><span class="ln">13828 </span></a>Example:: 
<a name="l13829"><span class="ln">13829 </span></a> 
<a name="l13830"><span class="ln">13830 </span></a>    &gt;&gt;&gt; sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]]) 
<a name="l13831"><span class="ln">13831 </span></a>    &gt;&gt;&gt; sorted_sequence 
<a name="l13832"><span class="ln">13832 </span></a>    tensor([[ 1,  3,  5,  7,  9], 
<a name="l13833"><span class="ln">13833 </span></a>            [ 2,  4,  6,  8, 10]]) 
<a name="l13834"><span class="ln">13834 </span></a>    &gt;&gt;&gt; values = torch.tensor([[3, 6, 9], [3, 6, 9]]) 
<a name="l13835"><span class="ln">13835 </span></a>    &gt;&gt;&gt; values 
<a name="l13836"><span class="ln">13836 </span></a>    tensor([[3, 6, 9], 
<a name="l13837"><span class="ln">13837 </span></a>            [3, 6, 9]]) 
<a name="l13838"><span class="ln">13838 </span></a>    &gt;&gt;&gt; torch.searchsorted(sorted_sequence, values) 
<a name="l13839"><span class="ln">13839 </span></a>    tensor([[1, 3, 4], 
<a name="l13840"><span class="ln">13840 </span></a>            [1, 2, 4]]) 
<a name="l13841"><span class="ln">13841 </span></a>    &gt;&gt;&gt; torch.searchsorted(sorted_sequence, values, side='right') 
<a name="l13842"><span class="ln">13842 </span></a>    tensor([[2, 3, 5], 
<a name="l13843"><span class="ln">13843 </span></a>            [1, 3, 4]]) 
<a name="l13844"><span class="ln">13844 </span></a> 
<a name="l13845"><span class="ln">13845 </span></a>    &gt;&gt;&gt; sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9]) 
<a name="l13846"><span class="ln">13846 </span></a>    &gt;&gt;&gt; sorted_sequence_1d 
<a name="l13847"><span class="ln">13847 </span></a>    tensor([1, 3, 5, 7, 9]) 
<a name="l13848"><span class="ln">13848 </span></a>    &gt;&gt;&gt; torch.searchsorted(sorted_sequence_1d, values) 
<a name="l13849"><span class="ln">13849 </span></a>    tensor([[1, 3, 4], 
<a name="l13850"><span class="ln">13850 </span></a>            [1, 3, 4]]) 
<a name="l13851"><span class="ln">13851 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13852"><span class="ln">13852 </span></a><span class="s3">)</span>
<a name="l13853"><span class="ln">13853 </span></a>
<a name="l13854"><span class="ln">13854 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13855"><span class="ln">13855 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">bucketize</span><span class="s3">,</span>
<a name="l13856"><span class="ln">13856 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13857"><span class="ln">13857 </span></a>bucketize(input, boundaries, *, out_int32=False, right=False, out=None) -&gt; Tensor 
<a name="l13858"><span class="ln">13858 </span></a> 
<a name="l13859"><span class="ln">13859 </span></a>Returns the indices of the buckets to which each value in the :attr:`input` belongs, where the 
<a name="l13860"><span class="ln">13860 </span></a>boundaries of the buckets are set by :attr:`boundaries`. Return a new tensor with the same size 
<a name="l13861"><span class="ln">13861 </span></a>as :attr:`input`. If :attr:`right` is False (default), then the left boundary is open. Note that 
<a name="l13862"><span class="ln">13862 </span></a>this behavior is opposite the behavior of 
<a name="l13863"><span class="ln">13863 </span></a>`numpy.digitize &lt;https://numpy.org/doc/stable/reference/generated/numpy.digitize.html&gt;`_. 
<a name="l13864"><span class="ln">13864 </span></a>More formally, the returned index satisfies the following rules: 
<a name="l13865"><span class="ln">13865 </span></a> 
<a name="l13866"><span class="ln">13866 </span></a>.. list-table:: 
<a name="l13867"><span class="ln">13867 </span></a>   :widths: 15 85 
<a name="l13868"><span class="ln">13868 </span></a>   :header-rows: 1 
<a name="l13869"><span class="ln">13869 </span></a> 
<a name="l13870"><span class="ln">13870 </span></a>   * - :attr:`right` 
<a name="l13871"><span class="ln">13871 </span></a>     - *returned index satisfies* 
<a name="l13872"><span class="ln">13872 </span></a>   * - False 
<a name="l13873"><span class="ln">13873 </span></a>     - ``boundaries[i-1] &lt; input[m][n]...[l][x] &lt;= boundaries[i]`` 
<a name="l13874"><span class="ln">13874 </span></a>   * - True 
<a name="l13875"><span class="ln">13875 </span></a>     - ``boundaries[i-1] &lt;= input[m][n]...[l][x] &lt; boundaries[i]`` 
<a name="l13876"><span class="ln">13876 </span></a> 
<a name="l13877"><span class="ln">13877 </span></a>Args: 
<a name="l13878"><span class="ln">13878 </span></a>    input (Tensor or Scalar): N-D tensor or a Scalar containing the search value(s). 
<a name="l13879"><span class="ln">13879 </span></a>    boundaries (Tensor): 1-D tensor, must contain a strictly increasing sequence, or the return value is undefined. 
<a name="l13880"><span class="ln">13880 </span></a> 
<a name="l13881"><span class="ln">13881 </span></a>Keyword args: 
<a name="l13882"><span class="ln">13882 </span></a>    out_int32 (bool, optional): indicate the output data type. torch.int32 if True, torch.int64 otherwise. 
<a name="l13883"><span class="ln">13883 </span></a>                                Default value is False, i.e. default output data type is torch.int64. 
<a name="l13884"><span class="ln">13884 </span></a>    right (bool, optional): determines the behavior for values in :attr:`boundaries`. See the table above. 
<a name="l13885"><span class="ln">13885 </span></a>    out (Tensor, optional): the output tensor, must be the same size as :attr:`input` if provided. 
<a name="l13886"><span class="ln">13886 </span></a> 
<a name="l13887"><span class="ln">13887 </span></a> 
<a name="l13888"><span class="ln">13888 </span></a>Example:: 
<a name="l13889"><span class="ln">13889 </span></a> 
<a name="l13890"><span class="ln">13890 </span></a>    &gt;&gt;&gt; boundaries = torch.tensor([1, 3, 5, 7, 9]) 
<a name="l13891"><span class="ln">13891 </span></a>    &gt;&gt;&gt; boundaries 
<a name="l13892"><span class="ln">13892 </span></a>    tensor([1, 3, 5, 7, 9]) 
<a name="l13893"><span class="ln">13893 </span></a>    &gt;&gt;&gt; v = torch.tensor([[3, 6, 9], [3, 6, 9]]) 
<a name="l13894"><span class="ln">13894 </span></a>    &gt;&gt;&gt; v 
<a name="l13895"><span class="ln">13895 </span></a>    tensor([[3, 6, 9], 
<a name="l13896"><span class="ln">13896 </span></a>            [3, 6, 9]]) 
<a name="l13897"><span class="ln">13897 </span></a>    &gt;&gt;&gt; torch.bucketize(v, boundaries) 
<a name="l13898"><span class="ln">13898 </span></a>    tensor([[1, 3, 4], 
<a name="l13899"><span class="ln">13899 </span></a>            [1, 3, 4]]) 
<a name="l13900"><span class="ln">13900 </span></a>    &gt;&gt;&gt; torch.bucketize(v, boundaries, right=True) 
<a name="l13901"><span class="ln">13901 </span></a>    tensor([[2, 3, 5], 
<a name="l13902"><span class="ln">13902 </span></a>            [2, 3, 5]]) 
<a name="l13903"><span class="ln">13903 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13904"><span class="ln">13904 </span></a><span class="s3">)</span>
<a name="l13905"><span class="ln">13905 </span></a>
<a name="l13906"><span class="ln">13906 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13907"><span class="ln">13907 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">view_as_real_copy</span><span class="s3">,</span>
<a name="l13908"><span class="ln">13908 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13909"><span class="ln">13909 </span></a>Performs the same operation as :func:`torch.view_as_real`, but all output tensors 
<a name="l13910"><span class="ln">13910 </span></a>are freshly created instead of aliasing the input. 
<a name="l13911"><span class="ln">13911 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13912"><span class="ln">13912 </span></a><span class="s3">)</span>
<a name="l13913"><span class="ln">13913 </span></a>
<a name="l13914"><span class="ln">13914 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13915"><span class="ln">13915 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">view_as_complex_copy</span><span class="s3">,</span>
<a name="l13916"><span class="ln">13916 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13917"><span class="ln">13917 </span></a>Performs the same operation as :func:`torch.view_as_complex`, but all output tensors 
<a name="l13918"><span class="ln">13918 </span></a>are freshly created instead of aliasing the input. 
<a name="l13919"><span class="ln">13919 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13920"><span class="ln">13920 </span></a><span class="s3">)</span>
<a name="l13921"><span class="ln">13921 </span></a>
<a name="l13922"><span class="ln">13922 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13923"><span class="ln">13923 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">as_strided_copy</span><span class="s3">,</span>
<a name="l13924"><span class="ln">13924 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13925"><span class="ln">13925 </span></a>Performs the same operation as :func:`torch.as_strided`, but all output tensors 
<a name="l13926"><span class="ln">13926 </span></a>are freshly created instead of aliasing the input. 
<a name="l13927"><span class="ln">13927 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13928"><span class="ln">13928 </span></a><span class="s3">)</span>
<a name="l13929"><span class="ln">13929 </span></a>
<a name="l13930"><span class="ln">13930 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13931"><span class="ln">13931 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">diagonal_copy</span><span class="s3">,</span>
<a name="l13932"><span class="ln">13932 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13933"><span class="ln">13933 </span></a>Performs the same operation as :func:`torch.diagonal`, but all output tensors 
<a name="l13934"><span class="ln">13934 </span></a>are freshly created instead of aliasing the input. 
<a name="l13935"><span class="ln">13935 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13936"><span class="ln">13936 </span></a><span class="s3">)</span>
<a name="l13937"><span class="ln">13937 </span></a>
<a name="l13938"><span class="ln">13938 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13939"><span class="ln">13939 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">expand_copy</span><span class="s3">,</span>
<a name="l13940"><span class="ln">13940 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13941"><span class="ln">13941 </span></a>Performs the same operation as :func:`torch.Tensor.expand`, but all output tensors 
<a name="l13942"><span class="ln">13942 </span></a>are freshly created instead of aliasing the input. 
<a name="l13943"><span class="ln">13943 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13944"><span class="ln">13944 </span></a><span class="s3">)</span>
<a name="l13945"><span class="ln">13945 </span></a>
<a name="l13946"><span class="ln">13946 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13947"><span class="ln">13947 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">permute_copy</span><span class="s3">,</span>
<a name="l13948"><span class="ln">13948 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13949"><span class="ln">13949 </span></a>Performs the same operation as :func:`torch.permute`, but all output tensors 
<a name="l13950"><span class="ln">13950 </span></a>are freshly created instead of aliasing the input. 
<a name="l13951"><span class="ln">13951 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13952"><span class="ln">13952 </span></a><span class="s3">)</span>
<a name="l13953"><span class="ln">13953 </span></a>
<a name="l13954"><span class="ln">13954 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13955"><span class="ln">13955 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">select_copy</span><span class="s3">,</span>
<a name="l13956"><span class="ln">13956 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13957"><span class="ln">13957 </span></a>Performs the same operation as :func:`torch.select`, but all output tensors 
<a name="l13958"><span class="ln">13958 </span></a>are freshly created instead of aliasing the input. 
<a name="l13959"><span class="ln">13959 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13960"><span class="ln">13960 </span></a><span class="s3">)</span>
<a name="l13961"><span class="ln">13961 </span></a>
<a name="l13962"><span class="ln">13962 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13963"><span class="ln">13963 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">detach_copy</span><span class="s3">,</span>
<a name="l13964"><span class="ln">13964 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13965"><span class="ln">13965 </span></a>Performs the same operation as :func:`torch.detach`, but all output tensors 
<a name="l13966"><span class="ln">13966 </span></a>are freshly created instead of aliasing the input. 
<a name="l13967"><span class="ln">13967 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13968"><span class="ln">13968 </span></a><span class="s3">)</span>
<a name="l13969"><span class="ln">13969 </span></a>
<a name="l13970"><span class="ln">13970 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13971"><span class="ln">13971 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">slice_copy</span><span class="s3">,</span>
<a name="l13972"><span class="ln">13972 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13973"><span class="ln">13973 </span></a>Performs the same operation as :func:`torch.slice`, but all output tensors 
<a name="l13974"><span class="ln">13974 </span></a>are freshly created instead of aliasing the input. 
<a name="l13975"><span class="ln">13975 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13976"><span class="ln">13976 </span></a><span class="s3">)</span>
<a name="l13977"><span class="ln">13977 </span></a>
<a name="l13978"><span class="ln">13978 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13979"><span class="ln">13979 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">split_copy</span><span class="s3">,</span>
<a name="l13980"><span class="ln">13980 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13981"><span class="ln">13981 </span></a>Performs the same operation as :func:`torch.split`, but all output tensors 
<a name="l13982"><span class="ln">13982 </span></a>are freshly created instead of aliasing the input. 
<a name="l13983"><span class="ln">13983 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13984"><span class="ln">13984 </span></a><span class="s3">)</span>
<a name="l13985"><span class="ln">13985 </span></a>
<a name="l13986"><span class="ln">13986 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13987"><span class="ln">13987 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">split_with_sizes_copy</span><span class="s3">,</span>
<a name="l13988"><span class="ln">13988 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13989"><span class="ln">13989 </span></a>Performs the same operation as :func:`torch.split_with_sizes`, but all output tensors 
<a name="l13990"><span class="ln">13990 </span></a>are freshly created instead of aliasing the input. 
<a name="l13991"><span class="ln">13991 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l13992"><span class="ln">13992 </span></a><span class="s3">)</span>
<a name="l13993"><span class="ln">13993 </span></a>
<a name="l13994"><span class="ln">13994 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l13995"><span class="ln">13995 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">squeeze_copy</span><span class="s3">,</span>
<a name="l13996"><span class="ln">13996 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l13997"><span class="ln">13997 </span></a>Performs the same operation as :func:`torch.squeeze`, but all output tensors 
<a name="l13998"><span class="ln">13998 </span></a>are freshly created instead of aliasing the input. 
<a name="l13999"><span class="ln">13999 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14000"><span class="ln">14000 </span></a><span class="s3">)</span>
<a name="l14001"><span class="ln">14001 </span></a>
<a name="l14002"><span class="ln">14002 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14003"><span class="ln">14003 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">t_copy</span><span class="s3">,</span>
<a name="l14004"><span class="ln">14004 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14005"><span class="ln">14005 </span></a>Performs the same operation as :func:`torch.t`, but all output tensors 
<a name="l14006"><span class="ln">14006 </span></a>are freshly created instead of aliasing the input. 
<a name="l14007"><span class="ln">14007 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14008"><span class="ln">14008 </span></a><span class="s3">)</span>
<a name="l14009"><span class="ln">14009 </span></a>
<a name="l14010"><span class="ln">14010 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14011"><span class="ln">14011 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">transpose_copy</span><span class="s3">,</span>
<a name="l14012"><span class="ln">14012 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14013"><span class="ln">14013 </span></a>Performs the same operation as :func:`torch.transpose`, but all output tensors 
<a name="l14014"><span class="ln">14014 </span></a>are freshly created instead of aliasing the input. 
<a name="l14015"><span class="ln">14015 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14016"><span class="ln">14016 </span></a><span class="s3">)</span>
<a name="l14017"><span class="ln">14017 </span></a>
<a name="l14018"><span class="ln">14018 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14019"><span class="ln">14019 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unsqueeze_copy</span><span class="s3">,</span>
<a name="l14020"><span class="ln">14020 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14021"><span class="ln">14021 </span></a>Performs the same operation as :func:`torch.unsqueeze`, but all output tensors 
<a name="l14022"><span class="ln">14022 </span></a>are freshly created instead of aliasing the input. 
<a name="l14023"><span class="ln">14023 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14024"><span class="ln">14024 </span></a><span class="s3">)</span>
<a name="l14025"><span class="ln">14025 </span></a>
<a name="l14026"><span class="ln">14026 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14027"><span class="ln">14027 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">indices_copy</span><span class="s3">,</span>
<a name="l14028"><span class="ln">14028 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14029"><span class="ln">14029 </span></a>Performs the same operation as :func:`torch.indices`, but all output tensors 
<a name="l14030"><span class="ln">14030 </span></a>are freshly created instead of aliasing the input. 
<a name="l14031"><span class="ln">14031 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14032"><span class="ln">14032 </span></a><span class="s3">)</span>
<a name="l14033"><span class="ln">14033 </span></a>
<a name="l14034"><span class="ln">14034 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14035"><span class="ln">14035 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">values_copy</span><span class="s3">,</span>
<a name="l14036"><span class="ln">14036 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14037"><span class="ln">14037 </span></a>Performs the same operation as :func:`torch.values`, but all output tensors 
<a name="l14038"><span class="ln">14038 </span></a>are freshly created instead of aliasing the input. 
<a name="l14039"><span class="ln">14039 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14040"><span class="ln">14040 </span></a><span class="s3">)</span>
<a name="l14041"><span class="ln">14041 </span></a>
<a name="l14042"><span class="ln">14042 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14043"><span class="ln">14043 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">crow_indices_copy</span><span class="s3">,</span>
<a name="l14044"><span class="ln">14044 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14045"><span class="ln">14045 </span></a>Performs the same operation as :func:`torch.crow_indices`, but all output tensors 
<a name="l14046"><span class="ln">14046 </span></a>are freshly created instead of aliasing the input. 
<a name="l14047"><span class="ln">14047 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14048"><span class="ln">14048 </span></a><span class="s3">)</span>
<a name="l14049"><span class="ln">14049 </span></a>
<a name="l14050"><span class="ln">14050 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14051"><span class="ln">14051 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">col_indices_copy</span><span class="s3">,</span>
<a name="l14052"><span class="ln">14052 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14053"><span class="ln">14053 </span></a>Performs the same operation as :func:`torch.col_indices`, but all output tensors 
<a name="l14054"><span class="ln">14054 </span></a>are freshly created instead of aliasing the input. 
<a name="l14055"><span class="ln">14055 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14056"><span class="ln">14056 </span></a><span class="s3">)</span>
<a name="l14057"><span class="ln">14057 </span></a>
<a name="l14058"><span class="ln">14058 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14059"><span class="ln">14059 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unbind_copy</span><span class="s3">,</span>
<a name="l14060"><span class="ln">14060 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14061"><span class="ln">14061 </span></a>Performs the same operation as :func:`torch.unbind`, but all output tensors 
<a name="l14062"><span class="ln">14062 </span></a>are freshly created instead of aliasing the input. 
<a name="l14063"><span class="ln">14063 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14064"><span class="ln">14064 </span></a><span class="s3">)</span>
<a name="l14065"><span class="ln">14065 </span></a>
<a name="l14066"><span class="ln">14066 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14067"><span class="ln">14067 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">view_copy</span><span class="s3">,</span>
<a name="l14068"><span class="ln">14068 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14069"><span class="ln">14069 </span></a>Performs the same operation as :func:`torch.view`, but all output tensors 
<a name="l14070"><span class="ln">14070 </span></a>are freshly created instead of aliasing the input. 
<a name="l14071"><span class="ln">14071 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14072"><span class="ln">14072 </span></a><span class="s3">)</span>
<a name="l14073"><span class="ln">14073 </span></a>
<a name="l14074"><span class="ln">14074 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14075"><span class="ln">14075 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">unfold_copy</span><span class="s3">,</span>
<a name="l14076"><span class="ln">14076 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14077"><span class="ln">14077 </span></a>Performs the same operation as :func:`torch.unfold`, but all output tensors 
<a name="l14078"><span class="ln">14078 </span></a>are freshly created instead of aliasing the input. 
<a name="l14079"><span class="ln">14079 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14080"><span class="ln">14080 </span></a><span class="s3">)</span>
<a name="l14081"><span class="ln">14081 </span></a>
<a name="l14082"><span class="ln">14082 </span></a><span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14083"><span class="ln">14083 </span></a>    <span class="s1">torch</span><span class="s3">.</span><span class="s1">alias_copy</span><span class="s3">,</span>
<a name="l14084"><span class="ln">14084 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l14085"><span class="ln">14085 </span></a>Performs the same operation as :func:`torch.alias`, but all output tensors 
<a name="l14086"><span class="ln">14086 </span></a>are freshly created instead of aliasing the input. 
<a name="l14087"><span class="ln">14087 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14088"><span class="ln">14088 </span></a><span class="s3">)</span>
<a name="l14089"><span class="ln">14089 </span></a>
<a name="l14090"><span class="ln">14090 </span></a><span class="s2">for </span><span class="s1">unary_base_func_name </span><span class="s2">in </span><span class="s3">(</span>
<a name="l14091"><span class="ln">14091 </span></a>    <span class="s4">&quot;exp&quot;</span><span class="s3">,</span>
<a name="l14092"><span class="ln">14092 </span></a>    <span class="s4">&quot;sqrt&quot;</span><span class="s3">,</span>
<a name="l14093"><span class="ln">14093 </span></a>    <span class="s4">&quot;abs&quot;</span><span class="s3">,</span>
<a name="l14094"><span class="ln">14094 </span></a>    <span class="s4">&quot;acos&quot;</span><span class="s3">,</span>
<a name="l14095"><span class="ln">14095 </span></a>    <span class="s4">&quot;asin&quot;</span><span class="s3">,</span>
<a name="l14096"><span class="ln">14096 </span></a>    <span class="s4">&quot;atan&quot;</span><span class="s3">,</span>
<a name="l14097"><span class="ln">14097 </span></a>    <span class="s4">&quot;ceil&quot;</span><span class="s3">,</span>
<a name="l14098"><span class="ln">14098 </span></a>    <span class="s4">&quot;cos&quot;</span><span class="s3">,</span>
<a name="l14099"><span class="ln">14099 </span></a>    <span class="s4">&quot;cosh&quot;</span><span class="s3">,</span>
<a name="l14100"><span class="ln">14100 </span></a>    <span class="s4">&quot;erf&quot;</span><span class="s3">,</span>
<a name="l14101"><span class="ln">14101 </span></a>    <span class="s4">&quot;erfc&quot;</span><span class="s3">,</span>
<a name="l14102"><span class="ln">14102 </span></a>    <span class="s4">&quot;expm1&quot;</span><span class="s3">,</span>
<a name="l14103"><span class="ln">14103 </span></a>    <span class="s4">&quot;floor&quot;</span><span class="s3">,</span>
<a name="l14104"><span class="ln">14104 </span></a>    <span class="s4">&quot;log&quot;</span><span class="s3">,</span>
<a name="l14105"><span class="ln">14105 </span></a>    <span class="s4">&quot;log10&quot;</span><span class="s3">,</span>
<a name="l14106"><span class="ln">14106 </span></a>    <span class="s4">&quot;log1p&quot;</span><span class="s3">,</span>
<a name="l14107"><span class="ln">14107 </span></a>    <span class="s4">&quot;log2&quot;</span><span class="s3">,</span>
<a name="l14108"><span class="ln">14108 </span></a>    <span class="s4">&quot;neg&quot;</span><span class="s3">,</span>
<a name="l14109"><span class="ln">14109 </span></a>    <span class="s4">&quot;tan&quot;</span><span class="s3">,</span>
<a name="l14110"><span class="ln">14110 </span></a>    <span class="s4">&quot;tanh&quot;</span><span class="s3">,</span>
<a name="l14111"><span class="ln">14111 </span></a>    <span class="s4">&quot;sin&quot;</span><span class="s3">,</span>
<a name="l14112"><span class="ln">14112 </span></a>    <span class="s4">&quot;sinh&quot;</span><span class="s3">,</span>
<a name="l14113"><span class="ln">14113 </span></a>    <span class="s4">&quot;round&quot;</span><span class="s3">,</span>
<a name="l14114"><span class="ln">14114 </span></a>    <span class="s4">&quot;lgamma&quot;</span><span class="s3">,</span>
<a name="l14115"><span class="ln">14115 </span></a>    <span class="s4">&quot;frac&quot;</span><span class="s3">,</span>
<a name="l14116"><span class="ln">14116 </span></a>    <span class="s4">&quot;reciprocal&quot;</span><span class="s3">,</span>
<a name="l14117"><span class="ln">14117 </span></a>    <span class="s4">&quot;sigmoid&quot;</span><span class="s3">,</span>
<a name="l14118"><span class="ln">14118 </span></a>    <span class="s4">&quot;trunc&quot;</span><span class="s3">,</span>
<a name="l14119"><span class="ln">14119 </span></a>    <span class="s4">&quot;zero&quot;</span><span class="s3">,</span>
<a name="l14120"><span class="ln">14120 </span></a><span class="s3">)</span><span class="s2">:</span>
<a name="l14121"><span class="ln">14121 </span></a>    <span class="s1">unary_foreach_func_name </span><span class="s2">= </span><span class="s4">f&quot;_foreach_{</span><span class="s1">unary_base_func_name</span><span class="s4">}&quot;</span>
<a name="l14122"><span class="ln">14122 </span></a>    <span class="s2">if </span><span class="s1">hasattr</span><span class="s3">(</span><span class="s1">torch</span><span class="s3">, </span><span class="s1">unary_foreach_func_name</span><span class="s3">)</span><span class="s2">:</span>
<a name="l14123"><span class="ln">14123 </span></a>        <span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14124"><span class="ln">14124 </span></a>            <span class="s1">getattr</span><span class="s3">(</span><span class="s1">torch</span><span class="s3">, </span><span class="s1">unary_foreach_func_name</span><span class="s3">),</span>
<a name="l14125"><span class="ln">14125 </span></a>            <span class="s4">rf&quot;&quot;&quot;</span>
<a name="l14126"><span class="ln">14126 </span></a><span class="s4">{</span><span class="s1">unary_foreach_func_name</span><span class="s4">}(self: List[Tensor]) -&gt; List[Tensor]</span>
<a name="l14127"><span class="ln">14127 </span></a>
<a name="l14128"><span class="ln">14128 </span></a><span class="s4">Apply :func:`torch.{</span><span class="s1">unary_base_func_name</span><span class="s4">}` to each Tensor of the input list.</span>
<a name="l14129"><span class="ln">14129 </span></a>            <span class="s4">&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14130"><span class="ln">14130 </span></a>        <span class="s3">)</span>
<a name="l14131"><span class="ln">14131 </span></a>    <span class="s1">unary_inplace_foreach_func_name </span><span class="s2">= </span><span class="s4">f&quot;{</span><span class="s1">unary_foreach_func_name</span><span class="s4">}_&quot;</span>
<a name="l14132"><span class="ln">14132 </span></a>    <span class="s2">if </span><span class="s1">hasattr</span><span class="s3">(</span><span class="s1">torch</span><span class="s3">, </span><span class="s1">unary_inplace_foreach_func_name</span><span class="s3">)</span><span class="s2">:</span>
<a name="l14133"><span class="ln">14133 </span></a>        <span class="s1">add_docstr</span><span class="s3">(</span>
<a name="l14134"><span class="ln">14134 </span></a>            <span class="s1">getattr</span><span class="s3">(</span><span class="s1">torch</span><span class="s3">, </span><span class="s1">unary_inplace_foreach_func_name</span><span class="s3">),</span>
<a name="l14135"><span class="ln">14135 </span></a>            <span class="s4">rf&quot;&quot;&quot;</span>
<a name="l14136"><span class="ln">14136 </span></a><span class="s4">{</span><span class="s1">unary_inplace_foreach_func_name</span><span class="s4">}(self: List[Tensor]) -&gt; None</span>
<a name="l14137"><span class="ln">14137 </span></a>
<a name="l14138"><span class="ln">14138 </span></a><span class="s4">Apply :func:`torch.{</span><span class="s1">unary_base_func_name</span><span class="s4">}` to each Tensor of the input list.</span>
<a name="l14139"><span class="ln">14139 </span></a>        <span class="s4">&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l14140"><span class="ln">14140 </span></a>        <span class="s3">)</span>
<a name="l14141"><span class="ln">14141 </span></a></pre>
</body>
</html>