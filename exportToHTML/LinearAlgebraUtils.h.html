<html>
<head>
<title>LinearAlgebraUtils.h</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #333333;}
.s1 { color: #000080; font-weight: bold;}
.s2 { color: #660e7a; font-weight: bold;}
.s3 { color: #969896; font-style: italic;}
.s4 { color: #0086b3;}
.s5 { color: #183691; font-weight: bold;}
.ln { color: #333333; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
LinearAlgebraUtils.h</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">#pragma once</span>
<a name="l2"><span class="ln">2    </span></a>
<a name="l3"><span class="ln">3    </span></a><span class="s1">#include </span><span class="s0">&lt;c10/core/ScalarType.h&gt;</span>
<a name="l4"><span class="ln">4    </span></a><span class="s1">#include </span><span class="s0">&lt;c10/util/irange.h&gt;</span>
<a name="l5"><span class="ln">5    </span></a><span class="s1">#include </span><span class="s0">&lt;c10/util/Exception.h&gt;</span>
<a name="l6"><span class="ln">6    </span></a><span class="s1">#include </span><span class="s0">&lt;c10/util/strides.h&gt;</span>
<a name="l7"><span class="ln">7    </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/core/Tensor.h&gt;</span>
<a name="l8"><span class="ln">8    </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/ExpandUtils.h&gt;</span>
<a name="l9"><span class="ln">9    </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/TensorUtils.h&gt;</span>
<a name="l10"><span class="ln">10   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/native/TensorIterator.h&gt;</span>
<a name="l11"><span class="ln">11   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/native/TransposeType.h&gt;</span>
<a name="l12"><span class="ln">12   </span></a><span class="s1">#include </span><span class="s0">&lt;limits&gt;</span>
<a name="l13"><span class="ln">13   </span></a><span class="s1">#include </span><span class="s0">&lt;type_traits&gt;</span>
<a name="l14"><span class="ln">14   </span></a><span class="s1">#include </span><span class="s0">&lt;sstream&gt;</span>
<a name="l15"><span class="ln">15   </span></a><span class="s1">#include </span><span class="s0">&lt;cstring&gt;</span>
<a name="l16"><span class="ln">16   </span></a><span class="s1">#include </span><span class="s0">&lt;cctype&gt;</span>
<a name="l17"><span class="ln">17   </span></a>
<a name="l18"><span class="ln">18   </span></a><span class="s1">#ifndef </span><span class="s0">AT_PER_OPERATOR_HEADERS</span>
<a name="l19"><span class="ln">19   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/Functions.h&gt;</span>
<a name="l20"><span class="ln">20   </span></a><span class="s1">#else</span>
<a name="l21"><span class="ln">21   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/ops/arange.h&gt;</span>
<a name="l22"><span class="ln">22   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/ops/empty.h&gt;</span>
<a name="l23"><span class="ln">23   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/ops/empty_like.h&gt;</span>
<a name="l24"><span class="ln">24   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/ops/empty_strided.h&gt;</span>
<a name="l25"><span class="ln">25   </span></a><span class="s1">#include </span><span class="s0">&lt;ATen/ops/zeros.h&gt;</span>
<a name="l26"><span class="ln">26   </span></a><span class="s1">#endif</span>
<a name="l27"><span class="ln">27   </span></a>
<a name="l28"><span class="ln">28   </span></a><span class="s2">namespace </span><span class="s0">at::native {</span>
<a name="l29"><span class="ln">29   </span></a>
<a name="l30"><span class="ln">30   </span></a><span class="s2">inline </span><span class="s0">c10::MaybeOwned&lt;Tensor&gt; expect_resolved_conj(</span><span class="s1">const </span><span class="s0">Tensor&amp; tensor) {</span>
<a name="l31"><span class="ln">31   </span></a>  <span class="s1">if </span><span class="s0">(tensor.is_conj()) {</span>
<a name="l32"><span class="ln">32   </span></a>    <span class="s1">return </span><span class="s0">c10::MaybeOwned&lt;Tensor&gt;::owned(tensor.resolve_conj());</span>
<a name="l33"><span class="ln">33   </span></a>  <span class="s0">} </span><span class="s1">else </span><span class="s0">{</span>
<a name="l34"><span class="ln">34   </span></a>    <span class="s1">return </span><span class="s0">c10::MaybeOwned&lt;Tensor&gt;::borrowed(tensor);</span>
<a name="l35"><span class="ln">35   </span></a>  <span class="s0">}</span>
<a name="l36"><span class="ln">36   </span></a><span class="s0">}</span>
<a name="l37"><span class="ln">37   </span></a>
<a name="l38"><span class="ln">38   </span></a><span class="s2">inline </span><span class="s0">DimVector batched_matrix_contiguous_strides(</span>
<a name="l39"><span class="ln">39   </span></a>    <span class="s1">const </span><span class="s0">IntArrayRef sizes,</span>
<a name="l40"><span class="ln">40   </span></a>    <span class="s1">const bool </span><span class="s0">f_contig = </span><span class="s1">false</span><span class="s0">) {</span>
<a name="l41"><span class="ln">41   </span></a>  <span class="s3">// f_contig chooses between the strides of a batch of Fortran (F-contiguous)</span>
<a name="l42"><span class="ln">42   </span></a>  <span class="s3">// and C-contiguous matrices</span>
<a name="l43"><span class="ln">43   </span></a>  <span class="s1">auto </span><span class="s0">strides = c10::contiguous_strides(sizes);</span>
<a name="l44"><span class="ln">44   </span></a>  <span class="s1">auto </span><span class="s0">dim = strides.size();</span>
<a name="l45"><span class="ln">45   </span></a>
<a name="l46"><span class="ln">46   </span></a>  <span class="s1">if </span><span class="s0">(f_contig &amp;&amp; dim &gt;= </span><span class="s4">2</span><span class="s0">) {</span>
<a name="l47"><span class="ln">47   </span></a>    <span class="s3">// Fix the strides of the last two dimensions, so that we return</span>
<a name="l48"><span class="ln">48   </span></a>    <span class="s3">// C-contiguous batches of F-contiguous matrices.</span>
<a name="l49"><span class="ln">49   </span></a>    <span class="s0">strides[dim - </span><span class="s4">1</span><span class="s0">] = std::max(sizes[dim - </span><span class="s4">2</span><span class="s0">], </span><span class="s2">static_cast</span><span class="s0">&lt;int64_t&gt;(</span><span class="s4">1</span><span class="s0">));</span>
<a name="l50"><span class="ln">50   </span></a>    <span class="s0">strides[dim - </span><span class="s4">2</span><span class="s0">] = </span><span class="s4">1</span><span class="s0">;</span>
<a name="l51"><span class="ln">51   </span></a>  <span class="s0">}</span>
<a name="l52"><span class="ln">52   </span></a>  <span class="s1">return </span><span class="s0">strides;</span>
<a name="l53"><span class="ln">53   </span></a><span class="s0">}</span>
<a name="l54"><span class="ln">54   </span></a>
<a name="l55"><span class="ln">55   </span></a><span class="s3">/* 
<a name="l56"><span class="ln">56   </span></a> * Clones a Tensor so that the following conditions hold: 
<a name="l57"><span class="ln">57   </span></a> * If we think of a Tensor of having size (B, M, N), where B is any number 
<a name="l58"><span class="ln">58   </span></a> * of batch dimensions, then: 
<a name="l59"><span class="ln">59   </span></a> * - Each (M, N) matrix is in column major form 
<a name="l60"><span class="ln">60   </span></a> * - Let Tensor P have size (B, M, N) and Q have size (B, M', N'). 
<a name="l61"><span class="ln">61   </span></a> *   Then when laid out in memory, the M by N matrix starting at 
<a name="l62"><span class="ln">62   </span></a> *   P.data_ptr()[B * M * N] is of the same corresponding batch as the M' by N' 
<a name="l63"><span class="ln">63   </span></a> *   matrix starting at Q.data_ptr()[B * M' * N']. 
<a name="l64"><span class="ln">64   </span></a> */</span>
<a name="l65"><span class="ln">65   </span></a><span class="s2">inline </span><span class="s0">Tensor cloneBatchedColumnMajor(</span><span class="s1">const </span><span class="s0">Tensor&amp; src) {</span>
<a name="l66"><span class="ln">66   </span></a>  <span class="s3">// If src is already in batched column major format, then</span>
<a name="l67"><span class="ln">67   </span></a>  <span class="s3">// this will be efficient (no reordering of the data will occur)</span>
<a name="l68"><span class="ln">68   </span></a>  <span class="s3">// because the first transpose will make the tensor contiguous,</span>
<a name="l69"><span class="ln">69   </span></a>  <span class="s3">// and cloning a contiguous tensor is fast.</span>
<a name="l70"><span class="ln">70   </span></a>  <span class="s1">auto </span><span class="s0">result = src.mT().clone(at::MemoryFormat::Contiguous);</span>
<a name="l71"><span class="ln">71   </span></a>  <span class="s0">result.transpose_(-</span><span class="s4">2</span><span class="s0">, -</span><span class="s4">1</span><span class="s0">);</span>
<a name="l72"><span class="ln">72   </span></a>  <span class="s1">return </span><span class="s0">result;</span>
<a name="l73"><span class="ln">73   </span></a><span class="s0">}</span>
<a name="l74"><span class="ln">74   </span></a>
<a name="l75"><span class="ln">75   </span></a><span class="s3">/* 
<a name="l76"><span class="ln">76   </span></a> * contig chooses between C-contig (true) and F-contig (false) 
<a name="l77"><span class="ln">77   </span></a> */</span>
<a name="l78"><span class="ln">78   </span></a><span class="s2">inline </span><span class="s0">c10::MaybeOwned&lt;Tensor&gt; borrow_else_clone(</span><span class="s1">const bool </span><span class="s0">cond, </span><span class="s1">const </span><span class="s0">Tensor&amp; borrow, </span><span class="s1">const </span><span class="s0">Tensor&amp; clone, </span><span class="s1">const bool </span><span class="s0">contig) {</span>
<a name="l79"><span class="ln">79   </span></a>  <span class="s1">return </span><span class="s0">cond ? c10::MaybeOwned&lt;Tensor&gt;::borrowed(borrow)</span>
<a name="l80"><span class="ln">80   </span></a>              <span class="s0">: c10::MaybeOwned&lt;Tensor&gt;::owned(contig ? clone.clone(MemoryFormat::Contiguous)</span>
<a name="l81"><span class="ln">81   </span></a>                                                      <span class="s0">: cloneBatchedColumnMajor(clone));</span>
<a name="l82"><span class="ln">82   </span></a><span class="s0">}</span>
<a name="l83"><span class="ln">83   </span></a>
<a name="l84"><span class="ln">84   </span></a><span class="s3">/* 
<a name="l85"><span class="ln">85   </span></a> * This method is designed to be a faster alternative to 
<a name="l86"><span class="ln">86   </span></a> * `cloneBatchedColumnMajor` with some additional features, 
<a name="l87"><span class="ln">87   </span></a> * namely: 
<a name="l88"><span class="ln">88   </span></a> * 1. It uses `copy` instead of `clone` which could be much faster. 
<a name="l89"><span class="ln">89   </span></a> * 2. `nrows` parameter used to create inputs with the number of rows larger 
<a name="l90"><span class="ln">90   </span></a> *  than the original input, which is required for some LAPACK/MAGMA methods. 
<a name="l91"><span class="ln">91   </span></a> * 3. `desired_batch_size` is used to create copies with the batch size 
<a name="l92"><span class="ln">92   </span></a> *  which is either the original batch size of the input, or its larger 
<a name="l93"><span class="ln">93   </span></a> *  broadcasted shape. 
<a name="l94"><span class="ln">94   </span></a> */</span>
<a name="l95"><span class="ln">95   </span></a><span class="s2">inline </span><span class="s0">Tensor copyBatchedColumnMajor(</span><span class="s1">const </span><span class="s0">Tensor&amp; src, int64_t nrows = -</span><span class="s4">1</span><span class="s0">,</span>
<a name="l96"><span class="ln">96   </span></a>    <span class="s0">at::OptionalIntArrayRef desired_batch_sizes = std::nullopt) {</span>
<a name="l97"><span class="ln">97   </span></a>  <span class="s0">nrows = (nrows == -</span><span class="s4">1</span><span class="s0">) ? src.size(-</span><span class="s4">2</span><span class="s0">) : nrows;</span>
<a name="l98"><span class="ln">98   </span></a>  <span class="s1">auto </span><span class="s0">copy_sizes = desired_batch_sizes.has_value()</span>
<a name="l99"><span class="ln">99   </span></a>    <span class="s0">? desired_batch_sizes.value().vec()</span>
<a name="l100"><span class="ln">100  </span></a>    <span class="s0">: IntArrayRef(src.sizes().data(), src.dim() - </span><span class="s4">2</span><span class="s0">).vec();</span>
<a name="l101"><span class="ln">101  </span></a>  <span class="s0">copy_sizes.insert(copy_sizes.end(), {nrows, src.size(-</span><span class="s4">1</span><span class="s0">)});</span>
<a name="l102"><span class="ln">102  </span></a>  <span class="s1">const auto </span><span class="s0">copy_strides = batched_matrix_contiguous_strides(copy_sizes, </span><span class="s3">/*f-contig*/</span><span class="s2">true</span><span class="s0">);</span>
<a name="l103"><span class="ln">103  </span></a>  <span class="s1">auto </span><span class="s0">copy = at::empty_strided(copy_sizes, copy_strides, src.options());</span>
<a name="l104"><span class="ln">104  </span></a>  <span class="s0">copy.narrow(-</span><span class="s4">2</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, src.size(-</span><span class="s4">2</span><span class="s0">)).copy_(src);</span>
<a name="l105"><span class="ln">105  </span></a>  <span class="s1">return </span><span class="s0">copy;</span>
<a name="l106"><span class="ln">106  </span></a><span class="s0">}</span>
<a name="l107"><span class="ln">107  </span></a>
<a name="l108"><span class="ln">108  </span></a><span class="s3">/* 
<a name="l109"><span class="ln">109  </span></a> * Given batches of matrices with arbitrary batch dim, 
<a name="l110"><span class="ln">110  </span></a> * computes the number of batches. 
<a name="l111"><span class="ln">111  </span></a> */</span>
<a name="l112"><span class="ln">112  </span></a><span class="s2">inline </span><span class="s0">int64_t batchCount(</span><span class="s1">const </span><span class="s0">Tensor&amp; batched_matrices) {</span>
<a name="l113"><span class="ln">113  </span></a>  <span class="s0">int64_t result = </span><span class="s4">1</span><span class="s0">;</span>
<a name="l114"><span class="ln">114  </span></a>  <span class="s1">for </span><span class="s0">(int64_t i = </span><span class="s4">0</span><span class="s0">; i &lt; batched_matrices.ndimension() - </span><span class="s4">2</span><span class="s0">; i++) {</span>
<a name="l115"><span class="ln">115  </span></a>    <span class="s0">result *= batched_matrices.size(i);</span>
<a name="l116"><span class="ln">116  </span></a>  <span class="s0">}</span>
<a name="l117"><span class="ln">117  </span></a>  <span class="s1">return </span><span class="s0">result;</span>
<a name="l118"><span class="ln">118  </span></a><span class="s0">}</span>
<a name="l119"><span class="ln">119  </span></a>
<a name="l120"><span class="ln">120  </span></a><span class="s3">// Computes the number of elements of a matrix in a batched matrix tensor</span>
<a name="l121"><span class="ln">121  </span></a><span class="s2">inline </span><span class="s0">int64_t matrixStride(</span><span class="s1">const </span><span class="s0">Tensor&amp; batched_matrices) {</span>
<a name="l122"><span class="ln">122  </span></a>  <span class="s1">return </span><span class="s0">batched_matrices.size(-</span><span class="s4">1</span><span class="s0">) * batched_matrices.size(-</span><span class="s4">2</span><span class="s0">);</span>
<a name="l123"><span class="ln">123  </span></a><span class="s0">}</span>
<a name="l124"><span class="ln">124  </span></a>
<a name="l125"><span class="ln">125  </span></a><span class="s3">// Validates input shapes for operations on batches of square matrices (inverse, cholesky, symeig, eig)</span>
<a name="l126"><span class="ln">126  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkIsMatrix(</span><span class="s1">const </span><span class="s0">Tensor&amp; A, </span><span class="s1">const char</span><span class="s0">* </span><span class="s1">const </span><span class="s0">f_name, </span><span class="s1">const char</span><span class="s0">* </span><span class="s1">const </span><span class="s0">arg_name = </span><span class="s5">&quot;A&quot;</span><span class="s0">) {</span>
<a name="l127"><span class="ln">127  </span></a>  <span class="s0">TORCH_CHECK(A.dim() &gt;= </span><span class="s4">2</span><span class="s0">, f_name, </span><span class="s5">&quot;: The input tensor &quot;</span><span class="s0">, arg_name, </span><span class="s5">&quot; must have at least 2 dimensions.&quot;</span><span class="s0">);</span>
<a name="l128"><span class="ln">128  </span></a><span class="s0">}</span>
<a name="l129"><span class="ln">129  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">squareCheckInputs(</span><span class="s1">const </span><span class="s0">Tensor&amp; self, </span><span class="s1">const char</span><span class="s0">* </span><span class="s1">const </span><span class="s0">f_name, </span><span class="s1">const char</span><span class="s0">* </span><span class="s1">const </span><span class="s0">arg_name = </span><span class="s5">&quot;A&quot;</span><span class="s0">) {</span>
<a name="l130"><span class="ln">130  </span></a>  <span class="s0">checkIsMatrix(self, f_name, arg_name);</span>
<a name="l131"><span class="ln">131  </span></a>  <span class="s0">TORCH_CHECK(self.sym_size(-</span><span class="s4">1</span><span class="s0">) == self.sym_size(-</span><span class="s4">2</span><span class="s0">),</span>
<a name="l132"><span class="ln">132  </span></a>              <span class="s0">f_name,</span>
<a name="l133"><span class="ln">133  </span></a>              <span class="s5">&quot;: &quot;</span><span class="s0">, arg_name, </span><span class="s5">&quot; must be batches of square matrices, &quot;</span>
<a name="l134"><span class="ln">134  </span></a>              <span class="s5">&quot;but they are &quot;</span><span class="s0">, self.sym_size(-</span><span class="s4">2</span><span class="s0">), </span><span class="s5">&quot; by &quot;</span><span class="s0">, self.sym_size(-</span><span class="s4">1</span><span class="s0">), </span><span class="s5">&quot; matrices&quot;</span><span class="s0">);</span>
<a name="l135"><span class="ln">135  </span></a><span class="s0">}</span>
<a name="l136"><span class="ln">136  </span></a>
<a name="l137"><span class="ln">137  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkInputsSolver(</span><span class="s1">const </span><span class="s0">Tensor&amp; A,</span>
<a name="l138"><span class="ln">138  </span></a>                                     <span class="s1">const </span><span class="s0">Tensor&amp; B,</span>
<a name="l139"><span class="ln">139  </span></a>                                     <span class="s1">const bool </span><span class="s0">left,</span>
<a name="l140"><span class="ln">140  </span></a>                                     <span class="s1">const char</span><span class="s0">* </span><span class="s1">const </span><span class="s0">f_name) {</span>
<a name="l141"><span class="ln">141  </span></a>  <span class="s0">squareCheckInputs(A, f_name, </span><span class="s5">&quot;A&quot;</span><span class="s0">);</span>
<a name="l142"><span class="ln">142  </span></a>  <span class="s0">checkIsMatrix(B, f_name, </span><span class="s5">&quot;B&quot;</span><span class="s0">);</span>
<a name="l143"><span class="ln">143  </span></a>  <span class="s0">TORCH_CHECK(left ? A.size(-</span><span class="s4">2</span><span class="s0">) == B.size(-</span><span class="s4">2</span><span class="s0">) : A.size(-</span><span class="s4">1</span><span class="s0">) == B.size(-</span><span class="s4">1</span><span class="s0">),</span>
<a name="l144"><span class="ln">144  </span></a>              <span class="s0">f_name, </span><span class="s5">&quot;: Incompatible shapes of A and B for the equation &quot;</span><span class="s0">,</span>
<a name="l145"><span class="ln">145  </span></a>              <span class="s0">left ? </span><span class="s5">&quot;AX = B&quot; </span><span class="s0">: </span><span class="s5">&quot;XA = B&quot;</span><span class="s0">,</span>
<a name="l146"><span class="ln">146  </span></a>              <span class="s5">&quot; (&quot;</span><span class="s0">, A.size(-</span><span class="s4">2</span><span class="s0">), </span><span class="s5">&quot;x&quot;</span><span class="s0">, A.size(-</span><span class="s4">1</span><span class="s0">), </span><span class="s5">&quot; and &quot;</span><span class="s0">, B.size(-</span><span class="s4">2</span><span class="s0">), </span><span class="s5">&quot;x&quot;</span><span class="s0">, B.size(-</span><span class="s4">1</span><span class="s0">), </span><span class="s5">&quot;)&quot;</span><span class="s0">);</span>
<a name="l147"><span class="ln">147  </span></a><span class="s0">}</span>
<a name="l148"><span class="ln">148  </span></a>
<a name="l149"><span class="ln">149  </span></a><span class="s2">inline </span><span class="s1">bool </span><span class="s0">is_row_or_column_contiguous(</span><span class="s1">const </span><span class="s0">Tensor&amp; t) {</span>
<a name="l150"><span class="ln">150  </span></a>  <span class="s3">// This could be made more general, similar to how it's checked in matmul, which would allow to</span>
<a name="l151"><span class="ln">151  </span></a>  <span class="s3">// ellide the copy with strides such as (6, 12, 1, 3) or (3, 1, 9), but this is quite tricky.</span>
<a name="l152"><span class="ln">152  </span></a>  <span class="s3">// We choose to be conservative for simplicity</span>
<a name="l153"><span class="ln">153  </span></a>  <span class="s1">return </span><span class="s0">t.is_contiguous() || t.transpose(-</span><span class="s4">2</span><span class="s0">, -</span><span class="s4">1</span><span class="s0">).is_contiguous();</span>
<a name="l154"><span class="ln">154  </span></a><span class="s0">}</span>
<a name="l155"><span class="ln">155  </span></a>
<a name="l156"><span class="ln">156  </span></a><span class="s2">inline </span><span class="s0">TransposeType to_transpose_type(</span><span class="s1">const bool </span><span class="s0">contig, </span><span class="s1">const bool </span><span class="s0">conj) {</span>
<a name="l157"><span class="ln">157  </span></a>  <span class="s1">if </span><span class="s0">(conj) {</span>
<a name="l158"><span class="ln">158  </span></a>    <span class="s1">if </span><span class="s0">(contig) { TORCH_INTERNAL_ASSERT(</span><span class="s1">false</span><span class="s0">, </span><span class="s5">&quot;Invalid transpose type&quot;</span><span class="s0">); }</span>
<a name="l159"><span class="ln">159  </span></a>    <span class="s1">else </span><span class="s0">{        </span><span class="s1">return </span><span class="s0">TransposeType::ConjTranspose; }</span>
<a name="l160"><span class="ln">160  </span></a>  <span class="s0">} </span><span class="s1">else </span><span class="s0">{</span>
<a name="l161"><span class="ln">161  </span></a>    <span class="s1">if </span><span class="s0">(contig) { </span><span class="s1">return </span><span class="s0">TransposeType::NoTranspose; }</span>
<a name="l162"><span class="ln">162  </span></a>    <span class="s1">else </span><span class="s0">{        </span><span class="s1">return </span><span class="s0">TransposeType::Transpose; }</span>
<a name="l163"><span class="ln">163  </span></a>  <span class="s0">}</span>
<a name="l164"><span class="ln">164  </span></a><span class="s0">}</span>
<a name="l165"><span class="ln">165  </span></a>
<a name="l166"><span class="ln">166  </span></a>
<a name="l167"><span class="ln">167  </span></a><span class="s3">// This function is designed to be used with linear algebra methods that minimize</span>
<a name="l168"><span class="ln">168  </span></a><span class="s3">// L(ax - b) = 0, where L is generally the identity map (`solve`, for example)</span>
<a name="l169"><span class="ln">169  </span></a><span class="s3">// or the L2 norm (`lstsq`).</span>
<a name="l170"><span class="ln">170  </span></a><span class="s3">// It is expected that `a` and `b` are contiguous tensors of column-major matrices</span>
<a name="l171"><span class="ln">171  </span></a><span class="s3">// (so that a.view({-1, a.size(-2), a.size(-1)}) succeeds, same for `b`),</span>
<a name="l172"><span class="ln">172  </span></a><span class="s3">// with the following additional properties:</span>
<a name="l173"><span class="ln">173  </span></a><span class="s3">//</span>
<a name="l174"><span class="ln">174  </span></a><span class="s3">// 1. a.dim() == b.dim()</span>
<a name="l175"><span class="ln">175  </span></a><span class="s3">// 2. a.shape[:-2] broadcasts over b.shape[:-2]</span>
<a name="l176"><span class="ln">176  </span></a><span class="s3">// 3. a.size(i) &lt;= b.size(i) for i=0,..., a.dim() - 3 (only for batch dimensions)</span>
<a name="l177"><span class="ln">177  </span></a><span class="s3">//</span>
<a name="l178"><span class="ln">178  </span></a><span class="s3">// MAGMA/LAPACK modify tensor `a` in-place, and the main goal of this method</span>
<a name="l179"><span class="ln">179  </span></a><span class="s3">// is to be memory efficient, which means that if there exists an index i such that</span>
<a name="l180"><span class="ln">180  </span></a><span class="s3">// a.shape[i] &lt; b.shape[i], 0 &lt;= i &lt;= a.dim() - 3,</span>
<a name="l181"><span class="ln">181  </span></a><span class="s3">// then instead of materializing copies of `a` in the broadcasted shape, we keep</span>
<a name="l182"><span class="ln">182  </span></a><span class="s3">// a buffer copy of `a` along with flags that check whether specific batch dimension</span>
<a name="l183"><span class="ln">183  </span></a><span class="s3">// indices for `a` were already accessed. If they were, we copy the data from the buffer</span>
<a name="l184"><span class="ln">184  </span></a><span class="s3">// into `a`. The number of copies does not exceed</span>
<a name="l185"><span class="ln">185  </span></a><span class="s3">// prod(max(a.shape[:-2], b.shape[:-2]) - a.shape[:-2] + 1)</span>
<a name="l186"><span class="ln">186  </span></a><span class="s3">// and this value is attained by tensors with non-empty batch dimensions.</span>
<a name="l187"><span class="ln">187  </span></a><span class="s3">//</span>
<a name="l188"><span class="ln">188  </span></a><span class="s3">// func_t `f` is a callable that is being supplied with</span>
<a name="l189"><span class="ln">189  </span></a><span class="s3">// scalar_t* a_working_ptr, scalar_t* b_working_ptr, int64_t a_linear_batch_idx.</span>
<a name="l190"><span class="ln">190  </span></a><span class="s3">// a_working_ptr and b_working_ptr can directly be passed to LAPACK/MAGMA routines,</span>
<a name="l191"><span class="ln">191  </span></a><span class="s3">// and a_linear_batch_idx is an index in the 3d representation which corresponds to</span>
<a name="l192"><span class="ln">192  </span></a><span class="s3">// the memory a_working_ptr points to, in other words:</span>
<a name="l193"><span class="ln">193  </span></a><span class="s3">// a_working_ptr == a.view({-1, a.size(-2), a.size(-1)}.select(0, a_linear_batch_idx).data_ptr&lt;scalar_t&gt;();</span>
<a name="l194"><span class="ln">194  </span></a><span class="s3">// a_linear_batch_idx is useful to store metadata related to `a`, such as, for example,</span>
<a name="l195"><span class="ln">195  </span></a><span class="s3">// its rank or singular values (see linalg_lstsq).</span>
<a name="l196"><span class="ln">196  </span></a><span class="s0">template&lt;</span><span class="s2">typename </span><span class="s0">scalar_t, </span><span class="s2">typename </span><span class="s0">func_t&gt;</span>
<a name="l197"><span class="ln">197  </span></a><span class="s1">void </span><span class="s0">batch_iterator_with_broadcasting(</span><span class="s1">const </span><span class="s0">Tensor&amp; a, </span><span class="s1">const </span><span class="s0">Tensor&amp; b, </span><span class="s1">const </span><span class="s0">func_t&amp; f) {</span>
<a name="l198"><span class="ln">198  </span></a>  <span class="s0">IntArrayRef a_batch_sizes(a.sizes().data(), a.dim() - </span><span class="s4">2</span><span class="s0">);</span>
<a name="l199"><span class="ln">199  </span></a>  <span class="s0">IntArrayRef b_batch_sizes(b.sizes().data(), b.dim() - </span><span class="s4">2</span><span class="s0">);</span>
<a name="l200"><span class="ln">200  </span></a>
<a name="l201"><span class="ln">201  </span></a>  <span class="s1">auto </span><span class="s0">a_linear_batch_idx = at::arange(batchCount(a)).view(a_batch_sizes);</span>
<a name="l202"><span class="ln">202  </span></a>  <span class="s1">auto </span><span class="s0">b_linear_batch_idx = at::arange(batchCount(b)).view(b_batch_sizes);</span>
<a name="l203"><span class="ln">203  </span></a>
<a name="l204"><span class="ln">204  </span></a>  <span class="s0">TensorIterator iter = TensorIteratorConfig()</span>
<a name="l205"><span class="ln">205  </span></a>    <span class="s0">.set_check_mem_overlap(</span><span class="s1">false</span><span class="s0">)</span>
<a name="l206"><span class="ln">206  </span></a>    <span class="s0">.check_all_same_dtype(</span><span class="s1">false</span><span class="s0">)</span>
<a name="l207"><span class="ln">207  </span></a>    <span class="s0">.resize_outputs(</span><span class="s1">false</span><span class="s0">)</span>
<a name="l208"><span class="ln">208  </span></a>    <span class="s0">.add_output(b_linear_batch_idx)</span>
<a name="l209"><span class="ln">209  </span></a>    <span class="s0">.add_input(a_linear_batch_idx)</span>
<a name="l210"><span class="ln">210  </span></a>    <span class="s0">.build();</span>
<a name="l211"><span class="ln">211  </span></a>
<a name="l212"><span class="ln">212  </span></a>  <span class="s1">auto </span><span class="s0">m = a.size(-</span><span class="s4">2</span><span class="s0">);</span>
<a name="l213"><span class="ln">213  </span></a>  <span class="s1">auto </span><span class="s0">n = a.size(-</span><span class="s4">1</span><span class="s0">);</span>
<a name="l214"><span class="ln">214  </span></a>  <span class="s1">auto </span><span class="s0">a_3d = a.view({batchCount(a), m, n});</span>
<a name="l215"><span class="ln">215  </span></a>  <span class="s1">auto </span><span class="s0">b_3d = b.view({batchCount(b), b.size(-</span><span class="s4">2</span><span class="s0">), b.size(-</span><span class="s4">1</span><span class="s0">)});</span>
<a name="l216"><span class="ln">216  </span></a>
<a name="l217"><span class="ln">217  </span></a>  <span class="s1">auto </span><span class="s0">a_broadcasts_over_b = (a_batch_sizes != b_batch_sizes);</span>
<a name="l218"><span class="ln">218  </span></a>  <span class="s0">Tensor a_buffer, a_was_accessed, a_buffer_3d;</span>
<a name="l219"><span class="ln">219  </span></a>  <span class="s0">std::function&lt;</span><span class="s1">void</span><span class="s0">(int64_t)&gt; check_if_copy_needed_for_a</span>
<a name="l220"><span class="ln">220  </span></a>    <span class="s0">= [](int64_t </span><span class="s3">/*a_curr_linear_batch_idx*/</span><span class="s0">){};</span>
<a name="l221"><span class="ln">221  </span></a>  <span class="s1">if </span><span class="s0">(a_broadcasts_over_b) {</span>
<a name="l222"><span class="ln">222  </span></a>    <span class="s0">a_buffer = at::empty_strided(a.sizes(), a.strides(), a.options())</span>
<a name="l223"><span class="ln">223  </span></a>      <span class="s0">.copy_(a);</span>
<a name="l224"><span class="ln">224  </span></a>    <span class="s0">a_was_accessed = at::zeros(batchCount(a), at::kBool);</span>
<a name="l225"><span class="ln">225  </span></a>    <span class="s0">a_buffer_3d = a_buffer.view({batchCount(a), m, n});</span>
<a name="l226"><span class="ln">226  </span></a>    <span class="s0">check_if_copy_needed_for_a = [&amp;](int64_t a_curr_linear_batch_idx) {</span>
<a name="l227"><span class="ln">227  </span></a>      <span class="s1">auto</span><span class="s0">* a_was_accessed_flag = a_was_accessed</span>
<a name="l228"><span class="ln">228  </span></a>        <span class="s0">.select(</span><span class="s4">0</span><span class="s0">, a_curr_linear_batch_idx)</span>
<a name="l229"><span class="ln">229  </span></a>        <span class="s0">.data_ptr&lt;</span><span class="s1">bool</span><span class="s0">&gt;();</span>
<a name="l230"><span class="ln">230  </span></a>      <span class="s1">if </span><span class="s0">(!(*a_was_accessed_flag)) {</span>
<a name="l231"><span class="ln">231  </span></a>        <span class="s0">*a_was_accessed_flag = </span><span class="s2">true</span><span class="s0">;</span>
<a name="l232"><span class="ln">232  </span></a>      <span class="s0">}</span>
<a name="l233"><span class="ln">233  </span></a>      <span class="s1">else </span><span class="s0">{</span>
<a name="l234"><span class="ln">234  </span></a>        <span class="s0">a_3d.select(</span><span class="s4">0</span><span class="s0">, a_curr_linear_batch_idx)</span>
<a name="l235"><span class="ln">235  </span></a>          <span class="s0">.copy_(a_buffer_3d.select(</span><span class="s4">0</span><span class="s0">, a_curr_linear_batch_idx));</span>
<a name="l236"><span class="ln">236  </span></a>      <span class="s0">}</span>
<a name="l237"><span class="ln">237  </span></a>    <span class="s0">};</span>
<a name="l238"><span class="ln">238  </span></a>  <span class="s0">}</span>
<a name="l239"><span class="ln">239  </span></a>
<a name="l240"><span class="ln">240  </span></a>  <span class="s1">auto </span><span class="s0">loop = [&amp;](</span><span class="s1">char</span><span class="s0">** data, </span><span class="s1">const </span><span class="s0">int64_t* strides, int64_t nelems) {</span>
<a name="l241"><span class="ln">241  </span></a>    <span class="s1">auto</span><span class="s0">* b_batch_idx_ptr = data[</span><span class="s4">0</span><span class="s0">];</span>
<a name="l242"><span class="ln">242  </span></a>    <span class="s1">auto</span><span class="s0">* a_batch_idx_ptr = data[</span><span class="s4">1</span><span class="s0">];</span>
<a name="l243"><span class="ln">243  </span></a>
<a name="l244"><span class="ln">244  </span></a>    <span class="s1">for </span><span class="s0">([[maybe_unused]] </span><span class="s1">const auto </span><span class="s0">elem : c10::irange(nelems)) {</span>
<a name="l245"><span class="ln">245  </span></a>      <span class="s1">auto </span><span class="s0">b_curr_linear_batch_idx =</span>
<a name="l246"><span class="ln">246  </span></a>          <span class="s0">*</span><span class="s2">reinterpret_cast</span><span class="s0">&lt;int64_t*&gt;(b_batch_idx_ptr);</span>
<a name="l247"><span class="ln">247  </span></a>      <span class="s1">auto </span><span class="s0">a_curr_linear_batch_idx = *</span><span class="s2">reinterpret_cast</span><span class="s0">&lt;int64_t*&gt;(a_batch_idx_ptr);</span>
<a name="l248"><span class="ln">248  </span></a>
<a name="l249"><span class="ln">249  </span></a>      <span class="s0">check_if_copy_needed_for_a(a_curr_linear_batch_idx);</span>
<a name="l250"><span class="ln">250  </span></a>
<a name="l251"><span class="ln">251  </span></a>      <span class="s1">auto</span><span class="s0">* a_working_ptr = a_3d.select(</span><span class="s4">0</span><span class="s0">, a_curr_linear_batch_idx)</span>
<a name="l252"><span class="ln">252  </span></a>        <span class="s0">.data_ptr&lt;scalar_t&gt;();</span>
<a name="l253"><span class="ln">253  </span></a>      <span class="s1">auto</span><span class="s0">* b_working_ptr = b_3d.select(</span><span class="s4">0</span><span class="s0">, b_curr_linear_batch_idx)</span>
<a name="l254"><span class="ln">254  </span></a>        <span class="s0">.data_ptr&lt;scalar_t&gt;();</span>
<a name="l255"><span class="ln">255  </span></a>      <span class="s0">f(a_working_ptr, b_working_ptr, a_curr_linear_batch_idx);</span>
<a name="l256"><span class="ln">256  </span></a>
<a name="l257"><span class="ln">257  </span></a>      <span class="s0">b_batch_idx_ptr += strides[</span><span class="s4">0</span><span class="s0">];</span>
<a name="l258"><span class="ln">258  </span></a>      <span class="s0">a_batch_idx_ptr += strides[</span><span class="s4">1</span><span class="s0">];</span>
<a name="l259"><span class="ln">259  </span></a>    <span class="s0">}</span>
<a name="l260"><span class="ln">260  </span></a>  <span class="s0">};</span>
<a name="l261"><span class="ln">261  </span></a>  <span class="s0">iter.serial_for_each(loop, {</span><span class="s4">0</span><span class="s0">, batchCount(b)});</span>
<a name="l262"><span class="ln">262  </span></a><span class="s0">}</span>
<a name="l263"><span class="ln">263  </span></a>
<a name="l264"><span class="ln">264  </span></a><span class="s3">// Returns the epsilon value for floating types except half</span>
<a name="l265"><span class="ln">265  </span></a><span class="s2">inline </span><span class="s1">double </span><span class="s0">_get_epsilon(</span><span class="s1">const </span><span class="s0">ScalarType&amp; sc_type) {</span>
<a name="l266"><span class="ln">266  </span></a>  <span class="s1">switch </span><span class="s0">(sc_type) {</span>
<a name="l267"><span class="ln">267  </span></a>    <span class="s1">case </span><span class="s0">at::ScalarType::Float:</span>
<a name="l268"><span class="ln">268  </span></a>      <span class="s1">return </span><span class="s2">static_cast</span><span class="s0">&lt;</span><span class="s1">double</span><span class="s0">&gt;(std::numeric_limits&lt;</span><span class="s1">float</span><span class="s0">&gt;::epsilon());</span>
<a name="l269"><span class="ln">269  </span></a>    <span class="s1">case </span><span class="s0">at::ScalarType::Double:</span>
<a name="l270"><span class="ln">270  </span></a>      <span class="s1">return </span><span class="s0">std::numeric_limits&lt;</span><span class="s1">double</span><span class="s0">&gt;::epsilon();</span>
<a name="l271"><span class="ln">271  </span></a>    <span class="s1">default</span><span class="s0">:</span>
<a name="l272"><span class="ln">272  </span></a>      <span class="s0">TORCH_CHECK(</span><span class="s1">false</span><span class="s0">, </span><span class="s5">&quot;This function doesn't handle types other than float and double&quot;</span><span class="s0">);</span>
<a name="l273"><span class="ln">273  </span></a>  <span class="s0">}</span>
<a name="l274"><span class="ln">274  </span></a><span class="s0">}</span>
<a name="l275"><span class="ln">275  </span></a>
<a name="l276"><span class="ln">276  </span></a><span class="s3">// Validates input shapes and devices</span>
<a name="l277"><span class="ln">277  </span></a><span class="s3">// for linear solve methods (solve, cholesky_solve, lu_solve, triangular_solve)</span>
<a name="l278"><span class="ln">278  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">linearSolveCheckInputs(</span><span class="s1">const </span><span class="s0">Tensor&amp; self, </span><span class="s1">const </span><span class="s0">Tensor&amp; A, </span><span class="s1">const char</span><span class="s0">* name) {</span>
<a name="l279"><span class="ln">279  </span></a>  <span class="s0">TORCH_CHECK(self.device() == A.device(),</span>
<a name="l280"><span class="ln">280  </span></a>              <span class="s5">&quot;Expected b and A to be on the same device, but found b on &quot;</span><span class="s0">,</span>
<a name="l281"><span class="ln">281  </span></a>              <span class="s0">self.device(), </span><span class="s5">&quot; and A on &quot;</span><span class="s0">, A.device(), </span><span class="s5">&quot; instead.&quot;</span><span class="s0">);</span>
<a name="l282"><span class="ln">282  </span></a>
<a name="l283"><span class="ln">283  </span></a>  <span class="s0">TORCH_CHECK(self.scalar_type() == A.scalar_type(),</span>
<a name="l284"><span class="ln">284  </span></a>              <span class="s5">&quot;Expected b and A to have the same dtype, but found b of type &quot;</span><span class="s0">,</span>
<a name="l285"><span class="ln">285  </span></a>              <span class="s0">self.scalar_type(), </span><span class="s5">&quot; and A of type &quot;</span><span class="s0">, A.scalar_type(), </span><span class="s5">&quot; instead.&quot;</span><span class="s0">);</span>
<a name="l286"><span class="ln">286  </span></a>
<a name="l287"><span class="ln">287  </span></a>  <span class="s0">TORCH_CHECK(A.size(-</span><span class="s4">1</span><span class="s0">) == A.size(-</span><span class="s4">2</span><span class="s0">),</span>
<a name="l288"><span class="ln">288  </span></a>              <span class="s5">&quot;A must be batches of square matrices, &quot;</span>
<a name="l289"><span class="ln">289  </span></a>              <span class="s5">&quot;but they are &quot;</span><span class="s0">, A.size(-</span><span class="s4">2</span><span class="s0">), </span><span class="s5">&quot; by &quot;</span><span class="s0">, A.size(-</span><span class="s4">1</span><span class="s0">), </span><span class="s5">&quot; matrices&quot;</span><span class="s0">);</span>
<a name="l290"><span class="ln">290  </span></a>
<a name="l291"><span class="ln">291  </span></a>  <span class="s0">TORCH_CHECK(A.size(-</span><span class="s4">1</span><span class="s0">) == self.size(-</span><span class="s4">2</span><span class="s0">),</span>
<a name="l292"><span class="ln">292  </span></a>              <span class="s5">&quot;Incompatible matrix sizes for &quot;</span><span class="s0">, name, </span><span class="s5">&quot;: each A &quot;</span>
<a name="l293"><span class="ln">293  </span></a>              <span class="s5">&quot;matrix is &quot;</span><span class="s0">, A.size(-</span><span class="s4">1</span><span class="s0">), </span><span class="s5">&quot; by &quot;</span><span class="s0">, A.size(-</span><span class="s4">1</span><span class="s0">),</span>
<a name="l294"><span class="ln">294  </span></a>              <span class="s5">&quot; but each b matrix is &quot;</span><span class="s0">, self.size(-</span><span class="s4">2</span><span class="s0">), </span><span class="s5">&quot; by &quot;</span><span class="s0">, self.size(-</span><span class="s4">1</span><span class="s0">));</span>
<a name="l295"><span class="ln">295  </span></a><span class="s0">}</span>
<a name="l296"><span class="ln">296  </span></a>
<a name="l297"><span class="ln">297  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkFloatingOrComplex(</span><span class="s1">const </span><span class="s0">Tensor&amp; t, </span><span class="s1">const char</span><span class="s0">* </span><span class="s1">const </span><span class="s0">f_name, </span><span class="s1">const bool </span><span class="s0">allow_low_precision_dtypes=</span><span class="s2">true</span><span class="s0">) {</span>
<a name="l298"><span class="ln">298  </span></a>  <span class="s1">auto </span><span class="s0">dtype = t.scalar_type();</span>
<a name="l299"><span class="ln">299  </span></a>  <span class="s0">TORCH_CHECK((at::isFloatingType(dtype) || at::isComplexType(dtype)),</span>
<a name="l300"><span class="ln">300  </span></a>              <span class="s0">f_name, </span><span class="s5">&quot;: Expected a floating point or complex tensor as input. Got &quot;</span><span class="s0">, dtype);</span>
<a name="l301"><span class="ln">301  </span></a>  <span class="s1">if </span><span class="s0">(!allow_low_precision_dtypes) {</span>
<a name="l302"><span class="ln">302  </span></a>    <span class="s0">TORCH_CHECK(dtype == kFloat || dtype == kDouble || dtype == kComplexFloat || dtype == kComplexDouble,</span>
<a name="l303"><span class="ln">303  </span></a>                <span class="s0">f_name, </span><span class="s5">&quot;: Low precision dtypes not supported. Got &quot;</span><span class="s0">, dtype);</span>
<a name="l304"><span class="ln">304  </span></a>  <span class="s0">}</span>
<a name="l305"><span class="ln">305  </span></a><span class="s0">}</span>
<a name="l306"><span class="ln">306  </span></a>
<a name="l307"><span class="ln">307  </span></a>
<a name="l308"><span class="ln">308  </span></a><span class="s3">// Checks if all the Tensors in a TensorList are of the same dimensions</span>
<a name="l309"><span class="ln">309  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkAllSameDim(TensorList tensors, int64_t dim) {</span>
<a name="l310"><span class="ln">310  </span></a>  <span class="s1">for </span><span class="s0">(</span><span class="s1">auto </span><span class="s0">&amp;t : tensors) {</span>
<a name="l311"><span class="ln">311  </span></a>    <span class="s0">TORCH_CHECK(t.dim() == dim, </span><span class="s5">&quot;Tensor dimension is &quot;</span><span class="s0">, t.dim(), </span><span class="s5">&quot;, expected &quot;</span><span class="s0">, dim, </span><span class="s5">&quot; instead.&quot;</span><span class="s0">);</span>
<a name="l312"><span class="ln">312  </span></a>  <span class="s0">}</span>
<a name="l313"><span class="ln">313  </span></a><span class="s0">}</span>
<a name="l314"><span class="ln">314  </span></a>
<a name="l315"><span class="ln">315  </span></a><span class="s2">inline </span><span class="s0">std::tuple&lt;std::vector&lt;int64_t&gt;, std::vector&lt;int64_t&gt;&gt; _linalg_broadcast_batch_dims(</span><span class="s1">const </span><span class="s0">Tensor&amp; arg1, </span><span class="s1">const </span><span class="s0">Tensor&amp; arg2) {</span>
<a name="l316"><span class="ln">316  </span></a>  <span class="s3">// broadcast the batch dimensions of arg1 and arg2.</span>
<a name="l317"><span class="ln">317  </span></a>  <span class="s0">IntArrayRef arg1_batch_sizes(arg1.sizes().data(), arg1.ndimension() - </span><span class="s4">2</span><span class="s0">);</span>
<a name="l318"><span class="ln">318  </span></a>  <span class="s0">IntArrayRef arg2_batch_sizes(arg2.sizes().data(), arg2.ndimension() - </span><span class="s4">2</span><span class="s0">);</span>
<a name="l319"><span class="ln">319  </span></a>  <span class="s0">std::vector&lt;int64_t&gt; expand_batch_portion = infer_size(arg1_batch_sizes, arg2_batch_sizes);</span>
<a name="l320"><span class="ln">320  </span></a>
<a name="l321"><span class="ln">321  </span></a>  <span class="s0">std::vector&lt;int64_t&gt; arg1_expand_size({expand_batch_portion});</span>
<a name="l322"><span class="ln">322  </span></a>  <span class="s0">arg1_expand_size.insert(arg1_expand_size.end(), { arg1.size(-</span><span class="s4">2</span><span class="s0">), arg1.size(-</span><span class="s4">1</span><span class="s0">) });</span>
<a name="l323"><span class="ln">323  </span></a>
<a name="l324"><span class="ln">324  </span></a>  <span class="s0">std::vector&lt;int64_t&gt; arg2_expand_size({expand_batch_portion});</span>
<a name="l325"><span class="ln">325  </span></a>  <span class="s0">arg2_expand_size.insert(arg2_expand_size.end(), { arg2.size(-</span><span class="s4">2</span><span class="s0">), arg2.size(-</span><span class="s4">1</span><span class="s0">) });</span>
<a name="l326"><span class="ln">326  </span></a>  <span class="s1">return </span><span class="s0">std::make_tuple(std::move(arg1_expand_size), std::move(arg2_expand_size));</span>
<a name="l327"><span class="ln">327  </span></a><span class="s0">}</span>
<a name="l328"><span class="ln">328  </span></a>
<a name="l329"><span class="ln">329  </span></a><span class="s2">inline </span><span class="s0">std::tuple&lt;Tensor,Tensor&gt; _linalg_broadcast_batch_dims(</span><span class="s1">const </span><span class="s0">Tensor&amp; arg1, </span><span class="s1">const </span><span class="s0">Tensor&amp; arg2, </span><span class="s1">const char</span><span class="s0">* name) {</span>
<a name="l330"><span class="ln">330  </span></a>  <span class="s3">// If there's no name we assume we don't want to check the errors</span>
<a name="l331"><span class="ln">331  </span></a>  <span class="s1">if </span><span class="s0">(name != nullptr) {</span>
<a name="l332"><span class="ln">332  </span></a>    <span class="s0">linearSolveCheckInputs(arg1, arg2, name);</span>
<a name="l333"><span class="ln">333  </span></a>  <span class="s0">}</span>
<a name="l334"><span class="ln">334  </span></a>
<a name="l335"><span class="ln">335  </span></a>  <span class="s1">auto </span><span class="s0">[arg1_expand_size, arg2_expand_size] = at::native::_linalg_broadcast_batch_dims(arg1, arg2);</span>
<a name="l336"><span class="ln">336  </span></a>
<a name="l337"><span class="ln">337  </span></a>  <span class="s1">auto </span><span class="s0">arg1_broadcasted  = arg1_expand_size == arg1.sizes() ? arg1 : arg1.expand(arg1_expand_size);</span>
<a name="l338"><span class="ln">338  </span></a>  <span class="s1">auto </span><span class="s0">arg2_broadcasted  = arg2_expand_size == arg2.sizes() ? arg2 : arg2.expand(arg2_expand_size);</span>
<a name="l339"><span class="ln">339  </span></a>  <span class="s1">return </span><span class="s0">std::make_tuple(arg1_broadcasted, arg2_broadcasted);</span>
<a name="l340"><span class="ln">340  </span></a><span class="s0">}</span>
<a name="l341"><span class="ln">341  </span></a>
<a name="l342"><span class="ln">342  </span></a><span class="s2">inline </span><span class="s0">std::vector&lt;int64_t&gt; broadcast_batch_size(</span><span class="s1">const </span><span class="s0">Tensor&amp; t1, </span><span class="s1">const </span><span class="s0">Tensor&amp; t2, int64_t n_batch_dims) {</span>
<a name="l343"><span class="ln">343  </span></a>  <span class="s0">IntArrayRef t1_batch_sizes(t1.sizes().data(), n_batch_dims);</span>
<a name="l344"><span class="ln">344  </span></a>  <span class="s0">IntArrayRef t2_batch_sizes(t2.sizes().data(), n_batch_dims);</span>
<a name="l345"><span class="ln">345  </span></a>  <span class="s1">auto </span><span class="s0">broadcasted_batch_sizes = infer_size(t1_batch_sizes, t2_batch_sizes);</span>
<a name="l346"><span class="ln">346  </span></a>  <span class="s1">return </span><span class="s0">broadcasted_batch_sizes;</span>
<a name="l347"><span class="ln">347  </span></a><span class="s0">}</span>
<a name="l348"><span class="ln">348  </span></a>
<a name="l349"><span class="ln">349  </span></a><span class="s3">// Return a permutation with the given axes moved to the end.</span>
<a name="l350"><span class="ln">350  </span></a><span class="s2">inline </span><span class="s0">Tensor _move_to_end(</span><span class="s1">const </span><span class="s0">Tensor&amp; self, IntArrayRef axes) {</span>
<a name="l351"><span class="ln">351  </span></a>  <span class="s1">const </span><span class="s0">std::vector&lt;int64_t&gt; a = axes.vec();</span>
<a name="l352"><span class="ln">352  </span></a>  <span class="s1">const </span><span class="s0">int64_t ndim = self.ndimension();</span>
<a name="l353"><span class="ln">353  </span></a>  <span class="s0">std::vector&lt;int64_t&gt; perm;</span>
<a name="l354"><span class="ln">354  </span></a>
<a name="l355"><span class="ln">355  </span></a>  <span class="s1">for </span><span class="s0">(</span><span class="s1">const auto </span><span class="s0">i : c10::irange(ndim)) {</span>
<a name="l356"><span class="ln">356  </span></a>    <span class="s1">auto </span><span class="s0">it = std::find(a.begin(), a.end(), i);</span>
<a name="l357"><span class="ln">357  </span></a>    <span class="s1">if </span><span class="s0">(it == a.end()) {</span>
<a name="l358"><span class="ln">358  </span></a>       <span class="s0">perm.push_back(i);</span>
<a name="l359"><span class="ln">359  </span></a>    <span class="s0">}</span>
<a name="l360"><span class="ln">360  </span></a>  <span class="s0">}</span>
<a name="l361"><span class="ln">361  </span></a>  <span class="s1">for </span><span class="s0">(</span><span class="s1">auto </span><span class="s0">i : a) {</span>
<a name="l362"><span class="ln">362  </span></a>    <span class="s0">perm.push_back(i);</span>
<a name="l363"><span class="ln">363  </span></a>  <span class="s0">}</span>
<a name="l364"><span class="ln">364  </span></a>
<a name="l365"><span class="ln">365  </span></a>  <span class="s0">TORCH_CHECK((int64_t)perm.size() == ndim,</span>
<a name="l366"><span class="ln">366  </span></a>    <span class="s5">&quot;duplicate or invalid axis in 'dim' argument for tensor with ndim==&quot;</span><span class="s0">, ndim);</span>
<a name="l367"><span class="ln">367  </span></a>
<a name="l368"><span class="ln">368  </span></a>  <span class="s1">return </span><span class="s0">self.permute(perm);</span>
<a name="l369"><span class="ln">369  </span></a><span class="s0">}</span>
<a name="l370"><span class="ln">370  </span></a>
<a name="l371"><span class="ln">371  </span></a><span class="s3">// parse the &quot;mode&quot; param in linalg_qr: return a tuple of bools (compute_q, reduced)</span>
<a name="l372"><span class="ln">372  </span></a><span class="s2">inline </span><span class="s0">std::tuple&lt;</span><span class="s1">bool</span><span class="s0">, </span><span class="s1">bool</span><span class="s0">&gt; _parse_qr_mode(std::string_view mode) {</span>
<a name="l373"><span class="ln">373  </span></a>  <span class="s1">bool </span><span class="s0">compute_q;</span>
<a name="l374"><span class="ln">374  </span></a>  <span class="s1">bool </span><span class="s0">reduced;</span>
<a name="l375"><span class="ln">375  </span></a>  <span class="s1">if </span><span class="s0">(mode == </span><span class="s5">&quot;reduced&quot;</span><span class="s0">) {</span>
<a name="l376"><span class="ln">376  </span></a>    <span class="s0">compute_q = </span><span class="s2">true</span><span class="s0">;</span>
<a name="l377"><span class="ln">377  </span></a>    <span class="s0">reduced = </span><span class="s2">true</span><span class="s0">;</span>
<a name="l378"><span class="ln">378  </span></a>  <span class="s0">} </span><span class="s1">else if </span><span class="s0">(mode == </span><span class="s5">&quot;complete&quot;</span><span class="s0">) {</span>
<a name="l379"><span class="ln">379  </span></a>    <span class="s0">compute_q = </span><span class="s2">true</span><span class="s0">;</span>
<a name="l380"><span class="ln">380  </span></a>    <span class="s0">reduced = </span><span class="s1">false</span><span class="s0">;</span>
<a name="l381"><span class="ln">381  </span></a>  <span class="s0">} </span><span class="s1">else if </span><span class="s0">(mode == </span><span class="s5">&quot;r&quot;</span><span class="s0">) {</span>
<a name="l382"><span class="ln">382  </span></a>    <span class="s0">compute_q = </span><span class="s1">false</span><span class="s0">;</span>
<a name="l383"><span class="ln">383  </span></a>    <span class="s0">reduced = </span><span class="s2">true</span><span class="s0">; </span><span class="s3">// this is actually irrelevant in this mode</span>
<a name="l384"><span class="ln">384  </span></a>  <span class="s0">} </span><span class="s1">else </span><span class="s0">{</span>
<a name="l385"><span class="ln">385  </span></a>      <span class="s0">TORCH_CHECK(</span><span class="s1">false</span><span class="s0">, </span><span class="s5">&quot;qr received unrecognized mode '&quot;</span><span class="s0">, mode,</span>
<a name="l386"><span class="ln">386  </span></a>                  <span class="s5">&quot;' but expected one of 'reduced' (default), 'r', or 'complete'&quot;</span><span class="s0">);</span>
<a name="l387"><span class="ln">387  </span></a>  <span class="s0">}</span>
<a name="l388"><span class="ln">388  </span></a>  <span class="s1">return </span><span class="s0">std::make_tuple(compute_q, reduced);</span>
<a name="l389"><span class="ln">389  </span></a><span class="s0">}</span>
<a name="l390"><span class="ln">390  </span></a>
<a name="l391"><span class="ln">391  </span></a><span class="s3">// Function to compute sizes, strides and the extra columns for the Q matrix in the QR Decomposition</span>
<a name="l392"><span class="ln">392  </span></a><span class="s2">inline </span><span class="s0">std::tuple&lt;DimVector, DimVector, int64_t&gt; _compute_geometry_for_Q(</span>
<a name="l393"><span class="ln">393  </span></a>    <span class="s1">const </span><span class="s0">Tensor&amp; input,</span>
<a name="l394"><span class="ln">394  </span></a>    <span class="s1">bool </span><span class="s0">reduced) {</span>
<a name="l395"><span class="ln">395  </span></a>  <span class="s0">int64_t m = input.size(-</span><span class="s4">2</span><span class="s0">), n = input.size(-</span><span class="s4">1</span><span class="s0">);</span>
<a name="l396"><span class="ln">396  </span></a>  <span class="s0">int64_t n_columns_q;</span>
<a name="l397"><span class="ln">397  </span></a>
<a name="l398"><span class="ln">398  </span></a>  <span class="s3">// We need to compute the required size of Q based on the `reduced` option</span>
<a name="l399"><span class="ln">399  </span></a>  <span class="s0">DimVector q_sizes(input.sizes());</span>
<a name="l400"><span class="ln">400  </span></a>  <span class="s1">if </span><span class="s0">(!reduced &amp;&amp; m &gt; n) {</span>
<a name="l401"><span class="ln">401  </span></a>    <span class="s0">q_sizes[input.dim() - </span><span class="s4">1</span><span class="s0">] = m;</span>
<a name="l402"><span class="ln">402  </span></a>    <span class="s0">n_columns_q = m;</span>
<a name="l403"><span class="ln">403  </span></a>  <span class="s0">} </span><span class="s1">else </span><span class="s0">{</span>
<a name="l404"><span class="ln">404  </span></a>    <span class="s0">q_sizes[input.dim() - </span><span class="s4">1</span><span class="s0">] = n;</span>
<a name="l405"><span class="ln">405  </span></a>    <span class="s0">n_columns_q = std::min(m, n);</span>
<a name="l406"><span class="ln">406  </span></a>  <span class="s0">}</span>
<a name="l407"><span class="ln">407  </span></a>  <span class="s1">auto </span><span class="s0">q_strides = batched_matrix_contiguous_strides(q_sizes, </span><span class="s3">/*f-contig*/</span><span class="s2">true</span><span class="s0">);</span>
<a name="l408"><span class="ln">408  </span></a>  <span class="s1">return </span><span class="s0">std::make_tuple(q_sizes, q_strides, n_columns_q);</span>
<a name="l409"><span class="ln">409  </span></a><span class="s0">}</span>
<a name="l410"><span class="ln">410  </span></a>
<a name="l411"><span class="ln">411  </span></a><span class="s2">inline </span><span class="s1">bool </span><span class="s0">svd_uses_cusolver(</span><span class="s1">const </span><span class="s0">Tensor&amp; A) {</span>
<a name="l412"><span class="ln">412  </span></a>  <span class="s3">// if cusolver is available, it is used unconditionally</span>
<a name="l413"><span class="ln">413  </span></a>  <span class="s1">return </span><span class="s0">A.is_cuda()</span>
<a name="l414"><span class="ln">414  </span></a>         <span class="s0">&amp;&amp; at::globalContext().hasCuSOLVER()</span>
<a name="l415"><span class="ln">415  </span></a>         <span class="s0">&amp;&amp; at::globalContext().linalgPreferredBackend() != at::LinalgBackend::Magma;</span>
<a name="l416"><span class="ln">416  </span></a><span class="s0">}</span>
<a name="l417"><span class="ln">417  </span></a>
<a name="l418"><span class="ln">418  </span></a>
<a name="l419"><span class="ln">419  </span></a><span class="s3">// Function used instead of .to so that the original strides are retained</span>
<a name="l420"><span class="ln">420  </span></a><span class="s3">// .to doesn't retain strides and make the output tensor contiguous</span>
<a name="l421"><span class="ln">421  </span></a><span class="s2">inline </span><span class="s0">Tensor same_stride_to(</span><span class="s1">const </span><span class="s0">Tensor&amp; original_tensor, </span><span class="s1">const </span><span class="s0">at::TensorOptions&amp; options) {</span>
<a name="l422"><span class="ln">422  </span></a>  <span class="s1">auto </span><span class="s0">strided_to = at::empty_strided(original_tensor.sizes(),</span>
<a name="l423"><span class="ln">423  </span></a>                                      <span class="s0">original_tensor.strides(),</span>
<a name="l424"><span class="ln">424  </span></a>                                      <span class="s0">options);</span>
<a name="l425"><span class="ln">425  </span></a>  <span class="s0">strided_to.copy_(original_tensor);</span>
<a name="l426"><span class="ln">426  </span></a>  <span class="s1">return </span><span class="s0">strided_to;</span>
<a name="l427"><span class="ln">427  </span></a><span class="s0">}</span>
<a name="l428"><span class="ln">428  </span></a>
<a name="l429"><span class="ln">429  </span></a><span class="s3">// Creates a dimension permutation array that can be given to `at::permute()`, which will shift</span>
<a name="l430"><span class="ln">430  </span></a><span class="s3">// the two specified dimensions to the end of a tensor, without changing the order of</span>
<a name="l431"><span class="ln">431  </span></a><span class="s3">// the other dimensions. `dim1` will be placed at the very end, and `dim0` will be</span>
<a name="l432"><span class="ln">432  </span></a><span class="s3">// placed just to the left of it.</span>
<a name="l433"><span class="ln">433  </span></a><span class="s3">//</span>
<a name="l434"><span class="ln">434  </span></a><span class="s3">// For instance, given a 4-D tensor, dimensions 1 and 3 can be shifted to the end by</span>
<a name="l435"><span class="ln">435  </span></a><span class="s3">// calling `create_dim_backshift_permutation(1, 3, 4)`. The resulting vector will</span>
<a name="l436"><span class="ln">436  </span></a><span class="s3">// be `vec(0, 2, 1, 3)`.</span>
<a name="l437"><span class="ln">437  </span></a><span class="s2">inline </span><span class="s0">std::vector&lt;int64_t&gt; create_dim_backshift_permutation(int64_t dim0, int64_t dim1, int64_t ndim) {</span>
<a name="l438"><span class="ln">438  </span></a>  <span class="s0">TORCH_CHECK(</span>
<a name="l439"><span class="ln">439  </span></a>    <span class="s0">(dim0 != dim1) &amp;&amp; (dim0 &lt; ndim) &amp;&amp; (dim0 &gt;= </span><span class="s4">0</span><span class="s0">) &amp;&amp; (dim1 &lt; ndim) &amp;&amp; (dim1 &gt;= </span><span class="s4">0</span><span class="s0">),</span>
<a name="l440"><span class="ln">440  </span></a>    <span class="s5">&quot;duplicate or invalid dimensions&quot;</span><span class="s0">);</span>
<a name="l441"><span class="ln">441  </span></a>  <span class="s0">std::vector&lt;int64_t&gt; permutation(ndim);</span>
<a name="l442"><span class="ln">442  </span></a>  <span class="s0">int64_t cur_permuted_dim = </span><span class="s4">0</span><span class="s0">;</span>
<a name="l443"><span class="ln">443  </span></a>  <span class="s1">for </span><span class="s0">(</span><span class="s1">const auto </span><span class="s0">dim_ind : c10::irange(ndim)) {</span>
<a name="l444"><span class="ln">444  </span></a>    <span class="s1">if </span><span class="s0">((dim_ind != dim0) &amp;&amp; (dim_ind != dim1)) {</span>
<a name="l445"><span class="ln">445  </span></a>      <span class="s0">permutation[cur_permuted_dim++] = dim_ind;</span>
<a name="l446"><span class="ln">446  </span></a>    <span class="s0">}</span>
<a name="l447"><span class="ln">447  </span></a>  <span class="s0">}</span>
<a name="l448"><span class="ln">448  </span></a>  <span class="s0">permutation[cur_permuted_dim++] = dim0;</span>
<a name="l449"><span class="ln">449  </span></a>  <span class="s0">permutation[cur_permuted_dim] = dim1;</span>
<a name="l450"><span class="ln">450  </span></a>  <span class="s1">return </span><span class="s0">permutation;</span>
<a name="l451"><span class="ln">451  </span></a><span class="s0">}</span>
<a name="l452"><span class="ln">452  </span></a>
<a name="l453"><span class="ln">453  </span></a><span class="s3">// Creates a dimension permutation array that can be given to `at::permute()`, which</span>
<a name="l454"><span class="ln">454  </span></a><span class="s3">// will reverse a given permutation.</span>
<a name="l455"><span class="ln">455  </span></a><span class="s3">// The reverse permutation array is created by swapping the indices and their</span>
<a name="l456"><span class="ln">456  </span></a><span class="s3">// associated values from the given permutation array.</span>
<a name="l457"><span class="ln">457  </span></a><span class="s2">inline </span><span class="s0">std::vector&lt;int64_t&gt; create_reverse_permutation(std::vector&lt;int64_t&gt; permutation) {</span>
<a name="l458"><span class="ln">458  </span></a>  <span class="s0">int64_t ndim = permutation.size();</span>
<a name="l459"><span class="ln">459  </span></a>  <span class="s0">std::vector&lt;int64_t&gt; reverse_permutation(ndim);</span>
<a name="l460"><span class="ln">460  </span></a>  <span class="s1">for </span><span class="s0">(</span><span class="s1">const auto </span><span class="s0">dim_ind : c10::irange(ndim)) {</span>
<a name="l461"><span class="ln">461  </span></a>    <span class="s0">reverse_permutation[permutation[dim_ind]] = dim_ind;</span>
<a name="l462"><span class="ln">462  </span></a>  <span class="s0">}</span>
<a name="l463"><span class="ln">463  </span></a>  <span class="s1">return </span><span class="s0">reverse_permutation;</span>
<a name="l464"><span class="ln">464  </span></a><span class="s0">}</span>
<a name="l465"><span class="ln">465  </span></a>
<a name="l466"><span class="ln">466  </span></a><span class="s3">// Compute R-work array size for MAGMA/LAPACK cgesdd/zgesdd</span>
<a name="l467"><span class="ln">467  </span></a><span class="s3">// See https://github.com/Reference-LAPACK/lapack/blob/122506cd8b6ce050a200920c3d4c0b153b150fd8/SRC/cgesdd.f#L186</span>
<a name="l468"><span class="ln">468  </span></a><span class="s2">inline </span><span class="s0">int64_t computeLRWorkDim(</span><span class="s1">const char </span><span class="s0">jobz, int64_t m, int64_t n) {</span>
<a name="l469"><span class="ln">469  </span></a>  <span class="s1">auto </span><span class="s0">mn = std::min(m, n);</span>
<a name="l470"><span class="ln">470  </span></a>  <span class="s1">auto </span><span class="s0">mx = std::max(m, n);</span>
<a name="l471"><span class="ln">471  </span></a>  <span class="s1">if </span><span class="s0">(jobz == </span><span class="s5">'N'</span><span class="s0">) {</span>
<a name="l472"><span class="ln">472  </span></a><span class="s1">#ifdef </span><span class="s0">__APPLE__</span>
<a name="l473"><span class="ln">473  </span></a>    <span class="s3">// According to `vecLib.framework/Headers/clapack.h` Accelerate.framework is based on LAPACK 3.2.1</span>
<a name="l474"><span class="ln">474  </span></a>    <span class="s1">return </span><span class="s4">7 </span><span class="s0">* mn;</span>
<a name="l475"><span class="ln">475  </span></a><span class="s1">#else</span>
<a name="l476"><span class="ln">476  </span></a>    <span class="s3">// These setting is valid for on LAPACK 3.6+</span>
<a name="l477"><span class="ln">477  </span></a>    <span class="s1">return </span><span class="s4">5 </span><span class="s0">* mn;</span>
<a name="l478"><span class="ln">478  </span></a><span class="s1">#endif</span>
<a name="l479"><span class="ln">479  </span></a>  <span class="s0">}</span>
<a name="l480"><span class="ln">480  </span></a>  <span class="s1">if </span><span class="s0">(mx &gt; </span><span class="s4">10 </span><span class="s0">* mn) {</span>
<a name="l481"><span class="ln">481  </span></a>    <span class="s1">return </span><span class="s4">5 </span><span class="s0">* mn * mn + </span><span class="s4">5 </span><span class="s0">* mn;</span>
<a name="l482"><span class="ln">482  </span></a>  <span class="s0">}</span>
<a name="l483"><span class="ln">483  </span></a>  <span class="s1">return </span><span class="s0">std::max(</span><span class="s4">5 </span><span class="s0">* mn * mn + </span><span class="s4">5 </span><span class="s0">* mn, </span><span class="s4">2 </span><span class="s0">* mx * mn + </span><span class="s4">2 </span><span class="s0">* mn * mn + mn);</span>
<a name="l484"><span class="ln">484  </span></a><span class="s0">}</span>
<a name="l485"><span class="ln">485  </span></a>
<a name="l486"><span class="ln">486  </span></a><span class="s3">// This function checks whether the uplo argument input is valid</span>
<a name="l487"><span class="ln">487  </span></a><span class="s3">// Allowed strings are &quot;u&quot;, &quot;U&quot;, &quot;l&quot;, &quot;L&quot;</span>
<a name="l488"><span class="ln">488  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkUplo(</span><span class="s1">const </span><span class="s0">std::string_view uplo) {</span>
<a name="l489"><span class="ln">489  </span></a>  <span class="s3">// To use std::toupper safely with plain chars (or signed chars), the argument should first be converted to unsigned char</span>
<a name="l490"><span class="ln">490  </span></a>  <span class="s1">char </span><span class="s0">uplo_uppercase = </span><span class="s2">static_cast</span><span class="s0">&lt;</span><span class="s1">char</span><span class="s0">&gt;(std::toupper(</span><span class="s2">static_cast</span><span class="s0">&lt;</span><span class="s1">unsigned char</span><span class="s0">&gt;(uplo[</span><span class="s4">0</span><span class="s0">])));</span>
<a name="l491"><span class="ln">491  </span></a>  <span class="s0">TORCH_CHECK(uplo.size() == </span><span class="s4">1 </span><span class="s0">&amp;&amp; (uplo_uppercase == </span><span class="s5">'U' </span><span class="s0">|| uplo_uppercase == </span><span class="s5">'L'</span><span class="s0">),</span>
<a name="l492"><span class="ln">492  </span></a>    <span class="s5">&quot;Expected UPLO argument to be 'L' or 'U', but got &quot;</span><span class="s0">, uplo);</span>
<a name="l493"><span class="ln">493  </span></a><span class="s0">}</span>
<a name="l494"><span class="ln">494  </span></a>
<a name="l495"><span class="ln">495  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkSameDevice(</span><span class="s1">const </span><span class="s0">std::string&amp; fn_name, Tensor result, Tensor input, </span><span class="s1">const </span><span class="s0">std::string&amp; result_name = </span><span class="s5">&quot;result&quot;</span><span class="s0">) {</span>
<a name="l496"><span class="ln">496  </span></a>  <span class="s0">TORCH_CHECK(</span>
<a name="l497"><span class="ln">497  </span></a>      <span class="s0">result.device() == input.device(),</span>
<a name="l498"><span class="ln">498  </span></a>      <span class="s0">fn_name,</span>
<a name="l499"><span class="ln">499  </span></a>      <span class="s5">&quot;: Expected &quot;</span><span class="s0">, result_name, </span><span class="s5">&quot; and input tensors to be on the same device, but got &quot;</span><span class="s0">,</span>
<a name="l500"><span class="ln">500  </span></a>      <span class="s0">result_name, </span><span class="s5">&quot; on &quot;</span><span class="s0">, result.device(), </span><span class="s5">&quot; and input on &quot;</span><span class="s0">, input.device());</span>
<a name="l501"><span class="ln">501  </span></a><span class="s0">}</span>
<a name="l502"><span class="ln">502  </span></a>
<a name="l503"><span class="ln">503  </span></a><span class="s3">// Check the dtype of result and input tensors (for _out variants).</span>
<a name="l504"><span class="ln">504  </span></a><span class="s3">// Most linear algebra functions have the same dtype for input and output</span>
<a name="l505"><span class="ln">505  </span></a><span class="s3">// (either floating or complex type input), so we can check whether input's dtype can be casted to result's dtype.</span>
<a name="l506"><span class="ln">506  </span></a><span class="s3">// According to https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch</span>
<a name="l507"><span class="ln">507  </span></a><span class="s3">// c10::canCast is used for checking the &quot;safe copy&quot; dtype requirements.</span>
<a name="l508"><span class="ln">508  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkLinalgCompatibleDtype(</span><span class="s1">const </span><span class="s0">std::string&amp; fn_name, Tensor result, Tensor input, </span><span class="s1">const </span><span class="s0">std::string&amp; result_name = </span><span class="s5">&quot;result&quot;</span><span class="s0">) {</span>
<a name="l509"><span class="ln">509  </span></a>  <span class="s1">bool </span><span class="s0">can_cast = c10::canCast(input.scalar_type(), result.scalar_type());</span>
<a name="l510"><span class="ln">510  </span></a>  <span class="s0">TORCH_CHECK(</span>
<a name="l511"><span class="ln">511  </span></a>      <span class="s0">can_cast,</span>
<a name="l512"><span class="ln">512  </span></a>      <span class="s0">fn_name,</span>
<a name="l513"><span class="ln">513  </span></a>      <span class="s5">&quot;: Expected &quot;</span><span class="s0">, result_name, </span><span class="s5">&quot; to be safely castable from &quot;</span><span class="s0">, input.scalar_type(), </span><span class="s5">&quot; dtype, but got &quot;</span><span class="s0">,</span>
<a name="l514"><span class="ln">514  </span></a>      <span class="s0">result_name, </span><span class="s5">&quot; with dtype &quot;</span><span class="s0">, result.scalar_type());</span>
<a name="l515"><span class="ln">515  </span></a><span class="s0">}</span>
<a name="l516"><span class="ln">516  </span></a>
<a name="l517"><span class="ln">517  </span></a><span class="s3">// Alternatively, we can check whether the specific expected output type (result_type) can be safely casted to out tensor dtype (out_type)</span>
<a name="l518"><span class="ln">518  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkLinalgCompatibleDtype(</span><span class="s1">const </span><span class="s0">std::string&amp; fn_name, ScalarType out_type, ScalarType result_type, </span><span class="s1">const </span><span class="s0">std::string&amp; out_name = </span><span class="s5">&quot;result&quot;</span><span class="s0">) {</span>
<a name="l519"><span class="ln">519  </span></a>  <span class="s1">bool </span><span class="s0">can_cast = c10::canCast(result_type, out_type);</span>
<a name="l520"><span class="ln">520  </span></a>  <span class="s0">TORCH_CHECK(</span>
<a name="l521"><span class="ln">521  </span></a>      <span class="s0">can_cast,</span>
<a name="l522"><span class="ln">522  </span></a>      <span class="s0">fn_name,</span>
<a name="l523"><span class="ln">523  </span></a>      <span class="s5">&quot;: Expected &quot;</span><span class="s0">, out_name, </span><span class="s5">&quot; to be safely castable from &quot;</span><span class="s0">, result_type, </span><span class="s5">&quot; dtype, but got &quot;</span><span class="s0">,</span>
<a name="l524"><span class="ln">524  </span></a>      <span class="s0">out_name, </span><span class="s5">&quot; with dtype &quot;</span><span class="s0">, out_type);</span>
<a name="l525"><span class="ln">525  </span></a><span class="s0">}</span>
<a name="l526"><span class="ln">526  </span></a>
<a name="l527"><span class="ln">527  </span></a><span class="s2">inline </span><span class="s1">void </span><span class="s0">checkNotComplexTolerance(</span><span class="s1">const </span><span class="s0">Tensor&amp; tol, </span><span class="s1">const </span><span class="s0">std::string_view f_name, </span><span class="s1">const </span><span class="s0">std::string_view tol_name) {</span>
<a name="l528"><span class="ln">528  </span></a>  <span class="s0">TORCH_CHECK(!at::isComplexType(tol.scalar_type()),</span>
<a name="l529"><span class="ln">529  </span></a>              <span class="s0">f_name, </span><span class="s5">&quot;: &quot;</span><span class="s0">, tol_name, </span><span class="s5">&quot; tensor of complex type is not supported. Got &quot;</span><span class="s0">, tol.scalar_type());</span>
<a name="l530"><span class="ln">530  </span></a><span class="s0">}</span>
<a name="l531"><span class="ln">531  </span></a>
<a name="l532"><span class="ln">532  </span></a><span class="s3">/* 
<a name="l533"><span class="ln">533  </span></a>  Two types of 'other' tensors are supported when solving 
<a name="l534"><span class="ln">534  </span></a>  a system of linear equations matmul(input, x) = other: 
<a name="l535"><span class="ln">535  </span></a>  * 1-dimensional (1D) tensor or batch of 1D tensors (vector case) 
<a name="l536"><span class="ln">536  </span></a>  * 2-dimensional (2D) tensor or batch of 2D tensors (matrix case). 
<a name="l537"><span class="ln">537  </span></a>  The original torch.solve supported only the matrix case, while NumPy works for both cases. 
<a name="l538"><span class="ln">538  </span></a>  For the batched input we need to be able to distinguish them. 
<a name="l539"><span class="ln">539  </span></a>  Let input.shape = (batch_dimensions, m, n), then 'other' is of vector type if other.shape == (batch_dimensions, m). 
<a name="l540"><span class="ln">540  </span></a>  This rule is compatible with NumPy, see https://github.com/numpy/numpy/blob/v1.20.0/numpy/linalg/linalg.py#L384-L389 
<a name="l541"><span class="ln">541  </span></a>*/</span>
<a name="l542"><span class="ln">542  </span></a><span class="s2">inline </span><span class="s1">bool </span><span class="s0">linalg_solve_is_vector_rhs(</span><span class="s1">const </span><span class="s0">Tensor&amp; input, </span><span class="s1">const </span><span class="s0">Tensor&amp; other) {</span>
<a name="l543"><span class="ln">543  </span></a>  <span class="s1">auto </span><span class="s0">expected_batched_rhs_shape = SymIntArrayRef(input.sym_sizes().data(), input.dim() - </span><span class="s4">1</span><span class="s0">); </span><span class="s3">// input.shape[:-1]</span>
<a name="l544"><span class="ln">544  </span></a>  <span class="s1">bool </span><span class="s0">vector_case = other.dim() == </span><span class="s4">1 </span><span class="s0">|| (input.dim() - </span><span class="s4">1 </span><span class="s0">== other.dim() &amp;&amp; other.sym_sizes().equals(expected_batched_rhs_shape));</span>
<a name="l545"><span class="ln">545  </span></a>  <span class="s1">return </span><span class="s0">vector_case;</span>
<a name="l546"><span class="ln">546  </span></a><span class="s0">}</span>
<a name="l547"><span class="ln">547  </span></a>
<a name="l548"><span class="ln">548  </span></a><span class="s3">/* 
<a name="l549"><span class="ln">549  </span></a>  Computes linear indices for a tensor with original_shape to access its elements like it was a materialized broadcast tensor. 
<a name="l550"><span class="ln">550  </span></a>*/</span>
<a name="l551"><span class="ln">551  </span></a><span class="s2">inline </span><span class="s0">Tensor get_linear_indices(int64_t numel, IntArrayRef original_shape, IntArrayRef broadcast_shape) {</span>
<a name="l552"><span class="ln">552  </span></a>  <span class="s0">TensorOptions options = at::TensorOptions().dtype(at::kLong).device(at::kCPU);</span>
<a name="l553"><span class="ln">553  </span></a>  <span class="s1">return </span><span class="s0">at::arange(numel, options).view(original_shape).broadcast_to(broadcast_shape).contiguous();</span>
<a name="l554"><span class="ln">554  </span></a><span class="s0">}</span>
<a name="l555"><span class="ln">555  </span></a>
<a name="l556"><span class="ln">556  </span></a><span class="s2">class </span><span class="s0">BroadcastLinearIndices {</span>
<a name="l557"><span class="ln">557  </span></a> <span class="s2">private</span><span class="s0">:</span>
<a name="l558"><span class="ln">558  </span></a>  <span class="s0">Tensor linear_indices_;</span>
<a name="l559"><span class="ln">559  </span></a>  <span class="s1">bool </span><span class="s0">is_broadcasting_;</span>
<a name="l560"><span class="ln">560  </span></a>
<a name="l561"><span class="ln">561  </span></a> <span class="s2">public</span><span class="s0">:</span>
<a name="l562"><span class="ln">562  </span></a>  <span class="s0">BroadcastLinearIndices(</span>
<a name="l563"><span class="ln">563  </span></a>      <span class="s0">int64_t numel,</span>
<a name="l564"><span class="ln">564  </span></a>      <span class="s0">IntArrayRef original_shape,</span>
<a name="l565"><span class="ln">565  </span></a>      <span class="s0">IntArrayRef broadcast_shape) : is_broadcasting_(!original_shape.equals(broadcast_shape)) {</span>
<a name="l566"><span class="ln">566  </span></a>    <span class="s3">// The assumption is that the broadcast_shape is a materialized broadcast</span>
<a name="l567"><span class="ln">567  </span></a>    <span class="s3">// shape of the original_shape. We need to compute the linear indices</span>
<a name="l568"><span class="ln">568  </span></a>    <span class="s3">// compatible with the original_shape to access the elements in the original</span>
<a name="l569"><span class="ln">569  </span></a>    <span class="s3">// tensor corresponding to the broadcast tensor.</span>
<a name="l570"><span class="ln">570  </span></a>    <span class="s1">if </span><span class="s0">(is_broadcasting_) {</span>
<a name="l571"><span class="ln">571  </span></a>      <span class="s0">linear_indices_ =</span>
<a name="l572"><span class="ln">572  </span></a>          <span class="s0">get_linear_indices(numel, original_shape, broadcast_shape);</span>
<a name="l573"><span class="ln">573  </span></a>    <span class="s0">}</span>
<a name="l574"><span class="ln">574  </span></a>  <span class="s0">}</span>
<a name="l575"><span class="ln">575  </span></a>  <span class="s0">int64_t </span><span class="s2">operator</span><span class="s0">()(int64_t broadcast_linear_index) {</span>
<a name="l576"><span class="ln">576  </span></a>    <span class="s1">return </span><span class="s0">is_broadcasting_</span>
<a name="l577"><span class="ln">577  </span></a>        <span class="s0">? linear_indices_.data_ptr&lt;int64_t&gt;()[broadcast_linear_index]</span>
<a name="l578"><span class="ln">578  </span></a>        <span class="s0">: broadcast_linear_index;</span>
<a name="l579"><span class="ln">579  </span></a>  <span class="s0">}</span>
<a name="l580"><span class="ln">580  </span></a><span class="s0">};</span>
<a name="l581"><span class="ln">581  </span></a>
<a name="l582"><span class="ln">582  </span></a><span class="s2">inline </span><span class="s1">bool </span><span class="s0">is_blas_compatible_column_major_order(</span><span class="s1">const </span><span class="s0">Tensor&amp; input) {</span>
<a name="l583"><span class="ln">583  </span></a>  <span class="s0">IntArrayRef input_strides = input.strides();</span>
<a name="l584"><span class="ln">584  </span></a>  <span class="s0">IntArrayRef input_sizes = input.sizes();</span>
<a name="l585"><span class="ln">585  </span></a>  <span class="s1">auto </span><span class="s0">ndim = input.dim();</span>
<a name="l586"><span class="ln">586  </span></a>  <span class="s0">TORCH_INTERNAL_ASSERT_DEBUG_ONLY(ndim &gt;= </span><span class="s4">2</span><span class="s0">);</span>
<a name="l587"><span class="ln">587  </span></a>  <span class="s1">if </span><span class="s0">(ndim &gt; </span><span class="s4">3</span><span class="s0">) {</span>
<a name="l588"><span class="ln">588  </span></a>    <span class="s1">return </span><span class="s0">input.transpose(-</span><span class="s4">2</span><span class="s0">, -</span><span class="s4">1</span><span class="s0">).is_contiguous();</span>
<a name="l589"><span class="ln">589  </span></a>  <span class="s0">}</span>
<a name="l590"><span class="ln">590  </span></a>  <span class="s1">auto </span><span class="s0">leading_dimension = input_strides[ndim - </span><span class="s4">1</span><span class="s0">];</span>
<a name="l591"><span class="ln">591  </span></a>  <span class="s1">auto </span><span class="s0">rows = input_sizes[ndim - </span><span class="s4">2</span><span class="s0">];</span>
<a name="l592"><span class="ln">592  </span></a>  <span class="s1">bool </span><span class="s0">batch_stride_compatible = </span><span class="s2">true</span><span class="s0">;</span>
<a name="l593"><span class="ln">593  </span></a>  <span class="s1">if </span><span class="s0">(ndim == </span><span class="s4">3</span><span class="s0">) {</span>
<a name="l594"><span class="ln">594  </span></a>    <span class="s1">auto </span><span class="s0">cols = input_sizes[ndim - </span><span class="s4">1</span><span class="s0">];</span>
<a name="l595"><span class="ln">595  </span></a>    <span class="s0">batch_stride_compatible =</span>
<a name="l596"><span class="ln">596  </span></a>        <span class="s0">input_strides[ndim - </span><span class="s4">3</span><span class="s0">] &gt;= leading_dimension * cols;</span>
<a name="l597"><span class="ln">597  </span></a>  <span class="s0">}</span>
<a name="l598"><span class="ln">598  </span></a>  <span class="s1">return </span><span class="s0">(input_strides[ndim - </span><span class="s4">2</span><span class="s0">] == </span><span class="s4">1</span><span class="s0">) &amp;&amp;</span>
<a name="l599"><span class="ln">599  </span></a>      <span class="s0">(leading_dimension &gt;= std::max&lt;int64_t&gt;(</span><span class="s4">1</span><span class="s0">, rows)) &amp;&amp;</span>
<a name="l600"><span class="ln">600  </span></a>      <span class="s0">batch_stride_compatible;</span>
<a name="l601"><span class="ln">601  </span></a><span class="s0">}</span>
<a name="l602"><span class="ln">602  </span></a>
<a name="l603"><span class="ln">603  </span></a><span class="s2">inline </span><span class="s1">bool </span><span class="s0">is_blas_compatible_row_major_order(</span><span class="s1">const </span><span class="s0">Tensor&amp; input) {</span>
<a name="l604"><span class="ln">604  </span></a>  <span class="s0">IntArrayRef input_strides = input.strides();</span>
<a name="l605"><span class="ln">605  </span></a>  <span class="s0">IntArrayRef input_sizes = input.sizes();</span>
<a name="l606"><span class="ln">606  </span></a>  <span class="s1">auto </span><span class="s0">ndim = input.dim();</span>
<a name="l607"><span class="ln">607  </span></a>  <span class="s0">TORCH_INTERNAL_ASSERT_DEBUG_ONLY(ndim &gt;= </span><span class="s4">2</span><span class="s0">);</span>
<a name="l608"><span class="ln">608  </span></a>  <span class="s1">if </span><span class="s0">(ndim &gt; </span><span class="s4">3</span><span class="s0">) {</span>
<a name="l609"><span class="ln">609  </span></a>    <span class="s1">return </span><span class="s0">input.is_contiguous();</span>
<a name="l610"><span class="ln">610  </span></a>  <span class="s0">}</span>
<a name="l611"><span class="ln">611  </span></a>  <span class="s1">auto </span><span class="s0">leading_dimension = input_strides[ndim - </span><span class="s4">2</span><span class="s0">];</span>
<a name="l612"><span class="ln">612  </span></a>  <span class="s1">auto </span><span class="s0">cols = input_sizes[ndim - </span><span class="s4">1</span><span class="s0">];</span>
<a name="l613"><span class="ln">613  </span></a>  <span class="s1">bool </span><span class="s0">batch_stride_compatible = </span><span class="s2">true</span><span class="s0">;</span>
<a name="l614"><span class="ln">614  </span></a>  <span class="s1">if </span><span class="s0">(ndim == </span><span class="s4">3</span><span class="s0">) {</span>
<a name="l615"><span class="ln">615  </span></a>    <span class="s1">auto </span><span class="s0">rows = input_sizes[ndim - </span><span class="s4">2</span><span class="s0">];</span>
<a name="l616"><span class="ln">616  </span></a>    <span class="s0">batch_stride_compatible =</span>
<a name="l617"><span class="ln">617  </span></a>        <span class="s0">input_strides[ndim - </span><span class="s4">3</span><span class="s0">] &gt;= leading_dimension * rows;</span>
<a name="l618"><span class="ln">618  </span></a>  <span class="s0">}</span>
<a name="l619"><span class="ln">619  </span></a>  <span class="s1">return </span><span class="s0">(input_strides[ndim - </span><span class="s4">1</span><span class="s0">] == </span><span class="s4">1</span><span class="s0">) &amp;&amp;</span>
<a name="l620"><span class="ln">620  </span></a>      <span class="s0">(leading_dimension &gt;= std::max&lt;int64_t&gt;(</span><span class="s4">1</span><span class="s0">, cols)) &amp;&amp;</span>
<a name="l621"><span class="ln">621  </span></a>      <span class="s0">batch_stride_compatible;</span>
<a name="l622"><span class="ln">622  </span></a><span class="s0">}</span>
<a name="l623"><span class="ln">623  </span></a>
<a name="l624"><span class="ln">624  </span></a><span class="s0">}  </span><span class="s3">// namespace at::native</span>
<a name="l625"><span class="ln">625  </span></a></pre>
</body>
</html>