<html>
<head>
<title>_tensor_docs.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #969896;}
.s1 { color: #333333;}
.s2 { color: #a71d5d;}
.s3 { color: #63a35c;}
.s4 { color: #183691;}
.ln { color: #333333; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_tensor_docs.py</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0"># mypy: allow-untyped-defs</span>
<a name="l2"><span class="ln">2    </span></a><span class="s0">&quot;&quot;&quot;Adds docstrings to Tensor functions&quot;&quot;&quot;</span>
<a name="l3"><span class="ln">3    </span></a>
<a name="l4"><span class="ln">4    </span></a><span class="s2">import </span><span class="s1">torch</span><span class="s3">.</span><span class="s1">_C</span>
<a name="l5"><span class="ln">5    </span></a><span class="s2">from </span><span class="s1">torch</span><span class="s3">.</span><span class="s1">_C </span><span class="s2">import </span><span class="s1">_add_docstr </span><span class="s2">as </span><span class="s1">add_docstr</span>
<a name="l6"><span class="ln">6    </span></a><span class="s2">from </span><span class="s1">torch</span><span class="s3">.</span><span class="s1">_torch_docs </span><span class="s2">import </span><span class="s1">parse_kwargs</span><span class="s3">, </span><span class="s1">reproducibility_notes</span>
<a name="l7"><span class="ln">7    </span></a>
<a name="l8"><span class="ln">8    </span></a>
<a name="l9"><span class="ln">9    </span></a><span class="s2">def </span><span class="s1">add_docstr_all</span><span class="s3">(</span><span class="s1">method</span><span class="s2">: </span><span class="s1">str</span><span class="s3">, </span><span class="s1">docstr</span><span class="s2">: </span><span class="s1">str</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l10"><span class="ln">10   </span></a>    <span class="s1">add_docstr</span><span class="s3">(</span><span class="s1">getattr</span><span class="s3">(</span><span class="s1">torch</span><span class="s3">.</span><span class="s1">_C</span><span class="s3">.</span><span class="s1">TensorBase</span><span class="s3">, </span><span class="s1">method</span><span class="s3">), </span><span class="s1">docstr</span><span class="s3">)</span>
<a name="l11"><span class="ln">11   </span></a>
<a name="l12"><span class="ln">12   </span></a>
<a name="l13"><span class="ln">13   </span></a><span class="s1">common_args </span><span class="s2">= </span><span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l14"><span class="ln">14   </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l15"><span class="ln">15   </span></a>    memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l16"><span class="ln">16   </span></a>        returned Tensor. Default: ``torch.preserve_format``. 
<a name="l17"><span class="ln">17   </span></a>&quot;&quot;&quot;</span>
<a name="l18"><span class="ln">18   </span></a><span class="s3">)</span>
<a name="l19"><span class="ln">19   </span></a>
<a name="l20"><span class="ln">20   </span></a><span class="s1">new_common_args </span><span class="s2">= </span><span class="s1">parse_kwargs</span><span class="s3">(</span>
<a name="l21"><span class="ln">21   </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l22"><span class="ln">22   </span></a>    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l23"><span class="ln">23   </span></a>        shape of the output tensor. 
<a name="l24"><span class="ln">24   </span></a>    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor. 
<a name="l25"><span class="ln">25   </span></a>        Default: if None, same :class:`torch.dtype` as this tensor. 
<a name="l26"><span class="ln">26   </span></a>    device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l27"><span class="ln">27   </span></a>        Default: if None, same :class:`torch.device` as this tensor. 
<a name="l28"><span class="ln">28   </span></a>    requires_grad (bool, optional): If autograd should record operations on the 
<a name="l29"><span class="ln">29   </span></a>        returned tensor. Default: ``False``. 
<a name="l30"><span class="ln">30   </span></a>    pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l31"><span class="ln">31   </span></a>        the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l32"><span class="ln">32   </span></a>    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l33"><span class="ln">33   </span></a>        Default: ``torch.strided``. 
<a name="l34"><span class="ln">34   </span></a>&quot;&quot;&quot;</span>
<a name="l35"><span class="ln">35   </span></a><span class="s3">)</span>
<a name="l36"><span class="ln">36   </span></a>
<a name="l37"><span class="ln">37   </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l38"><span class="ln">38   </span></a>    <span class="s4">&quot;new_tensor&quot;</span><span class="s3">,</span>
<a name="l39"><span class="ln">39   </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l40"><span class="ln">40   </span></a>new_tensor(data, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, \ 
<a name="l41"><span class="ln">41   </span></a>pin_memory=False) -&gt; Tensor 
<a name="l42"><span class="ln">42   </span></a>&quot;&quot;&quot;</span>
<a name="l43"><span class="ln">43   </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l44"><span class="ln">44   </span></a> 
<a name="l45"><span class="ln">45   </span></a>Returns a new Tensor with :attr:`data` as the tensor data. 
<a name="l46"><span class="ln">46   </span></a>By default, the returned Tensor has the same :class:`torch.dtype` and 
<a name="l47"><span class="ln">47   </span></a>:class:`torch.device` as this tensor. 
<a name="l48"><span class="ln">48   </span></a> 
<a name="l49"><span class="ln">49   </span></a>.. warning:: 
<a name="l50"><span class="ln">50   </span></a> 
<a name="l51"><span class="ln">51   </span></a>    :func:`new_tensor` always copies :attr:`data`. If you have a Tensor 
<a name="l52"><span class="ln">52   </span></a>    ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_` 
<a name="l53"><span class="ln">53   </span></a>    or :func:`torch.Tensor.detach`. 
<a name="l54"><span class="ln">54   </span></a>    If you have a numpy array and want to avoid a copy, use 
<a name="l55"><span class="ln">55   </span></a>    :func:`torch.from_numpy`. 
<a name="l56"><span class="ln">56   </span></a> 
<a name="l57"><span class="ln">57   </span></a>.. warning:: 
<a name="l58"><span class="ln">58   </span></a> 
<a name="l59"><span class="ln">59   </span></a>    When data is a tensor `x`, :func:`new_tensor()` reads out 'the data' from whatever it is passed, 
<a name="l60"><span class="ln">60   </span></a>    and constructs a leaf variable. Therefore ``tensor.new_tensor(x)`` is equivalent to ``x.detach().clone()`` 
<a name="l61"><span class="ln">61   </span></a>    and ``tensor.new_tensor(x, requires_grad=True)`` is equivalent to ``x.detach().clone().requires_grad_(True)``. 
<a name="l62"><span class="ln">62   </span></a>    The equivalents using ``detach()`` and ``clone()`` are recommended. 
<a name="l63"><span class="ln">63   </span></a> 
<a name="l64"><span class="ln">64   </span></a>Args: 
<a name="l65"><span class="ln">65   </span></a>    data (array_like): The returned Tensor copies :attr:`data`. 
<a name="l66"><span class="ln">66   </span></a> 
<a name="l67"><span class="ln">67   </span></a>Keyword args: 
<a name="l68"><span class="ln">68   </span></a>    {dtype} 
<a name="l69"><span class="ln">69   </span></a>    {device} 
<a name="l70"><span class="ln">70   </span></a>    {requires_grad} 
<a name="l71"><span class="ln">71   </span></a>    {layout} 
<a name="l72"><span class="ln">72   </span></a>    {pin_memory} 
<a name="l73"><span class="ln">73   </span></a> 
<a name="l74"><span class="ln">74   </span></a>Example:: 
<a name="l75"><span class="ln">75   </span></a> 
<a name="l76"><span class="ln">76   </span></a>    &gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.int8) 
<a name="l77"><span class="ln">77   </span></a>    &gt;&gt;&gt; data = [[0, 1], [2, 3]] 
<a name="l78"><span class="ln">78   </span></a>    &gt;&gt;&gt; tensor.new_tensor(data) 
<a name="l79"><span class="ln">79   </span></a>    tensor([[ 0,  1], 
<a name="l80"><span class="ln">80   </span></a>            [ 2,  3]], dtype=torch.int8) 
<a name="l81"><span class="ln">81   </span></a> 
<a name="l82"><span class="ln">82   </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">new_common_args</span><span class="s3">),</span>
<a name="l83"><span class="ln">83   </span></a><span class="s3">)</span>
<a name="l84"><span class="ln">84   </span></a>
<a name="l85"><span class="ln">85   </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l86"><span class="ln">86   </span></a>    <span class="s4">&quot;new_full&quot;</span><span class="s3">,</span>
<a name="l87"><span class="ln">87   </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l88"><span class="ln">88   </span></a>new_full(size, fill_value, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, \ 
<a name="l89"><span class="ln">89   </span></a>pin_memory=False) -&gt; Tensor 
<a name="l90"><span class="ln">90   </span></a>&quot;&quot;&quot;</span>
<a name="l91"><span class="ln">91   </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l92"><span class="ln">92   </span></a> 
<a name="l93"><span class="ln">93   </span></a>Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`. 
<a name="l94"><span class="ln">94   </span></a>By default, the returned Tensor has the same :class:`torch.dtype` and 
<a name="l95"><span class="ln">95   </span></a>:class:`torch.device` as this tensor. 
<a name="l96"><span class="ln">96   </span></a> 
<a name="l97"><span class="ln">97   </span></a>Args: 
<a name="l98"><span class="ln">98   </span></a>    fill_value (scalar): the number to fill the output tensor with. 
<a name="l99"><span class="ln">99   </span></a> 
<a name="l100"><span class="ln">100  </span></a>Keyword args: 
<a name="l101"><span class="ln">101  </span></a>    {dtype} 
<a name="l102"><span class="ln">102  </span></a>    {device} 
<a name="l103"><span class="ln">103  </span></a>    {requires_grad} 
<a name="l104"><span class="ln">104  </span></a>    {layout} 
<a name="l105"><span class="ln">105  </span></a>    {pin_memory} 
<a name="l106"><span class="ln">106  </span></a> 
<a name="l107"><span class="ln">107  </span></a>Example:: 
<a name="l108"><span class="ln">108  </span></a> 
<a name="l109"><span class="ln">109  </span></a>    &gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.float64) 
<a name="l110"><span class="ln">110  </span></a>    &gt;&gt;&gt; tensor.new_full((3, 4), 3.141592) 
<a name="l111"><span class="ln">111  </span></a>    tensor([[ 3.1416,  3.1416,  3.1416,  3.1416], 
<a name="l112"><span class="ln">112  </span></a>            [ 3.1416,  3.1416,  3.1416,  3.1416], 
<a name="l113"><span class="ln">113  </span></a>            [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64) 
<a name="l114"><span class="ln">114  </span></a> 
<a name="l115"><span class="ln">115  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">new_common_args</span><span class="s3">),</span>
<a name="l116"><span class="ln">116  </span></a><span class="s3">)</span>
<a name="l117"><span class="ln">117  </span></a>
<a name="l118"><span class="ln">118  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l119"><span class="ln">119  </span></a>    <span class="s4">&quot;new_empty&quot;</span><span class="s3">,</span>
<a name="l120"><span class="ln">120  </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l121"><span class="ln">121  </span></a>new_empty(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, \ 
<a name="l122"><span class="ln">122  </span></a>pin_memory=False) -&gt; Tensor 
<a name="l123"><span class="ln">123  </span></a>&quot;&quot;&quot;</span>
<a name="l124"><span class="ln">124  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l125"><span class="ln">125  </span></a> 
<a name="l126"><span class="ln">126  </span></a>Returns a Tensor of size :attr:`size` filled with uninitialized data. 
<a name="l127"><span class="ln">127  </span></a>By default, the returned Tensor has the same :class:`torch.dtype` and 
<a name="l128"><span class="ln">128  </span></a>:class:`torch.device` as this tensor. 
<a name="l129"><span class="ln">129  </span></a> 
<a name="l130"><span class="ln">130  </span></a>Args: 
<a name="l131"><span class="ln">131  </span></a>    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l132"><span class="ln">132  </span></a>        shape of the output tensor. 
<a name="l133"><span class="ln">133  </span></a> 
<a name="l134"><span class="ln">134  </span></a>Keyword args: 
<a name="l135"><span class="ln">135  </span></a>    {dtype} 
<a name="l136"><span class="ln">136  </span></a>    {device} 
<a name="l137"><span class="ln">137  </span></a>    {requires_grad} 
<a name="l138"><span class="ln">138  </span></a>    {layout} 
<a name="l139"><span class="ln">139  </span></a>    {pin_memory} 
<a name="l140"><span class="ln">140  </span></a> 
<a name="l141"><span class="ln">141  </span></a>Example:: 
<a name="l142"><span class="ln">142  </span></a> 
<a name="l143"><span class="ln">143  </span></a>    &gt;&gt;&gt; tensor = torch.ones(()) 
<a name="l144"><span class="ln">144  </span></a>    &gt;&gt;&gt; tensor.new_empty((2, 3)) 
<a name="l145"><span class="ln">145  </span></a>    tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30], 
<a name="l146"><span class="ln">146  </span></a>            [ 3.0949e-41,  4.4842e-44,  0.0000e+00]]) 
<a name="l147"><span class="ln">147  </span></a> 
<a name="l148"><span class="ln">148  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">new_common_args</span><span class="s3">),</span>
<a name="l149"><span class="ln">149  </span></a><span class="s3">)</span>
<a name="l150"><span class="ln">150  </span></a>
<a name="l151"><span class="ln">151  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l152"><span class="ln">152  </span></a>    <span class="s4">&quot;new_empty_strided&quot;</span><span class="s3">,</span>
<a name="l153"><span class="ln">153  </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l154"><span class="ln">154  </span></a>new_empty_strided(size, stride, dtype=None, device=None, requires_grad=False, layout=torch.strided, \ 
<a name="l155"><span class="ln">155  </span></a>pin_memory=False) -&gt; Tensor 
<a name="l156"><span class="ln">156  </span></a>&quot;&quot;&quot;</span>
<a name="l157"><span class="ln">157  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l158"><span class="ln">158  </span></a> 
<a name="l159"><span class="ln">159  </span></a>Returns a Tensor of size :attr:`size` and strides :attr:`stride` filled with 
<a name="l160"><span class="ln">160  </span></a>uninitialized data. By default, the returned Tensor has the same 
<a name="l161"><span class="ln">161  </span></a>:class:`torch.dtype` and :class:`torch.device` as this tensor. 
<a name="l162"><span class="ln">162  </span></a> 
<a name="l163"><span class="ln">163  </span></a>Args: 
<a name="l164"><span class="ln">164  </span></a>    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l165"><span class="ln">165  </span></a>        shape of the output tensor. 
<a name="l166"><span class="ln">166  </span></a> 
<a name="l167"><span class="ln">167  </span></a>Keyword args: 
<a name="l168"><span class="ln">168  </span></a>    {dtype} 
<a name="l169"><span class="ln">169  </span></a>    {device} 
<a name="l170"><span class="ln">170  </span></a>    {requires_grad} 
<a name="l171"><span class="ln">171  </span></a>    {layout} 
<a name="l172"><span class="ln">172  </span></a>    {pin_memory} 
<a name="l173"><span class="ln">173  </span></a> 
<a name="l174"><span class="ln">174  </span></a>Example:: 
<a name="l175"><span class="ln">175  </span></a> 
<a name="l176"><span class="ln">176  </span></a>    &gt;&gt;&gt; tensor = torch.ones(()) 
<a name="l177"><span class="ln">177  </span></a>    &gt;&gt;&gt; tensor.new_empty_strided((2, 3), (3, 1)) 
<a name="l178"><span class="ln">178  </span></a>    tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30], 
<a name="l179"><span class="ln">179  </span></a>            [ 3.0949e-41,  4.4842e-44,  0.0000e+00]]) 
<a name="l180"><span class="ln">180  </span></a> 
<a name="l181"><span class="ln">181  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">new_common_args</span><span class="s3">),</span>
<a name="l182"><span class="ln">182  </span></a><span class="s3">)</span>
<a name="l183"><span class="ln">183  </span></a>
<a name="l184"><span class="ln">184  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l185"><span class="ln">185  </span></a>    <span class="s4">&quot;new_ones&quot;</span><span class="s3">,</span>
<a name="l186"><span class="ln">186  </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l187"><span class="ln">187  </span></a>new_ones(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, \ 
<a name="l188"><span class="ln">188  </span></a>pin_memory=False) -&gt; Tensor 
<a name="l189"><span class="ln">189  </span></a>&quot;&quot;&quot;</span>
<a name="l190"><span class="ln">190  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l191"><span class="ln">191  </span></a> 
<a name="l192"><span class="ln">192  </span></a>Returns a Tensor of size :attr:`size` filled with ``1``. 
<a name="l193"><span class="ln">193  </span></a>By default, the returned Tensor has the same :class:`torch.dtype` and 
<a name="l194"><span class="ln">194  </span></a>:class:`torch.device` as this tensor. 
<a name="l195"><span class="ln">195  </span></a> 
<a name="l196"><span class="ln">196  </span></a>Args: 
<a name="l197"><span class="ln">197  </span></a>    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l198"><span class="ln">198  </span></a>        shape of the output tensor. 
<a name="l199"><span class="ln">199  </span></a> 
<a name="l200"><span class="ln">200  </span></a>Keyword args: 
<a name="l201"><span class="ln">201  </span></a>    {dtype} 
<a name="l202"><span class="ln">202  </span></a>    {device} 
<a name="l203"><span class="ln">203  </span></a>    {requires_grad} 
<a name="l204"><span class="ln">204  </span></a>    {layout} 
<a name="l205"><span class="ln">205  </span></a>    {pin_memory} 
<a name="l206"><span class="ln">206  </span></a> 
<a name="l207"><span class="ln">207  </span></a>Example:: 
<a name="l208"><span class="ln">208  </span></a> 
<a name="l209"><span class="ln">209  </span></a>    &gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.int32) 
<a name="l210"><span class="ln">210  </span></a>    &gt;&gt;&gt; tensor.new_ones((2, 3)) 
<a name="l211"><span class="ln">211  </span></a>    tensor([[ 1,  1,  1], 
<a name="l212"><span class="ln">212  </span></a>            [ 1,  1,  1]], dtype=torch.int32) 
<a name="l213"><span class="ln">213  </span></a> 
<a name="l214"><span class="ln">214  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">new_common_args</span><span class="s3">),</span>
<a name="l215"><span class="ln">215  </span></a><span class="s3">)</span>
<a name="l216"><span class="ln">216  </span></a>
<a name="l217"><span class="ln">217  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l218"><span class="ln">218  </span></a>    <span class="s4">&quot;new_zeros&quot;</span><span class="s3">,</span>
<a name="l219"><span class="ln">219  </span></a>    <span class="s4">&quot;&quot;&quot; 
<a name="l220"><span class="ln">220  </span></a>new_zeros(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, \ 
<a name="l221"><span class="ln">221  </span></a>pin_memory=False) -&gt; Tensor 
<a name="l222"><span class="ln">222  </span></a>&quot;&quot;&quot;</span>
<a name="l223"><span class="ln">223  </span></a>    <span class="s2">+ </span><span class="s4">r&quot;&quot;&quot; 
<a name="l224"><span class="ln">224  </span></a> 
<a name="l225"><span class="ln">225  </span></a>Returns a Tensor of size :attr:`size` filled with ``0``. 
<a name="l226"><span class="ln">226  </span></a>By default, the returned Tensor has the same :class:`torch.dtype` and 
<a name="l227"><span class="ln">227  </span></a>:class:`torch.device` as this tensor. 
<a name="l228"><span class="ln">228  </span></a> 
<a name="l229"><span class="ln">229  </span></a>Args: 
<a name="l230"><span class="ln">230  </span></a>    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l231"><span class="ln">231  </span></a>        shape of the output tensor. 
<a name="l232"><span class="ln">232  </span></a> 
<a name="l233"><span class="ln">233  </span></a>Keyword args: 
<a name="l234"><span class="ln">234  </span></a>    {dtype} 
<a name="l235"><span class="ln">235  </span></a>    {device} 
<a name="l236"><span class="ln">236  </span></a>    {requires_grad} 
<a name="l237"><span class="ln">237  </span></a>    {layout} 
<a name="l238"><span class="ln">238  </span></a>    {pin_memory} 
<a name="l239"><span class="ln">239  </span></a> 
<a name="l240"><span class="ln">240  </span></a>Example:: 
<a name="l241"><span class="ln">241  </span></a> 
<a name="l242"><span class="ln">242  </span></a>    &gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.float64) 
<a name="l243"><span class="ln">243  </span></a>    &gt;&gt;&gt; tensor.new_zeros((2, 3)) 
<a name="l244"><span class="ln">244  </span></a>    tensor([[ 0.,  0.,  0.], 
<a name="l245"><span class="ln">245  </span></a>            [ 0.,  0.,  0.]], dtype=torch.float64) 
<a name="l246"><span class="ln">246  </span></a> 
<a name="l247"><span class="ln">247  </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">new_common_args</span><span class="s3">),</span>
<a name="l248"><span class="ln">248  </span></a><span class="s3">)</span>
<a name="l249"><span class="ln">249  </span></a>
<a name="l250"><span class="ln">250  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l251"><span class="ln">251  </span></a>    <span class="s4">&quot;abs&quot;</span><span class="s3">,</span>
<a name="l252"><span class="ln">252  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l253"><span class="ln">253  </span></a>abs() -&gt; Tensor 
<a name="l254"><span class="ln">254  </span></a> 
<a name="l255"><span class="ln">255  </span></a>See :func:`torch.abs` 
<a name="l256"><span class="ln">256  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l257"><span class="ln">257  </span></a><span class="s3">)</span>
<a name="l258"><span class="ln">258  </span></a>
<a name="l259"><span class="ln">259  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l260"><span class="ln">260  </span></a>    <span class="s4">&quot;abs_&quot;</span><span class="s3">,</span>
<a name="l261"><span class="ln">261  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l262"><span class="ln">262  </span></a>abs_() -&gt; Tensor 
<a name="l263"><span class="ln">263  </span></a> 
<a name="l264"><span class="ln">264  </span></a>In-place version of :meth:`~Tensor.abs` 
<a name="l265"><span class="ln">265  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l266"><span class="ln">266  </span></a><span class="s3">)</span>
<a name="l267"><span class="ln">267  </span></a>
<a name="l268"><span class="ln">268  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l269"><span class="ln">269  </span></a>    <span class="s4">&quot;absolute&quot;</span><span class="s3">,</span>
<a name="l270"><span class="ln">270  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l271"><span class="ln">271  </span></a>absolute() -&gt; Tensor 
<a name="l272"><span class="ln">272  </span></a> 
<a name="l273"><span class="ln">273  </span></a>Alias for :func:`abs` 
<a name="l274"><span class="ln">274  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l275"><span class="ln">275  </span></a><span class="s3">)</span>
<a name="l276"><span class="ln">276  </span></a>
<a name="l277"><span class="ln">277  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l278"><span class="ln">278  </span></a>    <span class="s4">&quot;absolute_&quot;</span><span class="s3">,</span>
<a name="l279"><span class="ln">279  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l280"><span class="ln">280  </span></a>absolute_() -&gt; Tensor 
<a name="l281"><span class="ln">281  </span></a> 
<a name="l282"><span class="ln">282  </span></a>In-place version of :meth:`~Tensor.absolute` 
<a name="l283"><span class="ln">283  </span></a>Alias for :func:`abs_` 
<a name="l284"><span class="ln">284  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l285"><span class="ln">285  </span></a><span class="s3">)</span>
<a name="l286"><span class="ln">286  </span></a>
<a name="l287"><span class="ln">287  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l288"><span class="ln">288  </span></a>    <span class="s4">&quot;acos&quot;</span><span class="s3">,</span>
<a name="l289"><span class="ln">289  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l290"><span class="ln">290  </span></a>acos() -&gt; Tensor 
<a name="l291"><span class="ln">291  </span></a> 
<a name="l292"><span class="ln">292  </span></a>See :func:`torch.acos` 
<a name="l293"><span class="ln">293  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l294"><span class="ln">294  </span></a><span class="s3">)</span>
<a name="l295"><span class="ln">295  </span></a>
<a name="l296"><span class="ln">296  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l297"><span class="ln">297  </span></a>    <span class="s4">&quot;acos_&quot;</span><span class="s3">,</span>
<a name="l298"><span class="ln">298  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l299"><span class="ln">299  </span></a>acos_() -&gt; Tensor 
<a name="l300"><span class="ln">300  </span></a> 
<a name="l301"><span class="ln">301  </span></a>In-place version of :meth:`~Tensor.acos` 
<a name="l302"><span class="ln">302  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l303"><span class="ln">303  </span></a><span class="s3">)</span>
<a name="l304"><span class="ln">304  </span></a>
<a name="l305"><span class="ln">305  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l306"><span class="ln">306  </span></a>    <span class="s4">&quot;arccos&quot;</span><span class="s3">,</span>
<a name="l307"><span class="ln">307  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l308"><span class="ln">308  </span></a>arccos() -&gt; Tensor 
<a name="l309"><span class="ln">309  </span></a> 
<a name="l310"><span class="ln">310  </span></a>See :func:`torch.arccos` 
<a name="l311"><span class="ln">311  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l312"><span class="ln">312  </span></a><span class="s3">)</span>
<a name="l313"><span class="ln">313  </span></a>
<a name="l314"><span class="ln">314  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l315"><span class="ln">315  </span></a>    <span class="s4">&quot;arccos_&quot;</span><span class="s3">,</span>
<a name="l316"><span class="ln">316  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l317"><span class="ln">317  </span></a>arccos_() -&gt; Tensor 
<a name="l318"><span class="ln">318  </span></a> 
<a name="l319"><span class="ln">319  </span></a>In-place version of :meth:`~Tensor.arccos` 
<a name="l320"><span class="ln">320  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l321"><span class="ln">321  </span></a><span class="s3">)</span>
<a name="l322"><span class="ln">322  </span></a>
<a name="l323"><span class="ln">323  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l324"><span class="ln">324  </span></a>    <span class="s4">&quot;acosh&quot;</span><span class="s3">,</span>
<a name="l325"><span class="ln">325  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l326"><span class="ln">326  </span></a>acosh() -&gt; Tensor 
<a name="l327"><span class="ln">327  </span></a> 
<a name="l328"><span class="ln">328  </span></a>See :func:`torch.acosh` 
<a name="l329"><span class="ln">329  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l330"><span class="ln">330  </span></a><span class="s3">)</span>
<a name="l331"><span class="ln">331  </span></a>
<a name="l332"><span class="ln">332  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l333"><span class="ln">333  </span></a>    <span class="s4">&quot;acosh_&quot;</span><span class="s3">,</span>
<a name="l334"><span class="ln">334  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l335"><span class="ln">335  </span></a>acosh_() -&gt; Tensor 
<a name="l336"><span class="ln">336  </span></a> 
<a name="l337"><span class="ln">337  </span></a>In-place version of :meth:`~Tensor.acosh` 
<a name="l338"><span class="ln">338  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l339"><span class="ln">339  </span></a><span class="s3">)</span>
<a name="l340"><span class="ln">340  </span></a>
<a name="l341"><span class="ln">341  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l342"><span class="ln">342  </span></a>    <span class="s4">&quot;arccosh&quot;</span><span class="s3">,</span>
<a name="l343"><span class="ln">343  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l344"><span class="ln">344  </span></a>acosh() -&gt; Tensor 
<a name="l345"><span class="ln">345  </span></a> 
<a name="l346"><span class="ln">346  </span></a>See :func:`torch.arccosh` 
<a name="l347"><span class="ln">347  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l348"><span class="ln">348  </span></a><span class="s3">)</span>
<a name="l349"><span class="ln">349  </span></a>
<a name="l350"><span class="ln">350  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l351"><span class="ln">351  </span></a>    <span class="s4">&quot;arccosh_&quot;</span><span class="s3">,</span>
<a name="l352"><span class="ln">352  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l353"><span class="ln">353  </span></a>acosh_() -&gt; Tensor 
<a name="l354"><span class="ln">354  </span></a> 
<a name="l355"><span class="ln">355  </span></a>In-place version of :meth:`~Tensor.arccosh` 
<a name="l356"><span class="ln">356  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l357"><span class="ln">357  </span></a><span class="s3">)</span>
<a name="l358"><span class="ln">358  </span></a>
<a name="l359"><span class="ln">359  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l360"><span class="ln">360  </span></a>    <span class="s4">&quot;add&quot;</span><span class="s3">,</span>
<a name="l361"><span class="ln">361  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l362"><span class="ln">362  </span></a>add(other, *, alpha=1) -&gt; Tensor 
<a name="l363"><span class="ln">363  </span></a> 
<a name="l364"><span class="ln">364  </span></a>Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha` 
<a name="l365"><span class="ln">365  </span></a>and :attr:`other` are specified, each element of :attr:`other` is scaled by 
<a name="l366"><span class="ln">366  </span></a>:attr:`alpha` before being used. 
<a name="l367"><span class="ln">367  </span></a> 
<a name="l368"><span class="ln">368  </span></a>When :attr:`other` is a tensor, the shape of :attr:`other` must be 
<a name="l369"><span class="ln">369  </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the shape of the underlying 
<a name="l370"><span class="ln">370  </span></a>tensor 
<a name="l371"><span class="ln">371  </span></a> 
<a name="l372"><span class="ln">372  </span></a>See :func:`torch.add` 
<a name="l373"><span class="ln">373  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l374"><span class="ln">374  </span></a><span class="s3">)</span>
<a name="l375"><span class="ln">375  </span></a>
<a name="l376"><span class="ln">376  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l377"><span class="ln">377  </span></a>    <span class="s4">&quot;add_&quot;</span><span class="s3">,</span>
<a name="l378"><span class="ln">378  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l379"><span class="ln">379  </span></a>add_(other, *, alpha=1) -&gt; Tensor 
<a name="l380"><span class="ln">380  </span></a> 
<a name="l381"><span class="ln">381  </span></a>In-place version of :meth:`~Tensor.add` 
<a name="l382"><span class="ln">382  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l383"><span class="ln">383  </span></a><span class="s3">)</span>
<a name="l384"><span class="ln">384  </span></a>
<a name="l385"><span class="ln">385  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l386"><span class="ln">386  </span></a>    <span class="s4">&quot;addbmm&quot;</span><span class="s3">,</span>
<a name="l387"><span class="ln">387  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l388"><span class="ln">388  </span></a>addbmm(batch1, batch2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l389"><span class="ln">389  </span></a> 
<a name="l390"><span class="ln">390  </span></a>See :func:`torch.addbmm` 
<a name="l391"><span class="ln">391  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l392"><span class="ln">392  </span></a><span class="s3">)</span>
<a name="l393"><span class="ln">393  </span></a>
<a name="l394"><span class="ln">394  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l395"><span class="ln">395  </span></a>    <span class="s4">&quot;addbmm_&quot;</span><span class="s3">,</span>
<a name="l396"><span class="ln">396  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l397"><span class="ln">397  </span></a>addbmm_(batch1, batch2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l398"><span class="ln">398  </span></a> 
<a name="l399"><span class="ln">399  </span></a>In-place version of :meth:`~Tensor.addbmm` 
<a name="l400"><span class="ln">400  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l401"><span class="ln">401  </span></a><span class="s3">)</span>
<a name="l402"><span class="ln">402  </span></a>
<a name="l403"><span class="ln">403  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l404"><span class="ln">404  </span></a>    <span class="s4">&quot;addcdiv&quot;</span><span class="s3">,</span>
<a name="l405"><span class="ln">405  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l406"><span class="ln">406  </span></a>addcdiv(tensor1, tensor2, *, value=1) -&gt; Tensor 
<a name="l407"><span class="ln">407  </span></a> 
<a name="l408"><span class="ln">408  </span></a>See :func:`torch.addcdiv` 
<a name="l409"><span class="ln">409  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l410"><span class="ln">410  </span></a><span class="s3">)</span>
<a name="l411"><span class="ln">411  </span></a>
<a name="l412"><span class="ln">412  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l413"><span class="ln">413  </span></a>    <span class="s4">&quot;addcdiv_&quot;</span><span class="s3">,</span>
<a name="l414"><span class="ln">414  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l415"><span class="ln">415  </span></a>addcdiv_(tensor1, tensor2, *, value=1) -&gt; Tensor 
<a name="l416"><span class="ln">416  </span></a> 
<a name="l417"><span class="ln">417  </span></a>In-place version of :meth:`~Tensor.addcdiv` 
<a name="l418"><span class="ln">418  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l419"><span class="ln">419  </span></a><span class="s3">)</span>
<a name="l420"><span class="ln">420  </span></a>
<a name="l421"><span class="ln">421  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l422"><span class="ln">422  </span></a>    <span class="s4">&quot;addcmul&quot;</span><span class="s3">,</span>
<a name="l423"><span class="ln">423  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l424"><span class="ln">424  </span></a>addcmul(tensor1, tensor2, *, value=1) -&gt; Tensor 
<a name="l425"><span class="ln">425  </span></a> 
<a name="l426"><span class="ln">426  </span></a>See :func:`torch.addcmul` 
<a name="l427"><span class="ln">427  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l428"><span class="ln">428  </span></a><span class="s3">)</span>
<a name="l429"><span class="ln">429  </span></a>
<a name="l430"><span class="ln">430  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l431"><span class="ln">431  </span></a>    <span class="s4">&quot;addcmul_&quot;</span><span class="s3">,</span>
<a name="l432"><span class="ln">432  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l433"><span class="ln">433  </span></a>addcmul_(tensor1, tensor2, *, value=1) -&gt; Tensor 
<a name="l434"><span class="ln">434  </span></a> 
<a name="l435"><span class="ln">435  </span></a>In-place version of :meth:`~Tensor.addcmul` 
<a name="l436"><span class="ln">436  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l437"><span class="ln">437  </span></a><span class="s3">)</span>
<a name="l438"><span class="ln">438  </span></a>
<a name="l439"><span class="ln">439  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l440"><span class="ln">440  </span></a>    <span class="s4">&quot;addmm&quot;</span><span class="s3">,</span>
<a name="l441"><span class="ln">441  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l442"><span class="ln">442  </span></a>addmm(mat1, mat2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l443"><span class="ln">443  </span></a> 
<a name="l444"><span class="ln">444  </span></a>See :func:`torch.addmm` 
<a name="l445"><span class="ln">445  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l446"><span class="ln">446  </span></a><span class="s3">)</span>
<a name="l447"><span class="ln">447  </span></a>
<a name="l448"><span class="ln">448  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l449"><span class="ln">449  </span></a>    <span class="s4">&quot;addmm_&quot;</span><span class="s3">,</span>
<a name="l450"><span class="ln">450  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l451"><span class="ln">451  </span></a>addmm_(mat1, mat2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l452"><span class="ln">452  </span></a> 
<a name="l453"><span class="ln">453  </span></a>In-place version of :meth:`~Tensor.addmm` 
<a name="l454"><span class="ln">454  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l455"><span class="ln">455  </span></a><span class="s3">)</span>
<a name="l456"><span class="ln">456  </span></a>
<a name="l457"><span class="ln">457  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l458"><span class="ln">458  </span></a>    <span class="s4">&quot;addmv&quot;</span><span class="s3">,</span>
<a name="l459"><span class="ln">459  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l460"><span class="ln">460  </span></a>addmv(mat, vec, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l461"><span class="ln">461  </span></a> 
<a name="l462"><span class="ln">462  </span></a>See :func:`torch.addmv` 
<a name="l463"><span class="ln">463  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l464"><span class="ln">464  </span></a><span class="s3">)</span>
<a name="l465"><span class="ln">465  </span></a>
<a name="l466"><span class="ln">466  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l467"><span class="ln">467  </span></a>    <span class="s4">&quot;addmv_&quot;</span><span class="s3">,</span>
<a name="l468"><span class="ln">468  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l469"><span class="ln">469  </span></a>addmv_(mat, vec, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l470"><span class="ln">470  </span></a> 
<a name="l471"><span class="ln">471  </span></a>In-place version of :meth:`~Tensor.addmv` 
<a name="l472"><span class="ln">472  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l473"><span class="ln">473  </span></a><span class="s3">)</span>
<a name="l474"><span class="ln">474  </span></a>
<a name="l475"><span class="ln">475  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l476"><span class="ln">476  </span></a>    <span class="s4">&quot;sspaddmm&quot;</span><span class="s3">,</span>
<a name="l477"><span class="ln">477  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l478"><span class="ln">478  </span></a>sspaddmm(mat1, mat2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l479"><span class="ln">479  </span></a> 
<a name="l480"><span class="ln">480  </span></a>See :func:`torch.sspaddmm` 
<a name="l481"><span class="ln">481  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l482"><span class="ln">482  </span></a><span class="s3">)</span>
<a name="l483"><span class="ln">483  </span></a>
<a name="l484"><span class="ln">484  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l485"><span class="ln">485  </span></a>    <span class="s4">&quot;smm&quot;</span><span class="s3">,</span>
<a name="l486"><span class="ln">486  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l487"><span class="ln">487  </span></a>smm(mat) -&gt; Tensor 
<a name="l488"><span class="ln">488  </span></a> 
<a name="l489"><span class="ln">489  </span></a>See :func:`torch.smm` 
<a name="l490"><span class="ln">490  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l491"><span class="ln">491  </span></a><span class="s3">)</span>
<a name="l492"><span class="ln">492  </span></a>
<a name="l493"><span class="ln">493  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l494"><span class="ln">494  </span></a>    <span class="s4">&quot;addr&quot;</span><span class="s3">,</span>
<a name="l495"><span class="ln">495  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l496"><span class="ln">496  </span></a>addr(vec1, vec2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l497"><span class="ln">497  </span></a> 
<a name="l498"><span class="ln">498  </span></a>See :func:`torch.addr` 
<a name="l499"><span class="ln">499  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l500"><span class="ln">500  </span></a><span class="s3">)</span>
<a name="l501"><span class="ln">501  </span></a>
<a name="l502"><span class="ln">502  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l503"><span class="ln">503  </span></a>    <span class="s4">&quot;addr_&quot;</span><span class="s3">,</span>
<a name="l504"><span class="ln">504  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l505"><span class="ln">505  </span></a>addr_(vec1, vec2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l506"><span class="ln">506  </span></a> 
<a name="l507"><span class="ln">507  </span></a>In-place version of :meth:`~Tensor.addr` 
<a name="l508"><span class="ln">508  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l509"><span class="ln">509  </span></a><span class="s3">)</span>
<a name="l510"><span class="ln">510  </span></a>
<a name="l511"><span class="ln">511  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l512"><span class="ln">512  </span></a>    <span class="s4">&quot;align_as&quot;</span><span class="s3">,</span>
<a name="l513"><span class="ln">513  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l514"><span class="ln">514  </span></a>align_as(other) -&gt; Tensor 
<a name="l515"><span class="ln">515  </span></a> 
<a name="l516"><span class="ln">516  </span></a>Permutes the dimensions of the :attr:`self` tensor to match the dimension order 
<a name="l517"><span class="ln">517  </span></a>in the :attr:`other` tensor, adding size-one dims for any new names. 
<a name="l518"><span class="ln">518  </span></a> 
<a name="l519"><span class="ln">519  </span></a>This operation is useful for explicit broadcasting by names (see examples). 
<a name="l520"><span class="ln">520  </span></a> 
<a name="l521"><span class="ln">521  </span></a>All of the dims of :attr:`self` must be named in order to use this method. 
<a name="l522"><span class="ln">522  </span></a>The resulting tensor is a view on the original tensor. 
<a name="l523"><span class="ln">523  </span></a> 
<a name="l524"><span class="ln">524  </span></a>All dimension names of :attr:`self` must be present in ``other.names``. 
<a name="l525"><span class="ln">525  </span></a>:attr:`other` may contain named dimensions that are not in ``self.names``; 
<a name="l526"><span class="ln">526  </span></a>the output tensor has a size-one dimension for each of those new names. 
<a name="l527"><span class="ln">527  </span></a> 
<a name="l528"><span class="ln">528  </span></a>To align a tensor to a specific order, use :meth:`~Tensor.align_to`. 
<a name="l529"><span class="ln">529  </span></a> 
<a name="l530"><span class="ln">530  </span></a>Examples:: 
<a name="l531"><span class="ln">531  </span></a> 
<a name="l532"><span class="ln">532  </span></a>    # Example 1: Applying a mask 
<a name="l533"><span class="ln">533  </span></a>    &gt;&gt;&gt; mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H') 
<a name="l534"><span class="ln">534  </span></a>    &gt;&gt;&gt; imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C')) 
<a name="l535"><span class="ln">535  </span></a>    &gt;&gt;&gt; imgs.masked_fill_(mask.align_as(imgs), 0) 
<a name="l536"><span class="ln">536  </span></a> 
<a name="l537"><span class="ln">537  </span></a> 
<a name="l538"><span class="ln">538  </span></a>    # Example 2: Applying a per-channel-scale 
<a name="l539"><span class="ln">539  </span></a>    &gt;&gt;&gt; def scale_channels(input, scale): 
<a name="l540"><span class="ln">540  </span></a>    &gt;&gt;&gt;    scale = scale.refine_names('C') 
<a name="l541"><span class="ln">541  </span></a>    &gt;&gt;&gt;    return input * scale.align_as(input) 
<a name="l542"><span class="ln">542  </span></a> 
<a name="l543"><span class="ln">543  </span></a>    &gt;&gt;&gt; num_channels = 3 
<a name="l544"><span class="ln">544  </span></a>    &gt;&gt;&gt; scale = torch.randn(num_channels, names=('C',)) 
<a name="l545"><span class="ln">545  </span></a>    &gt;&gt;&gt; imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C')) 
<a name="l546"><span class="ln">546  </span></a>    &gt;&gt;&gt; more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W')) 
<a name="l547"><span class="ln">547  </span></a>    &gt;&gt;&gt; videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D')) 
<a name="l548"><span class="ln">548  </span></a> 
<a name="l549"><span class="ln">549  </span></a>    # scale_channels is agnostic to the dimension order of the input 
<a name="l550"><span class="ln">550  </span></a>    &gt;&gt;&gt; scale_channels(imgs, scale) 
<a name="l551"><span class="ln">551  </span></a>    &gt;&gt;&gt; scale_channels(more_imgs, scale) 
<a name="l552"><span class="ln">552  </span></a>    &gt;&gt;&gt; scale_channels(videos, scale) 
<a name="l553"><span class="ln">553  </span></a> 
<a name="l554"><span class="ln">554  </span></a>.. warning:: 
<a name="l555"><span class="ln">555  </span></a>    The named tensor API is experimental and subject to change. 
<a name="l556"><span class="ln">556  </span></a> 
<a name="l557"><span class="ln">557  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l558"><span class="ln">558  </span></a><span class="s3">)</span>
<a name="l559"><span class="ln">559  </span></a>
<a name="l560"><span class="ln">560  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l561"><span class="ln">561  </span></a>    <span class="s4">&quot;all&quot;</span><span class="s3">,</span>
<a name="l562"><span class="ln">562  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l563"><span class="ln">563  </span></a>all(dim=None, keepdim=False) -&gt; Tensor 
<a name="l564"><span class="ln">564  </span></a> 
<a name="l565"><span class="ln">565  </span></a>See :func:`torch.all` 
<a name="l566"><span class="ln">566  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l567"><span class="ln">567  </span></a><span class="s3">)</span>
<a name="l568"><span class="ln">568  </span></a>
<a name="l569"><span class="ln">569  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l570"><span class="ln">570  </span></a>    <span class="s4">&quot;allclose&quot;</span><span class="s3">,</span>
<a name="l571"><span class="ln">571  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l572"><span class="ln">572  </span></a>allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -&gt; Tensor 
<a name="l573"><span class="ln">573  </span></a> 
<a name="l574"><span class="ln">574  </span></a>See :func:`torch.allclose` 
<a name="l575"><span class="ln">575  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l576"><span class="ln">576  </span></a><span class="s3">)</span>
<a name="l577"><span class="ln">577  </span></a>
<a name="l578"><span class="ln">578  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l579"><span class="ln">579  </span></a>    <span class="s4">&quot;angle&quot;</span><span class="s3">,</span>
<a name="l580"><span class="ln">580  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l581"><span class="ln">581  </span></a>angle() -&gt; Tensor 
<a name="l582"><span class="ln">582  </span></a> 
<a name="l583"><span class="ln">583  </span></a>See :func:`torch.angle` 
<a name="l584"><span class="ln">584  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l585"><span class="ln">585  </span></a><span class="s3">)</span>
<a name="l586"><span class="ln">586  </span></a>
<a name="l587"><span class="ln">587  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l588"><span class="ln">588  </span></a>    <span class="s4">&quot;any&quot;</span><span class="s3">,</span>
<a name="l589"><span class="ln">589  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l590"><span class="ln">590  </span></a>any(dim=None, keepdim=False) -&gt; Tensor 
<a name="l591"><span class="ln">591  </span></a> 
<a name="l592"><span class="ln">592  </span></a>See :func:`torch.any` 
<a name="l593"><span class="ln">593  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l594"><span class="ln">594  </span></a><span class="s3">)</span>
<a name="l595"><span class="ln">595  </span></a>
<a name="l596"><span class="ln">596  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l597"><span class="ln">597  </span></a>    <span class="s4">&quot;apply_&quot;</span><span class="s3">,</span>
<a name="l598"><span class="ln">598  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l599"><span class="ln">599  </span></a>apply_(callable) -&gt; Tensor 
<a name="l600"><span class="ln">600  </span></a> 
<a name="l601"><span class="ln">601  </span></a>Applies the function :attr:`callable` to each element in the tensor, replacing 
<a name="l602"><span class="ln">602  </span></a>each element with the value returned by :attr:`callable`. 
<a name="l603"><span class="ln">603  </span></a> 
<a name="l604"><span class="ln">604  </span></a>.. note:: 
<a name="l605"><span class="ln">605  </span></a> 
<a name="l606"><span class="ln">606  </span></a>    This function only works with CPU tensors and should not be used in code 
<a name="l607"><span class="ln">607  </span></a>    sections that require high performance. 
<a name="l608"><span class="ln">608  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l609"><span class="ln">609  </span></a><span class="s3">)</span>
<a name="l610"><span class="ln">610  </span></a>
<a name="l611"><span class="ln">611  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l612"><span class="ln">612  </span></a>    <span class="s4">&quot;asin&quot;</span><span class="s3">,</span>
<a name="l613"><span class="ln">613  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l614"><span class="ln">614  </span></a>asin() -&gt; Tensor 
<a name="l615"><span class="ln">615  </span></a> 
<a name="l616"><span class="ln">616  </span></a>See :func:`torch.asin` 
<a name="l617"><span class="ln">617  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l618"><span class="ln">618  </span></a><span class="s3">)</span>
<a name="l619"><span class="ln">619  </span></a>
<a name="l620"><span class="ln">620  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l621"><span class="ln">621  </span></a>    <span class="s4">&quot;asin_&quot;</span><span class="s3">,</span>
<a name="l622"><span class="ln">622  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l623"><span class="ln">623  </span></a>asin_() -&gt; Tensor 
<a name="l624"><span class="ln">624  </span></a> 
<a name="l625"><span class="ln">625  </span></a>In-place version of :meth:`~Tensor.asin` 
<a name="l626"><span class="ln">626  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l627"><span class="ln">627  </span></a><span class="s3">)</span>
<a name="l628"><span class="ln">628  </span></a>
<a name="l629"><span class="ln">629  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l630"><span class="ln">630  </span></a>    <span class="s4">&quot;arcsin&quot;</span><span class="s3">,</span>
<a name="l631"><span class="ln">631  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l632"><span class="ln">632  </span></a>arcsin() -&gt; Tensor 
<a name="l633"><span class="ln">633  </span></a> 
<a name="l634"><span class="ln">634  </span></a>See :func:`torch.arcsin` 
<a name="l635"><span class="ln">635  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l636"><span class="ln">636  </span></a><span class="s3">)</span>
<a name="l637"><span class="ln">637  </span></a>
<a name="l638"><span class="ln">638  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l639"><span class="ln">639  </span></a>    <span class="s4">&quot;arcsin_&quot;</span><span class="s3">,</span>
<a name="l640"><span class="ln">640  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l641"><span class="ln">641  </span></a>arcsin_() -&gt; Tensor 
<a name="l642"><span class="ln">642  </span></a> 
<a name="l643"><span class="ln">643  </span></a>In-place version of :meth:`~Tensor.arcsin` 
<a name="l644"><span class="ln">644  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l645"><span class="ln">645  </span></a><span class="s3">)</span>
<a name="l646"><span class="ln">646  </span></a>
<a name="l647"><span class="ln">647  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l648"><span class="ln">648  </span></a>    <span class="s4">&quot;asinh&quot;</span><span class="s3">,</span>
<a name="l649"><span class="ln">649  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l650"><span class="ln">650  </span></a>asinh() -&gt; Tensor 
<a name="l651"><span class="ln">651  </span></a> 
<a name="l652"><span class="ln">652  </span></a>See :func:`torch.asinh` 
<a name="l653"><span class="ln">653  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l654"><span class="ln">654  </span></a><span class="s3">)</span>
<a name="l655"><span class="ln">655  </span></a>
<a name="l656"><span class="ln">656  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l657"><span class="ln">657  </span></a>    <span class="s4">&quot;asinh_&quot;</span><span class="s3">,</span>
<a name="l658"><span class="ln">658  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l659"><span class="ln">659  </span></a>asinh_() -&gt; Tensor 
<a name="l660"><span class="ln">660  </span></a> 
<a name="l661"><span class="ln">661  </span></a>In-place version of :meth:`~Tensor.asinh` 
<a name="l662"><span class="ln">662  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l663"><span class="ln">663  </span></a><span class="s3">)</span>
<a name="l664"><span class="ln">664  </span></a>
<a name="l665"><span class="ln">665  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l666"><span class="ln">666  </span></a>    <span class="s4">&quot;arcsinh&quot;</span><span class="s3">,</span>
<a name="l667"><span class="ln">667  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l668"><span class="ln">668  </span></a>arcsinh() -&gt; Tensor 
<a name="l669"><span class="ln">669  </span></a> 
<a name="l670"><span class="ln">670  </span></a>See :func:`torch.arcsinh` 
<a name="l671"><span class="ln">671  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l672"><span class="ln">672  </span></a><span class="s3">)</span>
<a name="l673"><span class="ln">673  </span></a>
<a name="l674"><span class="ln">674  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l675"><span class="ln">675  </span></a>    <span class="s4">&quot;arcsinh_&quot;</span><span class="s3">,</span>
<a name="l676"><span class="ln">676  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l677"><span class="ln">677  </span></a>arcsinh_() -&gt; Tensor 
<a name="l678"><span class="ln">678  </span></a> 
<a name="l679"><span class="ln">679  </span></a>In-place version of :meth:`~Tensor.arcsinh` 
<a name="l680"><span class="ln">680  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l681"><span class="ln">681  </span></a><span class="s3">)</span>
<a name="l682"><span class="ln">682  </span></a>
<a name="l683"><span class="ln">683  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l684"><span class="ln">684  </span></a>    <span class="s4">&quot;as_strided&quot;</span><span class="s3">,</span>
<a name="l685"><span class="ln">685  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l686"><span class="ln">686  </span></a>as_strided(size, stride, storage_offset=None) -&gt; Tensor 
<a name="l687"><span class="ln">687  </span></a> 
<a name="l688"><span class="ln">688  </span></a>See :func:`torch.as_strided` 
<a name="l689"><span class="ln">689  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l690"><span class="ln">690  </span></a><span class="s3">)</span>
<a name="l691"><span class="ln">691  </span></a>
<a name="l692"><span class="ln">692  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l693"><span class="ln">693  </span></a>    <span class="s4">&quot;as_strided_&quot;</span><span class="s3">,</span>
<a name="l694"><span class="ln">694  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l695"><span class="ln">695  </span></a>as_strided_(size, stride, storage_offset=None) -&gt; Tensor 
<a name="l696"><span class="ln">696  </span></a> 
<a name="l697"><span class="ln">697  </span></a>In-place version of :meth:`~Tensor.as_strided` 
<a name="l698"><span class="ln">698  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l699"><span class="ln">699  </span></a><span class="s3">)</span>
<a name="l700"><span class="ln">700  </span></a>
<a name="l701"><span class="ln">701  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l702"><span class="ln">702  </span></a>    <span class="s4">&quot;atan&quot;</span><span class="s3">,</span>
<a name="l703"><span class="ln">703  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l704"><span class="ln">704  </span></a>atan() -&gt; Tensor 
<a name="l705"><span class="ln">705  </span></a> 
<a name="l706"><span class="ln">706  </span></a>See :func:`torch.atan` 
<a name="l707"><span class="ln">707  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l708"><span class="ln">708  </span></a><span class="s3">)</span>
<a name="l709"><span class="ln">709  </span></a>
<a name="l710"><span class="ln">710  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l711"><span class="ln">711  </span></a>    <span class="s4">&quot;atan_&quot;</span><span class="s3">,</span>
<a name="l712"><span class="ln">712  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l713"><span class="ln">713  </span></a>atan_() -&gt; Tensor 
<a name="l714"><span class="ln">714  </span></a> 
<a name="l715"><span class="ln">715  </span></a>In-place version of :meth:`~Tensor.atan` 
<a name="l716"><span class="ln">716  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l717"><span class="ln">717  </span></a><span class="s3">)</span>
<a name="l718"><span class="ln">718  </span></a>
<a name="l719"><span class="ln">719  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l720"><span class="ln">720  </span></a>    <span class="s4">&quot;arctan&quot;</span><span class="s3">,</span>
<a name="l721"><span class="ln">721  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l722"><span class="ln">722  </span></a>arctan() -&gt; Tensor 
<a name="l723"><span class="ln">723  </span></a> 
<a name="l724"><span class="ln">724  </span></a>See :func:`torch.arctan` 
<a name="l725"><span class="ln">725  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l726"><span class="ln">726  </span></a><span class="s3">)</span>
<a name="l727"><span class="ln">727  </span></a>
<a name="l728"><span class="ln">728  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l729"><span class="ln">729  </span></a>    <span class="s4">&quot;arctan_&quot;</span><span class="s3">,</span>
<a name="l730"><span class="ln">730  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l731"><span class="ln">731  </span></a>arctan_() -&gt; Tensor 
<a name="l732"><span class="ln">732  </span></a> 
<a name="l733"><span class="ln">733  </span></a>In-place version of :meth:`~Tensor.arctan` 
<a name="l734"><span class="ln">734  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l735"><span class="ln">735  </span></a><span class="s3">)</span>
<a name="l736"><span class="ln">736  </span></a>
<a name="l737"><span class="ln">737  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l738"><span class="ln">738  </span></a>    <span class="s4">&quot;atan2&quot;</span><span class="s3">,</span>
<a name="l739"><span class="ln">739  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l740"><span class="ln">740  </span></a>atan2(other) -&gt; Tensor 
<a name="l741"><span class="ln">741  </span></a> 
<a name="l742"><span class="ln">742  </span></a>See :func:`torch.atan2` 
<a name="l743"><span class="ln">743  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l744"><span class="ln">744  </span></a><span class="s3">)</span>
<a name="l745"><span class="ln">745  </span></a>
<a name="l746"><span class="ln">746  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l747"><span class="ln">747  </span></a>    <span class="s4">&quot;atan2_&quot;</span><span class="s3">,</span>
<a name="l748"><span class="ln">748  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l749"><span class="ln">749  </span></a>atan2_(other) -&gt; Tensor 
<a name="l750"><span class="ln">750  </span></a> 
<a name="l751"><span class="ln">751  </span></a>In-place version of :meth:`~Tensor.atan2` 
<a name="l752"><span class="ln">752  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l753"><span class="ln">753  </span></a><span class="s3">)</span>
<a name="l754"><span class="ln">754  </span></a>
<a name="l755"><span class="ln">755  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l756"><span class="ln">756  </span></a>    <span class="s4">&quot;arctan2&quot;</span><span class="s3">,</span>
<a name="l757"><span class="ln">757  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l758"><span class="ln">758  </span></a>arctan2(other) -&gt; Tensor 
<a name="l759"><span class="ln">759  </span></a> 
<a name="l760"><span class="ln">760  </span></a>See :func:`torch.arctan2` 
<a name="l761"><span class="ln">761  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l762"><span class="ln">762  </span></a><span class="s3">)</span>
<a name="l763"><span class="ln">763  </span></a>
<a name="l764"><span class="ln">764  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l765"><span class="ln">765  </span></a>    <span class="s4">&quot;arctan2_&quot;</span><span class="s3">,</span>
<a name="l766"><span class="ln">766  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l767"><span class="ln">767  </span></a>atan2_(other) -&gt; Tensor 
<a name="l768"><span class="ln">768  </span></a> 
<a name="l769"><span class="ln">769  </span></a>In-place version of :meth:`~Tensor.arctan2` 
<a name="l770"><span class="ln">770  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l771"><span class="ln">771  </span></a><span class="s3">)</span>
<a name="l772"><span class="ln">772  </span></a>
<a name="l773"><span class="ln">773  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l774"><span class="ln">774  </span></a>    <span class="s4">&quot;atanh&quot;</span><span class="s3">,</span>
<a name="l775"><span class="ln">775  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l776"><span class="ln">776  </span></a>atanh() -&gt; Tensor 
<a name="l777"><span class="ln">777  </span></a> 
<a name="l778"><span class="ln">778  </span></a>See :func:`torch.atanh` 
<a name="l779"><span class="ln">779  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l780"><span class="ln">780  </span></a><span class="s3">)</span>
<a name="l781"><span class="ln">781  </span></a>
<a name="l782"><span class="ln">782  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l783"><span class="ln">783  </span></a>    <span class="s4">&quot;atanh_&quot;</span><span class="s3">,</span>
<a name="l784"><span class="ln">784  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l785"><span class="ln">785  </span></a>atanh_(other) -&gt; Tensor 
<a name="l786"><span class="ln">786  </span></a> 
<a name="l787"><span class="ln">787  </span></a>In-place version of :meth:`~Tensor.atanh` 
<a name="l788"><span class="ln">788  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l789"><span class="ln">789  </span></a><span class="s3">)</span>
<a name="l790"><span class="ln">790  </span></a>
<a name="l791"><span class="ln">791  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l792"><span class="ln">792  </span></a>    <span class="s4">&quot;arctanh&quot;</span><span class="s3">,</span>
<a name="l793"><span class="ln">793  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l794"><span class="ln">794  </span></a>arctanh() -&gt; Tensor 
<a name="l795"><span class="ln">795  </span></a> 
<a name="l796"><span class="ln">796  </span></a>See :func:`torch.arctanh` 
<a name="l797"><span class="ln">797  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l798"><span class="ln">798  </span></a><span class="s3">)</span>
<a name="l799"><span class="ln">799  </span></a>
<a name="l800"><span class="ln">800  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l801"><span class="ln">801  </span></a>    <span class="s4">&quot;arctanh_&quot;</span><span class="s3">,</span>
<a name="l802"><span class="ln">802  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l803"><span class="ln">803  </span></a>arctanh_(other) -&gt; Tensor 
<a name="l804"><span class="ln">804  </span></a> 
<a name="l805"><span class="ln">805  </span></a>In-place version of :meth:`~Tensor.arctanh` 
<a name="l806"><span class="ln">806  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l807"><span class="ln">807  </span></a><span class="s3">)</span>
<a name="l808"><span class="ln">808  </span></a>
<a name="l809"><span class="ln">809  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l810"><span class="ln">810  </span></a>    <span class="s4">&quot;baddbmm&quot;</span><span class="s3">,</span>
<a name="l811"><span class="ln">811  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l812"><span class="ln">812  </span></a>baddbmm(batch1, batch2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l813"><span class="ln">813  </span></a> 
<a name="l814"><span class="ln">814  </span></a>See :func:`torch.baddbmm` 
<a name="l815"><span class="ln">815  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l816"><span class="ln">816  </span></a><span class="s3">)</span>
<a name="l817"><span class="ln">817  </span></a>
<a name="l818"><span class="ln">818  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l819"><span class="ln">819  </span></a>    <span class="s4">&quot;baddbmm_&quot;</span><span class="s3">,</span>
<a name="l820"><span class="ln">820  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l821"><span class="ln">821  </span></a>baddbmm_(batch1, batch2, *, beta=1, alpha=1) -&gt; Tensor 
<a name="l822"><span class="ln">822  </span></a> 
<a name="l823"><span class="ln">823  </span></a>In-place version of :meth:`~Tensor.baddbmm` 
<a name="l824"><span class="ln">824  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l825"><span class="ln">825  </span></a><span class="s3">)</span>
<a name="l826"><span class="ln">826  </span></a>
<a name="l827"><span class="ln">827  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l828"><span class="ln">828  </span></a>    <span class="s4">&quot;bernoulli&quot;</span><span class="s3">,</span>
<a name="l829"><span class="ln">829  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l830"><span class="ln">830  </span></a>bernoulli(*, generator=None) -&gt; Tensor 
<a name="l831"><span class="ln">831  </span></a> 
<a name="l832"><span class="ln">832  </span></a>Returns a result tensor where each :math:`\texttt{result[i]}` is independently 
<a name="l833"><span class="ln">833  </span></a>sampled from :math:`\text{Bernoulli}(\texttt{self[i]})`. :attr:`self` must have 
<a name="l834"><span class="ln">834  </span></a>floating point ``dtype``, and the result will have the same ``dtype``. 
<a name="l835"><span class="ln">835  </span></a> 
<a name="l836"><span class="ln">836  </span></a>See :func:`torch.bernoulli` 
<a name="l837"><span class="ln">837  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l838"><span class="ln">838  </span></a><span class="s3">)</span>
<a name="l839"><span class="ln">839  </span></a>
<a name="l840"><span class="ln">840  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l841"><span class="ln">841  </span></a>    <span class="s4">&quot;bernoulli_&quot;</span><span class="s3">,</span>
<a name="l842"><span class="ln">842  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l843"><span class="ln">843  </span></a>bernoulli_(p=0.5, *, generator=None) -&gt; Tensor 
<a name="l844"><span class="ln">844  </span></a> 
<a name="l845"><span class="ln">845  </span></a>Fills each location of :attr:`self` with an independent sample from 
<a name="l846"><span class="ln">846  </span></a>:math:`\text{Bernoulli}(\texttt{p})`. :attr:`self` can have integral 
<a name="l847"><span class="ln">847  </span></a>``dtype``. 
<a name="l848"><span class="ln">848  </span></a> 
<a name="l849"><span class="ln">849  </span></a>:attr:`p` should either be a scalar or tensor containing probabilities to be 
<a name="l850"><span class="ln">850  </span></a>used for drawing the binary random number. 
<a name="l851"><span class="ln">851  </span></a> 
<a name="l852"><span class="ln">852  </span></a>If it is a tensor, the :math:`\text{i}^{th}` element of :attr:`self` tensor 
<a name="l853"><span class="ln">853  </span></a>will be set to a value sampled from 
<a name="l854"><span class="ln">854  </span></a>:math:`\text{Bernoulli}(\texttt{p\_tensor[i]})`. In this case `p` must have 
<a name="l855"><span class="ln">855  </span></a>floating point ``dtype``. 
<a name="l856"><span class="ln">856  </span></a> 
<a name="l857"><span class="ln">857  </span></a>See also :meth:`~Tensor.bernoulli` and :func:`torch.bernoulli` 
<a name="l858"><span class="ln">858  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l859"><span class="ln">859  </span></a><span class="s3">)</span>
<a name="l860"><span class="ln">860  </span></a>
<a name="l861"><span class="ln">861  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l862"><span class="ln">862  </span></a>    <span class="s4">&quot;bincount&quot;</span><span class="s3">,</span>
<a name="l863"><span class="ln">863  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l864"><span class="ln">864  </span></a>bincount(weights=None, minlength=0) -&gt; Tensor 
<a name="l865"><span class="ln">865  </span></a> 
<a name="l866"><span class="ln">866  </span></a>See :func:`torch.bincount` 
<a name="l867"><span class="ln">867  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l868"><span class="ln">868  </span></a><span class="s3">)</span>
<a name="l869"><span class="ln">869  </span></a>
<a name="l870"><span class="ln">870  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l871"><span class="ln">871  </span></a>    <span class="s4">&quot;bitwise_not&quot;</span><span class="s3">,</span>
<a name="l872"><span class="ln">872  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l873"><span class="ln">873  </span></a>bitwise_not() -&gt; Tensor 
<a name="l874"><span class="ln">874  </span></a> 
<a name="l875"><span class="ln">875  </span></a>See :func:`torch.bitwise_not` 
<a name="l876"><span class="ln">876  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l877"><span class="ln">877  </span></a><span class="s3">)</span>
<a name="l878"><span class="ln">878  </span></a>
<a name="l879"><span class="ln">879  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l880"><span class="ln">880  </span></a>    <span class="s4">&quot;bitwise_not_&quot;</span><span class="s3">,</span>
<a name="l881"><span class="ln">881  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l882"><span class="ln">882  </span></a>bitwise_not_() -&gt; Tensor 
<a name="l883"><span class="ln">883  </span></a> 
<a name="l884"><span class="ln">884  </span></a>In-place version of :meth:`~Tensor.bitwise_not` 
<a name="l885"><span class="ln">885  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l886"><span class="ln">886  </span></a><span class="s3">)</span>
<a name="l887"><span class="ln">887  </span></a>
<a name="l888"><span class="ln">888  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l889"><span class="ln">889  </span></a>    <span class="s4">&quot;bitwise_and&quot;</span><span class="s3">,</span>
<a name="l890"><span class="ln">890  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l891"><span class="ln">891  </span></a>bitwise_and() -&gt; Tensor 
<a name="l892"><span class="ln">892  </span></a> 
<a name="l893"><span class="ln">893  </span></a>See :func:`torch.bitwise_and` 
<a name="l894"><span class="ln">894  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l895"><span class="ln">895  </span></a><span class="s3">)</span>
<a name="l896"><span class="ln">896  </span></a>
<a name="l897"><span class="ln">897  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l898"><span class="ln">898  </span></a>    <span class="s4">&quot;bitwise_and_&quot;</span><span class="s3">,</span>
<a name="l899"><span class="ln">899  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l900"><span class="ln">900  </span></a>bitwise_and_() -&gt; Tensor 
<a name="l901"><span class="ln">901  </span></a> 
<a name="l902"><span class="ln">902  </span></a>In-place version of :meth:`~Tensor.bitwise_and` 
<a name="l903"><span class="ln">903  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l904"><span class="ln">904  </span></a><span class="s3">)</span>
<a name="l905"><span class="ln">905  </span></a>
<a name="l906"><span class="ln">906  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l907"><span class="ln">907  </span></a>    <span class="s4">&quot;bitwise_or&quot;</span><span class="s3">,</span>
<a name="l908"><span class="ln">908  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l909"><span class="ln">909  </span></a>bitwise_or() -&gt; Tensor 
<a name="l910"><span class="ln">910  </span></a> 
<a name="l911"><span class="ln">911  </span></a>See :func:`torch.bitwise_or` 
<a name="l912"><span class="ln">912  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l913"><span class="ln">913  </span></a><span class="s3">)</span>
<a name="l914"><span class="ln">914  </span></a>
<a name="l915"><span class="ln">915  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l916"><span class="ln">916  </span></a>    <span class="s4">&quot;bitwise_or_&quot;</span><span class="s3">,</span>
<a name="l917"><span class="ln">917  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l918"><span class="ln">918  </span></a>bitwise_or_() -&gt; Tensor 
<a name="l919"><span class="ln">919  </span></a> 
<a name="l920"><span class="ln">920  </span></a>In-place version of :meth:`~Tensor.bitwise_or` 
<a name="l921"><span class="ln">921  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l922"><span class="ln">922  </span></a><span class="s3">)</span>
<a name="l923"><span class="ln">923  </span></a>
<a name="l924"><span class="ln">924  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l925"><span class="ln">925  </span></a>    <span class="s4">&quot;bitwise_xor&quot;</span><span class="s3">,</span>
<a name="l926"><span class="ln">926  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l927"><span class="ln">927  </span></a>bitwise_xor() -&gt; Tensor 
<a name="l928"><span class="ln">928  </span></a> 
<a name="l929"><span class="ln">929  </span></a>See :func:`torch.bitwise_xor` 
<a name="l930"><span class="ln">930  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l931"><span class="ln">931  </span></a><span class="s3">)</span>
<a name="l932"><span class="ln">932  </span></a>
<a name="l933"><span class="ln">933  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l934"><span class="ln">934  </span></a>    <span class="s4">&quot;bitwise_xor_&quot;</span><span class="s3">,</span>
<a name="l935"><span class="ln">935  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l936"><span class="ln">936  </span></a>bitwise_xor_() -&gt; Tensor 
<a name="l937"><span class="ln">937  </span></a> 
<a name="l938"><span class="ln">938  </span></a>In-place version of :meth:`~Tensor.bitwise_xor` 
<a name="l939"><span class="ln">939  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l940"><span class="ln">940  </span></a><span class="s3">)</span>
<a name="l941"><span class="ln">941  </span></a>
<a name="l942"><span class="ln">942  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l943"><span class="ln">943  </span></a>    <span class="s4">&quot;bitwise_left_shift&quot;</span><span class="s3">,</span>
<a name="l944"><span class="ln">944  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l945"><span class="ln">945  </span></a>bitwise_left_shift(other) -&gt; Tensor 
<a name="l946"><span class="ln">946  </span></a> 
<a name="l947"><span class="ln">947  </span></a>See :func:`torch.bitwise_left_shift` 
<a name="l948"><span class="ln">948  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l949"><span class="ln">949  </span></a><span class="s3">)</span>
<a name="l950"><span class="ln">950  </span></a>
<a name="l951"><span class="ln">951  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l952"><span class="ln">952  </span></a>    <span class="s4">&quot;bitwise_left_shift_&quot;</span><span class="s3">,</span>
<a name="l953"><span class="ln">953  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l954"><span class="ln">954  </span></a>bitwise_left_shift_(other) -&gt; Tensor 
<a name="l955"><span class="ln">955  </span></a> 
<a name="l956"><span class="ln">956  </span></a>In-place version of :meth:`~Tensor.bitwise_left_shift` 
<a name="l957"><span class="ln">957  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l958"><span class="ln">958  </span></a><span class="s3">)</span>
<a name="l959"><span class="ln">959  </span></a>
<a name="l960"><span class="ln">960  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l961"><span class="ln">961  </span></a>    <span class="s4">&quot;bitwise_right_shift&quot;</span><span class="s3">,</span>
<a name="l962"><span class="ln">962  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l963"><span class="ln">963  </span></a>bitwise_right_shift(other) -&gt; Tensor 
<a name="l964"><span class="ln">964  </span></a> 
<a name="l965"><span class="ln">965  </span></a>See :func:`torch.bitwise_right_shift` 
<a name="l966"><span class="ln">966  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l967"><span class="ln">967  </span></a><span class="s3">)</span>
<a name="l968"><span class="ln">968  </span></a>
<a name="l969"><span class="ln">969  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l970"><span class="ln">970  </span></a>    <span class="s4">&quot;bitwise_right_shift_&quot;</span><span class="s3">,</span>
<a name="l971"><span class="ln">971  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l972"><span class="ln">972  </span></a>bitwise_right_shift_(other) -&gt; Tensor 
<a name="l973"><span class="ln">973  </span></a> 
<a name="l974"><span class="ln">974  </span></a>In-place version of :meth:`~Tensor.bitwise_right_shift` 
<a name="l975"><span class="ln">975  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l976"><span class="ln">976  </span></a><span class="s3">)</span>
<a name="l977"><span class="ln">977  </span></a>
<a name="l978"><span class="ln">978  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l979"><span class="ln">979  </span></a>    <span class="s4">&quot;broadcast_to&quot;</span><span class="s3">,</span>
<a name="l980"><span class="ln">980  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l981"><span class="ln">981  </span></a>broadcast_to(shape) -&gt; Tensor 
<a name="l982"><span class="ln">982  </span></a> 
<a name="l983"><span class="ln">983  </span></a>See :func:`torch.broadcast_to`. 
<a name="l984"><span class="ln">984  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l985"><span class="ln">985  </span></a><span class="s3">)</span>
<a name="l986"><span class="ln">986  </span></a>
<a name="l987"><span class="ln">987  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l988"><span class="ln">988  </span></a>    <span class="s4">&quot;logical_and&quot;</span><span class="s3">,</span>
<a name="l989"><span class="ln">989  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l990"><span class="ln">990  </span></a>logical_and() -&gt; Tensor 
<a name="l991"><span class="ln">991  </span></a> 
<a name="l992"><span class="ln">992  </span></a>See :func:`torch.logical_and` 
<a name="l993"><span class="ln">993  </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l994"><span class="ln">994  </span></a><span class="s3">)</span>
<a name="l995"><span class="ln">995  </span></a>
<a name="l996"><span class="ln">996  </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l997"><span class="ln">997  </span></a>    <span class="s4">&quot;logical_and_&quot;</span><span class="s3">,</span>
<a name="l998"><span class="ln">998  </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l999"><span class="ln">999  </span></a>logical_and_() -&gt; Tensor 
<a name="l1000"><span class="ln">1000 </span></a> 
<a name="l1001"><span class="ln">1001 </span></a>In-place version of :meth:`~Tensor.logical_and` 
<a name="l1002"><span class="ln">1002 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1003"><span class="ln">1003 </span></a><span class="s3">)</span>
<a name="l1004"><span class="ln">1004 </span></a>
<a name="l1005"><span class="ln">1005 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1006"><span class="ln">1006 </span></a>    <span class="s4">&quot;logical_not&quot;</span><span class="s3">,</span>
<a name="l1007"><span class="ln">1007 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1008"><span class="ln">1008 </span></a>logical_not() -&gt; Tensor 
<a name="l1009"><span class="ln">1009 </span></a> 
<a name="l1010"><span class="ln">1010 </span></a>See :func:`torch.logical_not` 
<a name="l1011"><span class="ln">1011 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1012"><span class="ln">1012 </span></a><span class="s3">)</span>
<a name="l1013"><span class="ln">1013 </span></a>
<a name="l1014"><span class="ln">1014 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1015"><span class="ln">1015 </span></a>    <span class="s4">&quot;logical_not_&quot;</span><span class="s3">,</span>
<a name="l1016"><span class="ln">1016 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1017"><span class="ln">1017 </span></a>logical_not_() -&gt; Tensor 
<a name="l1018"><span class="ln">1018 </span></a> 
<a name="l1019"><span class="ln">1019 </span></a>In-place version of :meth:`~Tensor.logical_not` 
<a name="l1020"><span class="ln">1020 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1021"><span class="ln">1021 </span></a><span class="s3">)</span>
<a name="l1022"><span class="ln">1022 </span></a>
<a name="l1023"><span class="ln">1023 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1024"><span class="ln">1024 </span></a>    <span class="s4">&quot;logical_or&quot;</span><span class="s3">,</span>
<a name="l1025"><span class="ln">1025 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1026"><span class="ln">1026 </span></a>logical_or() -&gt; Tensor 
<a name="l1027"><span class="ln">1027 </span></a> 
<a name="l1028"><span class="ln">1028 </span></a>See :func:`torch.logical_or` 
<a name="l1029"><span class="ln">1029 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1030"><span class="ln">1030 </span></a><span class="s3">)</span>
<a name="l1031"><span class="ln">1031 </span></a>
<a name="l1032"><span class="ln">1032 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1033"><span class="ln">1033 </span></a>    <span class="s4">&quot;logical_or_&quot;</span><span class="s3">,</span>
<a name="l1034"><span class="ln">1034 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1035"><span class="ln">1035 </span></a>logical_or_() -&gt; Tensor 
<a name="l1036"><span class="ln">1036 </span></a> 
<a name="l1037"><span class="ln">1037 </span></a>In-place version of :meth:`~Tensor.logical_or` 
<a name="l1038"><span class="ln">1038 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1039"><span class="ln">1039 </span></a><span class="s3">)</span>
<a name="l1040"><span class="ln">1040 </span></a>
<a name="l1041"><span class="ln">1041 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1042"><span class="ln">1042 </span></a>    <span class="s4">&quot;logical_xor&quot;</span><span class="s3">,</span>
<a name="l1043"><span class="ln">1043 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1044"><span class="ln">1044 </span></a>logical_xor() -&gt; Tensor 
<a name="l1045"><span class="ln">1045 </span></a> 
<a name="l1046"><span class="ln">1046 </span></a>See :func:`torch.logical_xor` 
<a name="l1047"><span class="ln">1047 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1048"><span class="ln">1048 </span></a><span class="s3">)</span>
<a name="l1049"><span class="ln">1049 </span></a>
<a name="l1050"><span class="ln">1050 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1051"><span class="ln">1051 </span></a>    <span class="s4">&quot;logical_xor_&quot;</span><span class="s3">,</span>
<a name="l1052"><span class="ln">1052 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1053"><span class="ln">1053 </span></a>logical_xor_() -&gt; Tensor 
<a name="l1054"><span class="ln">1054 </span></a> 
<a name="l1055"><span class="ln">1055 </span></a>In-place version of :meth:`~Tensor.logical_xor` 
<a name="l1056"><span class="ln">1056 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1057"><span class="ln">1057 </span></a><span class="s3">)</span>
<a name="l1058"><span class="ln">1058 </span></a>
<a name="l1059"><span class="ln">1059 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1060"><span class="ln">1060 </span></a>    <span class="s4">&quot;bmm&quot;</span><span class="s3">,</span>
<a name="l1061"><span class="ln">1061 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1062"><span class="ln">1062 </span></a>bmm(batch2) -&gt; Tensor 
<a name="l1063"><span class="ln">1063 </span></a> 
<a name="l1064"><span class="ln">1064 </span></a>See :func:`torch.bmm` 
<a name="l1065"><span class="ln">1065 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1066"><span class="ln">1066 </span></a><span class="s3">)</span>
<a name="l1067"><span class="ln">1067 </span></a>
<a name="l1068"><span class="ln">1068 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1069"><span class="ln">1069 </span></a>    <span class="s4">&quot;cauchy_&quot;</span><span class="s3">,</span>
<a name="l1070"><span class="ln">1070 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1071"><span class="ln">1071 </span></a>cauchy_(median=0, sigma=1, *, generator=None) -&gt; Tensor 
<a name="l1072"><span class="ln">1072 </span></a> 
<a name="l1073"><span class="ln">1073 </span></a>Fills the tensor with numbers drawn from the Cauchy distribution: 
<a name="l1074"><span class="ln">1074 </span></a> 
<a name="l1075"><span class="ln">1075 </span></a>.. math:: 
<a name="l1076"><span class="ln">1076 </span></a> 
<a name="l1077"><span class="ln">1077 </span></a>    f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - \text{median})^2 + \sigma^2} 
<a name="l1078"><span class="ln">1078 </span></a> 
<a name="l1079"><span class="ln">1079 </span></a>.. note:: 
<a name="l1080"><span class="ln">1080 </span></a>  Sigma (:math:`\sigma`) is used to denote the scale parameter in Cauchy distribution. 
<a name="l1081"><span class="ln">1081 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1082"><span class="ln">1082 </span></a><span class="s3">)</span>
<a name="l1083"><span class="ln">1083 </span></a>
<a name="l1084"><span class="ln">1084 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1085"><span class="ln">1085 </span></a>    <span class="s4">&quot;ceil&quot;</span><span class="s3">,</span>
<a name="l1086"><span class="ln">1086 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1087"><span class="ln">1087 </span></a>ceil() -&gt; Tensor 
<a name="l1088"><span class="ln">1088 </span></a> 
<a name="l1089"><span class="ln">1089 </span></a>See :func:`torch.ceil` 
<a name="l1090"><span class="ln">1090 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1091"><span class="ln">1091 </span></a><span class="s3">)</span>
<a name="l1092"><span class="ln">1092 </span></a>
<a name="l1093"><span class="ln">1093 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1094"><span class="ln">1094 </span></a>    <span class="s4">&quot;ceil_&quot;</span><span class="s3">,</span>
<a name="l1095"><span class="ln">1095 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1096"><span class="ln">1096 </span></a>ceil_() -&gt; Tensor 
<a name="l1097"><span class="ln">1097 </span></a> 
<a name="l1098"><span class="ln">1098 </span></a>In-place version of :meth:`~Tensor.ceil` 
<a name="l1099"><span class="ln">1099 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1100"><span class="ln">1100 </span></a><span class="s3">)</span>
<a name="l1101"><span class="ln">1101 </span></a>
<a name="l1102"><span class="ln">1102 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1103"><span class="ln">1103 </span></a>    <span class="s4">&quot;cholesky&quot;</span><span class="s3">,</span>
<a name="l1104"><span class="ln">1104 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1105"><span class="ln">1105 </span></a>cholesky(upper=False) -&gt; Tensor 
<a name="l1106"><span class="ln">1106 </span></a> 
<a name="l1107"><span class="ln">1107 </span></a>See :func:`torch.cholesky` 
<a name="l1108"><span class="ln">1108 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1109"><span class="ln">1109 </span></a><span class="s3">)</span>
<a name="l1110"><span class="ln">1110 </span></a>
<a name="l1111"><span class="ln">1111 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1112"><span class="ln">1112 </span></a>    <span class="s4">&quot;cholesky_solve&quot;</span><span class="s3">,</span>
<a name="l1113"><span class="ln">1113 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1114"><span class="ln">1114 </span></a>cholesky_solve(input2, upper=False) -&gt; Tensor 
<a name="l1115"><span class="ln">1115 </span></a> 
<a name="l1116"><span class="ln">1116 </span></a>See :func:`torch.cholesky_solve` 
<a name="l1117"><span class="ln">1117 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1118"><span class="ln">1118 </span></a><span class="s3">)</span>
<a name="l1119"><span class="ln">1119 </span></a>
<a name="l1120"><span class="ln">1120 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1121"><span class="ln">1121 </span></a>    <span class="s4">&quot;cholesky_inverse&quot;</span><span class="s3">,</span>
<a name="l1122"><span class="ln">1122 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1123"><span class="ln">1123 </span></a>cholesky_inverse(upper=False) -&gt; Tensor 
<a name="l1124"><span class="ln">1124 </span></a> 
<a name="l1125"><span class="ln">1125 </span></a>See :func:`torch.cholesky_inverse` 
<a name="l1126"><span class="ln">1126 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1127"><span class="ln">1127 </span></a><span class="s3">)</span>
<a name="l1128"><span class="ln">1128 </span></a>
<a name="l1129"><span class="ln">1129 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1130"><span class="ln">1130 </span></a>    <span class="s4">&quot;clamp&quot;</span><span class="s3">,</span>
<a name="l1131"><span class="ln">1131 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1132"><span class="ln">1132 </span></a>clamp(min=None, max=None) -&gt; Tensor 
<a name="l1133"><span class="ln">1133 </span></a> 
<a name="l1134"><span class="ln">1134 </span></a>See :func:`torch.clamp` 
<a name="l1135"><span class="ln">1135 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1136"><span class="ln">1136 </span></a><span class="s3">)</span>
<a name="l1137"><span class="ln">1137 </span></a>
<a name="l1138"><span class="ln">1138 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1139"><span class="ln">1139 </span></a>    <span class="s4">&quot;clamp_&quot;</span><span class="s3">,</span>
<a name="l1140"><span class="ln">1140 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1141"><span class="ln">1141 </span></a>clamp_(min=None, max=None) -&gt; Tensor 
<a name="l1142"><span class="ln">1142 </span></a> 
<a name="l1143"><span class="ln">1143 </span></a>In-place version of :meth:`~Tensor.clamp` 
<a name="l1144"><span class="ln">1144 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1145"><span class="ln">1145 </span></a><span class="s3">)</span>
<a name="l1146"><span class="ln">1146 </span></a>
<a name="l1147"><span class="ln">1147 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1148"><span class="ln">1148 </span></a>    <span class="s4">&quot;clip&quot;</span><span class="s3">,</span>
<a name="l1149"><span class="ln">1149 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1150"><span class="ln">1150 </span></a>clip(min=None, max=None) -&gt; Tensor 
<a name="l1151"><span class="ln">1151 </span></a> 
<a name="l1152"><span class="ln">1152 </span></a>Alias for :meth:`~Tensor.clamp`. 
<a name="l1153"><span class="ln">1153 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1154"><span class="ln">1154 </span></a><span class="s3">)</span>
<a name="l1155"><span class="ln">1155 </span></a>
<a name="l1156"><span class="ln">1156 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1157"><span class="ln">1157 </span></a>    <span class="s4">&quot;clip_&quot;</span><span class="s3">,</span>
<a name="l1158"><span class="ln">1158 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1159"><span class="ln">1159 </span></a>clip_(min=None, max=None) -&gt; Tensor 
<a name="l1160"><span class="ln">1160 </span></a> 
<a name="l1161"><span class="ln">1161 </span></a>Alias for :meth:`~Tensor.clamp_`. 
<a name="l1162"><span class="ln">1162 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1163"><span class="ln">1163 </span></a><span class="s3">)</span>
<a name="l1164"><span class="ln">1164 </span></a>
<a name="l1165"><span class="ln">1165 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1166"><span class="ln">1166 </span></a>    <span class="s4">&quot;clone&quot;</span><span class="s3">,</span>
<a name="l1167"><span class="ln">1167 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1168"><span class="ln">1168 </span></a>clone(*, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l1169"><span class="ln">1169 </span></a> 
<a name="l1170"><span class="ln">1170 </span></a>See :func:`torch.clone` 
<a name="l1171"><span class="ln">1171 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1172"><span class="ln">1172 </span></a><span class="s3">)</span>
<a name="l1173"><span class="ln">1173 </span></a>
<a name="l1174"><span class="ln">1174 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1175"><span class="ln">1175 </span></a>    <span class="s4">&quot;coalesce&quot;</span><span class="s3">,</span>
<a name="l1176"><span class="ln">1176 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1177"><span class="ln">1177 </span></a>coalesce() -&gt; Tensor 
<a name="l1178"><span class="ln">1178 </span></a> 
<a name="l1179"><span class="ln">1179 </span></a>Returns a coalesced copy of :attr:`self` if :attr:`self` is an 
<a name="l1180"><span class="ln">1180 </span></a>:ref:`uncoalesced tensor &lt;sparse-uncoalesced-coo-docs&gt;`. 
<a name="l1181"><span class="ln">1181 </span></a> 
<a name="l1182"><span class="ln">1182 </span></a>Returns :attr:`self` if :attr:`self` is a coalesced tensor. 
<a name="l1183"><span class="ln">1183 </span></a> 
<a name="l1184"><span class="ln">1184 </span></a>.. warning:: 
<a name="l1185"><span class="ln">1185 </span></a>  Throws an error if :attr:`self` is not a sparse COO tensor. 
<a name="l1186"><span class="ln">1186 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1187"><span class="ln">1187 </span></a><span class="s3">)</span>
<a name="l1188"><span class="ln">1188 </span></a>
<a name="l1189"><span class="ln">1189 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1190"><span class="ln">1190 </span></a>    <span class="s4">&quot;contiguous&quot;</span><span class="s3">,</span>
<a name="l1191"><span class="ln">1191 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1192"><span class="ln">1192 </span></a>contiguous(memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l1193"><span class="ln">1193 </span></a> 
<a name="l1194"><span class="ln">1194 </span></a>Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If 
<a name="l1195"><span class="ln">1195 </span></a>:attr:`self` tensor is already in the specified memory format, this function returns the 
<a name="l1196"><span class="ln">1196 </span></a>:attr:`self` tensor. 
<a name="l1197"><span class="ln">1197 </span></a> 
<a name="l1198"><span class="ln">1198 </span></a>Args: 
<a name="l1199"><span class="ln">1199 </span></a>    memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l1200"><span class="ln">1200 </span></a>        returned Tensor. Default: ``torch.contiguous_format``. 
<a name="l1201"><span class="ln">1201 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1202"><span class="ln">1202 </span></a><span class="s3">)</span>
<a name="l1203"><span class="ln">1203 </span></a>
<a name="l1204"><span class="ln">1204 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1205"><span class="ln">1205 </span></a>    <span class="s4">&quot;copy_&quot;</span><span class="s3">,</span>
<a name="l1206"><span class="ln">1206 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1207"><span class="ln">1207 </span></a>copy_(src, non_blocking=False) -&gt; Tensor 
<a name="l1208"><span class="ln">1208 </span></a> 
<a name="l1209"><span class="ln">1209 </span></a>Copies the elements from :attr:`src` into :attr:`self` tensor and returns 
<a name="l1210"><span class="ln">1210 </span></a>:attr:`self`. 
<a name="l1211"><span class="ln">1211 </span></a> 
<a name="l1212"><span class="ln">1212 </span></a>The :attr:`src` tensor must be :ref:`broadcastable &lt;broadcasting-semantics&gt;` 
<a name="l1213"><span class="ln">1213 </span></a>with the :attr:`self` tensor. It may be of a different data type or reside on a 
<a name="l1214"><span class="ln">1214 </span></a>different device. 
<a name="l1215"><span class="ln">1215 </span></a> 
<a name="l1216"><span class="ln">1216 </span></a>Args: 
<a name="l1217"><span class="ln">1217 </span></a>    src (Tensor): the source tensor to copy from 
<a name="l1218"><span class="ln">1218 </span></a>    non_blocking (bool): if ``True`` and this copy is between CPU and GPU, 
<a name="l1219"><span class="ln">1219 </span></a>        the copy may occur asynchronously with respect to the host. For other 
<a name="l1220"><span class="ln">1220 </span></a>        cases, this argument has no effect. 
<a name="l1221"><span class="ln">1221 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1222"><span class="ln">1222 </span></a><span class="s3">)</span>
<a name="l1223"><span class="ln">1223 </span></a>
<a name="l1224"><span class="ln">1224 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1225"><span class="ln">1225 </span></a>    <span class="s4">&quot;conj&quot;</span><span class="s3">,</span>
<a name="l1226"><span class="ln">1226 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1227"><span class="ln">1227 </span></a>conj() -&gt; Tensor 
<a name="l1228"><span class="ln">1228 </span></a> 
<a name="l1229"><span class="ln">1229 </span></a>See :func:`torch.conj` 
<a name="l1230"><span class="ln">1230 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1231"><span class="ln">1231 </span></a><span class="s3">)</span>
<a name="l1232"><span class="ln">1232 </span></a>
<a name="l1233"><span class="ln">1233 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1234"><span class="ln">1234 </span></a>    <span class="s4">&quot;conj_physical&quot;</span><span class="s3">,</span>
<a name="l1235"><span class="ln">1235 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1236"><span class="ln">1236 </span></a>conj_physical() -&gt; Tensor 
<a name="l1237"><span class="ln">1237 </span></a> 
<a name="l1238"><span class="ln">1238 </span></a>See :func:`torch.conj_physical` 
<a name="l1239"><span class="ln">1239 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1240"><span class="ln">1240 </span></a><span class="s3">)</span>
<a name="l1241"><span class="ln">1241 </span></a>
<a name="l1242"><span class="ln">1242 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1243"><span class="ln">1243 </span></a>    <span class="s4">&quot;conj_physical_&quot;</span><span class="s3">,</span>
<a name="l1244"><span class="ln">1244 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1245"><span class="ln">1245 </span></a>conj_physical_() -&gt; Tensor 
<a name="l1246"><span class="ln">1246 </span></a> 
<a name="l1247"><span class="ln">1247 </span></a>In-place version of :meth:`~Tensor.conj_physical` 
<a name="l1248"><span class="ln">1248 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1249"><span class="ln">1249 </span></a><span class="s3">)</span>
<a name="l1250"><span class="ln">1250 </span></a>
<a name="l1251"><span class="ln">1251 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1252"><span class="ln">1252 </span></a>    <span class="s4">&quot;resolve_conj&quot;</span><span class="s3">,</span>
<a name="l1253"><span class="ln">1253 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1254"><span class="ln">1254 </span></a>resolve_conj() -&gt; Tensor 
<a name="l1255"><span class="ln">1255 </span></a> 
<a name="l1256"><span class="ln">1256 </span></a>See :func:`torch.resolve_conj` 
<a name="l1257"><span class="ln">1257 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1258"><span class="ln">1258 </span></a><span class="s3">)</span>
<a name="l1259"><span class="ln">1259 </span></a>
<a name="l1260"><span class="ln">1260 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1261"><span class="ln">1261 </span></a>    <span class="s4">&quot;resolve_neg&quot;</span><span class="s3">,</span>
<a name="l1262"><span class="ln">1262 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1263"><span class="ln">1263 </span></a>resolve_neg() -&gt; Tensor 
<a name="l1264"><span class="ln">1264 </span></a> 
<a name="l1265"><span class="ln">1265 </span></a>See :func:`torch.resolve_neg` 
<a name="l1266"><span class="ln">1266 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1267"><span class="ln">1267 </span></a><span class="s3">)</span>
<a name="l1268"><span class="ln">1268 </span></a>
<a name="l1269"><span class="ln">1269 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1270"><span class="ln">1270 </span></a>    <span class="s4">&quot;copysign&quot;</span><span class="s3">,</span>
<a name="l1271"><span class="ln">1271 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1272"><span class="ln">1272 </span></a>copysign(other) -&gt; Tensor 
<a name="l1273"><span class="ln">1273 </span></a> 
<a name="l1274"><span class="ln">1274 </span></a>See :func:`torch.copysign` 
<a name="l1275"><span class="ln">1275 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1276"><span class="ln">1276 </span></a><span class="s3">)</span>
<a name="l1277"><span class="ln">1277 </span></a>
<a name="l1278"><span class="ln">1278 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1279"><span class="ln">1279 </span></a>    <span class="s4">&quot;copysign_&quot;</span><span class="s3">,</span>
<a name="l1280"><span class="ln">1280 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1281"><span class="ln">1281 </span></a>copysign_(other) -&gt; Tensor 
<a name="l1282"><span class="ln">1282 </span></a> 
<a name="l1283"><span class="ln">1283 </span></a>In-place version of :meth:`~Tensor.copysign` 
<a name="l1284"><span class="ln">1284 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1285"><span class="ln">1285 </span></a><span class="s3">)</span>
<a name="l1286"><span class="ln">1286 </span></a>
<a name="l1287"><span class="ln">1287 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1288"><span class="ln">1288 </span></a>    <span class="s4">&quot;cos&quot;</span><span class="s3">,</span>
<a name="l1289"><span class="ln">1289 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1290"><span class="ln">1290 </span></a>cos() -&gt; Tensor 
<a name="l1291"><span class="ln">1291 </span></a> 
<a name="l1292"><span class="ln">1292 </span></a>See :func:`torch.cos` 
<a name="l1293"><span class="ln">1293 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1294"><span class="ln">1294 </span></a><span class="s3">)</span>
<a name="l1295"><span class="ln">1295 </span></a>
<a name="l1296"><span class="ln">1296 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1297"><span class="ln">1297 </span></a>    <span class="s4">&quot;cos_&quot;</span><span class="s3">,</span>
<a name="l1298"><span class="ln">1298 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1299"><span class="ln">1299 </span></a>cos_() -&gt; Tensor 
<a name="l1300"><span class="ln">1300 </span></a> 
<a name="l1301"><span class="ln">1301 </span></a>In-place version of :meth:`~Tensor.cos` 
<a name="l1302"><span class="ln">1302 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1303"><span class="ln">1303 </span></a><span class="s3">)</span>
<a name="l1304"><span class="ln">1304 </span></a>
<a name="l1305"><span class="ln">1305 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1306"><span class="ln">1306 </span></a>    <span class="s4">&quot;cosh&quot;</span><span class="s3">,</span>
<a name="l1307"><span class="ln">1307 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1308"><span class="ln">1308 </span></a>cosh() -&gt; Tensor 
<a name="l1309"><span class="ln">1309 </span></a> 
<a name="l1310"><span class="ln">1310 </span></a>See :func:`torch.cosh` 
<a name="l1311"><span class="ln">1311 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1312"><span class="ln">1312 </span></a><span class="s3">)</span>
<a name="l1313"><span class="ln">1313 </span></a>
<a name="l1314"><span class="ln">1314 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1315"><span class="ln">1315 </span></a>    <span class="s4">&quot;cosh_&quot;</span><span class="s3">,</span>
<a name="l1316"><span class="ln">1316 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1317"><span class="ln">1317 </span></a>cosh_() -&gt; Tensor 
<a name="l1318"><span class="ln">1318 </span></a> 
<a name="l1319"><span class="ln">1319 </span></a>In-place version of :meth:`~Tensor.cosh` 
<a name="l1320"><span class="ln">1320 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1321"><span class="ln">1321 </span></a><span class="s3">)</span>
<a name="l1322"><span class="ln">1322 </span></a>
<a name="l1323"><span class="ln">1323 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1324"><span class="ln">1324 </span></a>    <span class="s4">&quot;cpu&quot;</span><span class="s3">,</span>
<a name="l1325"><span class="ln">1325 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1326"><span class="ln">1326 </span></a>cpu(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l1327"><span class="ln">1327 </span></a> 
<a name="l1328"><span class="ln">1328 </span></a>Returns a copy of this object in CPU memory. 
<a name="l1329"><span class="ln">1329 </span></a> 
<a name="l1330"><span class="ln">1330 </span></a>If this object is already in CPU memory, 
<a name="l1331"><span class="ln">1331 </span></a>then no copy is performed and the original object is returned. 
<a name="l1332"><span class="ln">1332 </span></a> 
<a name="l1333"><span class="ln">1333 </span></a>Args: 
<a name="l1334"><span class="ln">1334 </span></a>    {memory_format} 
<a name="l1335"><span class="ln">1335 </span></a> 
<a name="l1336"><span class="ln">1336 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1337"><span class="ln">1337 </span></a><span class="s3">)</span>
<a name="l1338"><span class="ln">1338 </span></a>
<a name="l1339"><span class="ln">1339 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1340"><span class="ln">1340 </span></a>    <span class="s4">&quot;count_nonzero&quot;</span><span class="s3">,</span>
<a name="l1341"><span class="ln">1341 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1342"><span class="ln">1342 </span></a>count_nonzero(dim=None) -&gt; Tensor 
<a name="l1343"><span class="ln">1343 </span></a> 
<a name="l1344"><span class="ln">1344 </span></a>See :func:`torch.count_nonzero` 
<a name="l1345"><span class="ln">1345 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1346"><span class="ln">1346 </span></a><span class="s3">)</span>
<a name="l1347"><span class="ln">1347 </span></a>
<a name="l1348"><span class="ln">1348 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1349"><span class="ln">1349 </span></a>    <span class="s4">&quot;cov&quot;</span><span class="s3">,</span>
<a name="l1350"><span class="ln">1350 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1351"><span class="ln">1351 </span></a>cov(*, correction=1, fweights=None, aweights=None) -&gt; Tensor 
<a name="l1352"><span class="ln">1352 </span></a> 
<a name="l1353"><span class="ln">1353 </span></a>See :func:`torch.cov` 
<a name="l1354"><span class="ln">1354 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1355"><span class="ln">1355 </span></a><span class="s3">)</span>
<a name="l1356"><span class="ln">1356 </span></a>
<a name="l1357"><span class="ln">1357 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1358"><span class="ln">1358 </span></a>    <span class="s4">&quot;corrcoef&quot;</span><span class="s3">,</span>
<a name="l1359"><span class="ln">1359 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1360"><span class="ln">1360 </span></a>corrcoef() -&gt; Tensor 
<a name="l1361"><span class="ln">1361 </span></a> 
<a name="l1362"><span class="ln">1362 </span></a>See :func:`torch.corrcoef` 
<a name="l1363"><span class="ln">1363 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1364"><span class="ln">1364 </span></a><span class="s3">)</span>
<a name="l1365"><span class="ln">1365 </span></a>
<a name="l1366"><span class="ln">1366 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1367"><span class="ln">1367 </span></a>    <span class="s4">&quot;cross&quot;</span><span class="s3">,</span>
<a name="l1368"><span class="ln">1368 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1369"><span class="ln">1369 </span></a>cross(other, dim=None) -&gt; Tensor 
<a name="l1370"><span class="ln">1370 </span></a> 
<a name="l1371"><span class="ln">1371 </span></a>See :func:`torch.cross` 
<a name="l1372"><span class="ln">1372 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1373"><span class="ln">1373 </span></a><span class="s3">)</span>
<a name="l1374"><span class="ln">1374 </span></a>
<a name="l1375"><span class="ln">1375 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1376"><span class="ln">1376 </span></a>    <span class="s4">&quot;cuda&quot;</span><span class="s3">,</span>
<a name="l1377"><span class="ln">1377 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1378"><span class="ln">1378 </span></a>cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l1379"><span class="ln">1379 </span></a> 
<a name="l1380"><span class="ln">1380 </span></a>Returns a copy of this object in CUDA memory. 
<a name="l1381"><span class="ln">1381 </span></a> 
<a name="l1382"><span class="ln">1382 </span></a>If this object is already in CUDA memory and on the correct device, 
<a name="l1383"><span class="ln">1383 </span></a>then no copy is performed and the original object is returned. 
<a name="l1384"><span class="ln">1384 </span></a> 
<a name="l1385"><span class="ln">1385 </span></a>Args: 
<a name="l1386"><span class="ln">1386 </span></a>    device (:class:`torch.device`): The destination GPU device. 
<a name="l1387"><span class="ln">1387 </span></a>        Defaults to the current CUDA device. 
<a name="l1388"><span class="ln">1388 </span></a>    non_blocking (bool): If ``True`` and the source is in pinned memory, 
<a name="l1389"><span class="ln">1389 </span></a>        the copy will be asynchronous with respect to the host. 
<a name="l1390"><span class="ln">1390 </span></a>        Otherwise, the argument has no effect. Default: ``False``. 
<a name="l1391"><span class="ln">1391 </span></a>    {memory_format} 
<a name="l1392"><span class="ln">1392 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1393"><span class="ln">1393 </span></a><span class="s3">)</span>
<a name="l1394"><span class="ln">1394 </span></a>
<a name="l1395"><span class="ln">1395 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1396"><span class="ln">1396 </span></a>    <span class="s4">&quot;mtia&quot;</span><span class="s3">,</span>
<a name="l1397"><span class="ln">1397 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1398"><span class="ln">1398 </span></a>mtia(device=None, non_blocking=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l1399"><span class="ln">1399 </span></a> 
<a name="l1400"><span class="ln">1400 </span></a>Returns a copy of this object in MTIA memory. 
<a name="l1401"><span class="ln">1401 </span></a> 
<a name="l1402"><span class="ln">1402 </span></a>If this object is already in MTIA memory and on the correct device, 
<a name="l1403"><span class="ln">1403 </span></a>then no copy is performed and the original object is returned. 
<a name="l1404"><span class="ln">1404 </span></a> 
<a name="l1405"><span class="ln">1405 </span></a>Args: 
<a name="l1406"><span class="ln">1406 </span></a>    device (:class:`torch.device`): The destination MTIA device. 
<a name="l1407"><span class="ln">1407 </span></a>        Defaults to the current MTIA device. 
<a name="l1408"><span class="ln">1408 </span></a>    non_blocking (bool): If ``True`` and the source is in pinned memory, 
<a name="l1409"><span class="ln">1409 </span></a>        the copy will be asynchronous with respect to the host. 
<a name="l1410"><span class="ln">1410 </span></a>        Otherwise, the argument has no effect. Default: ``False``. 
<a name="l1411"><span class="ln">1411 </span></a>    {memory_format} 
<a name="l1412"><span class="ln">1412 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1413"><span class="ln">1413 </span></a><span class="s3">)</span>
<a name="l1414"><span class="ln">1414 </span></a>
<a name="l1415"><span class="ln">1415 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1416"><span class="ln">1416 </span></a>    <span class="s4">&quot;ipu&quot;</span><span class="s3">,</span>
<a name="l1417"><span class="ln">1417 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1418"><span class="ln">1418 </span></a>ipu(device=None, non_blocking=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l1419"><span class="ln">1419 </span></a> 
<a name="l1420"><span class="ln">1420 </span></a>Returns a copy of this object in IPU memory. 
<a name="l1421"><span class="ln">1421 </span></a> 
<a name="l1422"><span class="ln">1422 </span></a>If this object is already in IPU memory and on the correct device, 
<a name="l1423"><span class="ln">1423 </span></a>then no copy is performed and the original object is returned. 
<a name="l1424"><span class="ln">1424 </span></a> 
<a name="l1425"><span class="ln">1425 </span></a>Args: 
<a name="l1426"><span class="ln">1426 </span></a>    device (:class:`torch.device`): The destination IPU device. 
<a name="l1427"><span class="ln">1427 </span></a>        Defaults to the current IPU device. 
<a name="l1428"><span class="ln">1428 </span></a>    non_blocking (bool): If ``True`` and the source is in pinned memory, 
<a name="l1429"><span class="ln">1429 </span></a>        the copy will be asynchronous with respect to the host. 
<a name="l1430"><span class="ln">1430 </span></a>        Otherwise, the argument has no effect. Default: ``False``. 
<a name="l1431"><span class="ln">1431 </span></a>    {memory_format} 
<a name="l1432"><span class="ln">1432 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1433"><span class="ln">1433 </span></a><span class="s3">)</span>
<a name="l1434"><span class="ln">1434 </span></a>
<a name="l1435"><span class="ln">1435 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1436"><span class="ln">1436 </span></a>    <span class="s4">&quot;xpu&quot;</span><span class="s3">,</span>
<a name="l1437"><span class="ln">1437 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1438"><span class="ln">1438 </span></a>xpu(device=None, non_blocking=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l1439"><span class="ln">1439 </span></a> 
<a name="l1440"><span class="ln">1440 </span></a>Returns a copy of this object in XPU memory. 
<a name="l1441"><span class="ln">1441 </span></a> 
<a name="l1442"><span class="ln">1442 </span></a>If this object is already in XPU memory and on the correct device, 
<a name="l1443"><span class="ln">1443 </span></a>then no copy is performed and the original object is returned. 
<a name="l1444"><span class="ln">1444 </span></a> 
<a name="l1445"><span class="ln">1445 </span></a>Args: 
<a name="l1446"><span class="ln">1446 </span></a>    device (:class:`torch.device`): The destination XPU device. 
<a name="l1447"><span class="ln">1447 </span></a>        Defaults to the current XPU device. 
<a name="l1448"><span class="ln">1448 </span></a>    non_blocking (bool): If ``True`` and the source is in pinned memory, 
<a name="l1449"><span class="ln">1449 </span></a>        the copy will be asynchronous with respect to the host. 
<a name="l1450"><span class="ln">1450 </span></a>        Otherwise, the argument has no effect. Default: ``False``. 
<a name="l1451"><span class="ln">1451 </span></a>    {memory_format} 
<a name="l1452"><span class="ln">1452 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l1453"><span class="ln">1453 </span></a><span class="s3">)</span>
<a name="l1454"><span class="ln">1454 </span></a>
<a name="l1455"><span class="ln">1455 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1456"><span class="ln">1456 </span></a>    <span class="s4">&quot;logcumsumexp&quot;</span><span class="s3">,</span>
<a name="l1457"><span class="ln">1457 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1458"><span class="ln">1458 </span></a>logcumsumexp(dim) -&gt; Tensor 
<a name="l1459"><span class="ln">1459 </span></a> 
<a name="l1460"><span class="ln">1460 </span></a>See :func:`torch.logcumsumexp` 
<a name="l1461"><span class="ln">1461 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1462"><span class="ln">1462 </span></a><span class="s3">)</span>
<a name="l1463"><span class="ln">1463 </span></a>
<a name="l1464"><span class="ln">1464 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1465"><span class="ln">1465 </span></a>    <span class="s4">&quot;cummax&quot;</span><span class="s3">,</span>
<a name="l1466"><span class="ln">1466 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1467"><span class="ln">1467 </span></a>cummax(dim) -&gt; (Tensor, Tensor) 
<a name="l1468"><span class="ln">1468 </span></a> 
<a name="l1469"><span class="ln">1469 </span></a>See :func:`torch.cummax` 
<a name="l1470"><span class="ln">1470 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1471"><span class="ln">1471 </span></a><span class="s3">)</span>
<a name="l1472"><span class="ln">1472 </span></a>
<a name="l1473"><span class="ln">1473 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1474"><span class="ln">1474 </span></a>    <span class="s4">&quot;cummin&quot;</span><span class="s3">,</span>
<a name="l1475"><span class="ln">1475 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1476"><span class="ln">1476 </span></a>cummin(dim) -&gt; (Tensor, Tensor) 
<a name="l1477"><span class="ln">1477 </span></a> 
<a name="l1478"><span class="ln">1478 </span></a>See :func:`torch.cummin` 
<a name="l1479"><span class="ln">1479 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1480"><span class="ln">1480 </span></a><span class="s3">)</span>
<a name="l1481"><span class="ln">1481 </span></a>
<a name="l1482"><span class="ln">1482 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1483"><span class="ln">1483 </span></a>    <span class="s4">&quot;cumprod&quot;</span><span class="s3">,</span>
<a name="l1484"><span class="ln">1484 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1485"><span class="ln">1485 </span></a>cumprod(dim, dtype=None) -&gt; Tensor 
<a name="l1486"><span class="ln">1486 </span></a> 
<a name="l1487"><span class="ln">1487 </span></a>See :func:`torch.cumprod` 
<a name="l1488"><span class="ln">1488 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1489"><span class="ln">1489 </span></a><span class="s3">)</span>
<a name="l1490"><span class="ln">1490 </span></a>
<a name="l1491"><span class="ln">1491 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1492"><span class="ln">1492 </span></a>    <span class="s4">&quot;cumprod_&quot;</span><span class="s3">,</span>
<a name="l1493"><span class="ln">1493 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1494"><span class="ln">1494 </span></a>cumprod_(dim, dtype=None) -&gt; Tensor 
<a name="l1495"><span class="ln">1495 </span></a> 
<a name="l1496"><span class="ln">1496 </span></a>In-place version of :meth:`~Tensor.cumprod` 
<a name="l1497"><span class="ln">1497 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1498"><span class="ln">1498 </span></a><span class="s3">)</span>
<a name="l1499"><span class="ln">1499 </span></a>
<a name="l1500"><span class="ln">1500 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1501"><span class="ln">1501 </span></a>    <span class="s4">&quot;cumsum&quot;</span><span class="s3">,</span>
<a name="l1502"><span class="ln">1502 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1503"><span class="ln">1503 </span></a>cumsum(dim, dtype=None) -&gt; Tensor 
<a name="l1504"><span class="ln">1504 </span></a> 
<a name="l1505"><span class="ln">1505 </span></a>See :func:`torch.cumsum` 
<a name="l1506"><span class="ln">1506 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1507"><span class="ln">1507 </span></a><span class="s3">)</span>
<a name="l1508"><span class="ln">1508 </span></a>
<a name="l1509"><span class="ln">1509 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1510"><span class="ln">1510 </span></a>    <span class="s4">&quot;cumsum_&quot;</span><span class="s3">,</span>
<a name="l1511"><span class="ln">1511 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1512"><span class="ln">1512 </span></a>cumsum_(dim, dtype=None) -&gt; Tensor 
<a name="l1513"><span class="ln">1513 </span></a> 
<a name="l1514"><span class="ln">1514 </span></a>In-place version of :meth:`~Tensor.cumsum` 
<a name="l1515"><span class="ln">1515 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1516"><span class="ln">1516 </span></a><span class="s3">)</span>
<a name="l1517"><span class="ln">1517 </span></a>
<a name="l1518"><span class="ln">1518 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1519"><span class="ln">1519 </span></a>    <span class="s4">&quot;data_ptr&quot;</span><span class="s3">,</span>
<a name="l1520"><span class="ln">1520 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1521"><span class="ln">1521 </span></a>data_ptr() -&gt; int 
<a name="l1522"><span class="ln">1522 </span></a> 
<a name="l1523"><span class="ln">1523 </span></a>Returns the address of the first element of :attr:`self` tensor. 
<a name="l1524"><span class="ln">1524 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1525"><span class="ln">1525 </span></a><span class="s3">)</span>
<a name="l1526"><span class="ln">1526 </span></a>
<a name="l1527"><span class="ln">1527 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1528"><span class="ln">1528 </span></a>    <span class="s4">&quot;dequantize&quot;</span><span class="s3">,</span>
<a name="l1529"><span class="ln">1529 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1530"><span class="ln">1530 </span></a>dequantize() -&gt; Tensor 
<a name="l1531"><span class="ln">1531 </span></a> 
<a name="l1532"><span class="ln">1532 </span></a>Given a quantized Tensor, dequantize it and return the dequantized float Tensor. 
<a name="l1533"><span class="ln">1533 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1534"><span class="ln">1534 </span></a><span class="s3">)</span>
<a name="l1535"><span class="ln">1535 </span></a>
<a name="l1536"><span class="ln">1536 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1537"><span class="ln">1537 </span></a>    <span class="s4">&quot;dense_dim&quot;</span><span class="s3">,</span>
<a name="l1538"><span class="ln">1538 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1539"><span class="ln">1539 </span></a>dense_dim() -&gt; int 
<a name="l1540"><span class="ln">1540 </span></a> 
<a name="l1541"><span class="ln">1541 </span></a>Return the number of dense dimensions in a :ref:`sparse tensor &lt;sparse-docs&gt;` :attr:`self`. 
<a name="l1542"><span class="ln">1542 </span></a> 
<a name="l1543"><span class="ln">1543 </span></a>.. note:: 
<a name="l1544"><span class="ln">1544 </span></a>  Returns ``len(self.shape)`` if :attr:`self` is not a sparse tensor. 
<a name="l1545"><span class="ln">1545 </span></a> 
<a name="l1546"><span class="ln">1546 </span></a>See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors &lt;sparse-hybrid-coo-docs&gt;`. 
<a name="l1547"><span class="ln">1547 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1548"><span class="ln">1548 </span></a><span class="s3">)</span>
<a name="l1549"><span class="ln">1549 </span></a>
<a name="l1550"><span class="ln">1550 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1551"><span class="ln">1551 </span></a>    <span class="s4">&quot;diag&quot;</span><span class="s3">,</span>
<a name="l1552"><span class="ln">1552 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1553"><span class="ln">1553 </span></a>diag(diagonal=0) -&gt; Tensor 
<a name="l1554"><span class="ln">1554 </span></a> 
<a name="l1555"><span class="ln">1555 </span></a>See :func:`torch.diag` 
<a name="l1556"><span class="ln">1556 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1557"><span class="ln">1557 </span></a><span class="s3">)</span>
<a name="l1558"><span class="ln">1558 </span></a>
<a name="l1559"><span class="ln">1559 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1560"><span class="ln">1560 </span></a>    <span class="s4">&quot;diag_embed&quot;</span><span class="s3">,</span>
<a name="l1561"><span class="ln">1561 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1562"><span class="ln">1562 </span></a>diag_embed(offset=0, dim1=-2, dim2=-1) -&gt; Tensor 
<a name="l1563"><span class="ln">1563 </span></a> 
<a name="l1564"><span class="ln">1564 </span></a>See :func:`torch.diag_embed` 
<a name="l1565"><span class="ln">1565 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1566"><span class="ln">1566 </span></a><span class="s3">)</span>
<a name="l1567"><span class="ln">1567 </span></a>
<a name="l1568"><span class="ln">1568 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1569"><span class="ln">1569 </span></a>    <span class="s4">&quot;diagflat&quot;</span><span class="s3">,</span>
<a name="l1570"><span class="ln">1570 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1571"><span class="ln">1571 </span></a>diagflat(offset=0) -&gt; Tensor 
<a name="l1572"><span class="ln">1572 </span></a> 
<a name="l1573"><span class="ln">1573 </span></a>See :func:`torch.diagflat` 
<a name="l1574"><span class="ln">1574 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1575"><span class="ln">1575 </span></a><span class="s3">)</span>
<a name="l1576"><span class="ln">1576 </span></a>
<a name="l1577"><span class="ln">1577 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1578"><span class="ln">1578 </span></a>    <span class="s4">&quot;diagonal&quot;</span><span class="s3">,</span>
<a name="l1579"><span class="ln">1579 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1580"><span class="ln">1580 </span></a>diagonal(offset=0, dim1=0, dim2=1) -&gt; Tensor 
<a name="l1581"><span class="ln">1581 </span></a> 
<a name="l1582"><span class="ln">1582 </span></a>See :func:`torch.diagonal` 
<a name="l1583"><span class="ln">1583 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1584"><span class="ln">1584 </span></a><span class="s3">)</span>
<a name="l1585"><span class="ln">1585 </span></a>
<a name="l1586"><span class="ln">1586 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1587"><span class="ln">1587 </span></a>    <span class="s4">&quot;diagonal_scatter&quot;</span><span class="s3">,</span>
<a name="l1588"><span class="ln">1588 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1589"><span class="ln">1589 </span></a>diagonal_scatter(src, offset=0, dim1=0, dim2=1) -&gt; Tensor 
<a name="l1590"><span class="ln">1590 </span></a> 
<a name="l1591"><span class="ln">1591 </span></a>See :func:`torch.diagonal_scatter` 
<a name="l1592"><span class="ln">1592 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1593"><span class="ln">1593 </span></a><span class="s3">)</span>
<a name="l1594"><span class="ln">1594 </span></a>
<a name="l1595"><span class="ln">1595 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1596"><span class="ln">1596 </span></a>    <span class="s4">&quot;as_strided_scatter&quot;</span><span class="s3">,</span>
<a name="l1597"><span class="ln">1597 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1598"><span class="ln">1598 </span></a>as_strided_scatter(src, size, stride, storage_offset=None) -&gt; Tensor 
<a name="l1599"><span class="ln">1599 </span></a> 
<a name="l1600"><span class="ln">1600 </span></a>See :func:`torch.as_strided_scatter` 
<a name="l1601"><span class="ln">1601 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1602"><span class="ln">1602 </span></a><span class="s3">)</span>
<a name="l1603"><span class="ln">1603 </span></a>
<a name="l1604"><span class="ln">1604 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1605"><span class="ln">1605 </span></a>    <span class="s4">&quot;fill_diagonal_&quot;</span><span class="s3">,</span>
<a name="l1606"><span class="ln">1606 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1607"><span class="ln">1607 </span></a>fill_diagonal_(fill_value, wrap=False) -&gt; Tensor 
<a name="l1608"><span class="ln">1608 </span></a> 
<a name="l1609"><span class="ln">1609 </span></a>Fill the main diagonal of a tensor that has at least 2-dimensions. 
<a name="l1610"><span class="ln">1610 </span></a>When dims&gt;2, all dimensions of input must be of equal length. 
<a name="l1611"><span class="ln">1611 </span></a>This function modifies the input tensor in-place, and returns the input tensor. 
<a name="l1612"><span class="ln">1612 </span></a> 
<a name="l1613"><span class="ln">1613 </span></a>Arguments: 
<a name="l1614"><span class="ln">1614 </span></a>    fill_value (Scalar): the fill value 
<a name="l1615"><span class="ln">1615 </span></a>    wrap (bool): the diagonal 'wrapped' after N columns for tall matrices. 
<a name="l1616"><span class="ln">1616 </span></a> 
<a name="l1617"><span class="ln">1617 </span></a>Example:: 
<a name="l1618"><span class="ln">1618 </span></a> 
<a name="l1619"><span class="ln">1619 </span></a>    &gt;&gt;&gt; a = torch.zeros(3, 3) 
<a name="l1620"><span class="ln">1620 </span></a>    &gt;&gt;&gt; a.fill_diagonal_(5) 
<a name="l1621"><span class="ln">1621 </span></a>    tensor([[5., 0., 0.], 
<a name="l1622"><span class="ln">1622 </span></a>            [0., 5., 0.], 
<a name="l1623"><span class="ln">1623 </span></a>            [0., 0., 5.]]) 
<a name="l1624"><span class="ln">1624 </span></a>    &gt;&gt;&gt; b = torch.zeros(7, 3) 
<a name="l1625"><span class="ln">1625 </span></a>    &gt;&gt;&gt; b.fill_diagonal_(5) 
<a name="l1626"><span class="ln">1626 </span></a>    tensor([[5., 0., 0.], 
<a name="l1627"><span class="ln">1627 </span></a>            [0., 5., 0.], 
<a name="l1628"><span class="ln">1628 </span></a>            [0., 0., 5.], 
<a name="l1629"><span class="ln">1629 </span></a>            [0., 0., 0.], 
<a name="l1630"><span class="ln">1630 </span></a>            [0., 0., 0.], 
<a name="l1631"><span class="ln">1631 </span></a>            [0., 0., 0.], 
<a name="l1632"><span class="ln">1632 </span></a>            [0., 0., 0.]]) 
<a name="l1633"><span class="ln">1633 </span></a>    &gt;&gt;&gt; c = torch.zeros(7, 3) 
<a name="l1634"><span class="ln">1634 </span></a>    &gt;&gt;&gt; c.fill_diagonal_(5, wrap=True) 
<a name="l1635"><span class="ln">1635 </span></a>    tensor([[5., 0., 0.], 
<a name="l1636"><span class="ln">1636 </span></a>            [0., 5., 0.], 
<a name="l1637"><span class="ln">1637 </span></a>            [0., 0., 5.], 
<a name="l1638"><span class="ln">1638 </span></a>            [0., 0., 0.], 
<a name="l1639"><span class="ln">1639 </span></a>            [5., 0., 0.], 
<a name="l1640"><span class="ln">1640 </span></a>            [0., 5., 0.], 
<a name="l1641"><span class="ln">1641 </span></a>            [0., 0., 5.]]) 
<a name="l1642"><span class="ln">1642 </span></a> 
<a name="l1643"><span class="ln">1643 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1644"><span class="ln">1644 </span></a><span class="s3">)</span>
<a name="l1645"><span class="ln">1645 </span></a>
<a name="l1646"><span class="ln">1646 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1647"><span class="ln">1647 </span></a>    <span class="s4">&quot;floor_divide&quot;</span><span class="s3">,</span>
<a name="l1648"><span class="ln">1648 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1649"><span class="ln">1649 </span></a>floor_divide(value) -&gt; Tensor 
<a name="l1650"><span class="ln">1650 </span></a> 
<a name="l1651"><span class="ln">1651 </span></a>See :func:`torch.floor_divide` 
<a name="l1652"><span class="ln">1652 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1653"><span class="ln">1653 </span></a><span class="s3">)</span>
<a name="l1654"><span class="ln">1654 </span></a>
<a name="l1655"><span class="ln">1655 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1656"><span class="ln">1656 </span></a>    <span class="s4">&quot;floor_divide_&quot;</span><span class="s3">,</span>
<a name="l1657"><span class="ln">1657 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1658"><span class="ln">1658 </span></a>floor_divide_(value) -&gt; Tensor 
<a name="l1659"><span class="ln">1659 </span></a> 
<a name="l1660"><span class="ln">1660 </span></a>In-place version of :meth:`~Tensor.floor_divide` 
<a name="l1661"><span class="ln">1661 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1662"><span class="ln">1662 </span></a><span class="s3">)</span>
<a name="l1663"><span class="ln">1663 </span></a>
<a name="l1664"><span class="ln">1664 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1665"><span class="ln">1665 </span></a>    <span class="s4">&quot;diff&quot;</span><span class="s3">,</span>
<a name="l1666"><span class="ln">1666 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1667"><span class="ln">1667 </span></a>diff(n=1, dim=-1, prepend=None, append=None) -&gt; Tensor 
<a name="l1668"><span class="ln">1668 </span></a> 
<a name="l1669"><span class="ln">1669 </span></a>See :func:`torch.diff` 
<a name="l1670"><span class="ln">1670 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1671"><span class="ln">1671 </span></a><span class="s3">)</span>
<a name="l1672"><span class="ln">1672 </span></a>
<a name="l1673"><span class="ln">1673 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1674"><span class="ln">1674 </span></a>    <span class="s4">&quot;digamma&quot;</span><span class="s3">,</span>
<a name="l1675"><span class="ln">1675 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1676"><span class="ln">1676 </span></a>digamma() -&gt; Tensor 
<a name="l1677"><span class="ln">1677 </span></a> 
<a name="l1678"><span class="ln">1678 </span></a>See :func:`torch.digamma` 
<a name="l1679"><span class="ln">1679 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1680"><span class="ln">1680 </span></a><span class="s3">)</span>
<a name="l1681"><span class="ln">1681 </span></a>
<a name="l1682"><span class="ln">1682 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1683"><span class="ln">1683 </span></a>    <span class="s4">&quot;digamma_&quot;</span><span class="s3">,</span>
<a name="l1684"><span class="ln">1684 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1685"><span class="ln">1685 </span></a>digamma_() -&gt; Tensor 
<a name="l1686"><span class="ln">1686 </span></a> 
<a name="l1687"><span class="ln">1687 </span></a>In-place version of :meth:`~Tensor.digamma` 
<a name="l1688"><span class="ln">1688 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1689"><span class="ln">1689 </span></a><span class="s3">)</span>
<a name="l1690"><span class="ln">1690 </span></a>
<a name="l1691"><span class="ln">1691 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1692"><span class="ln">1692 </span></a>    <span class="s4">&quot;dim&quot;</span><span class="s3">,</span>
<a name="l1693"><span class="ln">1693 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1694"><span class="ln">1694 </span></a>dim() -&gt; int 
<a name="l1695"><span class="ln">1695 </span></a> 
<a name="l1696"><span class="ln">1696 </span></a>Returns the number of dimensions of :attr:`self` tensor. 
<a name="l1697"><span class="ln">1697 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1698"><span class="ln">1698 </span></a><span class="s3">)</span>
<a name="l1699"><span class="ln">1699 </span></a>
<a name="l1700"><span class="ln">1700 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1701"><span class="ln">1701 </span></a>    <span class="s4">&quot;dist&quot;</span><span class="s3">,</span>
<a name="l1702"><span class="ln">1702 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1703"><span class="ln">1703 </span></a>dist(other, p=2) -&gt; Tensor 
<a name="l1704"><span class="ln">1704 </span></a> 
<a name="l1705"><span class="ln">1705 </span></a>See :func:`torch.dist` 
<a name="l1706"><span class="ln">1706 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1707"><span class="ln">1707 </span></a><span class="s3">)</span>
<a name="l1708"><span class="ln">1708 </span></a>
<a name="l1709"><span class="ln">1709 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1710"><span class="ln">1710 </span></a>    <span class="s4">&quot;div&quot;</span><span class="s3">,</span>
<a name="l1711"><span class="ln">1711 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1712"><span class="ln">1712 </span></a>div(value, *, rounding_mode=None) -&gt; Tensor 
<a name="l1713"><span class="ln">1713 </span></a> 
<a name="l1714"><span class="ln">1714 </span></a>See :func:`torch.div` 
<a name="l1715"><span class="ln">1715 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1716"><span class="ln">1716 </span></a><span class="s3">)</span>
<a name="l1717"><span class="ln">1717 </span></a>
<a name="l1718"><span class="ln">1718 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1719"><span class="ln">1719 </span></a>    <span class="s4">&quot;div_&quot;</span><span class="s3">,</span>
<a name="l1720"><span class="ln">1720 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1721"><span class="ln">1721 </span></a>div_(value, *, rounding_mode=None) -&gt; Tensor 
<a name="l1722"><span class="ln">1722 </span></a> 
<a name="l1723"><span class="ln">1723 </span></a>In-place version of :meth:`~Tensor.div` 
<a name="l1724"><span class="ln">1724 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1725"><span class="ln">1725 </span></a><span class="s3">)</span>
<a name="l1726"><span class="ln">1726 </span></a>
<a name="l1727"><span class="ln">1727 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1728"><span class="ln">1728 </span></a>    <span class="s4">&quot;divide&quot;</span><span class="s3">,</span>
<a name="l1729"><span class="ln">1729 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1730"><span class="ln">1730 </span></a>divide(value, *, rounding_mode=None) -&gt; Tensor 
<a name="l1731"><span class="ln">1731 </span></a> 
<a name="l1732"><span class="ln">1732 </span></a>See :func:`torch.divide` 
<a name="l1733"><span class="ln">1733 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1734"><span class="ln">1734 </span></a><span class="s3">)</span>
<a name="l1735"><span class="ln">1735 </span></a>
<a name="l1736"><span class="ln">1736 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1737"><span class="ln">1737 </span></a>    <span class="s4">&quot;divide_&quot;</span><span class="s3">,</span>
<a name="l1738"><span class="ln">1738 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1739"><span class="ln">1739 </span></a>divide_(value, *, rounding_mode=None) -&gt; Tensor 
<a name="l1740"><span class="ln">1740 </span></a> 
<a name="l1741"><span class="ln">1741 </span></a>In-place version of :meth:`~Tensor.divide` 
<a name="l1742"><span class="ln">1742 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1743"><span class="ln">1743 </span></a><span class="s3">)</span>
<a name="l1744"><span class="ln">1744 </span></a>
<a name="l1745"><span class="ln">1745 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1746"><span class="ln">1746 </span></a>    <span class="s4">&quot;dot&quot;</span><span class="s3">,</span>
<a name="l1747"><span class="ln">1747 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1748"><span class="ln">1748 </span></a>dot(other) -&gt; Tensor 
<a name="l1749"><span class="ln">1749 </span></a> 
<a name="l1750"><span class="ln">1750 </span></a>See :func:`torch.dot` 
<a name="l1751"><span class="ln">1751 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1752"><span class="ln">1752 </span></a><span class="s3">)</span>
<a name="l1753"><span class="ln">1753 </span></a>
<a name="l1754"><span class="ln">1754 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1755"><span class="ln">1755 </span></a>    <span class="s4">&quot;element_size&quot;</span><span class="s3">,</span>
<a name="l1756"><span class="ln">1756 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1757"><span class="ln">1757 </span></a>element_size() -&gt; int 
<a name="l1758"><span class="ln">1758 </span></a> 
<a name="l1759"><span class="ln">1759 </span></a>Returns the size in bytes of an individual element. 
<a name="l1760"><span class="ln">1760 </span></a> 
<a name="l1761"><span class="ln">1761 </span></a>Example:: 
<a name="l1762"><span class="ln">1762 </span></a> 
<a name="l1763"><span class="ln">1763 </span></a>    &gt;&gt;&gt; torch.tensor([]).element_size() 
<a name="l1764"><span class="ln">1764 </span></a>    4 
<a name="l1765"><span class="ln">1765 </span></a>    &gt;&gt;&gt; torch.tensor([], dtype=torch.uint8).element_size() 
<a name="l1766"><span class="ln">1766 </span></a>    1 
<a name="l1767"><span class="ln">1767 </span></a> 
<a name="l1768"><span class="ln">1768 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1769"><span class="ln">1769 </span></a><span class="s3">)</span>
<a name="l1770"><span class="ln">1770 </span></a>
<a name="l1771"><span class="ln">1771 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1772"><span class="ln">1772 </span></a>    <span class="s4">&quot;eq&quot;</span><span class="s3">,</span>
<a name="l1773"><span class="ln">1773 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1774"><span class="ln">1774 </span></a>eq(other) -&gt; Tensor 
<a name="l1775"><span class="ln">1775 </span></a> 
<a name="l1776"><span class="ln">1776 </span></a>See :func:`torch.eq` 
<a name="l1777"><span class="ln">1777 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1778"><span class="ln">1778 </span></a><span class="s3">)</span>
<a name="l1779"><span class="ln">1779 </span></a>
<a name="l1780"><span class="ln">1780 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1781"><span class="ln">1781 </span></a>    <span class="s4">&quot;eq_&quot;</span><span class="s3">,</span>
<a name="l1782"><span class="ln">1782 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1783"><span class="ln">1783 </span></a>eq_(other) -&gt; Tensor 
<a name="l1784"><span class="ln">1784 </span></a> 
<a name="l1785"><span class="ln">1785 </span></a>In-place version of :meth:`~Tensor.eq` 
<a name="l1786"><span class="ln">1786 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1787"><span class="ln">1787 </span></a><span class="s3">)</span>
<a name="l1788"><span class="ln">1788 </span></a>
<a name="l1789"><span class="ln">1789 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1790"><span class="ln">1790 </span></a>    <span class="s4">&quot;equal&quot;</span><span class="s3">,</span>
<a name="l1791"><span class="ln">1791 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1792"><span class="ln">1792 </span></a>equal(other) -&gt; bool 
<a name="l1793"><span class="ln">1793 </span></a> 
<a name="l1794"><span class="ln">1794 </span></a>See :func:`torch.equal` 
<a name="l1795"><span class="ln">1795 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1796"><span class="ln">1796 </span></a><span class="s3">)</span>
<a name="l1797"><span class="ln">1797 </span></a>
<a name="l1798"><span class="ln">1798 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1799"><span class="ln">1799 </span></a>    <span class="s4">&quot;erf&quot;</span><span class="s3">,</span>
<a name="l1800"><span class="ln">1800 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1801"><span class="ln">1801 </span></a>erf() -&gt; Tensor 
<a name="l1802"><span class="ln">1802 </span></a> 
<a name="l1803"><span class="ln">1803 </span></a>See :func:`torch.erf` 
<a name="l1804"><span class="ln">1804 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1805"><span class="ln">1805 </span></a><span class="s3">)</span>
<a name="l1806"><span class="ln">1806 </span></a>
<a name="l1807"><span class="ln">1807 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1808"><span class="ln">1808 </span></a>    <span class="s4">&quot;erf_&quot;</span><span class="s3">,</span>
<a name="l1809"><span class="ln">1809 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1810"><span class="ln">1810 </span></a>erf_() -&gt; Tensor 
<a name="l1811"><span class="ln">1811 </span></a> 
<a name="l1812"><span class="ln">1812 </span></a>In-place version of :meth:`~Tensor.erf` 
<a name="l1813"><span class="ln">1813 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1814"><span class="ln">1814 </span></a><span class="s3">)</span>
<a name="l1815"><span class="ln">1815 </span></a>
<a name="l1816"><span class="ln">1816 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1817"><span class="ln">1817 </span></a>    <span class="s4">&quot;erfc&quot;</span><span class="s3">,</span>
<a name="l1818"><span class="ln">1818 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1819"><span class="ln">1819 </span></a>erfc() -&gt; Tensor 
<a name="l1820"><span class="ln">1820 </span></a> 
<a name="l1821"><span class="ln">1821 </span></a>See :func:`torch.erfc` 
<a name="l1822"><span class="ln">1822 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1823"><span class="ln">1823 </span></a><span class="s3">)</span>
<a name="l1824"><span class="ln">1824 </span></a>
<a name="l1825"><span class="ln">1825 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1826"><span class="ln">1826 </span></a>    <span class="s4">&quot;erfc_&quot;</span><span class="s3">,</span>
<a name="l1827"><span class="ln">1827 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1828"><span class="ln">1828 </span></a>erfc_() -&gt; Tensor 
<a name="l1829"><span class="ln">1829 </span></a> 
<a name="l1830"><span class="ln">1830 </span></a>In-place version of :meth:`~Tensor.erfc` 
<a name="l1831"><span class="ln">1831 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1832"><span class="ln">1832 </span></a><span class="s3">)</span>
<a name="l1833"><span class="ln">1833 </span></a>
<a name="l1834"><span class="ln">1834 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1835"><span class="ln">1835 </span></a>    <span class="s4">&quot;erfinv&quot;</span><span class="s3">,</span>
<a name="l1836"><span class="ln">1836 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1837"><span class="ln">1837 </span></a>erfinv() -&gt; Tensor 
<a name="l1838"><span class="ln">1838 </span></a> 
<a name="l1839"><span class="ln">1839 </span></a>See :func:`torch.erfinv` 
<a name="l1840"><span class="ln">1840 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1841"><span class="ln">1841 </span></a><span class="s3">)</span>
<a name="l1842"><span class="ln">1842 </span></a>
<a name="l1843"><span class="ln">1843 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1844"><span class="ln">1844 </span></a>    <span class="s4">&quot;erfinv_&quot;</span><span class="s3">,</span>
<a name="l1845"><span class="ln">1845 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1846"><span class="ln">1846 </span></a>erfinv_() -&gt; Tensor 
<a name="l1847"><span class="ln">1847 </span></a> 
<a name="l1848"><span class="ln">1848 </span></a>In-place version of :meth:`~Tensor.erfinv` 
<a name="l1849"><span class="ln">1849 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1850"><span class="ln">1850 </span></a><span class="s3">)</span>
<a name="l1851"><span class="ln">1851 </span></a>
<a name="l1852"><span class="ln">1852 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1853"><span class="ln">1853 </span></a>    <span class="s4">&quot;exp&quot;</span><span class="s3">,</span>
<a name="l1854"><span class="ln">1854 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1855"><span class="ln">1855 </span></a>exp() -&gt; Tensor 
<a name="l1856"><span class="ln">1856 </span></a> 
<a name="l1857"><span class="ln">1857 </span></a>See :func:`torch.exp` 
<a name="l1858"><span class="ln">1858 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1859"><span class="ln">1859 </span></a><span class="s3">)</span>
<a name="l1860"><span class="ln">1860 </span></a>
<a name="l1861"><span class="ln">1861 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1862"><span class="ln">1862 </span></a>    <span class="s4">&quot;exp_&quot;</span><span class="s3">,</span>
<a name="l1863"><span class="ln">1863 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1864"><span class="ln">1864 </span></a>exp_() -&gt; Tensor 
<a name="l1865"><span class="ln">1865 </span></a> 
<a name="l1866"><span class="ln">1866 </span></a>In-place version of :meth:`~Tensor.exp` 
<a name="l1867"><span class="ln">1867 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1868"><span class="ln">1868 </span></a><span class="s3">)</span>
<a name="l1869"><span class="ln">1869 </span></a>
<a name="l1870"><span class="ln">1870 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1871"><span class="ln">1871 </span></a>    <span class="s4">&quot;exp2&quot;</span><span class="s3">,</span>
<a name="l1872"><span class="ln">1872 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1873"><span class="ln">1873 </span></a>exp2() -&gt; Tensor 
<a name="l1874"><span class="ln">1874 </span></a> 
<a name="l1875"><span class="ln">1875 </span></a>See :func:`torch.exp2` 
<a name="l1876"><span class="ln">1876 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1877"><span class="ln">1877 </span></a><span class="s3">)</span>
<a name="l1878"><span class="ln">1878 </span></a>
<a name="l1879"><span class="ln">1879 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1880"><span class="ln">1880 </span></a>    <span class="s4">&quot;exp2_&quot;</span><span class="s3">,</span>
<a name="l1881"><span class="ln">1881 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1882"><span class="ln">1882 </span></a>exp2_() -&gt; Tensor 
<a name="l1883"><span class="ln">1883 </span></a> 
<a name="l1884"><span class="ln">1884 </span></a>In-place version of :meth:`~Tensor.exp2` 
<a name="l1885"><span class="ln">1885 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1886"><span class="ln">1886 </span></a><span class="s3">)</span>
<a name="l1887"><span class="ln">1887 </span></a>
<a name="l1888"><span class="ln">1888 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1889"><span class="ln">1889 </span></a>    <span class="s4">&quot;expm1&quot;</span><span class="s3">,</span>
<a name="l1890"><span class="ln">1890 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1891"><span class="ln">1891 </span></a>expm1() -&gt; Tensor 
<a name="l1892"><span class="ln">1892 </span></a> 
<a name="l1893"><span class="ln">1893 </span></a>See :func:`torch.expm1` 
<a name="l1894"><span class="ln">1894 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1895"><span class="ln">1895 </span></a><span class="s3">)</span>
<a name="l1896"><span class="ln">1896 </span></a>
<a name="l1897"><span class="ln">1897 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1898"><span class="ln">1898 </span></a>    <span class="s4">&quot;expm1_&quot;</span><span class="s3">,</span>
<a name="l1899"><span class="ln">1899 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1900"><span class="ln">1900 </span></a>expm1_() -&gt; Tensor 
<a name="l1901"><span class="ln">1901 </span></a> 
<a name="l1902"><span class="ln">1902 </span></a>In-place version of :meth:`~Tensor.expm1` 
<a name="l1903"><span class="ln">1903 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1904"><span class="ln">1904 </span></a><span class="s3">)</span>
<a name="l1905"><span class="ln">1905 </span></a>
<a name="l1906"><span class="ln">1906 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1907"><span class="ln">1907 </span></a>    <span class="s4">&quot;exponential_&quot;</span><span class="s3">,</span>
<a name="l1908"><span class="ln">1908 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1909"><span class="ln">1909 </span></a>exponential_(lambd=1, *, generator=None) -&gt; Tensor 
<a name="l1910"><span class="ln">1910 </span></a> 
<a name="l1911"><span class="ln">1911 </span></a>Fills :attr:`self` tensor with elements drawn from the PDF (probability density function): 
<a name="l1912"><span class="ln">1912 </span></a> 
<a name="l1913"><span class="ln">1913 </span></a>.. math:: 
<a name="l1914"><span class="ln">1914 </span></a> 
<a name="l1915"><span class="ln">1915 </span></a>    f(x) = \lambda e^{-\lambda x}, x &gt; 0 
<a name="l1916"><span class="ln">1916 </span></a> 
<a name="l1917"><span class="ln">1917 </span></a>.. note:: 
<a name="l1918"><span class="ln">1918 </span></a>  In probability theory, exponential distribution is supported on interval [0, :math:`\inf`) (i.e., :math:`x &gt;= 0`) 
<a name="l1919"><span class="ln">1919 </span></a>  implying that zero can be sampled from the exponential distribution. 
<a name="l1920"><span class="ln">1920 </span></a>  However, :func:`torch.Tensor.exponential_` does not sample zero, 
<a name="l1921"><span class="ln">1921 </span></a>  which means that its actual support is the interval (0, :math:`\inf`). 
<a name="l1922"><span class="ln">1922 </span></a> 
<a name="l1923"><span class="ln">1923 </span></a>  Note that :func:`torch.distributions.exponential.Exponential` is supported on the interval [0, :math:`\inf`) and can sample zero. 
<a name="l1924"><span class="ln">1924 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1925"><span class="ln">1925 </span></a><span class="s3">)</span>
<a name="l1926"><span class="ln">1926 </span></a>
<a name="l1927"><span class="ln">1927 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1928"><span class="ln">1928 </span></a>    <span class="s4">&quot;fill_&quot;</span><span class="s3">,</span>
<a name="l1929"><span class="ln">1929 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1930"><span class="ln">1930 </span></a>fill_(value) -&gt; Tensor 
<a name="l1931"><span class="ln">1931 </span></a> 
<a name="l1932"><span class="ln">1932 </span></a>Fills :attr:`self` tensor with the specified value. 
<a name="l1933"><span class="ln">1933 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1934"><span class="ln">1934 </span></a><span class="s3">)</span>
<a name="l1935"><span class="ln">1935 </span></a>
<a name="l1936"><span class="ln">1936 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1937"><span class="ln">1937 </span></a>    <span class="s4">&quot;floor&quot;</span><span class="s3">,</span>
<a name="l1938"><span class="ln">1938 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1939"><span class="ln">1939 </span></a>floor() -&gt; Tensor 
<a name="l1940"><span class="ln">1940 </span></a> 
<a name="l1941"><span class="ln">1941 </span></a>See :func:`torch.floor` 
<a name="l1942"><span class="ln">1942 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1943"><span class="ln">1943 </span></a><span class="s3">)</span>
<a name="l1944"><span class="ln">1944 </span></a>
<a name="l1945"><span class="ln">1945 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1946"><span class="ln">1946 </span></a>    <span class="s4">&quot;flip&quot;</span><span class="s3">,</span>
<a name="l1947"><span class="ln">1947 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1948"><span class="ln">1948 </span></a>flip(dims) -&gt; Tensor 
<a name="l1949"><span class="ln">1949 </span></a> 
<a name="l1950"><span class="ln">1950 </span></a>See :func:`torch.flip` 
<a name="l1951"><span class="ln">1951 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1952"><span class="ln">1952 </span></a><span class="s3">)</span>
<a name="l1953"><span class="ln">1953 </span></a>
<a name="l1954"><span class="ln">1954 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1955"><span class="ln">1955 </span></a>    <span class="s4">&quot;fliplr&quot;</span><span class="s3">,</span>
<a name="l1956"><span class="ln">1956 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1957"><span class="ln">1957 </span></a>fliplr() -&gt; Tensor 
<a name="l1958"><span class="ln">1958 </span></a> 
<a name="l1959"><span class="ln">1959 </span></a>See :func:`torch.fliplr` 
<a name="l1960"><span class="ln">1960 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1961"><span class="ln">1961 </span></a><span class="s3">)</span>
<a name="l1962"><span class="ln">1962 </span></a>
<a name="l1963"><span class="ln">1963 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1964"><span class="ln">1964 </span></a>    <span class="s4">&quot;flipud&quot;</span><span class="s3">,</span>
<a name="l1965"><span class="ln">1965 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1966"><span class="ln">1966 </span></a>flipud() -&gt; Tensor 
<a name="l1967"><span class="ln">1967 </span></a> 
<a name="l1968"><span class="ln">1968 </span></a>See :func:`torch.flipud` 
<a name="l1969"><span class="ln">1969 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1970"><span class="ln">1970 </span></a><span class="s3">)</span>
<a name="l1971"><span class="ln">1971 </span></a>
<a name="l1972"><span class="ln">1972 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1973"><span class="ln">1973 </span></a>    <span class="s4">&quot;roll&quot;</span><span class="s3">,</span>
<a name="l1974"><span class="ln">1974 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1975"><span class="ln">1975 </span></a>roll(shifts, dims) -&gt; Tensor 
<a name="l1976"><span class="ln">1976 </span></a> 
<a name="l1977"><span class="ln">1977 </span></a>See :func:`torch.roll` 
<a name="l1978"><span class="ln">1978 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1979"><span class="ln">1979 </span></a><span class="s3">)</span>
<a name="l1980"><span class="ln">1980 </span></a>
<a name="l1981"><span class="ln">1981 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1982"><span class="ln">1982 </span></a>    <span class="s4">&quot;floor_&quot;</span><span class="s3">,</span>
<a name="l1983"><span class="ln">1983 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1984"><span class="ln">1984 </span></a>floor_() -&gt; Tensor 
<a name="l1985"><span class="ln">1985 </span></a> 
<a name="l1986"><span class="ln">1986 </span></a>In-place version of :meth:`~Tensor.floor` 
<a name="l1987"><span class="ln">1987 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1988"><span class="ln">1988 </span></a><span class="s3">)</span>
<a name="l1989"><span class="ln">1989 </span></a>
<a name="l1990"><span class="ln">1990 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l1991"><span class="ln">1991 </span></a>    <span class="s4">&quot;fmod&quot;</span><span class="s3">,</span>
<a name="l1992"><span class="ln">1992 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l1993"><span class="ln">1993 </span></a>fmod(divisor) -&gt; Tensor 
<a name="l1994"><span class="ln">1994 </span></a> 
<a name="l1995"><span class="ln">1995 </span></a>See :func:`torch.fmod` 
<a name="l1996"><span class="ln">1996 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l1997"><span class="ln">1997 </span></a><span class="s3">)</span>
<a name="l1998"><span class="ln">1998 </span></a>
<a name="l1999"><span class="ln">1999 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2000"><span class="ln">2000 </span></a>    <span class="s4">&quot;fmod_&quot;</span><span class="s3">,</span>
<a name="l2001"><span class="ln">2001 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2002"><span class="ln">2002 </span></a>fmod_(divisor) -&gt; Tensor 
<a name="l2003"><span class="ln">2003 </span></a> 
<a name="l2004"><span class="ln">2004 </span></a>In-place version of :meth:`~Tensor.fmod` 
<a name="l2005"><span class="ln">2005 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2006"><span class="ln">2006 </span></a><span class="s3">)</span>
<a name="l2007"><span class="ln">2007 </span></a>
<a name="l2008"><span class="ln">2008 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2009"><span class="ln">2009 </span></a>    <span class="s4">&quot;frac&quot;</span><span class="s3">,</span>
<a name="l2010"><span class="ln">2010 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2011"><span class="ln">2011 </span></a>frac() -&gt; Tensor 
<a name="l2012"><span class="ln">2012 </span></a> 
<a name="l2013"><span class="ln">2013 </span></a>See :func:`torch.frac` 
<a name="l2014"><span class="ln">2014 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2015"><span class="ln">2015 </span></a><span class="s3">)</span>
<a name="l2016"><span class="ln">2016 </span></a>
<a name="l2017"><span class="ln">2017 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2018"><span class="ln">2018 </span></a>    <span class="s4">&quot;frac_&quot;</span><span class="s3">,</span>
<a name="l2019"><span class="ln">2019 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2020"><span class="ln">2020 </span></a>frac_() -&gt; Tensor 
<a name="l2021"><span class="ln">2021 </span></a> 
<a name="l2022"><span class="ln">2022 </span></a>In-place version of :meth:`~Tensor.frac` 
<a name="l2023"><span class="ln">2023 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2024"><span class="ln">2024 </span></a><span class="s3">)</span>
<a name="l2025"><span class="ln">2025 </span></a>
<a name="l2026"><span class="ln">2026 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2027"><span class="ln">2027 </span></a>    <span class="s4">&quot;frexp&quot;</span><span class="s3">,</span>
<a name="l2028"><span class="ln">2028 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2029"><span class="ln">2029 </span></a>frexp(input) -&gt; (Tensor mantissa, Tensor exponent) 
<a name="l2030"><span class="ln">2030 </span></a> 
<a name="l2031"><span class="ln">2031 </span></a>See :func:`torch.frexp` 
<a name="l2032"><span class="ln">2032 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2033"><span class="ln">2033 </span></a><span class="s3">)</span>
<a name="l2034"><span class="ln">2034 </span></a>
<a name="l2035"><span class="ln">2035 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2036"><span class="ln">2036 </span></a>    <span class="s4">&quot;flatten&quot;</span><span class="s3">,</span>
<a name="l2037"><span class="ln">2037 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2038"><span class="ln">2038 </span></a>flatten(start_dim=0, end_dim=-1) -&gt; Tensor 
<a name="l2039"><span class="ln">2039 </span></a> 
<a name="l2040"><span class="ln">2040 </span></a>See :func:`torch.flatten` 
<a name="l2041"><span class="ln">2041 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2042"><span class="ln">2042 </span></a><span class="s3">)</span>
<a name="l2043"><span class="ln">2043 </span></a>
<a name="l2044"><span class="ln">2044 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2045"><span class="ln">2045 </span></a>    <span class="s4">&quot;gather&quot;</span><span class="s3">,</span>
<a name="l2046"><span class="ln">2046 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2047"><span class="ln">2047 </span></a>gather(dim, index) -&gt; Tensor 
<a name="l2048"><span class="ln">2048 </span></a> 
<a name="l2049"><span class="ln">2049 </span></a>See :func:`torch.gather` 
<a name="l2050"><span class="ln">2050 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2051"><span class="ln">2051 </span></a><span class="s3">)</span>
<a name="l2052"><span class="ln">2052 </span></a>
<a name="l2053"><span class="ln">2053 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2054"><span class="ln">2054 </span></a>    <span class="s4">&quot;gcd&quot;</span><span class="s3">,</span>
<a name="l2055"><span class="ln">2055 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2056"><span class="ln">2056 </span></a>gcd(other) -&gt; Tensor 
<a name="l2057"><span class="ln">2057 </span></a> 
<a name="l2058"><span class="ln">2058 </span></a>See :func:`torch.gcd` 
<a name="l2059"><span class="ln">2059 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2060"><span class="ln">2060 </span></a><span class="s3">)</span>
<a name="l2061"><span class="ln">2061 </span></a>
<a name="l2062"><span class="ln">2062 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2063"><span class="ln">2063 </span></a>    <span class="s4">&quot;gcd_&quot;</span><span class="s3">,</span>
<a name="l2064"><span class="ln">2064 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2065"><span class="ln">2065 </span></a>gcd_(other) -&gt; Tensor 
<a name="l2066"><span class="ln">2066 </span></a> 
<a name="l2067"><span class="ln">2067 </span></a>In-place version of :meth:`~Tensor.gcd` 
<a name="l2068"><span class="ln">2068 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2069"><span class="ln">2069 </span></a><span class="s3">)</span>
<a name="l2070"><span class="ln">2070 </span></a>
<a name="l2071"><span class="ln">2071 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2072"><span class="ln">2072 </span></a>    <span class="s4">&quot;ge&quot;</span><span class="s3">,</span>
<a name="l2073"><span class="ln">2073 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2074"><span class="ln">2074 </span></a>ge(other) -&gt; Tensor 
<a name="l2075"><span class="ln">2075 </span></a> 
<a name="l2076"><span class="ln">2076 </span></a>See :func:`torch.ge`. 
<a name="l2077"><span class="ln">2077 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2078"><span class="ln">2078 </span></a><span class="s3">)</span>
<a name="l2079"><span class="ln">2079 </span></a>
<a name="l2080"><span class="ln">2080 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2081"><span class="ln">2081 </span></a>    <span class="s4">&quot;ge_&quot;</span><span class="s3">,</span>
<a name="l2082"><span class="ln">2082 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2083"><span class="ln">2083 </span></a>ge_(other) -&gt; Tensor 
<a name="l2084"><span class="ln">2084 </span></a> 
<a name="l2085"><span class="ln">2085 </span></a>In-place version of :meth:`~Tensor.ge`. 
<a name="l2086"><span class="ln">2086 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2087"><span class="ln">2087 </span></a><span class="s3">)</span>
<a name="l2088"><span class="ln">2088 </span></a>
<a name="l2089"><span class="ln">2089 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2090"><span class="ln">2090 </span></a>    <span class="s4">&quot;greater_equal&quot;</span><span class="s3">,</span>
<a name="l2091"><span class="ln">2091 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2092"><span class="ln">2092 </span></a>greater_equal(other) -&gt; Tensor 
<a name="l2093"><span class="ln">2093 </span></a> 
<a name="l2094"><span class="ln">2094 </span></a>See :func:`torch.greater_equal`. 
<a name="l2095"><span class="ln">2095 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2096"><span class="ln">2096 </span></a><span class="s3">)</span>
<a name="l2097"><span class="ln">2097 </span></a>
<a name="l2098"><span class="ln">2098 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2099"><span class="ln">2099 </span></a>    <span class="s4">&quot;greater_equal_&quot;</span><span class="s3">,</span>
<a name="l2100"><span class="ln">2100 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2101"><span class="ln">2101 </span></a>greater_equal_(other) -&gt; Tensor 
<a name="l2102"><span class="ln">2102 </span></a> 
<a name="l2103"><span class="ln">2103 </span></a>In-place version of :meth:`~Tensor.greater_equal`. 
<a name="l2104"><span class="ln">2104 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2105"><span class="ln">2105 </span></a><span class="s3">)</span>
<a name="l2106"><span class="ln">2106 </span></a>
<a name="l2107"><span class="ln">2107 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2108"><span class="ln">2108 </span></a>    <span class="s4">&quot;geometric_&quot;</span><span class="s3">,</span>
<a name="l2109"><span class="ln">2109 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2110"><span class="ln">2110 </span></a>geometric_(p, *, generator=None) -&gt; Tensor 
<a name="l2111"><span class="ln">2111 </span></a> 
<a name="l2112"><span class="ln">2112 </span></a>Fills :attr:`self` tensor with elements drawn from the geometric distribution: 
<a name="l2113"><span class="ln">2113 </span></a> 
<a name="l2114"><span class="ln">2114 </span></a>.. math:: 
<a name="l2115"><span class="ln">2115 </span></a> 
<a name="l2116"><span class="ln">2116 </span></a>    P(X=k) = (1 - p)^{k - 1} p, k = 1, 2, ... 
<a name="l2117"><span class="ln">2117 </span></a> 
<a name="l2118"><span class="ln">2118 </span></a>.. note:: 
<a name="l2119"><span class="ln">2119 </span></a>  :func:`torch.Tensor.geometric_` `k`-th trial is the first success hence draws samples in :math:`\{1, 2, \ldots\}`, whereas 
<a name="l2120"><span class="ln">2120 </span></a>  :func:`torch.distributions.geometric.Geometric` :math:`(k+1)`-th trial is the first success 
<a name="l2121"><span class="ln">2121 </span></a>  hence draws samples in :math:`\{0, 1, \ldots\}`. 
<a name="l2122"><span class="ln">2122 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2123"><span class="ln">2123 </span></a><span class="s3">)</span>
<a name="l2124"><span class="ln">2124 </span></a>
<a name="l2125"><span class="ln">2125 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2126"><span class="ln">2126 </span></a>    <span class="s4">&quot;geqrf&quot;</span><span class="s3">,</span>
<a name="l2127"><span class="ln">2127 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2128"><span class="ln">2128 </span></a>geqrf() -&gt; (Tensor, Tensor) 
<a name="l2129"><span class="ln">2129 </span></a> 
<a name="l2130"><span class="ln">2130 </span></a>See :func:`torch.geqrf` 
<a name="l2131"><span class="ln">2131 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2132"><span class="ln">2132 </span></a><span class="s3">)</span>
<a name="l2133"><span class="ln">2133 </span></a>
<a name="l2134"><span class="ln">2134 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2135"><span class="ln">2135 </span></a>    <span class="s4">&quot;ger&quot;</span><span class="s3">,</span>
<a name="l2136"><span class="ln">2136 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2137"><span class="ln">2137 </span></a>ger(vec2) -&gt; Tensor 
<a name="l2138"><span class="ln">2138 </span></a> 
<a name="l2139"><span class="ln">2139 </span></a>See :func:`torch.ger` 
<a name="l2140"><span class="ln">2140 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2141"><span class="ln">2141 </span></a><span class="s3">)</span>
<a name="l2142"><span class="ln">2142 </span></a>
<a name="l2143"><span class="ln">2143 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2144"><span class="ln">2144 </span></a>    <span class="s4">&quot;inner&quot;</span><span class="s3">,</span>
<a name="l2145"><span class="ln">2145 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2146"><span class="ln">2146 </span></a>inner(other) -&gt; Tensor 
<a name="l2147"><span class="ln">2147 </span></a> 
<a name="l2148"><span class="ln">2148 </span></a>See :func:`torch.inner`. 
<a name="l2149"><span class="ln">2149 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2150"><span class="ln">2150 </span></a><span class="s3">)</span>
<a name="l2151"><span class="ln">2151 </span></a>
<a name="l2152"><span class="ln">2152 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2153"><span class="ln">2153 </span></a>    <span class="s4">&quot;outer&quot;</span><span class="s3">,</span>
<a name="l2154"><span class="ln">2154 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2155"><span class="ln">2155 </span></a>outer(vec2) -&gt; Tensor 
<a name="l2156"><span class="ln">2156 </span></a> 
<a name="l2157"><span class="ln">2157 </span></a>See :func:`torch.outer`. 
<a name="l2158"><span class="ln">2158 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2159"><span class="ln">2159 </span></a><span class="s3">)</span>
<a name="l2160"><span class="ln">2160 </span></a>
<a name="l2161"><span class="ln">2161 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2162"><span class="ln">2162 </span></a>    <span class="s4">&quot;hypot&quot;</span><span class="s3">,</span>
<a name="l2163"><span class="ln">2163 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2164"><span class="ln">2164 </span></a>hypot(other) -&gt; Tensor 
<a name="l2165"><span class="ln">2165 </span></a> 
<a name="l2166"><span class="ln">2166 </span></a>See :func:`torch.hypot` 
<a name="l2167"><span class="ln">2167 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2168"><span class="ln">2168 </span></a><span class="s3">)</span>
<a name="l2169"><span class="ln">2169 </span></a>
<a name="l2170"><span class="ln">2170 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2171"><span class="ln">2171 </span></a>    <span class="s4">&quot;hypot_&quot;</span><span class="s3">,</span>
<a name="l2172"><span class="ln">2172 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2173"><span class="ln">2173 </span></a>hypot_(other) -&gt; Tensor 
<a name="l2174"><span class="ln">2174 </span></a> 
<a name="l2175"><span class="ln">2175 </span></a>In-place version of :meth:`~Tensor.hypot` 
<a name="l2176"><span class="ln">2176 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2177"><span class="ln">2177 </span></a><span class="s3">)</span>
<a name="l2178"><span class="ln">2178 </span></a>
<a name="l2179"><span class="ln">2179 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2180"><span class="ln">2180 </span></a>    <span class="s4">&quot;i0&quot;</span><span class="s3">,</span>
<a name="l2181"><span class="ln">2181 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2182"><span class="ln">2182 </span></a>i0() -&gt; Tensor 
<a name="l2183"><span class="ln">2183 </span></a> 
<a name="l2184"><span class="ln">2184 </span></a>See :func:`torch.i0` 
<a name="l2185"><span class="ln">2185 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2186"><span class="ln">2186 </span></a><span class="s3">)</span>
<a name="l2187"><span class="ln">2187 </span></a>
<a name="l2188"><span class="ln">2188 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2189"><span class="ln">2189 </span></a>    <span class="s4">&quot;i0_&quot;</span><span class="s3">,</span>
<a name="l2190"><span class="ln">2190 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2191"><span class="ln">2191 </span></a>i0_() -&gt; Tensor 
<a name="l2192"><span class="ln">2192 </span></a> 
<a name="l2193"><span class="ln">2193 </span></a>In-place version of :meth:`~Tensor.i0` 
<a name="l2194"><span class="ln">2194 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2195"><span class="ln">2195 </span></a><span class="s3">)</span>
<a name="l2196"><span class="ln">2196 </span></a>
<a name="l2197"><span class="ln">2197 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2198"><span class="ln">2198 </span></a>    <span class="s4">&quot;igamma&quot;</span><span class="s3">,</span>
<a name="l2199"><span class="ln">2199 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2200"><span class="ln">2200 </span></a>igamma(other) -&gt; Tensor 
<a name="l2201"><span class="ln">2201 </span></a> 
<a name="l2202"><span class="ln">2202 </span></a>See :func:`torch.igamma` 
<a name="l2203"><span class="ln">2203 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2204"><span class="ln">2204 </span></a><span class="s3">)</span>
<a name="l2205"><span class="ln">2205 </span></a>
<a name="l2206"><span class="ln">2206 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2207"><span class="ln">2207 </span></a>    <span class="s4">&quot;igamma_&quot;</span><span class="s3">,</span>
<a name="l2208"><span class="ln">2208 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2209"><span class="ln">2209 </span></a>igamma_(other) -&gt; Tensor 
<a name="l2210"><span class="ln">2210 </span></a> 
<a name="l2211"><span class="ln">2211 </span></a>In-place version of :meth:`~Tensor.igamma` 
<a name="l2212"><span class="ln">2212 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2213"><span class="ln">2213 </span></a><span class="s3">)</span>
<a name="l2214"><span class="ln">2214 </span></a>
<a name="l2215"><span class="ln">2215 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2216"><span class="ln">2216 </span></a>    <span class="s4">&quot;igammac&quot;</span><span class="s3">,</span>
<a name="l2217"><span class="ln">2217 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2218"><span class="ln">2218 </span></a>igammac(other) -&gt; Tensor 
<a name="l2219"><span class="ln">2219 </span></a>See :func:`torch.igammac` 
<a name="l2220"><span class="ln">2220 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2221"><span class="ln">2221 </span></a><span class="s3">)</span>
<a name="l2222"><span class="ln">2222 </span></a>
<a name="l2223"><span class="ln">2223 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2224"><span class="ln">2224 </span></a>    <span class="s4">&quot;igammac_&quot;</span><span class="s3">,</span>
<a name="l2225"><span class="ln">2225 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2226"><span class="ln">2226 </span></a>igammac_(other) -&gt; Tensor 
<a name="l2227"><span class="ln">2227 </span></a>In-place version of :meth:`~Tensor.igammac` 
<a name="l2228"><span class="ln">2228 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2229"><span class="ln">2229 </span></a><span class="s3">)</span>
<a name="l2230"><span class="ln">2230 </span></a>
<a name="l2231"><span class="ln">2231 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2232"><span class="ln">2232 </span></a>    <span class="s4">&quot;indices&quot;</span><span class="s3">,</span>
<a name="l2233"><span class="ln">2233 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2234"><span class="ln">2234 </span></a>indices() -&gt; Tensor 
<a name="l2235"><span class="ln">2235 </span></a> 
<a name="l2236"><span class="ln">2236 </span></a>Return the indices tensor of a :ref:`sparse COO tensor &lt;sparse-coo-docs&gt;`. 
<a name="l2237"><span class="ln">2237 </span></a> 
<a name="l2238"><span class="ln">2238 </span></a>.. warning:: 
<a name="l2239"><span class="ln">2239 </span></a>  Throws an error if :attr:`self` is not a sparse COO tensor. 
<a name="l2240"><span class="ln">2240 </span></a> 
<a name="l2241"><span class="ln">2241 </span></a>See also :meth:`Tensor.values`. 
<a name="l2242"><span class="ln">2242 </span></a> 
<a name="l2243"><span class="ln">2243 </span></a>.. note:: 
<a name="l2244"><span class="ln">2244 </span></a>  This method can only be called on a coalesced sparse tensor. See 
<a name="l2245"><span class="ln">2245 </span></a>  :meth:`Tensor.coalesce` for details. 
<a name="l2246"><span class="ln">2246 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2247"><span class="ln">2247 </span></a><span class="s3">)</span>
<a name="l2248"><span class="ln">2248 </span></a>
<a name="l2249"><span class="ln">2249 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2250"><span class="ln">2250 </span></a>    <span class="s4">&quot;get_device&quot;</span><span class="s3">,</span>
<a name="l2251"><span class="ln">2251 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2252"><span class="ln">2252 </span></a>get_device() -&gt; Device ordinal (Integer) 
<a name="l2253"><span class="ln">2253 </span></a> 
<a name="l2254"><span class="ln">2254 </span></a>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. 
<a name="l2255"><span class="ln">2255 </span></a>For CPU tensors, this function returns `-1`. 
<a name="l2256"><span class="ln">2256 </span></a> 
<a name="l2257"><span class="ln">2257 </span></a>Example:: 
<a name="l2258"><span class="ln">2258 </span></a> 
<a name="l2259"><span class="ln">2259 </span></a>    &gt;&gt;&gt; x = torch.randn(3, 4, 5, device='cuda:0') 
<a name="l2260"><span class="ln">2260 </span></a>    &gt;&gt;&gt; x.get_device() 
<a name="l2261"><span class="ln">2261 </span></a>    0 
<a name="l2262"><span class="ln">2262 </span></a>    &gt;&gt;&gt; x.cpu().get_device() 
<a name="l2263"><span class="ln">2263 </span></a>    -1 
<a name="l2264"><span class="ln">2264 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2265"><span class="ln">2265 </span></a><span class="s3">)</span>
<a name="l2266"><span class="ln">2266 </span></a>
<a name="l2267"><span class="ln">2267 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2268"><span class="ln">2268 </span></a>    <span class="s4">&quot;values&quot;</span><span class="s3">,</span>
<a name="l2269"><span class="ln">2269 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2270"><span class="ln">2270 </span></a>values() -&gt; Tensor 
<a name="l2271"><span class="ln">2271 </span></a> 
<a name="l2272"><span class="ln">2272 </span></a>Return the values tensor of a :ref:`sparse COO tensor &lt;sparse-coo-docs&gt;`. 
<a name="l2273"><span class="ln">2273 </span></a> 
<a name="l2274"><span class="ln">2274 </span></a>.. warning:: 
<a name="l2275"><span class="ln">2275 </span></a>  Throws an error if :attr:`self` is not a sparse COO tensor. 
<a name="l2276"><span class="ln">2276 </span></a> 
<a name="l2277"><span class="ln">2277 </span></a>See also :meth:`Tensor.indices`. 
<a name="l2278"><span class="ln">2278 </span></a> 
<a name="l2279"><span class="ln">2279 </span></a>.. note:: 
<a name="l2280"><span class="ln">2280 </span></a>  This method can only be called on a coalesced sparse tensor. See 
<a name="l2281"><span class="ln">2281 </span></a>  :meth:`Tensor.coalesce` for details. 
<a name="l2282"><span class="ln">2282 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2283"><span class="ln">2283 </span></a><span class="s3">)</span>
<a name="l2284"><span class="ln">2284 </span></a>
<a name="l2285"><span class="ln">2285 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2286"><span class="ln">2286 </span></a>    <span class="s4">&quot;gt&quot;</span><span class="s3">,</span>
<a name="l2287"><span class="ln">2287 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2288"><span class="ln">2288 </span></a>gt(other) -&gt; Tensor 
<a name="l2289"><span class="ln">2289 </span></a> 
<a name="l2290"><span class="ln">2290 </span></a>See :func:`torch.gt`. 
<a name="l2291"><span class="ln">2291 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2292"><span class="ln">2292 </span></a><span class="s3">)</span>
<a name="l2293"><span class="ln">2293 </span></a>
<a name="l2294"><span class="ln">2294 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2295"><span class="ln">2295 </span></a>    <span class="s4">&quot;gt_&quot;</span><span class="s3">,</span>
<a name="l2296"><span class="ln">2296 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2297"><span class="ln">2297 </span></a>gt_(other) -&gt; Tensor 
<a name="l2298"><span class="ln">2298 </span></a> 
<a name="l2299"><span class="ln">2299 </span></a>In-place version of :meth:`~Tensor.gt`. 
<a name="l2300"><span class="ln">2300 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2301"><span class="ln">2301 </span></a><span class="s3">)</span>
<a name="l2302"><span class="ln">2302 </span></a>
<a name="l2303"><span class="ln">2303 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2304"><span class="ln">2304 </span></a>    <span class="s4">&quot;greater&quot;</span><span class="s3">,</span>
<a name="l2305"><span class="ln">2305 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2306"><span class="ln">2306 </span></a>greater(other) -&gt; Tensor 
<a name="l2307"><span class="ln">2307 </span></a> 
<a name="l2308"><span class="ln">2308 </span></a>See :func:`torch.greater`. 
<a name="l2309"><span class="ln">2309 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2310"><span class="ln">2310 </span></a><span class="s3">)</span>
<a name="l2311"><span class="ln">2311 </span></a>
<a name="l2312"><span class="ln">2312 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2313"><span class="ln">2313 </span></a>    <span class="s4">&quot;greater_&quot;</span><span class="s3">,</span>
<a name="l2314"><span class="ln">2314 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2315"><span class="ln">2315 </span></a>greater_(other) -&gt; Tensor 
<a name="l2316"><span class="ln">2316 </span></a> 
<a name="l2317"><span class="ln">2317 </span></a>In-place version of :meth:`~Tensor.greater`. 
<a name="l2318"><span class="ln">2318 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2319"><span class="ln">2319 </span></a><span class="s3">)</span>
<a name="l2320"><span class="ln">2320 </span></a>
<a name="l2321"><span class="ln">2321 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2322"><span class="ln">2322 </span></a>    <span class="s4">&quot;has_names&quot;</span><span class="s3">,</span>
<a name="l2323"><span class="ln">2323 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2324"><span class="ln">2324 </span></a>Is ``True`` if any of this tensor's dimensions are named. Otherwise, is ``False``. 
<a name="l2325"><span class="ln">2325 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2326"><span class="ln">2326 </span></a><span class="s3">)</span>
<a name="l2327"><span class="ln">2327 </span></a>
<a name="l2328"><span class="ln">2328 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2329"><span class="ln">2329 </span></a>    <span class="s4">&quot;hardshrink&quot;</span><span class="s3">,</span>
<a name="l2330"><span class="ln">2330 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2331"><span class="ln">2331 </span></a>hardshrink(lambd=0.5) -&gt; Tensor 
<a name="l2332"><span class="ln">2332 </span></a> 
<a name="l2333"><span class="ln">2333 </span></a>See :func:`torch.nn.functional.hardshrink` 
<a name="l2334"><span class="ln">2334 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2335"><span class="ln">2335 </span></a><span class="s3">)</span>
<a name="l2336"><span class="ln">2336 </span></a>
<a name="l2337"><span class="ln">2337 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2338"><span class="ln">2338 </span></a>    <span class="s4">&quot;heaviside&quot;</span><span class="s3">,</span>
<a name="l2339"><span class="ln">2339 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2340"><span class="ln">2340 </span></a>heaviside(values) -&gt; Tensor 
<a name="l2341"><span class="ln">2341 </span></a> 
<a name="l2342"><span class="ln">2342 </span></a>See :func:`torch.heaviside` 
<a name="l2343"><span class="ln">2343 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2344"><span class="ln">2344 </span></a><span class="s3">)</span>
<a name="l2345"><span class="ln">2345 </span></a>
<a name="l2346"><span class="ln">2346 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2347"><span class="ln">2347 </span></a>    <span class="s4">&quot;heaviside_&quot;</span><span class="s3">,</span>
<a name="l2348"><span class="ln">2348 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2349"><span class="ln">2349 </span></a>heaviside_(values) -&gt; Tensor 
<a name="l2350"><span class="ln">2350 </span></a> 
<a name="l2351"><span class="ln">2351 </span></a>In-place version of :meth:`~Tensor.heaviside` 
<a name="l2352"><span class="ln">2352 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2353"><span class="ln">2353 </span></a><span class="s3">)</span>
<a name="l2354"><span class="ln">2354 </span></a>
<a name="l2355"><span class="ln">2355 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2356"><span class="ln">2356 </span></a>    <span class="s4">&quot;histc&quot;</span><span class="s3">,</span>
<a name="l2357"><span class="ln">2357 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2358"><span class="ln">2358 </span></a>histc(bins=100, min=0, max=0) -&gt; Tensor 
<a name="l2359"><span class="ln">2359 </span></a> 
<a name="l2360"><span class="ln">2360 </span></a>See :func:`torch.histc` 
<a name="l2361"><span class="ln">2361 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2362"><span class="ln">2362 </span></a><span class="s3">)</span>
<a name="l2363"><span class="ln">2363 </span></a>
<a name="l2364"><span class="ln">2364 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2365"><span class="ln">2365 </span></a>    <span class="s4">&quot;histogram&quot;</span><span class="s3">,</span>
<a name="l2366"><span class="ln">2366 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2367"><span class="ln">2367 </span></a>histogram(input, bins, *, range=None, weight=None, density=False) -&gt; (Tensor, Tensor) 
<a name="l2368"><span class="ln">2368 </span></a> 
<a name="l2369"><span class="ln">2369 </span></a>See :func:`torch.histogram` 
<a name="l2370"><span class="ln">2370 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2371"><span class="ln">2371 </span></a><span class="s3">)</span>
<a name="l2372"><span class="ln">2372 </span></a>
<a name="l2373"><span class="ln">2373 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2374"><span class="ln">2374 </span></a>    <span class="s4">&quot;index_add_&quot;</span><span class="s3">,</span>
<a name="l2375"><span class="ln">2375 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2376"><span class="ln">2376 </span></a>index_add_(dim, index, source, *, alpha=1) -&gt; Tensor 
<a name="l2377"><span class="ln">2377 </span></a> 
<a name="l2378"><span class="ln">2378 </span></a>Accumulate the elements of :attr:`alpha` times ``source`` into the :attr:`self` 
<a name="l2379"><span class="ln">2379 </span></a>tensor by adding to the indices in the order given in :attr:`index`. For example, 
<a name="l2380"><span class="ln">2380 </span></a>if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\ th row of 
<a name="l2381"><span class="ln">2381 </span></a>``source`` is subtracted from the ``j``\ th row of :attr:`self`. 
<a name="l2382"><span class="ln">2382 </span></a> 
<a name="l2383"><span class="ln">2383 </span></a>The :attr:`dim`\ th dimension of ``source`` must have the same size as the 
<a name="l2384"><span class="ln">2384 </span></a>length of :attr:`index` (which must be a vector), and all other dimensions must 
<a name="l2385"><span class="ln">2385 </span></a>match :attr:`self`, or an error will be raised. 
<a name="l2386"><span class="ln">2386 </span></a> 
<a name="l2387"><span class="ln">2387 </span></a>For a 3-D tensor the output is given as:: 
<a name="l2388"><span class="ln">2388 </span></a> 
<a name="l2389"><span class="ln">2389 </span></a>    self[index[i], :, :] += alpha * src[i, :, :]  # if dim == 0 
<a name="l2390"><span class="ln">2390 </span></a>    self[:, index[i], :] += alpha * src[:, i, :]  # if dim == 1 
<a name="l2391"><span class="ln">2391 </span></a>    self[:, :, index[i]] += alpha * src[:, :, i]  # if dim == 2 
<a name="l2392"><span class="ln">2392 </span></a> 
<a name="l2393"><span class="ln">2393 </span></a>Note: 
<a name="l2394"><span class="ln">2394 </span></a>    {forward_reproducibility_note} 
<a name="l2395"><span class="ln">2395 </span></a> 
<a name="l2396"><span class="ln">2396 </span></a>Args: 
<a name="l2397"><span class="ln">2397 </span></a>    dim (int): dimension along which to index 
<a name="l2398"><span class="ln">2398 </span></a>    index (Tensor): indices of ``source`` to select from, 
<a name="l2399"><span class="ln">2399 </span></a>            should have dtype either `torch.int64` or `torch.int32` 
<a name="l2400"><span class="ln">2400 </span></a>    source (Tensor): the tensor containing values to add 
<a name="l2401"><span class="ln">2401 </span></a> 
<a name="l2402"><span class="ln">2402 </span></a>Keyword args: 
<a name="l2403"><span class="ln">2403 </span></a>    alpha (Number): the scalar multiplier for ``source`` 
<a name="l2404"><span class="ln">2404 </span></a> 
<a name="l2405"><span class="ln">2405 </span></a>Example:: 
<a name="l2406"><span class="ln">2406 </span></a> 
<a name="l2407"><span class="ln">2407 </span></a>    &gt;&gt;&gt; x = torch.ones(5, 3) 
<a name="l2408"><span class="ln">2408 </span></a>    &gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) 
<a name="l2409"><span class="ln">2409 </span></a>    &gt;&gt;&gt; index = torch.tensor([0, 4, 2]) 
<a name="l2410"><span class="ln">2410 </span></a>    &gt;&gt;&gt; x.index_add_(0, index, t) 
<a name="l2411"><span class="ln">2411 </span></a>    tensor([[  2.,   3.,   4.], 
<a name="l2412"><span class="ln">2412 </span></a>            [  1.,   1.,   1.], 
<a name="l2413"><span class="ln">2413 </span></a>            [  8.,   9.,  10.], 
<a name="l2414"><span class="ln">2414 </span></a>            [  1.,   1.,   1.], 
<a name="l2415"><span class="ln">2415 </span></a>            [  5.,   6.,   7.]]) 
<a name="l2416"><span class="ln">2416 </span></a>    &gt;&gt;&gt; x.index_add_(0, index, t, alpha=-1) 
<a name="l2417"><span class="ln">2417 </span></a>    tensor([[  1.,   1.,   1.], 
<a name="l2418"><span class="ln">2418 </span></a>            [  1.,   1.,   1.], 
<a name="l2419"><span class="ln">2419 </span></a>            [  1.,   1.,   1.], 
<a name="l2420"><span class="ln">2420 </span></a>            [  1.,   1.,   1.], 
<a name="l2421"><span class="ln">2421 </span></a>            [  1.,   1.,   1.]]) 
<a name="l2422"><span class="ln">2422 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reproducibility_notes</span><span class="s3">),</span>
<a name="l2423"><span class="ln">2423 </span></a><span class="s3">)</span>
<a name="l2424"><span class="ln">2424 </span></a>
<a name="l2425"><span class="ln">2425 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2426"><span class="ln">2426 </span></a>    <span class="s4">&quot;index_copy_&quot;</span><span class="s3">,</span>
<a name="l2427"><span class="ln">2427 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2428"><span class="ln">2428 </span></a>index_copy_(dim, index, tensor) -&gt; Tensor 
<a name="l2429"><span class="ln">2429 </span></a> 
<a name="l2430"><span class="ln">2430 </span></a>Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting 
<a name="l2431"><span class="ln">2431 </span></a>the indices in the order given in :attr:`index`. For example, if ``dim == 0`` 
<a name="l2432"><span class="ln">2432 </span></a>and ``index[i] == j``, then the ``i``\ th row of :attr:`tensor` is copied to the 
<a name="l2433"><span class="ln">2433 </span></a>``j``\ th row of :attr:`self`. 
<a name="l2434"><span class="ln">2434 </span></a> 
<a name="l2435"><span class="ln">2435 </span></a>The :attr:`dim`\ th dimension of :attr:`tensor` must have the same size as the 
<a name="l2436"><span class="ln">2436 </span></a>length of :attr:`index` (which must be a vector), and all other dimensions must 
<a name="l2437"><span class="ln">2437 </span></a>match :attr:`self`, or an error will be raised. 
<a name="l2438"><span class="ln">2438 </span></a> 
<a name="l2439"><span class="ln">2439 </span></a>.. note:: 
<a name="l2440"><span class="ln">2440 </span></a>    If :attr:`index` contains duplicate entries, multiple elements from 
<a name="l2441"><span class="ln">2441 </span></a>    :attr:`tensor` will be copied to the same index of :attr:`self`. The result 
<a name="l2442"><span class="ln">2442 </span></a>    is nondeterministic since it depends on which copy occurs last. 
<a name="l2443"><span class="ln">2443 </span></a> 
<a name="l2444"><span class="ln">2444 </span></a>Args: 
<a name="l2445"><span class="ln">2445 </span></a>    dim (int): dimension along which to index 
<a name="l2446"><span class="ln">2446 </span></a>    index (LongTensor): indices of :attr:`tensor` to select from 
<a name="l2447"><span class="ln">2447 </span></a>    tensor (Tensor): the tensor containing values to copy 
<a name="l2448"><span class="ln">2448 </span></a> 
<a name="l2449"><span class="ln">2449 </span></a>Example:: 
<a name="l2450"><span class="ln">2450 </span></a> 
<a name="l2451"><span class="ln">2451 </span></a>    &gt;&gt;&gt; x = torch.zeros(5, 3) 
<a name="l2452"><span class="ln">2452 </span></a>    &gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) 
<a name="l2453"><span class="ln">2453 </span></a>    &gt;&gt;&gt; index = torch.tensor([0, 4, 2]) 
<a name="l2454"><span class="ln">2454 </span></a>    &gt;&gt;&gt; x.index_copy_(0, index, t) 
<a name="l2455"><span class="ln">2455 </span></a>    tensor([[ 1.,  2.,  3.], 
<a name="l2456"><span class="ln">2456 </span></a>            [ 0.,  0.,  0.], 
<a name="l2457"><span class="ln">2457 </span></a>            [ 7.,  8.,  9.], 
<a name="l2458"><span class="ln">2458 </span></a>            [ 0.,  0.,  0.], 
<a name="l2459"><span class="ln">2459 </span></a>            [ 4.,  5.,  6.]]) 
<a name="l2460"><span class="ln">2460 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2461"><span class="ln">2461 </span></a><span class="s3">)</span>
<a name="l2462"><span class="ln">2462 </span></a>
<a name="l2463"><span class="ln">2463 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2464"><span class="ln">2464 </span></a>    <span class="s4">&quot;index_fill_&quot;</span><span class="s3">,</span>
<a name="l2465"><span class="ln">2465 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2466"><span class="ln">2466 </span></a>index_fill_(dim, index, value) -&gt; Tensor 
<a name="l2467"><span class="ln">2467 </span></a> 
<a name="l2468"><span class="ln">2468 </span></a>Fills the elements of the :attr:`self` tensor with value :attr:`value` by 
<a name="l2469"><span class="ln">2469 </span></a>selecting the indices in the order given in :attr:`index`. 
<a name="l2470"><span class="ln">2470 </span></a> 
<a name="l2471"><span class="ln">2471 </span></a>Args: 
<a name="l2472"><span class="ln">2472 </span></a>    dim (int): dimension along which to index 
<a name="l2473"><span class="ln">2473 </span></a>    index (LongTensor): indices of :attr:`self` tensor to fill in 
<a name="l2474"><span class="ln">2474 </span></a>    value (float): the value to fill with 
<a name="l2475"><span class="ln">2475 </span></a> 
<a name="l2476"><span class="ln">2476 </span></a>Example:: 
<a name="l2477"><span class="ln">2477 </span></a> 
<a name="l2478"><span class="ln">2478 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) 
<a name="l2479"><span class="ln">2479 </span></a>    &gt;&gt;&gt; index = torch.tensor([0, 2]) 
<a name="l2480"><span class="ln">2480 </span></a>    &gt;&gt;&gt; x.index_fill_(1, index, -1) 
<a name="l2481"><span class="ln">2481 </span></a>    tensor([[-1.,  2., -1.], 
<a name="l2482"><span class="ln">2482 </span></a>            [-1.,  5., -1.], 
<a name="l2483"><span class="ln">2483 </span></a>            [-1.,  8., -1.]]) 
<a name="l2484"><span class="ln">2484 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2485"><span class="ln">2485 </span></a><span class="s3">)</span>
<a name="l2486"><span class="ln">2486 </span></a>
<a name="l2487"><span class="ln">2487 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2488"><span class="ln">2488 </span></a>    <span class="s4">&quot;index_put_&quot;</span><span class="s3">,</span>
<a name="l2489"><span class="ln">2489 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2490"><span class="ln">2490 </span></a>index_put_(indices, values, accumulate=False) -&gt; Tensor 
<a name="l2491"><span class="ln">2491 </span></a> 
<a name="l2492"><span class="ln">2492 </span></a>Puts values from the tensor :attr:`values` into the tensor :attr:`self` using 
<a name="l2493"><span class="ln">2493 </span></a>the indices specified in :attr:`indices` (which is a tuple of Tensors). The 
<a name="l2494"><span class="ln">2494 </span></a>expression ``tensor.index_put_(indices, values)`` is equivalent to 
<a name="l2495"><span class="ln">2495 </span></a>``tensor[indices] = values``. Returns :attr:`self`. 
<a name="l2496"><span class="ln">2496 </span></a> 
<a name="l2497"><span class="ln">2497 </span></a>If :attr:`accumulate` is ``True``, the elements in :attr:`values` are added to 
<a name="l2498"><span class="ln">2498 </span></a>:attr:`self`. If accumulate is ``False``, the behavior is undefined if indices 
<a name="l2499"><span class="ln">2499 </span></a>contain duplicate elements. 
<a name="l2500"><span class="ln">2500 </span></a> 
<a name="l2501"><span class="ln">2501 </span></a>Args: 
<a name="l2502"><span class="ln">2502 </span></a>    indices (tuple of LongTensor): tensors used to index into `self`. 
<a name="l2503"><span class="ln">2503 </span></a>    values (Tensor): tensor of same dtype as `self`. 
<a name="l2504"><span class="ln">2504 </span></a>    accumulate (bool): whether to accumulate into self 
<a name="l2505"><span class="ln">2505 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2506"><span class="ln">2506 </span></a><span class="s3">)</span>
<a name="l2507"><span class="ln">2507 </span></a>
<a name="l2508"><span class="ln">2508 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2509"><span class="ln">2509 </span></a>    <span class="s4">&quot;index_put&quot;</span><span class="s3">,</span>
<a name="l2510"><span class="ln">2510 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2511"><span class="ln">2511 </span></a>index_put(indices, values, accumulate=False) -&gt; Tensor 
<a name="l2512"><span class="ln">2512 </span></a> 
<a name="l2513"><span class="ln">2513 </span></a>Out-place version of :meth:`~Tensor.index_put_`. 
<a name="l2514"><span class="ln">2514 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2515"><span class="ln">2515 </span></a><span class="s3">)</span>
<a name="l2516"><span class="ln">2516 </span></a>
<a name="l2517"><span class="ln">2517 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2518"><span class="ln">2518 </span></a>    <span class="s4">&quot;index_reduce_&quot;</span><span class="s3">,</span>
<a name="l2519"><span class="ln">2519 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2520"><span class="ln">2520 </span></a>index_reduce_(dim, index, source, reduce, *, include_self=True) -&gt; Tensor 
<a name="l2521"><span class="ln">2521 </span></a> 
<a name="l2522"><span class="ln">2522 </span></a>Accumulate the elements of ``source`` into the :attr:`self` 
<a name="l2523"><span class="ln">2523 </span></a>tensor by accumulating to the indices in the order given in :attr:`index` 
<a name="l2524"><span class="ln">2524 </span></a>using the reduction given by the ``reduce`` argument. For example, if ``dim == 0``, 
<a name="l2525"><span class="ln">2525 </span></a>``index[i] == j``, ``reduce == prod`` and ``include_self == True`` then the ``i``\ th 
<a name="l2526"><span class="ln">2526 </span></a>row of ``source`` is multiplied by the ``j``\ th row of :attr:`self`. If 
<a name="l2527"><span class="ln">2527 </span></a>:obj:`include_self=&quot;True&quot;`, the values in the :attr:`self` tensor are included 
<a name="l2528"><span class="ln">2528 </span></a>in the reduction, otherwise, rows in the :attr:`self` tensor that are accumulated 
<a name="l2529"><span class="ln">2529 </span></a>to are treated as if they were filled with the reduction identites. 
<a name="l2530"><span class="ln">2530 </span></a> 
<a name="l2531"><span class="ln">2531 </span></a>The :attr:`dim`\ th dimension of ``source`` must have the same size as the 
<a name="l2532"><span class="ln">2532 </span></a>length of :attr:`index` (which must be a vector), and all other dimensions must 
<a name="l2533"><span class="ln">2533 </span></a>match :attr:`self`, or an error will be raised. 
<a name="l2534"><span class="ln">2534 </span></a> 
<a name="l2535"><span class="ln">2535 </span></a>For a 3-D tensor with :obj:`reduce=&quot;prod&quot;` and :obj:`include_self=True` the 
<a name="l2536"><span class="ln">2536 </span></a>output is given as:: 
<a name="l2537"><span class="ln">2537 </span></a> 
<a name="l2538"><span class="ln">2538 </span></a>    self[index[i], :, :] *= src[i, :, :]  # if dim == 0 
<a name="l2539"><span class="ln">2539 </span></a>    self[:, index[i], :] *= src[:, i, :]  # if dim == 1 
<a name="l2540"><span class="ln">2540 </span></a>    self[:, :, index[i]] *= src[:, :, i]  # if dim == 2 
<a name="l2541"><span class="ln">2541 </span></a> 
<a name="l2542"><span class="ln">2542 </span></a>Note: 
<a name="l2543"><span class="ln">2543 </span></a>    {forward_reproducibility_note} 
<a name="l2544"><span class="ln">2544 </span></a> 
<a name="l2545"><span class="ln">2545 </span></a>.. note:: 
<a name="l2546"><span class="ln">2546 </span></a> 
<a name="l2547"><span class="ln">2547 </span></a>    This function only supports floating point tensors. 
<a name="l2548"><span class="ln">2548 </span></a> 
<a name="l2549"><span class="ln">2549 </span></a>.. warning:: 
<a name="l2550"><span class="ln">2550 </span></a> 
<a name="l2551"><span class="ln">2551 </span></a>    This function is in beta and may change in the near future. 
<a name="l2552"><span class="ln">2552 </span></a> 
<a name="l2553"><span class="ln">2553 </span></a>Args: 
<a name="l2554"><span class="ln">2554 </span></a>    dim (int): dimension along which to index 
<a name="l2555"><span class="ln">2555 </span></a>    index (Tensor): indices of ``source`` to select from, 
<a name="l2556"><span class="ln">2556 </span></a>        should have dtype either `torch.int64` or `torch.int32` 
<a name="l2557"><span class="ln">2557 </span></a>    source (FloatTensor): the tensor containing values to accumulate 
<a name="l2558"><span class="ln">2558 </span></a>    reduce (str): the reduction operation to apply 
<a name="l2559"><span class="ln">2559 </span></a>        (:obj:`&quot;prod&quot;`, :obj:`&quot;mean&quot;`, :obj:`&quot;amax&quot;`, :obj:`&quot;amin&quot;`) 
<a name="l2560"><span class="ln">2560 </span></a> 
<a name="l2561"><span class="ln">2561 </span></a>Keyword args: 
<a name="l2562"><span class="ln">2562 </span></a>    include_self (bool): whether the elements from the ``self`` tensor are 
<a name="l2563"><span class="ln">2563 </span></a>        included in the reduction 
<a name="l2564"><span class="ln">2564 </span></a> 
<a name="l2565"><span class="ln">2565 </span></a>Example:: 
<a name="l2566"><span class="ln">2566 </span></a> 
<a name="l2567"><span class="ln">2567 </span></a>    &gt;&gt;&gt; x = torch.empty(5, 3).fill_(2) 
<a name="l2568"><span class="ln">2568 </span></a>    &gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=torch.float) 
<a name="l2569"><span class="ln">2569 </span></a>    &gt;&gt;&gt; index = torch.tensor([0, 4, 2, 0]) 
<a name="l2570"><span class="ln">2570 </span></a>    &gt;&gt;&gt; x.index_reduce_(0, index, t, 'prod') 
<a name="l2571"><span class="ln">2571 </span></a>    tensor([[20., 44., 72.], 
<a name="l2572"><span class="ln">2572 </span></a>            [ 2.,  2.,  2.], 
<a name="l2573"><span class="ln">2573 </span></a>            [14., 16., 18.], 
<a name="l2574"><span class="ln">2574 </span></a>            [ 2.,  2.,  2.], 
<a name="l2575"><span class="ln">2575 </span></a>            [ 8., 10., 12.]]) 
<a name="l2576"><span class="ln">2576 </span></a>    &gt;&gt;&gt; x = torch.empty(5, 3).fill_(2) 
<a name="l2577"><span class="ln">2577 </span></a>    &gt;&gt;&gt; x.index_reduce_(0, index, t, 'prod', include_self=False) 
<a name="l2578"><span class="ln">2578 </span></a>    tensor([[10., 22., 36.], 
<a name="l2579"><span class="ln">2579 </span></a>            [ 2.,  2.,  2.], 
<a name="l2580"><span class="ln">2580 </span></a>            [ 7.,  8.,  9.], 
<a name="l2581"><span class="ln">2581 </span></a>            [ 2.,  2.,  2.], 
<a name="l2582"><span class="ln">2582 </span></a>            [ 4.,  5.,  6.]]) 
<a name="l2583"><span class="ln">2583 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reproducibility_notes</span><span class="s3">),</span>
<a name="l2584"><span class="ln">2584 </span></a><span class="s3">)</span>
<a name="l2585"><span class="ln">2585 </span></a>
<a name="l2586"><span class="ln">2586 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2587"><span class="ln">2587 </span></a>    <span class="s4">&quot;index_select&quot;</span><span class="s3">,</span>
<a name="l2588"><span class="ln">2588 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2589"><span class="ln">2589 </span></a>index_select(dim, index) -&gt; Tensor 
<a name="l2590"><span class="ln">2590 </span></a> 
<a name="l2591"><span class="ln">2591 </span></a>See :func:`torch.index_select` 
<a name="l2592"><span class="ln">2592 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2593"><span class="ln">2593 </span></a><span class="s3">)</span>
<a name="l2594"><span class="ln">2594 </span></a>
<a name="l2595"><span class="ln">2595 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2596"><span class="ln">2596 </span></a>    <span class="s4">&quot;sparse_mask&quot;</span><span class="s3">,</span>
<a name="l2597"><span class="ln">2597 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2598"><span class="ln">2598 </span></a>sparse_mask(mask) -&gt; Tensor 
<a name="l2599"><span class="ln">2599 </span></a> 
<a name="l2600"><span class="ln">2600 </span></a>Returns a new :ref:`sparse tensor &lt;sparse-docs&gt;` with values from a 
<a name="l2601"><span class="ln">2601 </span></a>strided tensor :attr:`self` filtered by the indices of the sparse 
<a name="l2602"><span class="ln">2602 </span></a>tensor :attr:`mask`. The values of :attr:`mask` sparse tensor are 
<a name="l2603"><span class="ln">2603 </span></a>ignored. :attr:`self` and :attr:`mask` tensors must have the same 
<a name="l2604"><span class="ln">2604 </span></a>shape. 
<a name="l2605"><span class="ln">2605 </span></a> 
<a name="l2606"><span class="ln">2606 </span></a>.. note:: 
<a name="l2607"><span class="ln">2607 </span></a> 
<a name="l2608"><span class="ln">2608 </span></a>  The returned sparse tensor might contain duplicate values if :attr:`mask` 
<a name="l2609"><span class="ln">2609 </span></a>  is not coalesced. It is therefore advisable to pass ``mask.coalesce()`` 
<a name="l2610"><span class="ln">2610 </span></a>  if such behavior is not desired. 
<a name="l2611"><span class="ln">2611 </span></a> 
<a name="l2612"><span class="ln">2612 </span></a>.. note:: 
<a name="l2613"><span class="ln">2613 </span></a> 
<a name="l2614"><span class="ln">2614 </span></a>  The returned sparse tensor has the same indices as the sparse tensor 
<a name="l2615"><span class="ln">2615 </span></a>  :attr:`mask`, even when the corresponding values in :attr:`self` are 
<a name="l2616"><span class="ln">2616 </span></a>  zeros. 
<a name="l2617"><span class="ln">2617 </span></a> 
<a name="l2618"><span class="ln">2618 </span></a>Args: 
<a name="l2619"><span class="ln">2619 </span></a>    mask (Tensor): a sparse tensor whose indices are used as a filter 
<a name="l2620"><span class="ln">2620 </span></a> 
<a name="l2621"><span class="ln">2621 </span></a>Example:: 
<a name="l2622"><span class="ln">2622 </span></a> 
<a name="l2623"><span class="ln">2623 </span></a>    &gt;&gt;&gt; nse = 5 
<a name="l2624"><span class="ln">2624 </span></a>    &gt;&gt;&gt; dims = (5, 5, 2, 2) 
<a name="l2625"><span class="ln">2625 </span></a>    &gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nse,)), 
<a name="l2626"><span class="ln">2626 </span></a>    ...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse) 
<a name="l2627"><span class="ln">2627 </span></a>    &gt;&gt;&gt; V = torch.randn(nse, dims[2], dims[3]) 
<a name="l2628"><span class="ln">2628 </span></a>    &gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, dims).coalesce() 
<a name="l2629"><span class="ln">2629 </span></a>    &gt;&gt;&gt; D = torch.randn(dims) 
<a name="l2630"><span class="ln">2630 </span></a>    &gt;&gt;&gt; D.sparse_mask(S) 
<a name="l2631"><span class="ln">2631 </span></a>    tensor(indices=tensor([[0, 0, 0, 2], 
<a name="l2632"><span class="ln">2632 </span></a>                           [0, 1, 4, 3]]), 
<a name="l2633"><span class="ln">2633 </span></a>           values=tensor([[[ 1.6550,  0.2397], 
<a name="l2634"><span class="ln">2634 </span></a>                           [-0.1611, -0.0779]], 
<a name="l2635"><span class="ln">2635 </span></a> 
<a name="l2636"><span class="ln">2636 </span></a>                          [[ 0.2326, -1.0558], 
<a name="l2637"><span class="ln">2637 </span></a>                           [ 1.4711,  1.9678]], 
<a name="l2638"><span class="ln">2638 </span></a> 
<a name="l2639"><span class="ln">2639 </span></a>                          [[-0.5138, -0.0411], 
<a name="l2640"><span class="ln">2640 </span></a>                           [ 1.9417,  0.5158]], 
<a name="l2641"><span class="ln">2641 </span></a> 
<a name="l2642"><span class="ln">2642 </span></a>                          [[ 0.0793,  0.0036], 
<a name="l2643"><span class="ln">2643 </span></a>                           [-0.2569, -0.1055]]]), 
<a name="l2644"><span class="ln">2644 </span></a>           size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo) 
<a name="l2645"><span class="ln">2645 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2646"><span class="ln">2646 </span></a><span class="s3">)</span>
<a name="l2647"><span class="ln">2647 </span></a>
<a name="l2648"><span class="ln">2648 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2649"><span class="ln">2649 </span></a>    <span class="s4">&quot;inverse&quot;</span><span class="s3">,</span>
<a name="l2650"><span class="ln">2650 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2651"><span class="ln">2651 </span></a>inverse() -&gt; Tensor 
<a name="l2652"><span class="ln">2652 </span></a> 
<a name="l2653"><span class="ln">2653 </span></a>See :func:`torch.inverse` 
<a name="l2654"><span class="ln">2654 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2655"><span class="ln">2655 </span></a><span class="s3">)</span>
<a name="l2656"><span class="ln">2656 </span></a>
<a name="l2657"><span class="ln">2657 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2658"><span class="ln">2658 </span></a>    <span class="s4">&quot;isnan&quot;</span><span class="s3">,</span>
<a name="l2659"><span class="ln">2659 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2660"><span class="ln">2660 </span></a>isnan() -&gt; Tensor 
<a name="l2661"><span class="ln">2661 </span></a> 
<a name="l2662"><span class="ln">2662 </span></a>See :func:`torch.isnan` 
<a name="l2663"><span class="ln">2663 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2664"><span class="ln">2664 </span></a><span class="s3">)</span>
<a name="l2665"><span class="ln">2665 </span></a>
<a name="l2666"><span class="ln">2666 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2667"><span class="ln">2667 </span></a>    <span class="s4">&quot;isinf&quot;</span><span class="s3">,</span>
<a name="l2668"><span class="ln">2668 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2669"><span class="ln">2669 </span></a>isinf() -&gt; Tensor 
<a name="l2670"><span class="ln">2670 </span></a> 
<a name="l2671"><span class="ln">2671 </span></a>See :func:`torch.isinf` 
<a name="l2672"><span class="ln">2672 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2673"><span class="ln">2673 </span></a><span class="s3">)</span>
<a name="l2674"><span class="ln">2674 </span></a>
<a name="l2675"><span class="ln">2675 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2676"><span class="ln">2676 </span></a>    <span class="s4">&quot;isposinf&quot;</span><span class="s3">,</span>
<a name="l2677"><span class="ln">2677 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2678"><span class="ln">2678 </span></a>isposinf() -&gt; Tensor 
<a name="l2679"><span class="ln">2679 </span></a> 
<a name="l2680"><span class="ln">2680 </span></a>See :func:`torch.isposinf` 
<a name="l2681"><span class="ln">2681 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2682"><span class="ln">2682 </span></a><span class="s3">)</span>
<a name="l2683"><span class="ln">2683 </span></a>
<a name="l2684"><span class="ln">2684 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2685"><span class="ln">2685 </span></a>    <span class="s4">&quot;isneginf&quot;</span><span class="s3">,</span>
<a name="l2686"><span class="ln">2686 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2687"><span class="ln">2687 </span></a>isneginf() -&gt; Tensor 
<a name="l2688"><span class="ln">2688 </span></a> 
<a name="l2689"><span class="ln">2689 </span></a>See :func:`torch.isneginf` 
<a name="l2690"><span class="ln">2690 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2691"><span class="ln">2691 </span></a><span class="s3">)</span>
<a name="l2692"><span class="ln">2692 </span></a>
<a name="l2693"><span class="ln">2693 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2694"><span class="ln">2694 </span></a>    <span class="s4">&quot;isfinite&quot;</span><span class="s3">,</span>
<a name="l2695"><span class="ln">2695 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2696"><span class="ln">2696 </span></a>isfinite() -&gt; Tensor 
<a name="l2697"><span class="ln">2697 </span></a> 
<a name="l2698"><span class="ln">2698 </span></a>See :func:`torch.isfinite` 
<a name="l2699"><span class="ln">2699 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2700"><span class="ln">2700 </span></a><span class="s3">)</span>
<a name="l2701"><span class="ln">2701 </span></a>
<a name="l2702"><span class="ln">2702 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2703"><span class="ln">2703 </span></a>    <span class="s4">&quot;isclose&quot;</span><span class="s3">,</span>
<a name="l2704"><span class="ln">2704 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2705"><span class="ln">2705 </span></a>isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -&gt; Tensor 
<a name="l2706"><span class="ln">2706 </span></a> 
<a name="l2707"><span class="ln">2707 </span></a>See :func:`torch.isclose` 
<a name="l2708"><span class="ln">2708 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2709"><span class="ln">2709 </span></a><span class="s3">)</span>
<a name="l2710"><span class="ln">2710 </span></a>
<a name="l2711"><span class="ln">2711 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2712"><span class="ln">2712 </span></a>    <span class="s4">&quot;isreal&quot;</span><span class="s3">,</span>
<a name="l2713"><span class="ln">2713 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2714"><span class="ln">2714 </span></a>isreal() -&gt; Tensor 
<a name="l2715"><span class="ln">2715 </span></a> 
<a name="l2716"><span class="ln">2716 </span></a>See :func:`torch.isreal` 
<a name="l2717"><span class="ln">2717 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2718"><span class="ln">2718 </span></a><span class="s3">)</span>
<a name="l2719"><span class="ln">2719 </span></a>
<a name="l2720"><span class="ln">2720 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2721"><span class="ln">2721 </span></a>    <span class="s4">&quot;is_coalesced&quot;</span><span class="s3">,</span>
<a name="l2722"><span class="ln">2722 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2723"><span class="ln">2723 </span></a>is_coalesced() -&gt; bool 
<a name="l2724"><span class="ln">2724 </span></a> 
<a name="l2725"><span class="ln">2725 </span></a>Returns ``True`` if :attr:`self` is a :ref:`sparse COO tensor 
<a name="l2726"><span class="ln">2726 </span></a>&lt;sparse-coo-docs&gt;` that is coalesced, ``False`` otherwise. 
<a name="l2727"><span class="ln">2727 </span></a> 
<a name="l2728"><span class="ln">2728 </span></a>.. warning:: 
<a name="l2729"><span class="ln">2729 </span></a>  Throws an error if :attr:`self` is not a sparse COO tensor. 
<a name="l2730"><span class="ln">2730 </span></a> 
<a name="l2731"><span class="ln">2731 </span></a>See :meth:`coalesce` and :ref:`uncoalesced tensors &lt;sparse-uncoalesced-coo-docs&gt;`. 
<a name="l2732"><span class="ln">2732 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2733"><span class="ln">2733 </span></a><span class="s3">)</span>
<a name="l2734"><span class="ln">2734 </span></a>
<a name="l2735"><span class="ln">2735 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2736"><span class="ln">2736 </span></a>    <span class="s4">&quot;is_contiguous&quot;</span><span class="s3">,</span>
<a name="l2737"><span class="ln">2737 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2738"><span class="ln">2738 </span></a>is_contiguous(memory_format=torch.contiguous_format) -&gt; bool 
<a name="l2739"><span class="ln">2739 </span></a> 
<a name="l2740"><span class="ln">2740 </span></a>Returns True if :attr:`self` tensor is contiguous in memory in the order specified 
<a name="l2741"><span class="ln">2741 </span></a>by memory format. 
<a name="l2742"><span class="ln">2742 </span></a> 
<a name="l2743"><span class="ln">2743 </span></a>Args: 
<a name="l2744"><span class="ln">2744 </span></a>    memory_format (:class:`torch.memory_format`, optional): Specifies memory allocation 
<a name="l2745"><span class="ln">2745 </span></a>        order. Default: ``torch.contiguous_format``. 
<a name="l2746"><span class="ln">2746 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2747"><span class="ln">2747 </span></a><span class="s3">)</span>
<a name="l2748"><span class="ln">2748 </span></a>
<a name="l2749"><span class="ln">2749 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2750"><span class="ln">2750 </span></a>    <span class="s4">&quot;is_pinned&quot;</span><span class="s3">,</span>
<a name="l2751"><span class="ln">2751 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2752"><span class="ln">2752 </span></a>Returns true if this tensor resides in pinned memory. 
<a name="l2753"><span class="ln">2753 </span></a>By default, the device pinned memory on will be the current :ref:`accelerator&lt;accelerators&gt;`. 
<a name="l2754"><span class="ln">2754 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2755"><span class="ln">2755 </span></a><span class="s3">)</span>
<a name="l2756"><span class="ln">2756 </span></a>
<a name="l2757"><span class="ln">2757 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2758"><span class="ln">2758 </span></a>    <span class="s4">&quot;is_floating_point&quot;</span><span class="s3">,</span>
<a name="l2759"><span class="ln">2759 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2760"><span class="ln">2760 </span></a>is_floating_point() -&gt; bool 
<a name="l2761"><span class="ln">2761 </span></a> 
<a name="l2762"><span class="ln">2762 </span></a>Returns True if the data type of :attr:`self` is a floating point data type. 
<a name="l2763"><span class="ln">2763 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2764"><span class="ln">2764 </span></a><span class="s3">)</span>
<a name="l2765"><span class="ln">2765 </span></a>
<a name="l2766"><span class="ln">2766 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2767"><span class="ln">2767 </span></a>    <span class="s4">&quot;is_complex&quot;</span><span class="s3">,</span>
<a name="l2768"><span class="ln">2768 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2769"><span class="ln">2769 </span></a>is_complex() -&gt; bool 
<a name="l2770"><span class="ln">2770 </span></a> 
<a name="l2771"><span class="ln">2771 </span></a>Returns True if the data type of :attr:`self` is a complex data type. 
<a name="l2772"><span class="ln">2772 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2773"><span class="ln">2773 </span></a><span class="s3">)</span>
<a name="l2774"><span class="ln">2774 </span></a>
<a name="l2775"><span class="ln">2775 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2776"><span class="ln">2776 </span></a>    <span class="s4">&quot;is_inference&quot;</span><span class="s3">,</span>
<a name="l2777"><span class="ln">2777 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2778"><span class="ln">2778 </span></a>is_inference() -&gt; bool 
<a name="l2779"><span class="ln">2779 </span></a> 
<a name="l2780"><span class="ln">2780 </span></a>See :func:`torch.is_inference` 
<a name="l2781"><span class="ln">2781 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2782"><span class="ln">2782 </span></a><span class="s3">)</span>
<a name="l2783"><span class="ln">2783 </span></a>
<a name="l2784"><span class="ln">2784 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2785"><span class="ln">2785 </span></a>    <span class="s4">&quot;is_conj&quot;</span><span class="s3">,</span>
<a name="l2786"><span class="ln">2786 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2787"><span class="ln">2787 </span></a>is_conj() -&gt; bool 
<a name="l2788"><span class="ln">2788 </span></a> 
<a name="l2789"><span class="ln">2789 </span></a>Returns True if the conjugate bit of :attr:`self` is set to true. 
<a name="l2790"><span class="ln">2790 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2791"><span class="ln">2791 </span></a><span class="s3">)</span>
<a name="l2792"><span class="ln">2792 </span></a>
<a name="l2793"><span class="ln">2793 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2794"><span class="ln">2794 </span></a>    <span class="s4">&quot;is_neg&quot;</span><span class="s3">,</span>
<a name="l2795"><span class="ln">2795 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2796"><span class="ln">2796 </span></a>is_neg() -&gt; bool 
<a name="l2797"><span class="ln">2797 </span></a> 
<a name="l2798"><span class="ln">2798 </span></a>Returns True if the negative bit of :attr:`self` is set to true. 
<a name="l2799"><span class="ln">2799 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2800"><span class="ln">2800 </span></a><span class="s3">)</span>
<a name="l2801"><span class="ln">2801 </span></a>
<a name="l2802"><span class="ln">2802 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2803"><span class="ln">2803 </span></a>    <span class="s4">&quot;is_signed&quot;</span><span class="s3">,</span>
<a name="l2804"><span class="ln">2804 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2805"><span class="ln">2805 </span></a>is_signed() -&gt; bool 
<a name="l2806"><span class="ln">2806 </span></a> 
<a name="l2807"><span class="ln">2807 </span></a>Returns True if the data type of :attr:`self` is a signed data type. 
<a name="l2808"><span class="ln">2808 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2809"><span class="ln">2809 </span></a><span class="s3">)</span>
<a name="l2810"><span class="ln">2810 </span></a>
<a name="l2811"><span class="ln">2811 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2812"><span class="ln">2812 </span></a>    <span class="s4">&quot;is_set_to&quot;</span><span class="s3">,</span>
<a name="l2813"><span class="ln">2813 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2814"><span class="ln">2814 </span></a>is_set_to(tensor) -&gt; bool 
<a name="l2815"><span class="ln">2815 </span></a> 
<a name="l2816"><span class="ln">2816 </span></a>Returns True if both tensors are pointing to the exact same memory (same 
<a name="l2817"><span class="ln">2817 </span></a>storage, offset, size and stride). 
<a name="l2818"><span class="ln">2818 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2819"><span class="ln">2819 </span></a><span class="s3">)</span>
<a name="l2820"><span class="ln">2820 </span></a>
<a name="l2821"><span class="ln">2821 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2822"><span class="ln">2822 </span></a>    <span class="s4">&quot;item&quot;</span><span class="s3">,</span>
<a name="l2823"><span class="ln">2823 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2824"><span class="ln">2824 </span></a>item() -&gt; number 
<a name="l2825"><span class="ln">2825 </span></a> 
<a name="l2826"><span class="ln">2826 </span></a>Returns the value of this tensor as a standard Python number. This only works 
<a name="l2827"><span class="ln">2827 </span></a>for tensors with one element. For other cases, see :meth:`~Tensor.tolist`. 
<a name="l2828"><span class="ln">2828 </span></a> 
<a name="l2829"><span class="ln">2829 </span></a>This operation is not differentiable. 
<a name="l2830"><span class="ln">2830 </span></a> 
<a name="l2831"><span class="ln">2831 </span></a>Example:: 
<a name="l2832"><span class="ln">2832 </span></a> 
<a name="l2833"><span class="ln">2833 </span></a>    &gt;&gt;&gt; x = torch.tensor([1.0]) 
<a name="l2834"><span class="ln">2834 </span></a>    &gt;&gt;&gt; x.item() 
<a name="l2835"><span class="ln">2835 </span></a>    1.0 
<a name="l2836"><span class="ln">2836 </span></a> 
<a name="l2837"><span class="ln">2837 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2838"><span class="ln">2838 </span></a><span class="s3">)</span>
<a name="l2839"><span class="ln">2839 </span></a>
<a name="l2840"><span class="ln">2840 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2841"><span class="ln">2841 </span></a>    <span class="s4">&quot;kron&quot;</span><span class="s3">,</span>
<a name="l2842"><span class="ln">2842 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2843"><span class="ln">2843 </span></a>kron(other) -&gt; Tensor 
<a name="l2844"><span class="ln">2844 </span></a> 
<a name="l2845"><span class="ln">2845 </span></a>See :func:`torch.kron` 
<a name="l2846"><span class="ln">2846 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2847"><span class="ln">2847 </span></a><span class="s3">)</span>
<a name="l2848"><span class="ln">2848 </span></a>
<a name="l2849"><span class="ln">2849 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2850"><span class="ln">2850 </span></a>    <span class="s4">&quot;kthvalue&quot;</span><span class="s3">,</span>
<a name="l2851"><span class="ln">2851 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2852"><span class="ln">2852 </span></a>kthvalue(k, dim=None, keepdim=False) -&gt; (Tensor, LongTensor) 
<a name="l2853"><span class="ln">2853 </span></a> 
<a name="l2854"><span class="ln">2854 </span></a>See :func:`torch.kthvalue` 
<a name="l2855"><span class="ln">2855 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2856"><span class="ln">2856 </span></a><span class="s3">)</span>
<a name="l2857"><span class="ln">2857 </span></a>
<a name="l2858"><span class="ln">2858 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2859"><span class="ln">2859 </span></a>    <span class="s4">&quot;ldexp&quot;</span><span class="s3">,</span>
<a name="l2860"><span class="ln">2860 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2861"><span class="ln">2861 </span></a>ldexp(other) -&gt; Tensor 
<a name="l2862"><span class="ln">2862 </span></a> 
<a name="l2863"><span class="ln">2863 </span></a>See :func:`torch.ldexp` 
<a name="l2864"><span class="ln">2864 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2865"><span class="ln">2865 </span></a><span class="s3">)</span>
<a name="l2866"><span class="ln">2866 </span></a>
<a name="l2867"><span class="ln">2867 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2868"><span class="ln">2868 </span></a>    <span class="s4">&quot;ldexp_&quot;</span><span class="s3">,</span>
<a name="l2869"><span class="ln">2869 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2870"><span class="ln">2870 </span></a>ldexp_(other) -&gt; Tensor 
<a name="l2871"><span class="ln">2871 </span></a> 
<a name="l2872"><span class="ln">2872 </span></a>In-place version of :meth:`~Tensor.ldexp` 
<a name="l2873"><span class="ln">2873 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2874"><span class="ln">2874 </span></a><span class="s3">)</span>
<a name="l2875"><span class="ln">2875 </span></a>
<a name="l2876"><span class="ln">2876 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2877"><span class="ln">2877 </span></a>    <span class="s4">&quot;lcm&quot;</span><span class="s3">,</span>
<a name="l2878"><span class="ln">2878 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2879"><span class="ln">2879 </span></a>lcm(other) -&gt; Tensor 
<a name="l2880"><span class="ln">2880 </span></a> 
<a name="l2881"><span class="ln">2881 </span></a>See :func:`torch.lcm` 
<a name="l2882"><span class="ln">2882 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2883"><span class="ln">2883 </span></a><span class="s3">)</span>
<a name="l2884"><span class="ln">2884 </span></a>
<a name="l2885"><span class="ln">2885 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2886"><span class="ln">2886 </span></a>    <span class="s4">&quot;lcm_&quot;</span><span class="s3">,</span>
<a name="l2887"><span class="ln">2887 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2888"><span class="ln">2888 </span></a>lcm_(other) -&gt; Tensor 
<a name="l2889"><span class="ln">2889 </span></a> 
<a name="l2890"><span class="ln">2890 </span></a>In-place version of :meth:`~Tensor.lcm` 
<a name="l2891"><span class="ln">2891 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2892"><span class="ln">2892 </span></a><span class="s3">)</span>
<a name="l2893"><span class="ln">2893 </span></a>
<a name="l2894"><span class="ln">2894 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2895"><span class="ln">2895 </span></a>    <span class="s4">&quot;le&quot;</span><span class="s3">,</span>
<a name="l2896"><span class="ln">2896 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2897"><span class="ln">2897 </span></a>le(other) -&gt; Tensor 
<a name="l2898"><span class="ln">2898 </span></a> 
<a name="l2899"><span class="ln">2899 </span></a>See :func:`torch.le`. 
<a name="l2900"><span class="ln">2900 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2901"><span class="ln">2901 </span></a><span class="s3">)</span>
<a name="l2902"><span class="ln">2902 </span></a>
<a name="l2903"><span class="ln">2903 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2904"><span class="ln">2904 </span></a>    <span class="s4">&quot;le_&quot;</span><span class="s3">,</span>
<a name="l2905"><span class="ln">2905 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2906"><span class="ln">2906 </span></a>le_(other) -&gt; Tensor 
<a name="l2907"><span class="ln">2907 </span></a> 
<a name="l2908"><span class="ln">2908 </span></a>In-place version of :meth:`~Tensor.le`. 
<a name="l2909"><span class="ln">2909 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2910"><span class="ln">2910 </span></a><span class="s3">)</span>
<a name="l2911"><span class="ln">2911 </span></a>
<a name="l2912"><span class="ln">2912 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2913"><span class="ln">2913 </span></a>    <span class="s4">&quot;less_equal&quot;</span><span class="s3">,</span>
<a name="l2914"><span class="ln">2914 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2915"><span class="ln">2915 </span></a>less_equal(other) -&gt; Tensor 
<a name="l2916"><span class="ln">2916 </span></a> 
<a name="l2917"><span class="ln">2917 </span></a>See :func:`torch.less_equal`. 
<a name="l2918"><span class="ln">2918 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2919"><span class="ln">2919 </span></a><span class="s3">)</span>
<a name="l2920"><span class="ln">2920 </span></a>
<a name="l2921"><span class="ln">2921 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2922"><span class="ln">2922 </span></a>    <span class="s4">&quot;less_equal_&quot;</span><span class="s3">,</span>
<a name="l2923"><span class="ln">2923 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2924"><span class="ln">2924 </span></a>less_equal_(other) -&gt; Tensor 
<a name="l2925"><span class="ln">2925 </span></a> 
<a name="l2926"><span class="ln">2926 </span></a>In-place version of :meth:`~Tensor.less_equal`. 
<a name="l2927"><span class="ln">2927 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2928"><span class="ln">2928 </span></a><span class="s3">)</span>
<a name="l2929"><span class="ln">2929 </span></a>
<a name="l2930"><span class="ln">2930 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2931"><span class="ln">2931 </span></a>    <span class="s4">&quot;lerp&quot;</span><span class="s3">,</span>
<a name="l2932"><span class="ln">2932 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2933"><span class="ln">2933 </span></a>lerp(end, weight) -&gt; Tensor 
<a name="l2934"><span class="ln">2934 </span></a> 
<a name="l2935"><span class="ln">2935 </span></a>See :func:`torch.lerp` 
<a name="l2936"><span class="ln">2936 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2937"><span class="ln">2937 </span></a><span class="s3">)</span>
<a name="l2938"><span class="ln">2938 </span></a>
<a name="l2939"><span class="ln">2939 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2940"><span class="ln">2940 </span></a>    <span class="s4">&quot;lerp_&quot;</span><span class="s3">,</span>
<a name="l2941"><span class="ln">2941 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2942"><span class="ln">2942 </span></a>lerp_(end, weight) -&gt; Tensor 
<a name="l2943"><span class="ln">2943 </span></a> 
<a name="l2944"><span class="ln">2944 </span></a>In-place version of :meth:`~Tensor.lerp` 
<a name="l2945"><span class="ln">2945 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2946"><span class="ln">2946 </span></a><span class="s3">)</span>
<a name="l2947"><span class="ln">2947 </span></a>
<a name="l2948"><span class="ln">2948 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2949"><span class="ln">2949 </span></a>    <span class="s4">&quot;lgamma&quot;</span><span class="s3">,</span>
<a name="l2950"><span class="ln">2950 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2951"><span class="ln">2951 </span></a>lgamma() -&gt; Tensor 
<a name="l2952"><span class="ln">2952 </span></a> 
<a name="l2953"><span class="ln">2953 </span></a>See :func:`torch.lgamma` 
<a name="l2954"><span class="ln">2954 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2955"><span class="ln">2955 </span></a><span class="s3">)</span>
<a name="l2956"><span class="ln">2956 </span></a>
<a name="l2957"><span class="ln">2957 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2958"><span class="ln">2958 </span></a>    <span class="s4">&quot;lgamma_&quot;</span><span class="s3">,</span>
<a name="l2959"><span class="ln">2959 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2960"><span class="ln">2960 </span></a>lgamma_() -&gt; Tensor 
<a name="l2961"><span class="ln">2961 </span></a> 
<a name="l2962"><span class="ln">2962 </span></a>In-place version of :meth:`~Tensor.lgamma` 
<a name="l2963"><span class="ln">2963 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2964"><span class="ln">2964 </span></a><span class="s3">)</span>
<a name="l2965"><span class="ln">2965 </span></a>
<a name="l2966"><span class="ln">2966 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2967"><span class="ln">2967 </span></a>    <span class="s4">&quot;log&quot;</span><span class="s3">,</span>
<a name="l2968"><span class="ln">2968 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2969"><span class="ln">2969 </span></a>log() -&gt; Tensor 
<a name="l2970"><span class="ln">2970 </span></a> 
<a name="l2971"><span class="ln">2971 </span></a>See :func:`torch.log` 
<a name="l2972"><span class="ln">2972 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2973"><span class="ln">2973 </span></a><span class="s3">)</span>
<a name="l2974"><span class="ln">2974 </span></a>
<a name="l2975"><span class="ln">2975 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2976"><span class="ln">2976 </span></a>    <span class="s4">&quot;log_&quot;</span><span class="s3">,</span>
<a name="l2977"><span class="ln">2977 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2978"><span class="ln">2978 </span></a>log_() -&gt; Tensor 
<a name="l2979"><span class="ln">2979 </span></a> 
<a name="l2980"><span class="ln">2980 </span></a>In-place version of :meth:`~Tensor.log` 
<a name="l2981"><span class="ln">2981 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2982"><span class="ln">2982 </span></a><span class="s3">)</span>
<a name="l2983"><span class="ln">2983 </span></a>
<a name="l2984"><span class="ln">2984 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2985"><span class="ln">2985 </span></a>    <span class="s4">&quot;log10&quot;</span><span class="s3">,</span>
<a name="l2986"><span class="ln">2986 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2987"><span class="ln">2987 </span></a>log10() -&gt; Tensor 
<a name="l2988"><span class="ln">2988 </span></a> 
<a name="l2989"><span class="ln">2989 </span></a>See :func:`torch.log10` 
<a name="l2990"><span class="ln">2990 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l2991"><span class="ln">2991 </span></a><span class="s3">)</span>
<a name="l2992"><span class="ln">2992 </span></a>
<a name="l2993"><span class="ln">2993 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l2994"><span class="ln">2994 </span></a>    <span class="s4">&quot;log10_&quot;</span><span class="s3">,</span>
<a name="l2995"><span class="ln">2995 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l2996"><span class="ln">2996 </span></a>log10_() -&gt; Tensor 
<a name="l2997"><span class="ln">2997 </span></a> 
<a name="l2998"><span class="ln">2998 </span></a>In-place version of :meth:`~Tensor.log10` 
<a name="l2999"><span class="ln">2999 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3000"><span class="ln">3000 </span></a><span class="s3">)</span>
<a name="l3001"><span class="ln">3001 </span></a>
<a name="l3002"><span class="ln">3002 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3003"><span class="ln">3003 </span></a>    <span class="s4">&quot;log1p&quot;</span><span class="s3">,</span>
<a name="l3004"><span class="ln">3004 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3005"><span class="ln">3005 </span></a>log1p() -&gt; Tensor 
<a name="l3006"><span class="ln">3006 </span></a> 
<a name="l3007"><span class="ln">3007 </span></a>See :func:`torch.log1p` 
<a name="l3008"><span class="ln">3008 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3009"><span class="ln">3009 </span></a><span class="s3">)</span>
<a name="l3010"><span class="ln">3010 </span></a>
<a name="l3011"><span class="ln">3011 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3012"><span class="ln">3012 </span></a>    <span class="s4">&quot;log1p_&quot;</span><span class="s3">,</span>
<a name="l3013"><span class="ln">3013 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3014"><span class="ln">3014 </span></a>log1p_() -&gt; Tensor 
<a name="l3015"><span class="ln">3015 </span></a> 
<a name="l3016"><span class="ln">3016 </span></a>In-place version of :meth:`~Tensor.log1p` 
<a name="l3017"><span class="ln">3017 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3018"><span class="ln">3018 </span></a><span class="s3">)</span>
<a name="l3019"><span class="ln">3019 </span></a>
<a name="l3020"><span class="ln">3020 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3021"><span class="ln">3021 </span></a>    <span class="s4">&quot;log2&quot;</span><span class="s3">,</span>
<a name="l3022"><span class="ln">3022 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3023"><span class="ln">3023 </span></a>log2() -&gt; Tensor 
<a name="l3024"><span class="ln">3024 </span></a> 
<a name="l3025"><span class="ln">3025 </span></a>See :func:`torch.log2` 
<a name="l3026"><span class="ln">3026 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3027"><span class="ln">3027 </span></a><span class="s3">)</span>
<a name="l3028"><span class="ln">3028 </span></a>
<a name="l3029"><span class="ln">3029 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3030"><span class="ln">3030 </span></a>    <span class="s4">&quot;log2_&quot;</span><span class="s3">,</span>
<a name="l3031"><span class="ln">3031 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3032"><span class="ln">3032 </span></a>log2_() -&gt; Tensor 
<a name="l3033"><span class="ln">3033 </span></a> 
<a name="l3034"><span class="ln">3034 </span></a>In-place version of :meth:`~Tensor.log2` 
<a name="l3035"><span class="ln">3035 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3036"><span class="ln">3036 </span></a><span class="s3">)</span>
<a name="l3037"><span class="ln">3037 </span></a>
<a name="l3038"><span class="ln">3038 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3039"><span class="ln">3039 </span></a>    <span class="s4">&quot;logaddexp&quot;</span><span class="s3">,</span>
<a name="l3040"><span class="ln">3040 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3041"><span class="ln">3041 </span></a>logaddexp(other) -&gt; Tensor 
<a name="l3042"><span class="ln">3042 </span></a> 
<a name="l3043"><span class="ln">3043 </span></a>See :func:`torch.logaddexp` 
<a name="l3044"><span class="ln">3044 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3045"><span class="ln">3045 </span></a><span class="s3">)</span>
<a name="l3046"><span class="ln">3046 </span></a>
<a name="l3047"><span class="ln">3047 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3048"><span class="ln">3048 </span></a>    <span class="s4">&quot;logaddexp2&quot;</span><span class="s3">,</span>
<a name="l3049"><span class="ln">3049 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3050"><span class="ln">3050 </span></a>logaddexp2(other) -&gt; Tensor 
<a name="l3051"><span class="ln">3051 </span></a> 
<a name="l3052"><span class="ln">3052 </span></a>See :func:`torch.logaddexp2` 
<a name="l3053"><span class="ln">3053 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3054"><span class="ln">3054 </span></a><span class="s3">)</span>
<a name="l3055"><span class="ln">3055 </span></a>
<a name="l3056"><span class="ln">3056 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3057"><span class="ln">3057 </span></a>    <span class="s4">&quot;log_normal_&quot;</span><span class="s3">,</span>
<a name="l3058"><span class="ln">3058 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3059"><span class="ln">3059 </span></a>log_normal_(mean=1, std=2, *, generator=None) 
<a name="l3060"><span class="ln">3060 </span></a> 
<a name="l3061"><span class="ln">3061 </span></a>Fills :attr:`self` tensor with numbers samples from the log-normal distribution 
<a name="l3062"><span class="ln">3062 </span></a>parameterized by the given mean :math:`\mu` and standard deviation 
<a name="l3063"><span class="ln">3063 </span></a>:math:`\sigma`. Note that :attr:`mean` and :attr:`std` are the mean and 
<a name="l3064"><span class="ln">3064 </span></a>standard deviation of the underlying normal distribution, and not of the 
<a name="l3065"><span class="ln">3065 </span></a>returned distribution: 
<a name="l3066"><span class="ln">3066 </span></a> 
<a name="l3067"><span class="ln">3067 </span></a>.. math:: 
<a name="l3068"><span class="ln">3068 </span></a> 
<a name="l3069"><span class="ln">3069 </span></a>    f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}} 
<a name="l3070"><span class="ln">3070 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3071"><span class="ln">3071 </span></a><span class="s3">)</span>
<a name="l3072"><span class="ln">3072 </span></a>
<a name="l3073"><span class="ln">3073 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3074"><span class="ln">3074 </span></a>    <span class="s4">&quot;logsumexp&quot;</span><span class="s3">,</span>
<a name="l3075"><span class="ln">3075 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3076"><span class="ln">3076 </span></a>logsumexp(dim, keepdim=False) -&gt; Tensor 
<a name="l3077"><span class="ln">3077 </span></a> 
<a name="l3078"><span class="ln">3078 </span></a>See :func:`torch.logsumexp` 
<a name="l3079"><span class="ln">3079 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3080"><span class="ln">3080 </span></a><span class="s3">)</span>
<a name="l3081"><span class="ln">3081 </span></a>
<a name="l3082"><span class="ln">3082 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3083"><span class="ln">3083 </span></a>    <span class="s4">&quot;lt&quot;</span><span class="s3">,</span>
<a name="l3084"><span class="ln">3084 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3085"><span class="ln">3085 </span></a>lt(other) -&gt; Tensor 
<a name="l3086"><span class="ln">3086 </span></a> 
<a name="l3087"><span class="ln">3087 </span></a>See :func:`torch.lt`. 
<a name="l3088"><span class="ln">3088 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3089"><span class="ln">3089 </span></a><span class="s3">)</span>
<a name="l3090"><span class="ln">3090 </span></a>
<a name="l3091"><span class="ln">3091 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3092"><span class="ln">3092 </span></a>    <span class="s4">&quot;lt_&quot;</span><span class="s3">,</span>
<a name="l3093"><span class="ln">3093 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3094"><span class="ln">3094 </span></a>lt_(other) -&gt; Tensor 
<a name="l3095"><span class="ln">3095 </span></a> 
<a name="l3096"><span class="ln">3096 </span></a>In-place version of :meth:`~Tensor.lt`. 
<a name="l3097"><span class="ln">3097 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3098"><span class="ln">3098 </span></a><span class="s3">)</span>
<a name="l3099"><span class="ln">3099 </span></a>
<a name="l3100"><span class="ln">3100 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3101"><span class="ln">3101 </span></a>    <span class="s4">&quot;less&quot;</span><span class="s3">,</span>
<a name="l3102"><span class="ln">3102 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3103"><span class="ln">3103 </span></a>lt(other) -&gt; Tensor 
<a name="l3104"><span class="ln">3104 </span></a> 
<a name="l3105"><span class="ln">3105 </span></a>See :func:`torch.less`. 
<a name="l3106"><span class="ln">3106 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3107"><span class="ln">3107 </span></a><span class="s3">)</span>
<a name="l3108"><span class="ln">3108 </span></a>
<a name="l3109"><span class="ln">3109 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3110"><span class="ln">3110 </span></a>    <span class="s4">&quot;less_&quot;</span><span class="s3">,</span>
<a name="l3111"><span class="ln">3111 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3112"><span class="ln">3112 </span></a>less_(other) -&gt; Tensor 
<a name="l3113"><span class="ln">3113 </span></a> 
<a name="l3114"><span class="ln">3114 </span></a>In-place version of :meth:`~Tensor.less`. 
<a name="l3115"><span class="ln">3115 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3116"><span class="ln">3116 </span></a><span class="s3">)</span>
<a name="l3117"><span class="ln">3117 </span></a>
<a name="l3118"><span class="ln">3118 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3119"><span class="ln">3119 </span></a>    <span class="s4">&quot;lu_solve&quot;</span><span class="s3">,</span>
<a name="l3120"><span class="ln">3120 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3121"><span class="ln">3121 </span></a>lu_solve(LU_data, LU_pivots) -&gt; Tensor 
<a name="l3122"><span class="ln">3122 </span></a> 
<a name="l3123"><span class="ln">3123 </span></a>See :func:`torch.lu_solve` 
<a name="l3124"><span class="ln">3124 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3125"><span class="ln">3125 </span></a><span class="s3">)</span>
<a name="l3126"><span class="ln">3126 </span></a>
<a name="l3127"><span class="ln">3127 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3128"><span class="ln">3128 </span></a>    <span class="s4">&quot;map_&quot;</span><span class="s3">,</span>
<a name="l3129"><span class="ln">3129 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3130"><span class="ln">3130 </span></a>map_(tensor, callable) 
<a name="l3131"><span class="ln">3131 </span></a> 
<a name="l3132"><span class="ln">3132 </span></a>Applies :attr:`callable` for each element in :attr:`self` tensor and the given 
<a name="l3133"><span class="ln">3133 </span></a>:attr:`tensor` and stores the results in :attr:`self` tensor. :attr:`self` tensor and 
<a name="l3134"><span class="ln">3134 </span></a>the given :attr:`tensor` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l3135"><span class="ln">3135 </span></a> 
<a name="l3136"><span class="ln">3136 </span></a>The :attr:`callable` should have the signature:: 
<a name="l3137"><span class="ln">3137 </span></a> 
<a name="l3138"><span class="ln">3138 </span></a>    def callable(a, b) -&gt; number 
<a name="l3139"><span class="ln">3139 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3140"><span class="ln">3140 </span></a><span class="s3">)</span>
<a name="l3141"><span class="ln">3141 </span></a>
<a name="l3142"><span class="ln">3142 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3143"><span class="ln">3143 </span></a>    <span class="s4">&quot;masked_scatter_&quot;</span><span class="s3">,</span>
<a name="l3144"><span class="ln">3144 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3145"><span class="ln">3145 </span></a>masked_scatter_(mask, source) 
<a name="l3146"><span class="ln">3146 </span></a> 
<a name="l3147"><span class="ln">3147 </span></a>Copies elements from :attr:`source` into :attr:`self` tensor at positions where 
<a name="l3148"><span class="ln">3148 </span></a>the :attr:`mask` is True. Elements from :attr:`source` are copied into :attr:`self` 
<a name="l3149"><span class="ln">3149 </span></a>starting at position 0 of :attr:`source` and continuing in order one-by-one for each 
<a name="l3150"><span class="ln">3150 </span></a>occurrence of :attr:`mask` being True. 
<a name="l3151"><span class="ln">3151 </span></a>The shape of :attr:`mask` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;` 
<a name="l3152"><span class="ln">3152 </span></a>with the shape of the underlying tensor. The :attr:`source` should have at least 
<a name="l3153"><span class="ln">3153 </span></a>as many elements as the number of ones in :attr:`mask`. 
<a name="l3154"><span class="ln">3154 </span></a> 
<a name="l3155"><span class="ln">3155 </span></a>Args: 
<a name="l3156"><span class="ln">3156 </span></a>    mask (BoolTensor): the boolean mask 
<a name="l3157"><span class="ln">3157 </span></a>    source (Tensor): the tensor to copy from 
<a name="l3158"><span class="ln">3158 </span></a> 
<a name="l3159"><span class="ln">3159 </span></a>.. note:: 
<a name="l3160"><span class="ln">3160 </span></a> 
<a name="l3161"><span class="ln">3161 </span></a>    The :attr:`mask` operates on the :attr:`self` tensor, not on the given 
<a name="l3162"><span class="ln">3162 </span></a>    :attr:`source` tensor. 
<a name="l3163"><span class="ln">3163 </span></a> 
<a name="l3164"><span class="ln">3164 </span></a>Example: 
<a name="l3165"><span class="ln">3165 </span></a> 
<a name="l3166"><span class="ln">3166 </span></a>    &gt;&gt;&gt; self = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]) 
<a name="l3167"><span class="ln">3167 </span></a>    &gt;&gt;&gt; mask = torch.tensor( 
<a name="l3168"><span class="ln">3168 </span></a>    ...     [[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], 
<a name="l3169"><span class="ln">3169 </span></a>    ...     dtype=torch.bool, 
<a name="l3170"><span class="ln">3170 </span></a>    ... ) 
<a name="l3171"><span class="ln">3171 </span></a>    &gt;&gt;&gt; source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) 
<a name="l3172"><span class="ln">3172 </span></a>    &gt;&gt;&gt; self.masked_scatter_(mask, source) 
<a name="l3173"><span class="ln">3173 </span></a>    tensor([[0, 0, 0, 0, 1], 
<a name="l3174"><span class="ln">3174 </span></a>            [2, 3, 0, 4, 5]]) 
<a name="l3175"><span class="ln">3175 </span></a> 
<a name="l3176"><span class="ln">3176 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3177"><span class="ln">3177 </span></a><span class="s3">)</span>
<a name="l3178"><span class="ln">3178 </span></a>
<a name="l3179"><span class="ln">3179 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3180"><span class="ln">3180 </span></a>    <span class="s4">&quot;masked_fill_&quot;</span><span class="s3">,</span>
<a name="l3181"><span class="ln">3181 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3182"><span class="ln">3182 </span></a>masked_fill_(mask, value) 
<a name="l3183"><span class="ln">3183 </span></a> 
<a name="l3184"><span class="ln">3184 </span></a>Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is 
<a name="l3185"><span class="ln">3185 </span></a>True. The shape of :attr:`mask` must be 
<a name="l3186"><span class="ln">3186 </span></a>:ref:`broadcastable &lt;broadcasting-semantics&gt;` with the shape of the underlying 
<a name="l3187"><span class="ln">3187 </span></a>tensor. 
<a name="l3188"><span class="ln">3188 </span></a> 
<a name="l3189"><span class="ln">3189 </span></a>Args: 
<a name="l3190"><span class="ln">3190 </span></a>    mask (BoolTensor): the boolean mask 
<a name="l3191"><span class="ln">3191 </span></a>    value (float): the value to fill in with 
<a name="l3192"><span class="ln">3192 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3193"><span class="ln">3193 </span></a><span class="s3">)</span>
<a name="l3194"><span class="ln">3194 </span></a>
<a name="l3195"><span class="ln">3195 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3196"><span class="ln">3196 </span></a>    <span class="s4">&quot;masked_select&quot;</span><span class="s3">,</span>
<a name="l3197"><span class="ln">3197 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3198"><span class="ln">3198 </span></a>masked_select(mask) -&gt; Tensor 
<a name="l3199"><span class="ln">3199 </span></a> 
<a name="l3200"><span class="ln">3200 </span></a>See :func:`torch.masked_select` 
<a name="l3201"><span class="ln">3201 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3202"><span class="ln">3202 </span></a><span class="s3">)</span>
<a name="l3203"><span class="ln">3203 </span></a>
<a name="l3204"><span class="ln">3204 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3205"><span class="ln">3205 </span></a>    <span class="s4">&quot;matrix_power&quot;</span><span class="s3">,</span>
<a name="l3206"><span class="ln">3206 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3207"><span class="ln">3207 </span></a>matrix_power(n) -&gt; Tensor 
<a name="l3208"><span class="ln">3208 </span></a> 
<a name="l3209"><span class="ln">3209 </span></a>.. note:: :meth:`~Tensor.matrix_power` is deprecated, use :func:`torch.linalg.matrix_power` instead. 
<a name="l3210"><span class="ln">3210 </span></a> 
<a name="l3211"><span class="ln">3211 </span></a>Alias for :func:`torch.linalg.matrix_power` 
<a name="l3212"><span class="ln">3212 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3213"><span class="ln">3213 </span></a><span class="s3">)</span>
<a name="l3214"><span class="ln">3214 </span></a>
<a name="l3215"><span class="ln">3215 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3216"><span class="ln">3216 </span></a>    <span class="s4">&quot;matrix_exp&quot;</span><span class="s3">,</span>
<a name="l3217"><span class="ln">3217 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3218"><span class="ln">3218 </span></a>matrix_exp() -&gt; Tensor 
<a name="l3219"><span class="ln">3219 </span></a> 
<a name="l3220"><span class="ln">3220 </span></a>See :func:`torch.matrix_exp` 
<a name="l3221"><span class="ln">3221 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3222"><span class="ln">3222 </span></a><span class="s3">)</span>
<a name="l3223"><span class="ln">3223 </span></a>
<a name="l3224"><span class="ln">3224 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3225"><span class="ln">3225 </span></a>    <span class="s4">&quot;max&quot;</span><span class="s3">,</span>
<a name="l3226"><span class="ln">3226 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3227"><span class="ln">3227 </span></a>max(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor) 
<a name="l3228"><span class="ln">3228 </span></a> 
<a name="l3229"><span class="ln">3229 </span></a>See :func:`torch.max` 
<a name="l3230"><span class="ln">3230 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3231"><span class="ln">3231 </span></a><span class="s3">)</span>
<a name="l3232"><span class="ln">3232 </span></a>
<a name="l3233"><span class="ln">3233 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3234"><span class="ln">3234 </span></a>    <span class="s4">&quot;amax&quot;</span><span class="s3">,</span>
<a name="l3235"><span class="ln">3235 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3236"><span class="ln">3236 </span></a>amax(dim=None, keepdim=False) -&gt; Tensor 
<a name="l3237"><span class="ln">3237 </span></a> 
<a name="l3238"><span class="ln">3238 </span></a>See :func:`torch.amax` 
<a name="l3239"><span class="ln">3239 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3240"><span class="ln">3240 </span></a><span class="s3">)</span>
<a name="l3241"><span class="ln">3241 </span></a>
<a name="l3242"><span class="ln">3242 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3243"><span class="ln">3243 </span></a>    <span class="s4">&quot;maximum&quot;</span><span class="s3">,</span>
<a name="l3244"><span class="ln">3244 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3245"><span class="ln">3245 </span></a>maximum(other) -&gt; Tensor 
<a name="l3246"><span class="ln">3246 </span></a> 
<a name="l3247"><span class="ln">3247 </span></a>See :func:`torch.maximum` 
<a name="l3248"><span class="ln">3248 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3249"><span class="ln">3249 </span></a><span class="s3">)</span>
<a name="l3250"><span class="ln">3250 </span></a>
<a name="l3251"><span class="ln">3251 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3252"><span class="ln">3252 </span></a>    <span class="s4">&quot;fmax&quot;</span><span class="s3">,</span>
<a name="l3253"><span class="ln">3253 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3254"><span class="ln">3254 </span></a>fmax(other) -&gt; Tensor 
<a name="l3255"><span class="ln">3255 </span></a> 
<a name="l3256"><span class="ln">3256 </span></a>See :func:`torch.fmax` 
<a name="l3257"><span class="ln">3257 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3258"><span class="ln">3258 </span></a><span class="s3">)</span>
<a name="l3259"><span class="ln">3259 </span></a>
<a name="l3260"><span class="ln">3260 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3261"><span class="ln">3261 </span></a>    <span class="s4">&quot;argmax&quot;</span><span class="s3">,</span>
<a name="l3262"><span class="ln">3262 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3263"><span class="ln">3263 </span></a>argmax(dim=None, keepdim=False) -&gt; LongTensor 
<a name="l3264"><span class="ln">3264 </span></a> 
<a name="l3265"><span class="ln">3265 </span></a>See :func:`torch.argmax` 
<a name="l3266"><span class="ln">3266 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3267"><span class="ln">3267 </span></a><span class="s3">)</span>
<a name="l3268"><span class="ln">3268 </span></a>
<a name="l3269"><span class="ln">3269 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3270"><span class="ln">3270 </span></a>    <span class="s4">&quot;argwhere&quot;</span><span class="s3">,</span>
<a name="l3271"><span class="ln">3271 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3272"><span class="ln">3272 </span></a>argwhere() -&gt; Tensor 
<a name="l3273"><span class="ln">3273 </span></a> 
<a name="l3274"><span class="ln">3274 </span></a>See :func:`torch.argwhere` 
<a name="l3275"><span class="ln">3275 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3276"><span class="ln">3276 </span></a><span class="s3">)</span>
<a name="l3277"><span class="ln">3277 </span></a>
<a name="l3278"><span class="ln">3278 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3279"><span class="ln">3279 </span></a>    <span class="s4">&quot;mean&quot;</span><span class="s3">,</span>
<a name="l3280"><span class="ln">3280 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3281"><span class="ln">3281 </span></a>mean(dim=None, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l3282"><span class="ln">3282 </span></a> 
<a name="l3283"><span class="ln">3283 </span></a>See :func:`torch.mean` 
<a name="l3284"><span class="ln">3284 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3285"><span class="ln">3285 </span></a><span class="s3">)</span>
<a name="l3286"><span class="ln">3286 </span></a>
<a name="l3287"><span class="ln">3287 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3288"><span class="ln">3288 </span></a>    <span class="s4">&quot;nanmean&quot;</span><span class="s3">,</span>
<a name="l3289"><span class="ln">3289 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3290"><span class="ln">3290 </span></a>nanmean(dim=None, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l3291"><span class="ln">3291 </span></a> 
<a name="l3292"><span class="ln">3292 </span></a>See :func:`torch.nanmean` 
<a name="l3293"><span class="ln">3293 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3294"><span class="ln">3294 </span></a><span class="s3">)</span>
<a name="l3295"><span class="ln">3295 </span></a>
<a name="l3296"><span class="ln">3296 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3297"><span class="ln">3297 </span></a>    <span class="s4">&quot;median&quot;</span><span class="s3">,</span>
<a name="l3298"><span class="ln">3298 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3299"><span class="ln">3299 </span></a>median(dim=None, keepdim=False) -&gt; (Tensor, LongTensor) 
<a name="l3300"><span class="ln">3300 </span></a> 
<a name="l3301"><span class="ln">3301 </span></a>See :func:`torch.median` 
<a name="l3302"><span class="ln">3302 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3303"><span class="ln">3303 </span></a><span class="s3">)</span>
<a name="l3304"><span class="ln">3304 </span></a>
<a name="l3305"><span class="ln">3305 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3306"><span class="ln">3306 </span></a>    <span class="s4">&quot;nanmedian&quot;</span><span class="s3">,</span>
<a name="l3307"><span class="ln">3307 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3308"><span class="ln">3308 </span></a>nanmedian(dim=None, keepdim=False) -&gt; (Tensor, LongTensor) 
<a name="l3309"><span class="ln">3309 </span></a> 
<a name="l3310"><span class="ln">3310 </span></a>See :func:`torch.nanmedian` 
<a name="l3311"><span class="ln">3311 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3312"><span class="ln">3312 </span></a><span class="s3">)</span>
<a name="l3313"><span class="ln">3313 </span></a>
<a name="l3314"><span class="ln">3314 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3315"><span class="ln">3315 </span></a>    <span class="s4">&quot;min&quot;</span><span class="s3">,</span>
<a name="l3316"><span class="ln">3316 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3317"><span class="ln">3317 </span></a>min(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor) 
<a name="l3318"><span class="ln">3318 </span></a> 
<a name="l3319"><span class="ln">3319 </span></a>See :func:`torch.min` 
<a name="l3320"><span class="ln">3320 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3321"><span class="ln">3321 </span></a><span class="s3">)</span>
<a name="l3322"><span class="ln">3322 </span></a>
<a name="l3323"><span class="ln">3323 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3324"><span class="ln">3324 </span></a>    <span class="s4">&quot;amin&quot;</span><span class="s3">,</span>
<a name="l3325"><span class="ln">3325 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3326"><span class="ln">3326 </span></a>amin(dim=None, keepdim=False) -&gt; Tensor 
<a name="l3327"><span class="ln">3327 </span></a> 
<a name="l3328"><span class="ln">3328 </span></a>See :func:`torch.amin` 
<a name="l3329"><span class="ln">3329 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3330"><span class="ln">3330 </span></a><span class="s3">)</span>
<a name="l3331"><span class="ln">3331 </span></a>
<a name="l3332"><span class="ln">3332 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3333"><span class="ln">3333 </span></a>    <span class="s4">&quot;minimum&quot;</span><span class="s3">,</span>
<a name="l3334"><span class="ln">3334 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3335"><span class="ln">3335 </span></a>minimum(other) -&gt; Tensor 
<a name="l3336"><span class="ln">3336 </span></a> 
<a name="l3337"><span class="ln">3337 </span></a>See :func:`torch.minimum` 
<a name="l3338"><span class="ln">3338 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3339"><span class="ln">3339 </span></a><span class="s3">)</span>
<a name="l3340"><span class="ln">3340 </span></a>
<a name="l3341"><span class="ln">3341 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3342"><span class="ln">3342 </span></a>    <span class="s4">&quot;aminmax&quot;</span><span class="s3">,</span>
<a name="l3343"><span class="ln">3343 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3344"><span class="ln">3344 </span></a>aminmax(*, dim=None, keepdim=False) -&gt; (Tensor min, Tensor max) 
<a name="l3345"><span class="ln">3345 </span></a> 
<a name="l3346"><span class="ln">3346 </span></a>See :func:`torch.aminmax` 
<a name="l3347"><span class="ln">3347 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3348"><span class="ln">3348 </span></a><span class="s3">)</span>
<a name="l3349"><span class="ln">3349 </span></a>
<a name="l3350"><span class="ln">3350 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3351"><span class="ln">3351 </span></a>    <span class="s4">&quot;fmin&quot;</span><span class="s3">,</span>
<a name="l3352"><span class="ln">3352 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3353"><span class="ln">3353 </span></a>fmin(other) -&gt; Tensor 
<a name="l3354"><span class="ln">3354 </span></a> 
<a name="l3355"><span class="ln">3355 </span></a>See :func:`torch.fmin` 
<a name="l3356"><span class="ln">3356 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3357"><span class="ln">3357 </span></a><span class="s3">)</span>
<a name="l3358"><span class="ln">3358 </span></a>
<a name="l3359"><span class="ln">3359 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3360"><span class="ln">3360 </span></a>    <span class="s4">&quot;argmin&quot;</span><span class="s3">,</span>
<a name="l3361"><span class="ln">3361 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3362"><span class="ln">3362 </span></a>argmin(dim=None, keepdim=False) -&gt; LongTensor 
<a name="l3363"><span class="ln">3363 </span></a> 
<a name="l3364"><span class="ln">3364 </span></a>See :func:`torch.argmin` 
<a name="l3365"><span class="ln">3365 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3366"><span class="ln">3366 </span></a><span class="s3">)</span>
<a name="l3367"><span class="ln">3367 </span></a>
<a name="l3368"><span class="ln">3368 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3369"><span class="ln">3369 </span></a>    <span class="s4">&quot;mm&quot;</span><span class="s3">,</span>
<a name="l3370"><span class="ln">3370 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3371"><span class="ln">3371 </span></a>mm(mat2) -&gt; Tensor 
<a name="l3372"><span class="ln">3372 </span></a> 
<a name="l3373"><span class="ln">3373 </span></a>See :func:`torch.mm` 
<a name="l3374"><span class="ln">3374 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3375"><span class="ln">3375 </span></a><span class="s3">)</span>
<a name="l3376"><span class="ln">3376 </span></a>
<a name="l3377"><span class="ln">3377 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3378"><span class="ln">3378 </span></a>    <span class="s4">&quot;mode&quot;</span><span class="s3">,</span>
<a name="l3379"><span class="ln">3379 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3380"><span class="ln">3380 </span></a>mode(dim=None, keepdim=False) -&gt; (Tensor, LongTensor) 
<a name="l3381"><span class="ln">3381 </span></a> 
<a name="l3382"><span class="ln">3382 </span></a>See :func:`torch.mode` 
<a name="l3383"><span class="ln">3383 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3384"><span class="ln">3384 </span></a><span class="s3">)</span>
<a name="l3385"><span class="ln">3385 </span></a>
<a name="l3386"><span class="ln">3386 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3387"><span class="ln">3387 </span></a>    <span class="s4">&quot;movedim&quot;</span><span class="s3">,</span>
<a name="l3388"><span class="ln">3388 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3389"><span class="ln">3389 </span></a>movedim(source, destination) -&gt; Tensor 
<a name="l3390"><span class="ln">3390 </span></a> 
<a name="l3391"><span class="ln">3391 </span></a>See :func:`torch.movedim` 
<a name="l3392"><span class="ln">3392 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3393"><span class="ln">3393 </span></a><span class="s3">)</span>
<a name="l3394"><span class="ln">3394 </span></a>
<a name="l3395"><span class="ln">3395 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3396"><span class="ln">3396 </span></a>    <span class="s4">&quot;moveaxis&quot;</span><span class="s3">,</span>
<a name="l3397"><span class="ln">3397 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3398"><span class="ln">3398 </span></a>moveaxis(source, destination) -&gt; Tensor 
<a name="l3399"><span class="ln">3399 </span></a> 
<a name="l3400"><span class="ln">3400 </span></a>See :func:`torch.moveaxis` 
<a name="l3401"><span class="ln">3401 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3402"><span class="ln">3402 </span></a><span class="s3">)</span>
<a name="l3403"><span class="ln">3403 </span></a>
<a name="l3404"><span class="ln">3404 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3405"><span class="ln">3405 </span></a>    <span class="s4">&quot;mul&quot;</span><span class="s3">,</span>
<a name="l3406"><span class="ln">3406 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3407"><span class="ln">3407 </span></a>mul(value) -&gt; Tensor 
<a name="l3408"><span class="ln">3408 </span></a> 
<a name="l3409"><span class="ln">3409 </span></a>See :func:`torch.mul`. 
<a name="l3410"><span class="ln">3410 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3411"><span class="ln">3411 </span></a><span class="s3">)</span>
<a name="l3412"><span class="ln">3412 </span></a>
<a name="l3413"><span class="ln">3413 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3414"><span class="ln">3414 </span></a>    <span class="s4">&quot;mul_&quot;</span><span class="s3">,</span>
<a name="l3415"><span class="ln">3415 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3416"><span class="ln">3416 </span></a>mul_(value) -&gt; Tensor 
<a name="l3417"><span class="ln">3417 </span></a> 
<a name="l3418"><span class="ln">3418 </span></a>In-place version of :meth:`~Tensor.mul`. 
<a name="l3419"><span class="ln">3419 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3420"><span class="ln">3420 </span></a><span class="s3">)</span>
<a name="l3421"><span class="ln">3421 </span></a>
<a name="l3422"><span class="ln">3422 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3423"><span class="ln">3423 </span></a>    <span class="s4">&quot;multiply&quot;</span><span class="s3">,</span>
<a name="l3424"><span class="ln">3424 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3425"><span class="ln">3425 </span></a>multiply(value) -&gt; Tensor 
<a name="l3426"><span class="ln">3426 </span></a> 
<a name="l3427"><span class="ln">3427 </span></a>See :func:`torch.multiply`. 
<a name="l3428"><span class="ln">3428 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3429"><span class="ln">3429 </span></a><span class="s3">)</span>
<a name="l3430"><span class="ln">3430 </span></a>
<a name="l3431"><span class="ln">3431 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3432"><span class="ln">3432 </span></a>    <span class="s4">&quot;multiply_&quot;</span><span class="s3">,</span>
<a name="l3433"><span class="ln">3433 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3434"><span class="ln">3434 </span></a>multiply_(value) -&gt; Tensor 
<a name="l3435"><span class="ln">3435 </span></a> 
<a name="l3436"><span class="ln">3436 </span></a>In-place version of :meth:`~Tensor.multiply`. 
<a name="l3437"><span class="ln">3437 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3438"><span class="ln">3438 </span></a><span class="s3">)</span>
<a name="l3439"><span class="ln">3439 </span></a>
<a name="l3440"><span class="ln">3440 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3441"><span class="ln">3441 </span></a>    <span class="s4">&quot;multinomial&quot;</span><span class="s3">,</span>
<a name="l3442"><span class="ln">3442 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3443"><span class="ln">3443 </span></a>multinomial(num_samples, replacement=False, *, generator=None) -&gt; Tensor 
<a name="l3444"><span class="ln">3444 </span></a> 
<a name="l3445"><span class="ln">3445 </span></a>See :func:`torch.multinomial` 
<a name="l3446"><span class="ln">3446 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3447"><span class="ln">3447 </span></a><span class="s3">)</span>
<a name="l3448"><span class="ln">3448 </span></a>
<a name="l3449"><span class="ln">3449 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3450"><span class="ln">3450 </span></a>    <span class="s4">&quot;mv&quot;</span><span class="s3">,</span>
<a name="l3451"><span class="ln">3451 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3452"><span class="ln">3452 </span></a>mv(vec) -&gt; Tensor 
<a name="l3453"><span class="ln">3453 </span></a> 
<a name="l3454"><span class="ln">3454 </span></a>See :func:`torch.mv` 
<a name="l3455"><span class="ln">3455 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3456"><span class="ln">3456 </span></a><span class="s3">)</span>
<a name="l3457"><span class="ln">3457 </span></a>
<a name="l3458"><span class="ln">3458 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3459"><span class="ln">3459 </span></a>    <span class="s4">&quot;mvlgamma&quot;</span><span class="s3">,</span>
<a name="l3460"><span class="ln">3460 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3461"><span class="ln">3461 </span></a>mvlgamma(p) -&gt; Tensor 
<a name="l3462"><span class="ln">3462 </span></a> 
<a name="l3463"><span class="ln">3463 </span></a>See :func:`torch.mvlgamma` 
<a name="l3464"><span class="ln">3464 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3465"><span class="ln">3465 </span></a><span class="s3">)</span>
<a name="l3466"><span class="ln">3466 </span></a>
<a name="l3467"><span class="ln">3467 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3468"><span class="ln">3468 </span></a>    <span class="s4">&quot;mvlgamma_&quot;</span><span class="s3">,</span>
<a name="l3469"><span class="ln">3469 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3470"><span class="ln">3470 </span></a>mvlgamma_(p) -&gt; Tensor 
<a name="l3471"><span class="ln">3471 </span></a> 
<a name="l3472"><span class="ln">3472 </span></a>In-place version of :meth:`~Tensor.mvlgamma` 
<a name="l3473"><span class="ln">3473 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3474"><span class="ln">3474 </span></a><span class="s3">)</span>
<a name="l3475"><span class="ln">3475 </span></a>
<a name="l3476"><span class="ln">3476 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3477"><span class="ln">3477 </span></a>    <span class="s4">&quot;narrow&quot;</span><span class="s3">,</span>
<a name="l3478"><span class="ln">3478 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3479"><span class="ln">3479 </span></a>narrow(dimension, start, length) -&gt; Tensor 
<a name="l3480"><span class="ln">3480 </span></a> 
<a name="l3481"><span class="ln">3481 </span></a>See :func:`torch.narrow`. 
<a name="l3482"><span class="ln">3482 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3483"><span class="ln">3483 </span></a><span class="s3">)</span>
<a name="l3484"><span class="ln">3484 </span></a>
<a name="l3485"><span class="ln">3485 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3486"><span class="ln">3486 </span></a>    <span class="s4">&quot;narrow_copy&quot;</span><span class="s3">,</span>
<a name="l3487"><span class="ln">3487 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3488"><span class="ln">3488 </span></a>narrow_copy(dimension, start, length) -&gt; Tensor 
<a name="l3489"><span class="ln">3489 </span></a> 
<a name="l3490"><span class="ln">3490 </span></a>See :func:`torch.narrow_copy`. 
<a name="l3491"><span class="ln">3491 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3492"><span class="ln">3492 </span></a><span class="s3">)</span>
<a name="l3493"><span class="ln">3493 </span></a>
<a name="l3494"><span class="ln">3494 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3495"><span class="ln">3495 </span></a>    <span class="s4">&quot;ndimension&quot;</span><span class="s3">,</span>
<a name="l3496"><span class="ln">3496 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3497"><span class="ln">3497 </span></a>ndimension() -&gt; int 
<a name="l3498"><span class="ln">3498 </span></a> 
<a name="l3499"><span class="ln">3499 </span></a>Alias for :meth:`~Tensor.dim()` 
<a name="l3500"><span class="ln">3500 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3501"><span class="ln">3501 </span></a><span class="s3">)</span>
<a name="l3502"><span class="ln">3502 </span></a>
<a name="l3503"><span class="ln">3503 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3504"><span class="ln">3504 </span></a>    <span class="s4">&quot;nan_to_num&quot;</span><span class="s3">,</span>
<a name="l3505"><span class="ln">3505 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3506"><span class="ln">3506 </span></a>nan_to_num(nan=0.0, posinf=None, neginf=None) -&gt; Tensor 
<a name="l3507"><span class="ln">3507 </span></a> 
<a name="l3508"><span class="ln">3508 </span></a>See :func:`torch.nan_to_num`. 
<a name="l3509"><span class="ln">3509 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3510"><span class="ln">3510 </span></a><span class="s3">)</span>
<a name="l3511"><span class="ln">3511 </span></a>
<a name="l3512"><span class="ln">3512 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3513"><span class="ln">3513 </span></a>    <span class="s4">&quot;nan_to_num_&quot;</span><span class="s3">,</span>
<a name="l3514"><span class="ln">3514 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3515"><span class="ln">3515 </span></a>nan_to_num_(nan=0.0, posinf=None, neginf=None) -&gt; Tensor 
<a name="l3516"><span class="ln">3516 </span></a> 
<a name="l3517"><span class="ln">3517 </span></a>In-place version of :meth:`~Tensor.nan_to_num`. 
<a name="l3518"><span class="ln">3518 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3519"><span class="ln">3519 </span></a><span class="s3">)</span>
<a name="l3520"><span class="ln">3520 </span></a>
<a name="l3521"><span class="ln">3521 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3522"><span class="ln">3522 </span></a>    <span class="s4">&quot;ne&quot;</span><span class="s3">,</span>
<a name="l3523"><span class="ln">3523 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3524"><span class="ln">3524 </span></a>ne(other) -&gt; Tensor 
<a name="l3525"><span class="ln">3525 </span></a> 
<a name="l3526"><span class="ln">3526 </span></a>See :func:`torch.ne`. 
<a name="l3527"><span class="ln">3527 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3528"><span class="ln">3528 </span></a><span class="s3">)</span>
<a name="l3529"><span class="ln">3529 </span></a>
<a name="l3530"><span class="ln">3530 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3531"><span class="ln">3531 </span></a>    <span class="s4">&quot;ne_&quot;</span><span class="s3">,</span>
<a name="l3532"><span class="ln">3532 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3533"><span class="ln">3533 </span></a>ne_(other) -&gt; Tensor 
<a name="l3534"><span class="ln">3534 </span></a> 
<a name="l3535"><span class="ln">3535 </span></a>In-place version of :meth:`~Tensor.ne`. 
<a name="l3536"><span class="ln">3536 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3537"><span class="ln">3537 </span></a><span class="s3">)</span>
<a name="l3538"><span class="ln">3538 </span></a>
<a name="l3539"><span class="ln">3539 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3540"><span class="ln">3540 </span></a>    <span class="s4">&quot;not_equal&quot;</span><span class="s3">,</span>
<a name="l3541"><span class="ln">3541 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3542"><span class="ln">3542 </span></a>not_equal(other) -&gt; Tensor 
<a name="l3543"><span class="ln">3543 </span></a> 
<a name="l3544"><span class="ln">3544 </span></a>See :func:`torch.not_equal`. 
<a name="l3545"><span class="ln">3545 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3546"><span class="ln">3546 </span></a><span class="s3">)</span>
<a name="l3547"><span class="ln">3547 </span></a>
<a name="l3548"><span class="ln">3548 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3549"><span class="ln">3549 </span></a>    <span class="s4">&quot;not_equal_&quot;</span><span class="s3">,</span>
<a name="l3550"><span class="ln">3550 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3551"><span class="ln">3551 </span></a>not_equal_(other) -&gt; Tensor 
<a name="l3552"><span class="ln">3552 </span></a> 
<a name="l3553"><span class="ln">3553 </span></a>In-place version of :meth:`~Tensor.not_equal`. 
<a name="l3554"><span class="ln">3554 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3555"><span class="ln">3555 </span></a><span class="s3">)</span>
<a name="l3556"><span class="ln">3556 </span></a>
<a name="l3557"><span class="ln">3557 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3558"><span class="ln">3558 </span></a>    <span class="s4">&quot;neg&quot;</span><span class="s3">,</span>
<a name="l3559"><span class="ln">3559 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3560"><span class="ln">3560 </span></a>neg() -&gt; Tensor 
<a name="l3561"><span class="ln">3561 </span></a> 
<a name="l3562"><span class="ln">3562 </span></a>See :func:`torch.neg` 
<a name="l3563"><span class="ln">3563 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3564"><span class="ln">3564 </span></a><span class="s3">)</span>
<a name="l3565"><span class="ln">3565 </span></a>
<a name="l3566"><span class="ln">3566 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3567"><span class="ln">3567 </span></a>    <span class="s4">&quot;negative&quot;</span><span class="s3">,</span>
<a name="l3568"><span class="ln">3568 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3569"><span class="ln">3569 </span></a>negative() -&gt; Tensor 
<a name="l3570"><span class="ln">3570 </span></a> 
<a name="l3571"><span class="ln">3571 </span></a>See :func:`torch.negative` 
<a name="l3572"><span class="ln">3572 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3573"><span class="ln">3573 </span></a><span class="s3">)</span>
<a name="l3574"><span class="ln">3574 </span></a>
<a name="l3575"><span class="ln">3575 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3576"><span class="ln">3576 </span></a>    <span class="s4">&quot;neg_&quot;</span><span class="s3">,</span>
<a name="l3577"><span class="ln">3577 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3578"><span class="ln">3578 </span></a>neg_() -&gt; Tensor 
<a name="l3579"><span class="ln">3579 </span></a> 
<a name="l3580"><span class="ln">3580 </span></a>In-place version of :meth:`~Tensor.neg` 
<a name="l3581"><span class="ln">3581 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3582"><span class="ln">3582 </span></a><span class="s3">)</span>
<a name="l3583"><span class="ln">3583 </span></a>
<a name="l3584"><span class="ln">3584 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3585"><span class="ln">3585 </span></a>    <span class="s4">&quot;negative_&quot;</span><span class="s3">,</span>
<a name="l3586"><span class="ln">3586 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3587"><span class="ln">3587 </span></a>negative_() -&gt; Tensor 
<a name="l3588"><span class="ln">3588 </span></a> 
<a name="l3589"><span class="ln">3589 </span></a>In-place version of :meth:`~Tensor.negative` 
<a name="l3590"><span class="ln">3590 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3591"><span class="ln">3591 </span></a><span class="s3">)</span>
<a name="l3592"><span class="ln">3592 </span></a>
<a name="l3593"><span class="ln">3593 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3594"><span class="ln">3594 </span></a>    <span class="s4">&quot;nelement&quot;</span><span class="s3">,</span>
<a name="l3595"><span class="ln">3595 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3596"><span class="ln">3596 </span></a>nelement() -&gt; int 
<a name="l3597"><span class="ln">3597 </span></a> 
<a name="l3598"><span class="ln">3598 </span></a>Alias for :meth:`~Tensor.numel` 
<a name="l3599"><span class="ln">3599 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3600"><span class="ln">3600 </span></a><span class="s3">)</span>
<a name="l3601"><span class="ln">3601 </span></a>
<a name="l3602"><span class="ln">3602 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3603"><span class="ln">3603 </span></a>    <span class="s4">&quot;nextafter&quot;</span><span class="s3">,</span>
<a name="l3604"><span class="ln">3604 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3605"><span class="ln">3605 </span></a>nextafter(other) -&gt; Tensor 
<a name="l3606"><span class="ln">3606 </span></a>See :func:`torch.nextafter` 
<a name="l3607"><span class="ln">3607 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3608"><span class="ln">3608 </span></a><span class="s3">)</span>
<a name="l3609"><span class="ln">3609 </span></a>
<a name="l3610"><span class="ln">3610 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3611"><span class="ln">3611 </span></a>    <span class="s4">&quot;nextafter_&quot;</span><span class="s3">,</span>
<a name="l3612"><span class="ln">3612 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3613"><span class="ln">3613 </span></a>nextafter_(other) -&gt; Tensor 
<a name="l3614"><span class="ln">3614 </span></a>In-place version of :meth:`~Tensor.nextafter` 
<a name="l3615"><span class="ln">3615 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3616"><span class="ln">3616 </span></a><span class="s3">)</span>
<a name="l3617"><span class="ln">3617 </span></a>
<a name="l3618"><span class="ln">3618 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3619"><span class="ln">3619 </span></a>    <span class="s4">&quot;nonzero&quot;</span><span class="s3">,</span>
<a name="l3620"><span class="ln">3620 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3621"><span class="ln">3621 </span></a>nonzero() -&gt; LongTensor 
<a name="l3622"><span class="ln">3622 </span></a> 
<a name="l3623"><span class="ln">3623 </span></a>See :func:`torch.nonzero` 
<a name="l3624"><span class="ln">3624 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3625"><span class="ln">3625 </span></a><span class="s3">)</span>
<a name="l3626"><span class="ln">3626 </span></a>
<a name="l3627"><span class="ln">3627 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3628"><span class="ln">3628 </span></a>    <span class="s4">&quot;nonzero_static&quot;</span><span class="s3">,</span>
<a name="l3629"><span class="ln">3629 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3630"><span class="ln">3630 </span></a>nonzero_static(input, *, size, fill_value=-1) -&gt; Tensor 
<a name="l3631"><span class="ln">3631 </span></a> 
<a name="l3632"><span class="ln">3632 </span></a>Returns a 2-D tensor where each row is the index for a non-zero value. 
<a name="l3633"><span class="ln">3633 </span></a>The returned Tensor has the same `torch.dtype` as `torch.nonzero()`. 
<a name="l3634"><span class="ln">3634 </span></a> 
<a name="l3635"><span class="ln">3635 </span></a>Args: 
<a name="l3636"><span class="ln">3636 </span></a>    input (Tensor): the input tensor to count non-zero elements. 
<a name="l3637"><span class="ln">3637 </span></a> 
<a name="l3638"><span class="ln">3638 </span></a>Keyword args: 
<a name="l3639"><span class="ln">3639 </span></a>    size (int): the size of non-zero elements expected to be included in the out 
<a name="l3640"><span class="ln">3640 </span></a>        tensor. Pad the out tensor with `fill_value` if the `size` is larger 
<a name="l3641"><span class="ln">3641 </span></a>        than total number of non-zero elements, truncate out tensor if `size` 
<a name="l3642"><span class="ln">3642 </span></a>        is smaller. The size must be a non-negative integer. 
<a name="l3643"><span class="ln">3643 </span></a>    fill_value (int): the value to fill the output tensor with when `size` is larger 
<a name="l3644"><span class="ln">3644 </span></a>        than the total number of non-zero elements. Default is `-1` to represent 
<a name="l3645"><span class="ln">3645 </span></a>        invalid index. 
<a name="l3646"><span class="ln">3646 </span></a> 
<a name="l3647"><span class="ln">3647 </span></a>Example: 
<a name="l3648"><span class="ln">3648 </span></a> 
<a name="l3649"><span class="ln">3649 </span></a>    # Example 1: Padding 
<a name="l3650"><span class="ln">3650 </span></a>    &gt;&gt;&gt; input_tensor = torch.tensor([[1, 0], [3, 2]]) 
<a name="l3651"><span class="ln">3651 </span></a>    &gt;&gt;&gt; static_size = 4 
<a name="l3652"><span class="ln">3652 </span></a>    &gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size=static_size) 
<a name="l3653"><span class="ln">3653 </span></a>    tensor([[  0,   0], 
<a name="l3654"><span class="ln">3654 </span></a>            [  1,   0], 
<a name="l3655"><span class="ln">3655 </span></a>            [  1,   1], 
<a name="l3656"><span class="ln">3656 </span></a>            [  -1, -1]], dtype=torch.int64) 
<a name="l3657"><span class="ln">3657 </span></a> 
<a name="l3658"><span class="ln">3658 </span></a>    # Example 2: Truncating 
<a name="l3659"><span class="ln">3659 </span></a>    &gt;&gt;&gt; input_tensor = torch.tensor([[1, 0], [3, 2]]) 
<a name="l3660"><span class="ln">3660 </span></a>    &gt;&gt;&gt; static_size = 2 
<a name="l3661"><span class="ln">3661 </span></a>    &gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size=static_size) 
<a name="l3662"><span class="ln">3662 </span></a>    tensor([[  0,   0], 
<a name="l3663"><span class="ln">3663 </span></a>            [  1,   0]], dtype=torch.int64) 
<a name="l3664"><span class="ln">3664 </span></a> 
<a name="l3665"><span class="ln">3665 </span></a>    # Example 3: 0 size 
<a name="l3666"><span class="ln">3666 </span></a>    &gt;&gt;&gt; input_tensor = torch.tensor([10]) 
<a name="l3667"><span class="ln">3667 </span></a>    &gt;&gt;&gt; static_size = 0 
<a name="l3668"><span class="ln">3668 </span></a>    &gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size=static_size) 
<a name="l3669"><span class="ln">3669 </span></a>    tensor([], size=(0, 1), dtype=torch.int64) 
<a name="l3670"><span class="ln">3670 </span></a> 
<a name="l3671"><span class="ln">3671 </span></a>    # Example 4: 0 rank input 
<a name="l3672"><span class="ln">3672 </span></a>    &gt;&gt;&gt; input_tensor = torch.tensor(10) 
<a name="l3673"><span class="ln">3673 </span></a>    &gt;&gt;&gt; static_size = 2 
<a name="l3674"><span class="ln">3674 </span></a>    &gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size=static_size) 
<a name="l3675"><span class="ln">3675 </span></a>    tensor([], size=(2, 0), dtype=torch.int64) 
<a name="l3676"><span class="ln">3676 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3677"><span class="ln">3677 </span></a><span class="s3">)</span>
<a name="l3678"><span class="ln">3678 </span></a>
<a name="l3679"><span class="ln">3679 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3680"><span class="ln">3680 </span></a>    <span class="s4">&quot;norm&quot;</span><span class="s3">,</span>
<a name="l3681"><span class="ln">3681 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3682"><span class="ln">3682 </span></a>norm(p=2, dim=None, keepdim=False) -&gt; Tensor 
<a name="l3683"><span class="ln">3683 </span></a> 
<a name="l3684"><span class="ln">3684 </span></a>See :func:`torch.norm` 
<a name="l3685"><span class="ln">3685 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3686"><span class="ln">3686 </span></a><span class="s3">)</span>
<a name="l3687"><span class="ln">3687 </span></a>
<a name="l3688"><span class="ln">3688 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3689"><span class="ln">3689 </span></a>    <span class="s4">&quot;normal_&quot;</span><span class="s3">,</span>
<a name="l3690"><span class="ln">3690 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3691"><span class="ln">3691 </span></a>normal_(mean=0, std=1, *, generator=None) -&gt; Tensor 
<a name="l3692"><span class="ln">3692 </span></a> 
<a name="l3693"><span class="ln">3693 </span></a>Fills :attr:`self` tensor with elements samples from the normal distribution 
<a name="l3694"><span class="ln">3694 </span></a>parameterized by :attr:`mean` and :attr:`std`. 
<a name="l3695"><span class="ln">3695 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3696"><span class="ln">3696 </span></a><span class="s3">)</span>
<a name="l3697"><span class="ln">3697 </span></a>
<a name="l3698"><span class="ln">3698 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3699"><span class="ln">3699 </span></a>    <span class="s4">&quot;numel&quot;</span><span class="s3">,</span>
<a name="l3700"><span class="ln">3700 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3701"><span class="ln">3701 </span></a>numel() -&gt; int 
<a name="l3702"><span class="ln">3702 </span></a> 
<a name="l3703"><span class="ln">3703 </span></a>See :func:`torch.numel` 
<a name="l3704"><span class="ln">3704 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3705"><span class="ln">3705 </span></a><span class="s3">)</span>
<a name="l3706"><span class="ln">3706 </span></a>
<a name="l3707"><span class="ln">3707 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3708"><span class="ln">3708 </span></a>    <span class="s4">&quot;numpy&quot;</span><span class="s3">,</span>
<a name="l3709"><span class="ln">3709 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3710"><span class="ln">3710 </span></a>numpy(*, force=False) -&gt; numpy.ndarray 
<a name="l3711"><span class="ln">3711 </span></a> 
<a name="l3712"><span class="ln">3712 </span></a>Returns the tensor as a NumPy :class:`ndarray`. 
<a name="l3713"><span class="ln">3713 </span></a> 
<a name="l3714"><span class="ln">3714 </span></a>If :attr:`force` is ``False`` (the default), the conversion 
<a name="l3715"><span class="ln">3715 </span></a>is performed only if the tensor is on the CPU, does not require grad, 
<a name="l3716"><span class="ln">3716 </span></a>does not have its conjugate bit set, and is a dtype and layout that 
<a name="l3717"><span class="ln">3717 </span></a>NumPy supports. The returned ndarray and the tensor will share their 
<a name="l3718"><span class="ln">3718 </span></a>storage, so changes to the tensor will be reflected in the ndarray 
<a name="l3719"><span class="ln">3719 </span></a>and vice versa. 
<a name="l3720"><span class="ln">3720 </span></a> 
<a name="l3721"><span class="ln">3721 </span></a>If :attr:`force` is ``True`` this is equivalent to 
<a name="l3722"><span class="ln">3722 </span></a>calling ``t.detach().cpu().resolve_conj().resolve_neg().numpy()``. 
<a name="l3723"><span class="ln">3723 </span></a>If the tensor isn't on the CPU or the conjugate or negative bit is set, 
<a name="l3724"><span class="ln">3724 </span></a>the tensor won't share its storage with the returned ndarray. 
<a name="l3725"><span class="ln">3725 </span></a>Setting :attr:`force` to ``True`` can be a useful shorthand. 
<a name="l3726"><span class="ln">3726 </span></a> 
<a name="l3727"><span class="ln">3727 </span></a>Args: 
<a name="l3728"><span class="ln">3728 </span></a>    force (bool): if ``True``, the ndarray may be a copy of the tensor 
<a name="l3729"><span class="ln">3729 </span></a>               instead of always sharing memory, defaults to ``False``. 
<a name="l3730"><span class="ln">3730 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3731"><span class="ln">3731 </span></a><span class="s3">)</span>
<a name="l3732"><span class="ln">3732 </span></a>
<a name="l3733"><span class="ln">3733 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3734"><span class="ln">3734 </span></a>    <span class="s4">&quot;orgqr&quot;</span><span class="s3">,</span>
<a name="l3735"><span class="ln">3735 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3736"><span class="ln">3736 </span></a>orgqr(input2) -&gt; Tensor 
<a name="l3737"><span class="ln">3737 </span></a> 
<a name="l3738"><span class="ln">3738 </span></a>See :func:`torch.orgqr` 
<a name="l3739"><span class="ln">3739 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3740"><span class="ln">3740 </span></a><span class="s3">)</span>
<a name="l3741"><span class="ln">3741 </span></a>
<a name="l3742"><span class="ln">3742 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3743"><span class="ln">3743 </span></a>    <span class="s4">&quot;ormqr&quot;</span><span class="s3">,</span>
<a name="l3744"><span class="ln">3744 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3745"><span class="ln">3745 </span></a>ormqr(input2, input3, left=True, transpose=False) -&gt; Tensor 
<a name="l3746"><span class="ln">3746 </span></a> 
<a name="l3747"><span class="ln">3747 </span></a>See :func:`torch.ormqr` 
<a name="l3748"><span class="ln">3748 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3749"><span class="ln">3749 </span></a><span class="s3">)</span>
<a name="l3750"><span class="ln">3750 </span></a>
<a name="l3751"><span class="ln">3751 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3752"><span class="ln">3752 </span></a>    <span class="s4">&quot;permute&quot;</span><span class="s3">,</span>
<a name="l3753"><span class="ln">3753 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3754"><span class="ln">3754 </span></a>permute(*dims) -&gt; Tensor 
<a name="l3755"><span class="ln">3755 </span></a> 
<a name="l3756"><span class="ln">3756 </span></a>See :func:`torch.permute` 
<a name="l3757"><span class="ln">3757 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3758"><span class="ln">3758 </span></a><span class="s3">)</span>
<a name="l3759"><span class="ln">3759 </span></a>
<a name="l3760"><span class="ln">3760 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3761"><span class="ln">3761 </span></a>    <span class="s4">&quot;polygamma&quot;</span><span class="s3">,</span>
<a name="l3762"><span class="ln">3762 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3763"><span class="ln">3763 </span></a>polygamma(n) -&gt; Tensor 
<a name="l3764"><span class="ln">3764 </span></a> 
<a name="l3765"><span class="ln">3765 </span></a>See :func:`torch.polygamma` 
<a name="l3766"><span class="ln">3766 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3767"><span class="ln">3767 </span></a><span class="s3">)</span>
<a name="l3768"><span class="ln">3768 </span></a>
<a name="l3769"><span class="ln">3769 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3770"><span class="ln">3770 </span></a>    <span class="s4">&quot;polygamma_&quot;</span><span class="s3">,</span>
<a name="l3771"><span class="ln">3771 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3772"><span class="ln">3772 </span></a>polygamma_(n) -&gt; Tensor 
<a name="l3773"><span class="ln">3773 </span></a> 
<a name="l3774"><span class="ln">3774 </span></a>In-place version of :meth:`~Tensor.polygamma` 
<a name="l3775"><span class="ln">3775 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3776"><span class="ln">3776 </span></a><span class="s3">)</span>
<a name="l3777"><span class="ln">3777 </span></a>
<a name="l3778"><span class="ln">3778 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3779"><span class="ln">3779 </span></a>    <span class="s4">&quot;positive&quot;</span><span class="s3">,</span>
<a name="l3780"><span class="ln">3780 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3781"><span class="ln">3781 </span></a>positive() -&gt; Tensor 
<a name="l3782"><span class="ln">3782 </span></a> 
<a name="l3783"><span class="ln">3783 </span></a>See :func:`torch.positive` 
<a name="l3784"><span class="ln">3784 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3785"><span class="ln">3785 </span></a><span class="s3">)</span>
<a name="l3786"><span class="ln">3786 </span></a>
<a name="l3787"><span class="ln">3787 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3788"><span class="ln">3788 </span></a>    <span class="s4">&quot;pow&quot;</span><span class="s3">,</span>
<a name="l3789"><span class="ln">3789 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3790"><span class="ln">3790 </span></a>pow(exponent) -&gt; Tensor 
<a name="l3791"><span class="ln">3791 </span></a> 
<a name="l3792"><span class="ln">3792 </span></a>See :func:`torch.pow` 
<a name="l3793"><span class="ln">3793 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3794"><span class="ln">3794 </span></a><span class="s3">)</span>
<a name="l3795"><span class="ln">3795 </span></a>
<a name="l3796"><span class="ln">3796 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3797"><span class="ln">3797 </span></a>    <span class="s4">&quot;pow_&quot;</span><span class="s3">,</span>
<a name="l3798"><span class="ln">3798 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3799"><span class="ln">3799 </span></a>pow_(exponent) -&gt; Tensor 
<a name="l3800"><span class="ln">3800 </span></a> 
<a name="l3801"><span class="ln">3801 </span></a>In-place version of :meth:`~Tensor.pow` 
<a name="l3802"><span class="ln">3802 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3803"><span class="ln">3803 </span></a><span class="s3">)</span>
<a name="l3804"><span class="ln">3804 </span></a>
<a name="l3805"><span class="ln">3805 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3806"><span class="ln">3806 </span></a>    <span class="s4">&quot;float_power&quot;</span><span class="s3">,</span>
<a name="l3807"><span class="ln">3807 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3808"><span class="ln">3808 </span></a>float_power(exponent) -&gt; Tensor 
<a name="l3809"><span class="ln">3809 </span></a> 
<a name="l3810"><span class="ln">3810 </span></a>See :func:`torch.float_power` 
<a name="l3811"><span class="ln">3811 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3812"><span class="ln">3812 </span></a><span class="s3">)</span>
<a name="l3813"><span class="ln">3813 </span></a>
<a name="l3814"><span class="ln">3814 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3815"><span class="ln">3815 </span></a>    <span class="s4">&quot;float_power_&quot;</span><span class="s3">,</span>
<a name="l3816"><span class="ln">3816 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3817"><span class="ln">3817 </span></a>float_power_(exponent) -&gt; Tensor 
<a name="l3818"><span class="ln">3818 </span></a> 
<a name="l3819"><span class="ln">3819 </span></a>In-place version of :meth:`~Tensor.float_power` 
<a name="l3820"><span class="ln">3820 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3821"><span class="ln">3821 </span></a><span class="s3">)</span>
<a name="l3822"><span class="ln">3822 </span></a>
<a name="l3823"><span class="ln">3823 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3824"><span class="ln">3824 </span></a>    <span class="s4">&quot;prod&quot;</span><span class="s3">,</span>
<a name="l3825"><span class="ln">3825 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3826"><span class="ln">3826 </span></a>prod(dim=None, keepdim=False, dtype=None) -&gt; Tensor 
<a name="l3827"><span class="ln">3827 </span></a> 
<a name="l3828"><span class="ln">3828 </span></a>See :func:`torch.prod` 
<a name="l3829"><span class="ln">3829 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3830"><span class="ln">3830 </span></a><span class="s3">)</span>
<a name="l3831"><span class="ln">3831 </span></a>
<a name="l3832"><span class="ln">3832 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3833"><span class="ln">3833 </span></a>    <span class="s4">&quot;put_&quot;</span><span class="s3">,</span>
<a name="l3834"><span class="ln">3834 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3835"><span class="ln">3835 </span></a>put_(index, source, accumulate=False) -&gt; Tensor 
<a name="l3836"><span class="ln">3836 </span></a> 
<a name="l3837"><span class="ln">3837 </span></a>Copies the elements from :attr:`source` into the positions specified by 
<a name="l3838"><span class="ln">3838 </span></a>:attr:`index`. For the purpose of indexing, the :attr:`self` tensor is treated as if 
<a name="l3839"><span class="ln">3839 </span></a>it were a 1-D tensor. 
<a name="l3840"><span class="ln">3840 </span></a> 
<a name="l3841"><span class="ln">3841 </span></a>:attr:`index` and :attr:`source` need to have the same number of elements, but not necessarily 
<a name="l3842"><span class="ln">3842 </span></a>the same shape. 
<a name="l3843"><span class="ln">3843 </span></a> 
<a name="l3844"><span class="ln">3844 </span></a>If :attr:`accumulate` is ``True``, the elements in :attr:`source` are added to 
<a name="l3845"><span class="ln">3845 </span></a>:attr:`self`. If accumulate is ``False``, the behavior is undefined if :attr:`index` 
<a name="l3846"><span class="ln">3846 </span></a>contain duplicate elements. 
<a name="l3847"><span class="ln">3847 </span></a> 
<a name="l3848"><span class="ln">3848 </span></a>Args: 
<a name="l3849"><span class="ln">3849 </span></a>    index (LongTensor): the indices into self 
<a name="l3850"><span class="ln">3850 </span></a>    source (Tensor): the tensor containing values to copy from 
<a name="l3851"><span class="ln">3851 </span></a>    accumulate (bool): whether to accumulate into self 
<a name="l3852"><span class="ln">3852 </span></a> 
<a name="l3853"><span class="ln">3853 </span></a>Example:: 
<a name="l3854"><span class="ln">3854 </span></a> 
<a name="l3855"><span class="ln">3855 </span></a>    &gt;&gt;&gt; src = torch.tensor([[4, 3, 5], 
<a name="l3856"><span class="ln">3856 </span></a>    ...                     [6, 7, 8]]) 
<a name="l3857"><span class="ln">3857 </span></a>    &gt;&gt;&gt; src.put_(torch.tensor([1, 3]), torch.tensor([9, 10])) 
<a name="l3858"><span class="ln">3858 </span></a>    tensor([[  4,   9,   5], 
<a name="l3859"><span class="ln">3859 </span></a>            [ 10,   7,   8]]) 
<a name="l3860"><span class="ln">3860 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3861"><span class="ln">3861 </span></a><span class="s3">)</span>
<a name="l3862"><span class="ln">3862 </span></a>
<a name="l3863"><span class="ln">3863 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3864"><span class="ln">3864 </span></a>    <span class="s4">&quot;put&quot;</span><span class="s3">,</span>
<a name="l3865"><span class="ln">3865 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3866"><span class="ln">3866 </span></a>put(input, index, source, accumulate=False) -&gt; Tensor 
<a name="l3867"><span class="ln">3867 </span></a> 
<a name="l3868"><span class="ln">3868 </span></a>Out-of-place version of :meth:`torch.Tensor.put_`. 
<a name="l3869"><span class="ln">3869 </span></a>`input` corresponds to `self` in :meth:`torch.Tensor.put_`. 
<a name="l3870"><span class="ln">3870 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3871"><span class="ln">3871 </span></a><span class="s3">)</span>
<a name="l3872"><span class="ln">3872 </span></a>
<a name="l3873"><span class="ln">3873 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3874"><span class="ln">3874 </span></a>    <span class="s4">&quot;qr&quot;</span><span class="s3">,</span>
<a name="l3875"><span class="ln">3875 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3876"><span class="ln">3876 </span></a>qr(some=True) -&gt; (Tensor, Tensor) 
<a name="l3877"><span class="ln">3877 </span></a> 
<a name="l3878"><span class="ln">3878 </span></a>See :func:`torch.qr` 
<a name="l3879"><span class="ln">3879 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3880"><span class="ln">3880 </span></a><span class="s3">)</span>
<a name="l3881"><span class="ln">3881 </span></a>
<a name="l3882"><span class="ln">3882 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3883"><span class="ln">3883 </span></a>    <span class="s4">&quot;qscheme&quot;</span><span class="s3">,</span>
<a name="l3884"><span class="ln">3884 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3885"><span class="ln">3885 </span></a>qscheme() -&gt; torch.qscheme 
<a name="l3886"><span class="ln">3886 </span></a> 
<a name="l3887"><span class="ln">3887 </span></a>Returns the quantization scheme of a given QTensor. 
<a name="l3888"><span class="ln">3888 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3889"><span class="ln">3889 </span></a><span class="s3">)</span>
<a name="l3890"><span class="ln">3890 </span></a>
<a name="l3891"><span class="ln">3891 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3892"><span class="ln">3892 </span></a>    <span class="s4">&quot;quantile&quot;</span><span class="s3">,</span>
<a name="l3893"><span class="ln">3893 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3894"><span class="ln">3894 </span></a>quantile(q, dim=None, keepdim=False, *, interpolation='linear') -&gt; Tensor 
<a name="l3895"><span class="ln">3895 </span></a> 
<a name="l3896"><span class="ln">3896 </span></a>See :func:`torch.quantile` 
<a name="l3897"><span class="ln">3897 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3898"><span class="ln">3898 </span></a><span class="s3">)</span>
<a name="l3899"><span class="ln">3899 </span></a>
<a name="l3900"><span class="ln">3900 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3901"><span class="ln">3901 </span></a>    <span class="s4">&quot;nanquantile&quot;</span><span class="s3">,</span>
<a name="l3902"><span class="ln">3902 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3903"><span class="ln">3903 </span></a>nanquantile(q, dim=None, keepdim=False, *, interpolation='linear') -&gt; Tensor 
<a name="l3904"><span class="ln">3904 </span></a> 
<a name="l3905"><span class="ln">3905 </span></a>See :func:`torch.nanquantile` 
<a name="l3906"><span class="ln">3906 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3907"><span class="ln">3907 </span></a><span class="s3">)</span>
<a name="l3908"><span class="ln">3908 </span></a>
<a name="l3909"><span class="ln">3909 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3910"><span class="ln">3910 </span></a>    <span class="s4">&quot;q_scale&quot;</span><span class="s3">,</span>
<a name="l3911"><span class="ln">3911 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3912"><span class="ln">3912 </span></a>q_scale() -&gt; float 
<a name="l3913"><span class="ln">3913 </span></a> 
<a name="l3914"><span class="ln">3914 </span></a>Given a Tensor quantized by linear(affine) quantization, 
<a name="l3915"><span class="ln">3915 </span></a>returns the scale of the underlying quantizer(). 
<a name="l3916"><span class="ln">3916 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3917"><span class="ln">3917 </span></a><span class="s3">)</span>
<a name="l3918"><span class="ln">3918 </span></a>
<a name="l3919"><span class="ln">3919 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3920"><span class="ln">3920 </span></a>    <span class="s4">&quot;q_zero_point&quot;</span><span class="s3">,</span>
<a name="l3921"><span class="ln">3921 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3922"><span class="ln">3922 </span></a>q_zero_point() -&gt; int 
<a name="l3923"><span class="ln">3923 </span></a> 
<a name="l3924"><span class="ln">3924 </span></a>Given a Tensor quantized by linear(affine) quantization, 
<a name="l3925"><span class="ln">3925 </span></a>returns the zero_point of the underlying quantizer(). 
<a name="l3926"><span class="ln">3926 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3927"><span class="ln">3927 </span></a><span class="s3">)</span>
<a name="l3928"><span class="ln">3928 </span></a>
<a name="l3929"><span class="ln">3929 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3930"><span class="ln">3930 </span></a>    <span class="s4">&quot;q_per_channel_scales&quot;</span><span class="s3">,</span>
<a name="l3931"><span class="ln">3931 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3932"><span class="ln">3932 </span></a>q_per_channel_scales() -&gt; Tensor 
<a name="l3933"><span class="ln">3933 </span></a> 
<a name="l3934"><span class="ln">3934 </span></a>Given a Tensor quantized by linear (affine) per-channel quantization, 
<a name="l3935"><span class="ln">3935 </span></a>returns a Tensor of scales of the underlying quantizer. It has the number of 
<a name="l3936"><span class="ln">3936 </span></a>elements that matches the corresponding dimensions (from q_per_channel_axis) of 
<a name="l3937"><span class="ln">3937 </span></a>the tensor. 
<a name="l3938"><span class="ln">3938 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3939"><span class="ln">3939 </span></a><span class="s3">)</span>
<a name="l3940"><span class="ln">3940 </span></a>
<a name="l3941"><span class="ln">3941 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3942"><span class="ln">3942 </span></a>    <span class="s4">&quot;q_per_channel_zero_points&quot;</span><span class="s3">,</span>
<a name="l3943"><span class="ln">3943 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3944"><span class="ln">3944 </span></a>q_per_channel_zero_points() -&gt; Tensor 
<a name="l3945"><span class="ln">3945 </span></a> 
<a name="l3946"><span class="ln">3946 </span></a>Given a Tensor quantized by linear (affine) per-channel quantization, 
<a name="l3947"><span class="ln">3947 </span></a>returns a tensor of zero_points of the underlying quantizer. It has the number of 
<a name="l3948"><span class="ln">3948 </span></a>elements that matches the corresponding dimensions (from q_per_channel_axis) of 
<a name="l3949"><span class="ln">3949 </span></a>the tensor. 
<a name="l3950"><span class="ln">3950 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3951"><span class="ln">3951 </span></a><span class="s3">)</span>
<a name="l3952"><span class="ln">3952 </span></a>
<a name="l3953"><span class="ln">3953 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3954"><span class="ln">3954 </span></a>    <span class="s4">&quot;q_per_channel_axis&quot;</span><span class="s3">,</span>
<a name="l3955"><span class="ln">3955 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3956"><span class="ln">3956 </span></a>q_per_channel_axis() -&gt; int 
<a name="l3957"><span class="ln">3957 </span></a> 
<a name="l3958"><span class="ln">3958 </span></a>Given a Tensor quantized by linear (affine) per-channel quantization, 
<a name="l3959"><span class="ln">3959 </span></a>returns the index of dimension on which per-channel quantization is applied. 
<a name="l3960"><span class="ln">3960 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3961"><span class="ln">3961 </span></a><span class="s3">)</span>
<a name="l3962"><span class="ln">3962 </span></a>
<a name="l3963"><span class="ln">3963 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3964"><span class="ln">3964 </span></a>    <span class="s4">&quot;random_&quot;</span><span class="s3">,</span>
<a name="l3965"><span class="ln">3965 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3966"><span class="ln">3966 </span></a>random_(from=0, to=None, *, generator=None) -&gt; Tensor 
<a name="l3967"><span class="ln">3967 </span></a> 
<a name="l3968"><span class="ln">3968 </span></a>Fills :attr:`self` tensor with numbers sampled from the discrete uniform 
<a name="l3969"><span class="ln">3969 </span></a>distribution over ``[from, to - 1]``. If not specified, the values are usually 
<a name="l3970"><span class="ln">3970 </span></a>only bounded by :attr:`self` tensor's data type. However, for floating point 
<a name="l3971"><span class="ln">3971 </span></a>types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every 
<a name="l3972"><span class="ln">3972 </span></a>value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()` 
<a name="l3973"><span class="ln">3973 </span></a>will be uniform in ``[0, 2^53]``. 
<a name="l3974"><span class="ln">3974 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3975"><span class="ln">3975 </span></a><span class="s3">)</span>
<a name="l3976"><span class="ln">3976 </span></a>
<a name="l3977"><span class="ln">3977 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3978"><span class="ln">3978 </span></a>    <span class="s4">&quot;rad2deg&quot;</span><span class="s3">,</span>
<a name="l3979"><span class="ln">3979 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3980"><span class="ln">3980 </span></a>rad2deg() -&gt; Tensor 
<a name="l3981"><span class="ln">3981 </span></a> 
<a name="l3982"><span class="ln">3982 </span></a>See :func:`torch.rad2deg` 
<a name="l3983"><span class="ln">3983 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3984"><span class="ln">3984 </span></a><span class="s3">)</span>
<a name="l3985"><span class="ln">3985 </span></a>
<a name="l3986"><span class="ln">3986 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3987"><span class="ln">3987 </span></a>    <span class="s4">&quot;rad2deg_&quot;</span><span class="s3">,</span>
<a name="l3988"><span class="ln">3988 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3989"><span class="ln">3989 </span></a>rad2deg_() -&gt; Tensor 
<a name="l3990"><span class="ln">3990 </span></a> 
<a name="l3991"><span class="ln">3991 </span></a>In-place version of :meth:`~Tensor.rad2deg` 
<a name="l3992"><span class="ln">3992 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l3993"><span class="ln">3993 </span></a><span class="s3">)</span>
<a name="l3994"><span class="ln">3994 </span></a>
<a name="l3995"><span class="ln">3995 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l3996"><span class="ln">3996 </span></a>    <span class="s4">&quot;deg2rad&quot;</span><span class="s3">,</span>
<a name="l3997"><span class="ln">3997 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l3998"><span class="ln">3998 </span></a>deg2rad() -&gt; Tensor 
<a name="l3999"><span class="ln">3999 </span></a> 
<a name="l4000"><span class="ln">4000 </span></a>See :func:`torch.deg2rad` 
<a name="l4001"><span class="ln">4001 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4002"><span class="ln">4002 </span></a><span class="s3">)</span>
<a name="l4003"><span class="ln">4003 </span></a>
<a name="l4004"><span class="ln">4004 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4005"><span class="ln">4005 </span></a>    <span class="s4">&quot;deg2rad_&quot;</span><span class="s3">,</span>
<a name="l4006"><span class="ln">4006 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4007"><span class="ln">4007 </span></a>deg2rad_() -&gt; Tensor 
<a name="l4008"><span class="ln">4008 </span></a> 
<a name="l4009"><span class="ln">4009 </span></a>In-place version of :meth:`~Tensor.deg2rad` 
<a name="l4010"><span class="ln">4010 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4011"><span class="ln">4011 </span></a><span class="s3">)</span>
<a name="l4012"><span class="ln">4012 </span></a>
<a name="l4013"><span class="ln">4013 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4014"><span class="ln">4014 </span></a>    <span class="s4">&quot;ravel&quot;</span><span class="s3">,</span>
<a name="l4015"><span class="ln">4015 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4016"><span class="ln">4016 </span></a>ravel() -&gt; Tensor 
<a name="l4017"><span class="ln">4017 </span></a> 
<a name="l4018"><span class="ln">4018 </span></a>see :func:`torch.ravel` 
<a name="l4019"><span class="ln">4019 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4020"><span class="ln">4020 </span></a><span class="s3">)</span>
<a name="l4021"><span class="ln">4021 </span></a>
<a name="l4022"><span class="ln">4022 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4023"><span class="ln">4023 </span></a>    <span class="s4">&quot;reciprocal&quot;</span><span class="s3">,</span>
<a name="l4024"><span class="ln">4024 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4025"><span class="ln">4025 </span></a>reciprocal() -&gt; Tensor 
<a name="l4026"><span class="ln">4026 </span></a> 
<a name="l4027"><span class="ln">4027 </span></a>See :func:`torch.reciprocal` 
<a name="l4028"><span class="ln">4028 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4029"><span class="ln">4029 </span></a><span class="s3">)</span>
<a name="l4030"><span class="ln">4030 </span></a>
<a name="l4031"><span class="ln">4031 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4032"><span class="ln">4032 </span></a>    <span class="s4">&quot;reciprocal_&quot;</span><span class="s3">,</span>
<a name="l4033"><span class="ln">4033 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4034"><span class="ln">4034 </span></a>reciprocal_() -&gt; Tensor 
<a name="l4035"><span class="ln">4035 </span></a> 
<a name="l4036"><span class="ln">4036 </span></a>In-place version of :meth:`~Tensor.reciprocal` 
<a name="l4037"><span class="ln">4037 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4038"><span class="ln">4038 </span></a><span class="s3">)</span>
<a name="l4039"><span class="ln">4039 </span></a>
<a name="l4040"><span class="ln">4040 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4041"><span class="ln">4041 </span></a>    <span class="s4">&quot;record_stream&quot;</span><span class="s3">,</span>
<a name="l4042"><span class="ln">4042 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4043"><span class="ln">4043 </span></a>record_stream(stream) 
<a name="l4044"><span class="ln">4044 </span></a> 
<a name="l4045"><span class="ln">4045 </span></a>Marks the tensor as having been used by this stream.  When the tensor 
<a name="l4046"><span class="ln">4046 </span></a>is deallocated, ensure the tensor memory is not reused for another tensor 
<a name="l4047"><span class="ln">4047 </span></a>until all work queued on :attr:`stream` at the time of deallocation is 
<a name="l4048"><span class="ln">4048 </span></a>complete. 
<a name="l4049"><span class="ln">4049 </span></a> 
<a name="l4050"><span class="ln">4050 </span></a>.. note:: 
<a name="l4051"><span class="ln">4051 </span></a> 
<a name="l4052"><span class="ln">4052 </span></a>    The caching allocator is aware of only the stream where a tensor was 
<a name="l4053"><span class="ln">4053 </span></a>    allocated. Due to the awareness, it already correctly manages the life 
<a name="l4054"><span class="ln">4054 </span></a>    cycle of tensors on only one stream. But if a tensor is used on a stream 
<a name="l4055"><span class="ln">4055 </span></a>    different from the stream of origin, the allocator might reuse the memory 
<a name="l4056"><span class="ln">4056 </span></a>    unexpectedly. Calling this method lets the allocator know which streams 
<a name="l4057"><span class="ln">4057 </span></a>    have used the tensor. 
<a name="l4058"><span class="ln">4058 </span></a> 
<a name="l4059"><span class="ln">4059 </span></a>.. warning:: 
<a name="l4060"><span class="ln">4060 </span></a> 
<a name="l4061"><span class="ln">4061 </span></a>    This method is most suitable for use cases where you are providing a 
<a name="l4062"><span class="ln">4062 </span></a>    function that created a tensor on a side stream, and want users to be able 
<a name="l4063"><span class="ln">4063 </span></a>    to make use of the tensor without having to think carefully about stream 
<a name="l4064"><span class="ln">4064 </span></a>    safety when making use of them.  These safety guarantees come at some 
<a name="l4065"><span class="ln">4065 </span></a>    performance and predictability cost (analogous to the tradeoff between GC 
<a name="l4066"><span class="ln">4066 </span></a>    and manual memory management), so if you are in a situation where 
<a name="l4067"><span class="ln">4067 </span></a>    you manage the full lifetime of your tensors, you may consider instead 
<a name="l4068"><span class="ln">4068 </span></a>    manually managing CUDA events so that calling this method is not necessary. 
<a name="l4069"><span class="ln">4069 </span></a>    In particular, when you call this method, on later allocations the 
<a name="l4070"><span class="ln">4070 </span></a>    allocator will poll the recorded stream to see if all operations have 
<a name="l4071"><span class="ln">4071 </span></a>    completed yet; you can potentially race with side stream computation and 
<a name="l4072"><span class="ln">4072 </span></a>    non-deterministically reuse or fail to reuse memory for an allocation. 
<a name="l4073"><span class="ln">4073 </span></a> 
<a name="l4074"><span class="ln">4074 </span></a>    You can safely use tensors allocated on side streams without 
<a name="l4075"><span class="ln">4075 </span></a>    :meth:`~Tensor.record_stream`; you must manually ensure that 
<a name="l4076"><span class="ln">4076 </span></a>    any non-creation stream uses of a tensor are synced back to the creation 
<a name="l4077"><span class="ln">4077 </span></a>    stream before you deallocate the tensor.  As the CUDA caching allocator 
<a name="l4078"><span class="ln">4078 </span></a>    guarantees that the memory will only be reused with the same creation stream, 
<a name="l4079"><span class="ln">4079 </span></a>    this is sufficient to ensure that writes to future reallocations of the 
<a name="l4080"><span class="ln">4080 </span></a>    memory will be delayed until non-creation stream uses are done. 
<a name="l4081"><span class="ln">4081 </span></a>    (Counterintuitively, you may observe that on the CPU side we have already 
<a name="l4082"><span class="ln">4082 </span></a>    reallocated the tensor, even though CUDA kernels on the old tensor are 
<a name="l4083"><span class="ln">4083 </span></a>    still in progress.  This is fine, because CUDA operations on the new 
<a name="l4084"><span class="ln">4084 </span></a>    tensor will appropriately wait for the old operations to complete, as they 
<a name="l4085"><span class="ln">4085 </span></a>    are all on the same stream.) 
<a name="l4086"><span class="ln">4086 </span></a> 
<a name="l4087"><span class="ln">4087 </span></a>    Concretely, this looks like this:: 
<a name="l4088"><span class="ln">4088 </span></a> 
<a name="l4089"><span class="ln">4089 </span></a>        with torch.cuda.stream(s0): 
<a name="l4090"><span class="ln">4090 </span></a>            x = torch.zeros(N) 
<a name="l4091"><span class="ln">4091 </span></a> 
<a name="l4092"><span class="ln">4092 </span></a>        s1.wait_stream(s0) 
<a name="l4093"><span class="ln">4093 </span></a>        with torch.cuda.stream(s1): 
<a name="l4094"><span class="ln">4094 </span></a>            y = some_comm_op(x) 
<a name="l4095"><span class="ln">4095 </span></a> 
<a name="l4096"><span class="ln">4096 </span></a>        ... some compute on s0 ... 
<a name="l4097"><span class="ln">4097 </span></a> 
<a name="l4098"><span class="ln">4098 </span></a>        # synchronize creation stream s0 to side stream s1 
<a name="l4099"><span class="ln">4099 </span></a>        # before deallocating x 
<a name="l4100"><span class="ln">4100 </span></a>        s0.wait_stream(s1) 
<a name="l4101"><span class="ln">4101 </span></a>        del x 
<a name="l4102"><span class="ln">4102 </span></a> 
<a name="l4103"><span class="ln">4103 </span></a>    Note that some discretion is required when deciding when to perform 
<a name="l4104"><span class="ln">4104 </span></a>    ``s0.wait_stream(s1)``.  In particular, if we were to wait immediately 
<a name="l4105"><span class="ln">4105 </span></a>    after ``some_comm_op``, there wouldn't be any point in having the side 
<a name="l4106"><span class="ln">4106 </span></a>    stream; it would be equivalent to have run ``some_comm_op`` on ``s0``. 
<a name="l4107"><span class="ln">4107 </span></a>    Instead, the synchronization must be placed at some appropriate, later 
<a name="l4108"><span class="ln">4108 </span></a>    point in time where you expect the side stream ``s1`` to have finished 
<a name="l4109"><span class="ln">4109 </span></a>    work.  This location is typically identified via profiling, e.g., using 
<a name="l4110"><span class="ln">4110 </span></a>    Chrome traces produced 
<a name="l4111"><span class="ln">4111 </span></a>    :meth:`torch.autograd.profiler.profile.export_chrome_trace`.  If you 
<a name="l4112"><span class="ln">4112 </span></a>    place the wait too early, work on s0 will block until ``s1`` has finished, 
<a name="l4113"><span class="ln">4113 </span></a>    preventing further overlapping of communication and computation.  If you 
<a name="l4114"><span class="ln">4114 </span></a>    place the wait too late, you will use more memory than is strictly 
<a name="l4115"><span class="ln">4115 </span></a>    necessary (as you are keeping ``x`` live for longer.)  For a concrete 
<a name="l4116"><span class="ln">4116 </span></a>    example of how this guidance can be applied in practice, see this post: 
<a name="l4117"><span class="ln">4117 </span></a>    `FSDP and CUDACachingAllocator 
<a name="l4118"><span class="ln">4118 </span></a>    &lt;https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486&gt;`_. 
<a name="l4119"><span class="ln">4119 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4120"><span class="ln">4120 </span></a><span class="s3">)</span>
<a name="l4121"><span class="ln">4121 </span></a>
<a name="l4122"><span class="ln">4122 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4123"><span class="ln">4123 </span></a>    <span class="s4">&quot;remainder&quot;</span><span class="s3">,</span>
<a name="l4124"><span class="ln">4124 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4125"><span class="ln">4125 </span></a>remainder(divisor) -&gt; Tensor 
<a name="l4126"><span class="ln">4126 </span></a> 
<a name="l4127"><span class="ln">4127 </span></a>See :func:`torch.remainder` 
<a name="l4128"><span class="ln">4128 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4129"><span class="ln">4129 </span></a><span class="s3">)</span>
<a name="l4130"><span class="ln">4130 </span></a>
<a name="l4131"><span class="ln">4131 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4132"><span class="ln">4132 </span></a>    <span class="s4">&quot;remainder_&quot;</span><span class="s3">,</span>
<a name="l4133"><span class="ln">4133 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4134"><span class="ln">4134 </span></a>remainder_(divisor) -&gt; Tensor 
<a name="l4135"><span class="ln">4135 </span></a> 
<a name="l4136"><span class="ln">4136 </span></a>In-place version of :meth:`~Tensor.remainder` 
<a name="l4137"><span class="ln">4137 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4138"><span class="ln">4138 </span></a><span class="s3">)</span>
<a name="l4139"><span class="ln">4139 </span></a>
<a name="l4140"><span class="ln">4140 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4141"><span class="ln">4141 </span></a>    <span class="s4">&quot;renorm&quot;</span><span class="s3">,</span>
<a name="l4142"><span class="ln">4142 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4143"><span class="ln">4143 </span></a>renorm(p, dim, maxnorm) -&gt; Tensor 
<a name="l4144"><span class="ln">4144 </span></a> 
<a name="l4145"><span class="ln">4145 </span></a>See :func:`torch.renorm` 
<a name="l4146"><span class="ln">4146 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4147"><span class="ln">4147 </span></a><span class="s3">)</span>
<a name="l4148"><span class="ln">4148 </span></a>
<a name="l4149"><span class="ln">4149 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4150"><span class="ln">4150 </span></a>    <span class="s4">&quot;renorm_&quot;</span><span class="s3">,</span>
<a name="l4151"><span class="ln">4151 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4152"><span class="ln">4152 </span></a>renorm_(p, dim, maxnorm) -&gt; Tensor 
<a name="l4153"><span class="ln">4153 </span></a> 
<a name="l4154"><span class="ln">4154 </span></a>In-place version of :meth:`~Tensor.renorm` 
<a name="l4155"><span class="ln">4155 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4156"><span class="ln">4156 </span></a><span class="s3">)</span>
<a name="l4157"><span class="ln">4157 </span></a>
<a name="l4158"><span class="ln">4158 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4159"><span class="ln">4159 </span></a>    <span class="s4">&quot;repeat&quot;</span><span class="s3">,</span>
<a name="l4160"><span class="ln">4160 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4161"><span class="ln">4161 </span></a>repeat(*repeats) -&gt; Tensor 
<a name="l4162"><span class="ln">4162 </span></a> 
<a name="l4163"><span class="ln">4163 </span></a>Repeats this tensor along the specified dimensions. 
<a name="l4164"><span class="ln">4164 </span></a> 
<a name="l4165"><span class="ln">4165 </span></a>Unlike :meth:`~Tensor.expand`, this function copies the tensor's data. 
<a name="l4166"><span class="ln">4166 </span></a> 
<a name="l4167"><span class="ln">4167 </span></a>.. warning:: 
<a name="l4168"><span class="ln">4168 </span></a> 
<a name="l4169"><span class="ln">4169 </span></a>    :meth:`~Tensor.repeat` behaves differently from 
<a name="l4170"><span class="ln">4170 </span></a>    `numpy.repeat &lt;https://numpy.org/doc/stable/reference/generated/numpy.repeat.html&gt;`_, 
<a name="l4171"><span class="ln">4171 </span></a>    but is more similar to 
<a name="l4172"><span class="ln">4172 </span></a>    `numpy.tile &lt;https://numpy.org/doc/stable/reference/generated/numpy.tile.html&gt;`_. 
<a name="l4173"><span class="ln">4173 </span></a>    For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`. 
<a name="l4174"><span class="ln">4174 </span></a> 
<a name="l4175"><span class="ln">4175 </span></a>Args: 
<a name="l4176"><span class="ln">4176 </span></a>    repeat (torch.Size, int..., tuple of int or list of int): The number of times to repeat this tensor along each dimension 
<a name="l4177"><span class="ln">4177 </span></a> 
<a name="l4178"><span class="ln">4178 </span></a>Example:: 
<a name="l4179"><span class="ln">4179 </span></a> 
<a name="l4180"><span class="ln">4180 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l4181"><span class="ln">4181 </span></a>    &gt;&gt;&gt; x.repeat(4, 2) 
<a name="l4182"><span class="ln">4182 </span></a>    tensor([[ 1,  2,  3,  1,  2,  3], 
<a name="l4183"><span class="ln">4183 </span></a>            [ 1,  2,  3,  1,  2,  3], 
<a name="l4184"><span class="ln">4184 </span></a>            [ 1,  2,  3,  1,  2,  3], 
<a name="l4185"><span class="ln">4185 </span></a>            [ 1,  2,  3,  1,  2,  3]]) 
<a name="l4186"><span class="ln">4186 </span></a>    &gt;&gt;&gt; x.repeat(4, 2, 1).size() 
<a name="l4187"><span class="ln">4187 </span></a>    torch.Size([4, 2, 3]) 
<a name="l4188"><span class="ln">4188 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4189"><span class="ln">4189 </span></a><span class="s3">)</span>
<a name="l4190"><span class="ln">4190 </span></a>
<a name="l4191"><span class="ln">4191 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4192"><span class="ln">4192 </span></a>    <span class="s4">&quot;repeat_interleave&quot;</span><span class="s3">,</span>
<a name="l4193"><span class="ln">4193 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4194"><span class="ln">4194 </span></a>repeat_interleave(repeats, dim=None, *, output_size=None) -&gt; Tensor 
<a name="l4195"><span class="ln">4195 </span></a> 
<a name="l4196"><span class="ln">4196 </span></a>See :func:`torch.repeat_interleave`. 
<a name="l4197"><span class="ln">4197 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4198"><span class="ln">4198 </span></a><span class="s3">)</span>
<a name="l4199"><span class="ln">4199 </span></a>
<a name="l4200"><span class="ln">4200 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4201"><span class="ln">4201 </span></a>    <span class="s4">&quot;requires_grad_&quot;</span><span class="s3">,</span>
<a name="l4202"><span class="ln">4202 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4203"><span class="ln">4203 </span></a>requires_grad_(requires_grad=True) -&gt; Tensor 
<a name="l4204"><span class="ln">4204 </span></a> 
<a name="l4205"><span class="ln">4205 </span></a>Change if autograd should record operations on this tensor: sets this tensor's 
<a name="l4206"><span class="ln">4206 </span></a>:attr:`requires_grad` attribute in-place. Returns this tensor. 
<a name="l4207"><span class="ln">4207 </span></a> 
<a name="l4208"><span class="ln">4208 </span></a>:func:`requires_grad_`'s main use case is to tell autograd to begin recording 
<a name="l4209"><span class="ln">4209 </span></a>operations on a Tensor ``tensor``. If ``tensor`` has ``requires_grad=False`` 
<a name="l4210"><span class="ln">4210 </span></a>(because it was obtained through a DataLoader, or required preprocessing or 
<a name="l4211"><span class="ln">4211 </span></a>initialization), ``tensor.requires_grad_()`` makes it so that autograd will 
<a name="l4212"><span class="ln">4212 </span></a>begin to record operations on ``tensor``. 
<a name="l4213"><span class="ln">4213 </span></a> 
<a name="l4214"><span class="ln">4214 </span></a>Args: 
<a name="l4215"><span class="ln">4215 </span></a>    requires_grad (bool): If autograd should record operations on this tensor. 
<a name="l4216"><span class="ln">4216 </span></a>        Default: ``True``. 
<a name="l4217"><span class="ln">4217 </span></a> 
<a name="l4218"><span class="ln">4218 </span></a>Example:: 
<a name="l4219"><span class="ln">4219 </span></a> 
<a name="l4220"><span class="ln">4220 </span></a>    &gt;&gt;&gt; # Let's say we want to preprocess some saved weights and use 
<a name="l4221"><span class="ln">4221 </span></a>    &gt;&gt;&gt; # the result as new weights. 
<a name="l4222"><span class="ln">4222 </span></a>    &gt;&gt;&gt; saved_weights = [0.1, 0.2, 0.3, 0.25] 
<a name="l4223"><span class="ln">4223 </span></a>    &gt;&gt;&gt; loaded_weights = torch.tensor(saved_weights) 
<a name="l4224"><span class="ln">4224 </span></a>    &gt;&gt;&gt; weights = preprocess(loaded_weights)  # some function 
<a name="l4225"><span class="ln">4225 </span></a>    &gt;&gt;&gt; weights 
<a name="l4226"><span class="ln">4226 </span></a>    tensor([-0.5503,  0.4926, -2.1158, -0.8303]) 
<a name="l4227"><span class="ln">4227 </span></a> 
<a name="l4228"><span class="ln">4228 </span></a>    &gt;&gt;&gt; # Now, start to record operations done to weights 
<a name="l4229"><span class="ln">4229 </span></a>    &gt;&gt;&gt; weights.requires_grad_() 
<a name="l4230"><span class="ln">4230 </span></a>    &gt;&gt;&gt; out = weights.pow(2).sum() 
<a name="l4231"><span class="ln">4231 </span></a>    &gt;&gt;&gt; out.backward() 
<a name="l4232"><span class="ln">4232 </span></a>    &gt;&gt;&gt; weights.grad 
<a name="l4233"><span class="ln">4233 </span></a>    tensor([-1.1007,  0.9853, -4.2316, -1.6606]) 
<a name="l4234"><span class="ln">4234 </span></a> 
<a name="l4235"><span class="ln">4235 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4236"><span class="ln">4236 </span></a><span class="s3">)</span>
<a name="l4237"><span class="ln">4237 </span></a>
<a name="l4238"><span class="ln">4238 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4239"><span class="ln">4239 </span></a>    <span class="s4">&quot;reshape&quot;</span><span class="s3">,</span>
<a name="l4240"><span class="ln">4240 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4241"><span class="ln">4241 </span></a>reshape(*shape) -&gt; Tensor 
<a name="l4242"><span class="ln">4242 </span></a> 
<a name="l4243"><span class="ln">4243 </span></a>Returns a tensor with the same data and number of elements as :attr:`self` 
<a name="l4244"><span class="ln">4244 </span></a>but with the specified shape. This method returns a view if :attr:`shape` is 
<a name="l4245"><span class="ln">4245 </span></a>compatible with the current shape. See :meth:`torch.Tensor.view` on when it is 
<a name="l4246"><span class="ln">4246 </span></a>possible to return a view. 
<a name="l4247"><span class="ln">4247 </span></a> 
<a name="l4248"><span class="ln">4248 </span></a>See :func:`torch.reshape` 
<a name="l4249"><span class="ln">4249 </span></a> 
<a name="l4250"><span class="ln">4250 </span></a>Args: 
<a name="l4251"><span class="ln">4251 </span></a>    shape (tuple of ints or int...): the desired shape 
<a name="l4252"><span class="ln">4252 </span></a> 
<a name="l4253"><span class="ln">4253 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4254"><span class="ln">4254 </span></a><span class="s3">)</span>
<a name="l4255"><span class="ln">4255 </span></a>
<a name="l4256"><span class="ln">4256 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4257"><span class="ln">4257 </span></a>    <span class="s4">&quot;reshape_as&quot;</span><span class="s3">,</span>
<a name="l4258"><span class="ln">4258 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4259"><span class="ln">4259 </span></a>reshape_as(other) -&gt; Tensor 
<a name="l4260"><span class="ln">4260 </span></a> 
<a name="l4261"><span class="ln">4261 </span></a>Returns this tensor as the same shape as :attr:`other`. 
<a name="l4262"><span class="ln">4262 </span></a>``self.reshape_as(other)`` is equivalent to ``self.reshape(other.sizes())``. 
<a name="l4263"><span class="ln">4263 </span></a>This method returns a view if ``other.sizes()`` is compatible with the current 
<a name="l4264"><span class="ln">4264 </span></a>shape. See :meth:`torch.Tensor.view` on when it is possible to return a view. 
<a name="l4265"><span class="ln">4265 </span></a> 
<a name="l4266"><span class="ln">4266 </span></a>Please see :meth:`reshape` for more information about ``reshape``. 
<a name="l4267"><span class="ln">4267 </span></a> 
<a name="l4268"><span class="ln">4268 </span></a>Args: 
<a name="l4269"><span class="ln">4269 </span></a>    other (:class:`torch.Tensor`): The result tensor has the same shape 
<a name="l4270"><span class="ln">4270 </span></a>        as :attr:`other`. 
<a name="l4271"><span class="ln">4271 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4272"><span class="ln">4272 </span></a><span class="s3">)</span>
<a name="l4273"><span class="ln">4273 </span></a>
<a name="l4274"><span class="ln">4274 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4275"><span class="ln">4275 </span></a>    <span class="s4">&quot;resize_&quot;</span><span class="s3">,</span>
<a name="l4276"><span class="ln">4276 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4277"><span class="ln">4277 </span></a>resize_(*sizes, memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l4278"><span class="ln">4278 </span></a> 
<a name="l4279"><span class="ln">4279 </span></a>Resizes :attr:`self` tensor to the specified size. If the number of elements is 
<a name="l4280"><span class="ln">4280 </span></a>larger than the current storage size, then the underlying storage is resized 
<a name="l4281"><span class="ln">4281 </span></a>to fit the new number of elements. If the number of elements is smaller, the 
<a name="l4282"><span class="ln">4282 </span></a>underlying storage is not changed. Existing elements are preserved but any new 
<a name="l4283"><span class="ln">4283 </span></a>memory is uninitialized. 
<a name="l4284"><span class="ln">4284 </span></a> 
<a name="l4285"><span class="ln">4285 </span></a>.. warning:: 
<a name="l4286"><span class="ln">4286 </span></a> 
<a name="l4287"><span class="ln">4287 </span></a>    This is a low-level method. The storage is reinterpreted as C-contiguous, 
<a name="l4288"><span class="ln">4288 </span></a>    ignoring the current strides (unless the target size equals the current 
<a name="l4289"><span class="ln">4289 </span></a>    size, in which case the tensor is left unchanged). For most purposes, you 
<a name="l4290"><span class="ln">4290 </span></a>    will instead want to use :meth:`~Tensor.view()`, which checks for 
<a name="l4291"><span class="ln">4291 </span></a>    contiguity, or :meth:`~Tensor.reshape()`, which copies data if needed. To 
<a name="l4292"><span class="ln">4292 </span></a>    change the size in-place with custom strides, see :meth:`~Tensor.set_()`. 
<a name="l4293"><span class="ln">4293 </span></a> 
<a name="l4294"><span class="ln">4294 </span></a>.. note:: 
<a name="l4295"><span class="ln">4295 </span></a> 
<a name="l4296"><span class="ln">4296 </span></a>    If :func:`torch.use_deterministic_algorithms()` and 
<a name="l4297"><span class="ln">4297 </span></a>    :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l4298"><span class="ln">4298 </span></a>    ``True``, new elements are initialized to prevent nondeterministic behavior 
<a name="l4299"><span class="ln">4299 </span></a>    from using the result as an input to an operation. Floating point and 
<a name="l4300"><span class="ln">4300 </span></a>    complex values are set to NaN, and integer values are set to the maximum 
<a name="l4301"><span class="ln">4301 </span></a>    value. 
<a name="l4302"><span class="ln">4302 </span></a> 
<a name="l4303"><span class="ln">4303 </span></a>Args: 
<a name="l4304"><span class="ln">4304 </span></a>    sizes (torch.Size or int...): the desired size 
<a name="l4305"><span class="ln">4305 </span></a>    memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l4306"><span class="ln">4306 </span></a>        Tensor. Default: ``torch.contiguous_format``. Note that memory format of 
<a name="l4307"><span class="ln">4307 </span></a>        :attr:`self` is going to be unaffected if ``self.size()`` matches ``sizes``. 
<a name="l4308"><span class="ln">4308 </span></a> 
<a name="l4309"><span class="ln">4309 </span></a>Example:: 
<a name="l4310"><span class="ln">4310 </span></a> 
<a name="l4311"><span class="ln">4311 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]]) 
<a name="l4312"><span class="ln">4312 </span></a>    &gt;&gt;&gt; x.resize_(2, 2) 
<a name="l4313"><span class="ln">4313 </span></a>    tensor([[ 1,  2], 
<a name="l4314"><span class="ln">4314 </span></a>            [ 3,  4]]) 
<a name="l4315"><span class="ln">4315 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4316"><span class="ln">4316 </span></a><span class="s3">)</span>
<a name="l4317"><span class="ln">4317 </span></a>
<a name="l4318"><span class="ln">4318 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4319"><span class="ln">4319 </span></a>    <span class="s4">&quot;resize_as_&quot;</span><span class="s3">,</span>
<a name="l4320"><span class="ln">4320 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4321"><span class="ln">4321 </span></a>resize_as_(tensor, memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l4322"><span class="ln">4322 </span></a> 
<a name="l4323"><span class="ln">4323 </span></a>Resizes the :attr:`self` tensor to be the same size as the specified 
<a name="l4324"><span class="ln">4324 </span></a>:attr:`tensor`. This is equivalent to ``self.resize_(tensor.size())``. 
<a name="l4325"><span class="ln">4325 </span></a> 
<a name="l4326"><span class="ln">4326 </span></a>Args: 
<a name="l4327"><span class="ln">4327 </span></a>    memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l4328"><span class="ln">4328 </span></a>        Tensor. Default: ``torch.contiguous_format``. Note that memory format of 
<a name="l4329"><span class="ln">4329 </span></a>        :attr:`self` is going to be unaffected if ``self.size()`` matches ``tensor.size()``. 
<a name="l4330"><span class="ln">4330 </span></a> 
<a name="l4331"><span class="ln">4331 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4332"><span class="ln">4332 </span></a><span class="s3">)</span>
<a name="l4333"><span class="ln">4333 </span></a>
<a name="l4334"><span class="ln">4334 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4335"><span class="ln">4335 </span></a>    <span class="s4">&quot;rot90&quot;</span><span class="s3">,</span>
<a name="l4336"><span class="ln">4336 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4337"><span class="ln">4337 </span></a>rot90(k, dims) -&gt; Tensor 
<a name="l4338"><span class="ln">4338 </span></a> 
<a name="l4339"><span class="ln">4339 </span></a>See :func:`torch.rot90` 
<a name="l4340"><span class="ln">4340 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4341"><span class="ln">4341 </span></a><span class="s3">)</span>
<a name="l4342"><span class="ln">4342 </span></a>
<a name="l4343"><span class="ln">4343 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4344"><span class="ln">4344 </span></a>    <span class="s4">&quot;round&quot;</span><span class="s3">,</span>
<a name="l4345"><span class="ln">4345 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4346"><span class="ln">4346 </span></a>round(decimals=0) -&gt; Tensor 
<a name="l4347"><span class="ln">4347 </span></a> 
<a name="l4348"><span class="ln">4348 </span></a>See :func:`torch.round` 
<a name="l4349"><span class="ln">4349 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4350"><span class="ln">4350 </span></a><span class="s3">)</span>
<a name="l4351"><span class="ln">4351 </span></a>
<a name="l4352"><span class="ln">4352 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4353"><span class="ln">4353 </span></a>    <span class="s4">&quot;round_&quot;</span><span class="s3">,</span>
<a name="l4354"><span class="ln">4354 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4355"><span class="ln">4355 </span></a>round_(decimals=0) -&gt; Tensor 
<a name="l4356"><span class="ln">4356 </span></a> 
<a name="l4357"><span class="ln">4357 </span></a>In-place version of :meth:`~Tensor.round` 
<a name="l4358"><span class="ln">4358 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4359"><span class="ln">4359 </span></a><span class="s3">)</span>
<a name="l4360"><span class="ln">4360 </span></a>
<a name="l4361"><span class="ln">4361 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4362"><span class="ln">4362 </span></a>    <span class="s4">&quot;rsqrt&quot;</span><span class="s3">,</span>
<a name="l4363"><span class="ln">4363 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4364"><span class="ln">4364 </span></a>rsqrt() -&gt; Tensor 
<a name="l4365"><span class="ln">4365 </span></a> 
<a name="l4366"><span class="ln">4366 </span></a>See :func:`torch.rsqrt` 
<a name="l4367"><span class="ln">4367 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4368"><span class="ln">4368 </span></a><span class="s3">)</span>
<a name="l4369"><span class="ln">4369 </span></a>
<a name="l4370"><span class="ln">4370 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4371"><span class="ln">4371 </span></a>    <span class="s4">&quot;rsqrt_&quot;</span><span class="s3">,</span>
<a name="l4372"><span class="ln">4372 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4373"><span class="ln">4373 </span></a>rsqrt_() -&gt; Tensor 
<a name="l4374"><span class="ln">4374 </span></a> 
<a name="l4375"><span class="ln">4375 </span></a>In-place version of :meth:`~Tensor.rsqrt` 
<a name="l4376"><span class="ln">4376 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4377"><span class="ln">4377 </span></a><span class="s3">)</span>
<a name="l4378"><span class="ln">4378 </span></a>
<a name="l4379"><span class="ln">4379 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4380"><span class="ln">4380 </span></a>    <span class="s4">&quot;scatter_&quot;</span><span class="s3">,</span>
<a name="l4381"><span class="ln">4381 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4382"><span class="ln">4382 </span></a>scatter_(dim, index, src, *, reduce=None) -&gt; Tensor 
<a name="l4383"><span class="ln">4383 </span></a> 
<a name="l4384"><span class="ln">4384 </span></a>Writes all values from the tensor :attr:`src` into :attr:`self` at the indices 
<a name="l4385"><span class="ln">4385 </span></a>specified in the :attr:`index` tensor. For each value in :attr:`src`, its output 
<a name="l4386"><span class="ln">4386 </span></a>index is specified by its index in :attr:`src` for ``dimension != dim`` and by 
<a name="l4387"><span class="ln">4387 </span></a>the corresponding value in :attr:`index` for ``dimension = dim``. 
<a name="l4388"><span class="ln">4388 </span></a> 
<a name="l4389"><span class="ln">4389 </span></a>For a 3-D tensor, :attr:`self` is updated as:: 
<a name="l4390"><span class="ln">4390 </span></a> 
<a name="l4391"><span class="ln">4391 </span></a>    self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0 
<a name="l4392"><span class="ln">4392 </span></a>    self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1 
<a name="l4393"><span class="ln">4393 </span></a>    self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2 
<a name="l4394"><span class="ln">4394 </span></a> 
<a name="l4395"><span class="ln">4395 </span></a>This is the reverse operation of the manner described in :meth:`~Tensor.gather`. 
<a name="l4396"><span class="ln">4396 </span></a> 
<a name="l4397"><span class="ln">4397 </span></a>:attr:`self`, :attr:`index` and :attr:`src` (if it is a Tensor) should all have 
<a name="l4398"><span class="ln">4398 </span></a>the same number of dimensions. It is also required that 
<a name="l4399"><span class="ln">4399 </span></a>``index.size(d) &lt;= src.size(d)`` for all dimensions ``d``, and that 
<a name="l4400"><span class="ln">4400 </span></a>``index.size(d) &lt;= self.size(d)`` for all dimensions ``d != dim``. 
<a name="l4401"><span class="ln">4401 </span></a>Note that ``index`` and ``src`` do not broadcast. 
<a name="l4402"><span class="ln">4402 </span></a> 
<a name="l4403"><span class="ln">4403 </span></a>Moreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be 
<a name="l4404"><span class="ln">4404 </span></a>between ``0`` and ``self.size(dim) - 1`` inclusive. 
<a name="l4405"><span class="ln">4405 </span></a> 
<a name="l4406"><span class="ln">4406 </span></a>.. warning:: 
<a name="l4407"><span class="ln">4407 </span></a> 
<a name="l4408"><span class="ln">4408 </span></a>    When indices are not unique, the behavior is non-deterministic (one of the 
<a name="l4409"><span class="ln">4409 </span></a>    values from ``src`` will be picked arbitrarily) and the gradient will be 
<a name="l4410"><span class="ln">4410 </span></a>    incorrect (it will be propagated to all locations in the source that 
<a name="l4411"><span class="ln">4411 </span></a>    correspond to the same index)! 
<a name="l4412"><span class="ln">4412 </span></a> 
<a name="l4413"><span class="ln">4413 </span></a>.. note:: 
<a name="l4414"><span class="ln">4414 </span></a> 
<a name="l4415"><span class="ln">4415 </span></a>    The backward pass is implemented only for ``src.shape == index.shape``. 
<a name="l4416"><span class="ln">4416 </span></a> 
<a name="l4417"><span class="ln">4417 </span></a>Additionally accepts an optional :attr:`reduce` argument that allows 
<a name="l4418"><span class="ln">4418 </span></a>specification of an optional reduction operation, which is applied to all 
<a name="l4419"><span class="ln">4419 </span></a>values in the tensor :attr:`src` into :attr:`self` at the indices 
<a name="l4420"><span class="ln">4420 </span></a>specified in the :attr:`index`. For each value in :attr:`src`, the reduction 
<a name="l4421"><span class="ln">4421 </span></a>operation is applied to an index in :attr:`self` which is specified by 
<a name="l4422"><span class="ln">4422 </span></a>its index in :attr:`src` for ``dimension != dim`` and by the corresponding 
<a name="l4423"><span class="ln">4423 </span></a>value in :attr:`index` for ``dimension = dim``. 
<a name="l4424"><span class="ln">4424 </span></a> 
<a name="l4425"><span class="ln">4425 </span></a>Given a 3-D tensor and reduction using the multiplication operation, :attr:`self` 
<a name="l4426"><span class="ln">4426 </span></a>is updated as:: 
<a name="l4427"><span class="ln">4427 </span></a> 
<a name="l4428"><span class="ln">4428 </span></a>    self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0 
<a name="l4429"><span class="ln">4429 </span></a>    self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1 
<a name="l4430"><span class="ln">4430 </span></a>    self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2 
<a name="l4431"><span class="ln">4431 </span></a> 
<a name="l4432"><span class="ln">4432 </span></a>Reducing with the addition operation is the same as using 
<a name="l4433"><span class="ln">4433 </span></a>:meth:`~torch.Tensor.scatter_add_`. 
<a name="l4434"><span class="ln">4434 </span></a> 
<a name="l4435"><span class="ln">4435 </span></a>.. warning:: 
<a name="l4436"><span class="ln">4436 </span></a>    The reduce argument with Tensor ``src`` is deprecated and will be removed in 
<a name="l4437"><span class="ln">4437 </span></a>    a future PyTorch release. Please use :meth:`~torch.Tensor.scatter_reduce_` 
<a name="l4438"><span class="ln">4438 </span></a>    instead for more reduction options. 
<a name="l4439"><span class="ln">4439 </span></a> 
<a name="l4440"><span class="ln">4440 </span></a>Args: 
<a name="l4441"><span class="ln">4441 </span></a>    dim (int): the axis along which to index 
<a name="l4442"><span class="ln">4442 </span></a>    index (LongTensor): the indices of elements to scatter, can be either empty 
<a name="l4443"><span class="ln">4443 </span></a>        or of the same dimensionality as ``src``. When empty, the operation 
<a name="l4444"><span class="ln">4444 </span></a>        returns ``self`` unchanged. 
<a name="l4445"><span class="ln">4445 </span></a>    src (Tensor): the source element(s) to scatter. 
<a name="l4446"><span class="ln">4446 </span></a> 
<a name="l4447"><span class="ln">4447 </span></a>Keyword args: 
<a name="l4448"><span class="ln">4448 </span></a>    reduce (str, optional): reduction operation to apply, can be either 
<a name="l4449"><span class="ln">4449 </span></a>        ``'add'`` or ``'multiply'``. 
<a name="l4450"><span class="ln">4450 </span></a> 
<a name="l4451"><span class="ln">4451 </span></a>Example:: 
<a name="l4452"><span class="ln">4452 </span></a> 
<a name="l4453"><span class="ln">4453 </span></a>    &gt;&gt;&gt; src = torch.arange(1, 11).reshape((2, 5)) 
<a name="l4454"><span class="ln">4454 </span></a>    &gt;&gt;&gt; src 
<a name="l4455"><span class="ln">4455 </span></a>    tensor([[ 1,  2,  3,  4,  5], 
<a name="l4456"><span class="ln">4456 </span></a>            [ 6,  7,  8,  9, 10]]) 
<a name="l4457"><span class="ln">4457 </span></a>    &gt;&gt;&gt; index = torch.tensor([[0, 1, 2, 0]]) 
<a name="l4458"><span class="ln">4458 </span></a>    &gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src) 
<a name="l4459"><span class="ln">4459 </span></a>    tensor([[1, 0, 0, 4, 0], 
<a name="l4460"><span class="ln">4460 </span></a>            [0, 2, 0, 0, 0], 
<a name="l4461"><span class="ln">4461 </span></a>            [0, 0, 3, 0, 0]]) 
<a name="l4462"><span class="ln">4462 </span></a>    &gt;&gt;&gt; index = torch.tensor([[0, 1, 2], [0, 1, 4]]) 
<a name="l4463"><span class="ln">4463 </span></a>    &gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src) 
<a name="l4464"><span class="ln">4464 </span></a>    tensor([[1, 2, 3, 0, 0], 
<a name="l4465"><span class="ln">4465 </span></a>            [6, 7, 0, 0, 8], 
<a name="l4466"><span class="ln">4466 </span></a>            [0, 0, 0, 0, 0]]) 
<a name="l4467"><span class="ln">4467 </span></a> 
<a name="l4468"><span class="ln">4468 </span></a>    &gt;&gt;&gt; torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]), 
<a name="l4469"><span class="ln">4469 </span></a>    ...            1.23, reduce='multiply') 
<a name="l4470"><span class="ln">4470 </span></a>    tensor([[2.0000, 2.0000, 2.4600, 2.0000], 
<a name="l4471"><span class="ln">4471 </span></a>            [2.0000, 2.0000, 2.0000, 2.4600]]) 
<a name="l4472"><span class="ln">4472 </span></a>    &gt;&gt;&gt; torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]), 
<a name="l4473"><span class="ln">4473 </span></a>    ...            1.23, reduce='add') 
<a name="l4474"><span class="ln">4474 </span></a>    tensor([[2.0000, 2.0000, 3.2300, 2.0000], 
<a name="l4475"><span class="ln">4475 </span></a>            [2.0000, 2.0000, 2.0000, 3.2300]]) 
<a name="l4476"><span class="ln">4476 </span></a> 
<a name="l4477"><span class="ln">4477 </span></a>.. function:: scatter_(dim, index, value, *, reduce=None) -&gt; Tensor: 
<a name="l4478"><span class="ln">4478 </span></a>   :noindex: 
<a name="l4479"><span class="ln">4479 </span></a> 
<a name="l4480"><span class="ln">4480 </span></a>Writes the value from :attr:`value` into :attr:`self` at the indices 
<a name="l4481"><span class="ln">4481 </span></a>specified in the :attr:`index` tensor.  This operation is equivalent to the previous version, 
<a name="l4482"><span class="ln">4482 </span></a>with the :attr:`src` tensor filled entirely with :attr:`value`. 
<a name="l4483"><span class="ln">4483 </span></a> 
<a name="l4484"><span class="ln">4484 </span></a>Args: 
<a name="l4485"><span class="ln">4485 </span></a>    dim (int): the axis along which to index 
<a name="l4486"><span class="ln">4486 </span></a>    index (LongTensor): the indices of elements to scatter, can be either empty 
<a name="l4487"><span class="ln">4487 </span></a>        or of the same dimensionality as ``src``. When empty, the operation 
<a name="l4488"><span class="ln">4488 </span></a>        returns ``self`` unchanged. 
<a name="l4489"><span class="ln">4489 </span></a>    value (Scalar): the value to scatter. 
<a name="l4490"><span class="ln">4490 </span></a> 
<a name="l4491"><span class="ln">4491 </span></a>Keyword args: 
<a name="l4492"><span class="ln">4492 </span></a>    reduce (str, optional): reduction operation to apply, can be either 
<a name="l4493"><span class="ln">4493 </span></a>        ``'add'`` or ``'multiply'``. 
<a name="l4494"><span class="ln">4494 </span></a> 
<a name="l4495"><span class="ln">4495 </span></a>Example:: 
<a name="l4496"><span class="ln">4496 </span></a> 
<a name="l4497"><span class="ln">4497 </span></a>    &gt;&gt;&gt; index = torch.tensor([[0, 1]]) 
<a name="l4498"><span class="ln">4498 </span></a>    &gt;&gt;&gt; value = 2 
<a name="l4499"><span class="ln">4499 </span></a>    &gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, index, value) 
<a name="l4500"><span class="ln">4500 </span></a>    tensor([[2., 0., 0., 0., 0.], 
<a name="l4501"><span class="ln">4501 </span></a>            [0., 2., 0., 0., 0.], 
<a name="l4502"><span class="ln">4502 </span></a>            [0., 0., 0., 0., 0.]]) 
<a name="l4503"><span class="ln">4503 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4504"><span class="ln">4504 </span></a><span class="s3">)</span>
<a name="l4505"><span class="ln">4505 </span></a>
<a name="l4506"><span class="ln">4506 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4507"><span class="ln">4507 </span></a>    <span class="s4">&quot;scatter_add_&quot;</span><span class="s3">,</span>
<a name="l4508"><span class="ln">4508 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4509"><span class="ln">4509 </span></a>scatter_add_(dim, index, src) -&gt; Tensor 
<a name="l4510"><span class="ln">4510 </span></a> 
<a name="l4511"><span class="ln">4511 </span></a>Adds all values from the tensor :attr:`src` into :attr:`self` at the indices 
<a name="l4512"><span class="ln">4512 </span></a>specified in the :attr:`index` tensor in a similar fashion as 
<a name="l4513"><span class="ln">4513 </span></a>:meth:`~torch.Tensor.scatter_`. For each value in :attr:`src`, it is added to 
<a name="l4514"><span class="ln">4514 </span></a>an index in :attr:`self` which is specified by its index in :attr:`src` 
<a name="l4515"><span class="ln">4515 </span></a>for ``dimension != dim`` and by the corresponding value in :attr:`index` for 
<a name="l4516"><span class="ln">4516 </span></a>``dimension = dim``. 
<a name="l4517"><span class="ln">4517 </span></a> 
<a name="l4518"><span class="ln">4518 </span></a>For a 3-D tensor, :attr:`self` is updated as:: 
<a name="l4519"><span class="ln">4519 </span></a> 
<a name="l4520"><span class="ln">4520 </span></a>    self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0 
<a name="l4521"><span class="ln">4521 </span></a>    self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1 
<a name="l4522"><span class="ln">4522 </span></a>    self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2 
<a name="l4523"><span class="ln">4523 </span></a> 
<a name="l4524"><span class="ln">4524 </span></a>:attr:`self`, :attr:`index` and :attr:`src` should have same number of 
<a name="l4525"><span class="ln">4525 </span></a>dimensions. It is also required that ``index.size(d) &lt;= src.size(d)`` for all 
<a name="l4526"><span class="ln">4526 </span></a>dimensions ``d``, and that ``index.size(d) &lt;= self.size(d)`` for all dimensions 
<a name="l4527"><span class="ln">4527 </span></a>``d != dim``. Note that ``index`` and ``src`` do not broadcast. 
<a name="l4528"><span class="ln">4528 </span></a> 
<a name="l4529"><span class="ln">4529 </span></a>Note: 
<a name="l4530"><span class="ln">4530 </span></a>    {forward_reproducibility_note} 
<a name="l4531"><span class="ln">4531 </span></a> 
<a name="l4532"><span class="ln">4532 </span></a>.. note:: 
<a name="l4533"><span class="ln">4533 </span></a> 
<a name="l4534"><span class="ln">4534 </span></a>    The backward pass is implemented only for ``src.shape == index.shape``. 
<a name="l4535"><span class="ln">4535 </span></a> 
<a name="l4536"><span class="ln">4536 </span></a>Args: 
<a name="l4537"><span class="ln">4537 </span></a>    dim (int): the axis along which to index 
<a name="l4538"><span class="ln">4538 </span></a>    index (LongTensor): the indices of elements to scatter and add, can be 
<a name="l4539"><span class="ln">4539 </span></a>        either empty or of the same dimensionality as ``src``. When empty, the 
<a name="l4540"><span class="ln">4540 </span></a>        operation returns ``self`` unchanged. 
<a name="l4541"><span class="ln">4541 </span></a>    src (Tensor): the source elements to scatter and add 
<a name="l4542"><span class="ln">4542 </span></a> 
<a name="l4543"><span class="ln">4543 </span></a>Example:: 
<a name="l4544"><span class="ln">4544 </span></a> 
<a name="l4545"><span class="ln">4545 </span></a>    &gt;&gt;&gt; src = torch.ones((2, 5)) 
<a name="l4546"><span class="ln">4546 </span></a>    &gt;&gt;&gt; index = torch.tensor([[0, 1, 2, 0, 0]]) 
<a name="l4547"><span class="ln">4547 </span></a>    &gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src) 
<a name="l4548"><span class="ln">4548 </span></a>    tensor([[1., 0., 0., 1., 1.], 
<a name="l4549"><span class="ln">4549 </span></a>            [0., 1., 0., 0., 0.], 
<a name="l4550"><span class="ln">4550 </span></a>            [0., 0., 1., 0., 0.]]) 
<a name="l4551"><span class="ln">4551 </span></a>    &gt;&gt;&gt; index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]]) 
<a name="l4552"><span class="ln">4552 </span></a>    &gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src) 
<a name="l4553"><span class="ln">4553 </span></a>    tensor([[2., 0., 0., 1., 1.], 
<a name="l4554"><span class="ln">4554 </span></a>            [0., 2., 0., 0., 0.], 
<a name="l4555"><span class="ln">4555 </span></a>            [0., 0., 2., 1., 1.]]) 
<a name="l4556"><span class="ln">4556 </span></a> 
<a name="l4557"><span class="ln">4557 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reproducibility_notes</span><span class="s3">),</span>
<a name="l4558"><span class="ln">4558 </span></a><span class="s3">)</span>
<a name="l4559"><span class="ln">4559 </span></a>
<a name="l4560"><span class="ln">4560 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4561"><span class="ln">4561 </span></a>    <span class="s4">&quot;scatter_reduce_&quot;</span><span class="s3">,</span>
<a name="l4562"><span class="ln">4562 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4563"><span class="ln">4563 </span></a>scatter_reduce_(dim, index, src, reduce, *, include_self=True) -&gt; Tensor 
<a name="l4564"><span class="ln">4564 </span></a> 
<a name="l4565"><span class="ln">4565 </span></a>Reduces all values from the :attr:`src` tensor to the indices specified in 
<a name="l4566"><span class="ln">4566 </span></a>the :attr:`index` tensor in the :attr:`self` tensor using the applied reduction 
<a name="l4567"><span class="ln">4567 </span></a>defined via the :attr:`reduce` argument (:obj:`&quot;sum&quot;`, :obj:`&quot;prod&quot;`, :obj:`&quot;mean&quot;`, 
<a name="l4568"><span class="ln">4568 </span></a>:obj:`&quot;amax&quot;`, :obj:`&quot;amin&quot;`). For each value in :attr:`src`, it is reduced to an 
<a name="l4569"><span class="ln">4569 </span></a>index in :attr:`self` which is specified by its index in :attr:`src` for 
<a name="l4570"><span class="ln">4570 </span></a>``dimension != dim`` and by the corresponding value in :attr:`index` for 
<a name="l4571"><span class="ln">4571 </span></a>``dimension = dim``. If :obj:`include_self=&quot;True&quot;`, the values in the :attr:`self` 
<a name="l4572"><span class="ln">4572 </span></a>tensor are included in the reduction. 
<a name="l4573"><span class="ln">4573 </span></a> 
<a name="l4574"><span class="ln">4574 </span></a>:attr:`self`, :attr:`index` and :attr:`src` should all have 
<a name="l4575"><span class="ln">4575 </span></a>the same number of dimensions. It is also required that 
<a name="l4576"><span class="ln">4576 </span></a>``index.size(d) &lt;= src.size(d)`` for all dimensions ``d``, and that 
<a name="l4577"><span class="ln">4577 </span></a>``index.size(d) &lt;= self.size(d)`` for all dimensions ``d != dim``. 
<a name="l4578"><span class="ln">4578 </span></a>Note that ``index`` and ``src`` do not broadcast. 
<a name="l4579"><span class="ln">4579 </span></a> 
<a name="l4580"><span class="ln">4580 </span></a>For a 3-D tensor with :obj:`reduce=&quot;sum&quot;` and :obj:`include_self=True` the 
<a name="l4581"><span class="ln">4581 </span></a>output is given as:: 
<a name="l4582"><span class="ln">4582 </span></a> 
<a name="l4583"><span class="ln">4583 </span></a>    self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0 
<a name="l4584"><span class="ln">4584 </span></a>    self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1 
<a name="l4585"><span class="ln">4585 </span></a>    self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2 
<a name="l4586"><span class="ln">4586 </span></a> 
<a name="l4587"><span class="ln">4587 </span></a>Note: 
<a name="l4588"><span class="ln">4588 </span></a>    {forward_reproducibility_note} 
<a name="l4589"><span class="ln">4589 </span></a> 
<a name="l4590"><span class="ln">4590 </span></a>.. note:: 
<a name="l4591"><span class="ln">4591 </span></a> 
<a name="l4592"><span class="ln">4592 </span></a>    The backward pass is implemented only for ``src.shape == index.shape``. 
<a name="l4593"><span class="ln">4593 </span></a> 
<a name="l4594"><span class="ln">4594 </span></a>.. warning:: 
<a name="l4595"><span class="ln">4595 </span></a> 
<a name="l4596"><span class="ln">4596 </span></a>    This function is in beta and may change in the near future. 
<a name="l4597"><span class="ln">4597 </span></a> 
<a name="l4598"><span class="ln">4598 </span></a>Args: 
<a name="l4599"><span class="ln">4599 </span></a>    dim (int): the axis along which to index 
<a name="l4600"><span class="ln">4600 </span></a>    index (LongTensor): the indices of elements to scatter and reduce. 
<a name="l4601"><span class="ln">4601 </span></a>    src (Tensor): the source elements to scatter and reduce 
<a name="l4602"><span class="ln">4602 </span></a>    reduce (str): the reduction operation to apply for non-unique indices 
<a name="l4603"><span class="ln">4603 </span></a>        (:obj:`&quot;sum&quot;`, :obj:`&quot;prod&quot;`, :obj:`&quot;mean&quot;`, :obj:`&quot;amax&quot;`, :obj:`&quot;amin&quot;`) 
<a name="l4604"><span class="ln">4604 </span></a>    include_self (bool): whether elements from the :attr:`self` tensor are 
<a name="l4605"><span class="ln">4605 </span></a>        included in the reduction 
<a name="l4606"><span class="ln">4606 </span></a> 
<a name="l4607"><span class="ln">4607 </span></a>Example:: 
<a name="l4608"><span class="ln">4608 </span></a> 
<a name="l4609"><span class="ln">4609 </span></a>    &gt;&gt;&gt; src = torch.tensor([1., 2., 3., 4., 5., 6.]) 
<a name="l4610"><span class="ln">4610 </span></a>    &gt;&gt;&gt; index = torch.tensor([0, 1, 0, 1, 2, 1]) 
<a name="l4611"><span class="ln">4611 </span></a>    &gt;&gt;&gt; input = torch.tensor([1., 2., 3., 4.]) 
<a name="l4612"><span class="ln">4612 </span></a>    &gt;&gt;&gt; input.scatter_reduce(0, index, src, reduce=&quot;sum&quot;) 
<a name="l4613"><span class="ln">4613 </span></a>    tensor([5., 14., 8., 4.]) 
<a name="l4614"><span class="ln">4614 </span></a>    &gt;&gt;&gt; input.scatter_reduce(0, index, src, reduce=&quot;sum&quot;, include_self=False) 
<a name="l4615"><span class="ln">4615 </span></a>    tensor([4., 12., 5., 4.]) 
<a name="l4616"><span class="ln">4616 </span></a>    &gt;&gt;&gt; input2 = torch.tensor([5., 4., 3., 2.]) 
<a name="l4617"><span class="ln">4617 </span></a>    &gt;&gt;&gt; input2.scatter_reduce(0, index, src, reduce=&quot;amax&quot;) 
<a name="l4618"><span class="ln">4618 </span></a>    tensor([5., 6., 5., 2.]) 
<a name="l4619"><span class="ln">4619 </span></a>    &gt;&gt;&gt; input2.scatter_reduce(0, index, src, reduce=&quot;amax&quot;, include_self=False) 
<a name="l4620"><span class="ln">4620 </span></a>    tensor([3., 6., 5., 2.]) 
<a name="l4621"><span class="ln">4621 </span></a> 
<a name="l4622"><span class="ln">4622 </span></a> 
<a name="l4623"><span class="ln">4623 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">reproducibility_notes</span><span class="s3">),</span>
<a name="l4624"><span class="ln">4624 </span></a><span class="s3">)</span>
<a name="l4625"><span class="ln">4625 </span></a>
<a name="l4626"><span class="ln">4626 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4627"><span class="ln">4627 </span></a>    <span class="s4">&quot;select&quot;</span><span class="s3">,</span>
<a name="l4628"><span class="ln">4628 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4629"><span class="ln">4629 </span></a>select(dim, index) -&gt; Tensor 
<a name="l4630"><span class="ln">4630 </span></a> 
<a name="l4631"><span class="ln">4631 </span></a>See :func:`torch.select` 
<a name="l4632"><span class="ln">4632 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4633"><span class="ln">4633 </span></a><span class="s3">)</span>
<a name="l4634"><span class="ln">4634 </span></a>
<a name="l4635"><span class="ln">4635 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4636"><span class="ln">4636 </span></a>    <span class="s4">&quot;select_scatter&quot;</span><span class="s3">,</span>
<a name="l4637"><span class="ln">4637 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4638"><span class="ln">4638 </span></a>select_scatter(src, dim, index) -&gt; Tensor 
<a name="l4639"><span class="ln">4639 </span></a> 
<a name="l4640"><span class="ln">4640 </span></a>See :func:`torch.select_scatter` 
<a name="l4641"><span class="ln">4641 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4642"><span class="ln">4642 </span></a><span class="s3">)</span>
<a name="l4643"><span class="ln">4643 </span></a>
<a name="l4644"><span class="ln">4644 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4645"><span class="ln">4645 </span></a>    <span class="s4">&quot;slice_scatter&quot;</span><span class="s3">,</span>
<a name="l4646"><span class="ln">4646 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4647"><span class="ln">4647 </span></a>slice_scatter(src, dim=0, start=None, end=None, step=1) -&gt; Tensor 
<a name="l4648"><span class="ln">4648 </span></a> 
<a name="l4649"><span class="ln">4649 </span></a>See :func:`torch.slice_scatter` 
<a name="l4650"><span class="ln">4650 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4651"><span class="ln">4651 </span></a><span class="s3">)</span>
<a name="l4652"><span class="ln">4652 </span></a>
<a name="l4653"><span class="ln">4653 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4654"><span class="ln">4654 </span></a>    <span class="s4">&quot;set_&quot;</span><span class="s3">,</span>
<a name="l4655"><span class="ln">4655 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4656"><span class="ln">4656 </span></a>set_(source=None, storage_offset=0, size=None, stride=None) -&gt; Tensor 
<a name="l4657"><span class="ln">4657 </span></a> 
<a name="l4658"><span class="ln">4658 </span></a>Sets the underlying storage, size, and strides. If :attr:`source` is a tensor, 
<a name="l4659"><span class="ln">4659 </span></a>:attr:`self` tensor will share the same storage and have the same size and 
<a name="l4660"><span class="ln">4660 </span></a>strides as :attr:`source`. Changes to elements in one tensor will be reflected 
<a name="l4661"><span class="ln">4661 </span></a>in the other. 
<a name="l4662"><span class="ln">4662 </span></a> 
<a name="l4663"><span class="ln">4663 </span></a>If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying 
<a name="l4664"><span class="ln">4664 </span></a>storage, offset, size, and stride. 
<a name="l4665"><span class="ln">4665 </span></a> 
<a name="l4666"><span class="ln">4666 </span></a>Args: 
<a name="l4667"><span class="ln">4667 </span></a>    source (Tensor or Storage): the tensor or storage to use 
<a name="l4668"><span class="ln">4668 </span></a>    storage_offset (int, optional): the offset in the storage 
<a name="l4669"><span class="ln">4669 </span></a>    size (torch.Size, optional): the desired size. Defaults to the size of the source. 
<a name="l4670"><span class="ln">4670 </span></a>    stride (tuple, optional): the desired stride. Defaults to C-contiguous strides. 
<a name="l4671"><span class="ln">4671 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4672"><span class="ln">4672 </span></a><span class="s3">)</span>
<a name="l4673"><span class="ln">4673 </span></a>
<a name="l4674"><span class="ln">4674 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4675"><span class="ln">4675 </span></a>    <span class="s4">&quot;sigmoid&quot;</span><span class="s3">,</span>
<a name="l4676"><span class="ln">4676 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4677"><span class="ln">4677 </span></a>sigmoid() -&gt; Tensor 
<a name="l4678"><span class="ln">4678 </span></a> 
<a name="l4679"><span class="ln">4679 </span></a>See :func:`torch.sigmoid` 
<a name="l4680"><span class="ln">4680 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4681"><span class="ln">4681 </span></a><span class="s3">)</span>
<a name="l4682"><span class="ln">4682 </span></a>
<a name="l4683"><span class="ln">4683 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4684"><span class="ln">4684 </span></a>    <span class="s4">&quot;sigmoid_&quot;</span><span class="s3">,</span>
<a name="l4685"><span class="ln">4685 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4686"><span class="ln">4686 </span></a>sigmoid_() -&gt; Tensor 
<a name="l4687"><span class="ln">4687 </span></a> 
<a name="l4688"><span class="ln">4688 </span></a>In-place version of :meth:`~Tensor.sigmoid` 
<a name="l4689"><span class="ln">4689 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4690"><span class="ln">4690 </span></a><span class="s3">)</span>
<a name="l4691"><span class="ln">4691 </span></a>
<a name="l4692"><span class="ln">4692 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4693"><span class="ln">4693 </span></a>    <span class="s4">&quot;logit&quot;</span><span class="s3">,</span>
<a name="l4694"><span class="ln">4694 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4695"><span class="ln">4695 </span></a>logit() -&gt; Tensor 
<a name="l4696"><span class="ln">4696 </span></a> 
<a name="l4697"><span class="ln">4697 </span></a>See :func:`torch.logit` 
<a name="l4698"><span class="ln">4698 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4699"><span class="ln">4699 </span></a><span class="s3">)</span>
<a name="l4700"><span class="ln">4700 </span></a>
<a name="l4701"><span class="ln">4701 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4702"><span class="ln">4702 </span></a>    <span class="s4">&quot;logit_&quot;</span><span class="s3">,</span>
<a name="l4703"><span class="ln">4703 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4704"><span class="ln">4704 </span></a>logit_() -&gt; Tensor 
<a name="l4705"><span class="ln">4705 </span></a> 
<a name="l4706"><span class="ln">4706 </span></a>In-place version of :meth:`~Tensor.logit` 
<a name="l4707"><span class="ln">4707 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4708"><span class="ln">4708 </span></a><span class="s3">)</span>
<a name="l4709"><span class="ln">4709 </span></a>
<a name="l4710"><span class="ln">4710 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4711"><span class="ln">4711 </span></a>    <span class="s4">&quot;sign&quot;</span><span class="s3">,</span>
<a name="l4712"><span class="ln">4712 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4713"><span class="ln">4713 </span></a>sign() -&gt; Tensor 
<a name="l4714"><span class="ln">4714 </span></a> 
<a name="l4715"><span class="ln">4715 </span></a>See :func:`torch.sign` 
<a name="l4716"><span class="ln">4716 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4717"><span class="ln">4717 </span></a><span class="s3">)</span>
<a name="l4718"><span class="ln">4718 </span></a>
<a name="l4719"><span class="ln">4719 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4720"><span class="ln">4720 </span></a>    <span class="s4">&quot;sign_&quot;</span><span class="s3">,</span>
<a name="l4721"><span class="ln">4721 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4722"><span class="ln">4722 </span></a>sign_() -&gt; Tensor 
<a name="l4723"><span class="ln">4723 </span></a> 
<a name="l4724"><span class="ln">4724 </span></a>In-place version of :meth:`~Tensor.sign` 
<a name="l4725"><span class="ln">4725 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4726"><span class="ln">4726 </span></a><span class="s3">)</span>
<a name="l4727"><span class="ln">4727 </span></a>
<a name="l4728"><span class="ln">4728 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4729"><span class="ln">4729 </span></a>    <span class="s4">&quot;signbit&quot;</span><span class="s3">,</span>
<a name="l4730"><span class="ln">4730 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4731"><span class="ln">4731 </span></a>signbit() -&gt; Tensor 
<a name="l4732"><span class="ln">4732 </span></a> 
<a name="l4733"><span class="ln">4733 </span></a>See :func:`torch.signbit` 
<a name="l4734"><span class="ln">4734 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4735"><span class="ln">4735 </span></a><span class="s3">)</span>
<a name="l4736"><span class="ln">4736 </span></a>
<a name="l4737"><span class="ln">4737 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4738"><span class="ln">4738 </span></a>    <span class="s4">&quot;sgn&quot;</span><span class="s3">,</span>
<a name="l4739"><span class="ln">4739 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4740"><span class="ln">4740 </span></a>sgn() -&gt; Tensor 
<a name="l4741"><span class="ln">4741 </span></a> 
<a name="l4742"><span class="ln">4742 </span></a>See :func:`torch.sgn` 
<a name="l4743"><span class="ln">4743 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4744"><span class="ln">4744 </span></a><span class="s3">)</span>
<a name="l4745"><span class="ln">4745 </span></a>
<a name="l4746"><span class="ln">4746 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4747"><span class="ln">4747 </span></a>    <span class="s4">&quot;sgn_&quot;</span><span class="s3">,</span>
<a name="l4748"><span class="ln">4748 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4749"><span class="ln">4749 </span></a>sgn_() -&gt; Tensor 
<a name="l4750"><span class="ln">4750 </span></a> 
<a name="l4751"><span class="ln">4751 </span></a>In-place version of :meth:`~Tensor.sgn` 
<a name="l4752"><span class="ln">4752 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4753"><span class="ln">4753 </span></a><span class="s3">)</span>
<a name="l4754"><span class="ln">4754 </span></a>
<a name="l4755"><span class="ln">4755 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4756"><span class="ln">4756 </span></a>    <span class="s4">&quot;sin&quot;</span><span class="s3">,</span>
<a name="l4757"><span class="ln">4757 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4758"><span class="ln">4758 </span></a>sin() -&gt; Tensor 
<a name="l4759"><span class="ln">4759 </span></a> 
<a name="l4760"><span class="ln">4760 </span></a>See :func:`torch.sin` 
<a name="l4761"><span class="ln">4761 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4762"><span class="ln">4762 </span></a><span class="s3">)</span>
<a name="l4763"><span class="ln">4763 </span></a>
<a name="l4764"><span class="ln">4764 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4765"><span class="ln">4765 </span></a>    <span class="s4">&quot;sin_&quot;</span><span class="s3">,</span>
<a name="l4766"><span class="ln">4766 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4767"><span class="ln">4767 </span></a>sin_() -&gt; Tensor 
<a name="l4768"><span class="ln">4768 </span></a> 
<a name="l4769"><span class="ln">4769 </span></a>In-place version of :meth:`~Tensor.sin` 
<a name="l4770"><span class="ln">4770 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4771"><span class="ln">4771 </span></a><span class="s3">)</span>
<a name="l4772"><span class="ln">4772 </span></a>
<a name="l4773"><span class="ln">4773 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4774"><span class="ln">4774 </span></a>    <span class="s4">&quot;sinc&quot;</span><span class="s3">,</span>
<a name="l4775"><span class="ln">4775 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4776"><span class="ln">4776 </span></a>sinc() -&gt; Tensor 
<a name="l4777"><span class="ln">4777 </span></a> 
<a name="l4778"><span class="ln">4778 </span></a>See :func:`torch.sinc` 
<a name="l4779"><span class="ln">4779 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4780"><span class="ln">4780 </span></a><span class="s3">)</span>
<a name="l4781"><span class="ln">4781 </span></a>
<a name="l4782"><span class="ln">4782 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4783"><span class="ln">4783 </span></a>    <span class="s4">&quot;sinc_&quot;</span><span class="s3">,</span>
<a name="l4784"><span class="ln">4784 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4785"><span class="ln">4785 </span></a>sinc_() -&gt; Tensor 
<a name="l4786"><span class="ln">4786 </span></a> 
<a name="l4787"><span class="ln">4787 </span></a>In-place version of :meth:`~Tensor.sinc` 
<a name="l4788"><span class="ln">4788 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4789"><span class="ln">4789 </span></a><span class="s3">)</span>
<a name="l4790"><span class="ln">4790 </span></a>
<a name="l4791"><span class="ln">4791 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4792"><span class="ln">4792 </span></a>    <span class="s4">&quot;sinh&quot;</span><span class="s3">,</span>
<a name="l4793"><span class="ln">4793 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4794"><span class="ln">4794 </span></a>sinh() -&gt; Tensor 
<a name="l4795"><span class="ln">4795 </span></a> 
<a name="l4796"><span class="ln">4796 </span></a>See :func:`torch.sinh` 
<a name="l4797"><span class="ln">4797 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4798"><span class="ln">4798 </span></a><span class="s3">)</span>
<a name="l4799"><span class="ln">4799 </span></a>
<a name="l4800"><span class="ln">4800 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4801"><span class="ln">4801 </span></a>    <span class="s4">&quot;sinh_&quot;</span><span class="s3">,</span>
<a name="l4802"><span class="ln">4802 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4803"><span class="ln">4803 </span></a>sinh_() -&gt; Tensor 
<a name="l4804"><span class="ln">4804 </span></a> 
<a name="l4805"><span class="ln">4805 </span></a>In-place version of :meth:`~Tensor.sinh` 
<a name="l4806"><span class="ln">4806 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4807"><span class="ln">4807 </span></a><span class="s3">)</span>
<a name="l4808"><span class="ln">4808 </span></a>
<a name="l4809"><span class="ln">4809 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4810"><span class="ln">4810 </span></a>    <span class="s4">&quot;size&quot;</span><span class="s3">,</span>
<a name="l4811"><span class="ln">4811 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4812"><span class="ln">4812 </span></a>size(dim=None) -&gt; torch.Size or int 
<a name="l4813"><span class="ln">4813 </span></a> 
<a name="l4814"><span class="ln">4814 </span></a>Returns the size of the :attr:`self` tensor. If ``dim`` is not specified, 
<a name="l4815"><span class="ln">4815 </span></a>the returned value is a :class:`torch.Size`, a subclass of :class:`tuple`. 
<a name="l4816"><span class="ln">4816 </span></a>If ``dim`` is specified, returns an int holding the size of that dimension. 
<a name="l4817"><span class="ln">4817 </span></a> 
<a name="l4818"><span class="ln">4818 </span></a>Args: 
<a name="l4819"><span class="ln">4819 </span></a>  dim (int, optional): The dimension for which to retrieve the size. 
<a name="l4820"><span class="ln">4820 </span></a> 
<a name="l4821"><span class="ln">4821 </span></a>Example:: 
<a name="l4822"><span class="ln">4822 </span></a> 
<a name="l4823"><span class="ln">4823 </span></a>    &gt;&gt;&gt; t = torch.empty(3, 4, 5) 
<a name="l4824"><span class="ln">4824 </span></a>    &gt;&gt;&gt; t.size() 
<a name="l4825"><span class="ln">4825 </span></a>    torch.Size([3, 4, 5]) 
<a name="l4826"><span class="ln">4826 </span></a>    &gt;&gt;&gt; t.size(dim=1) 
<a name="l4827"><span class="ln">4827 </span></a>    4 
<a name="l4828"><span class="ln">4828 </span></a> 
<a name="l4829"><span class="ln">4829 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4830"><span class="ln">4830 </span></a><span class="s3">)</span>
<a name="l4831"><span class="ln">4831 </span></a>
<a name="l4832"><span class="ln">4832 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4833"><span class="ln">4833 </span></a>    <span class="s4">&quot;shape&quot;</span><span class="s3">,</span>
<a name="l4834"><span class="ln">4834 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4835"><span class="ln">4835 </span></a>shape() -&gt; torch.Size 
<a name="l4836"><span class="ln">4836 </span></a> 
<a name="l4837"><span class="ln">4837 </span></a>Returns the size of the :attr:`self` tensor. Alias for :attr:`size`. 
<a name="l4838"><span class="ln">4838 </span></a> 
<a name="l4839"><span class="ln">4839 </span></a>See also :meth:`Tensor.size`. 
<a name="l4840"><span class="ln">4840 </span></a> 
<a name="l4841"><span class="ln">4841 </span></a>Example:: 
<a name="l4842"><span class="ln">4842 </span></a> 
<a name="l4843"><span class="ln">4843 </span></a>    &gt;&gt;&gt; t = torch.empty(3, 4, 5) 
<a name="l4844"><span class="ln">4844 </span></a>    &gt;&gt;&gt; t.size() 
<a name="l4845"><span class="ln">4845 </span></a>    torch.Size([3, 4, 5]) 
<a name="l4846"><span class="ln">4846 </span></a>    &gt;&gt;&gt; t.shape 
<a name="l4847"><span class="ln">4847 </span></a>    torch.Size([3, 4, 5]) 
<a name="l4848"><span class="ln">4848 </span></a> 
<a name="l4849"><span class="ln">4849 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4850"><span class="ln">4850 </span></a><span class="s3">)</span>
<a name="l4851"><span class="ln">4851 </span></a>
<a name="l4852"><span class="ln">4852 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4853"><span class="ln">4853 </span></a>    <span class="s4">&quot;sort&quot;</span><span class="s3">,</span>
<a name="l4854"><span class="ln">4854 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4855"><span class="ln">4855 </span></a>sort(dim=-1, descending=False) -&gt; (Tensor, LongTensor) 
<a name="l4856"><span class="ln">4856 </span></a> 
<a name="l4857"><span class="ln">4857 </span></a>See :func:`torch.sort` 
<a name="l4858"><span class="ln">4858 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4859"><span class="ln">4859 </span></a><span class="s3">)</span>
<a name="l4860"><span class="ln">4860 </span></a>
<a name="l4861"><span class="ln">4861 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4862"><span class="ln">4862 </span></a>    <span class="s4">&quot;msort&quot;</span><span class="s3">,</span>
<a name="l4863"><span class="ln">4863 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4864"><span class="ln">4864 </span></a>msort() -&gt; Tensor 
<a name="l4865"><span class="ln">4865 </span></a> 
<a name="l4866"><span class="ln">4866 </span></a>See :func:`torch.msort` 
<a name="l4867"><span class="ln">4867 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4868"><span class="ln">4868 </span></a><span class="s3">)</span>
<a name="l4869"><span class="ln">4869 </span></a>
<a name="l4870"><span class="ln">4870 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4871"><span class="ln">4871 </span></a>    <span class="s4">&quot;argsort&quot;</span><span class="s3">,</span>
<a name="l4872"><span class="ln">4872 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4873"><span class="ln">4873 </span></a>argsort(dim=-1, descending=False) -&gt; LongTensor 
<a name="l4874"><span class="ln">4874 </span></a> 
<a name="l4875"><span class="ln">4875 </span></a>See :func:`torch.argsort` 
<a name="l4876"><span class="ln">4876 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4877"><span class="ln">4877 </span></a><span class="s3">)</span>
<a name="l4878"><span class="ln">4878 </span></a>
<a name="l4879"><span class="ln">4879 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4880"><span class="ln">4880 </span></a>    <span class="s4">&quot;sparse_dim&quot;</span><span class="s3">,</span>
<a name="l4881"><span class="ln">4881 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4882"><span class="ln">4882 </span></a>sparse_dim() -&gt; int 
<a name="l4883"><span class="ln">4883 </span></a> 
<a name="l4884"><span class="ln">4884 </span></a>Return the number of sparse dimensions in a :ref:`sparse tensor &lt;sparse-docs&gt;` :attr:`self`. 
<a name="l4885"><span class="ln">4885 </span></a> 
<a name="l4886"><span class="ln">4886 </span></a>.. note:: 
<a name="l4887"><span class="ln">4887 </span></a>  Returns ``0`` if :attr:`self` is not a sparse tensor. 
<a name="l4888"><span class="ln">4888 </span></a> 
<a name="l4889"><span class="ln">4889 </span></a>See also :meth:`Tensor.dense_dim` and :ref:`hybrid tensors &lt;sparse-hybrid-coo-docs&gt;`. 
<a name="l4890"><span class="ln">4890 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4891"><span class="ln">4891 </span></a><span class="s3">)</span>
<a name="l4892"><span class="ln">4892 </span></a>
<a name="l4893"><span class="ln">4893 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4894"><span class="ln">4894 </span></a>    <span class="s4">&quot;sparse_resize_&quot;</span><span class="s3">,</span>
<a name="l4895"><span class="ln">4895 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4896"><span class="ln">4896 </span></a>sparse_resize_(size, sparse_dim, dense_dim) -&gt; Tensor 
<a name="l4897"><span class="ln">4897 </span></a> 
<a name="l4898"><span class="ln">4898 </span></a>Resizes :attr:`self` :ref:`sparse tensor &lt;sparse-docs&gt;` to the desired 
<a name="l4899"><span class="ln">4899 </span></a>size and the number of sparse and dense dimensions. 
<a name="l4900"><span class="ln">4900 </span></a> 
<a name="l4901"><span class="ln">4901 </span></a>.. note:: 
<a name="l4902"><span class="ln">4902 </span></a>  If the number of specified elements in :attr:`self` is zero, then 
<a name="l4903"><span class="ln">4903 </span></a>  :attr:`size`, :attr:`sparse_dim`, and :attr:`dense_dim` can be any 
<a name="l4904"><span class="ln">4904 </span></a>  size and positive integers such that ``len(size) == sparse_dim + 
<a name="l4905"><span class="ln">4905 </span></a>  dense_dim``. 
<a name="l4906"><span class="ln">4906 </span></a> 
<a name="l4907"><span class="ln">4907 </span></a>  If :attr:`self` specifies one or more elements, however, then each 
<a name="l4908"><span class="ln">4908 </span></a>  dimension in :attr:`size` must not be smaller than the corresponding 
<a name="l4909"><span class="ln">4909 </span></a>  dimension of :attr:`self`, :attr:`sparse_dim` must equal the number 
<a name="l4910"><span class="ln">4910 </span></a>  of sparse dimensions in :attr:`self`, and :attr:`dense_dim` must 
<a name="l4911"><span class="ln">4911 </span></a>  equal the number of dense dimensions in :attr:`self`. 
<a name="l4912"><span class="ln">4912 </span></a> 
<a name="l4913"><span class="ln">4913 </span></a>.. warning:: 
<a name="l4914"><span class="ln">4914 </span></a>  Throws an error if :attr:`self` is not a sparse tensor. 
<a name="l4915"><span class="ln">4915 </span></a> 
<a name="l4916"><span class="ln">4916 </span></a>Args: 
<a name="l4917"><span class="ln">4917 </span></a>    size (torch.Size): the desired size. If :attr:`self` is non-empty 
<a name="l4918"><span class="ln">4918 </span></a>      sparse tensor, the desired size cannot be smaller than the 
<a name="l4919"><span class="ln">4919 </span></a>      original size. 
<a name="l4920"><span class="ln">4920 </span></a>    sparse_dim (int): the number of sparse dimensions 
<a name="l4921"><span class="ln">4921 </span></a>    dense_dim (int): the number of dense dimensions 
<a name="l4922"><span class="ln">4922 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4923"><span class="ln">4923 </span></a><span class="s3">)</span>
<a name="l4924"><span class="ln">4924 </span></a>
<a name="l4925"><span class="ln">4925 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4926"><span class="ln">4926 </span></a>    <span class="s4">&quot;sparse_resize_and_clear_&quot;</span><span class="s3">,</span>
<a name="l4927"><span class="ln">4927 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4928"><span class="ln">4928 </span></a>sparse_resize_and_clear_(size, sparse_dim, dense_dim) -&gt; Tensor 
<a name="l4929"><span class="ln">4929 </span></a> 
<a name="l4930"><span class="ln">4930 </span></a>Removes all specified elements from a :ref:`sparse tensor 
<a name="l4931"><span class="ln">4931 </span></a>&lt;sparse-docs&gt;` :attr:`self` and resizes :attr:`self` to the desired 
<a name="l4932"><span class="ln">4932 </span></a>size and the number of sparse and dense dimensions. 
<a name="l4933"><span class="ln">4933 </span></a> 
<a name="l4934"><span class="ln">4934 </span></a>.. warning: 
<a name="l4935"><span class="ln">4935 </span></a>  Throws an error if :attr:`self` is not a sparse tensor. 
<a name="l4936"><span class="ln">4936 </span></a> 
<a name="l4937"><span class="ln">4937 </span></a>Args: 
<a name="l4938"><span class="ln">4938 </span></a>    size (torch.Size): the desired size. 
<a name="l4939"><span class="ln">4939 </span></a>    sparse_dim (int): the number of sparse dimensions 
<a name="l4940"><span class="ln">4940 </span></a>    dense_dim (int): the number of dense dimensions 
<a name="l4941"><span class="ln">4941 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4942"><span class="ln">4942 </span></a><span class="s3">)</span>
<a name="l4943"><span class="ln">4943 </span></a>
<a name="l4944"><span class="ln">4944 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4945"><span class="ln">4945 </span></a>    <span class="s4">&quot;sqrt&quot;</span><span class="s3">,</span>
<a name="l4946"><span class="ln">4946 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4947"><span class="ln">4947 </span></a>sqrt() -&gt; Tensor 
<a name="l4948"><span class="ln">4948 </span></a> 
<a name="l4949"><span class="ln">4949 </span></a>See :func:`torch.sqrt` 
<a name="l4950"><span class="ln">4950 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4951"><span class="ln">4951 </span></a><span class="s3">)</span>
<a name="l4952"><span class="ln">4952 </span></a>
<a name="l4953"><span class="ln">4953 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4954"><span class="ln">4954 </span></a>    <span class="s4">&quot;sqrt_&quot;</span><span class="s3">,</span>
<a name="l4955"><span class="ln">4955 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4956"><span class="ln">4956 </span></a>sqrt_() -&gt; Tensor 
<a name="l4957"><span class="ln">4957 </span></a> 
<a name="l4958"><span class="ln">4958 </span></a>In-place version of :meth:`~Tensor.sqrt` 
<a name="l4959"><span class="ln">4959 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4960"><span class="ln">4960 </span></a><span class="s3">)</span>
<a name="l4961"><span class="ln">4961 </span></a>
<a name="l4962"><span class="ln">4962 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4963"><span class="ln">4963 </span></a>    <span class="s4">&quot;square&quot;</span><span class="s3">,</span>
<a name="l4964"><span class="ln">4964 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4965"><span class="ln">4965 </span></a>square() -&gt; Tensor 
<a name="l4966"><span class="ln">4966 </span></a> 
<a name="l4967"><span class="ln">4967 </span></a>See :func:`torch.square` 
<a name="l4968"><span class="ln">4968 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4969"><span class="ln">4969 </span></a><span class="s3">)</span>
<a name="l4970"><span class="ln">4970 </span></a>
<a name="l4971"><span class="ln">4971 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4972"><span class="ln">4972 </span></a>    <span class="s4">&quot;square_&quot;</span><span class="s3">,</span>
<a name="l4973"><span class="ln">4973 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4974"><span class="ln">4974 </span></a>square_() -&gt; Tensor 
<a name="l4975"><span class="ln">4975 </span></a> 
<a name="l4976"><span class="ln">4976 </span></a>In-place version of :meth:`~Tensor.square` 
<a name="l4977"><span class="ln">4977 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4978"><span class="ln">4978 </span></a><span class="s3">)</span>
<a name="l4979"><span class="ln">4979 </span></a>
<a name="l4980"><span class="ln">4980 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4981"><span class="ln">4981 </span></a>    <span class="s4">&quot;squeeze&quot;</span><span class="s3">,</span>
<a name="l4982"><span class="ln">4982 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4983"><span class="ln">4983 </span></a>squeeze(dim=None) -&gt; Tensor 
<a name="l4984"><span class="ln">4984 </span></a> 
<a name="l4985"><span class="ln">4985 </span></a>See :func:`torch.squeeze` 
<a name="l4986"><span class="ln">4986 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4987"><span class="ln">4987 </span></a><span class="s3">)</span>
<a name="l4988"><span class="ln">4988 </span></a>
<a name="l4989"><span class="ln">4989 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4990"><span class="ln">4990 </span></a>    <span class="s4">&quot;squeeze_&quot;</span><span class="s3">,</span>
<a name="l4991"><span class="ln">4991 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l4992"><span class="ln">4992 </span></a>squeeze_(dim=None) -&gt; Tensor 
<a name="l4993"><span class="ln">4993 </span></a> 
<a name="l4994"><span class="ln">4994 </span></a>In-place version of :meth:`~Tensor.squeeze` 
<a name="l4995"><span class="ln">4995 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l4996"><span class="ln">4996 </span></a><span class="s3">)</span>
<a name="l4997"><span class="ln">4997 </span></a>
<a name="l4998"><span class="ln">4998 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l4999"><span class="ln">4999 </span></a>    <span class="s4">&quot;std&quot;</span><span class="s3">,</span>
<a name="l5000"><span class="ln">5000 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5001"><span class="ln">5001 </span></a>std(dim=None, *, correction=1, keepdim=False) -&gt; Tensor 
<a name="l5002"><span class="ln">5002 </span></a> 
<a name="l5003"><span class="ln">5003 </span></a>See :func:`torch.std` 
<a name="l5004"><span class="ln">5004 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5005"><span class="ln">5005 </span></a><span class="s3">)</span>
<a name="l5006"><span class="ln">5006 </span></a>
<a name="l5007"><span class="ln">5007 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5008"><span class="ln">5008 </span></a>    <span class="s4">&quot;storage_offset&quot;</span><span class="s3">,</span>
<a name="l5009"><span class="ln">5009 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5010"><span class="ln">5010 </span></a>storage_offset() -&gt; int 
<a name="l5011"><span class="ln">5011 </span></a> 
<a name="l5012"><span class="ln">5012 </span></a>Returns :attr:`self` tensor's offset in the underlying storage in terms of 
<a name="l5013"><span class="ln">5013 </span></a>number of storage elements (not bytes). 
<a name="l5014"><span class="ln">5014 </span></a> 
<a name="l5015"><span class="ln">5015 </span></a>Example:: 
<a name="l5016"><span class="ln">5016 </span></a> 
<a name="l5017"><span class="ln">5017 </span></a>    &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5]) 
<a name="l5018"><span class="ln">5018 </span></a>    &gt;&gt;&gt; x.storage_offset() 
<a name="l5019"><span class="ln">5019 </span></a>    0 
<a name="l5020"><span class="ln">5020 </span></a>    &gt;&gt;&gt; x[3:].storage_offset() 
<a name="l5021"><span class="ln">5021 </span></a>    3 
<a name="l5022"><span class="ln">5022 </span></a> 
<a name="l5023"><span class="ln">5023 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5024"><span class="ln">5024 </span></a><span class="s3">)</span>
<a name="l5025"><span class="ln">5025 </span></a>
<a name="l5026"><span class="ln">5026 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5027"><span class="ln">5027 </span></a>    <span class="s4">&quot;untyped_storage&quot;</span><span class="s3">,</span>
<a name="l5028"><span class="ln">5028 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5029"><span class="ln">5029 </span></a>untyped_storage() -&gt; torch.UntypedStorage 
<a name="l5030"><span class="ln">5030 </span></a> 
<a name="l5031"><span class="ln">5031 </span></a>Returns the underlying :class:`UntypedStorage`. 
<a name="l5032"><span class="ln">5032 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5033"><span class="ln">5033 </span></a><span class="s3">)</span>
<a name="l5034"><span class="ln">5034 </span></a>
<a name="l5035"><span class="ln">5035 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5036"><span class="ln">5036 </span></a>    <span class="s4">&quot;stride&quot;</span><span class="s3">,</span>
<a name="l5037"><span class="ln">5037 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5038"><span class="ln">5038 </span></a>stride(dim) -&gt; tuple or int 
<a name="l5039"><span class="ln">5039 </span></a> 
<a name="l5040"><span class="ln">5040 </span></a>Returns the stride of :attr:`self` tensor. 
<a name="l5041"><span class="ln">5041 </span></a> 
<a name="l5042"><span class="ln">5042 </span></a>Stride is the jump necessary to go from one element to the next one in the 
<a name="l5043"><span class="ln">5043 </span></a>specified dimension :attr:`dim`. A tuple of all strides is returned when no 
<a name="l5044"><span class="ln">5044 </span></a>argument is passed in. Otherwise, an integer value is returned as the stride in 
<a name="l5045"><span class="ln">5045 </span></a>the particular dimension :attr:`dim`. 
<a name="l5046"><span class="ln">5046 </span></a> 
<a name="l5047"><span class="ln">5047 </span></a>Args: 
<a name="l5048"><span class="ln">5048 </span></a>    dim (int, optional): the desired dimension in which stride is required 
<a name="l5049"><span class="ln">5049 </span></a> 
<a name="l5050"><span class="ln">5050 </span></a>Example:: 
<a name="l5051"><span class="ln">5051 </span></a> 
<a name="l5052"><span class="ln">5052 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) 
<a name="l5053"><span class="ln">5053 </span></a>    &gt;&gt;&gt; x.stride() 
<a name="l5054"><span class="ln">5054 </span></a>    (5, 1) 
<a name="l5055"><span class="ln">5055 </span></a>    &gt;&gt;&gt; x.stride(0) 
<a name="l5056"><span class="ln">5056 </span></a>    5 
<a name="l5057"><span class="ln">5057 </span></a>    &gt;&gt;&gt; x.stride(-1) 
<a name="l5058"><span class="ln">5058 </span></a>    1 
<a name="l5059"><span class="ln">5059 </span></a> 
<a name="l5060"><span class="ln">5060 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5061"><span class="ln">5061 </span></a><span class="s3">)</span>
<a name="l5062"><span class="ln">5062 </span></a>
<a name="l5063"><span class="ln">5063 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5064"><span class="ln">5064 </span></a>    <span class="s4">&quot;sub&quot;</span><span class="s3">,</span>
<a name="l5065"><span class="ln">5065 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5066"><span class="ln">5066 </span></a>sub(other, *, alpha=1) -&gt; Tensor 
<a name="l5067"><span class="ln">5067 </span></a> 
<a name="l5068"><span class="ln">5068 </span></a>See :func:`torch.sub`. 
<a name="l5069"><span class="ln">5069 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5070"><span class="ln">5070 </span></a><span class="s3">)</span>
<a name="l5071"><span class="ln">5071 </span></a>
<a name="l5072"><span class="ln">5072 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5073"><span class="ln">5073 </span></a>    <span class="s4">&quot;sub_&quot;</span><span class="s3">,</span>
<a name="l5074"><span class="ln">5074 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5075"><span class="ln">5075 </span></a>sub_(other, *, alpha=1) -&gt; Tensor 
<a name="l5076"><span class="ln">5076 </span></a> 
<a name="l5077"><span class="ln">5077 </span></a>In-place version of :meth:`~Tensor.sub` 
<a name="l5078"><span class="ln">5078 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5079"><span class="ln">5079 </span></a><span class="s3">)</span>
<a name="l5080"><span class="ln">5080 </span></a>
<a name="l5081"><span class="ln">5081 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5082"><span class="ln">5082 </span></a>    <span class="s4">&quot;subtract&quot;</span><span class="s3">,</span>
<a name="l5083"><span class="ln">5083 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5084"><span class="ln">5084 </span></a>subtract(other, *, alpha=1) -&gt; Tensor 
<a name="l5085"><span class="ln">5085 </span></a> 
<a name="l5086"><span class="ln">5086 </span></a>See :func:`torch.subtract`. 
<a name="l5087"><span class="ln">5087 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5088"><span class="ln">5088 </span></a><span class="s3">)</span>
<a name="l5089"><span class="ln">5089 </span></a>
<a name="l5090"><span class="ln">5090 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5091"><span class="ln">5091 </span></a>    <span class="s4">&quot;subtract_&quot;</span><span class="s3">,</span>
<a name="l5092"><span class="ln">5092 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5093"><span class="ln">5093 </span></a>subtract_(other, *, alpha=1) -&gt; Tensor 
<a name="l5094"><span class="ln">5094 </span></a> 
<a name="l5095"><span class="ln">5095 </span></a>In-place version of :meth:`~Tensor.subtract`. 
<a name="l5096"><span class="ln">5096 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5097"><span class="ln">5097 </span></a><span class="s3">)</span>
<a name="l5098"><span class="ln">5098 </span></a>
<a name="l5099"><span class="ln">5099 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5100"><span class="ln">5100 </span></a>    <span class="s4">&quot;sum&quot;</span><span class="s3">,</span>
<a name="l5101"><span class="ln">5101 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5102"><span class="ln">5102 </span></a>sum(dim=None, keepdim=False, dtype=None) -&gt; Tensor 
<a name="l5103"><span class="ln">5103 </span></a> 
<a name="l5104"><span class="ln">5104 </span></a>See :func:`torch.sum` 
<a name="l5105"><span class="ln">5105 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5106"><span class="ln">5106 </span></a><span class="s3">)</span>
<a name="l5107"><span class="ln">5107 </span></a>
<a name="l5108"><span class="ln">5108 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5109"><span class="ln">5109 </span></a>    <span class="s4">&quot;nansum&quot;</span><span class="s3">,</span>
<a name="l5110"><span class="ln">5110 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5111"><span class="ln">5111 </span></a>nansum(dim=None, keepdim=False, dtype=None) -&gt; Tensor 
<a name="l5112"><span class="ln">5112 </span></a> 
<a name="l5113"><span class="ln">5113 </span></a>See :func:`torch.nansum` 
<a name="l5114"><span class="ln">5114 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5115"><span class="ln">5115 </span></a><span class="s3">)</span>
<a name="l5116"><span class="ln">5116 </span></a>
<a name="l5117"><span class="ln">5117 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5118"><span class="ln">5118 </span></a>    <span class="s4">&quot;svd&quot;</span><span class="s3">,</span>
<a name="l5119"><span class="ln">5119 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5120"><span class="ln">5120 </span></a>svd(some=True, compute_uv=True) -&gt; (Tensor, Tensor, Tensor) 
<a name="l5121"><span class="ln">5121 </span></a> 
<a name="l5122"><span class="ln">5122 </span></a>See :func:`torch.svd` 
<a name="l5123"><span class="ln">5123 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5124"><span class="ln">5124 </span></a><span class="s3">)</span>
<a name="l5125"><span class="ln">5125 </span></a>
<a name="l5126"><span class="ln">5126 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5127"><span class="ln">5127 </span></a>    <span class="s4">&quot;swapdims&quot;</span><span class="s3">,</span>
<a name="l5128"><span class="ln">5128 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5129"><span class="ln">5129 </span></a>swapdims(dim0, dim1) -&gt; Tensor 
<a name="l5130"><span class="ln">5130 </span></a> 
<a name="l5131"><span class="ln">5131 </span></a>See :func:`torch.swapdims` 
<a name="l5132"><span class="ln">5132 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5133"><span class="ln">5133 </span></a><span class="s3">)</span>
<a name="l5134"><span class="ln">5134 </span></a>
<a name="l5135"><span class="ln">5135 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5136"><span class="ln">5136 </span></a>    <span class="s4">&quot;swapdims_&quot;</span><span class="s3">,</span>
<a name="l5137"><span class="ln">5137 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5138"><span class="ln">5138 </span></a>swapdims_(dim0, dim1) -&gt; Tensor 
<a name="l5139"><span class="ln">5139 </span></a> 
<a name="l5140"><span class="ln">5140 </span></a>In-place version of :meth:`~Tensor.swapdims` 
<a name="l5141"><span class="ln">5141 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5142"><span class="ln">5142 </span></a><span class="s3">)</span>
<a name="l5143"><span class="ln">5143 </span></a>
<a name="l5144"><span class="ln">5144 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5145"><span class="ln">5145 </span></a>    <span class="s4">&quot;swapaxes&quot;</span><span class="s3">,</span>
<a name="l5146"><span class="ln">5146 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5147"><span class="ln">5147 </span></a>swapaxes(axis0, axis1) -&gt; Tensor 
<a name="l5148"><span class="ln">5148 </span></a> 
<a name="l5149"><span class="ln">5149 </span></a>See :func:`torch.swapaxes` 
<a name="l5150"><span class="ln">5150 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5151"><span class="ln">5151 </span></a><span class="s3">)</span>
<a name="l5152"><span class="ln">5152 </span></a>
<a name="l5153"><span class="ln">5153 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5154"><span class="ln">5154 </span></a>    <span class="s4">&quot;swapaxes_&quot;</span><span class="s3">,</span>
<a name="l5155"><span class="ln">5155 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5156"><span class="ln">5156 </span></a>swapaxes_(axis0, axis1) -&gt; Tensor 
<a name="l5157"><span class="ln">5157 </span></a> 
<a name="l5158"><span class="ln">5158 </span></a>In-place version of :meth:`~Tensor.swapaxes` 
<a name="l5159"><span class="ln">5159 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5160"><span class="ln">5160 </span></a><span class="s3">)</span>
<a name="l5161"><span class="ln">5161 </span></a>
<a name="l5162"><span class="ln">5162 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5163"><span class="ln">5163 </span></a>    <span class="s4">&quot;t&quot;</span><span class="s3">,</span>
<a name="l5164"><span class="ln">5164 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5165"><span class="ln">5165 </span></a>t() -&gt; Tensor 
<a name="l5166"><span class="ln">5166 </span></a> 
<a name="l5167"><span class="ln">5167 </span></a>See :func:`torch.t` 
<a name="l5168"><span class="ln">5168 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5169"><span class="ln">5169 </span></a><span class="s3">)</span>
<a name="l5170"><span class="ln">5170 </span></a>
<a name="l5171"><span class="ln">5171 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5172"><span class="ln">5172 </span></a>    <span class="s4">&quot;t_&quot;</span><span class="s3">,</span>
<a name="l5173"><span class="ln">5173 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5174"><span class="ln">5174 </span></a>t_() -&gt; Tensor 
<a name="l5175"><span class="ln">5175 </span></a> 
<a name="l5176"><span class="ln">5176 </span></a>In-place version of :meth:`~Tensor.t` 
<a name="l5177"><span class="ln">5177 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5178"><span class="ln">5178 </span></a><span class="s3">)</span>
<a name="l5179"><span class="ln">5179 </span></a>
<a name="l5180"><span class="ln">5180 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5181"><span class="ln">5181 </span></a>    <span class="s4">&quot;tile&quot;</span><span class="s3">,</span>
<a name="l5182"><span class="ln">5182 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5183"><span class="ln">5183 </span></a>tile(dims) -&gt; Tensor 
<a name="l5184"><span class="ln">5184 </span></a> 
<a name="l5185"><span class="ln">5185 </span></a>See :func:`torch.tile` 
<a name="l5186"><span class="ln">5186 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5187"><span class="ln">5187 </span></a><span class="s3">)</span>
<a name="l5188"><span class="ln">5188 </span></a>
<a name="l5189"><span class="ln">5189 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5190"><span class="ln">5190 </span></a>    <span class="s4">&quot;to&quot;</span><span class="s3">,</span>
<a name="l5191"><span class="ln">5191 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5192"><span class="ln">5192 </span></a>to(*args, **kwargs) -&gt; Tensor 
<a name="l5193"><span class="ln">5193 </span></a> 
<a name="l5194"><span class="ln">5194 </span></a>Performs Tensor dtype and/or device conversion. A :class:`torch.dtype` and :class:`torch.device` are 
<a name="l5195"><span class="ln">5195 </span></a>inferred from the arguments of ``self.to(*args, **kwargs)``. 
<a name="l5196"><span class="ln">5196 </span></a> 
<a name="l5197"><span class="ln">5197 </span></a>.. note:: 
<a name="l5198"><span class="ln">5198 </span></a> 
<a name="l5199"><span class="ln">5199 </span></a>    If the ``self`` Tensor already 
<a name="l5200"><span class="ln">5200 </span></a>    has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned. 
<a name="l5201"><span class="ln">5201 </span></a>    Otherwise, the returned tensor is a copy of ``self`` with the desired 
<a name="l5202"><span class="ln">5202 </span></a>    :class:`torch.dtype` and :class:`torch.device`. 
<a name="l5203"><span class="ln">5203 </span></a> 
<a name="l5204"><span class="ln">5204 </span></a>.. note:: 
<a name="l5205"><span class="ln">5205 </span></a> 
<a name="l5206"><span class="ln">5206 </span></a>    If ``self`` requires gradients (``requires_grad=True``) but the target 
<a name="l5207"><span class="ln">5207 </span></a>    ``dtype`` specified is an integer type, the returned tensor will implicitly 
<a name="l5208"><span class="ln">5208 </span></a>    set ``requires_grad=False``. This is because only tensors with 
<a name="l5209"><span class="ln">5209 </span></a>    floating-point or complex dtypes can require gradients. 
<a name="l5210"><span class="ln">5210 </span></a> 
<a name="l5211"><span class="ln">5211 </span></a>Here are the ways to call ``to``: 
<a name="l5212"><span class="ln">5212 </span></a> 
<a name="l5213"><span class="ln">5213 </span></a>.. method:: to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5214"><span class="ln">5214 </span></a>   :noindex: 
<a name="l5215"><span class="ln">5215 </span></a> 
<a name="l5216"><span class="ln">5216 </span></a>    Returns a Tensor with the specified :attr:`dtype` 
<a name="l5217"><span class="ln">5217 </span></a> 
<a name="l5218"><span class="ln">5218 </span></a>    Args: 
<a name="l5219"><span class="ln">5219 </span></a>        {memory_format} 
<a name="l5220"><span class="ln">5220 </span></a> 
<a name="l5221"><span class="ln">5221 </span></a>.. note:: 
<a name="l5222"><span class="ln">5222 </span></a> 
<a name="l5223"><span class="ln">5223 </span></a>    According to `C++ type conversion rules &lt;https://en.cppreference.com/w/cpp/language/implicit_conversion.html&gt;`_, 
<a name="l5224"><span class="ln">5224 </span></a>    converting floating point value to integer type will truncate the fractional part. 
<a name="l5225"><span class="ln">5225 </span></a>    If the truncated value cannot fit into the target type (e.g., casting ``torch.inf`` to ``torch.long``), 
<a name="l5226"><span class="ln">5226 </span></a>    the behavior is undefined and the result may vary across platforms. 
<a name="l5227"><span class="ln">5227 </span></a> 
<a name="l5228"><span class="ln">5228 </span></a>.. method:: to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5229"><span class="ln">5229 </span></a>   :noindex: 
<a name="l5230"><span class="ln">5230 </span></a> 
<a name="l5231"><span class="ln">5231 </span></a>    Returns a Tensor with the specified :attr:`device` and (optional) 
<a name="l5232"><span class="ln">5232 </span></a>    :attr:`dtype`. If :attr:`dtype` is ``None`` it is inferred to be ``self.dtype``. 
<a name="l5233"><span class="ln">5233 </span></a>    When :attr:`non_blocking` is set to ``True``, the function attempts to perform 
<a name="l5234"><span class="ln">5234 </span></a>    the conversion asynchronously with respect to the host, if possible. This 
<a name="l5235"><span class="ln">5235 </span></a>    asynchronous behavior applies to both pinned and pageable memory. However, 
<a name="l5236"><span class="ln">5236 </span></a>    caution is advised when using this feature. For more information, refer to the 
<a name="l5237"><span class="ln">5237 </span></a>    `tutorial on good usage of non_blocking and pin_memory &lt;https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html&gt;`__. 
<a name="l5238"><span class="ln">5238 </span></a>    When :attr:`copy` is set, a new Tensor is created even when the Tensor 
<a name="l5239"><span class="ln">5239 </span></a>    already matches the desired conversion. 
<a name="l5240"><span class="ln">5240 </span></a> 
<a name="l5241"><span class="ln">5241 </span></a>    Args: 
<a name="l5242"><span class="ln">5242 </span></a>        {memory_format} 
<a name="l5243"><span class="ln">5243 </span></a> 
<a name="l5244"><span class="ln">5244 </span></a>.. method:: to(other, non_blocking=False, copy=False) -&gt; Tensor 
<a name="l5245"><span class="ln">5245 </span></a>   :noindex: 
<a name="l5246"><span class="ln">5246 </span></a> 
<a name="l5247"><span class="ln">5247 </span></a>    Returns a Tensor with same :class:`torch.dtype` and :class:`torch.device` as 
<a name="l5248"><span class="ln">5248 </span></a>    the Tensor :attr:`other`. 
<a name="l5249"><span class="ln">5249 </span></a>    When :attr:`non_blocking` is set to ``True``, the function attempts to perform 
<a name="l5250"><span class="ln">5250 </span></a>    the conversion asynchronously with respect to the host, if possible. This 
<a name="l5251"><span class="ln">5251 </span></a>    asynchronous behavior applies to both pinned and pageable memory. However, 
<a name="l5252"><span class="ln">5252 </span></a>    caution is advised when using this feature. For more information, refer to the 
<a name="l5253"><span class="ln">5253 </span></a>    `tutorial on good usage of non_blocking and pin_memory &lt;https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html&gt;`__. 
<a name="l5254"><span class="ln">5254 </span></a>    When :attr:`copy` is set, a new Tensor is created even when the Tensor 
<a name="l5255"><span class="ln">5255 </span></a>    already matches the desired conversion. 
<a name="l5256"><span class="ln">5256 </span></a> 
<a name="l5257"><span class="ln">5257 </span></a>Example:: 
<a name="l5258"><span class="ln">5258 </span></a> 
<a name="l5259"><span class="ln">5259 </span></a>    &gt;&gt;&gt; tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu 
<a name="l5260"><span class="ln">5260 </span></a>    &gt;&gt;&gt; tensor.to(torch.float64) 
<a name="l5261"><span class="ln">5261 </span></a>    tensor([[-0.5044,  0.0005], 
<a name="l5262"><span class="ln">5262 </span></a>            [ 0.3310, -0.0584]], dtype=torch.float64) 
<a name="l5263"><span class="ln">5263 </span></a> 
<a name="l5264"><span class="ln">5264 </span></a>    &gt;&gt;&gt; cuda0 = torch.device('cuda:0') 
<a name="l5265"><span class="ln">5265 </span></a>    &gt;&gt;&gt; tensor.to(cuda0) 
<a name="l5266"><span class="ln">5266 </span></a>    tensor([[-0.5044,  0.0005], 
<a name="l5267"><span class="ln">5267 </span></a>            [ 0.3310, -0.0584]], device='cuda:0') 
<a name="l5268"><span class="ln">5268 </span></a> 
<a name="l5269"><span class="ln">5269 </span></a>    &gt;&gt;&gt; tensor.to(cuda0, dtype=torch.float64) 
<a name="l5270"><span class="ln">5270 </span></a>    tensor([[-0.5044,  0.0005], 
<a name="l5271"><span class="ln">5271 </span></a>            [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0') 
<a name="l5272"><span class="ln">5272 </span></a> 
<a name="l5273"><span class="ln">5273 </span></a>    &gt;&gt;&gt; other = torch.randn((), dtype=torch.float64, device=cuda0) 
<a name="l5274"><span class="ln">5274 </span></a>    &gt;&gt;&gt; tensor.to(other, non_blocking=True) 
<a name="l5275"><span class="ln">5275 </span></a>    tensor([[-0.5044,  0.0005], 
<a name="l5276"><span class="ln">5276 </span></a>            [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0') 
<a name="l5277"><span class="ln">5277 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5278"><span class="ln">5278 </span></a><span class="s3">)</span>
<a name="l5279"><span class="ln">5279 </span></a>
<a name="l5280"><span class="ln">5280 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5281"><span class="ln">5281 </span></a>    <span class="s4">&quot;byte&quot;</span><span class="s3">,</span>
<a name="l5282"><span class="ln">5282 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5283"><span class="ln">5283 </span></a>byte(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5284"><span class="ln">5284 </span></a> 
<a name="l5285"><span class="ln">5285 </span></a>``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`. 
<a name="l5286"><span class="ln">5286 </span></a> 
<a name="l5287"><span class="ln">5287 </span></a>Args: 
<a name="l5288"><span class="ln">5288 </span></a>    {memory_format} 
<a name="l5289"><span class="ln">5289 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5290"><span class="ln">5290 </span></a><span class="s3">)</span>
<a name="l5291"><span class="ln">5291 </span></a>
<a name="l5292"><span class="ln">5292 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5293"><span class="ln">5293 </span></a>    <span class="s4">&quot;bool&quot;</span><span class="s3">,</span>
<a name="l5294"><span class="ln">5294 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5295"><span class="ln">5295 </span></a>bool(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5296"><span class="ln">5296 </span></a> 
<a name="l5297"><span class="ln">5297 </span></a>``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`. 
<a name="l5298"><span class="ln">5298 </span></a> 
<a name="l5299"><span class="ln">5299 </span></a>Args: 
<a name="l5300"><span class="ln">5300 </span></a>    {memory_format} 
<a name="l5301"><span class="ln">5301 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5302"><span class="ln">5302 </span></a><span class="s3">)</span>
<a name="l5303"><span class="ln">5303 </span></a>
<a name="l5304"><span class="ln">5304 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5305"><span class="ln">5305 </span></a>    <span class="s4">&quot;char&quot;</span><span class="s3">,</span>
<a name="l5306"><span class="ln">5306 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5307"><span class="ln">5307 </span></a>char(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5308"><span class="ln">5308 </span></a> 
<a name="l5309"><span class="ln">5309 </span></a>``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`. 
<a name="l5310"><span class="ln">5310 </span></a> 
<a name="l5311"><span class="ln">5311 </span></a>Args: 
<a name="l5312"><span class="ln">5312 </span></a>    {memory_format} 
<a name="l5313"><span class="ln">5313 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5314"><span class="ln">5314 </span></a><span class="s3">)</span>
<a name="l5315"><span class="ln">5315 </span></a>
<a name="l5316"><span class="ln">5316 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5317"><span class="ln">5317 </span></a>    <span class="s4">&quot;bfloat16&quot;</span><span class="s3">,</span>
<a name="l5318"><span class="ln">5318 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5319"><span class="ln">5319 </span></a>bfloat16(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5320"><span class="ln">5320 </span></a>``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`. 
<a name="l5321"><span class="ln">5321 </span></a> 
<a name="l5322"><span class="ln">5322 </span></a>Args: 
<a name="l5323"><span class="ln">5323 </span></a>    {memory_format} 
<a name="l5324"><span class="ln">5324 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5325"><span class="ln">5325 </span></a><span class="s3">)</span>
<a name="l5326"><span class="ln">5326 </span></a>
<a name="l5327"><span class="ln">5327 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5328"><span class="ln">5328 </span></a>    <span class="s4">&quot;double&quot;</span><span class="s3">,</span>
<a name="l5329"><span class="ln">5329 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5330"><span class="ln">5330 </span></a>double(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5331"><span class="ln">5331 </span></a> 
<a name="l5332"><span class="ln">5332 </span></a>``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`. 
<a name="l5333"><span class="ln">5333 </span></a> 
<a name="l5334"><span class="ln">5334 </span></a>Args: 
<a name="l5335"><span class="ln">5335 </span></a>    {memory_format} 
<a name="l5336"><span class="ln">5336 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5337"><span class="ln">5337 </span></a><span class="s3">)</span>
<a name="l5338"><span class="ln">5338 </span></a>
<a name="l5339"><span class="ln">5339 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5340"><span class="ln">5340 </span></a>    <span class="s4">&quot;float&quot;</span><span class="s3">,</span>
<a name="l5341"><span class="ln">5341 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5342"><span class="ln">5342 </span></a>float(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5343"><span class="ln">5343 </span></a> 
<a name="l5344"><span class="ln">5344 </span></a>``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`. 
<a name="l5345"><span class="ln">5345 </span></a> 
<a name="l5346"><span class="ln">5346 </span></a>Args: 
<a name="l5347"><span class="ln">5347 </span></a>    {memory_format} 
<a name="l5348"><span class="ln">5348 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5349"><span class="ln">5349 </span></a><span class="s3">)</span>
<a name="l5350"><span class="ln">5350 </span></a>
<a name="l5351"><span class="ln">5351 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5352"><span class="ln">5352 </span></a>    <span class="s4">&quot;cdouble&quot;</span><span class="s3">,</span>
<a name="l5353"><span class="ln">5353 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5354"><span class="ln">5354 </span></a>cdouble(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5355"><span class="ln">5355 </span></a> 
<a name="l5356"><span class="ln">5356 </span></a>``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`. 
<a name="l5357"><span class="ln">5357 </span></a> 
<a name="l5358"><span class="ln">5358 </span></a>Args: 
<a name="l5359"><span class="ln">5359 </span></a>    {memory_format} 
<a name="l5360"><span class="ln">5360 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5361"><span class="ln">5361 </span></a><span class="s3">)</span>
<a name="l5362"><span class="ln">5362 </span></a>
<a name="l5363"><span class="ln">5363 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5364"><span class="ln">5364 </span></a>    <span class="s4">&quot;cfloat&quot;</span><span class="s3">,</span>
<a name="l5365"><span class="ln">5365 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5366"><span class="ln">5366 </span></a>cfloat(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5367"><span class="ln">5367 </span></a> 
<a name="l5368"><span class="ln">5368 </span></a>``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`. 
<a name="l5369"><span class="ln">5369 </span></a> 
<a name="l5370"><span class="ln">5370 </span></a>Args: 
<a name="l5371"><span class="ln">5371 </span></a>    {memory_format} 
<a name="l5372"><span class="ln">5372 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5373"><span class="ln">5373 </span></a><span class="s3">)</span>
<a name="l5374"><span class="ln">5374 </span></a>
<a name="l5375"><span class="ln">5375 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5376"><span class="ln">5376 </span></a>    <span class="s4">&quot;chalf&quot;</span><span class="s3">,</span>
<a name="l5377"><span class="ln">5377 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5378"><span class="ln">5378 </span></a>chalf(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5379"><span class="ln">5379 </span></a> 
<a name="l5380"><span class="ln">5380 </span></a>``self.chalf()`` is equivalent to ``self.to(torch.complex32)``. See :func:`to`. 
<a name="l5381"><span class="ln">5381 </span></a> 
<a name="l5382"><span class="ln">5382 </span></a>Args: 
<a name="l5383"><span class="ln">5383 </span></a>     {memory_format} 
<a name="l5384"><span class="ln">5384 </span></a> &quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5385"><span class="ln">5385 </span></a><span class="s3">)</span>
<a name="l5386"><span class="ln">5386 </span></a>
<a name="l5387"><span class="ln">5387 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5388"><span class="ln">5388 </span></a>    <span class="s4">&quot;half&quot;</span><span class="s3">,</span>
<a name="l5389"><span class="ln">5389 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5390"><span class="ln">5390 </span></a>half(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5391"><span class="ln">5391 </span></a> 
<a name="l5392"><span class="ln">5392 </span></a>``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`. 
<a name="l5393"><span class="ln">5393 </span></a> 
<a name="l5394"><span class="ln">5394 </span></a>Args: 
<a name="l5395"><span class="ln">5395 </span></a>    {memory_format} 
<a name="l5396"><span class="ln">5396 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5397"><span class="ln">5397 </span></a><span class="s3">)</span>
<a name="l5398"><span class="ln">5398 </span></a>
<a name="l5399"><span class="ln">5399 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5400"><span class="ln">5400 </span></a>    <span class="s4">&quot;int&quot;</span><span class="s3">,</span>
<a name="l5401"><span class="ln">5401 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5402"><span class="ln">5402 </span></a>int(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5403"><span class="ln">5403 </span></a> 
<a name="l5404"><span class="ln">5404 </span></a>``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`. 
<a name="l5405"><span class="ln">5405 </span></a> 
<a name="l5406"><span class="ln">5406 </span></a>Args: 
<a name="l5407"><span class="ln">5407 </span></a>    {memory_format} 
<a name="l5408"><span class="ln">5408 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5409"><span class="ln">5409 </span></a><span class="s3">)</span>
<a name="l5410"><span class="ln">5410 </span></a>
<a name="l5411"><span class="ln">5411 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5412"><span class="ln">5412 </span></a>    <span class="s4">&quot;int_repr&quot;</span><span class="s3">,</span>
<a name="l5413"><span class="ln">5413 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5414"><span class="ln">5414 </span></a>int_repr() -&gt; Tensor 
<a name="l5415"><span class="ln">5415 </span></a> 
<a name="l5416"><span class="ln">5416 </span></a>Given a quantized Tensor, 
<a name="l5417"><span class="ln">5417 </span></a>``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the 
<a name="l5418"><span class="ln">5418 </span></a>underlying uint8_t values of the given Tensor. 
<a name="l5419"><span class="ln">5419 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5420"><span class="ln">5420 </span></a><span class="s3">)</span>
<a name="l5421"><span class="ln">5421 </span></a>
<a name="l5422"><span class="ln">5422 </span></a>
<a name="l5423"><span class="ln">5423 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5424"><span class="ln">5424 </span></a>    <span class="s4">&quot;long&quot;</span><span class="s3">,</span>
<a name="l5425"><span class="ln">5425 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5426"><span class="ln">5426 </span></a>long(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5427"><span class="ln">5427 </span></a> 
<a name="l5428"><span class="ln">5428 </span></a>``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`. 
<a name="l5429"><span class="ln">5429 </span></a> 
<a name="l5430"><span class="ln">5430 </span></a>Args: 
<a name="l5431"><span class="ln">5431 </span></a>    {memory_format} 
<a name="l5432"><span class="ln">5432 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5433"><span class="ln">5433 </span></a><span class="s3">)</span>
<a name="l5434"><span class="ln">5434 </span></a>
<a name="l5435"><span class="ln">5435 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5436"><span class="ln">5436 </span></a>    <span class="s4">&quot;short&quot;</span><span class="s3">,</span>
<a name="l5437"><span class="ln">5437 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5438"><span class="ln">5438 </span></a>short(memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l5439"><span class="ln">5439 </span></a> 
<a name="l5440"><span class="ln">5440 </span></a>``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`. 
<a name="l5441"><span class="ln">5441 </span></a> 
<a name="l5442"><span class="ln">5442 </span></a>Args: 
<a name="l5443"><span class="ln">5443 </span></a>    {memory_format} 
<a name="l5444"><span class="ln">5444 </span></a>&quot;&quot;&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s2">**</span><span class="s1">common_args</span><span class="s3">),</span>
<a name="l5445"><span class="ln">5445 </span></a><span class="s3">)</span>
<a name="l5446"><span class="ln">5446 </span></a>
<a name="l5447"><span class="ln">5447 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5448"><span class="ln">5448 </span></a>    <span class="s4">&quot;take&quot;</span><span class="s3">,</span>
<a name="l5449"><span class="ln">5449 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5450"><span class="ln">5450 </span></a>take(indices) -&gt; Tensor 
<a name="l5451"><span class="ln">5451 </span></a> 
<a name="l5452"><span class="ln">5452 </span></a>See :func:`torch.take` 
<a name="l5453"><span class="ln">5453 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5454"><span class="ln">5454 </span></a><span class="s3">)</span>
<a name="l5455"><span class="ln">5455 </span></a>
<a name="l5456"><span class="ln">5456 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5457"><span class="ln">5457 </span></a>    <span class="s4">&quot;take_along_dim&quot;</span><span class="s3">,</span>
<a name="l5458"><span class="ln">5458 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5459"><span class="ln">5459 </span></a>take_along_dim(indices, dim) -&gt; Tensor 
<a name="l5460"><span class="ln">5460 </span></a> 
<a name="l5461"><span class="ln">5461 </span></a>See :func:`torch.take_along_dim` 
<a name="l5462"><span class="ln">5462 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5463"><span class="ln">5463 </span></a><span class="s3">)</span>
<a name="l5464"><span class="ln">5464 </span></a>
<a name="l5465"><span class="ln">5465 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5466"><span class="ln">5466 </span></a>    <span class="s4">&quot;tan&quot;</span><span class="s3">,</span>
<a name="l5467"><span class="ln">5467 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5468"><span class="ln">5468 </span></a>tan() -&gt; Tensor 
<a name="l5469"><span class="ln">5469 </span></a> 
<a name="l5470"><span class="ln">5470 </span></a>See :func:`torch.tan` 
<a name="l5471"><span class="ln">5471 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5472"><span class="ln">5472 </span></a><span class="s3">)</span>
<a name="l5473"><span class="ln">5473 </span></a>
<a name="l5474"><span class="ln">5474 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5475"><span class="ln">5475 </span></a>    <span class="s4">&quot;tan_&quot;</span><span class="s3">,</span>
<a name="l5476"><span class="ln">5476 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5477"><span class="ln">5477 </span></a>tan_() -&gt; Tensor 
<a name="l5478"><span class="ln">5478 </span></a> 
<a name="l5479"><span class="ln">5479 </span></a>In-place version of :meth:`~Tensor.tan` 
<a name="l5480"><span class="ln">5480 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5481"><span class="ln">5481 </span></a><span class="s3">)</span>
<a name="l5482"><span class="ln">5482 </span></a>
<a name="l5483"><span class="ln">5483 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5484"><span class="ln">5484 </span></a>    <span class="s4">&quot;tanh&quot;</span><span class="s3">,</span>
<a name="l5485"><span class="ln">5485 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5486"><span class="ln">5486 </span></a>tanh() -&gt; Tensor 
<a name="l5487"><span class="ln">5487 </span></a> 
<a name="l5488"><span class="ln">5488 </span></a>See :func:`torch.tanh` 
<a name="l5489"><span class="ln">5489 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5490"><span class="ln">5490 </span></a><span class="s3">)</span>
<a name="l5491"><span class="ln">5491 </span></a>
<a name="l5492"><span class="ln">5492 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5493"><span class="ln">5493 </span></a>    <span class="s4">&quot;softmax&quot;</span><span class="s3">,</span>
<a name="l5494"><span class="ln">5494 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5495"><span class="ln">5495 </span></a>softmax(dim) -&gt; Tensor 
<a name="l5496"><span class="ln">5496 </span></a> 
<a name="l5497"><span class="ln">5497 </span></a>Alias for :func:`torch.nn.functional.softmax`. 
<a name="l5498"><span class="ln">5498 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5499"><span class="ln">5499 </span></a><span class="s3">)</span>
<a name="l5500"><span class="ln">5500 </span></a>
<a name="l5501"><span class="ln">5501 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5502"><span class="ln">5502 </span></a>    <span class="s4">&quot;tanh_&quot;</span><span class="s3">,</span>
<a name="l5503"><span class="ln">5503 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5504"><span class="ln">5504 </span></a>tanh_() -&gt; Tensor 
<a name="l5505"><span class="ln">5505 </span></a> 
<a name="l5506"><span class="ln">5506 </span></a>In-place version of :meth:`~Tensor.tanh` 
<a name="l5507"><span class="ln">5507 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5508"><span class="ln">5508 </span></a><span class="s3">)</span>
<a name="l5509"><span class="ln">5509 </span></a>
<a name="l5510"><span class="ln">5510 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5511"><span class="ln">5511 </span></a>    <span class="s4">&quot;tolist&quot;</span><span class="s3">,</span>
<a name="l5512"><span class="ln">5512 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5513"><span class="ln">5513 </span></a>tolist() -&gt; list or number 
<a name="l5514"><span class="ln">5514 </span></a> 
<a name="l5515"><span class="ln">5515 </span></a>Returns the tensor as a (nested) list. For scalars, a standard 
<a name="l5516"><span class="ln">5516 </span></a>Python number is returned, just like with :meth:`~Tensor.item`. 
<a name="l5517"><span class="ln">5517 </span></a>Tensors are automatically moved to the CPU first if necessary. 
<a name="l5518"><span class="ln">5518 </span></a> 
<a name="l5519"><span class="ln">5519 </span></a>This operation is not differentiable. 
<a name="l5520"><span class="ln">5520 </span></a> 
<a name="l5521"><span class="ln">5521 </span></a>Examples:: 
<a name="l5522"><span class="ln">5522 </span></a> 
<a name="l5523"><span class="ln">5523 </span></a>    &gt;&gt;&gt; a = torch.randn(2, 2) 
<a name="l5524"><span class="ln">5524 </span></a>    &gt;&gt;&gt; a.tolist() 
<a name="l5525"><span class="ln">5525 </span></a>    [[0.012766935862600803, 0.5415473580360413], 
<a name="l5526"><span class="ln">5526 </span></a>     [-0.08909505605697632, 0.7729271650314331]] 
<a name="l5527"><span class="ln">5527 </span></a>    &gt;&gt;&gt; a[0,0].tolist() 
<a name="l5528"><span class="ln">5528 </span></a>    0.012766935862600803 
<a name="l5529"><span class="ln">5529 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5530"><span class="ln">5530 </span></a><span class="s3">)</span>
<a name="l5531"><span class="ln">5531 </span></a>
<a name="l5532"><span class="ln">5532 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5533"><span class="ln">5533 </span></a>    <span class="s4">&quot;topk&quot;</span><span class="s3">,</span>
<a name="l5534"><span class="ln">5534 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5535"><span class="ln">5535 </span></a>topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor) 
<a name="l5536"><span class="ln">5536 </span></a> 
<a name="l5537"><span class="ln">5537 </span></a>See :func:`torch.topk` 
<a name="l5538"><span class="ln">5538 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5539"><span class="ln">5539 </span></a><span class="s3">)</span>
<a name="l5540"><span class="ln">5540 </span></a>
<a name="l5541"><span class="ln">5541 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5542"><span class="ln">5542 </span></a>    <span class="s4">&quot;to_dense&quot;</span><span class="s3">,</span>
<a name="l5543"><span class="ln">5543 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5544"><span class="ln">5544 </span></a>to_dense(dtype=None, *, masked_grad=True) -&gt; Tensor 
<a name="l5545"><span class="ln">5545 </span></a> 
<a name="l5546"><span class="ln">5546 </span></a>Creates a strided copy of :attr:`self` if :attr:`self` is not a strided tensor, otherwise returns :attr:`self`. 
<a name="l5547"><span class="ln">5547 </span></a> 
<a name="l5548"><span class="ln">5548 </span></a>Keyword args: 
<a name="l5549"><span class="ln">5549 </span></a>    {dtype} 
<a name="l5550"><span class="ln">5550 </span></a>    masked_grad (bool, optional): If set to ``True`` (default) and 
<a name="l5551"><span class="ln">5551 </span></a>      :attr:`self` has a sparse layout then the backward of 
<a name="l5552"><span class="ln">5552 </span></a>      :meth:`to_dense` returns ``grad.sparse_mask(self)``. 
<a name="l5553"><span class="ln">5553 </span></a> 
<a name="l5554"><span class="ln">5554 </span></a>Example:: 
<a name="l5555"><span class="ln">5555 </span></a> 
<a name="l5556"><span class="ln">5556 </span></a>    &gt;&gt;&gt; s = torch.sparse_coo_tensor( 
<a name="l5557"><span class="ln">5557 </span></a>    ...        torch.tensor([[1, 1], 
<a name="l5558"><span class="ln">5558 </span></a>    ...                      [0, 2]]), 
<a name="l5559"><span class="ln">5559 </span></a>    ...        torch.tensor([9, 10]), 
<a name="l5560"><span class="ln">5560 </span></a>    ...        size=(3, 3)) 
<a name="l5561"><span class="ln">5561 </span></a>    &gt;&gt;&gt; s.to_dense() 
<a name="l5562"><span class="ln">5562 </span></a>    tensor([[ 0,  0,  0], 
<a name="l5563"><span class="ln">5563 </span></a>            [ 9,  0, 10], 
<a name="l5564"><span class="ln">5564 </span></a>            [ 0,  0,  0]]) 
<a name="l5565"><span class="ln">5565 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5566"><span class="ln">5566 </span></a><span class="s3">)</span>
<a name="l5567"><span class="ln">5567 </span></a>
<a name="l5568"><span class="ln">5568 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5569"><span class="ln">5569 </span></a>    <span class="s4">&quot;to_sparse&quot;</span><span class="s3">,</span>
<a name="l5570"><span class="ln">5570 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5571"><span class="ln">5571 </span></a>to_sparse(sparseDims) -&gt; Tensor 
<a name="l5572"><span class="ln">5572 </span></a> 
<a name="l5573"><span class="ln">5573 </span></a>Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in 
<a name="l5574"><span class="ln">5574 </span></a>:ref:`coordinate format &lt;sparse-coo-docs&gt;`. 
<a name="l5575"><span class="ln">5575 </span></a> 
<a name="l5576"><span class="ln">5576 </span></a>Args: 
<a name="l5577"><span class="ln">5577 </span></a>    sparseDims (int, optional): the number of sparse dimensions to include in the new sparse tensor 
<a name="l5578"><span class="ln">5578 </span></a> 
<a name="l5579"><span class="ln">5579 </span></a>Example:: 
<a name="l5580"><span class="ln">5580 </span></a> 
<a name="l5581"><span class="ln">5581 </span></a>    &gt;&gt;&gt; d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]]) 
<a name="l5582"><span class="ln">5582 </span></a>    &gt;&gt;&gt; d 
<a name="l5583"><span class="ln">5583 </span></a>    tensor([[ 0,  0,  0], 
<a name="l5584"><span class="ln">5584 </span></a>            [ 9,  0, 10], 
<a name="l5585"><span class="ln">5585 </span></a>            [ 0,  0,  0]]) 
<a name="l5586"><span class="ln">5586 </span></a>    &gt;&gt;&gt; d.to_sparse() 
<a name="l5587"><span class="ln">5587 </span></a>    tensor(indices=tensor([[1, 1], 
<a name="l5588"><span class="ln">5588 </span></a>                           [0, 2]]), 
<a name="l5589"><span class="ln">5589 </span></a>           values=tensor([ 9, 10]), 
<a name="l5590"><span class="ln">5590 </span></a>           size=(3, 3), nnz=2, layout=torch.sparse_coo) 
<a name="l5591"><span class="ln">5591 </span></a>    &gt;&gt;&gt; d.to_sparse(1) 
<a name="l5592"><span class="ln">5592 </span></a>    tensor(indices=tensor([[1]]), 
<a name="l5593"><span class="ln">5593 </span></a>           values=tensor([[ 9,  0, 10]]), 
<a name="l5594"><span class="ln">5594 </span></a>           size=(3, 3), nnz=1, layout=torch.sparse_coo) 
<a name="l5595"><span class="ln">5595 </span></a> 
<a name="l5596"><span class="ln">5596 </span></a>.. method:: to_sparse(*, layout=None, blocksize=None, dense_dim=None) -&gt; Tensor 
<a name="l5597"><span class="ln">5597 </span></a>   :noindex: 
<a name="l5598"><span class="ln">5598 </span></a> 
<a name="l5599"><span class="ln">5599 </span></a>Returns a sparse tensor with the specified layout and blocksize.  If 
<a name="l5600"><span class="ln">5600 </span></a>the :attr:`self` is strided, the number of dense dimensions could be 
<a name="l5601"><span class="ln">5601 </span></a>specified, and a hybrid sparse tensor will be created, with 
<a name="l5602"><span class="ln">5602 </span></a>`dense_dim` dense dimensions and `self.dim() - 2 - dense_dim` batch 
<a name="l5603"><span class="ln">5603 </span></a>dimension. 
<a name="l5604"><span class="ln">5604 </span></a> 
<a name="l5605"><span class="ln">5605 </span></a>.. note:: If the :attr:`self` layout and blocksize parameters match 
<a name="l5606"><span class="ln">5606 </span></a>          with the specified layout and blocksize, return 
<a name="l5607"><span class="ln">5607 </span></a>          :attr:`self`. Otherwise, return a sparse tensor copy of 
<a name="l5608"><span class="ln">5608 </span></a>          :attr:`self`. 
<a name="l5609"><span class="ln">5609 </span></a> 
<a name="l5610"><span class="ln">5610 </span></a>Args: 
<a name="l5611"><span class="ln">5611 </span></a> 
<a name="l5612"><span class="ln">5612 </span></a>    layout (:class:`torch.layout`, optional): The desired sparse 
<a name="l5613"><span class="ln">5613 </span></a>      layout. One of ``torch.sparse_coo``, ``torch.sparse_csr``, 
<a name="l5614"><span class="ln">5614 </span></a>      ``torch.sparse_csc``, ``torch.sparse_bsr``, or 
<a name="l5615"><span class="ln">5615 </span></a>      ``torch.sparse_bsc``. Default: if ``None``, 
<a name="l5616"><span class="ln">5616 </span></a>      ``torch.sparse_coo``. 
<a name="l5617"><span class="ln">5617 </span></a> 
<a name="l5618"><span class="ln">5618 </span></a>    blocksize (list, tuple, :class:`torch.Size`, optional): Block size 
<a name="l5619"><span class="ln">5619 </span></a>      of the resulting BSR or BSC tensor. For other layouts, 
<a name="l5620"><span class="ln">5620 </span></a>      specifying the block size that is not ``None`` will result in a 
<a name="l5621"><span class="ln">5621 </span></a>      RuntimeError exception.  A block size must be a tuple of length 
<a name="l5622"><span class="ln">5622 </span></a>      two such that its items evenly divide the two sparse dimensions. 
<a name="l5623"><span class="ln">5623 </span></a> 
<a name="l5624"><span class="ln">5624 </span></a>    dense_dim (int, optional): Number of dense dimensions of the 
<a name="l5625"><span class="ln">5625 </span></a>      resulting CSR, CSC, BSR or BSC tensor.  This argument should be 
<a name="l5626"><span class="ln">5626 </span></a>      used only if :attr:`self` is a strided tensor, and must be a 
<a name="l5627"><span class="ln">5627 </span></a>      value between 0 and dimension of :attr:`self` tensor minus two. 
<a name="l5628"><span class="ln">5628 </span></a> 
<a name="l5629"><span class="ln">5629 </span></a>Example:: 
<a name="l5630"><span class="ln">5630 </span></a> 
<a name="l5631"><span class="ln">5631 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1, 0], [0, 0], [2, 3]]) 
<a name="l5632"><span class="ln">5632 </span></a>    &gt;&gt;&gt; x.to_sparse(layout=torch.sparse_coo) 
<a name="l5633"><span class="ln">5633 </span></a>    tensor(indices=tensor([[0, 2, 2], 
<a name="l5634"><span class="ln">5634 </span></a>                           [0, 0, 1]]), 
<a name="l5635"><span class="ln">5635 </span></a>           values=tensor([1, 2, 3]), 
<a name="l5636"><span class="ln">5636 </span></a>           size=(3, 2), nnz=3, layout=torch.sparse_coo) 
<a name="l5637"><span class="ln">5637 </span></a>    &gt;&gt;&gt; x.to_sparse(layout=torch.sparse_bsr, blocksize=(1, 2)) 
<a name="l5638"><span class="ln">5638 </span></a>    tensor(crow_indices=tensor([0, 1, 1, 2]), 
<a name="l5639"><span class="ln">5639 </span></a>           col_indices=tensor([0, 0]), 
<a name="l5640"><span class="ln">5640 </span></a>           values=tensor([[[1, 0]], 
<a name="l5641"><span class="ln">5641 </span></a>                          [[2, 3]]]), size=(3, 2), nnz=2, layout=torch.sparse_bsr) 
<a name="l5642"><span class="ln">5642 </span></a>    &gt;&gt;&gt; x.to_sparse(layout=torch.sparse_bsr, blocksize=(2, 1)) 
<a name="l5643"><span class="ln">5643 </span></a>    RuntimeError: Tensor size(-2) 3 needs to be divisible by blocksize[0] 2 
<a name="l5644"><span class="ln">5644 </span></a>    &gt;&gt;&gt; x.to_sparse(layout=torch.sparse_csr, blocksize=(3, 1)) 
<a name="l5645"><span class="ln">5645 </span></a>    RuntimeError: to_sparse for Strided to SparseCsr conversion does not use specified blocksize 
<a name="l5646"><span class="ln">5646 </span></a> 
<a name="l5647"><span class="ln">5647 </span></a>    &gt;&gt;&gt; x = torch.tensor([[[1], [0]], [[0], [0]], [[2], [3]]]) 
<a name="l5648"><span class="ln">5648 </span></a>    &gt;&gt;&gt; x.to_sparse(layout=torch.sparse_csr, dense_dim=1) 
<a name="l5649"><span class="ln">5649 </span></a>    tensor(crow_indices=tensor([0, 1, 1, 3]), 
<a name="l5650"><span class="ln">5650 </span></a>           col_indices=tensor([0, 0, 1]), 
<a name="l5651"><span class="ln">5651 </span></a>           values=tensor([[1], 
<a name="l5652"><span class="ln">5652 </span></a>                          [2], 
<a name="l5653"><span class="ln">5653 </span></a>                          [3]]), size=(3, 2, 1), nnz=3, layout=torch.sparse_csr) 
<a name="l5654"><span class="ln">5654 </span></a> 
<a name="l5655"><span class="ln">5655 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5656"><span class="ln">5656 </span></a><span class="s3">)</span>
<a name="l5657"><span class="ln">5657 </span></a>
<a name="l5658"><span class="ln">5658 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5659"><span class="ln">5659 </span></a>    <span class="s4">&quot;to_sparse_csr&quot;</span><span class="s3">,</span>
<a name="l5660"><span class="ln">5660 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5661"><span class="ln">5661 </span></a>to_sparse_csr(dense_dim=None) -&gt; Tensor 
<a name="l5662"><span class="ln">5662 </span></a> 
<a name="l5663"><span class="ln">5663 </span></a>Convert a tensor to compressed row storage format (CSR).  Except for 
<a name="l5664"><span class="ln">5664 </span></a>strided tensors, only works with 2D tensors.  If the :attr:`self` is 
<a name="l5665"><span class="ln">5665 </span></a>strided, then the number of dense dimensions could be specified, and a 
<a name="l5666"><span class="ln">5666 </span></a>hybrid CSR tensor will be created, with `dense_dim` dense dimensions 
<a name="l5667"><span class="ln">5667 </span></a>and `self.dim() - 2 - dense_dim` batch dimension. 
<a name="l5668"><span class="ln">5668 </span></a> 
<a name="l5669"><span class="ln">5669 </span></a>Args: 
<a name="l5670"><span class="ln">5670 </span></a> 
<a name="l5671"><span class="ln">5671 </span></a>    dense_dim (int, optional): Number of dense dimensions of the 
<a name="l5672"><span class="ln">5672 </span></a>      resulting CSR tensor.  This argument should be used only if 
<a name="l5673"><span class="ln">5673 </span></a>      :attr:`self` is a strided tensor, and must be a value between 0 
<a name="l5674"><span class="ln">5674 </span></a>      and dimension of :attr:`self` tensor minus two. 
<a name="l5675"><span class="ln">5675 </span></a> 
<a name="l5676"><span class="ln">5676 </span></a>Example:: 
<a name="l5677"><span class="ln">5677 </span></a> 
<a name="l5678"><span class="ln">5678 </span></a>    &gt;&gt;&gt; dense = torch.randn(5, 5) 
<a name="l5679"><span class="ln">5679 </span></a>    &gt;&gt;&gt; sparse = dense.to_sparse_csr() 
<a name="l5680"><span class="ln">5680 </span></a>    &gt;&gt;&gt; sparse._nnz() 
<a name="l5681"><span class="ln">5681 </span></a>    25 
<a name="l5682"><span class="ln">5682 </span></a> 
<a name="l5683"><span class="ln">5683 </span></a>    &gt;&gt;&gt; dense = torch.zeros(3, 3, 1, 1) 
<a name="l5684"><span class="ln">5684 </span></a>    &gt;&gt;&gt; dense[0, 0] = dense[1, 2] = dense[2, 1] = 1 
<a name="l5685"><span class="ln">5685 </span></a>    &gt;&gt;&gt; dense.to_sparse_csr(dense_dim=2) 
<a name="l5686"><span class="ln">5686 </span></a>    tensor(crow_indices=tensor([0, 1, 2, 3]), 
<a name="l5687"><span class="ln">5687 </span></a>           col_indices=tensor([0, 2, 1]), 
<a name="l5688"><span class="ln">5688 </span></a>           values=tensor([[[1.]], 
<a name="l5689"><span class="ln">5689 </span></a> 
<a name="l5690"><span class="ln">5690 </span></a>                          [[1.]], 
<a name="l5691"><span class="ln">5691 </span></a> 
<a name="l5692"><span class="ln">5692 </span></a>                          [[1.]]]), size=(3, 3, 1, 1), nnz=3, 
<a name="l5693"><span class="ln">5693 </span></a>           layout=torch.sparse_csr) 
<a name="l5694"><span class="ln">5694 </span></a> 
<a name="l5695"><span class="ln">5695 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5696"><span class="ln">5696 </span></a><span class="s3">)</span>
<a name="l5697"><span class="ln">5697 </span></a>
<a name="l5698"><span class="ln">5698 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5699"><span class="ln">5699 </span></a>    <span class="s4">&quot;to_sparse_csc&quot;</span><span class="s3">,</span>
<a name="l5700"><span class="ln">5700 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5701"><span class="ln">5701 </span></a>to_sparse_csc() -&gt; Tensor 
<a name="l5702"><span class="ln">5702 </span></a> 
<a name="l5703"><span class="ln">5703 </span></a>Convert a tensor to compressed column storage (CSC) format.  Except 
<a name="l5704"><span class="ln">5704 </span></a>for strided tensors, only works with 2D tensors.  If the :attr:`self` 
<a name="l5705"><span class="ln">5705 </span></a>is strided, then the number of dense dimensions could be specified, 
<a name="l5706"><span class="ln">5706 </span></a>and a hybrid CSC tensor will be created, with `dense_dim` dense 
<a name="l5707"><span class="ln">5707 </span></a>dimensions and `self.dim() - 2 - dense_dim` batch dimension. 
<a name="l5708"><span class="ln">5708 </span></a> 
<a name="l5709"><span class="ln">5709 </span></a>Args: 
<a name="l5710"><span class="ln">5710 </span></a> 
<a name="l5711"><span class="ln">5711 </span></a>    dense_dim (int, optional): Number of dense dimensions of the 
<a name="l5712"><span class="ln">5712 </span></a>      resulting CSC tensor.  This argument should be used only if 
<a name="l5713"><span class="ln">5713 </span></a>      :attr:`self` is a strided tensor, and must be a value between 0 
<a name="l5714"><span class="ln">5714 </span></a>      and dimension of :attr:`self` tensor minus two. 
<a name="l5715"><span class="ln">5715 </span></a> 
<a name="l5716"><span class="ln">5716 </span></a>Example:: 
<a name="l5717"><span class="ln">5717 </span></a> 
<a name="l5718"><span class="ln">5718 </span></a>    &gt;&gt;&gt; dense = torch.randn(5, 5) 
<a name="l5719"><span class="ln">5719 </span></a>    &gt;&gt;&gt; sparse = dense.to_sparse_csc() 
<a name="l5720"><span class="ln">5720 </span></a>    &gt;&gt;&gt; sparse._nnz() 
<a name="l5721"><span class="ln">5721 </span></a>    25 
<a name="l5722"><span class="ln">5722 </span></a> 
<a name="l5723"><span class="ln">5723 </span></a>    &gt;&gt;&gt; dense = torch.zeros(3, 3, 1, 1) 
<a name="l5724"><span class="ln">5724 </span></a>    &gt;&gt;&gt; dense[0, 0] = dense[1, 2] = dense[2, 1] = 1 
<a name="l5725"><span class="ln">5725 </span></a>    &gt;&gt;&gt; dense.to_sparse_csc(dense_dim=2) 
<a name="l5726"><span class="ln">5726 </span></a>    tensor(ccol_indices=tensor([0, 1, 2, 3]), 
<a name="l5727"><span class="ln">5727 </span></a>           row_indices=tensor([0, 2, 1]), 
<a name="l5728"><span class="ln">5728 </span></a>           values=tensor([[[1.]], 
<a name="l5729"><span class="ln">5729 </span></a> 
<a name="l5730"><span class="ln">5730 </span></a>                          [[1.]], 
<a name="l5731"><span class="ln">5731 </span></a> 
<a name="l5732"><span class="ln">5732 </span></a>                          [[1.]]]), size=(3, 3, 1, 1), nnz=3, 
<a name="l5733"><span class="ln">5733 </span></a>           layout=torch.sparse_csc) 
<a name="l5734"><span class="ln">5734 </span></a> 
<a name="l5735"><span class="ln">5735 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5736"><span class="ln">5736 </span></a><span class="s3">)</span>
<a name="l5737"><span class="ln">5737 </span></a>
<a name="l5738"><span class="ln">5738 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5739"><span class="ln">5739 </span></a>    <span class="s4">&quot;to_sparse_bsr&quot;</span><span class="s3">,</span>
<a name="l5740"><span class="ln">5740 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5741"><span class="ln">5741 </span></a>to_sparse_bsr(blocksize, dense_dim) -&gt; Tensor 
<a name="l5742"><span class="ln">5742 </span></a> 
<a name="l5743"><span class="ln">5743 </span></a>Convert a tensor to a block sparse row (BSR) storage format of given 
<a name="l5744"><span class="ln">5744 </span></a>blocksize.  If the :attr:`self` is strided, then the number of dense 
<a name="l5745"><span class="ln">5745 </span></a>dimensions could be specified, and a hybrid BSR tensor will be 
<a name="l5746"><span class="ln">5746 </span></a>created, with `dense_dim` dense dimensions and `self.dim() - 2 - 
<a name="l5747"><span class="ln">5747 </span></a>dense_dim` batch dimension. 
<a name="l5748"><span class="ln">5748 </span></a> 
<a name="l5749"><span class="ln">5749 </span></a>Args: 
<a name="l5750"><span class="ln">5750 </span></a> 
<a name="l5751"><span class="ln">5751 </span></a>    blocksize (list, tuple, :class:`torch.Size`, optional): Block size 
<a name="l5752"><span class="ln">5752 </span></a>      of the resulting BSR tensor. A block size must be a tuple of 
<a name="l5753"><span class="ln">5753 </span></a>      length two such that its items evenly divide the two sparse 
<a name="l5754"><span class="ln">5754 </span></a>      dimensions. 
<a name="l5755"><span class="ln">5755 </span></a> 
<a name="l5756"><span class="ln">5756 </span></a>    dense_dim (int, optional): Number of dense dimensions of the 
<a name="l5757"><span class="ln">5757 </span></a>      resulting BSR tensor.  This argument should be used only if 
<a name="l5758"><span class="ln">5758 </span></a>      :attr:`self` is a strided tensor, and must be a value between 0 
<a name="l5759"><span class="ln">5759 </span></a>      and dimension of :attr:`self` tensor minus two. 
<a name="l5760"><span class="ln">5760 </span></a> 
<a name="l5761"><span class="ln">5761 </span></a>Example:: 
<a name="l5762"><span class="ln">5762 </span></a> 
<a name="l5763"><span class="ln">5763 </span></a>    &gt;&gt;&gt; dense = torch.randn(10, 10) 
<a name="l5764"><span class="ln">5764 </span></a>    &gt;&gt;&gt; sparse = dense.to_sparse_csr() 
<a name="l5765"><span class="ln">5765 </span></a>    &gt;&gt;&gt; sparse_bsr = sparse.to_sparse_bsr((5, 5)) 
<a name="l5766"><span class="ln">5766 </span></a>    &gt;&gt;&gt; sparse_bsr.col_indices() 
<a name="l5767"><span class="ln">5767 </span></a>    tensor([0, 1, 0, 1]) 
<a name="l5768"><span class="ln">5768 </span></a> 
<a name="l5769"><span class="ln">5769 </span></a>    &gt;&gt;&gt; dense = torch.zeros(4, 3, 1) 
<a name="l5770"><span class="ln">5770 </span></a>    &gt;&gt;&gt; dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1 
<a name="l5771"><span class="ln">5771 </span></a>    &gt;&gt;&gt; dense.to_sparse_bsr((2, 1), 1) 
<a name="l5772"><span class="ln">5772 </span></a>    tensor(crow_indices=tensor([0, 2, 3]), 
<a name="l5773"><span class="ln">5773 </span></a>           col_indices=tensor([0, 2, 1]), 
<a name="l5774"><span class="ln">5774 </span></a>           values=tensor([[[[1.]], 
<a name="l5775"><span class="ln">5775 </span></a> 
<a name="l5776"><span class="ln">5776 </span></a>                           [[1.]]], 
<a name="l5777"><span class="ln">5777 </span></a> 
<a name="l5778"><span class="ln">5778 </span></a> 
<a name="l5779"><span class="ln">5779 </span></a>                          [[[1.]], 
<a name="l5780"><span class="ln">5780 </span></a> 
<a name="l5781"><span class="ln">5781 </span></a>                           [[1.]]], 
<a name="l5782"><span class="ln">5782 </span></a> 
<a name="l5783"><span class="ln">5783 </span></a> 
<a name="l5784"><span class="ln">5784 </span></a>                          [[[1.]], 
<a name="l5785"><span class="ln">5785 </span></a> 
<a name="l5786"><span class="ln">5786 </span></a>                           [[1.]]]]), size=(4, 3, 1), nnz=3, 
<a name="l5787"><span class="ln">5787 </span></a>           layout=torch.sparse_bsr) 
<a name="l5788"><span class="ln">5788 </span></a> 
<a name="l5789"><span class="ln">5789 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5790"><span class="ln">5790 </span></a><span class="s3">)</span>
<a name="l5791"><span class="ln">5791 </span></a>
<a name="l5792"><span class="ln">5792 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5793"><span class="ln">5793 </span></a>    <span class="s4">&quot;to_sparse_bsc&quot;</span><span class="s3">,</span>
<a name="l5794"><span class="ln">5794 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5795"><span class="ln">5795 </span></a>to_sparse_bsc(blocksize, dense_dim) -&gt; Tensor 
<a name="l5796"><span class="ln">5796 </span></a> 
<a name="l5797"><span class="ln">5797 </span></a>Convert a tensor to a block sparse column (BSC) storage format of 
<a name="l5798"><span class="ln">5798 </span></a>given blocksize.  If the :attr:`self` is strided, then the number of 
<a name="l5799"><span class="ln">5799 </span></a>dense dimensions could be specified, and a hybrid BSC tensor will be 
<a name="l5800"><span class="ln">5800 </span></a>created, with `dense_dim` dense dimensions and `self.dim() - 2 - 
<a name="l5801"><span class="ln">5801 </span></a>dense_dim` batch dimension. 
<a name="l5802"><span class="ln">5802 </span></a> 
<a name="l5803"><span class="ln">5803 </span></a>Args: 
<a name="l5804"><span class="ln">5804 </span></a> 
<a name="l5805"><span class="ln">5805 </span></a>    blocksize (list, tuple, :class:`torch.Size`, optional): Block size 
<a name="l5806"><span class="ln">5806 </span></a>      of the resulting BSC tensor. A block size must be a tuple of 
<a name="l5807"><span class="ln">5807 </span></a>      length two such that its items evenly divide the two sparse 
<a name="l5808"><span class="ln">5808 </span></a>      dimensions. 
<a name="l5809"><span class="ln">5809 </span></a> 
<a name="l5810"><span class="ln">5810 </span></a>    dense_dim (int, optional): Number of dense dimensions of the 
<a name="l5811"><span class="ln">5811 </span></a>      resulting BSC tensor.  This argument should be used only if 
<a name="l5812"><span class="ln">5812 </span></a>      :attr:`self` is a strided tensor, and must be a value between 0 
<a name="l5813"><span class="ln">5813 </span></a>      and dimension of :attr:`self` tensor minus two. 
<a name="l5814"><span class="ln">5814 </span></a> 
<a name="l5815"><span class="ln">5815 </span></a>Example:: 
<a name="l5816"><span class="ln">5816 </span></a> 
<a name="l5817"><span class="ln">5817 </span></a>    &gt;&gt;&gt; dense = torch.randn(10, 10) 
<a name="l5818"><span class="ln">5818 </span></a>    &gt;&gt;&gt; sparse = dense.to_sparse_csr() 
<a name="l5819"><span class="ln">5819 </span></a>    &gt;&gt;&gt; sparse_bsc = sparse.to_sparse_bsc((5, 5)) 
<a name="l5820"><span class="ln">5820 </span></a>    &gt;&gt;&gt; sparse_bsc.row_indices() 
<a name="l5821"><span class="ln">5821 </span></a>    tensor([0, 1, 0, 1]) 
<a name="l5822"><span class="ln">5822 </span></a> 
<a name="l5823"><span class="ln">5823 </span></a>    &gt;&gt;&gt; dense = torch.zeros(4, 3, 1) 
<a name="l5824"><span class="ln">5824 </span></a>    &gt;&gt;&gt; dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1 
<a name="l5825"><span class="ln">5825 </span></a>    &gt;&gt;&gt; dense.to_sparse_bsc((2, 1), 1) 
<a name="l5826"><span class="ln">5826 </span></a>    tensor(ccol_indices=tensor([0, 1, 2, 3]), 
<a name="l5827"><span class="ln">5827 </span></a>           row_indices=tensor([0, 1, 0]), 
<a name="l5828"><span class="ln">5828 </span></a>           values=tensor([[[[1.]], 
<a name="l5829"><span class="ln">5829 </span></a> 
<a name="l5830"><span class="ln">5830 </span></a>                           [[1.]]], 
<a name="l5831"><span class="ln">5831 </span></a> 
<a name="l5832"><span class="ln">5832 </span></a> 
<a name="l5833"><span class="ln">5833 </span></a>                          [[[1.]], 
<a name="l5834"><span class="ln">5834 </span></a> 
<a name="l5835"><span class="ln">5835 </span></a>                           [[1.]]], 
<a name="l5836"><span class="ln">5836 </span></a> 
<a name="l5837"><span class="ln">5837 </span></a> 
<a name="l5838"><span class="ln">5838 </span></a>                          [[[1.]], 
<a name="l5839"><span class="ln">5839 </span></a> 
<a name="l5840"><span class="ln">5840 </span></a>                           [[1.]]]]), size=(4, 3, 1), nnz=3, 
<a name="l5841"><span class="ln">5841 </span></a>           layout=torch.sparse_bsc) 
<a name="l5842"><span class="ln">5842 </span></a> 
<a name="l5843"><span class="ln">5843 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5844"><span class="ln">5844 </span></a><span class="s3">)</span>
<a name="l5845"><span class="ln">5845 </span></a>
<a name="l5846"><span class="ln">5846 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5847"><span class="ln">5847 </span></a>    <span class="s4">&quot;to_mkldnn&quot;</span><span class="s3">,</span>
<a name="l5848"><span class="ln">5848 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5849"><span class="ln">5849 </span></a>to_mkldnn() -&gt; Tensor 
<a name="l5850"><span class="ln">5850 </span></a>Returns a copy of the tensor in ``torch.mkldnn`` layout. 
<a name="l5851"><span class="ln">5851 </span></a> 
<a name="l5852"><span class="ln">5852 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5853"><span class="ln">5853 </span></a><span class="s3">)</span>
<a name="l5854"><span class="ln">5854 </span></a>
<a name="l5855"><span class="ln">5855 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5856"><span class="ln">5856 </span></a>    <span class="s4">&quot;trace&quot;</span><span class="s3">,</span>
<a name="l5857"><span class="ln">5857 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5858"><span class="ln">5858 </span></a>trace() -&gt; Tensor 
<a name="l5859"><span class="ln">5859 </span></a> 
<a name="l5860"><span class="ln">5860 </span></a>See :func:`torch.trace` 
<a name="l5861"><span class="ln">5861 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5862"><span class="ln">5862 </span></a><span class="s3">)</span>
<a name="l5863"><span class="ln">5863 </span></a>
<a name="l5864"><span class="ln">5864 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5865"><span class="ln">5865 </span></a>    <span class="s4">&quot;transpose&quot;</span><span class="s3">,</span>
<a name="l5866"><span class="ln">5866 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5867"><span class="ln">5867 </span></a>transpose(dim0, dim1) -&gt; Tensor 
<a name="l5868"><span class="ln">5868 </span></a> 
<a name="l5869"><span class="ln">5869 </span></a>See :func:`torch.transpose` 
<a name="l5870"><span class="ln">5870 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5871"><span class="ln">5871 </span></a><span class="s3">)</span>
<a name="l5872"><span class="ln">5872 </span></a>
<a name="l5873"><span class="ln">5873 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5874"><span class="ln">5874 </span></a>    <span class="s4">&quot;transpose_&quot;</span><span class="s3">,</span>
<a name="l5875"><span class="ln">5875 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5876"><span class="ln">5876 </span></a>transpose_(dim0, dim1) -&gt; Tensor 
<a name="l5877"><span class="ln">5877 </span></a> 
<a name="l5878"><span class="ln">5878 </span></a>In-place version of :meth:`~Tensor.transpose` 
<a name="l5879"><span class="ln">5879 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5880"><span class="ln">5880 </span></a><span class="s3">)</span>
<a name="l5881"><span class="ln">5881 </span></a>
<a name="l5882"><span class="ln">5882 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5883"><span class="ln">5883 </span></a>    <span class="s4">&quot;triangular_solve&quot;</span><span class="s3">,</span>
<a name="l5884"><span class="ln">5884 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5885"><span class="ln">5885 </span></a>triangular_solve(A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor) 
<a name="l5886"><span class="ln">5886 </span></a> 
<a name="l5887"><span class="ln">5887 </span></a>See :func:`torch.triangular_solve` 
<a name="l5888"><span class="ln">5888 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5889"><span class="ln">5889 </span></a><span class="s3">)</span>
<a name="l5890"><span class="ln">5890 </span></a>
<a name="l5891"><span class="ln">5891 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5892"><span class="ln">5892 </span></a>    <span class="s4">&quot;tril&quot;</span><span class="s3">,</span>
<a name="l5893"><span class="ln">5893 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5894"><span class="ln">5894 </span></a>tril(diagonal=0) -&gt; Tensor 
<a name="l5895"><span class="ln">5895 </span></a> 
<a name="l5896"><span class="ln">5896 </span></a>See :func:`torch.tril` 
<a name="l5897"><span class="ln">5897 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5898"><span class="ln">5898 </span></a><span class="s3">)</span>
<a name="l5899"><span class="ln">5899 </span></a>
<a name="l5900"><span class="ln">5900 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5901"><span class="ln">5901 </span></a>    <span class="s4">&quot;tril_&quot;</span><span class="s3">,</span>
<a name="l5902"><span class="ln">5902 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5903"><span class="ln">5903 </span></a>tril_(diagonal=0) -&gt; Tensor 
<a name="l5904"><span class="ln">5904 </span></a> 
<a name="l5905"><span class="ln">5905 </span></a>In-place version of :meth:`~Tensor.tril` 
<a name="l5906"><span class="ln">5906 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5907"><span class="ln">5907 </span></a><span class="s3">)</span>
<a name="l5908"><span class="ln">5908 </span></a>
<a name="l5909"><span class="ln">5909 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5910"><span class="ln">5910 </span></a>    <span class="s4">&quot;triu&quot;</span><span class="s3">,</span>
<a name="l5911"><span class="ln">5911 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5912"><span class="ln">5912 </span></a>triu(diagonal=0) -&gt; Tensor 
<a name="l5913"><span class="ln">5913 </span></a> 
<a name="l5914"><span class="ln">5914 </span></a>See :func:`torch.triu` 
<a name="l5915"><span class="ln">5915 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5916"><span class="ln">5916 </span></a><span class="s3">)</span>
<a name="l5917"><span class="ln">5917 </span></a>
<a name="l5918"><span class="ln">5918 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5919"><span class="ln">5919 </span></a>    <span class="s4">&quot;triu_&quot;</span><span class="s3">,</span>
<a name="l5920"><span class="ln">5920 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5921"><span class="ln">5921 </span></a>triu_(diagonal=0) -&gt; Tensor 
<a name="l5922"><span class="ln">5922 </span></a> 
<a name="l5923"><span class="ln">5923 </span></a>In-place version of :meth:`~Tensor.triu` 
<a name="l5924"><span class="ln">5924 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5925"><span class="ln">5925 </span></a><span class="s3">)</span>
<a name="l5926"><span class="ln">5926 </span></a>
<a name="l5927"><span class="ln">5927 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5928"><span class="ln">5928 </span></a>    <span class="s4">&quot;true_divide&quot;</span><span class="s3">,</span>
<a name="l5929"><span class="ln">5929 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5930"><span class="ln">5930 </span></a>true_divide(value) -&gt; Tensor 
<a name="l5931"><span class="ln">5931 </span></a> 
<a name="l5932"><span class="ln">5932 </span></a>See :func:`torch.true_divide` 
<a name="l5933"><span class="ln">5933 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5934"><span class="ln">5934 </span></a><span class="s3">)</span>
<a name="l5935"><span class="ln">5935 </span></a>
<a name="l5936"><span class="ln">5936 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5937"><span class="ln">5937 </span></a>    <span class="s4">&quot;true_divide_&quot;</span><span class="s3">,</span>
<a name="l5938"><span class="ln">5938 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5939"><span class="ln">5939 </span></a>true_divide_(value) -&gt; Tensor 
<a name="l5940"><span class="ln">5940 </span></a> 
<a name="l5941"><span class="ln">5941 </span></a>In-place version of :meth:`~Tensor.true_divide_` 
<a name="l5942"><span class="ln">5942 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5943"><span class="ln">5943 </span></a><span class="s3">)</span>
<a name="l5944"><span class="ln">5944 </span></a>
<a name="l5945"><span class="ln">5945 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5946"><span class="ln">5946 </span></a>    <span class="s4">&quot;trunc&quot;</span><span class="s3">,</span>
<a name="l5947"><span class="ln">5947 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5948"><span class="ln">5948 </span></a>trunc() -&gt; Tensor 
<a name="l5949"><span class="ln">5949 </span></a> 
<a name="l5950"><span class="ln">5950 </span></a>See :func:`torch.trunc` 
<a name="l5951"><span class="ln">5951 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5952"><span class="ln">5952 </span></a><span class="s3">)</span>
<a name="l5953"><span class="ln">5953 </span></a>
<a name="l5954"><span class="ln">5954 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5955"><span class="ln">5955 </span></a>    <span class="s4">&quot;fix&quot;</span><span class="s3">,</span>
<a name="l5956"><span class="ln">5956 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5957"><span class="ln">5957 </span></a>fix() -&gt; Tensor 
<a name="l5958"><span class="ln">5958 </span></a> 
<a name="l5959"><span class="ln">5959 </span></a>See :func:`torch.fix`. 
<a name="l5960"><span class="ln">5960 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5961"><span class="ln">5961 </span></a><span class="s3">)</span>
<a name="l5962"><span class="ln">5962 </span></a>
<a name="l5963"><span class="ln">5963 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5964"><span class="ln">5964 </span></a>    <span class="s4">&quot;trunc_&quot;</span><span class="s3">,</span>
<a name="l5965"><span class="ln">5965 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5966"><span class="ln">5966 </span></a>trunc_() -&gt; Tensor 
<a name="l5967"><span class="ln">5967 </span></a> 
<a name="l5968"><span class="ln">5968 </span></a>In-place version of :meth:`~Tensor.trunc` 
<a name="l5969"><span class="ln">5969 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5970"><span class="ln">5970 </span></a><span class="s3">)</span>
<a name="l5971"><span class="ln">5971 </span></a>
<a name="l5972"><span class="ln">5972 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5973"><span class="ln">5973 </span></a>    <span class="s4">&quot;fix_&quot;</span><span class="s3">,</span>
<a name="l5974"><span class="ln">5974 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5975"><span class="ln">5975 </span></a>fix_() -&gt; Tensor 
<a name="l5976"><span class="ln">5976 </span></a> 
<a name="l5977"><span class="ln">5977 </span></a>In-place version of :meth:`~Tensor.fix` 
<a name="l5978"><span class="ln">5978 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l5979"><span class="ln">5979 </span></a><span class="s3">)</span>
<a name="l5980"><span class="ln">5980 </span></a>
<a name="l5981"><span class="ln">5981 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l5982"><span class="ln">5982 </span></a>    <span class="s4">&quot;type&quot;</span><span class="s3">,</span>
<a name="l5983"><span class="ln">5983 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l5984"><span class="ln">5984 </span></a>type(dtype=None, non_blocking=False, **kwargs) -&gt; str or Tensor 
<a name="l5985"><span class="ln">5985 </span></a>Returns the type if `dtype` is not provided, else casts this object to 
<a name="l5986"><span class="ln">5986 </span></a>the specified type. 
<a name="l5987"><span class="ln">5987 </span></a> 
<a name="l5988"><span class="ln">5988 </span></a>If this is already of the correct type, no copy is performed and the 
<a name="l5989"><span class="ln">5989 </span></a>original object is returned. 
<a name="l5990"><span class="ln">5990 </span></a> 
<a name="l5991"><span class="ln">5991 </span></a>Args: 
<a name="l5992"><span class="ln">5992 </span></a>    dtype (dtype or string): The desired type 
<a name="l5993"><span class="ln">5993 </span></a>    non_blocking (bool): If ``True``, and the source is in pinned memory 
<a name="l5994"><span class="ln">5994 </span></a>        and destination is on the GPU or vice versa, the copy is performed 
<a name="l5995"><span class="ln">5995 </span></a>        asynchronously with respect to the host. Otherwise, the argument 
<a name="l5996"><span class="ln">5996 </span></a>        has no effect. 
<a name="l5997"><span class="ln">5997 </span></a>    **kwargs: For compatibility, may contain the key ``async`` in place of 
<a name="l5998"><span class="ln">5998 </span></a>        the ``non_blocking`` argument. The ``async`` arg is deprecated. 
<a name="l5999"><span class="ln">5999 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6000"><span class="ln">6000 </span></a><span class="s3">)</span>
<a name="l6001"><span class="ln">6001 </span></a>
<a name="l6002"><span class="ln">6002 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6003"><span class="ln">6003 </span></a>    <span class="s4">&quot;type_as&quot;</span><span class="s3">,</span>
<a name="l6004"><span class="ln">6004 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6005"><span class="ln">6005 </span></a>type_as(tensor) -&gt; Tensor 
<a name="l6006"><span class="ln">6006 </span></a> 
<a name="l6007"><span class="ln">6007 </span></a>Returns this tensor cast to the type of the given tensor. 
<a name="l6008"><span class="ln">6008 </span></a> 
<a name="l6009"><span class="ln">6009 </span></a>This is a no-op if the tensor is already of the correct type. This is 
<a name="l6010"><span class="ln">6010 </span></a>equivalent to ``self.type(tensor.type())`` 
<a name="l6011"><span class="ln">6011 </span></a> 
<a name="l6012"><span class="ln">6012 </span></a>Args: 
<a name="l6013"><span class="ln">6013 </span></a>    tensor (Tensor): the tensor which has the desired type 
<a name="l6014"><span class="ln">6014 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6015"><span class="ln">6015 </span></a><span class="s3">)</span>
<a name="l6016"><span class="ln">6016 </span></a>
<a name="l6017"><span class="ln">6017 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6018"><span class="ln">6018 </span></a>    <span class="s4">&quot;unfold&quot;</span><span class="s3">,</span>
<a name="l6019"><span class="ln">6019 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6020"><span class="ln">6020 </span></a>unfold(dimension, size, step) -&gt; Tensor 
<a name="l6021"><span class="ln">6021 </span></a> 
<a name="l6022"><span class="ln">6022 </span></a>Returns a view of the original tensor which contains all slices of size :attr:`size` from 
<a name="l6023"><span class="ln">6023 </span></a>:attr:`self` tensor in the dimension :attr:`dimension`. 
<a name="l6024"><span class="ln">6024 </span></a> 
<a name="l6025"><span class="ln">6025 </span></a>Step between two slices is given by :attr:`step`. 
<a name="l6026"><span class="ln">6026 </span></a> 
<a name="l6027"><span class="ln">6027 </span></a>If `sizedim` is the size of dimension :attr:`dimension` for :attr:`self`, the size of 
<a name="l6028"><span class="ln">6028 </span></a>dimension :attr:`dimension` in the returned tensor will be 
<a name="l6029"><span class="ln">6029 </span></a>`(sizedim - size) / step + 1`. 
<a name="l6030"><span class="ln">6030 </span></a> 
<a name="l6031"><span class="ln">6031 </span></a>An additional dimension of size :attr:`size` is appended in the returned tensor. 
<a name="l6032"><span class="ln">6032 </span></a> 
<a name="l6033"><span class="ln">6033 </span></a>Args: 
<a name="l6034"><span class="ln">6034 </span></a>    dimension (int): dimension in which unfolding happens 
<a name="l6035"><span class="ln">6035 </span></a>    size (int): the size of each slice that is unfolded 
<a name="l6036"><span class="ln">6036 </span></a>    step (int): the step between each slice 
<a name="l6037"><span class="ln">6037 </span></a> 
<a name="l6038"><span class="ln">6038 </span></a>Example:: 
<a name="l6039"><span class="ln">6039 </span></a> 
<a name="l6040"><span class="ln">6040 </span></a>    &gt;&gt;&gt; x = torch.arange(1., 8) 
<a name="l6041"><span class="ln">6041 </span></a>    &gt;&gt;&gt; x 
<a name="l6042"><span class="ln">6042 </span></a>    tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.]) 
<a name="l6043"><span class="ln">6043 </span></a>    &gt;&gt;&gt; x.unfold(0, 2, 1) 
<a name="l6044"><span class="ln">6044 </span></a>    tensor([[ 1.,  2.], 
<a name="l6045"><span class="ln">6045 </span></a>            [ 2.,  3.], 
<a name="l6046"><span class="ln">6046 </span></a>            [ 3.,  4.], 
<a name="l6047"><span class="ln">6047 </span></a>            [ 4.,  5.], 
<a name="l6048"><span class="ln">6048 </span></a>            [ 5.,  6.], 
<a name="l6049"><span class="ln">6049 </span></a>            [ 6.,  7.]]) 
<a name="l6050"><span class="ln">6050 </span></a>    &gt;&gt;&gt; x.unfold(0, 2, 2) 
<a name="l6051"><span class="ln">6051 </span></a>    tensor([[ 1.,  2.], 
<a name="l6052"><span class="ln">6052 </span></a>            [ 3.,  4.], 
<a name="l6053"><span class="ln">6053 </span></a>            [ 5.,  6.]]) 
<a name="l6054"><span class="ln">6054 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6055"><span class="ln">6055 </span></a><span class="s3">)</span>
<a name="l6056"><span class="ln">6056 </span></a>
<a name="l6057"><span class="ln">6057 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6058"><span class="ln">6058 </span></a>    <span class="s4">&quot;uniform_&quot;</span><span class="s3">,</span>
<a name="l6059"><span class="ln">6059 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6060"><span class="ln">6060 </span></a>uniform_(from=0, to=1, *, generator=None) -&gt; Tensor 
<a name="l6061"><span class="ln">6061 </span></a> 
<a name="l6062"><span class="ln">6062 </span></a>Fills :attr:`self` tensor with numbers sampled from the continuous uniform 
<a name="l6063"><span class="ln">6063 </span></a>distribution: 
<a name="l6064"><span class="ln">6064 </span></a> 
<a name="l6065"><span class="ln">6065 </span></a>.. math:: 
<a name="l6066"><span class="ln">6066 </span></a>    f(x) = \dfrac{1}{\text{to} - \text{from}} 
<a name="l6067"><span class="ln">6067 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6068"><span class="ln">6068 </span></a><span class="s3">)</span>
<a name="l6069"><span class="ln">6069 </span></a>
<a name="l6070"><span class="ln">6070 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6071"><span class="ln">6071 </span></a>    <span class="s4">&quot;unsqueeze&quot;</span><span class="s3">,</span>
<a name="l6072"><span class="ln">6072 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6073"><span class="ln">6073 </span></a>unsqueeze(dim) -&gt; Tensor 
<a name="l6074"><span class="ln">6074 </span></a> 
<a name="l6075"><span class="ln">6075 </span></a>See :func:`torch.unsqueeze` 
<a name="l6076"><span class="ln">6076 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6077"><span class="ln">6077 </span></a><span class="s3">)</span>
<a name="l6078"><span class="ln">6078 </span></a>
<a name="l6079"><span class="ln">6079 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6080"><span class="ln">6080 </span></a>    <span class="s4">&quot;unsqueeze_&quot;</span><span class="s3">,</span>
<a name="l6081"><span class="ln">6081 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6082"><span class="ln">6082 </span></a>unsqueeze_(dim) -&gt; Tensor 
<a name="l6083"><span class="ln">6083 </span></a> 
<a name="l6084"><span class="ln">6084 </span></a>In-place version of :meth:`~Tensor.unsqueeze` 
<a name="l6085"><span class="ln">6085 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6086"><span class="ln">6086 </span></a><span class="s3">)</span>
<a name="l6087"><span class="ln">6087 </span></a>
<a name="l6088"><span class="ln">6088 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6089"><span class="ln">6089 </span></a>    <span class="s4">&quot;var&quot;</span><span class="s3">,</span>
<a name="l6090"><span class="ln">6090 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6091"><span class="ln">6091 </span></a>var(dim=None, *, correction=1, keepdim=False) -&gt; Tensor 
<a name="l6092"><span class="ln">6092 </span></a> 
<a name="l6093"><span class="ln">6093 </span></a>See :func:`torch.var` 
<a name="l6094"><span class="ln">6094 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6095"><span class="ln">6095 </span></a><span class="s3">)</span>
<a name="l6096"><span class="ln">6096 </span></a>
<a name="l6097"><span class="ln">6097 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6098"><span class="ln">6098 </span></a>    <span class="s4">&quot;vdot&quot;</span><span class="s3">,</span>
<a name="l6099"><span class="ln">6099 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6100"><span class="ln">6100 </span></a>vdot(other) -&gt; Tensor 
<a name="l6101"><span class="ln">6101 </span></a> 
<a name="l6102"><span class="ln">6102 </span></a>See :func:`torch.vdot` 
<a name="l6103"><span class="ln">6103 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6104"><span class="ln">6104 </span></a><span class="s3">)</span>
<a name="l6105"><span class="ln">6105 </span></a>
<a name="l6106"><span class="ln">6106 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6107"><span class="ln">6107 </span></a>    <span class="s4">&quot;view&quot;</span><span class="s3">,</span>
<a name="l6108"><span class="ln">6108 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6109"><span class="ln">6109 </span></a>view(*shape) -&gt; Tensor 
<a name="l6110"><span class="ln">6110 </span></a> 
<a name="l6111"><span class="ln">6111 </span></a>Returns a new tensor with the same data as the :attr:`self` tensor but of a 
<a name="l6112"><span class="ln">6112 </span></a>different :attr:`shape`. 
<a name="l6113"><span class="ln">6113 </span></a> 
<a name="l6114"><span class="ln">6114 </span></a>The returned tensor shares the same data and must have the same number 
<a name="l6115"><span class="ln">6115 </span></a>of elements, but may have a different size. For a tensor to be viewed, the new 
<a name="l6116"><span class="ln">6116 </span></a>view size must be compatible with its original size and stride, i.e., each new 
<a name="l6117"><span class="ln">6117 </span></a>view dimension must either be a subspace of an original dimension, or only span 
<a name="l6118"><span class="ln">6118 </span></a>across original dimensions :math:`d, d+1, \dots, d+k` that satisfy the following 
<a name="l6119"><span class="ln">6119 </span></a>contiguity-like condition that :math:`\forall i = d, \dots, d+k-1`, 
<a name="l6120"><span class="ln">6120 </span></a> 
<a name="l6121"><span class="ln">6121 </span></a>.. math:: 
<a name="l6122"><span class="ln">6122 </span></a> 
<a name="l6123"><span class="ln">6123 </span></a>  \text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1] 
<a name="l6124"><span class="ln">6124 </span></a> 
<a name="l6125"><span class="ln">6125 </span></a>Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape` 
<a name="l6126"><span class="ln">6126 </span></a>without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a 
<a name="l6127"><span class="ln">6127 </span></a>:meth:`view` can be performed, it is advisable to use :meth:`reshape`, which 
<a name="l6128"><span class="ln">6128 </span></a>returns a view if the shapes are compatible, and copies (equivalent to calling 
<a name="l6129"><span class="ln">6129 </span></a>:meth:`contiguous`) otherwise. 
<a name="l6130"><span class="ln">6130 </span></a> 
<a name="l6131"><span class="ln">6131 </span></a>Args: 
<a name="l6132"><span class="ln">6132 </span></a>    shape (torch.Size or int...): the desired size 
<a name="l6133"><span class="ln">6133 </span></a> 
<a name="l6134"><span class="ln">6134 </span></a>Example:: 
<a name="l6135"><span class="ln">6135 </span></a> 
<a name="l6136"><span class="ln">6136 </span></a>    &gt;&gt;&gt; x = torch.randn(4, 4) 
<a name="l6137"><span class="ln">6137 </span></a>    &gt;&gt;&gt; x.size() 
<a name="l6138"><span class="ln">6138 </span></a>    torch.Size([4, 4]) 
<a name="l6139"><span class="ln">6139 </span></a>    &gt;&gt;&gt; y = x.view(16) 
<a name="l6140"><span class="ln">6140 </span></a>    &gt;&gt;&gt; y.size() 
<a name="l6141"><span class="ln">6141 </span></a>    torch.Size([16]) 
<a name="l6142"><span class="ln">6142 </span></a>    &gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions 
<a name="l6143"><span class="ln">6143 </span></a>    &gt;&gt;&gt; z.size() 
<a name="l6144"><span class="ln">6144 </span></a>    torch.Size([2, 8]) 
<a name="l6145"><span class="ln">6145 </span></a> 
<a name="l6146"><span class="ln">6146 </span></a>    &gt;&gt;&gt; a = torch.randn(1, 2, 3, 4) 
<a name="l6147"><span class="ln">6147 </span></a>    &gt;&gt;&gt; a.size() 
<a name="l6148"><span class="ln">6148 </span></a>    torch.Size([1, 2, 3, 4]) 
<a name="l6149"><span class="ln">6149 </span></a>    &gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension 
<a name="l6150"><span class="ln">6150 </span></a>    &gt;&gt;&gt; b.size() 
<a name="l6151"><span class="ln">6151 </span></a>    torch.Size([1, 3, 2, 4]) 
<a name="l6152"><span class="ln">6152 </span></a>    &gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory 
<a name="l6153"><span class="ln">6153 </span></a>    &gt;&gt;&gt; c.size() 
<a name="l6154"><span class="ln">6154 </span></a>    torch.Size([1, 3, 2, 4]) 
<a name="l6155"><span class="ln">6155 </span></a>    &gt;&gt;&gt; torch.equal(b, c) 
<a name="l6156"><span class="ln">6156 </span></a>    False 
<a name="l6157"><span class="ln">6157 </span></a> 
<a name="l6158"><span class="ln">6158 </span></a> 
<a name="l6159"><span class="ln">6159 </span></a>.. method:: view(dtype) -&gt; Tensor 
<a name="l6160"><span class="ln">6160 </span></a>   :noindex: 
<a name="l6161"><span class="ln">6161 </span></a> 
<a name="l6162"><span class="ln">6162 </span></a>Returns a new tensor with the same data as the :attr:`self` tensor but of a 
<a name="l6163"><span class="ln">6163 </span></a>different :attr:`dtype`. 
<a name="l6164"><span class="ln">6164 </span></a> 
<a name="l6165"><span class="ln">6165 </span></a>If the element size of :attr:`dtype` is different than that of ``self.dtype``, 
<a name="l6166"><span class="ln">6166 </span></a>then the size of the last dimension of the output will be scaled 
<a name="l6167"><span class="ln">6167 </span></a>proportionally.  For instance, if :attr:`dtype` element size is twice that of 
<a name="l6168"><span class="ln">6168 </span></a>``self.dtype``, then each pair of elements in the last dimension of 
<a name="l6169"><span class="ln">6169 </span></a>:attr:`self` will be combined, and the size of the last dimension of the output 
<a name="l6170"><span class="ln">6170 </span></a>will be half that of :attr:`self`. If :attr:`dtype` element size is half that 
<a name="l6171"><span class="ln">6171 </span></a>of ``self.dtype``, then each element in the last dimension of :attr:`self` will 
<a name="l6172"><span class="ln">6172 </span></a>be split in two, and the size of the last dimension of the output will be 
<a name="l6173"><span class="ln">6173 </span></a>double that of :attr:`self`. For this to be possible, the following conditions 
<a name="l6174"><span class="ln">6174 </span></a>must be true: 
<a name="l6175"><span class="ln">6175 </span></a> 
<a name="l6176"><span class="ln">6176 </span></a>    * ``self.dim()`` must be greater than 0. 
<a name="l6177"><span class="ln">6177 </span></a>    * ``self.stride(-1)`` must be 1. 
<a name="l6178"><span class="ln">6178 </span></a> 
<a name="l6179"><span class="ln">6179 </span></a>Additionally, if the element size of :attr:`dtype` is greater than that of 
<a name="l6180"><span class="ln">6180 </span></a>``self.dtype``, the following conditions must be true as well: 
<a name="l6181"><span class="ln">6181 </span></a> 
<a name="l6182"><span class="ln">6182 </span></a>    * ``self.size(-1)`` must be divisible by the ratio between the element 
<a name="l6183"><span class="ln">6183 </span></a>      sizes of the dtypes. 
<a name="l6184"><span class="ln">6184 </span></a>    * ``self.storage_offset()`` must be divisible by the ratio between the 
<a name="l6185"><span class="ln">6185 </span></a>      element sizes of the dtypes. 
<a name="l6186"><span class="ln">6186 </span></a>    * The strides of all dimensions, except the last dimension, must be 
<a name="l6187"><span class="ln">6187 </span></a>      divisible by the ratio between the element sizes of the dtypes. 
<a name="l6188"><span class="ln">6188 </span></a> 
<a name="l6189"><span class="ln">6189 </span></a>If any of the above conditions are not met, an error is thrown. 
<a name="l6190"><span class="ln">6190 </span></a> 
<a name="l6191"><span class="ln">6191 </span></a>.. warning:: 
<a name="l6192"><span class="ln">6192 </span></a> 
<a name="l6193"><span class="ln">6193 </span></a>    This overload is not supported by TorchScript, and using it in a Torchscript 
<a name="l6194"><span class="ln">6194 </span></a>    program will cause undefined behavior. 
<a name="l6195"><span class="ln">6195 </span></a> 
<a name="l6196"><span class="ln">6196 </span></a> 
<a name="l6197"><span class="ln">6197 </span></a>Args: 
<a name="l6198"><span class="ln">6198 </span></a>    dtype (:class:`torch.dtype`): the desired dtype 
<a name="l6199"><span class="ln">6199 </span></a> 
<a name="l6200"><span class="ln">6200 </span></a>Example:: 
<a name="l6201"><span class="ln">6201 </span></a> 
<a name="l6202"><span class="ln">6202 </span></a>    &gt;&gt;&gt; x = torch.randn(4, 4) 
<a name="l6203"><span class="ln">6203 </span></a>    &gt;&gt;&gt; x 
<a name="l6204"><span class="ln">6204 </span></a>    tensor([[ 0.9482, -0.0310,  1.4999, -0.5316], 
<a name="l6205"><span class="ln">6205 </span></a>            [-0.1520,  0.7472,  0.5617, -0.8649], 
<a name="l6206"><span class="ln">6206 </span></a>            [-2.4724, -0.0334, -0.2976, -0.8499], 
<a name="l6207"><span class="ln">6207 </span></a>            [-0.2109,  1.9913, -0.9607, -0.6123]]) 
<a name="l6208"><span class="ln">6208 </span></a>    &gt;&gt;&gt; x.dtype 
<a name="l6209"><span class="ln">6209 </span></a>    torch.float32 
<a name="l6210"><span class="ln">6210 </span></a> 
<a name="l6211"><span class="ln">6211 </span></a>    &gt;&gt;&gt; y = x.view(torch.int32) 
<a name="l6212"><span class="ln">6212 </span></a>    &gt;&gt;&gt; y 
<a name="l6213"><span class="ln">6213 </span></a>    tensor([[ 1064483442, -1124191867,  1069546515, -1089989247], 
<a name="l6214"><span class="ln">6214 </span></a>            [-1105482831,  1061112040,  1057999968, -1084397505], 
<a name="l6215"><span class="ln">6215 </span></a>            [-1071760287, -1123489973, -1097310419, -1084649136], 
<a name="l6216"><span class="ln">6216 </span></a>            [-1101533110,  1073668768, -1082790149, -1088634448]], 
<a name="l6217"><span class="ln">6217 </span></a>        dtype=torch.int32) 
<a name="l6218"><span class="ln">6218 </span></a>    &gt;&gt;&gt; y[0, 0] = 1000000000 
<a name="l6219"><span class="ln">6219 </span></a>    &gt;&gt;&gt; x 
<a name="l6220"><span class="ln">6220 </span></a>    tensor([[ 0.0047, -0.0310,  1.4999, -0.5316], 
<a name="l6221"><span class="ln">6221 </span></a>            [-0.1520,  0.7472,  0.5617, -0.8649], 
<a name="l6222"><span class="ln">6222 </span></a>            [-2.4724, -0.0334, -0.2976, -0.8499], 
<a name="l6223"><span class="ln">6223 </span></a>            [-0.2109,  1.9913, -0.9607, -0.6123]]) 
<a name="l6224"><span class="ln">6224 </span></a> 
<a name="l6225"><span class="ln">6225 </span></a>    &gt;&gt;&gt; x.view(torch.cfloat) 
<a name="l6226"><span class="ln">6226 </span></a>    tensor([[ 0.0047-0.0310j,  1.4999-0.5316j], 
<a name="l6227"><span class="ln">6227 </span></a>            [-0.1520+0.7472j,  0.5617-0.8649j], 
<a name="l6228"><span class="ln">6228 </span></a>            [-2.4724-0.0334j, -0.2976-0.8499j], 
<a name="l6229"><span class="ln">6229 </span></a>            [-0.2109+1.9913j, -0.9607-0.6123j]]) 
<a name="l6230"><span class="ln">6230 </span></a>    &gt;&gt;&gt; x.view(torch.cfloat).size() 
<a name="l6231"><span class="ln">6231 </span></a>    torch.Size([4, 2]) 
<a name="l6232"><span class="ln">6232 </span></a> 
<a name="l6233"><span class="ln">6233 </span></a>    &gt;&gt;&gt; x.view(torch.uint8) 
<a name="l6234"><span class="ln">6234 </span></a>    tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22, 
<a name="l6235"><span class="ln">6235 </span></a>               8, 191], 
<a name="l6236"><span class="ln">6236 </span></a>            [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106, 
<a name="l6237"><span class="ln">6237 </span></a>              93, 191], 
<a name="l6238"><span class="ln">6238 </span></a>            [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147, 
<a name="l6239"><span class="ln">6239 </span></a>              89, 191], 
<a name="l6240"><span class="ln">6240 </span></a>            [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191, 
<a name="l6241"><span class="ln">6241 </span></a>              28, 191]], dtype=torch.uint8) 
<a name="l6242"><span class="ln">6242 </span></a>    &gt;&gt;&gt; x.view(torch.uint8).size() 
<a name="l6243"><span class="ln">6243 </span></a>    torch.Size([4, 16]) 
<a name="l6244"><span class="ln">6244 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6245"><span class="ln">6245 </span></a><span class="s3">)</span>
<a name="l6246"><span class="ln">6246 </span></a>
<a name="l6247"><span class="ln">6247 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6248"><span class="ln">6248 </span></a>    <span class="s4">&quot;view_as&quot;</span><span class="s3">,</span>
<a name="l6249"><span class="ln">6249 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6250"><span class="ln">6250 </span></a>view_as(other) -&gt; Tensor 
<a name="l6251"><span class="ln">6251 </span></a> 
<a name="l6252"><span class="ln">6252 </span></a>View this tensor as the same size as :attr:`other`. 
<a name="l6253"><span class="ln">6253 </span></a>``self.view_as(other)`` is equivalent to ``self.view(other.size())``. 
<a name="l6254"><span class="ln">6254 </span></a> 
<a name="l6255"><span class="ln">6255 </span></a>Please see :meth:`~Tensor.view` for more information about ``view``. 
<a name="l6256"><span class="ln">6256 </span></a> 
<a name="l6257"><span class="ln">6257 </span></a>Args: 
<a name="l6258"><span class="ln">6258 </span></a>    other (:class:`torch.Tensor`): The result tensor has the same size 
<a name="l6259"><span class="ln">6259 </span></a>        as :attr:`other`. 
<a name="l6260"><span class="ln">6260 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6261"><span class="ln">6261 </span></a><span class="s3">)</span>
<a name="l6262"><span class="ln">6262 </span></a>
<a name="l6263"><span class="ln">6263 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6264"><span class="ln">6264 </span></a>    <span class="s4">&quot;expand&quot;</span><span class="s3">,</span>
<a name="l6265"><span class="ln">6265 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6266"><span class="ln">6266 </span></a>expand(*sizes) -&gt; Tensor 
<a name="l6267"><span class="ln">6267 </span></a> 
<a name="l6268"><span class="ln">6268 </span></a>Returns a new view of the :attr:`self` tensor with singleton dimensions expanded 
<a name="l6269"><span class="ln">6269 </span></a>to a larger size. 
<a name="l6270"><span class="ln">6270 </span></a> 
<a name="l6271"><span class="ln">6271 </span></a>Passing -1 as the size for a dimension means not changing the size of 
<a name="l6272"><span class="ln">6272 </span></a>that dimension. 
<a name="l6273"><span class="ln">6273 </span></a> 
<a name="l6274"><span class="ln">6274 </span></a>Tensor can be also expanded to a larger number of dimensions, and the 
<a name="l6275"><span class="ln">6275 </span></a>new ones will be appended at the front. For the new dimensions, the 
<a name="l6276"><span class="ln">6276 </span></a>size cannot be set to -1. 
<a name="l6277"><span class="ln">6277 </span></a> 
<a name="l6278"><span class="ln">6278 </span></a>Expanding a tensor does not allocate new memory, but only creates a 
<a name="l6279"><span class="ln">6279 </span></a>new view on the existing tensor where a dimension of size one is 
<a name="l6280"><span class="ln">6280 </span></a>expanded to a larger size by setting the ``stride`` to 0. Any dimension 
<a name="l6281"><span class="ln">6281 </span></a>of size 1 can be expanded to an arbitrary value without allocating new 
<a name="l6282"><span class="ln">6282 </span></a>memory. 
<a name="l6283"><span class="ln">6283 </span></a> 
<a name="l6284"><span class="ln">6284 </span></a>Args: 
<a name="l6285"><span class="ln">6285 </span></a>    *sizes (torch.Size or int...): the desired expanded size 
<a name="l6286"><span class="ln">6286 </span></a> 
<a name="l6287"><span class="ln">6287 </span></a>.. warning:: 
<a name="l6288"><span class="ln">6288 </span></a> 
<a name="l6289"><span class="ln">6289 </span></a>    More than one element of an expanded tensor may refer to a single 
<a name="l6290"><span class="ln">6290 </span></a>    memory location. As a result, in-place operations (especially ones that 
<a name="l6291"><span class="ln">6291 </span></a>    are vectorized) may result in incorrect behavior. If you need to write 
<a name="l6292"><span class="ln">6292 </span></a>    to the tensors, please clone them first. 
<a name="l6293"><span class="ln">6293 </span></a> 
<a name="l6294"><span class="ln">6294 </span></a>Example:: 
<a name="l6295"><span class="ln">6295 </span></a> 
<a name="l6296"><span class="ln">6296 </span></a>    &gt;&gt;&gt; x = torch.tensor([[1], [2], [3]]) 
<a name="l6297"><span class="ln">6297 </span></a>    &gt;&gt;&gt; x.size() 
<a name="l6298"><span class="ln">6298 </span></a>    torch.Size([3, 1]) 
<a name="l6299"><span class="ln">6299 </span></a>    &gt;&gt;&gt; x.expand(3, 4) 
<a name="l6300"><span class="ln">6300 </span></a>    tensor([[ 1,  1,  1,  1], 
<a name="l6301"><span class="ln">6301 </span></a>            [ 2,  2,  2,  2], 
<a name="l6302"><span class="ln">6302 </span></a>            [ 3,  3,  3,  3]]) 
<a name="l6303"><span class="ln">6303 </span></a>    &gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension 
<a name="l6304"><span class="ln">6304 </span></a>    tensor([[ 1,  1,  1,  1], 
<a name="l6305"><span class="ln">6305 </span></a>            [ 2,  2,  2,  2], 
<a name="l6306"><span class="ln">6306 </span></a>            [ 3,  3,  3,  3]]) 
<a name="l6307"><span class="ln">6307 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6308"><span class="ln">6308 </span></a><span class="s3">)</span>
<a name="l6309"><span class="ln">6309 </span></a>
<a name="l6310"><span class="ln">6310 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6311"><span class="ln">6311 </span></a>    <span class="s4">&quot;expand_as&quot;</span><span class="s3">,</span>
<a name="l6312"><span class="ln">6312 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6313"><span class="ln">6313 </span></a>expand_as(other) -&gt; Tensor 
<a name="l6314"><span class="ln">6314 </span></a> 
<a name="l6315"><span class="ln">6315 </span></a>Expand this tensor to the same size as :attr:`other`. 
<a name="l6316"><span class="ln">6316 </span></a>``self.expand_as(other)`` is equivalent to ``self.expand(other.size())``. 
<a name="l6317"><span class="ln">6317 </span></a> 
<a name="l6318"><span class="ln">6318 </span></a>Please see :meth:`~Tensor.expand` for more information about ``expand``. 
<a name="l6319"><span class="ln">6319 </span></a> 
<a name="l6320"><span class="ln">6320 </span></a>Args: 
<a name="l6321"><span class="ln">6321 </span></a>    other (:class:`torch.Tensor`): The result tensor has the same size 
<a name="l6322"><span class="ln">6322 </span></a>        as :attr:`other`. 
<a name="l6323"><span class="ln">6323 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6324"><span class="ln">6324 </span></a><span class="s3">)</span>
<a name="l6325"><span class="ln">6325 </span></a>
<a name="l6326"><span class="ln">6326 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6327"><span class="ln">6327 </span></a>    <span class="s4">&quot;sum_to_size&quot;</span><span class="s3">,</span>
<a name="l6328"><span class="ln">6328 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6329"><span class="ln">6329 </span></a>sum_to_size(*size) -&gt; Tensor 
<a name="l6330"><span class="ln">6330 </span></a> 
<a name="l6331"><span class="ln">6331 </span></a>Sum ``this`` tensor to :attr:`size`. 
<a name="l6332"><span class="ln">6332 </span></a>:attr:`size` must be broadcastable to ``this`` tensor size. 
<a name="l6333"><span class="ln">6333 </span></a> 
<a name="l6334"><span class="ln">6334 </span></a>Args: 
<a name="l6335"><span class="ln">6335 </span></a>    size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l6336"><span class="ln">6336 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6337"><span class="ln">6337 </span></a><span class="s3">)</span>
<a name="l6338"><span class="ln">6338 </span></a>
<a name="l6339"><span class="ln">6339 </span></a>
<a name="l6340"><span class="ln">6340 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6341"><span class="ln">6341 </span></a>    <span class="s4">&quot;zero_&quot;</span><span class="s3">,</span>
<a name="l6342"><span class="ln">6342 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6343"><span class="ln">6343 </span></a>zero_() -&gt; Tensor 
<a name="l6344"><span class="ln">6344 </span></a> 
<a name="l6345"><span class="ln">6345 </span></a>Fills :attr:`self` tensor with zeros. 
<a name="l6346"><span class="ln">6346 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6347"><span class="ln">6347 </span></a><span class="s3">)</span>
<a name="l6348"><span class="ln">6348 </span></a>
<a name="l6349"><span class="ln">6349 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6350"><span class="ln">6350 </span></a>    <span class="s4">&quot;matmul&quot;</span><span class="s3">,</span>
<a name="l6351"><span class="ln">6351 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6352"><span class="ln">6352 </span></a>matmul(tensor2) -&gt; Tensor 
<a name="l6353"><span class="ln">6353 </span></a> 
<a name="l6354"><span class="ln">6354 </span></a>See :func:`torch.matmul` 
<a name="l6355"><span class="ln">6355 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6356"><span class="ln">6356 </span></a><span class="s3">)</span>
<a name="l6357"><span class="ln">6357 </span></a>
<a name="l6358"><span class="ln">6358 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6359"><span class="ln">6359 </span></a>    <span class="s4">&quot;chunk&quot;</span><span class="s3">,</span>
<a name="l6360"><span class="ln">6360 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6361"><span class="ln">6361 </span></a>chunk(chunks, dim=0) -&gt; List of Tensors 
<a name="l6362"><span class="ln">6362 </span></a> 
<a name="l6363"><span class="ln">6363 </span></a>See :func:`torch.chunk` 
<a name="l6364"><span class="ln">6364 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6365"><span class="ln">6365 </span></a><span class="s3">)</span>
<a name="l6366"><span class="ln">6366 </span></a>
<a name="l6367"><span class="ln">6367 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6368"><span class="ln">6368 </span></a>    <span class="s4">&quot;unsafe_chunk&quot;</span><span class="s3">,</span>
<a name="l6369"><span class="ln">6369 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6370"><span class="ln">6370 </span></a>unsafe_chunk(chunks, dim=0) -&gt; List of Tensors 
<a name="l6371"><span class="ln">6371 </span></a> 
<a name="l6372"><span class="ln">6372 </span></a>See :func:`torch.unsafe_chunk` 
<a name="l6373"><span class="ln">6373 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6374"><span class="ln">6374 </span></a><span class="s3">)</span>
<a name="l6375"><span class="ln">6375 </span></a>
<a name="l6376"><span class="ln">6376 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6377"><span class="ln">6377 </span></a>    <span class="s4">&quot;unsafe_split&quot;</span><span class="s3">,</span>
<a name="l6378"><span class="ln">6378 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6379"><span class="ln">6379 </span></a>unsafe_split(split_size, dim=0) -&gt; List of Tensors 
<a name="l6380"><span class="ln">6380 </span></a> 
<a name="l6381"><span class="ln">6381 </span></a>See :func:`torch.unsafe_split` 
<a name="l6382"><span class="ln">6382 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6383"><span class="ln">6383 </span></a><span class="s3">)</span>
<a name="l6384"><span class="ln">6384 </span></a>
<a name="l6385"><span class="ln">6385 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6386"><span class="ln">6386 </span></a>    <span class="s4">&quot;tensor_split&quot;</span><span class="s3">,</span>
<a name="l6387"><span class="ln">6387 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6388"><span class="ln">6388 </span></a>tensor_split(indices_or_sections, dim=0) -&gt; List of Tensors 
<a name="l6389"><span class="ln">6389 </span></a> 
<a name="l6390"><span class="ln">6390 </span></a>See :func:`torch.tensor_split` 
<a name="l6391"><span class="ln">6391 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6392"><span class="ln">6392 </span></a><span class="s3">)</span>
<a name="l6393"><span class="ln">6393 </span></a>
<a name="l6394"><span class="ln">6394 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6395"><span class="ln">6395 </span></a>    <span class="s4">&quot;hsplit&quot;</span><span class="s3">,</span>
<a name="l6396"><span class="ln">6396 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6397"><span class="ln">6397 </span></a>hsplit(split_size_or_sections) -&gt; List of Tensors 
<a name="l6398"><span class="ln">6398 </span></a> 
<a name="l6399"><span class="ln">6399 </span></a>See :func:`torch.hsplit` 
<a name="l6400"><span class="ln">6400 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6401"><span class="ln">6401 </span></a><span class="s3">)</span>
<a name="l6402"><span class="ln">6402 </span></a>
<a name="l6403"><span class="ln">6403 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6404"><span class="ln">6404 </span></a>    <span class="s4">&quot;vsplit&quot;</span><span class="s3">,</span>
<a name="l6405"><span class="ln">6405 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6406"><span class="ln">6406 </span></a>vsplit(split_size_or_sections) -&gt; List of Tensors 
<a name="l6407"><span class="ln">6407 </span></a> 
<a name="l6408"><span class="ln">6408 </span></a>See :func:`torch.vsplit` 
<a name="l6409"><span class="ln">6409 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6410"><span class="ln">6410 </span></a><span class="s3">)</span>
<a name="l6411"><span class="ln">6411 </span></a>
<a name="l6412"><span class="ln">6412 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6413"><span class="ln">6413 </span></a>    <span class="s4">&quot;dsplit&quot;</span><span class="s3">,</span>
<a name="l6414"><span class="ln">6414 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6415"><span class="ln">6415 </span></a>dsplit(split_size_or_sections) -&gt; List of Tensors 
<a name="l6416"><span class="ln">6416 </span></a> 
<a name="l6417"><span class="ln">6417 </span></a>See :func:`torch.dsplit` 
<a name="l6418"><span class="ln">6418 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6419"><span class="ln">6419 </span></a><span class="s3">)</span>
<a name="l6420"><span class="ln">6420 </span></a>
<a name="l6421"><span class="ln">6421 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6422"><span class="ln">6422 </span></a>    <span class="s4">&quot;stft&quot;</span><span class="s3">,</span>
<a name="l6423"><span class="ln">6423 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6424"><span class="ln">6424 </span></a>stft(frame_length, hop, fft_size=None, return_onesided=True, window=None, 
<a name="l6425"><span class="ln">6425 </span></a> pad_end=0, align_to_window=None) -&gt; Tensor 
<a name="l6426"><span class="ln">6426 </span></a> 
<a name="l6427"><span class="ln">6427 </span></a>See :func:`torch.stft` 
<a name="l6428"><span class="ln">6428 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6429"><span class="ln">6429 </span></a><span class="s3">)</span>
<a name="l6430"><span class="ln">6430 </span></a>
<a name="l6431"><span class="ln">6431 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6432"><span class="ln">6432 </span></a>    <span class="s4">&quot;istft&quot;</span><span class="s3">,</span>
<a name="l6433"><span class="ln">6433 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6434"><span class="ln">6434 </span></a>istft(n_fft, hop_length=None, win_length=None, window=None, 
<a name="l6435"><span class="ln">6435 </span></a> center=True, normalized=False, onesided=True, length=None) -&gt; Tensor 
<a name="l6436"><span class="ln">6436 </span></a> 
<a name="l6437"><span class="ln">6437 </span></a>See :func:`torch.istft` 
<a name="l6438"><span class="ln">6438 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6439"><span class="ln">6439 </span></a><span class="s3">)</span>
<a name="l6440"><span class="ln">6440 </span></a>
<a name="l6441"><span class="ln">6441 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6442"><span class="ln">6442 </span></a>    <span class="s4">&quot;det&quot;</span><span class="s3">,</span>
<a name="l6443"><span class="ln">6443 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6444"><span class="ln">6444 </span></a>det() -&gt; Tensor 
<a name="l6445"><span class="ln">6445 </span></a> 
<a name="l6446"><span class="ln">6446 </span></a>See :func:`torch.det` 
<a name="l6447"><span class="ln">6447 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6448"><span class="ln">6448 </span></a><span class="s3">)</span>
<a name="l6449"><span class="ln">6449 </span></a>
<a name="l6450"><span class="ln">6450 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6451"><span class="ln">6451 </span></a>    <span class="s4">&quot;where&quot;</span><span class="s3">,</span>
<a name="l6452"><span class="ln">6452 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6453"><span class="ln">6453 </span></a>where(condition, y) -&gt; Tensor 
<a name="l6454"><span class="ln">6454 </span></a> 
<a name="l6455"><span class="ln">6455 </span></a>``self.where(condition, y)`` is equivalent to ``torch.where(condition, self, y)``. 
<a name="l6456"><span class="ln">6456 </span></a>See :func:`torch.where` 
<a name="l6457"><span class="ln">6457 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6458"><span class="ln">6458 </span></a><span class="s3">)</span>
<a name="l6459"><span class="ln">6459 </span></a>
<a name="l6460"><span class="ln">6460 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6461"><span class="ln">6461 </span></a>    <span class="s4">&quot;logdet&quot;</span><span class="s3">,</span>
<a name="l6462"><span class="ln">6462 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6463"><span class="ln">6463 </span></a>logdet() -&gt; Tensor 
<a name="l6464"><span class="ln">6464 </span></a> 
<a name="l6465"><span class="ln">6465 </span></a>See :func:`torch.logdet` 
<a name="l6466"><span class="ln">6466 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6467"><span class="ln">6467 </span></a><span class="s3">)</span>
<a name="l6468"><span class="ln">6468 </span></a>
<a name="l6469"><span class="ln">6469 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6470"><span class="ln">6470 </span></a>    <span class="s4">&quot;slogdet&quot;</span><span class="s3">,</span>
<a name="l6471"><span class="ln">6471 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6472"><span class="ln">6472 </span></a>slogdet() -&gt; (Tensor, Tensor) 
<a name="l6473"><span class="ln">6473 </span></a> 
<a name="l6474"><span class="ln">6474 </span></a>See :func:`torch.slogdet` 
<a name="l6475"><span class="ln">6475 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6476"><span class="ln">6476 </span></a><span class="s3">)</span>
<a name="l6477"><span class="ln">6477 </span></a>
<a name="l6478"><span class="ln">6478 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6479"><span class="ln">6479 </span></a>    <span class="s4">&quot;unbind&quot;</span><span class="s3">,</span>
<a name="l6480"><span class="ln">6480 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6481"><span class="ln">6481 </span></a>unbind(dim=0) -&gt; seq 
<a name="l6482"><span class="ln">6482 </span></a> 
<a name="l6483"><span class="ln">6483 </span></a>See :func:`torch.unbind` 
<a name="l6484"><span class="ln">6484 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6485"><span class="ln">6485 </span></a><span class="s3">)</span>
<a name="l6486"><span class="ln">6486 </span></a>
<a name="l6487"><span class="ln">6487 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6488"><span class="ln">6488 </span></a>    <span class="s4">&quot;pin_memory&quot;</span><span class="s3">,</span>
<a name="l6489"><span class="ln">6489 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6490"><span class="ln">6490 </span></a>pin_memory() -&gt; Tensor 
<a name="l6491"><span class="ln">6491 </span></a> 
<a name="l6492"><span class="ln">6492 </span></a>Copies the tensor to pinned memory, if it's not already pinned. 
<a name="l6493"><span class="ln">6493 </span></a>By default, the device pinned memory on will be the current :ref:`accelerator&lt;accelerators&gt;`. 
<a name="l6494"><span class="ln">6494 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6495"><span class="ln">6495 </span></a><span class="s3">)</span>
<a name="l6496"><span class="ln">6496 </span></a>
<a name="l6497"><span class="ln">6497 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6498"><span class="ln">6498 </span></a>    <span class="s4">&quot;pinverse&quot;</span><span class="s3">,</span>
<a name="l6499"><span class="ln">6499 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6500"><span class="ln">6500 </span></a>pinverse() -&gt; Tensor 
<a name="l6501"><span class="ln">6501 </span></a> 
<a name="l6502"><span class="ln">6502 </span></a>See :func:`torch.pinverse` 
<a name="l6503"><span class="ln">6503 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6504"><span class="ln">6504 </span></a><span class="s3">)</span>
<a name="l6505"><span class="ln">6505 </span></a>
<a name="l6506"><span class="ln">6506 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6507"><span class="ln">6507 </span></a>    <span class="s4">&quot;index_add&quot;</span><span class="s3">,</span>
<a name="l6508"><span class="ln">6508 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6509"><span class="ln">6509 </span></a>index_add(dim, index, source, *, alpha=1) -&gt; Tensor 
<a name="l6510"><span class="ln">6510 </span></a> 
<a name="l6511"><span class="ln">6511 </span></a>Out-of-place version of :meth:`torch.Tensor.index_add_`. 
<a name="l6512"><span class="ln">6512 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6513"><span class="ln">6513 </span></a><span class="s3">)</span>
<a name="l6514"><span class="ln">6514 </span></a>
<a name="l6515"><span class="ln">6515 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6516"><span class="ln">6516 </span></a>    <span class="s4">&quot;index_copy&quot;</span><span class="s3">,</span>
<a name="l6517"><span class="ln">6517 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6518"><span class="ln">6518 </span></a>index_copy(dim, index, tensor2) -&gt; Tensor 
<a name="l6519"><span class="ln">6519 </span></a> 
<a name="l6520"><span class="ln">6520 </span></a>Out-of-place version of :meth:`torch.Tensor.index_copy_`. 
<a name="l6521"><span class="ln">6521 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6522"><span class="ln">6522 </span></a><span class="s3">)</span>
<a name="l6523"><span class="ln">6523 </span></a>
<a name="l6524"><span class="ln">6524 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6525"><span class="ln">6525 </span></a>    <span class="s4">&quot;index_fill&quot;</span><span class="s3">,</span>
<a name="l6526"><span class="ln">6526 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6527"><span class="ln">6527 </span></a>index_fill(dim, index, value) -&gt; Tensor 
<a name="l6528"><span class="ln">6528 </span></a> 
<a name="l6529"><span class="ln">6529 </span></a>Out-of-place version of :meth:`torch.Tensor.index_fill_`. 
<a name="l6530"><span class="ln">6530 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6531"><span class="ln">6531 </span></a><span class="s3">)</span>
<a name="l6532"><span class="ln">6532 </span></a>
<a name="l6533"><span class="ln">6533 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6534"><span class="ln">6534 </span></a>    <span class="s4">&quot;scatter&quot;</span><span class="s3">,</span>
<a name="l6535"><span class="ln">6535 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6536"><span class="ln">6536 </span></a>scatter(dim, index, src) -&gt; Tensor 
<a name="l6537"><span class="ln">6537 </span></a> 
<a name="l6538"><span class="ln">6538 </span></a>Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l6539"><span class="ln">6539 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6540"><span class="ln">6540 </span></a><span class="s3">)</span>
<a name="l6541"><span class="ln">6541 </span></a>
<a name="l6542"><span class="ln">6542 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6543"><span class="ln">6543 </span></a>    <span class="s4">&quot;scatter_add&quot;</span><span class="s3">,</span>
<a name="l6544"><span class="ln">6544 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6545"><span class="ln">6545 </span></a>scatter_add(dim, index, src) -&gt; Tensor 
<a name="l6546"><span class="ln">6546 </span></a> 
<a name="l6547"><span class="ln">6547 </span></a>Out-of-place version of :meth:`torch.Tensor.scatter_add_` 
<a name="l6548"><span class="ln">6548 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6549"><span class="ln">6549 </span></a><span class="s3">)</span>
<a name="l6550"><span class="ln">6550 </span></a>
<a name="l6551"><span class="ln">6551 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6552"><span class="ln">6552 </span></a>    <span class="s4">&quot;scatter_reduce&quot;</span><span class="s3">,</span>
<a name="l6553"><span class="ln">6553 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6554"><span class="ln">6554 </span></a>scatter_reduce(dim, index, src, reduce, *, include_self=True) -&gt; Tensor 
<a name="l6555"><span class="ln">6555 </span></a> 
<a name="l6556"><span class="ln">6556 </span></a>Out-of-place version of :meth:`torch.Tensor.scatter_reduce_` 
<a name="l6557"><span class="ln">6557 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6558"><span class="ln">6558 </span></a><span class="s3">)</span>
<a name="l6559"><span class="ln">6559 </span></a>
<a name="l6560"><span class="ln">6560 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6561"><span class="ln">6561 </span></a>    <span class="s4">&quot;masked_scatter&quot;</span><span class="s3">,</span>
<a name="l6562"><span class="ln">6562 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6563"><span class="ln">6563 </span></a>masked_scatter(mask, tensor) -&gt; Tensor 
<a name="l6564"><span class="ln">6564 </span></a> 
<a name="l6565"><span class="ln">6565 </span></a>Out-of-place version of :meth:`torch.Tensor.masked_scatter_` 
<a name="l6566"><span class="ln">6566 </span></a> 
<a name="l6567"><span class="ln">6567 </span></a>.. note:: 
<a name="l6568"><span class="ln">6568 </span></a> 
<a name="l6569"><span class="ln">6569 </span></a>    The inputs :attr:`self` and :attr:`mask` 
<a name="l6570"><span class="ln">6570 </span></a>    :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l6571"><span class="ln">6571 </span></a> 
<a name="l6572"><span class="ln">6572 </span></a>Example: 
<a name="l6573"><span class="ln">6573 </span></a> 
<a name="l6574"><span class="ln">6574 </span></a>    &gt;&gt;&gt; self = torch.tensor([0, 0, 0, 0, 0]) 
<a name="l6575"><span class="ln">6575 </span></a>    &gt;&gt;&gt; mask = torch.tensor( 
<a name="l6576"><span class="ln">6576 </span></a>    ...     [[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], 
<a name="l6577"><span class="ln">6577 </span></a>    ...     dtype=torch.bool, 
<a name="l6578"><span class="ln">6578 </span></a>    ... ) 
<a name="l6579"><span class="ln">6579 </span></a>    &gt;&gt;&gt; source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) 
<a name="l6580"><span class="ln">6580 </span></a>    &gt;&gt;&gt; self.masked_scatter(mask, source) 
<a name="l6581"><span class="ln">6581 </span></a>    tensor([[0, 0, 0, 0, 1], 
<a name="l6582"><span class="ln">6582 </span></a>            [2, 3, 0, 4, 5]]) 
<a name="l6583"><span class="ln">6583 </span></a> 
<a name="l6584"><span class="ln">6584 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6585"><span class="ln">6585 </span></a><span class="s3">)</span>
<a name="l6586"><span class="ln">6586 </span></a>
<a name="l6587"><span class="ln">6587 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6588"><span class="ln">6588 </span></a>    <span class="s4">&quot;xlogy&quot;</span><span class="s3">,</span>
<a name="l6589"><span class="ln">6589 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6590"><span class="ln">6590 </span></a>xlogy(other) -&gt; Tensor 
<a name="l6591"><span class="ln">6591 </span></a> 
<a name="l6592"><span class="ln">6592 </span></a>See :func:`torch.xlogy` 
<a name="l6593"><span class="ln">6593 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6594"><span class="ln">6594 </span></a><span class="s3">)</span>
<a name="l6595"><span class="ln">6595 </span></a>
<a name="l6596"><span class="ln">6596 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6597"><span class="ln">6597 </span></a>    <span class="s4">&quot;xlogy_&quot;</span><span class="s3">,</span>
<a name="l6598"><span class="ln">6598 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6599"><span class="ln">6599 </span></a>xlogy_(other) -&gt; Tensor 
<a name="l6600"><span class="ln">6600 </span></a> 
<a name="l6601"><span class="ln">6601 </span></a>In-place version of :meth:`~Tensor.xlogy` 
<a name="l6602"><span class="ln">6602 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6603"><span class="ln">6603 </span></a><span class="s3">)</span>
<a name="l6604"><span class="ln">6604 </span></a>
<a name="l6605"><span class="ln">6605 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6606"><span class="ln">6606 </span></a>    <span class="s4">&quot;masked_fill&quot;</span><span class="s3">,</span>
<a name="l6607"><span class="ln">6607 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6608"><span class="ln">6608 </span></a>masked_fill(mask, value) -&gt; Tensor 
<a name="l6609"><span class="ln">6609 </span></a> 
<a name="l6610"><span class="ln">6610 </span></a>Out-of-place version of :meth:`torch.Tensor.masked_fill_` 
<a name="l6611"><span class="ln">6611 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6612"><span class="ln">6612 </span></a><span class="s3">)</span>
<a name="l6613"><span class="ln">6613 </span></a>
<a name="l6614"><span class="ln">6614 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6615"><span class="ln">6615 </span></a>    <span class="s4">&quot;grad&quot;</span><span class="s3">,</span>
<a name="l6616"><span class="ln">6616 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6617"><span class="ln">6617 </span></a>This attribute is ``None`` by default and becomes a Tensor the first time a call to 
<a name="l6618"><span class="ln">6618 </span></a>:func:`backward` computes gradients for ``self``. 
<a name="l6619"><span class="ln">6619 </span></a>The attribute will then contain the gradients computed and future calls to 
<a name="l6620"><span class="ln">6620 </span></a>:func:`backward` will accumulate (add) gradients into it. 
<a name="l6621"><span class="ln">6621 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6622"><span class="ln">6622 </span></a><span class="s3">)</span>
<a name="l6623"><span class="ln">6623 </span></a>
<a name="l6624"><span class="ln">6624 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6625"><span class="ln">6625 </span></a>    <span class="s4">&quot;retain_grad&quot;</span><span class="s3">,</span>
<a name="l6626"><span class="ln">6626 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6627"><span class="ln">6627 </span></a>retain_grad() -&gt; None 
<a name="l6628"><span class="ln">6628 </span></a> 
<a name="l6629"><span class="ln">6629 </span></a>Enables this Tensor to have their :attr:`grad` populated during 
<a name="l6630"><span class="ln">6630 </span></a>:func:`backward`. This is a no-op for leaf tensors. 
<a name="l6631"><span class="ln">6631 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6632"><span class="ln">6632 </span></a><span class="s3">)</span>
<a name="l6633"><span class="ln">6633 </span></a>
<a name="l6634"><span class="ln">6634 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6635"><span class="ln">6635 </span></a>    <span class="s4">&quot;retains_grad&quot;</span><span class="s3">,</span>
<a name="l6636"><span class="ln">6636 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6637"><span class="ln">6637 </span></a>Is ``True`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be 
<a name="l6638"><span class="ln">6638 </span></a>populated during :func:`backward`, ``False`` otherwise. 
<a name="l6639"><span class="ln">6639 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6640"><span class="ln">6640 </span></a><span class="s3">)</span>
<a name="l6641"><span class="ln">6641 </span></a>
<a name="l6642"><span class="ln">6642 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6643"><span class="ln">6643 </span></a>    <span class="s4">&quot;requires_grad&quot;</span><span class="s3">,</span>
<a name="l6644"><span class="ln">6644 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6645"><span class="ln">6645 </span></a>Is ``True`` if gradients need to be computed for this Tensor, ``False`` otherwise. 
<a name="l6646"><span class="ln">6646 </span></a> 
<a name="l6647"><span class="ln">6647 </span></a>.. note:: 
<a name="l6648"><span class="ln">6648 </span></a> 
<a name="l6649"><span class="ln">6649 </span></a>    The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad` 
<a name="l6650"><span class="ln">6650 </span></a>    attribute will be populated, see :attr:`is_leaf` for more details. 
<a name="l6651"><span class="ln">6651 </span></a> 
<a name="l6652"><span class="ln">6652 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6653"><span class="ln">6653 </span></a><span class="s3">)</span>
<a name="l6654"><span class="ln">6654 </span></a>
<a name="l6655"><span class="ln">6655 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6656"><span class="ln">6656 </span></a>    <span class="s4">&quot;is_leaf&quot;</span><span class="s3">,</span>
<a name="l6657"><span class="ln">6657 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6658"><span class="ln">6658 </span></a>All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention. 
<a name="l6659"><span class="ln">6659 </span></a> 
<a name="l6660"><span class="ln">6660 </span></a>For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were 
<a name="l6661"><span class="ln">6661 </span></a>created by the user. This means that they are not the result of an operation and so 
<a name="l6662"><span class="ln">6662 </span></a>:attr:`grad_fn` is None. 
<a name="l6663"><span class="ln">6663 </span></a> 
<a name="l6664"><span class="ln">6664 </span></a>Only leaf Tensors will have their :attr:`grad` populated during a call to :func:`backward`. 
<a name="l6665"><span class="ln">6665 </span></a>To get :attr:`grad` populated for non-leaf Tensors, you can use :func:`retain_grad`. 
<a name="l6666"><span class="ln">6666 </span></a> 
<a name="l6667"><span class="ln">6667 </span></a>Example:: 
<a name="l6668"><span class="ln">6668 </span></a> 
<a name="l6669"><span class="ln">6669 </span></a>    &gt;&gt;&gt; a = torch.rand(10, requires_grad=True) 
<a name="l6670"><span class="ln">6670 </span></a>    &gt;&gt;&gt; a.is_leaf 
<a name="l6671"><span class="ln">6671 </span></a>    True 
<a name="l6672"><span class="ln">6672 </span></a>    &gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda() 
<a name="l6673"><span class="ln">6673 </span></a>    &gt;&gt;&gt; b.is_leaf 
<a name="l6674"><span class="ln">6674 </span></a>    False 
<a name="l6675"><span class="ln">6675 </span></a>    # b was created by the operation that cast a cpu Tensor into a cuda Tensor 
<a name="l6676"><span class="ln">6676 </span></a>    &gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2 
<a name="l6677"><span class="ln">6677 </span></a>    &gt;&gt;&gt; c.is_leaf 
<a name="l6678"><span class="ln">6678 </span></a>    False 
<a name="l6679"><span class="ln">6679 </span></a>    # c was created by the addition operation 
<a name="l6680"><span class="ln">6680 </span></a>    &gt;&gt;&gt; d = torch.rand(10).cuda() 
<a name="l6681"><span class="ln">6681 </span></a>    &gt;&gt;&gt; d.is_leaf 
<a name="l6682"><span class="ln">6682 </span></a>    True 
<a name="l6683"><span class="ln">6683 </span></a>    # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine) 
<a name="l6684"><span class="ln">6684 </span></a>    &gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_() 
<a name="l6685"><span class="ln">6685 </span></a>    &gt;&gt;&gt; e.is_leaf 
<a name="l6686"><span class="ln">6686 </span></a>    True 
<a name="l6687"><span class="ln">6687 </span></a>    # e requires gradients and has no operations creating it 
<a name="l6688"><span class="ln">6688 </span></a>    &gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=&quot;cuda&quot;) 
<a name="l6689"><span class="ln">6689 </span></a>    &gt;&gt;&gt; f.is_leaf 
<a name="l6690"><span class="ln">6690 </span></a>    True 
<a name="l6691"><span class="ln">6691 </span></a>    # f requires grad, has no operation creating it 
<a name="l6692"><span class="ln">6692 </span></a> 
<a name="l6693"><span class="ln">6693 </span></a> 
<a name="l6694"><span class="ln">6694 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6695"><span class="ln">6695 </span></a><span class="s3">)</span>
<a name="l6696"><span class="ln">6696 </span></a>
<a name="l6697"><span class="ln">6697 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6698"><span class="ln">6698 </span></a>    <span class="s4">&quot;names&quot;</span><span class="s3">,</span>
<a name="l6699"><span class="ln">6699 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6700"><span class="ln">6700 </span></a>Stores names for each of this tensor's dimensions. 
<a name="l6701"><span class="ln">6701 </span></a> 
<a name="l6702"><span class="ln">6702 </span></a>``names[idx]`` corresponds to the name of tensor dimension ``idx``. 
<a name="l6703"><span class="ln">6703 </span></a>Names are either a string if the dimension is named or ``None`` if the 
<a name="l6704"><span class="ln">6704 </span></a>dimension is unnamed. 
<a name="l6705"><span class="ln">6705 </span></a> 
<a name="l6706"><span class="ln">6706 </span></a>Dimension names may contain characters or underscore. Furthermore, a dimension 
<a name="l6707"><span class="ln">6707 </span></a>name must be a valid Python variable name (i.e., does not start with underscore). 
<a name="l6708"><span class="ln">6708 </span></a> 
<a name="l6709"><span class="ln">6709 </span></a>Tensors may not have two named dimensions with the same name. 
<a name="l6710"><span class="ln">6710 </span></a> 
<a name="l6711"><span class="ln">6711 </span></a>.. warning:: 
<a name="l6712"><span class="ln">6712 </span></a>    The named tensor API is experimental and subject to change. 
<a name="l6713"><span class="ln">6713 </span></a> 
<a name="l6714"><span class="ln">6714 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6715"><span class="ln">6715 </span></a><span class="s3">)</span>
<a name="l6716"><span class="ln">6716 </span></a>
<a name="l6717"><span class="ln">6717 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6718"><span class="ln">6718 </span></a>    <span class="s4">&quot;is_cuda&quot;</span><span class="s3">,</span>
<a name="l6719"><span class="ln">6719 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6720"><span class="ln">6720 </span></a>Is ``True`` if the Tensor is stored on the GPU, ``False`` otherwise. 
<a name="l6721"><span class="ln">6721 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6722"><span class="ln">6722 </span></a><span class="s3">)</span>
<a name="l6723"><span class="ln">6723 </span></a>
<a name="l6724"><span class="ln">6724 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6725"><span class="ln">6725 </span></a>    <span class="s4">&quot;is_cpu&quot;</span><span class="s3">,</span>
<a name="l6726"><span class="ln">6726 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6727"><span class="ln">6727 </span></a>Is ``True`` if the Tensor is stored on the CPU, ``False`` otherwise. 
<a name="l6728"><span class="ln">6728 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6729"><span class="ln">6729 </span></a><span class="s3">)</span>
<a name="l6730"><span class="ln">6730 </span></a>
<a name="l6731"><span class="ln">6731 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6732"><span class="ln">6732 </span></a>    <span class="s4">&quot;is_xla&quot;</span><span class="s3">,</span>
<a name="l6733"><span class="ln">6733 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6734"><span class="ln">6734 </span></a>Is ``True`` if the Tensor is stored on an XLA device, ``False`` otherwise. 
<a name="l6735"><span class="ln">6735 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6736"><span class="ln">6736 </span></a><span class="s3">)</span>
<a name="l6737"><span class="ln">6737 </span></a>
<a name="l6738"><span class="ln">6738 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6739"><span class="ln">6739 </span></a>    <span class="s4">&quot;is_ipu&quot;</span><span class="s3">,</span>
<a name="l6740"><span class="ln">6740 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6741"><span class="ln">6741 </span></a>Is ``True`` if the Tensor is stored on the IPU, ``False`` otherwise. 
<a name="l6742"><span class="ln">6742 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6743"><span class="ln">6743 </span></a><span class="s3">)</span>
<a name="l6744"><span class="ln">6744 </span></a>
<a name="l6745"><span class="ln">6745 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6746"><span class="ln">6746 </span></a>    <span class="s4">&quot;is_xpu&quot;</span><span class="s3">,</span>
<a name="l6747"><span class="ln">6747 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6748"><span class="ln">6748 </span></a>Is ``True`` if the Tensor is stored on the XPU, ``False`` otherwise. 
<a name="l6749"><span class="ln">6749 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6750"><span class="ln">6750 </span></a><span class="s3">)</span>
<a name="l6751"><span class="ln">6751 </span></a>
<a name="l6752"><span class="ln">6752 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6753"><span class="ln">6753 </span></a>    <span class="s4">&quot;is_quantized&quot;</span><span class="s3">,</span>
<a name="l6754"><span class="ln">6754 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6755"><span class="ln">6755 </span></a>Is ``True`` if the Tensor is quantized, ``False`` otherwise. 
<a name="l6756"><span class="ln">6756 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6757"><span class="ln">6757 </span></a><span class="s3">)</span>
<a name="l6758"><span class="ln">6758 </span></a>
<a name="l6759"><span class="ln">6759 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6760"><span class="ln">6760 </span></a>    <span class="s4">&quot;is_meta&quot;</span><span class="s3">,</span>
<a name="l6761"><span class="ln">6761 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6762"><span class="ln">6762 </span></a>Is ``True`` if the Tensor is a meta tensor, ``False`` otherwise.  Meta tensors 
<a name="l6763"><span class="ln">6763 </span></a>are like normal tensors, but they carry no data. 
<a name="l6764"><span class="ln">6764 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6765"><span class="ln">6765 </span></a><span class="s3">)</span>
<a name="l6766"><span class="ln">6766 </span></a>
<a name="l6767"><span class="ln">6767 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6768"><span class="ln">6768 </span></a>    <span class="s4">&quot;is_mps&quot;</span><span class="s3">,</span>
<a name="l6769"><span class="ln">6769 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6770"><span class="ln">6770 </span></a>Is ``True`` if the Tensor is stored on the MPS device, ``False`` otherwise. 
<a name="l6771"><span class="ln">6771 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6772"><span class="ln">6772 </span></a><span class="s3">)</span>
<a name="l6773"><span class="ln">6773 </span></a>
<a name="l6774"><span class="ln">6774 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6775"><span class="ln">6775 </span></a>    <span class="s4">&quot;is_sparse&quot;</span><span class="s3">,</span>
<a name="l6776"><span class="ln">6776 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6777"><span class="ln">6777 </span></a>Is ``True`` if the Tensor uses sparse COO storage layout, ``False`` otherwise. 
<a name="l6778"><span class="ln">6778 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6779"><span class="ln">6779 </span></a><span class="s3">)</span>
<a name="l6780"><span class="ln">6780 </span></a>
<a name="l6781"><span class="ln">6781 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6782"><span class="ln">6782 </span></a>    <span class="s4">&quot;is_sparse_csr&quot;</span><span class="s3">,</span>
<a name="l6783"><span class="ln">6783 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6784"><span class="ln">6784 </span></a>Is ``True`` if the Tensor uses sparse CSR storage layout, ``False`` otherwise. 
<a name="l6785"><span class="ln">6785 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6786"><span class="ln">6786 </span></a><span class="s3">)</span>
<a name="l6787"><span class="ln">6787 </span></a>
<a name="l6788"><span class="ln">6788 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6789"><span class="ln">6789 </span></a>    <span class="s4">&quot;device&quot;</span><span class="s3">,</span>
<a name="l6790"><span class="ln">6790 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6791"><span class="ln">6791 </span></a>Is the :class:`torch.device` where this Tensor is. 
<a name="l6792"><span class="ln">6792 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6793"><span class="ln">6793 </span></a><span class="s3">)</span>
<a name="l6794"><span class="ln">6794 </span></a>
<a name="l6795"><span class="ln">6795 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6796"><span class="ln">6796 </span></a>    <span class="s4">&quot;ndim&quot;</span><span class="s3">,</span>
<a name="l6797"><span class="ln">6797 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6798"><span class="ln">6798 </span></a>Alias for :meth:`~Tensor.dim()` 
<a name="l6799"><span class="ln">6799 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6800"><span class="ln">6800 </span></a><span class="s3">)</span>
<a name="l6801"><span class="ln">6801 </span></a>
<a name="l6802"><span class="ln">6802 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6803"><span class="ln">6803 </span></a>    <span class="s4">&quot;itemsize&quot;</span><span class="s3">,</span>
<a name="l6804"><span class="ln">6804 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6805"><span class="ln">6805 </span></a>Alias for :meth:`~Tensor.element_size()` 
<a name="l6806"><span class="ln">6806 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6807"><span class="ln">6807 </span></a><span class="s3">)</span>
<a name="l6808"><span class="ln">6808 </span></a>
<a name="l6809"><span class="ln">6809 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6810"><span class="ln">6810 </span></a>    <span class="s4">&quot;nbytes&quot;</span><span class="s3">,</span>
<a name="l6811"><span class="ln">6811 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6812"><span class="ln">6812 </span></a>Returns the number of bytes consumed by the &quot;view&quot; of elements of the Tensor 
<a name="l6813"><span class="ln">6813 </span></a>if the Tensor does not use sparse storage layout. 
<a name="l6814"><span class="ln">6814 </span></a>Defined to be :meth:`~Tensor.numel()` * :meth:`~Tensor.element_size()` 
<a name="l6815"><span class="ln">6815 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6816"><span class="ln">6816 </span></a><span class="s3">)</span>
<a name="l6817"><span class="ln">6817 </span></a>
<a name="l6818"><span class="ln">6818 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6819"><span class="ln">6819 </span></a>    <span class="s4">&quot;T&quot;</span><span class="s3">,</span>
<a name="l6820"><span class="ln">6820 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6821"><span class="ln">6821 </span></a>Returns a view of this tensor with its dimensions reversed. 
<a name="l6822"><span class="ln">6822 </span></a> 
<a name="l6823"><span class="ln">6823 </span></a>If ``n`` is the number of dimensions in ``x``, 
<a name="l6824"><span class="ln">6824 </span></a>``x.T`` is equivalent to ``x.permute(n-1, n-2, ..., 0)``. 
<a name="l6825"><span class="ln">6825 </span></a> 
<a name="l6826"><span class="ln">6826 </span></a>.. warning:: 
<a name="l6827"><span class="ln">6827 </span></a>    The use of :func:`Tensor.T` on tensors of dimension other than 2 to reverse their shape 
<a name="l6828"><span class="ln">6828 </span></a>    is deprecated and it will throw an error in a future release. Consider :attr:`~.Tensor.mT` 
<a name="l6829"><span class="ln">6829 </span></a>    to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse 
<a name="l6830"><span class="ln">6830 </span></a>    the dimensions of a tensor. 
<a name="l6831"><span class="ln">6831 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6832"><span class="ln">6832 </span></a><span class="s3">)</span>
<a name="l6833"><span class="ln">6833 </span></a>
<a name="l6834"><span class="ln">6834 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6835"><span class="ln">6835 </span></a>    <span class="s4">&quot;H&quot;</span><span class="s3">,</span>
<a name="l6836"><span class="ln">6836 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6837"><span class="ln">6837 </span></a>Returns a view of a matrix (2-D tensor) conjugated and transposed. 
<a name="l6838"><span class="ln">6838 </span></a> 
<a name="l6839"><span class="ln">6839 </span></a>``x.H`` is equivalent to ``x.transpose(0, 1).conj()`` for complex matrices and 
<a name="l6840"><span class="ln">6840 </span></a>``x.transpose(0, 1)`` for real matrices. 
<a name="l6841"><span class="ln">6841 </span></a> 
<a name="l6842"><span class="ln">6842 </span></a>.. seealso:: 
<a name="l6843"><span class="ln">6843 </span></a> 
<a name="l6844"><span class="ln">6844 </span></a>        :attr:`~.Tensor.mH`: An attribute that also works on batches of matrices. 
<a name="l6845"><span class="ln">6845 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6846"><span class="ln">6846 </span></a><span class="s3">)</span>
<a name="l6847"><span class="ln">6847 </span></a>
<a name="l6848"><span class="ln">6848 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6849"><span class="ln">6849 </span></a>    <span class="s4">&quot;mT&quot;</span><span class="s3">,</span>
<a name="l6850"><span class="ln">6850 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6851"><span class="ln">6851 </span></a>Returns a view of this tensor with the last two dimensions transposed. 
<a name="l6852"><span class="ln">6852 </span></a> 
<a name="l6853"><span class="ln">6853 </span></a>``x.mT`` is equivalent to ``x.transpose(-2, -1)``. 
<a name="l6854"><span class="ln">6854 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6855"><span class="ln">6855 </span></a><span class="s3">)</span>
<a name="l6856"><span class="ln">6856 </span></a>
<a name="l6857"><span class="ln">6857 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6858"><span class="ln">6858 </span></a>    <span class="s4">&quot;mH&quot;</span><span class="s3">,</span>
<a name="l6859"><span class="ln">6859 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6860"><span class="ln">6860 </span></a>Accessing this property is equivalent to calling :func:`adjoint`. 
<a name="l6861"><span class="ln">6861 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6862"><span class="ln">6862 </span></a><span class="s3">)</span>
<a name="l6863"><span class="ln">6863 </span></a>
<a name="l6864"><span class="ln">6864 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6865"><span class="ln">6865 </span></a>    <span class="s4">&quot;adjoint&quot;</span><span class="s3">,</span>
<a name="l6866"><span class="ln">6866 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6867"><span class="ln">6867 </span></a>adjoint() -&gt; Tensor 
<a name="l6868"><span class="ln">6868 </span></a> 
<a name="l6869"><span class="ln">6869 </span></a>Alias for :func:`adjoint` 
<a name="l6870"><span class="ln">6870 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6871"><span class="ln">6871 </span></a><span class="s3">)</span>
<a name="l6872"><span class="ln">6872 </span></a>
<a name="l6873"><span class="ln">6873 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6874"><span class="ln">6874 </span></a>    <span class="s4">&quot;real&quot;</span><span class="s3">,</span>
<a name="l6875"><span class="ln">6875 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6876"><span class="ln">6876 </span></a>Returns a new tensor containing real values of the :attr:`self` tensor for a complex-valued input tensor. 
<a name="l6877"><span class="ln">6877 </span></a>The returned tensor and :attr:`self` share the same underlying storage. 
<a name="l6878"><span class="ln">6878 </span></a> 
<a name="l6879"><span class="ln">6879 </span></a>Returns :attr:`self` if :attr:`self` is a real-valued tensor tensor. 
<a name="l6880"><span class="ln">6880 </span></a> 
<a name="l6881"><span class="ln">6881 </span></a>Example:: 
<a name="l6882"><span class="ln">6882 </span></a> 
<a name="l6883"><span class="ln">6883 </span></a>    &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l6884"><span class="ln">6884 </span></a>    &gt;&gt;&gt; x 
<a name="l6885"><span class="ln">6885 </span></a>    tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)]) 
<a name="l6886"><span class="ln">6886 </span></a>    &gt;&gt;&gt; x.real 
<a name="l6887"><span class="ln">6887 </span></a>    tensor([ 0.3100, -0.5445, -1.6492, -0.0638]) 
<a name="l6888"><span class="ln">6888 </span></a> 
<a name="l6889"><span class="ln">6889 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6890"><span class="ln">6890 </span></a><span class="s3">)</span>
<a name="l6891"><span class="ln">6891 </span></a>
<a name="l6892"><span class="ln">6892 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6893"><span class="ln">6893 </span></a>    <span class="s4">&quot;imag&quot;</span><span class="s3">,</span>
<a name="l6894"><span class="ln">6894 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6895"><span class="ln">6895 </span></a>Returns a new tensor containing imaginary values of the :attr:`self` tensor. 
<a name="l6896"><span class="ln">6896 </span></a>The returned tensor and :attr:`self` share the same underlying storage. 
<a name="l6897"><span class="ln">6897 </span></a> 
<a name="l6898"><span class="ln">6898 </span></a>.. warning:: 
<a name="l6899"><span class="ln">6899 </span></a>    :func:`imag` is only supported for tensors with complex dtypes. 
<a name="l6900"><span class="ln">6900 </span></a> 
<a name="l6901"><span class="ln">6901 </span></a>Example:: 
<a name="l6902"><span class="ln">6902 </span></a> 
<a name="l6903"><span class="ln">6903 </span></a>    &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l6904"><span class="ln">6904 </span></a>    &gt;&gt;&gt; x 
<a name="l6905"><span class="ln">6905 </span></a>    tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)]) 
<a name="l6906"><span class="ln">6906 </span></a>    &gt;&gt;&gt; x.imag 
<a name="l6907"><span class="ln">6907 </span></a>    tensor([ 0.3553, -0.7896, -0.0633, -0.8119]) 
<a name="l6908"><span class="ln">6908 </span></a> 
<a name="l6909"><span class="ln">6909 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6910"><span class="ln">6910 </span></a><span class="s3">)</span>
<a name="l6911"><span class="ln">6911 </span></a>
<a name="l6912"><span class="ln">6912 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6913"><span class="ln">6913 </span></a>    <span class="s4">&quot;as_subclass&quot;</span><span class="s3">,</span>
<a name="l6914"><span class="ln">6914 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6915"><span class="ln">6915 </span></a>as_subclass(cls) -&gt; Tensor 
<a name="l6916"><span class="ln">6916 </span></a> 
<a name="l6917"><span class="ln">6917 </span></a>Makes a ``cls`` instance with the same data pointer as ``self``. Changes 
<a name="l6918"><span class="ln">6918 </span></a>in the output mirror changes in ``self``, and the output stays attached 
<a name="l6919"><span class="ln">6919 </span></a>to the autograd graph. ``cls`` must be a subclass of ``Tensor``. 
<a name="l6920"><span class="ln">6920 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6921"><span class="ln">6921 </span></a><span class="s3">)</span>
<a name="l6922"><span class="ln">6922 </span></a>
<a name="l6923"><span class="ln">6923 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6924"><span class="ln">6924 </span></a>    <span class="s4">&quot;crow_indices&quot;</span><span class="s3">,</span>
<a name="l6925"><span class="ln">6925 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6926"><span class="ln">6926 </span></a>crow_indices() -&gt; IntTensor 
<a name="l6927"><span class="ln">6927 </span></a> 
<a name="l6928"><span class="ln">6928 </span></a>Returns the tensor containing the compressed row indices of the :attr:`self` 
<a name="l6929"><span class="ln">6929 </span></a>tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``. 
<a name="l6930"><span class="ln">6930 </span></a>The ``crow_indices`` tensor is strictly of shape (:attr:`self`.size(0) + 1) 
<a name="l6931"><span class="ln">6931 </span></a>and of type ``int32`` or ``int64``. When using MKL routines such as sparse 
<a name="l6932"><span class="ln">6932 </span></a>matrix multiplication, it is necessary to use ``int32`` indexing in order 
<a name="l6933"><span class="ln">6933 </span></a>to avoid downcasting and potentially losing information. 
<a name="l6934"><span class="ln">6934 </span></a> 
<a name="l6935"><span class="ln">6935 </span></a>Example:: 
<a name="l6936"><span class="ln">6936 </span></a> 
<a name="l6937"><span class="ln">6937 </span></a>    &gt;&gt;&gt; csr = torch.eye(5,5).to_sparse_csr() 
<a name="l6938"><span class="ln">6938 </span></a>    &gt;&gt;&gt; csr.crow_indices() 
<a name="l6939"><span class="ln">6939 </span></a>    tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32) 
<a name="l6940"><span class="ln">6940 </span></a> 
<a name="l6941"><span class="ln">6941 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6942"><span class="ln">6942 </span></a><span class="s3">)</span>
<a name="l6943"><span class="ln">6943 </span></a>
<a name="l6944"><span class="ln">6944 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6945"><span class="ln">6945 </span></a>    <span class="s4">&quot;col_indices&quot;</span><span class="s3">,</span>
<a name="l6946"><span class="ln">6946 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6947"><span class="ln">6947 </span></a>col_indices() -&gt; IntTensor 
<a name="l6948"><span class="ln">6948 </span></a> 
<a name="l6949"><span class="ln">6949 </span></a>Returns the tensor containing the column indices of the :attr:`self` 
<a name="l6950"><span class="ln">6950 </span></a>tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``. 
<a name="l6951"><span class="ln">6951 </span></a>The ``col_indices`` tensor is strictly of shape (:attr:`self`.nnz()) 
<a name="l6952"><span class="ln">6952 </span></a>and of type ``int32`` or ``int64``.  When using MKL routines such as sparse 
<a name="l6953"><span class="ln">6953 </span></a>matrix multiplication, it is necessary to use ``int32`` indexing in order 
<a name="l6954"><span class="ln">6954 </span></a>to avoid downcasting and potentially losing information. 
<a name="l6955"><span class="ln">6955 </span></a> 
<a name="l6956"><span class="ln">6956 </span></a>Example:: 
<a name="l6957"><span class="ln">6957 </span></a> 
<a name="l6958"><span class="ln">6958 </span></a>    &gt;&gt;&gt; csr = torch.eye(5,5).to_sparse_csr() 
<a name="l6959"><span class="ln">6959 </span></a>    &gt;&gt;&gt; csr.col_indices() 
<a name="l6960"><span class="ln">6960 </span></a>    tensor([0, 1, 2, 3, 4], dtype=torch.int32) 
<a name="l6961"><span class="ln">6961 </span></a> 
<a name="l6962"><span class="ln">6962 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6963"><span class="ln">6963 </span></a><span class="s3">)</span>
<a name="l6964"><span class="ln">6964 </span></a>
<a name="l6965"><span class="ln">6965 </span></a><span class="s1">add_docstr_all</span><span class="s3">(</span>
<a name="l6966"><span class="ln">6966 </span></a>    <span class="s4">&quot;to_padded_tensor&quot;</span><span class="s3">,</span>
<a name="l6967"><span class="ln">6967 </span></a>    <span class="s4">r&quot;&quot;&quot; 
<a name="l6968"><span class="ln">6968 </span></a>to_padded_tensor(padding, output_size=None) -&gt; Tensor 
<a name="l6969"><span class="ln">6969 </span></a>See :func:`to_padded_tensor` 
<a name="l6970"><span class="ln">6970 </span></a>&quot;&quot;&quot;</span><span class="s3">,</span>
<a name="l6971"><span class="ln">6971 </span></a><span class="s3">)</span>
<a name="l6972"><span class="ln">6972 </span></a></pre>
</body>
</html>