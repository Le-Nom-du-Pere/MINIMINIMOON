<html>
<head>
<title>LegacyTypeDispatch.h</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #333333;}
.s1 { color: #969896; font-style: italic;}
.s2 { color: #000080; font-weight: bold;}
.s3 { color: #660e7a; font-weight: bold;}
.s4 { color: #183691; font-weight: bold;}
.ln { color: #333333; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
LegacyTypeDispatch.h</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">#pragma once</span>
<a name="l2"><span class="ln">2    </span></a>
<a name="l3"><span class="ln">3    </span></a><span class="s1">// The legacy mechanism for dispatching operators in ATen is a Type</span>
<a name="l4"><span class="ln">4    </span></a><span class="s1">// object, which is essentially a giant virtual dispatch table</span>
<a name="l5"><span class="ln">5    </span></a><span class="s1">// for every operation we support dynamically dispatching over.</span>
<a name="l6"><span class="ln">6    </span></a><span class="s1">//</span>
<a name="l7"><span class="ln">7    </span></a><span class="s1">// This has been deprecated in favor of ATenDispatch, and in the future,</span>
<a name="l8"><span class="ln">8    </span></a><span class="s1">// c10 dispatcher.</span>
<a name="l9"><span class="ln">9    </span></a><span class="s1">// TODO: Clean up what remains here</span>
<a name="l10"><span class="ln">10   </span></a>
<a name="l11"><span class="ln">11   </span></a><span class="s2">#include </span><span class="s0">&lt;c10/core/impl/LocalDispatchKeySet.h&gt;</span>
<a name="l12"><span class="ln">12   </span></a>
<a name="l13"><span class="ln">13   </span></a><span class="s3">namespace </span><span class="s0">at {</span>
<a name="l14"><span class="ln">14   </span></a>
<a name="l15"><span class="ln">15   </span></a><span class="s1">// A RAII, thread local (!) guard that will disable dispatch to variable</span>
<a name="l16"><span class="ln">16   </span></a><span class="s1">// handler.</span>
<a name="l17"><span class="ln">17   </span></a><span class="s1">//</span>
<a name="l18"><span class="ln">18   </span></a><span class="s1">// NOTE [ Treating Variables as non-Variables in type dispatch ]</span>
<a name="l19"><span class="ln">19   </span></a><span class="s1">//</span>
<a name="l20"><span class="ln">20   </span></a><span class="s1">// What exactly does AutoDispatchBelowAutograd do?  The short answer is, it causes</span>
<a name="l21"><span class="ln">21   </span></a><span class="s1">// dispatches on ATen functions to go to the non-variable implementation,</span>
<a name="l22"><span class="ln">22   </span></a><span class="s1">// bypassing autograd handling (and also profiling and tracing).</span>
<a name="l23"><span class="ln">23   </span></a><span class="s1">//</span>
<a name="l24"><span class="ln">24   </span></a><span class="s1">// To understand why this guard exists, it's helpful to understand the history</span>
<a name="l25"><span class="ln">25   </span></a><span class="s1">// behind how Variable was implemented.  Previously, Variables were implemented</span>
<a name="l26"><span class="ln">26   </span></a><span class="s1">// as a wrapper on Tensors; so the act of processing a Variable involved</span>
<a name="l27"><span class="ln">27   </span></a><span class="s1">// unwrapping the underlying Tensor, and then calling the underlying base</span>
<a name="l28"><span class="ln">28   </span></a><span class="s1">// operation on /that/ operation</span>
<a name="l29"><span class="ln">29   </span></a><span class="s1">//</span>
<a name="l30"><span class="ln">30   </span></a><span class="s1">// However, after the Variable/Tensor merge, there is no concept of unwrapping</span>
<a name="l31"><span class="ln">31   </span></a><span class="s1">// a tensor anymore.  If you just call the operation on the same variable</span>
<a name="l32"><span class="ln">32   </span></a><span class="s1">// again inside your VariableType handler, you'll dispatch back to</span>
<a name="l33"><span class="ln">33   </span></a><span class="s1">// VariableType, which is not what we want.</span>
<a name="l34"><span class="ln">34   </span></a><span class="s1">//</span>
<a name="l35"><span class="ln">35   </span></a><span class="s1">// The solution to the above problem is to add `at::AutoDispatchBelowAutograd`, which</span>
<a name="l36"><span class="ln">36   </span></a><span class="s1">// when enabled will cause `legacyTensorType()` and `getType()` to always return</span>
<a name="l37"><span class="ln">37   </span></a><span class="s1">// non-Variable type, even if the tensor being called on is a variable.</span>
<a name="l38"><span class="ln">38   </span></a>
<a name="l39"><span class="ln">39   </span></a><span class="s1">/* Note [AutoDispatchBelowAutograd] 
<a name="l40"><span class="ln">40   </span></a> * AutoDispatchBelowAutograd is **INTERNAL ONLY** that it should be used 
<a name="l41"><span class="ln">41   </span></a> * for kernel implementations and customized C++ kernels. 
<a name="l42"><span class="ln">42   </span></a> * If you are looking for a guard to run workload in inference mode, please use 
<a name="l43"><span class="ln">43   </span></a> * c10::InferenceMode RAII which is user facing API. 
<a name="l44"><span class="ln">44   </span></a> * In the past AutoDispatchBelowAutograd(or its old version AutoNonVariableTypeMode) 
<a name="l45"><span class="ln">45   </span></a> * was used in the user code for inference-only workload, this was under risk of 
<a name="l46"><span class="ln">46   </span></a> * producing wrong results silently in some edge cases. For example: 
<a name="l47"><span class="ln">47   </span></a> * ``` 
<a name="l48"><span class="ln">48   </span></a> *  torch::Tensor s = torch::ones({1, 2, 3}).set_requires_grad(true); 
<a name="l49"><span class="ln">49   </span></a> *  torch::Tensor out = s * s; 
<a name="l50"><span class="ln">50   </span></a> *  { 
<a name="l51"><span class="ln">51   </span></a> *    at::AutoDispatchBelowAutograd guard; 
<a name="l52"><span class="ln">52   </span></a> *    s.add_(1);  // Skips version bump on `s`. 
<a name="l53"><span class="ln">53   </span></a> *  } 
<a name="l54"><span class="ln">54   </span></a> *  // WRONG GRADIENT! s.grad() are now computed using `s` value after the 
<a name="l55"><span class="ln">55   </span></a> *  // inplace update. 
<a name="l56"><span class="ln">56   </span></a> *  out.backward(torch::ones_like(out)); 
<a name="l57"><span class="ln">57   </span></a> * ``` 
<a name="l58"><span class="ln">58   </span></a> * Users should use `c10::InferenceMode` here so that it'll properly throw an 
<a name="l59"><span class="ln">59   </span></a> * error saying &quot;one of the variables needed for gradient computation has be modified.&quot; 
<a name="l60"><span class="ln">60   </span></a> */</span>
<a name="l61"><span class="ln">61   </span></a><span class="s2">struct </span><span class="s0">TORCH_API AutoDispatchBelowAutograd {</span>
<a name="l62"><span class="ln">62   </span></a>  <span class="s0">AutoDispatchBelowAutograd() :</span>
<a name="l63"><span class="ln">63   </span></a>    <span class="s0">autograd_guard_(c10::autograd_dispatch_keyset) {</span>
<a name="l64"><span class="ln">64   </span></a>  <span class="s0">}</span>
<a name="l65"><span class="ln">65   </span></a>
<a name="l66"><span class="ln">66   </span></a>  <span class="s1">// disable all autograd dispatch keys</span>
<a name="l67"><span class="ln">67   </span></a>  <span class="s0">c10::impl::ExcludeDispatchKeyGuard autograd_guard_;</span>
<a name="l68"><span class="ln">68   </span></a><span class="s0">};</span>
<a name="l69"><span class="ln">69   </span></a>
<a name="l70"><span class="ln">70   </span></a><span class="s1">// TODO: AutoNonVariableTypeMode should be removed in release 1.10.</span>
<a name="l71"><span class="ln">71   </span></a><span class="s2">struct </span><span class="s0">TORCH_API AutoNonVariableTypeMode {</span>
<a name="l72"><span class="ln">72   </span></a>  <span class="s0">AutoNonVariableTypeMode(</span><span class="s2">bool </span><span class="s0">enabled = </span><span class="s3">true</span><span class="s0">) :</span>
<a name="l73"><span class="ln">73   </span></a>    <span class="s0">autograd_guard_(c10::autograd_dispatch_keyset) {</span>
<a name="l74"><span class="ln">74   </span></a>    <span class="s0">TORCH_WARN_ONCE(</span><span class="s4">&quot;AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. &quot;</span>
<a name="l75"><span class="ln">75   </span></a>        <span class="s4">&quot;For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, &quot;</span>
<a name="l76"><span class="ln">76   </span></a>        <span class="s4">&quot;If you are looking for a user facing API to enable running your inference-only &quot;</span>
<a name="l77"><span class="ln">77   </span></a>        <span class="s4">&quot;workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code &quot;</span>
<a name="l78"><span class="ln">78   </span></a>        <span class="s4">&quot;is under risk of producing silent wrong result in some edge cases. &quot;</span>
<a name="l79"><span class="ln">79   </span></a>        <span class="s4">&quot;See Note [AutoDispatchBelowAutograd] for more details.&quot;</span><span class="s0">);</span>
<a name="l80"><span class="ln">80   </span></a>    <span class="s0">TORCH_INTERNAL_ASSERT(enabled);</span>
<a name="l81"><span class="ln">81   </span></a>  <span class="s0">}</span>
<a name="l82"><span class="ln">82   </span></a>
<a name="l83"><span class="ln">83   </span></a>  <span class="s1">// disable all autograd dispatch keys</span>
<a name="l84"><span class="ln">84   </span></a>  <span class="s0">c10::impl::ExcludeDispatchKeyGuard autograd_guard_;</span>
<a name="l85"><span class="ln">85   </span></a><span class="s0">};</span>
<a name="l86"><span class="ln">86   </span></a>
<a name="l87"><span class="ln">87   </span></a><span class="s2">struct </span><span class="s0">TORCH_API AutoDispatchSkipFunctionalize {</span>
<a name="l88"><span class="ln">88   </span></a>  <span class="s0">AutoDispatchSkipFunctionalize() :</span>
<a name="l89"><span class="ln">89   </span></a>    <span class="s0">dispatch_key_guard_(c10::DispatchKeySet(c10::DispatchKey::Functionalize)) {</span>
<a name="l90"><span class="ln">90   </span></a>  <span class="s0">}</span>
<a name="l91"><span class="ln">91   </span></a>  <span class="s0">c10::impl::ExcludeDispatchKeyGuard dispatch_key_guard_;</span>
<a name="l92"><span class="ln">92   </span></a><span class="s0">};</span>
<a name="l93"><span class="ln">93   </span></a>
<a name="l94"><span class="ln">94   </span></a><span class="s1">/* Note [AutoDispatchBelowADInplaceOrView] 
<a name="l95"><span class="ln">95   </span></a> * AutoDispatchBelowADInplaceOrView is equivalent to AutoNonVariableTypeMode 
<a name="l96"><span class="ln">96   </span></a> * before we split inplace &amp; view ops out of VariableType kernel. 
<a name="l97"><span class="ln">97   </span></a> * Note this guard is used in VariableType kernels for functional ops 
<a name="l98"><span class="ln">98   </span></a> * as well as ADInplaceOrView kernels for inplace/view ops to enforce the 
<a name="l99"><span class="ln">99   </span></a> * Invariant: 
<a name="l100"><span class="ln">100  </span></a> *   Once you are in VariableType/ADInplaceOrView kernel for an op, 
<a name="l101"><span class="ln">101  </span></a> *   you never go back to a kernel on same dispatch key until 
<a name="l102"><span class="ln">102  </span></a> *   you finish the current op. 
<a name="l103"><span class="ln">103  </span></a> */</span>
<a name="l104"><span class="ln">104  </span></a><span class="s2">struct </span><span class="s0">TORCH_API AutoDispatchBelowADInplaceOrView {</span>
<a name="l105"><span class="ln">105  </span></a>  <span class="s0">AutoDispatchBelowADInplaceOrView() :</span>
<a name="l106"><span class="ln">106  </span></a>    <span class="s0">dispatch_key_guard_(c10::autograd_dispatch_keyset_with_ADInplaceOrView) {</span>
<a name="l107"><span class="ln">107  </span></a>  <span class="s0">}</span>
<a name="l108"><span class="ln">108  </span></a>  <span class="s1">// disable Autograd &amp; ADInplaceOrView dispatch keys</span>
<a name="l109"><span class="ln">109  </span></a>  <span class="s0">c10::impl::ExcludeDispatchKeyGuard dispatch_key_guard_;</span>
<a name="l110"><span class="ln">110  </span></a><span class="s0">};</span>
<a name="l111"><span class="ln">111  </span></a><span class="s0">} </span><span class="s1">// namespace at</span>
<a name="l112"><span class="ln">112  </span></a></pre>
</body>
</html>