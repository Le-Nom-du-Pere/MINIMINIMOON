<html>
<head>
<title>RegistrationDeclarations.h</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #969896; font-style: italic;}
.s1 { color: #333333;}
.s2 { color: #000080; font-weight: bold;}
.s3 { color: #0086b3;}
.ln { color: #333333; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
RegistrationDeclarations.h</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">// This file contains all native_functions that can be registered to</span>
<a name="l2"><span class="ln">2    </span></a><span class="s0">// and the schema string that they should be registered with</span>
<a name="l3"><span class="ln">3    </span></a>
<a name="l4"><span class="ln">4    </span></a><span class="s1">at::Tensor _cast_Byte(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Byte(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l5"><span class="ln">5    </span></a><span class="s1">at::Tensor _cast_Char(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Char(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l6"><span class="ln">6    </span></a><span class="s1">at::Tensor _cast_Double(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Double(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l7"><span class="ln">7    </span></a><span class="s1">at::Tensor _cast_Float(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Float(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l8"><span class="ln">8    </span></a><span class="s1">at::Tensor _cast_Int(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Int(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l9"><span class="ln">9    </span></a><span class="s1">at::Tensor _cast_Long(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Long(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l10"><span class="ln">10   </span></a><span class="s1">at::Tensor _cast_Short(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Short(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l11"><span class="ln">11   </span></a><span class="s1">at::Tensor _cast_Half(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cast_Half(Tensor self, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l12"><span class="ln">12   </span></a><span class="s2">void </span><span class="s1">_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::TensorList inputs, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; gradient, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; retain_graph, </span><span class="s2">bool </span><span class="s1">create_graph); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l13"><span class="ln">13   </span></a><span class="s2">void </span><span class="s1">set_data(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; new_data); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set_data(Tensor(a!) self, Tensor new_data) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l14"><span class="ln">14   </span></a><span class="s1">at::Tensor data(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::data(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l15"><span class="ln">15   </span></a><span class="s2">bool </span><span class="s1">is_leaf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_leaf(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l16"><span class="ln">16   </span></a><span class="s1">int64_t output_nr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::output_nr(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l17"><span class="ln">17   </span></a><span class="s1">int64_t _version(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_version(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l18"><span class="ln">18   </span></a><span class="s1">at::Tensor &amp; requires_grad_(at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">requires_grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l19"><span class="ln">19   </span></a><span class="s2">void </span><span class="s1">retain_grad(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::retain_grad(Tensor(a!) self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l20"><span class="ln">20   </span></a><span class="s2">bool </span><span class="s1">retains_grad(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::retains_grad(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l21"><span class="ln">21   </span></a><span class="s1">at::Tensor _fw_primal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t level); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fw_primal(Tensor(a) self, int level) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l22"><span class="ln">22   </span></a><span class="s1">at::Tensor _make_dual(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; primal, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tangent, int64_t level); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l23"><span class="ln">23   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _unpack_dual(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; dual, int64_t level); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unpack_dual(Tensor(a) dual, int level) -&gt; (Tensor(a) primal, Tensor tangent)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l24"><span class="ln">24   </span></a><span class="s1">at::Tensor _new_zeros_with_same_feature_meta(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, int64_t self_num_batch_dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_new_zeros_with_same_feature_meta(Tensor self, Tensor other, *, int self_num_batch_dims=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l25"><span class="ln">25   </span></a><span class="s2">bool </span><span class="s1">_has_same_storage_numel(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_has_same_storage_numel(Tensor self, Tensor other) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l26"><span class="ln">26   </span></a><span class="s1">at::Tensor &amp; rename_(at::Tensor &amp; self, ::std::optional&lt;at::DimnameList&gt; names); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rename_(Tensor(a!) self, Dimname[]? names) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l27"><span class="ln">27   </span></a><span class="s1">at::Tensor rename(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::DimnameList&gt; names); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rename(Tensor(a) self, Dimname[]? names) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l28"><span class="ln">28   </span></a><span class="s1">at::Tensor align_to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList names); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::align_to(Tensor(a) self, Dimname[] names) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l29"><span class="ln">29   </span></a><span class="s1">at::Tensor align_to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList order, int64_t ellipsis_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l30"><span class="ln">30   </span></a><span class="s1">at::Tensor align_as(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::align_as(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l31"><span class="ln">31   </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; align_tensors(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::align_tensors(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l32"><span class="ln">32   </span></a><span class="s2">void </span><span class="s1">_assert_async(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_assert_async(Tensor self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l33"><span class="ln">33   </span></a><span class="s2">void </span><span class="s1">_assert_async(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view assert_msg); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_assert_async.msg(Tensor self, str assert_msg) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l34"><span class="ln">34   </span></a><span class="s2">void </span><span class="s1">_assert_scalar(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, c10::string_view assert_msg); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_assert_scalar(Scalar self, str assert_msg) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l35"><span class="ln">35   </span></a><span class="s1">at::Tensor _functional_assert_scalar(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, c10::string_view assert_msg, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dep_token); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_functional_assert_scalar(Scalar self, str assert_msg, Tensor dep_token) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l36"><span class="ln">36   </span></a><span class="s1">at::Tensor _functional_assert_async(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view assert_msg, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dep_token); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_functional_assert_async.msg(Tensor self, str assert_msg, Tensor dep_token) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l37"><span class="ln">37   </span></a><span class="s2">void </span><span class="s1">_assert_tensor_metadata(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; a, at::OptionalSymIntArrayRef size, at::OptionalSymIntArrayRef stride, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;at::Layout&gt; layout); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_assert_tensor_metadata(Tensor a, SymInt[]? size=None, SymInt[]? stride=None, ScalarType? dtype=None, *, Device? device=None, Layout? layout=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l38"><span class="ln">38   </span></a><span class="s2">void </span><span class="s1">_print(c10::string_view s); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_print(str s) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l39"><span class="ln">39   </span></a><span class="s2">void </span><span class="s1">sym_constrain_range(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; size, ::std::optional&lt;int64_t&gt; min, ::std::optional&lt;int64_t&gt; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sym_constrain_range(Scalar size, *, int? min=None, int? max=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l40"><span class="ln">40   </span></a><span class="s2">void </span><span class="s1">sym_constrain_range_for_size(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; size, ::std::optional&lt;int64_t&gt; min, ::std::optional&lt;int64_t&gt; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sym_constrain_range_for_size(Scalar size, *, int? min=None, int? max=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l41"><span class="ln">41   </span></a><span class="s1">at::Tensor _functional_sym_constrain_range(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; size, ::std::optional&lt;int64_t&gt; min, ::std::optional&lt;int64_t&gt; max, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dep_token); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_functional_sym_constrain_range(Scalar size, int? min, int? max, Tensor dep_token) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l42"><span class="ln">42   </span></a><span class="s1">at::Tensor _functional_sym_constrain_range_for_size(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; size, ::std::optional&lt;int64_t&gt; min, ::std::optional&lt;int64_t&gt; max, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dep_token); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_functional_sym_constrain_range_for_size(Scalar size, int? min, int? max, Tensor dep_token) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l43"><span class="ln">43   </span></a><span class="s1">at::Tensor _make_dep_token(::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_dep_token(*, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l44"><span class="ln">44   </span></a><span class="s1">at::Tensor refine_names(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList names); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::refine_names(Tensor(a) self, Dimname[] names) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l45"><span class="ln">45   </span></a><span class="s2">bool </span><span class="s1">_use_cudnn_ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l46"><span class="ln">46   </span></a><span class="s2">bool </span><span class="s1">_use_cudnn_ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target_lengths, int64_t blank); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_use_cudnn_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l47"><span class="ln">47   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _cudnn_ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l48"><span class="ln">48   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _cudnn_ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target_lengths, int64_t blank, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, bool deterministic, bool zero_infinity) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l49"><span class="ln">49   </span></a><span class="s2">bool </span><span class="s1">_use_cudnn_rnn_flatten_weight(); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_use_cudnn_rnn_flatten_weight() -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l50"><span class="ln">50   </span></a><span class="s1">at::Tensor _cudnn_rnn_flatten_weight(at::TensorList weight_arr, int64_t weight_stride0, c10::SymInt input_size, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">bool </span><span class="s1">bidirectional); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l51"><span class="ln">51   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _cudnn_rnn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight_buf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, c10::SymIntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l52"><span class="ln">52   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,::std::vector&lt;at::Tensor&gt;&gt; _cudnn_rnn_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight_buf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, c10::SymIntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reserve, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">4</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -&gt; (Tensor, Tensor, Tensor, Tensor[])&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l53"><span class="ln">53   </span></a><span class="s1">at::Tensor _cudnn_init_dropout_state(</span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, int64_t dropout_seed, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l54"><span class="ln">54   </span></a><span class="s1">int64_t _debug_has_internal_overlap(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_debug_has_internal_overlap(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l55"><span class="ln">55   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _fused_dropout(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_dropout(Tensor self, float p, Generator? generator=None) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l56"><span class="ln">56   </span></a><span class="s1">at::Tensor _masked_scale(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">double </span><span class="s1">scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_masked_scale(Tensor self, Tensor mask, float scale) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l57"><span class="ln">57   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; native_dropout(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_dropout(Tensor input, float p, bool? train) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l58"><span class="ln">58   </span></a><span class="s1">at::Tensor native_dropout_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">double </span><span class="s1">scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l59"><span class="ln">59   </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _sobol_engine_draw(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; quasi, int64_t n, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; sobolstate, int64_t dimension, int64_t num_generated, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l60"><span class="ln">60   </span></a><span class="s1">at::Tensor &amp; _sobol_engine_ff_(at::Tensor &amp; self, int64_t n, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; sobolstate, int64_t dimension, int64_t num_generated); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l61"><span class="ln">61   </span></a><span class="s1">at::Tensor &amp; _sobol_engine_scramble_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ltm, int64_t dimension); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l62"><span class="ln">62   </span></a><span class="s1">at::Tensor &amp; _sobol_engine_initialize_state_(at::Tensor &amp; self, int64_t dimension); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l63"><span class="ln">63   </span></a><span class="s1">at::Tensor _reshape_from_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; shape); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_reshape_from_tensor(Tensor self, Tensor shape) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l64"><span class="ln">64   </span></a><span class="s1">at::Tensor _shape_as_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_shape_as_tensor(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l65"><span class="ln">65   </span></a><span class="s1">at::Tensor dropout(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dropout(Tensor input, float p, bool train) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l66"><span class="ln">66   </span></a><span class="s1">at::Tensor &amp; dropout_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dropout_(Tensor(a!) self, float p, bool train) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l67"><span class="ln">67   </span></a><span class="s1">at::Tensor feature_dropout(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::feature_dropout(Tensor input, float p, bool train) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l68"><span class="ln">68   </span></a><span class="s1">at::Tensor &amp; feature_dropout_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::feature_dropout_(Tensor(a!) self, float p, bool train) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l69"><span class="ln">69   </span></a><span class="s1">at::Tensor alpha_dropout(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::alpha_dropout(Tensor input, float p, bool train) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l70"><span class="ln">70   </span></a><span class="s1">at::Tensor &amp; alpha_dropout_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l71"><span class="ln">71   </span></a><span class="s1">at::Tensor feature_alpha_dropout(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::feature_alpha_dropout(Tensor input, float p, bool train) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l72"><span class="ln">72   </span></a><span class="s1">at::Tensor &amp; feature_alpha_dropout_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l73"><span class="ln">73   </span></a><span class="s1">at::Tensor abs(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::abs(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l74"><span class="ln">74   </span></a><span class="s1">at::Tensor &amp; abs_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::abs_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l75"><span class="ln">75   </span></a><span class="s1">at::Tensor &amp; abs_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::abs.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l76"><span class="ln">76   </span></a><span class="s1">at::Tensor absolute(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::absolute(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l77"><span class="ln">77   </span></a><span class="s1">at::Tensor &amp; absolute_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::absolute_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l78"><span class="ln">78   </span></a><span class="s1">at::Tensor &amp; absolute_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::absolute.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l79"><span class="ln">79   </span></a><span class="s1">at::Tensor angle(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::angle(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l80"><span class="ln">80   </span></a><span class="s1">at::Tensor &amp; angle_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::angle.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l81"><span class="ln">81   </span></a><span class="s1">at::Tensor view_as_real(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_as_real(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l82"><span class="ln">82   </span></a><span class="s1">at::Tensor view_as_complex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_as_complex(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l83"><span class="ln">83   </span></a><span class="s1">at::Tensor sgn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sgn(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l84"><span class="ln">84   </span></a><span class="s1">at::Tensor &amp; sgn_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sgn_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l85"><span class="ln">85   </span></a><span class="s1">at::Tensor &amp; sgn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sgn.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l86"><span class="ln">86   </span></a><span class="s1">at::Tensor chalf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l87"><span class="ln">87   </span></a><span class="s1">at::Tensor real(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::real(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l88"><span class="ln">88   </span></a><span class="s1">at::Tensor imag(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::imag(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l89"><span class="ln">89   </span></a><span class="s1">at::Tensor _conj(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_conj(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l90"><span class="ln">90   </span></a><span class="s1">at::Tensor conj(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conj(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l91"><span class="ln">91   </span></a><span class="s1">at::Tensor _conj_physical(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_conj_physical(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l92"><span class="ln">92   </span></a><span class="s1">at::Tensor conj_physical(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conj_physical(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l93"><span class="ln">93   </span></a><span class="s1">at::Tensor &amp; conj_physical_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l94"><span class="ln">94   </span></a><span class="s1">at::Tensor &amp; conj_physical_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conj_physical_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l95"><span class="ln">95   </span></a><span class="s1">at::Tensor resolve_conj(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resolve_conj(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l96"><span class="ln">96   </span></a><span class="s1">at::Tensor resolve_neg(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resolve_neg(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l97"><span class="ln">97   </span></a><span class="s1">at::Tensor _neg_view(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_neg_view(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l98"><span class="ln">98   </span></a><span class="s1">at::Tensor acos(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::acos(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l99"><span class="ln">99   </span></a><span class="s1">at::Tensor &amp; acos_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::acos_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l100"><span class="ln">100  </span></a><span class="s1">at::Tensor &amp; acos_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::acos.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l101"><span class="ln">101  </span></a><span class="s1">at::Tensor arccos(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arccos(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l102"><span class="ln">102  </span></a><span class="s1">at::Tensor &amp; arccos_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arccos_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l103"><span class="ln">103  </span></a><span class="s1">at::Tensor &amp; arccos_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arccos.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l104"><span class="ln">104  </span></a><span class="s1">at::Tensor avg_pool1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l105"><span class="ln">105  </span></a><span class="s1">at::Tensor adaptive_avg_pool1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l106"><span class="ln">106  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; adaptive_max_pool1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l107"><span class="ln">107  </span></a><span class="s1">at::Tensor add(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l108"><span class="ln">108  </span></a><span class="s1">at::Tensor &amp; add_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l109"><span class="ln">109  </span></a><span class="s1">at::Tensor &amp; add_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l110"><span class="ln">110  </span></a><span class="s1">at::Tensor _add_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l111"><span class="ln">111  </span></a><span class="s1">at::Tensor &amp; _add_relu_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l112"><span class="ln">112  </span></a><span class="s1">at::Tensor &amp; _add_relu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l113"><span class="ln">113  </span></a><span class="s1">at::Tensor _add_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l114"><span class="ln">114  </span></a><span class="s1">at::Tensor &amp; _add_relu_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l115"><span class="ln">115  </span></a><span class="s1">at::Tensor add(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l116"><span class="ln">116  </span></a><span class="s1">at::Tensor &amp; add_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l117"><span class="ln">117  </span></a><span class="s1">at::Tensor addmv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l118"><span class="ln">118  </span></a><span class="s1">at::Tensor &amp; addmv_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l119"><span class="ln">119  </span></a><span class="s1">at::Tensor &amp; addmv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l120"><span class="ln">120  </span></a><span class="s1">at::Tensor addr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l121"><span class="ln">121  </span></a><span class="s1">at::Tensor &amp; addr_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l122"><span class="ln">122  </span></a><span class="s1">at::Tensor &amp; addr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l123"><span class="ln">123  </span></a><span class="s1">at::Tensor affine_grid_generator(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; theta, c10::SymIntArrayRef size, </span><span class="s2">bool </span><span class="s1">align_corners); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l124"><span class="ln">124  </span></a><span class="s1">at::Tensor affine_grid_generator_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, c10::SymIntArrayRef size, </span><span class="s2">bool </span><span class="s1">align_corners); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l125"><span class="ln">125  </span></a><span class="s1">at::Tensor _is_all_true(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_is_all_true(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l126"><span class="ln">126  </span></a><span class="s1">at::Tensor _is_any_true(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_is_any_true(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l127"><span class="ln">127  </span></a><span class="s1">at::Tensor _test_check_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_check_tensor(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l128"><span class="ln">128  </span></a><span class="s1">at::Tensor _test_functorch_fallback(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_functorch_fallback(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l129"><span class="ln">129  </span></a><span class="s1">at::Tensor all(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all.dim(Tensor self, int dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l130"><span class="ln">130  </span></a><span class="s1">at::Tensor all(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all.dims(Tensor self, int[]? dim=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l131"><span class="ln">131  </span></a><span class="s1">at::Tensor &amp; all_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l132"><span class="ln">132  </span></a><span class="s1">at::Tensor &amp; all_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l133"><span class="ln">133  </span></a><span class="s1">at::Tensor all(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l134"><span class="ln">134  </span></a><span class="s1">at::Tensor &amp; all_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l135"><span class="ln">135  </span></a><span class="s2">bool </span><span class="s1">allclose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">double </span><span class="s1">rtol, </span><span class="s2">double </span><span class="s1">atol, </span><span class="s2">bool </span><span class="s1">equal_nan); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l136"><span class="ln">136  </span></a><span class="s1">at::Tensor any(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any.dim(Tensor self, int dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l137"><span class="ln">137  </span></a><span class="s1">at::Tensor any(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any.dims(Tensor self, int[]? dim=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l138"><span class="ln">138  </span></a><span class="s1">at::Tensor &amp; any_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l139"><span class="ln">139  </span></a><span class="s1">at::Tensor &amp; any_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l140"><span class="ln">140  </span></a><span class="s1">at::Tensor any(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l141"><span class="ln">141  </span></a><span class="s1">at::Tensor &amp; any_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l142"><span class="ln">142  </span></a><span class="s1">at::Tensor arange(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l143"><span class="ln">143  </span></a><span class="s1">at::Tensor arange(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l144"><span class="ln">144  </span></a><span class="s1">at::Tensor arange(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; step, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l145"><span class="ln">145  </span></a><span class="s1">at::Tensor &amp; arange_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arange.out(Scalar end, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l146"><span class="ln">146  </span></a><span class="s1">at::Tensor &amp; arange_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; step, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l147"><span class="ln">147  </span></a><span class="s1">at::Tensor _dim_arange(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; like, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_dim_arange(Tensor like, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l148"><span class="ln">148  </span></a><span class="s1">at::Tensor argmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l149"><span class="ln">149  </span></a><span class="s1">at::Tensor &amp; argmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l150"><span class="ln">150  </span></a><span class="s1">at::Tensor argmin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l151"><span class="ln">151  </span></a><span class="s1">at::Tensor &amp; argmin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l152"><span class="ln">152  </span></a><span class="s1">at::Tensor acosh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::acosh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l153"><span class="ln">153  </span></a><span class="s1">at::Tensor &amp; acosh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::acosh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l154"><span class="ln">154  </span></a><span class="s1">at::Tensor &amp; acosh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::acosh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l155"><span class="ln">155  </span></a><span class="s1">at::Tensor arccosh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arccosh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l156"><span class="ln">156  </span></a><span class="s1">at::Tensor &amp; arccosh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arccosh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l157"><span class="ln">157  </span></a><span class="s1">at::Tensor &amp; arccosh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arccosh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l158"><span class="ln">158  </span></a><span class="s1">at::Tensor asinh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::asinh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l159"><span class="ln">159  </span></a><span class="s1">at::Tensor &amp; asinh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::asinh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l160"><span class="ln">160  </span></a><span class="s1">at::Tensor &amp; asinh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::asinh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l161"><span class="ln">161  </span></a><span class="s1">at::Tensor arcsinh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arcsinh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l162"><span class="ln">162  </span></a><span class="s1">at::Tensor &amp; arcsinh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arcsinh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l163"><span class="ln">163  </span></a><span class="s1">at::Tensor &amp; arcsinh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l164"><span class="ln">164  </span></a><span class="s1">at::Tensor atanh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atanh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l165"><span class="ln">165  </span></a><span class="s1">at::Tensor &amp; atanh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atanh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l166"><span class="ln">166  </span></a><span class="s1">at::Tensor &amp; atanh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atanh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l167"><span class="ln">167  </span></a><span class="s1">at::Tensor arctanh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctanh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l168"><span class="ln">168  </span></a><span class="s1">at::Tensor &amp; arctanh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctanh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l169"><span class="ln">169  </span></a><span class="s1">at::Tensor &amp; arctanh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctanh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l170"><span class="ln">170  </span></a><span class="s1">at::Tensor as_strided(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;c10::SymInt&gt; storage_offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l171"><span class="ln">171  </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; as_strided_(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;c10::SymInt&gt; storage_offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l172"><span class="ln">172  </span></a><span class="s1">at::Tensor asin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::asin(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l173"><span class="ln">173  </span></a><span class="s1">at::Tensor &amp; asin_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::asin_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l174"><span class="ln">174  </span></a><span class="s1">at::Tensor &amp; asin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::asin.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l175"><span class="ln">175  </span></a><span class="s1">at::Tensor arcsin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arcsin(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l176"><span class="ln">176  </span></a><span class="s1">at::Tensor &amp; arcsin_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arcsin_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l177"><span class="ln">177  </span></a><span class="s1">at::Tensor &amp; arcsin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arcsin.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l178"><span class="ln">178  </span></a><span class="s1">at::Tensor atan(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atan(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l179"><span class="ln">179  </span></a><span class="s1">at::Tensor &amp; atan_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atan_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l180"><span class="ln">180  </span></a><span class="s1">at::Tensor &amp; atan_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atan.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l181"><span class="ln">181  </span></a><span class="s1">at::Tensor arctan(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctan(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l182"><span class="ln">182  </span></a><span class="s1">at::Tensor &amp; arctan_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctan_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l183"><span class="ln">183  </span></a><span class="s1">at::Tensor &amp; arctan_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctan.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l184"><span class="ln">184  </span></a><span class="s1">at::Tensor atleast_1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atleast_1d(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l185"><span class="ln">185  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; atleast_1d(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atleast_1d.Sequence(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l186"><span class="ln">186  </span></a><span class="s1">at::Tensor atleast_2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atleast_2d(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l187"><span class="ln">187  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; atleast_2d(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atleast_2d.Sequence(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l188"><span class="ln">188  </span></a><span class="s1">at::Tensor atleast_3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atleast_3d(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l189"><span class="ln">189  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; atleast_3d(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atleast_3d.Sequence(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l190"><span class="ln">190  </span></a><span class="s1">at::Tensor baddbmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l191"><span class="ln">191  </span></a><span class="s1">at::Tensor &amp; baddbmm_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l192"><span class="ln">192  </span></a><span class="s1">at::Tensor &amp; baddbmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l193"><span class="ln">193  </span></a><span class="s1">at::Tensor baddbmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, at::ScalarType out_dtype, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::baddbmm.dtype(Tensor self, Tensor batch1, Tensor batch2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l194"><span class="ln">194  </span></a><span class="s1">at::Tensor &amp; baddbmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, at::ScalarType out_dtype, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::baddbmm.dtype_out(Tensor self, Tensor batch1, Tensor batch2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l195"><span class="ln">195  </span></a><span class="s1">at::Tensor bartlett_window(int64_t window_length, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l196"><span class="ln">196  </span></a><span class="s1">at::Tensor bartlett_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l197"><span class="ln">197  </span></a><span class="s1">at::Tensor batch_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">cudnn_enabled); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l198"><span class="ln">198  </span></a><span class="s1">at::Tensor quantized_batch_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; var, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">double </span><span class="s1">output_scale, int64_t output_zero_point); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l199"><span class="ln">199  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t&gt; _batch_norm_impl_index(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">cudnn_enabled); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -&gt; (Tensor, Tensor, Tensor, Tensor, int)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l200"><span class="ln">200  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _batch_norm_impl_index_backward(int64_t impl_index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_var_transform, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">double </span><span class="s1">eps, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reservedSpace); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l201"><span class="ln">201  </span></a><span class="s1">at::Tensor bernoulli(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli(Tensor self, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l202"><span class="ln">202  </span></a><span class="s1">at::Tensor &amp; bernoulli_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l203"><span class="ln">203  </span></a><span class="s1">at::Tensor &amp; bernoulli_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; p, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l204"><span class="ln">204  </span></a><span class="s1">at::Tensor &amp; bernoulli_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l205"><span class="ln">205  </span></a><span class="s1">at::Tensor bernoulli(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l206"><span class="ln">206  </span></a><span class="s1">at::Tensor bilinear(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l207"><span class="ln">207  </span></a><span class="s1">at::Tensor binary_cross_entropy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l208"><span class="ln">208  </span></a><span class="s1">at::Tensor &amp; binary_cross_entropy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l209"><span class="ln">209  </span></a><span class="s1">at::Tensor binary_cross_entropy_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l210"><span class="ln">210  </span></a><span class="s1">at::Tensor &amp; binary_cross_entropy_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l211"><span class="ln">211  </span></a><span class="s1">at::Tensor binary_cross_entropy_with_logits(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; pos_weight, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l212"><span class="ln">212  </span></a><span class="s1">at::Tensor bincount(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weights, c10::SymInt minlength); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bincount(Tensor self, Tensor? weights=None, SymInt minlength=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l213"><span class="ln">213  </span></a><span class="s1">at::Tensor bitwise_not(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_not(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l214"><span class="ln">214  </span></a><span class="s1">at::Tensor &amp; bitwise_not_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_not_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l215"><span class="ln">215  </span></a><span class="s1">at::Tensor &amp; bitwise_not_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l216"><span class="ln">216  </span></a><span class="s1">at::Tensor &amp; copysign_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l217"><span class="ln">217  </span></a><span class="s1">at::Tensor copysign(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copysign.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l218"><span class="ln">218  </span></a><span class="s1">at::Tensor &amp; copysign_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l219"><span class="ln">219  </span></a><span class="s1">at::Tensor copysign(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copysign.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l220"><span class="ln">220  </span></a><span class="s1">at::Tensor &amp; copysign_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l221"><span class="ln">221  </span></a><span class="s1">at::Tensor &amp; copysign_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l222"><span class="ln">222  </span></a><span class="s1">at::Tensor _lazy_clone(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_lazy_clone(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l223"><span class="ln">223  </span></a><span class="s1">at::Tensor logical_not(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_not(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l224"><span class="ln">224  </span></a><span class="s1">at::Tensor &amp; logical_not_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_not_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l225"><span class="ln">225  </span></a><span class="s1">at::Tensor &amp; logical_not_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_not.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l226"><span class="ln">226  </span></a><span class="s1">at::Tensor logical_xor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_xor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l227"><span class="ln">227  </span></a><span class="s1">at::Tensor &amp; logical_xor_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_xor_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l228"><span class="ln">228  </span></a><span class="s1">at::Tensor &amp; logical_xor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l229"><span class="ln">229  </span></a><span class="s1">at::Tensor logical_and(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_and(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l230"><span class="ln">230  </span></a><span class="s1">at::Tensor &amp; logical_and_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_and_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l231"><span class="ln">231  </span></a><span class="s1">at::Tensor &amp; logical_and_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l232"><span class="ln">232  </span></a><span class="s1">at::Tensor logical_or(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_or(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l233"><span class="ln">233  </span></a><span class="s1">at::Tensor &amp; logical_or_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_or_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l234"><span class="ln">234  </span></a><span class="s1">at::Tensor &amp; logical_or_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l235"><span class="ln">235  </span></a><span class="s1">at::Tensor blackman_window(int64_t window_length, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l236"><span class="ln">236  </span></a><span class="s1">at::Tensor blackman_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l237"><span class="ln">237  </span></a><span class="s1">at::Tensor bmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bmm(Tensor self, Tensor mat2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l238"><span class="ln">238  </span></a><span class="s1">at::Tensor &amp; bmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l239"><span class="ln">239  </span></a><span class="s1">at::Tensor bmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::ScalarType out_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bmm.dtype(Tensor self, Tensor mat2, ScalarType out_dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l240"><span class="ln">240  </span></a><span class="s1">at::Tensor &amp; bmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::ScalarType out_dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bmm.dtype_out(Tensor self, Tensor mat2, ScalarType out_dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l241"><span class="ln">241  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; broadcast_tensors(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::broadcast_tensors(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l242"><span class="ln">242  </span></a><span class="s1">at::Tensor broadcast_to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::broadcast_to(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l243"><span class="ln">243  </span></a><span class="s1">at::Tensor _sparse_broadcast_to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_broadcast_to(Tensor(a) self, int[] size) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l244"><span class="ln">244  </span></a><span class="s1">at::Tensor cat(</span><span class="s2">const </span><span class="s1">at::ITensorListRef &amp; tensors, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cat(Tensor[] tensors, int dim=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l245"><span class="ln">245  </span></a><span class="s1">at::Tensor &amp; cat_out(</span><span class="s2">const </span><span class="s1">at::ITensorListRef &amp; tensors, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l246"><span class="ln">246  </span></a><span class="s1">at::Tensor cat(at::TensorList tensors, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cat.names(Tensor[] tensors, Dimname dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l247"><span class="ln">247  </span></a><span class="s1">at::Tensor &amp; cat_out(at::TensorList tensors, at::Dimname dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l248"><span class="ln">248  </span></a><span class="s1">at::Tensor concat(at::TensorList tensors, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concat(Tensor[] tensors, int dim=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l249"><span class="ln">249  </span></a><span class="s1">at::Tensor &amp; concat_out(at::TensorList tensors, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l250"><span class="ln">250  </span></a><span class="s1">at::Tensor concat(at::TensorList tensors, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concat.names(Tensor[] tensors, Dimname dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l251"><span class="ln">251  </span></a><span class="s1">at::Tensor &amp; concat_out(at::TensorList tensors, at::Dimname dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l252"><span class="ln">252  </span></a><span class="s1">at::Tensor concatenate(at::TensorList tensors, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concatenate(Tensor[] tensors, int dim=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l253"><span class="ln">253  </span></a><span class="s1">at::Tensor &amp; concatenate_out(at::TensorList tensors, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l254"><span class="ln">254  </span></a><span class="s1">at::Tensor concatenate(at::TensorList tensors, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concatenate.names(Tensor[] tensors, Dimname dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l255"><span class="ln">255  </span></a><span class="s1">at::Tensor &amp; concatenate_out(at::TensorList tensors, at::Dimname dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l256"><span class="ln">256  </span></a><span class="s1">at::Tensor block_diag(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::block_diag(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l257"><span class="ln">257  </span></a><span class="s1">at::Tensor ceil(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ceil(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l258"><span class="ln">258  </span></a><span class="s1">at::Tensor &amp; ceil_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ceil_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l259"><span class="ln">259  </span></a><span class="s1">at::Tensor &amp; ceil_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ceil.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l260"><span class="ln">260  </span></a><span class="s1">at::Tensor chain_matmul(at::TensorList matrices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::chain_matmul(Tensor[] matrices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l261"><span class="ln">261  </span></a><span class="s1">at::Tensor &amp; chain_matmul_out(at::TensorList matrices, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l262"><span class="ln">262  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; unsafe_chunk(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t chunks, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l263"><span class="ln">263  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; chunk(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t chunks, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::chunk(Tensor(a -&gt; *) self, int chunks, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l264"><span class="ln">264  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; tensor_split(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt sections, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tensor_split.sections(Tensor(a -&gt; *) self, SymInt sections, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l265"><span class="ln">265  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; tensor_split(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef indices, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tensor_split.indices(Tensor(a -&gt; *) self, SymInt[] indices, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l266"><span class="ln">266  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; tensor_split(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor_indices_or_sections, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tensor_split.tensor_indices_or_sections(Tensor(a -&gt; *) self, Tensor tensor_indices_or_sections, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l267"><span class="ln">267  </span></a><span class="s1">at::Tensor clamp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l268"><span class="ln">268  </span></a><span class="s1">at::Tensor clamp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l269"><span class="ln">269  </span></a><span class="s1">at::Tensor &amp; clamp_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l270"><span class="ln">270  </span></a><span class="s1">at::Tensor &amp; clamp_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l271"><span class="ln">271  </span></a><span class="s1">at::Tensor &amp; clamp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; max, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l272"><span class="ln">272  </span></a><span class="s1">at::Tensor &amp; clamp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l273"><span class="ln">273  </span></a><span class="s1">at::Tensor clamp_max(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_max(Tensor self, Scalar max) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l274"><span class="ln">274  </span></a><span class="s1">at::Tensor clamp_max(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_max.Tensor(Tensor self, Tensor max) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l275"><span class="ln">275  </span></a><span class="s1">at::Tensor &amp; clamp_max_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_max_(Tensor(a!) self, Scalar max) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l276"><span class="ln">276  </span></a><span class="s1">at::Tensor &amp; clamp_max_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l277"><span class="ln">277  </span></a><span class="s1">at::Tensor &amp; clamp_max_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l278"><span class="ln">278  </span></a><span class="s1">at::Tensor &amp; clamp_max_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; max, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l279"><span class="ln">279  </span></a><span class="s1">at::Tensor clamp_min(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_min(Tensor self, Scalar min) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l280"><span class="ln">280  </span></a><span class="s1">at::Tensor clamp_min(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; min); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_min.Tensor(Tensor self, Tensor min) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l281"><span class="ln">281  </span></a><span class="s1">at::Tensor &amp; clamp_min_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_min_(Tensor(a!) self, Scalar min) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l282"><span class="ln">282  </span></a><span class="s1">at::Tensor &amp; clamp_min_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; min); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l283"><span class="ln">283  </span></a><span class="s1">at::Tensor &amp; clamp_min_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l284"><span class="ln">284  </span></a><span class="s1">at::Tensor &amp; clamp_min_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; min, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l285"><span class="ln">285  </span></a><span class="s1">at::Tensor clip(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l286"><span class="ln">286  </span></a><span class="s1">at::Tensor clip(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l287"><span class="ln">287  </span></a><span class="s1">at::Tensor &amp; clip_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l288"><span class="ln">288  </span></a><span class="s1">at::Tensor &amp; clip_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l289"><span class="ln">289  </span></a><span class="s1">at::Tensor &amp; clip_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; max, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l290"><span class="ln">290  </span></a><span class="s1">at::Tensor &amp; clip_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l291"><span class="ln">291  </span></a><span class="s2">bool </span><span class="s1">cudnn_is_acceptable(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_is_acceptable(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l292"><span class="ln">292  </span></a><span class="s1">at::Tensor complex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; real, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; imag); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::complex(Tensor real, Tensor imag) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l293"><span class="ln">293  </span></a><span class="s1">at::Tensor &amp; complex_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; real, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; imag, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l294"><span class="ln">294  </span></a><span class="s1">at::Tensor polar(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; abs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; angle); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::polar(Tensor abs, Tensor angle) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l295"><span class="ln">295  </span></a><span class="s1">at::Tensor &amp; polar_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; abs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; angle, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l296"><span class="ln">296  </span></a><span class="s1">at::Tensor constant_pad_nd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef pad, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l297"><span class="ln">297  </span></a><span class="s1">at::Tensor contiguous(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::MemoryFormat memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l298"><span class="ln">298  </span></a><span class="s1">at::Tensor convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l299"><span class="ln">299  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; convolution_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, at::OptionalSymIntArrayRef bias_sizes, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l300"><span class="ln">300  </span></a><span class="s1">at::Tensor convolution_overrideable(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l301"><span class="ln">301  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; convolution_backward_overrideable(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -&gt; (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l302"><span class="ln">302  </span></a><span class="s1">at::Tensor _convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">cudnn_enabled, </span><span class="s2">bool </span><span class="s1">allow_tf32); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l303"><span class="ln">303  </span></a><span class="s1">at::Tensor _convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, at::IntArrayRef output_padding, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">cudnn_enabled); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, int[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l304"><span class="ln">304  </span></a><span class="s1">at::Tensor _convolution_mode(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, str padding, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l305"><span class="ln">305  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _convolution_double_backward(</span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; ggI, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; ggW, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; ggb, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; gO, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l306"><span class="ln">306  </span></a><span class="s1">at::Tensor conv1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] dilation=1, SymInt groups=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l307"><span class="ln">307  </span></a><span class="s1">at::Tensor conv2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1, SymInt groups=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l308"><span class="ln">308  </span></a><span class="s1">at::Tensor conv3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1, SymInt groups=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l309"><span class="ln">309  </span></a><span class="s1">at::Tensor conv1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, str padding=\&quot;valid\&quot;, SymInt[1] dilation=1, SymInt groups=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l310"><span class="ln">310  </span></a><span class="s1">at::Tensor conv2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=1, str padding=\&quot;valid\&quot;, SymInt[2] dilation=1, SymInt groups=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l311"><span class="ln">311  </span></a><span class="s1">at::Tensor conv3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, str padding=\&quot;valid\&quot;, SymInt[3] dilation=1, SymInt groups=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l312"><span class="ln">312  </span></a><span class="s1">at::Tensor conv_tbc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias, int64_t pad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l313"><span class="ln">313  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; conv_tbc_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias, int64_t pad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l314"><span class="ln">314  </span></a><span class="s1">at::Tensor conv_transpose1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] output_padding=0, SymInt groups=1, SymInt[1] dilation=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l315"><span class="ln">315  </span></a><span class="s1">at::Tensor conv_transpose2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt groups=1, SymInt[2] dilation=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l316"><span class="ln">316  </span></a><span class="s1">at::Tensor conv_transpose3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt groups=1, SymInt[3] dilation=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l317"><span class="ln">317  </span></a><span class="s1">at::Tensor copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copy(Tensor self, Tensor src, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l318"><span class="ln">318  </span></a><span class="s1">at::Tensor &amp; copy_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l319"><span class="ln">319  </span></a><span class="s1">at::Tensor _copy_from(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dst, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l320"><span class="ln">320  </span></a><span class="s1">at::Tensor _copy_from_and_resize(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dst); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_copy_from_and_resize(Tensor self, Tensor dst) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l321"><span class="ln">321  </span></a><span class="s1">at::Tensor cos(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cos(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l322"><span class="ln">322  </span></a><span class="s1">at::Tensor &amp; cos_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cos_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l323"><span class="ln">323  </span></a><span class="s1">at::Tensor &amp; cos_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cos.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l324"><span class="ln">324  </span></a><span class="s1">at::Tensor cosh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cosh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l325"><span class="ln">325  </span></a><span class="s1">at::Tensor &amp; cosh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cosh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l326"><span class="ln">326  </span></a><span class="s1">at::Tensor &amp; cosh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cosh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l327"><span class="ln">327  </span></a><span class="s1">at::Tensor cosine_embedding_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">double </span><span class="s1">margin, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l328"><span class="ln">328  </span></a><span class="s1">at::Tensor count_nonzero(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l329"><span class="ln">329  </span></a><span class="s1">at::Tensor count_nonzero(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::count_nonzero(Tensor self, int? dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l330"><span class="ln">330  </span></a><span class="s1">at::Tensor cov(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t correction, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; fweights, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; aweights); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l331"><span class="ln">331  </span></a><span class="s1">at::Tensor corrcoef(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::corrcoef(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l332"><span class="ln">332  </span></a><span class="s1">at::Tensor cudnn_affine_grid_generator(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; theta, int64_t N, int64_t C, int64_t H, int64_t W); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -&gt; Tensor grid&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l333"><span class="ln">333  </span></a><span class="s1">at::Tensor cudnn_affine_grid_generator_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, int64_t N, int64_t C, int64_t H, int64_t W); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -&gt; Tensor grad_theta&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l334"><span class="ln">334  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; cudnn_batch_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">exponential_average_factor, </span><span class="s2">double </span><span class="s1">epsilon); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l335"><span class="ln">335  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; cudnn_batch_norm_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_var, </span><span class="s2">double </span><span class="s1">epsilon, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reserveSpace); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l336"><span class="ln">336  </span></a><span class="s1">at::Tensor cudnn_convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">allow_tf32); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l337"><span class="ln">337  </span></a><span class="s1">at::Tensor &amp; cudnn_convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">allow_tf32, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l338"><span class="ln">338  </span></a><span class="s1">at::Tensor cudnn_convolution_transpose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">allow_tf32); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution_transpose(Tensor self, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l339"><span class="ln">339  </span></a><span class="s1">at::Tensor _mps_convolution_transpose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mps_convolution_transpose(Tensor self, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l340"><span class="ln">340  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; mps_convolution_transpose_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mps_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool[2] output_mask) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l341"><span class="ln">341  </span></a><span class="s1">at::Tensor cudnn_convolution_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l342"><span class="ln">342  </span></a><span class="s1">at::Tensor cudnn_convolution_add_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; z, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; alpha, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l343"><span class="ln">343  </span></a><span class="s1">at::Tensor cudnn_grid_sampler(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_grid_sampler(Tensor self, Tensor grid) -&gt; Tensor output&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l344"><span class="ln">344  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; cudnn_grid_sampler_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -&gt; (Tensor grad_self, Tensor grad_grid)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l345"><span class="ln">345  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; cummax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummax(Tensor self, int dim) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l346"><span class="ln">346  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; cummax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l347"><span class="ln">347  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; cummax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummax.dimname(Tensor self, Dimname dim) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l348"><span class="ln">348  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; cummax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l349"><span class="ln">349  </span></a><span class="s2">void </span><span class="s1">_cummax_helper(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; values, at::Tensor &amp; indices, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l350"><span class="ln">350  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; cummin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummin(Tensor self, int dim) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l351"><span class="ln">351  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; cummin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l352"><span class="ln">352  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; cummin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummin.dimname(Tensor self, Dimname dim) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l353"><span class="ln">353  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; cummin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l354"><span class="ln">354  </span></a><span class="s2">void </span><span class="s1">_cummin_helper(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; values, at::Tensor &amp; indices, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l355"><span class="ln">355  </span></a><span class="s1">at::Tensor cummaxmin_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l356"><span class="ln">356  </span></a><span class="s1">at::Tensor cumprod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l357"><span class="ln">357  </span></a><span class="s1">at::Tensor &amp; cumprod_(at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l358"><span class="ln">358  </span></a><span class="s1">at::Tensor &amp; cumprod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l359"><span class="ln">359  </span></a><span class="s1">at::Tensor cumprod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l360"><span class="ln">360  </span></a><span class="s1">at::Tensor &amp; cumprod_(at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l361"><span class="ln">361  </span></a><span class="s1">at::Tensor &amp; cumprod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l362"><span class="ln">362  </span></a><span class="s1">at::Tensor cumprod_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l363"><span class="ln">363  </span></a><span class="s1">at::Tensor cumsum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l364"><span class="ln">364  </span></a><span class="s1">at::Tensor &amp; cumsum_(at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l365"><span class="ln">365  </span></a><span class="s1">at::Tensor &amp; cumsum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l366"><span class="ln">366  </span></a><span class="s1">at::Tensor cumsum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l367"><span class="ln">367  </span></a><span class="s1">at::Tensor &amp; cumsum_(at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l368"><span class="ln">368  </span></a><span class="s1">at::Tensor &amp; cumsum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l369"><span class="ln">369  </span></a><span class="s1">at::Tensor cumulative_trapezoid(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l370"><span class="ln">370  </span></a><span class="s1">at::Tensor cumulative_trapezoid(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; dx, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l371"><span class="ln">371  </span></a><span class="s1">at::Tensor ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l372"><span class="ln">372  </span></a><span class="s1">at::Tensor ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target_lengths, int64_t blank, int64_t reduction, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l373"><span class="ln">373  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l374"><span class="ln">374  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _ctc_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target_lengths, int64_t blank, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l375"><span class="ln">375  </span></a><span class="s1">at::Tensor _ctc_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; neg_log_likelihood, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_alpha, int64_t blank, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l376"><span class="ln">376  </span></a><span class="s1">at::Tensor _ctc_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; neg_log_likelihood, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_alpha, int64_t blank, </span><span class="s2">bool </span><span class="s1">zero_infinity); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_ctc_loss_backward.Tensor(Tensor grad, Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l377"><span class="ln">377  </span></a><span class="s1">at::Tensor diag_embed(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t offset, int64_t dim1, int64_t dim2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l378"><span class="ln">378  </span></a><span class="s1">at::Tensor diagflat(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagflat(Tensor self, int offset=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l379"><span class="ln">379  </span></a><span class="s1">at::Tensor diagonal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t offset, int64_t dim1, int64_t dim2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l380"><span class="ln">380  </span></a><span class="s1">at::Tensor linalg_diagonal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, int64_t offset, int64_t dim1, int64_t dim2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_diagonal(Tensor(a) A, *, int offset=0, int dim1=-2, int dim2=-1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l381"><span class="ln">381  </span></a><span class="s1">at::Tensor diagonal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l382"><span class="ln">382  </span></a><span class="s1">at::Tensor diagonal_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal_backward(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l383"><span class="ln">383  </span></a><span class="s1">at::Tensor &amp; fill_diagonal_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, </span><span class="s2">bool </span><span class="s1">wrap); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l384"><span class="ln">384  </span></a><span class="s1">at::Tensor diff(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n, int64_t dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; prepend, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; append); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l385"><span class="ln">385  </span></a><span class="s1">at::Tensor &amp; diff_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n, int64_t dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; prepend, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; append, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l386"><span class="ln">386  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; gradient(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; spacing, ::std::optional&lt;int64_t&gt; dim, int64_t edge_order); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l387"><span class="ln">387  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; gradient(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; spacing, at::IntArrayRef dim, int64_t edge_order); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l388"><span class="ln">388  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; gradient(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, int64_t edge_order); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gradient.array(Tensor self, *, int[] dim, int edge_order=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l389"><span class="ln">389  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; gradient(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ArrayRef&lt;at::Scalar&gt; spacing, ::std::optional&lt;int64_t&gt; dim, int64_t edge_order); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l390"><span class="ln">390  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; gradient(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ArrayRef&lt;at::Scalar&gt; spacing, at::IntArrayRef dim, int64_t edge_order); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l391"><span class="ln">391  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; gradient(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::TensorList spacing, ::std::optional&lt;int64_t&gt; dim, int64_t edge_order); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l392"><span class="ln">392  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; gradient(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l393"><span class="ln">393  </span></a><span class="s1">at::Tensor div(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l394"><span class="ln">394  </span></a><span class="s1">at::Tensor &amp; div_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l395"><span class="ln">395  </span></a><span class="s1">at::Tensor &amp; div_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l396"><span class="ln">396  </span></a><span class="s1">at::Tensor div(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l397"><span class="ln">397  </span></a><span class="s1">at::Tensor &amp; div_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l398"><span class="ln">398  </span></a><span class="s1">at::Tensor &amp; div_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l399"><span class="ln">399  </span></a><span class="s1">at::Tensor div(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l400"><span class="ln">400  </span></a><span class="s1">at::Tensor &amp; div_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l401"><span class="ln">401  </span></a><span class="s1">at::Tensor div(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l402"><span class="ln">402  </span></a><span class="s1">at::Tensor &amp; div_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l403"><span class="ln">403  </span></a><span class="s1">at::Tensor divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l404"><span class="ln">404  </span></a><span class="s1">at::Tensor &amp; divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l405"><span class="ln">405  </span></a><span class="s1">at::Tensor &amp; divide_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l406"><span class="ln">406  </span></a><span class="s1">at::Tensor divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l407"><span class="ln">407  </span></a><span class="s1">at::Tensor &amp; divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l408"><span class="ln">408  </span></a><span class="s1">at::Tensor divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l409"><span class="ln">409  </span></a><span class="s1">at::Tensor &amp; divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l410"><span class="ln">410  </span></a><span class="s1">at::Tensor &amp; divide_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l411"><span class="ln">411  </span></a><span class="s1">at::Tensor divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l412"><span class="ln">412  </span></a><span class="s1">at::Tensor &amp; divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l413"><span class="ln">413  </span></a><span class="s1">at::Tensor true_divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::true_divide.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l414"><span class="ln">414  </span></a><span class="s1">at::Tensor &amp; true_divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l415"><span class="ln">415  </span></a><span class="s1">at::Tensor &amp; true_divide_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l416"><span class="ln">416  </span></a><span class="s1">at::Tensor true_divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::true_divide.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l417"><span class="ln">417  </span></a><span class="s1">at::Tensor &amp; true_divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l418"><span class="ln">418  </span></a><span class="s1">at::Tensor dot(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dot(Tensor self, Tensor tensor) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l419"><span class="ln">419  </span></a><span class="s1">at::Tensor &amp; dot_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l420"><span class="ln">420  </span></a><span class="s1">at::Tensor vdot(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::vdot(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l421"><span class="ln">421  </span></a><span class="s1">at::Tensor &amp; vdot_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l422"><span class="ln">422  </span></a><span class="s1">at::Tensor einsum(c10::string_view equation, at::TensorList tensors, at::OptionalIntArrayRef path); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::einsum(str equation, Tensor[] tensors, *, int[]? path=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l423"><span class="ln">423  </span></a><span class="s1">at::Tensor embedding(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymInt padding_idx, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, </span><span class="s2">bool </span><span class="s1">sparse); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l424"><span class="ln">424  </span></a><span class="s1">at::Tensor embedding_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymInt num_weights, c10::SymInt padding_idx, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, </span><span class="s2">bool </span><span class="s1">sparse); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_backward(Tensor grad, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, bool sparse) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l425"><span class="ln">425  </span></a><span class="s1">at::Tensor embedding_dense_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymInt num_weights, c10::SymInt padding_idx, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l426"><span class="ln">426  </span></a><span class="s1">at::Tensor &amp; embedding_renorm_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">double </span><span class="s1">max_norm, </span><span class="s2">double </span><span class="s1">norm_type); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l427"><span class="ln">427  </span></a><span class="s1">at::Tensor embedding_sparse_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, int64_t num_weights, int64_t padding_idx, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l428"><span class="ln">428  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _embedding_bag_forward_only(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">bool </span><span class="s1">sparse, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, </span><span class="s2">bool </span><span class="s1">include_last_offset, int64_t padding_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l429"><span class="ln">429  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _rowwise_prune(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, at::ScalarType compressed_indices_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l430"><span class="ln">430  </span></a><span class="s1">at::Tensor row_stack(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::row_stack(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l431"><span class="ln">431  </span></a><span class="s1">at::Tensor &amp; row_stack_out(at::TensorList tensors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l432"><span class="ln">432  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; embedding_bag(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">bool </span><span class="s1">sparse, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, </span><span class="s2">bool </span><span class="s1">include_last_offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l433"><span class="ln">433  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; embedding_bag(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">bool </span><span class="s1">sparse, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, </span><span class="s2">bool </span><span class="s1">include_last_offset, ::std::optional&lt;int64_t&gt; padding_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l434"><span class="ln">434  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _embedding_bag(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">bool </span><span class="s1">sparse, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, </span><span class="s2">bool </span><span class="s1">include_last_offset, int64_t padding_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l435"><span class="ln">435  </span></a><span class="s1">at::Tensor _embedding_bag_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offset2bag, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bag_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; maximum_indices, c10::SymInt num_weights, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">bool </span><span class="s1">sparse, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, int64_t padding_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l436"><span class="ln">436  </span></a><span class="s1">at::Tensor _embedding_bag_sparse_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offset2bag, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bag_size, c10::SymInt num_weights, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, int64_t padding_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l437"><span class="ln">437  </span></a><span class="s1">at::Tensor _embedding_bag_dense_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offset2bag, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bag_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; maximum_indices, c10::SymInt num_weights, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, int64_t padding_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l438"><span class="ln">438  </span></a><span class="s1">at::Tensor _embedding_bag_per_sample_weights_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offset2bag, int64_t mode, int64_t padding_idx); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l439"><span class="ln">439  </span></a><span class="s1">at::Tensor empty(at::IntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l440"><span class="ln">440  </span></a><span class="s1">at::Tensor empty(c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l441"><span class="ln">441  </span></a><span class="s1">at::Tensor empty_permuted(c10::SymIntArrayRef size, at::IntArrayRef physical_layout, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l442"><span class="ln">442  </span></a><span class="s1">at::Tensor new_empty(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l443"><span class="ln">443  </span></a><span class="s1">at::Tensor new_empty_strided(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l444"><span class="ln">444  </span></a><span class="s1">at::Tensor new_full(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l445"><span class="ln">445  </span></a><span class="s1">at::Tensor new_zeros(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l446"><span class="ln">446  </span></a><span class="s1">at::Tensor new_ones(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l447"><span class="ln">447  </span></a><span class="s1">at::Tensor _empty_affine_quantized(c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l448"><span class="ln">448  </span></a><span class="s1">at::Tensor _empty_per_channel_affine_quantized(c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_points, int64_t axis, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_empty_per_channel_affine_quantized(SymInt[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l449"><span class="ln">449  </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; resize_(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l450"><span class="ln">450  </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; _resize_output_(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Device device); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_resize_output_(Tensor(a!) self, SymInt[] size, Device device) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l451"><span class="ln">451  </span></a><span class="s1">at::Tensor empty_quantized(at::IntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qtensor, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l452"><span class="ln">452  </span></a><span class="s1">at::Tensor &amp; empty_out(c10::SymIntArrayRef size, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l453"><span class="ln">453  </span></a><span class="s1">at::Tensor empty_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l454"><span class="ln">454  </span></a><span class="s1">at::Tensor empty_strided(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l455"><span class="ln">455  </span></a><span class="s1">at::Tensor erf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erf(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l456"><span class="ln">456  </span></a><span class="s1">at::Tensor &amp; erf_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erf_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l457"><span class="ln">457  </span></a><span class="s1">at::Tensor &amp; erf_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erf.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l458"><span class="ln">458  </span></a><span class="s1">at::Tensor erfc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erfc(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l459"><span class="ln">459  </span></a><span class="s1">at::Tensor &amp; erfc_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erfc_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l460"><span class="ln">460  </span></a><span class="s1">at::Tensor &amp; erfc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erfc.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l461"><span class="ln">461  </span></a><span class="s1">at::Tensor exp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exp(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l462"><span class="ln">462  </span></a><span class="s1">at::Tensor &amp; exp_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exp_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l463"><span class="ln">463  </span></a><span class="s1">at::Tensor &amp; exp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exp.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l464"><span class="ln">464  </span></a><span class="s1">at::Tensor exp2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exp2(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l465"><span class="ln">465  </span></a><span class="s1">at::Tensor &amp; exp2_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exp2_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l466"><span class="ln">466  </span></a><span class="s1">at::Tensor &amp; exp2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exp2.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l467"><span class="ln">467  </span></a><span class="s1">at::Tensor expm1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::expm1(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l468"><span class="ln">468  </span></a><span class="s1">at::Tensor &amp; expm1_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::expm1_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l469"><span class="ln">469  </span></a><span class="s1">at::Tensor &amp; expm1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::expm1.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l470"><span class="ln">470  </span></a><span class="s1">at::Tensor expand(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, </span><span class="s2">bool </span><span class="s1">implicit); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l471"><span class="ln">471  </span></a><span class="s1">at::Tensor expand_as(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::expand_as(Tensor(a) self, Tensor other) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l472"><span class="ln">472  </span></a><span class="s1">at::Tensor eye(c10::SymInt n, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l473"><span class="ln">473  </span></a><span class="s1">at::Tensor eye(c10::SymInt n, c10::SymInt m, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l474"><span class="ln">474  </span></a><span class="s1">at::Tensor &amp; eye_out(c10::SymInt n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eye.out(SymInt n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l475"><span class="ln">475  </span></a><span class="s1">at::Tensor &amp; eye_out(c10::SymInt n, c10::SymInt m, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l476"><span class="ln">476  </span></a><span class="s1">at::Tensor flatten(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t start_dim, int64_t end_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l477"><span class="ln">477  </span></a><span class="s1">at::Tensor flatten(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l478"><span class="ln">478  </span></a><span class="s1">at::Tensor flatten(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l479"><span class="ln">479  </span></a><span class="s1">at::Tensor flatten(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dims, at::Dimname out_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l480"><span class="ln">480  </span></a><span class="s1">at::Tensor unflatten(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymIntArrayRef sizes); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l481"><span class="ln">481  </span></a><span class="s1">at::Tensor unflatten(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, c10::SymIntArrayRef sizes, at::DimnameList names); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l482"><span class="ln">482  </span></a><span class="s1">at::Tensor fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fill.Scalar(Tensor self, Scalar value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l483"><span class="ln">483  </span></a><span class="s1">at::Tensor fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fill.Tensor(Tensor self, Tensor value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l484"><span class="ln">484  </span></a><span class="s1">at::Tensor &amp; fill_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fill_.Scalar(Tensor(a!) self, Scalar value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l485"><span class="ln">485  </span></a><span class="s1">at::Tensor &amp; fill_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fill_.Tensor(Tensor(a!) self, Tensor value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l486"><span class="ln">486  </span></a><span class="s1">at::Tensor floor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l487"><span class="ln">487  </span></a><span class="s1">at::Tensor &amp; floor_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l488"><span class="ln">488  </span></a><span class="s1">at::Tensor &amp; floor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l489"><span class="ln">489  </span></a><span class="s1">at::Tensor floor_divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor_divide(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l490"><span class="ln">490  </span></a><span class="s1">at::Tensor &amp; floor_divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l491"><span class="ln">491  </span></a><span class="s1">at::Tensor &amp; floor_divide_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l492"><span class="ln">492  </span></a><span class="s1">at::Tensor floor_divide(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor_divide.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l493"><span class="ln">493  </span></a><span class="s1">at::Tensor &amp; floor_divide_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l494"><span class="ln">494  </span></a><span class="s1">at::Tensor frac(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::frac(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l495"><span class="ln">495  </span></a><span class="s1">at::Tensor &amp; frac_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::frac_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l496"><span class="ln">496  </span></a><span class="s1">at::Tensor &amp; frac_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::frac.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l497"><span class="ln">497  </span></a><span class="s1">at::Tensor full(at::IntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l498"><span class="ln">498  </span></a><span class="s1">at::Tensor full(c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l499"><span class="ln">499  </span></a><span class="s1">at::Tensor &amp; full_out(c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l500"><span class="ln">500  </span></a><span class="s1">at::Tensor full_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l501"><span class="ln">501  </span></a><span class="s1">at::Tensor from_file(c10::string_view filename, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; shared, ::std::optional&lt;int64_t&gt; size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l502"><span class="ln">502  </span></a><span class="s1">at::Tensor &amp; gcd_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l503"><span class="ln">503  </span></a><span class="s1">at::Tensor gcd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gcd(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l504"><span class="ln">504  </span></a><span class="s1">at::Tensor &amp; gcd_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gcd_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l505"><span class="ln">505  </span></a><span class="s1">at::Tensor &amp; lcm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l506"><span class="ln">506  </span></a><span class="s1">at::Tensor lcm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lcm(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l507"><span class="ln">507  </span></a><span class="s1">at::Tensor &amp; lcm_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lcm_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l508"><span class="ln">508  </span></a><span class="s1">at::Tensor grid_sampler(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l509"><span class="ln">509  </span></a><span class="s1">at::Tensor grid_sampler_2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l510"><span class="ln">510  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; grid_sampler_2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l511"><span class="ln">511  </span></a><span class="s1">at::Tensor _grid_sampler_2d_cpu_fallback(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l512"><span class="ln">512  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _grid_sampler_2d_cpu_fallback_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l513"><span class="ln">513  </span></a><span class="s1">at::Tensor grid_sampler_3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l514"><span class="ln">514  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; grid_sampler_3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l515"><span class="ln">515  </span></a><span class="s1">at::Tensor hann_window(int64_t window_length, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l516"><span class="ln">516  </span></a><span class="s1">at::Tensor hann_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l517"><span class="ln">517  </span></a><span class="s1">at::Tensor hamming_window(int64_t window_length, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l518"><span class="ln">518  </span></a><span class="s1">at::Tensor hamming_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l519"><span class="ln">519  </span></a><span class="s1">at::Tensor hamming_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, </span><span class="s2">double </span><span class="s1">alpha, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l520"><span class="ln">520  </span></a><span class="s1">at::Tensor hamming_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, </span><span class="s2">double </span><span class="s1">alpha, </span><span class="s2">double </span><span class="s1">beta, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l521"><span class="ln">521  </span></a><span class="s1">at::Tensor kaiser_window(int64_t window_length, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l522"><span class="ln">522  </span></a><span class="s1">at::Tensor kaiser_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l523"><span class="ln">523  </span></a><span class="s1">at::Tensor kaiser_window(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, </span><span class="s2">double </span><span class="s1">beta, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l524"><span class="ln">524  </span></a><span class="s1">at::Tensor hinge_embedding_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">double </span><span class="s1">margin, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l525"><span class="ln">525  </span></a><span class="s1">at::Tensor group_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, int64_t num_groups, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">cudnn_enabled); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l526"><span class="ln">526  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; native_group_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l527"><span class="ln">527  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; native_group_norm_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; rstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l528"><span class="ln">528  </span></a><span class="s1">at::Tensor _fft_r2c(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, int64_t normalization, </span><span class="s2">bool </span><span class="s1">onesided); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l529"><span class="ln">529  </span></a><span class="s1">at::Tensor &amp; _fft_r2c_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, int64_t normalization, </span><span class="s2">bool </span><span class="s1">onesided, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l530"><span class="ln">530  </span></a><span class="s1">at::Tensor _fft_c2r(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, int64_t normalization, c10::SymInt last_dim_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fft_c2r(Tensor self, int[] dim, int normalization, SymInt last_dim_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l531"><span class="ln">531  </span></a><span class="s1">at::Tensor &amp; _fft_c2r_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, int64_t normalization, c10::SymInt last_dim_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, SymInt last_dim_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l532"><span class="ln">532  </span></a><span class="s1">at::Tensor _fft_c2c(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef dim, int64_t normalization, </span><span class="s2">bool </span><span class="s1">forward); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l533"><span class="ln">533  </span></a><span class="s1">at::Tensor &amp; _fft_c2c_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef dim, int64_t normalization, </span><span class="s2">bool </span><span class="s1">forward, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l534"><span class="ln">534  </span></a><span class="s2">void </span><span class="s1">_validate_compressed_sparse_indices(</span><span class="s2">bool </span><span class="s1">is_crow, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; compressed_idx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; plain_idx, int64_t cdim, int64_t dim, int64_t nnz); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l535"><span class="ln">535  </span></a><span class="s1">int64_t _cufft_get_plan_cache_size(at::DeviceIndex device_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cufft_get_plan_cache_size(DeviceIndex device_index) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l536"><span class="ln">536  </span></a><span class="s1">int64_t _cufft_get_plan_cache_max_size(at::DeviceIndex device_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cufft_get_plan_cache_max_size(DeviceIndex device_index) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l537"><span class="ln">537  </span></a><span class="s2">void </span><span class="s1">_cufft_set_plan_cache_max_size(at::DeviceIndex device_index, int64_t max_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cufft_set_plan_cache_max_size(DeviceIndex device_index, int max_size) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l538"><span class="ln">538  </span></a><span class="s2">void </span><span class="s1">_cufft_clear_plan_cache(at::DeviceIndex device_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cufft_clear_plan_cache(DeviceIndex device_index) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l539"><span class="ln">539  </span></a><span class="s1">at::Tensor index(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index.Tensor(Tensor self, Tensor?[] indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l540"><span class="ln">540  </span></a><span class="s1">at::Tensor &amp; index_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l541"><span class="ln">541  </span></a><span class="s1">at::Tensor _unsafe_index(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unsafe_index.Tensor(Tensor self, Tensor?[] indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l542"><span class="ln">542  </span></a><span class="s1">at::Tensor _unsafe_masked_index(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unsafe_masked_index(Tensor self, Tensor mask, Tensor?[] indices, Scalar fill) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l543"><span class="ln">543  </span></a><span class="s1">at::Tensor _unsafe_masked_index_put_accumulate(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unsafe_masked_index_put_accumulate(Tensor self, Tensor mask, Tensor?[] indices, Tensor values) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l544"><span class="ln">544  </span></a><span class="s1">at::Tensor &amp; index_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l545"><span class="ln">545  </span></a><span class="s1">at::Tensor &amp; index_copy_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l546"><span class="ln">546  </span></a><span class="s1">at::Tensor index_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l547"><span class="ln">547  </span></a><span class="s1">at::Tensor &amp; index_copy_(at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l548"><span class="ln">548  </span></a><span class="s1">at::Tensor index_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l549"><span class="ln">549  </span></a><span class="s1">at::Tensor &amp; index_put_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, </span><span class="s2">bool </span><span class="s1">accumulate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l550"><span class="ln">550  </span></a><span class="s1">at::Tensor index_put(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, </span><span class="s2">bool </span><span class="s1">accumulate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l551"><span class="ln">551  </span></a><span class="s1">at::Tensor _unsafe_index_put(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, </span><span class="s2">bool </span><span class="s1">accumulate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unsafe_index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l552"><span class="ln">552  </span></a><span class="s1">at::Tensor &amp; _index_put_impl_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, </span><span class="s2">bool </span><span class="s1">accumulate, </span><span class="s2">bool </span><span class="s1">unsafe); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l553"><span class="ln">553  </span></a><span class="s1">at::Tensor instance_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">use_input_stats, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">cudnn_enabled); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l554"><span class="ln">554  </span></a><span class="s1">at::Tensor isclose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">double </span><span class="s1">rtol, </span><span class="s2">double </span><span class="s1">atol, </span><span class="s2">bool </span><span class="s1">equal_nan); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l555"><span class="ln">555  </span></a><span class="s1">at::Tensor &amp; isin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; elements, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; test_elements, </span><span class="s2">bool </span><span class="s1">assume_unique, </span><span class="s2">bool </span><span class="s1">invert, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l556"><span class="ln">556  </span></a><span class="s1">at::Tensor isin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; elements, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; test_elements, </span><span class="s2">bool </span><span class="s1">assume_unique, </span><span class="s2">bool </span><span class="s1">invert); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l557"><span class="ln">557  </span></a><span class="s1">at::Tensor &amp; isin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; elements, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; test_element, </span><span class="s2">bool </span><span class="s1">assume_unique, </span><span class="s2">bool </span><span class="s1">invert, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l558"><span class="ln">558  </span></a><span class="s1">at::Tensor isin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; elements, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; test_element, </span><span class="s2">bool </span><span class="s1">assume_unique, </span><span class="s2">bool </span><span class="s1">invert); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l559"><span class="ln">559  </span></a><span class="s1">at::Tensor &amp; isin_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; element, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; test_elements, </span><span class="s2">bool </span><span class="s1">assume_unique, </span><span class="s2">bool </span><span class="s1">invert, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l560"><span class="ln">560  </span></a><span class="s1">at::Tensor isin(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; element, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; test_elements, </span><span class="s2">bool </span><span class="s1">assume_unique, </span><span class="s2">bool </span><span class="s1">invert); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l561"><span class="ln">561  </span></a><span class="s1">at::Tensor isnan(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isnan(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l562"><span class="ln">562  </span></a><span class="s2">bool </span><span class="s1">is_distributed(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_distributed(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l563"><span class="ln">563  </span></a><span class="s2">bool </span><span class="s1">is_floating_point(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_floating_point(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l564"><span class="ln">564  </span></a><span class="s2">bool </span><span class="s1">is_complex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_complex(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l565"><span class="ln">565  </span></a><span class="s2">bool </span><span class="s1">is_conj(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_conj(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l566"><span class="ln">566  </span></a><span class="s2">bool </span><span class="s1">_is_zerotensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_is_zerotensor(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l567"><span class="ln">567  </span></a><span class="s2">bool </span><span class="s1">is_neg(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_neg(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l568"><span class="ln">568  </span></a><span class="s1">at::Tensor isreal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isreal(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l569"><span class="ln">569  </span></a><span class="s2">bool </span><span class="s1">is_nonzero(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_nonzero(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l570"><span class="ln">570  </span></a><span class="s2">bool </span><span class="s1">is_same_size(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_same_size(Tensor self, Tensor other) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l571"><span class="ln">571  </span></a><span class="s2">bool </span><span class="s1">is_signed(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_signed(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l572"><span class="ln">572  </span></a><span class="s2">bool </span><span class="s1">is_inference(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_inference(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l573"><span class="ln">573  </span></a><span class="s1">at::Tensor kl_div(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">bool </span><span class="s1">log_target); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l574"><span class="ln">574  </span></a><span class="s1">at::Tensor kron(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kron(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l575"><span class="ln">575  </span></a><span class="s1">at::Tensor &amp; kron_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l576"><span class="ln">576  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; kthvalue(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt k, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kthvalue(Tensor self, SymInt k, int dim=-1, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l577"><span class="ln">577  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; kthvalue_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt k, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kthvalue.values(Tensor self, SymInt k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l578"><span class="ln">578  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; kthvalue(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt k, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kthvalue.dimname(Tensor self, SymInt k, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l579"><span class="ln">579  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; kthvalue_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt k, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kthvalue.dimname_out(Tensor self, SymInt k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l580"><span class="ln">580  </span></a><span class="s1">at::Tensor layer_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, c10::SymIntArrayRef normalized_shape, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">cudnn_enable); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l581"><span class="ln">581  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; native_layer_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, c10::SymIntArrayRef normalized_shape, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l582"><span class="ln">582  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; native_layer_norm_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, c10::SymIntArrayRef normalized_shape, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; rstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l583"><span class="ln">583  </span></a><span class="s1">at::Tensor rms_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, c10::SymIntArrayRef normalized_shape, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rms_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, float? eps=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l584"><span class="ln">584  </span></a><span class="s1">at::Tensor _fused_rms_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, int64_t normalized_shape_ndim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_rms_norm(Tensor input, int normalized_shape_ndim, Tensor weight, float eps) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l585"><span class="ln">585  </span></a><span class="s1">at::Tensor nan_to_num(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; nan, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; posinf, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; neginf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l586"><span class="ln">586  </span></a><span class="s1">at::Tensor &amp; nan_to_num_(at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; nan, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; posinf, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; neginf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l587"><span class="ln">587  </span></a><span class="s1">at::Tensor &amp; nan_to_num_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; nan, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; posinf, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; neginf, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l588"><span class="ln">588  </span></a><span class="s1">at::Tensor linear(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l589"><span class="ln">589  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; linear_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l590"><span class="ln">590  </span></a><span class="s1">at::Tensor &amp; linear_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l591"><span class="ln">591  </span></a><span class="s1">at::Tensor mkldnn_linear(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l592"><span class="ln">592  </span></a><span class="s1">at::Tensor mkldnn_linear_backward_input(at::IntArrayRef input_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l593"><span class="ln">593  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; mkldnn_linear_backward_weights(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">bool </span><span class="s1">bias_defined); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l594"><span class="ln">594  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; mkldnn_linear_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l595"><span class="ln">595  </span></a><span class="s1">at::Tensor _cslt_compress(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cslt_compress(Tensor input) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l596"><span class="ln">596  </span></a><span class="s1">at::Tensor _cslt_sparse_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; compressed_A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dense_B, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; alpha, ::std::optional&lt;at::ScalarType&gt; out_dtype, </span><span class="s2">bool </span><span class="s1">transpose_result, int64_t alg_id, int64_t split_k, int64_t split_k_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cslt_sparse_mm(Tensor compressed_A, Tensor dense_B, Tensor? bias=None, Tensor? alpha=None, ScalarType? out_dtype=None, bool transpose_result=False, int alg_id=0, int split_k=1, int split_k_mode=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l597"><span class="ln">597  </span></a><span class="s1">int64_t _cslt_sparse_mm_search(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; compressed_A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dense_B, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; alpha, ::std::optional&lt;at::ScalarType&gt; out_dtype, </span><span class="s2">bool </span><span class="s1">transpose_result); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cslt_sparse_mm_search(Tensor compressed_A, Tensor dense_B, Tensor? bias=None, Tensor? alpha=None, ScalarType? out_dtype=None, bool transpose_result=False) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l598"><span class="ln">598  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _sparse_semi_structured_tile(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, c10::string_view algorithm, </span><span class="s2">bool </span><span class="s1">use_cutlass); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_semi_structured_tile(Tensor input, str algorithm=\&quot;\&quot;, bool use_cutlass=True) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l599"><span class="ln">599  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _sparse_semi_structured_apply(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; thread_masks); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_semi_structured_apply(Tensor input, Tensor thread_masks) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l600"><span class="ln">600  </span></a><span class="s1">at::Tensor _sparse_semi_structured_apply_dense(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; thread_masks); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_semi_structured_apply_dense(Tensor input, Tensor thread_masks) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l601"><span class="ln">601  </span></a><span class="s1">at::Tensor _sparse_semi_structured_linear(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; meta, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, ::std::optional&lt;c10::string_view&gt; activation, ::std::optional&lt;at::ScalarType&gt; out_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_semi_structured_linear(Tensor input, Tensor weight, Tensor meta, *, Tensor? bias=None, str? activation=None, ScalarType? out_dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l602"><span class="ln">602  </span></a><span class="s1">at::Tensor _sparse_semi_structured_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1_meta, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, ::std::optional&lt;at::ScalarType&gt; out_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_semi_structured_mm(Tensor mat1, Tensor mat1_meta, Tensor mat2, *, ScalarType? out_dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l603"><span class="ln">603  </span></a><span class="s1">at::Tensor _sparse_semi_structured_addmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1_meta, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, ::std::optional&lt;at::ScalarType&gt; out_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_semi_structured_addmm(Tensor input, Tensor mat1, Tensor mat1_meta, Tensor mat2, *, Scalar alpha=1, Scalar beta=1, ScalarType? out_dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l604"><span class="ln">604  </span></a><span class="s1">at::Tensor _mixed_dtypes_linear(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, ::std::optional&lt;c10::string_view&gt; activation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mixed_dtypes_linear(Tensor input, Tensor weight, Tensor scale, *, Tensor? bias=None, str? activation=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l605"><span class="ln">605  </span></a><span class="s1">at::Tensor fbgemm_linear_int8_weight_fp32_activation(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight_scale, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight_zero_point, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l606"><span class="ln">606  </span></a><span class="s1">at::Tensor fbgemm_linear_int8_weight(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight_scale, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight_zero_point, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l607"><span class="ln">607  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,</span><span class="s2">double</span><span class="s1">,int64_t&gt; fbgemm_linear_quantize_weight(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_linear_quantize_weight(Tensor input) -&gt; (Tensor, Tensor, float, int)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l608"><span class="ln">608  </span></a><span class="s1">at::Tensor fbgemm_pack_gemm_matrix_fp16(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l609"><span class="ln">609  </span></a><span class="s1">at::Tensor _wrapped_linear_prepack(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight_scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight_zero_point, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_wrapped_linear_prepack(Tensor weight, Tensor weight_scale, Tensor weight_zero_point, Tensor bias) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l610"><span class="ln">610  </span></a><span class="s1">at::Tensor _wrapped_quantized_linear_prepacked(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_zero_point, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output_scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output_zero_point, int64_t out_channel); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_wrapped_quantized_linear_prepacked(Tensor input, Tensor input_scale, Tensor input_zero_point, Tensor packed_weight, Tensor output_scale, Tensor output_zero_point, int out_channel) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l611"><span class="ln">611  </span></a><span class="s1">at::Tensor fbgemm_linear_fp16_weight_fp32_activation(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l612"><span class="ln">612  </span></a><span class="s1">at::Tensor fbgemm_linear_fp16_weight(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l613"><span class="ln">613  </span></a><span class="s1">at::Tensor fbgemm_pack_quantized_matrix(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_pack_quantized_matrix(Tensor input) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l614"><span class="ln">614  </span></a><span class="s1">at::Tensor fbgemm_pack_quantized_matrix(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, int64_t K, int64_t N); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l615"><span class="ln">615  </span></a><span class="s1">at::Tensor ldexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ldexp.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l616"><span class="ln">616  </span></a><span class="s1">at::Tensor &amp; ldexp_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ldexp_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l617"><span class="ln">617  </span></a><span class="s1">at::Tensor &amp; ldexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l618"><span class="ln">618  </span></a><span class="s1">at::Tensor linspace(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l619"><span class="ln">619  </span></a><span class="s1">at::Tensor linspace(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace.Tensor_Tensor(Tensor start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l620"><span class="ln">620  </span></a><span class="s1">at::Tensor linspace(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace.Tensor_Scalar(Tensor start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l621"><span class="ln">621  </span></a><span class="s1">at::Tensor linspace(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace.Scalar_Tensor(Scalar start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l622"><span class="ln">622  </span></a><span class="s1">at::Tensor &amp; linspace_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l623"><span class="ln">623  </span></a><span class="s1">at::Tensor &amp; linspace_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace.Tensor_Tensor_out(Tensor start, Tensor end, int steps, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l624"><span class="ln">624  </span></a><span class="s1">at::Tensor &amp; linspace_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace.Tensor_Scalar_out(Tensor start, Scalar end, int steps, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l625"><span class="ln">625  </span></a><span class="s1">at::Tensor &amp; linspace_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linspace.Scalar_Tensor_out(Scalar start, Tensor end, int steps, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l626"><span class="ln">626  </span></a><span class="s1">at::Tensor log(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l627"><span class="ln">627  </span></a><span class="s1">at::Tensor &amp; log_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l628"><span class="ln">628  </span></a><span class="s1">at::Tensor &amp; log_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l629"><span class="ln">629  </span></a><span class="s1">at::Tensor log10(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log10(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l630"><span class="ln">630  </span></a><span class="s1">at::Tensor &amp; log10_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log10_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l631"><span class="ln">631  </span></a><span class="s1">at::Tensor &amp; log10_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log10.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l632"><span class="ln">632  </span></a><span class="s1">at::Tensor log1p(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log1p(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l633"><span class="ln">633  </span></a><span class="s1">at::Tensor &amp; log1p_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log1p_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l634"><span class="ln">634  </span></a><span class="s1">at::Tensor &amp; log1p_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log1p.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l635"><span class="ln">635  </span></a><span class="s1">at::Tensor log2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log2(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l636"><span class="ln">636  </span></a><span class="s1">at::Tensor &amp; log2_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log2_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l637"><span class="ln">637  </span></a><span class="s1">at::Tensor &amp; log2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log2.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l638"><span class="ln">638  </span></a><span class="s1">at::Tensor &amp; logaddexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l639"><span class="ln">639  </span></a><span class="s1">at::Tensor logaddexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logaddexp(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l640"><span class="ln">640  </span></a><span class="s1">at::Tensor &amp; logaddexp2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l641"><span class="ln">641  </span></a><span class="s1">at::Tensor logaddexp2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logaddexp2(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l642"><span class="ln">642  </span></a><span class="s1">at::Tensor xlogy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l643"><span class="ln">643  </span></a><span class="s1">at::Tensor xlogy(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy.Scalar_Self(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l644"><span class="ln">644  </span></a><span class="s1">at::Tensor xlogy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy.Scalar_Other(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l645"><span class="ln">645  </span></a><span class="s1">at::Tensor &amp; xlogy_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l646"><span class="ln">646  </span></a><span class="s1">at::Tensor &amp; xlogy_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l647"><span class="ln">647  </span></a><span class="s1">at::Tensor &amp; xlogy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l648"><span class="ln">648  </span></a><span class="s1">at::Tensor &amp; xlogy_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l649"><span class="ln">649  </span></a><span class="s1">at::Tensor &amp; xlogy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l650"><span class="ln">650  </span></a><span class="s1">at::Tensor logspace(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l651"><span class="ln">651  </span></a><span class="s1">at::Tensor logspace(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace.Tensor_Tensor(Tensor start, Tensor end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l652"><span class="ln">652  </span></a><span class="s1">at::Tensor logspace(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace.Tensor_Scalar(Tensor start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l653"><span class="ln">653  </span></a><span class="s1">at::Tensor logspace(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace.Scalar_Tensor(Scalar start, Tensor end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l654"><span class="ln">654  </span></a><span class="s1">at::Tensor &amp; logspace_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l655"><span class="ln">655  </span></a><span class="s1">at::Tensor &amp; logspace_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace.Tensor_Tensor_out(Tensor start, Tensor end, int steps, float base=10.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l656"><span class="ln">656  </span></a><span class="s1">at::Tensor &amp; logspace_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace.Tensor_Scalar_out(Tensor start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l657"><span class="ln">657  </span></a><span class="s1">at::Tensor &amp; logspace_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, int64_t steps, </span><span class="s2">double </span><span class="s1">base, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logspace.Scalar_Tensor_out(Scalar start, Tensor end, int steps, float base=10.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l658"><span class="ln">658  </span></a><span class="s1">at::Tensor log_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l659"><span class="ln">659  </span></a><span class="s1">at::Tensor &amp; log_softmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l660"><span class="ln">660  </span></a><span class="s1">at::Tensor log_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l661"><span class="ln">661  </span></a><span class="s1">at::Tensor _log_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_log_softmax(Tensor self, int dim, bool half_to_float) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l662"><span class="ln">662  </span></a><span class="s1">at::Tensor &amp; _log_softmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l663"><span class="ln">663  </span></a><span class="s1">at::Tensor _log_softmax_backward_data(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, at::ScalarType input_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l664"><span class="ln">664  </span></a><span class="s1">at::Tensor &amp; _log_softmax_backward_data_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, at::ScalarType input_dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l665"><span class="ln">665  </span></a><span class="s1">at::Tensor _logcumsumexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_logcumsumexp(Tensor self, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l666"><span class="ln">666  </span></a><span class="s1">at::Tensor &amp; _logcumsumexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l667"><span class="ln">667  </span></a><span class="s1">at::Tensor logcumsumexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logcumsumexp(Tensor self, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l668"><span class="ln">668  </span></a><span class="s1">at::Tensor &amp; logcumsumexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l669"><span class="ln">669  </span></a><span class="s1">at::Tensor logcumsumexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logcumsumexp.dimname(Tensor self, Dimname dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l670"><span class="ln">670  </span></a><span class="s1">at::Tensor &amp; logcumsumexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l671"><span class="ln">671  </span></a><span class="s1">at::Tensor logsumexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l672"><span class="ln">672  </span></a><span class="s1">at::Tensor &amp; logsumexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l673"><span class="ln">673  </span></a><span class="s1">at::Tensor logsumexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l674"><span class="ln">674  </span></a><span class="s1">at::Tensor &amp; logsumexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l675"><span class="ln">675  </span></a><span class="s1">at::Tensor margin_ranking_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">double </span><span class="s1">margin, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l676"><span class="ln">676  </span></a><span class="s1">at::Tensor matmul(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matmul(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l677"><span class="ln">677  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; matmul_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matmul_backward(Tensor grad, Tensor self, Tensor other, bool[2] mask) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l678"><span class="ln">678  </span></a><span class="s1">at::Tensor &amp; matmul_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l679"><span class="ln">679  </span></a><span class="s1">at::Tensor matrix_power(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matrix_power(Tensor self, int n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l680"><span class="ln">680  </span></a><span class="s1">at::Tensor &amp; matrix_power_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l681"><span class="ln">681  </span></a><span class="s1">at::Tensor matrix_exp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matrix_exp(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l682"><span class="ln">682  </span></a><span class="s1">at::Tensor matrix_exp_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matrix_exp_backward(Tensor self, Tensor grad) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l683"><span class="ln">683  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _aminmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_aminmax(Tensor self) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l684"><span class="ln">684  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _aminmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_aminmax.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l685"><span class="ln">685  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; aminmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -&gt; (Tensor min, Tensor max)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l686"><span class="ln">686  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; aminmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; min, at::Tensor &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -&gt; (Tensor(a!) min, Tensor(b!) max)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l687"><span class="ln">687  </span></a><span class="s1">at::Tensor _compute_linear_combination(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; coefficients); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_compute_linear_combination(Tensor input, Tensor coefficients) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l688"><span class="ln">688  </span></a><span class="s1">at::Tensor &amp; _compute_linear_combination_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; coefficients, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l689"><span class="ln">689  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; max(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l690"><span class="ln">690  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; max_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; max, at::Tensor &amp; max_values); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l691"><span class="ln">691  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; max(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l692"><span class="ln">692  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; max_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; max, at::Tensor &amp; max_values); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l693"><span class="ln">693  </span></a><span class="s1">at::Tensor value_selecting_reduction_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymIntArrayRef sizes, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l694"><span class="ln">694  </span></a><span class="s1">at::Tensor amax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l695"><span class="ln">695  </span></a><span class="s1">at::Tensor &amp; amax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l696"><span class="ln">696  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; max_pool1d_with_indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l697"><span class="ln">697  </span></a><span class="s1">at::Tensor max_pool1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l698"><span class="ln">698  </span></a><span class="s1">at::Tensor max_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l699"><span class="ln">699  </span></a><span class="s1">at::Tensor max_pool2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l700"><span class="ln">700  </span></a><span class="s1">at::Tensor mkldnn_max_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l701"><span class="ln">701  </span></a><span class="s1">at::Tensor mkldnn_max_pool2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l702"><span class="ln">702  </span></a><span class="s1">at::Tensor mkldnn_max_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l703"><span class="ln">703  </span></a><span class="s1">at::Tensor mkldnn_max_pool3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l704"><span class="ln">704  </span></a><span class="s1">at::Tensor quantized_max_pool1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l705"><span class="ln">705  </span></a><span class="s1">at::Tensor quantized_max_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l706"><span class="ln">706  </span></a><span class="s1">at::Tensor quantized_max_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l707"><span class="ln">707  </span></a><span class="s1">at::Tensor max_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l708"><span class="ln">708  </span></a><span class="s1">at::Tensor mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mean(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l709"><span class="ln">709  </span></a><span class="s1">at::Tensor &amp; mean_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mean.dtype_out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l710"><span class="ln">710  </span></a><span class="s1">at::Tensor mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l711"><span class="ln">711  </span></a><span class="s1">at::Tensor &amp; mean_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l712"><span class="ln">712  </span></a><span class="s1">at::Tensor mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l713"><span class="ln">713  </span></a><span class="s1">at::Tensor &amp; mean_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l714"><span class="ln">714  </span></a><span class="s1">at::Tensor nanmean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l715"><span class="ln">715  </span></a><span class="s1">at::Tensor &amp; nanmean_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l716"><span class="ln">716  </span></a><span class="s1">at::Tensor median(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::median(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l717"><span class="ln">717  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; median(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::median.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l718"><span class="ln">718  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; median_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l719"><span class="ln">719  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; median(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l720"><span class="ln">720  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; median_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l721"><span class="ln">721  </span></a><span class="s1">at::Tensor nanmedian(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmedian(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l722"><span class="ln">722  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; nanmedian(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l723"><span class="ln">723  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; nanmedian_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l724"><span class="ln">724  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; nanmedian(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l725"><span class="ln">725  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; nanmedian_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l726"><span class="ln">726  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; min(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l727"><span class="ln">727  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; min_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; min, at::Tensor &amp; min_indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l728"><span class="ln">728  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; min(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l729"><span class="ln">729  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; min_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; min, at::Tensor &amp; min_indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l730"><span class="ln">730  </span></a><span class="s1">at::Tensor amin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l731"><span class="ln">731  </span></a><span class="s1">at::Tensor &amp; amin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l732"><span class="ln">732  </span></a><span class="s1">at::Tensor _mps_convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mps_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l733"><span class="ln">733  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; mps_convolution_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mps_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l734"><span class="ln">734  </span></a><span class="s1">at::Tensor mkldnn_convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l735"><span class="ln">735  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; mkldnn_rnn_layer(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight3, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx_, </span><span class="s2">bool </span><span class="s1">reverse, at::IntArrayRef batch_sizes, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">has_biases, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">bool </span><span class="s1">train); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_rnn_layer(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l736"><span class="ln">736  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; mkldnn_rnn_layer_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight3, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight4, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx_tmp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hy_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cy_, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">bool </span><span class="s1">reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">has_biases, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, at::IntArrayRef batch_sizes, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; workspace); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l737"><span class="ln">737  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; miopen_batch_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">exponential_average_factor, </span><span class="s2">double </span><span class="s1">epsilon); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l738"><span class="ln">738  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; miopen_batch_norm_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_var, </span><span class="s2">double </span><span class="s1">epsilon); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l739"><span class="ln">739  </span></a><span class="s1">at::Tensor miopen_convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l740"><span class="ln">740  </span></a><span class="s1">at::Tensor miopen_convolution_transpose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l741"><span class="ln">741  </span></a><span class="s1">at::Tensor miopen_depthwise_convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l742"><span class="ln">742  </span></a><span class="s1">at::Tensor miopen_convolution_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l743"><span class="ln">743  </span></a><span class="s1">at::Tensor miopen_convolution_add_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; z, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; alpha, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l744"><span class="ln">744  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; miopen_rnn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, at::IntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l745"><span class="ln">745  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,::std::vector&lt;at::Tensor&gt;&gt; miopen_rnn_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight_buf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, at::IntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reserve, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">4</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -&gt; (Tensor, Tensor, Tensor, Tensor[])&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l746"><span class="ln">746  </span></a><span class="s1">at::Tensor mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mm(Tensor self, Tensor mat2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l747"><span class="ln">747  </span></a><span class="s1">at::Tensor &amp; mm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l748"><span class="ln">748  </span></a><span class="s1">at::Tensor mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::ScalarType out_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mm.dtype(Tensor self, Tensor mat2, ScalarType out_dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l749"><span class="ln">749  </span></a><span class="s1">at::Tensor &amp; mm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::ScalarType out_dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mm.dtype_out(Tensor self, Tensor mat2, ScalarType out_dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l750"><span class="ln">750  </span></a><span class="s1">at::Tensor _int_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_int_mm(Tensor self, Tensor mat2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l751"><span class="ln">751  </span></a><span class="s1">at::Tensor &amp; _int_mm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_int_mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l752"><span class="ln">752  </span></a><span class="s1">at::Tensor _convert_weight_to_int4pack(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t innerKTiles); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convert_weight_to_int4pack(Tensor self, int innerKTiles) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l753"><span class="ln">753  </span></a><span class="s1">at::Tensor _weight_int4pack_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, int64_t qGroupSize, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qScaleAndZeros); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_int4pack_mm(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l754"><span class="ln">754  </span></a><span class="s1">at::Tensor _weight_int4pack_mm_with_scales_and_zeros(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, int64_t qGroupSize, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qScale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qZeros); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_int4pack_mm_with_scales_and_zeros(Tensor self, Tensor mat2, int qGroupSize, Tensor qScale, Tensor qZeros) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l755"><span class="ln">755  </span></a><span class="s1">at::Tensor _convert_weight_to_int4pack_for_cpu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t innerKTiles); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convert_weight_to_int4pack_for_cpu(Tensor self, int innerKTiles) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l756"><span class="ln">756  </span></a><span class="s1">at::Tensor _weight_int4pack_mm_for_cpu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, int64_t qGroupSize, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qScaleAndZeros); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_int4pack_mm_for_cpu(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l757"><span class="ln">757  </span></a><span class="s1">at::Tensor _dyn_quant_pack_4bit_weight(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weights, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales_zeros, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, int64_t block_size, int64_t in_features, int64_t out_features); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_dyn_quant_pack_4bit_weight(Tensor weights, Tensor scales_zeros, Tensor? bias, int block_size, int in_features, int out_features) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l758"><span class="ln">758  </span></a><span class="s1">at::Tensor _dyn_quant_matmul_4bit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; inp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_weights, int64_t block_size, int64_t in_features, int64_t out_features); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_dyn_quant_matmul_4bit(Tensor inp, Tensor packed_weights, int block_size, int in_features, int out_features) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l759"><span class="ln">759  </span></a><span class="s1">at::Tensor _weight_int8pack_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_int8pack_mm(Tensor self, Tensor mat2, Tensor scales) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l760"><span class="ln">760  </span></a><span class="s1">at::Tensor _sparse_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; sparse, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dense); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_mm(Tensor sparse, Tensor dense) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l761"><span class="ln">761  </span></a><span class="s1">at::Tensor _sparse_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; sparse, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dense, c10::string_view reduce); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_mm.reduce(Tensor sparse, Tensor dense, str reduce) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l762"><span class="ln">762  </span></a><span class="s1">at::Tensor _sparse_sparse_matmul(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sparse_matmul(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l763"><span class="ln">763  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; mode(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mode(Tensor self, int dim=-1, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l764"><span class="ln">764  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; mode_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l765"><span class="ln">765  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; mode(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l766"><span class="ln">766  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; mode_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l767"><span class="ln">767  </span></a><span class="s1">at::Tensor mul(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mul.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l768"><span class="ln">768  </span></a><span class="s1">at::Tensor &amp; mul_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mul_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l769"><span class="ln">769  </span></a><span class="s1">at::Tensor &amp; mul_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l770"><span class="ln">770  </span></a><span class="s1">at::Tensor mul(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mul.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l771"><span class="ln">771  </span></a><span class="s1">at::Tensor &amp; mul_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mul_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l772"><span class="ln">772  </span></a><span class="s1">at::Tensor multiply(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multiply.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l773"><span class="ln">773  </span></a><span class="s1">at::Tensor &amp; multiply_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l774"><span class="ln">774  </span></a><span class="s1">at::Tensor &amp; multiply_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l775"><span class="ln">775  </span></a><span class="s1">at::Tensor multiply(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multiply.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l776"><span class="ln">776  </span></a><span class="s1">at::Tensor &amp; multiply_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l777"><span class="ln">777  </span></a><span class="s1">at::Tensor mv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mv(Tensor self, Tensor vec) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l778"><span class="ln">778  </span></a><span class="s1">at::Tensor &amp; mv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l779"><span class="ln">779  </span></a><span class="s1">at::Tensor &amp; mvlgamma_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l780"><span class="ln">780  </span></a><span class="s1">at::Tensor mvlgamma(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mvlgamma(Tensor self, int p) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l781"><span class="ln">781  </span></a><span class="s1">at::Tensor &amp; mvlgamma_(at::Tensor &amp; self, int64_t p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mvlgamma_(Tensor(a!) self, int p) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l782"><span class="ln">782  </span></a><span class="s1">at::Tensor narrow_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymInt start, c10::SymInt length); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l783"><span class="ln">783  </span></a><span class="s1">at::Tensor &amp; narrow_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymInt start, c10::SymInt length, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l784"><span class="ln">784  </span></a><span class="s1">at::Tensor narrow(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymInt start, c10::SymInt length); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l785"><span class="ln">785  </span></a><span class="s1">at::Tensor narrow(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; start, c10::SymInt length); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l786"><span class="ln">786  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; native_batch_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l787"><span class="ln">787  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; native_batch_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out, at::Tensor &amp; save_mean, at::Tensor &amp; save_invstd); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l788"><span class="ln">788  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _native_batch_norm_legit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, at::Tensor &amp; running_mean, at::Tensor &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l789"><span class="ln">789  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _native_batch_norm_legit_no_training(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l790"><span class="ln">790  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _native_batch_norm_legit_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, at::Tensor &amp; running_mean, at::Tensor &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out, at::Tensor &amp; save_mean, at::Tensor &amp; save_invstd); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_batch_norm_legit.out(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps, *, Tensor(d!) out, Tensor(e!) save_mean, Tensor(f!) save_invstd) -&gt; (Tensor(d!), Tensor(e!), Tensor(f!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l791"><span class="ln">791  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _native_batch_norm_legit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l792"><span class="ln">792  </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _native_batch_norm_legit_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out, at::Tensor &amp; save_mean, at::Tensor &amp; save_invstd); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_batch_norm_legit.no_stats_out(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l793"><span class="ln">793  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; batch_norm_stats(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_stats(Tensor input, float eps) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l794"><span class="ln">794  </span></a><span class="s1">at::Tensor batch_norm_elemt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l795"><span class="ln">795  </span></a><span class="s1">at::Tensor &amp; batch_norm_elemt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l796"><span class="ln">796  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; batch_norm_gather_stats(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, int64_t count); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l797"><span class="ln">797  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; batch_norm_gather_stats_with_counts(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; counts); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l798"><span class="ln">798  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; native_batch_norm_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_invstd, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">double </span><span class="s1">eps, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l799"><span class="ln">799  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; batch_norm_backward_reduce(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">input_g, </span><span class="s2">bool </span><span class="s1">weight_g, </span><span class="s2">bool </span><span class="s1">bias_g); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l800"><span class="ln">800  </span></a><span class="s1">at::Tensor batch_norm_backward_elemt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; sum_dy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; sum_dy_xmu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; count); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l801"><span class="ln">801  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; batch_norm_update_stats(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l802"><span class="ln">802  </span></a><span class="s2">bool </span><span class="s1">is_vulkan_available(); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_vulkan_available() -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l803"><span class="ln">803  </span></a><span class="s2">bool </span><span class="s1">_nnpack_available(); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nnpack_available() -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l804"><span class="ln">804  </span></a><span class="s1">at::Tensor _nnpack_spatial_convolution(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l805"><span class="ln">805  </span></a><span class="s1">at::Tensor ones(at::IntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l806"><span class="ln">806  </span></a><span class="s1">at::Tensor ones(c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l807"><span class="ln">807  </span></a><span class="s1">at::Tensor &amp; ones_out(c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ones.out(SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l808"><span class="ln">808  </span></a><span class="s1">at::Tensor ones_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l809"><span class="ln">809  </span></a><span class="s1">at::Tensor pairwise_distance(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l810"><span class="ln">810  </span></a><span class="s1">at::Tensor cdist(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;int64_t&gt; compute_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l811"><span class="ln">811  </span></a><span class="s1">at::Tensor _euclidean_dist(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_euclidean_dist(Tensor x1, Tensor x2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l812"><span class="ln">812  </span></a><span class="s1">at::Tensor _cdist_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;int64_t&gt; compute_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l813"><span class="ln">813  </span></a><span class="s1">at::Tensor _cdist_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cdist); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l814"><span class="ln">814  </span></a><span class="s1">at::Tensor pdist(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pdist(Tensor self, float p=2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l815"><span class="ln">815  </span></a><span class="s1">at::Tensor _pdist_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pdist_forward(Tensor self, float p=2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l816"><span class="ln">816  </span></a><span class="s1">at::Tensor _pdist_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; pdist); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l817"><span class="ln">817  </span></a><span class="s1">at::Tensor cosine_similarity(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, int64_t dim, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l818"><span class="ln">818  </span></a><span class="s1">at::Tensor permute(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::permute(Tensor(a) self, int[] dims) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l819"><span class="ln">819  </span></a><span class="s1">at::Tensor movedim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef source, at::IntArrayRef destination); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l820"><span class="ln">820  </span></a><span class="s1">at::Tensor movedim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t source, int64_t destination); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::movedim.int(Tensor(a) self, int source, int destination) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l821"><span class="ln">821  </span></a><span class="s1">at::Tensor moveaxis(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef source, at::IntArrayRef destination); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l822"><span class="ln">822  </span></a><span class="s1">at::Tensor moveaxis(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t source, int64_t destination); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::moveaxis.int(Tensor(a) self, int source, int destination) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l823"><span class="ln">823  </span></a><span class="s1">at::Tensor numpy_T(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::numpy_T(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l824"><span class="ln">824  </span></a><span class="s1">at::Tensor matrix_H(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matrix_H(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l825"><span class="ln">825  </span></a><span class="s1">at::Tensor mT(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mT(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l826"><span class="ln">826  </span></a><span class="s1">at::Tensor mH(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mH(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l827"><span class="ln">827  </span></a><span class="s1">at::Tensor adjoint(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adjoint(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l828"><span class="ln">828  </span></a><span class="s1">at::Tensor pixel_shuffle(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t upscale_factor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pixel_shuffle(Tensor self, int upscale_factor) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l829"><span class="ln">829  </span></a><span class="s1">at::Tensor pixel_unshuffle(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t downscale_factor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pixel_unshuffle(Tensor self, int downscale_factor) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l830"><span class="ln">830  </span></a><span class="s1">at::Tensor channel_shuffle(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::channel_shuffle(Tensor self, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l831"><span class="ln">831  </span></a><span class="s1">at::Tensor native_channel_shuffle(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt groups); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_channel_shuffle(Tensor self, SymInt groups) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l832"><span class="ln">832  </span></a><span class="s2">bool </span><span class="s1">is_pinned(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Device&gt; device); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_pinned(Tensor self, Device? device=None) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l833"><span class="ln">833  </span></a><span class="s1">at::Tensor pin_memory(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Device&gt; device); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pin_memory(Tensor(a) self, Device? device=None) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l834"><span class="ln">834  </span></a><span class="s1">at::Tensor _pin_memory(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Device&gt; device); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pin_memory(Tensor self, Device? device=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l835"><span class="ln">835  </span></a><span class="s1">at::Tensor pinverse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">rcond); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pinverse(Tensor self, float rcond=1e-15) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l836"><span class="ln">836  </span></a><span class="s1">at::Tensor poisson_nll_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">bool </span><span class="s1">log_input, </span><span class="s2">bool </span><span class="s1">full, </span><span class="s2">double </span><span class="s1">eps, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l837"><span class="ln">837  </span></a><span class="s1">at::Tensor rad2deg(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rad2deg(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l838"><span class="ln">838  </span></a><span class="s1">at::Tensor &amp; rad2deg_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rad2deg_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l839"><span class="ln">839  </span></a><span class="s1">at::Tensor &amp; rad2deg_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l840"><span class="ln">840  </span></a><span class="s1">at::Tensor deg2rad(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::deg2rad(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l841"><span class="ln">841  </span></a><span class="s1">at::Tensor &amp; deg2rad_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::deg2rad_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l842"><span class="ln">842  </span></a><span class="s1">at::Tensor &amp; deg2rad_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l843"><span class="ln">843  </span></a><span class="s1">at::Tensor scalar_tensor(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; s, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l844"><span class="ln">844  </span></a><span class="s1">at::Tensor rand(c10::SymIntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l845"><span class="ln">845  </span></a><span class="s1">at::Tensor rand(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l846"><span class="ln">846  </span></a><span class="s1">at::Tensor rand(c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l847"><span class="ln">847  </span></a><span class="s1">at::Tensor rand(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l848"><span class="ln">848  </span></a><span class="s1">at::Tensor &amp; rand_out(c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand.out(SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l849"><span class="ln">849  </span></a><span class="s1">at::Tensor &amp; rand_out(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l850"><span class="ln">850  </span></a><span class="s1">at::Tensor rand_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l851"><span class="ln">851  </span></a><span class="s1">at::Tensor randint(c10::SymInt high, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint(SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l852"><span class="ln">852  </span></a><span class="s1">at::Tensor randint(c10::SymInt high, c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l853"><span class="ln">853  </span></a><span class="s1">at::Tensor randint(c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l854"><span class="ln">854  </span></a><span class="s1">at::Tensor randint(c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l855"><span class="ln">855  </span></a><span class="s1">at::Tensor &amp; randint_out(c10::SymInt high, c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint.out(SymInt high, SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l856"><span class="ln">856  </span></a><span class="s1">at::Tensor &amp; randint_out(c10::SymInt high, c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint.generator_out(SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l857"><span class="ln">857  </span></a><span class="s1">at::Tensor &amp; randint_out(c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint.low_out(SymInt low, SymInt high, SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l858"><span class="ln">858  </span></a><span class="s1">at::Tensor &amp; randint_out(c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint.low_generator_out(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l859"><span class="ln">859  </span></a><span class="s1">at::Tensor randint_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt high, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l860"><span class="ln">860  </span></a><span class="s1">at::Tensor randint_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; high, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint_like.Tensor(Tensor self, Tensor high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l861"><span class="ln">861  </span></a><span class="s1">at::Tensor randint_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt low, c10::SymInt high, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l862"><span class="ln">862  </span></a><span class="s1">at::Tensor randn(c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l863"><span class="ln">863  </span></a><span class="s1">at::Tensor randn(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l864"><span class="ln">864  </span></a><span class="s1">at::Tensor randn(c10::SymIntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l865"><span class="ln">865  </span></a><span class="s1">at::Tensor randn(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l866"><span class="ln">866  </span></a><span class="s1">at::Tensor &amp; randn_out(c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn.out(SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l867"><span class="ln">867  </span></a><span class="s1">at::Tensor &amp; randn_out(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l868"><span class="ln">868  </span></a><span class="s1">at::Tensor randn_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l869"><span class="ln">869  </span></a><span class="s1">at::Tensor randperm(c10::SymInt n, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l870"><span class="ln">870  </span></a><span class="s1">at::Tensor randperm(c10::SymInt n, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l871"><span class="ln">871  </span></a><span class="s1">at::Tensor &amp; randperm_out(c10::SymInt n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randperm.out(SymInt n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l872"><span class="ln">872  </span></a><span class="s1">at::Tensor &amp; randperm_out(c10::SymInt n, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l873"><span class="ln">873  </span></a><span class="s1">at::Tensor range(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; step, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l874"><span class="ln">874  </span></a><span class="s1">at::Tensor range(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l875"><span class="ln">875  </span></a><span class="s1">at::Tensor &amp; range_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::range.out_(Scalar start, Scalar end, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l876"><span class="ln">876  </span></a><span class="s1">at::Tensor &amp; range_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; start, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; end, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; step, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l877"><span class="ln">877  </span></a><span class="s1">at::Tensor ravel(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ravel(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l878"><span class="ln">878  </span></a><span class="s1">at::Tensor reciprocal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reciprocal(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l879"><span class="ln">879  </span></a><span class="s1">at::Tensor &amp; reciprocal_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reciprocal_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l880"><span class="ln">880  </span></a><span class="s1">at::Tensor &amp; reciprocal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l881"><span class="ln">881  </span></a><span class="s1">at::Tensor neg(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::neg(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l882"><span class="ln">882  </span></a><span class="s1">at::Tensor &amp; neg_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::neg_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l883"><span class="ln">883  </span></a><span class="s1">at::Tensor &amp; neg_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::neg.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l884"><span class="ln">884  </span></a><span class="s1">at::Tensor negative(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::negative(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l885"><span class="ln">885  </span></a><span class="s1">at::Tensor &amp; negative_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::negative_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l886"><span class="ln">886  </span></a><span class="s1">at::Tensor &amp; negative_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::negative.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l887"><span class="ln">887  </span></a><span class="s1">at::Tensor repeat(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef repeats); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::repeat(Tensor self, SymInt[] repeats) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l888"><span class="ln">888  </span></a><span class="s1">at::Tensor repeat_interleave(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; repeats, ::std::optional&lt;c10::SymInt&gt; output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::repeat_interleave.Tensor(Tensor repeats, *, SymInt? output_size=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l889"><span class="ln">889  </span></a><span class="s1">at::Tensor repeat_interleave(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; repeats, ::std::optional&lt;int64_t&gt; dim, ::std::optional&lt;c10::SymInt&gt; output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l890"><span class="ln">890  </span></a><span class="s1">at::Tensor repeat_interleave(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt repeats, ::std::optional&lt;int64_t&gt; dim, ::std::optional&lt;c10::SymInt&gt; output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l891"><span class="ln">891  </span></a><span class="s1">at::Tensor reshape(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef shape); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reshape(Tensor(a) self, SymInt[] shape) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l892"><span class="ln">892  </span></a><span class="s1">at::Tensor _reshape_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_reshape_copy(Tensor self, SymInt[] size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l893"><span class="ln">893  </span></a><span class="s1">at::Tensor _reshape_alias(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l894"><span class="ln">894  </span></a><span class="s1">at::Tensor _mkldnn_reshape(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef shape); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mkldnn_reshape(Tensor self, int[] shape) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l895"><span class="ln">895  </span></a><span class="s1">at::Tensor reshape_as(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reshape_as(Tensor(a) self, Tensor other) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l896"><span class="ln">896  </span></a><span class="s1">at::Tensor round(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::round(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l897"><span class="ln">897  </span></a><span class="s1">at::Tensor &amp; round_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::round_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l898"><span class="ln">898  </span></a><span class="s1">at::Tensor &amp; round_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::round.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l899"><span class="ln">899  </span></a><span class="s1">at::Tensor round(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t decimals); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::round.decimals(Tensor self, *, int decimals) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l900"><span class="ln">900  </span></a><span class="s1">at::Tensor &amp; round_(at::Tensor &amp; self, int64_t decimals); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::round_.decimals(Tensor(a!) self, *, int decimals) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l901"><span class="ln">901  </span></a><span class="s1">at::Tensor &amp; round_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t decimals, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l902"><span class="ln">902  </span></a><span class="s1">at::Tensor rrelu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l903"><span class="ln">903  </span></a><span class="s1">at::Tensor &amp; rrelu_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l904"><span class="ln">904  </span></a><span class="s1">at::Tensor relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::relu(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l905"><span class="ln">905  </span></a><span class="s1">at::Tensor &amp; relu_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::relu_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l906"><span class="ln">906  </span></a><span class="s1">at::Tensor relu6(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::relu6(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l907"><span class="ln">907  </span></a><span class="s1">at::Tensor &amp; relu6_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::relu6_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l908"><span class="ln">908  </span></a><span class="s1">at::Tensor prelu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::prelu(Tensor self, Tensor weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l909"><span class="ln">909  </span></a><span class="s1">at::Tensor _prelu_kernel(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_prelu_kernel(Tensor self, Tensor weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l910"><span class="ln">910  </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _prelu_kernel_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_prelu_kernel_backward(Tensor grad_output, Tensor self, Tensor weight) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l911"><span class="ln">911  </span></a><span class="s1">at::Tensor &amp; gelu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view approximate, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l912"><span class="ln">912  </span></a><span class="s1">at::Tensor &amp; gelu_(at::Tensor &amp; self, c10::string_view approximate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gelu_(Tensor(a!) self, *, str approximate='none') -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l913"><span class="ln">913  </span></a><span class="s1">at::Tensor gelu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view approximate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gelu(Tensor self, *, str approximate='none') -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l914"><span class="ln">914  </span></a><span class="s1">at::Tensor &amp; gelu_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view approximate, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l915"><span class="ln">915  </span></a><span class="s1">at::Tensor gelu_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view approximate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l916"><span class="ln">916  </span></a><span class="s1">at::Tensor infinitely_differentiable_gelu_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l917"><span class="ln">917  </span></a><span class="s1">at::Tensor &amp; hardshrink_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l918"><span class="ln">918  </span></a><span class="s1">at::Tensor hardshrink(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardshrink(Tensor self, Scalar lambd=0.5) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l919"><span class="ln">919  </span></a><span class="s1">at::Tensor &amp; hardshrink_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l920"><span class="ln">920  </span></a><span class="s1">at::Tensor hardshrink_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l921"><span class="ln">921  </span></a><span class="s1">at::Tensor rsqrt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rsqrt(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l922"><span class="ln">922  </span></a><span class="s1">at::Tensor &amp; rsqrt_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rsqrt_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l923"><span class="ln">923  </span></a><span class="s1">at::Tensor &amp; rsqrt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l924"><span class="ln">924  </span></a><span class="s1">at::Tensor select(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, int64_t index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l925"><span class="ln">925  </span></a><span class="s1">at::Tensor select(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymInt index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select.int(Tensor(a) self, int dim, SymInt index) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l926"><span class="ln">926  </span></a><span class="s1">at::Tensor select_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l927"><span class="ln">927  </span></a><span class="s1">at::Tensor _nested_select_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymInt index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_select_backward(Tensor grad_output, Tensor self, int dim, SymInt index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l928"><span class="ln">928  </span></a><span class="s1">at::Tensor selu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::selu(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l929"><span class="ln">929  </span></a><span class="s1">at::Tensor &amp; selu_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::selu_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l930"><span class="ln">930  </span></a><span class="s1">at::Tensor celu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::celu(Tensor self, Scalar alpha=1.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l931"><span class="ln">931  </span></a><span class="s1">at::Tensor &amp; celu_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l932"><span class="ln">932  </span></a><span class="s1">at::Tensor silu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::silu(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l933"><span class="ln">933  </span></a><span class="s1">at::Tensor &amp; silu_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::silu_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l934"><span class="ln">934  </span></a><span class="s1">at::Tensor &amp; silu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::silu.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l935"><span class="ln">935  </span></a><span class="s1">at::Tensor &amp; silu_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l936"><span class="ln">936  </span></a><span class="s1">at::Tensor silu_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::silu_backward(Tensor grad_output, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l937"><span class="ln">937  </span></a><span class="s1">at::Tensor mish(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mish(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l938"><span class="ln">938  </span></a><span class="s1">at::Tensor &amp; mish_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mish_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l939"><span class="ln">939  </span></a><span class="s1">at::Tensor &amp; mish_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mish.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l940"><span class="ln">940  </span></a><span class="s1">at::Tensor mish_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mish_backward(Tensor grad_output, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l941"><span class="ln">941  </span></a><span class="s1">at::Tensor sigmoid(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sigmoid(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l942"><span class="ln">942  </span></a><span class="s1">at::Tensor &amp; sigmoid_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sigmoid_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l943"><span class="ln">943  </span></a><span class="s1">at::Tensor &amp; sigmoid_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l944"><span class="ln">944  </span></a><span class="s1">at::Tensor logit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logit(Tensor self, float? eps=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l945"><span class="ln">945  </span></a><span class="s1">at::Tensor &amp; logit_(at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logit_(Tensor(a!) self, float? eps=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l946"><span class="ln">946  </span></a><span class="s1">at::Tensor &amp; logit_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l947"><span class="ln">947  </span></a><span class="s1">at::Tensor sin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sin(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l948"><span class="ln">948  </span></a><span class="s1">at::Tensor &amp; sin_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sin_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l949"><span class="ln">949  </span></a><span class="s1">at::Tensor &amp; sin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sin.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l950"><span class="ln">950  </span></a><span class="s1">at::Tensor sinc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sinc(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l951"><span class="ln">951  </span></a><span class="s1">at::Tensor &amp; sinc_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sinc_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l952"><span class="ln">952  </span></a><span class="s1">at::Tensor &amp; sinc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sinc.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l953"><span class="ln">953  </span></a><span class="s1">at::Tensor sinh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sinh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l954"><span class="ln">954  </span></a><span class="s1">at::Tensor &amp; sinh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sinh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l955"><span class="ln">955  </span></a><span class="s1">at::Tensor &amp; sinh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sinh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l956"><span class="ln">956  </span></a><span class="s1">at::Tensor detach(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::detach(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l957"><span class="ln">957  </span></a><span class="s1">at::Tensor &amp; detach_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::detach_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l958"><span class="ln">958  </span></a><span class="s1">int64_t size(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::size.int(Tensor self, int dim) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l959"><span class="ln">959  </span></a><span class="s1">int64_t size(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::size.Dimname(Tensor self, Dimname dim) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l960"><span class="ln">960  </span></a><span class="s1">c10::SymInt sym_size(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sym_size.int(Tensor self, int dim) -&gt; SymInt&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l961"><span class="ln">961  </span></a><span class="s1">c10::SymInt sym_numel(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sym_numel(Tensor self) -&gt; SymInt&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l962"><span class="ln">962  </span></a><span class="s1">c10::SymInt sym_storage_offset(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sym_storage_offset(Tensor self) -&gt; SymInt&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l963"><span class="ln">963  </span></a><span class="s1">at::Tensor slice(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;c10::SymInt&gt; start, ::std::optional&lt;c10::SymInt&gt; end, c10::SymInt step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l964"><span class="ln">964  </span></a><span class="s1">at::Tensor slice_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l965"><span class="ln">965  </span></a><span class="s1">at::Tensor slice_inverse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t dim, ::std::optional&lt;c10::SymInt&gt; start, ::std::optional&lt;c10::SymInt&gt; end, c10::SymInt step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice_inverse(Tensor(a) self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l966"><span class="ln">966  </span></a><span class="s1">at::Tensor slice_scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t dim, ::std::optional&lt;c10::SymInt&gt; start, ::std::optional&lt;c10::SymInt&gt; end, c10::SymInt step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l967"><span class="ln">967  </span></a><span class="s1">at::Tensor select_scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t dim, c10::SymInt index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l968"><span class="ln">968  </span></a><span class="s1">at::Tensor diagonal_scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t offset, int64_t dim1, int64_t dim2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l969"><span class="ln">969  </span></a><span class="s1">at::Tensor as_strided_scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;c10::SymInt&gt; storage_offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l970"><span class="ln">970  </span></a><span class="s1">at::Tensor smm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::smm(Tensor self, Tensor mat2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l971"><span class="ln">971  </span></a><span class="s1">at::Tensor softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l972"><span class="ln">972  </span></a><span class="s1">at::Tensor &amp; softmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l973"><span class="ln">973  </span></a><span class="s1">at::Tensor softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l974"><span class="ln">974  </span></a><span class="s1">at::Tensor _softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_softmax(Tensor self, int dim, bool half_to_float) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l975"><span class="ln">975  </span></a><span class="s1">at::Tensor &amp; _softmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l976"><span class="ln">976  </span></a><span class="s1">at::Tensor _softmax_backward_data(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, at::ScalarType input_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l977"><span class="ln">977  </span></a><span class="s1">at::Tensor &amp; _softmax_backward_data_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, at::ScalarType input_dtype, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l978"><span class="ln">978  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; unsafe_split(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt split_size, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l979"><span class="ln">979  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; split(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt split_size, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::split.Tensor(Tensor(a -&gt; *) self, SymInt split_size, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l980"><span class="ln">980  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; split(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef split_size, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::split.sizes(Tensor(a -&gt; *) self, SymInt[] split_size, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l981"><span class="ln">981  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; unsafe_split_with_sizes(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef split_sizes, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l982"><span class="ln">982  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; split_with_sizes(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef split_sizes, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::split_with_sizes(Tensor(a -&gt; *) self, SymInt[] split_sizes, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l983"><span class="ln">983  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; hsplit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t sections); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hsplit.int(Tensor(a -&gt; *) self, int sections) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l984"><span class="ln">984  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; hsplit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hsplit.array(Tensor(a -&gt; *) self, int[] indices) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l985"><span class="ln">985  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; vsplit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t sections); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::vsplit.int(Tensor(a -&gt; *) self, int sections) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l986"><span class="ln">986  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; vsplit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::vsplit.array(Tensor(a -&gt; *) self, int[] indices) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l987"><span class="ln">987  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; dsplit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t sections); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dsplit.int(Tensor(a -&gt; *) self, int sections) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l988"><span class="ln">988  </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; dsplit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dsplit.array(Tensor(a -&gt; *) self, int[] indices) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l989"><span class="ln">989  </span></a><span class="s1">at::Tensor squeeze(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l990"><span class="ln">990  </span></a><span class="s1">at::Tensor squeeze(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze.dim(Tensor(a) self, int dim) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l991"><span class="ln">991  </span></a><span class="s1">at::Tensor squeeze(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze.dimname(Tensor(a) self, Dimname dim) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l992"><span class="ln">992  </span></a><span class="s1">at::Tensor squeeze(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze.dims(Tensor(a) self, int[] dim) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l993"><span class="ln">993  </span></a><span class="s1">at::Tensor &amp; squeeze_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l994"><span class="ln">994  </span></a><span class="s1">at::Tensor &amp; squeeze_(at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_.dim(Tensor(a!) self, int dim) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l995"><span class="ln">995  </span></a><span class="s1">at::Tensor &amp; squeeze_(at::Tensor &amp; self, at::IntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_.dims(Tensor(a!) self, int[] dim) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l996"><span class="ln">996  </span></a><span class="s1">at::Tensor &amp; squeeze_(at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l997"><span class="ln">997  </span></a><span class="s1">at::Tensor sspaddmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l998"><span class="ln">998  </span></a><span class="s1">at::Tensor &amp; sspaddmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l999"><span class="ln">999  </span></a><span class="s1">at::Tensor _chunk_cat(at::TensorList tensors, int64_t dim, int64_t num_chunks); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_chunk_cat(Tensor[] tensors, int dim, int num_chunks) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1000"><span class="ln">1000 </span></a><span class="s1">at::Tensor &amp; _chunk_cat_out(at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1001"><span class="ln">1001 </span></a><span class="s1">at::Tensor stack(at::TensorList tensors, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::stack(Tensor[] tensors, int dim=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1002"><span class="ln">1002 </span></a><span class="s1">at::Tensor &amp; stack_out(at::TensorList tensors, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1003"><span class="ln">1003 </span></a><span class="s1">at::Tensor _stack(at::TensorList tensors, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_stack(Tensor[] tensors, int dim=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1004"><span class="ln">1004 </span></a><span class="s1">at::Tensor &amp; _stack_out(at::TensorList tensors, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1005"><span class="ln">1005 </span></a><span class="s1">at::Tensor hstack(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hstack(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1006"><span class="ln">1006 </span></a><span class="s1">at::Tensor &amp; hstack_out(at::TensorList tensors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1007"><span class="ln">1007 </span></a><span class="s1">at::Tensor vstack(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::vstack(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1008"><span class="ln">1008 </span></a><span class="s1">at::Tensor &amp; vstack_out(at::TensorList tensors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1009"><span class="ln">1009 </span></a><span class="s1">at::Tensor dstack(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dstack(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1010"><span class="ln">1010 </span></a><span class="s1">at::Tensor &amp; dstack_out(at::TensorList tensors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1011"><span class="ln">1011 </span></a><span class="s1">at::Tensor stft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n_fft, ::std::optional&lt;int64_t&gt; hop_length, ::std::optional&lt;int64_t&gt; win_length, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; window, </span><span class="s2">bool </span><span class="s1">normalized, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; onesided, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; return_complex, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; align_to_window); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1012"><span class="ln">1012 </span></a><span class="s1">at::Tensor stft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n_fft, ::std::optional&lt;int64_t&gt; hop_length, ::std::optional&lt;int64_t&gt; win_length, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; window, </span><span class="s2">bool </span><span class="s1">center, c10::string_view pad_mode, </span><span class="s2">bool </span><span class="s1">normalized, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; onesided, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; return_complex, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; align_to_window); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode=\&quot;reflect\&quot;, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1013"><span class="ln">1013 </span></a><span class="s1">at::Tensor istft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n_fft, ::std::optional&lt;int64_t&gt; hop_length, ::std::optional&lt;int64_t&gt; win_length, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; window, </span><span class="s2">bool </span><span class="s1">center, </span><span class="s2">bool </span><span class="s1">normalized, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; onesided, ::std::optional&lt;int64_t&gt; length, </span><span class="s2">bool </span><span class="s1">return_complex); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1014"><span class="ln">1014 </span></a><span class="s1">int64_t stride(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::stride.int(Tensor self, int dim) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1015"><span class="ln">1015 </span></a><span class="s1">int64_t stride(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::stride.Dimname(Tensor self, Dimname dim) -&gt; int&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1016"><span class="ln">1016 </span></a><span class="s1">c10::SymInt sym_stride(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sym_stride.int(Tensor self, int dim) -&gt; SymInt&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1017"><span class="ln">1017 </span></a><span class="s1">at::Tensor sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sum(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1018"><span class="ln">1018 </span></a><span class="s1">at::Tensor sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1019"><span class="ln">1019 </span></a><span class="s1">at::Tensor sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1020"><span class="ln">1020 </span></a><span class="s1">at::Tensor &amp; sum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1021"><span class="ln">1021 </span></a><span class="s1">at::Tensor &amp; sum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1022"><span class="ln">1022 </span></a><span class="s1">at::Tensor _nested_sum_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_sum_backward(Tensor grad, Tensor self, int[1]? dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1023"><span class="ln">1023 </span></a><span class="s1">at::Tensor nansum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1024"><span class="ln">1024 </span></a><span class="s1">at::Tensor &amp; nansum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nansum.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1025"><span class="ln">1025 </span></a><span class="s1">at::Tensor sum_to_size(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sum_to_size(Tensor self, SymInt[] size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1026"><span class="ln">1026 </span></a><span class="s1">at::Tensor sqrt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sqrt(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1027"><span class="ln">1027 </span></a><span class="s1">at::Tensor &amp; sqrt_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sqrt_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1028"><span class="ln">1028 </span></a><span class="s1">at::Tensor &amp; sqrt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sqrt.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1029"><span class="ln">1029 </span></a><span class="s1">at::Tensor square(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::square(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1030"><span class="ln">1030 </span></a><span class="s1">at::Tensor &amp; square_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::square_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1031"><span class="ln">1031 </span></a><span class="s1">at::Tensor &amp; square_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::square.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1032"><span class="ln">1032 </span></a><span class="s1">at::Tensor std(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">unbiased); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std(Tensor self, bool unbiased=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1033"><span class="ln">1033 </span></a><span class="s1">at::Tensor std(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1034"><span class="ln">1034 </span></a><span class="s1">at::Tensor std(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1035"><span class="ln">1035 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; std_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">unbiased); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std_mean(Tensor self, bool unbiased=True) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1036"><span class="ln">1036 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; std_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1037"><span class="ln">1037 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; std_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1038"><span class="ln">1038 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; std_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1039"><span class="ln">1039 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; std_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1040"><span class="ln">1040 </span></a><span class="s1">at::Tensor &amp; std_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1041"><span class="ln">1041 </span></a><span class="s1">at::Tensor &amp; std_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1042"><span class="ln">1042 </span></a><span class="s1">at::Tensor std(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1043"><span class="ln">1043 </span></a><span class="s1">at::Tensor &amp; std_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1044"><span class="ln">1044 </span></a><span class="s1">at::Tensor std(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1045"><span class="ln">1045 </span></a><span class="s1">at::Tensor &amp; std_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1046"><span class="ln">1046 </span></a><span class="s1">at::Tensor prod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::prod(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1047"><span class="ln">1047 </span></a><span class="s1">at::Tensor prod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1048"><span class="ln">1048 </span></a><span class="s1">at::Tensor &amp; prod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1049"><span class="ln">1049 </span></a><span class="s1">at::Tensor prod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1050"><span class="ln">1050 </span></a><span class="s1">at::Tensor &amp; prod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1051"><span class="ln">1051 </span></a><span class="s1">at::Tensor t(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::t(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1052"><span class="ln">1052 </span></a><span class="s1">at::Tensor &amp; t_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::t_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1053"><span class="ln">1053 </span></a><span class="s1">at::Tensor tan(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tan(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1054"><span class="ln">1054 </span></a><span class="s1">at::Tensor &amp; tan_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tan_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1055"><span class="ln">1055 </span></a><span class="s1">at::Tensor &amp; tan_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tan.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1056"><span class="ln">1056 </span></a><span class="s1">at::Tensor tanh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tanh(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1057"><span class="ln">1057 </span></a><span class="s1">at::Tensor &amp; tanh_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tanh_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1058"><span class="ln">1058 </span></a><span class="s1">at::Tensor &amp; tanh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tanh.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1059"><span class="ln">1059 </span></a><span class="s1">at::Tensor tensordot(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::IntArrayRef dims_self, at::IntArrayRef dims_other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1060"><span class="ln">1060 </span></a><span class="s1">at::Tensor &amp; tensordot_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1061"><span class="ln">1061 </span></a><span class="s1">at::Tensor threshold(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::threshold(Tensor self, Scalar threshold, Scalar value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1062"><span class="ln">1062 </span></a><span class="s1">at::Tensor &amp; threshold_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1063"><span class="ln">1063 </span></a><span class="s1">at::Tensor &amp; threshold_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1064"><span class="ln">1064 </span></a><span class="s1">at::Tensor &amp; threshold_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1065"><span class="ln">1065 </span></a><span class="s1">at::Tensor threshold_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1066"><span class="ln">1066 </span></a><span class="s1">at::Tensor tile(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tile(Tensor self, SymInt[] dims) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1067"><span class="ln">1067 </span></a><span class="s1">at::Tensor transpose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim0, int64_t dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::transpose.int(Tensor(a) self, int dim0, int dim1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1068"><span class="ln">1068 </span></a><span class="s1">at::Tensor transpose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim0, at::Dimname dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1069"><span class="ln">1069 </span></a><span class="s1">at::Tensor _mkldnn_transpose(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim0, int64_t dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1070"><span class="ln">1070 </span></a><span class="s1">at::Tensor &amp; transpose_(at::Tensor &amp; self, int64_t dim0, int64_t dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::transpose_(Tensor(a!) self, int dim0, int dim1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1071"><span class="ln">1071 </span></a><span class="s1">at::Tensor &amp; _mkldnn_transpose_(at::Tensor &amp; self, int64_t dim0, int64_t dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1072"><span class="ln">1072 </span></a><span class="s1">at::Tensor one_hot(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t num_classes); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::one_hot(Tensor self, int num_classes=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1073"><span class="ln">1073 </span></a><span class="s1">at::Tensor flip(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flip(Tensor self, int[] dims) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1074"><span class="ln">1074 </span></a><span class="s1">at::Tensor fliplr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fliplr(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1075"><span class="ln">1075 </span></a><span class="s1">at::Tensor flipud(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flipud(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1076"><span class="ln">1076 </span></a><span class="s1">at::Tensor roll(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef shifts, at::IntArrayRef dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1077"><span class="ln">1077 </span></a><span class="s1">at::Tensor rot90(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t k, at::IntArrayRef dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1078"><span class="ln">1078 </span></a><span class="s1">at::Tensor trapezoid(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1079"><span class="ln">1079 </span></a><span class="s1">at::Tensor trapezoid(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; dx, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1080"><span class="ln">1080 </span></a><span class="s1">at::Tensor trapz(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1081"><span class="ln">1081 </span></a><span class="s1">at::Tensor trapz(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, </span><span class="s2">double </span><span class="s1">dx, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1082"><span class="ln">1082 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _transform_bias_rescale_qkv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, int64_t num_heads); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_transform_bias_rescale_qkv(Tensor qkv, Tensor qkv_bias, int num_heads) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1083"><span class="ln">1083 </span></a><span class="s1">at::Tensor _nested_tensor_from_mask(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; t, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">bool </span><span class="s1">mask_check); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_from_mask(Tensor t, Tensor mask, bool mask_check=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1084"><span class="ln">1084 </span></a><span class="s2">bool </span><span class="s1">_nested_tensor_from_mask_left_aligned(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; t, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_from_mask_left_aligned(Tensor t, Tensor mask) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1085"><span class="ln">1085 </span></a><span class="s1">at::Tensor _nested_from_padded(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; padded, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cpu_nested_shape_example, </span><span class="s2">bool </span><span class="s1">fuse_transform_0213); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_from_padded(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1086"><span class="ln">1086 </span></a><span class="s1">at::Tensor _nested_tensor_size(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_size(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1087"><span class="ln">1087 </span></a><span class="s1">at::Tensor _nested_tensor_strides(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_strides(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1088"><span class="ln">1088 </span></a><span class="s1">at::Tensor _nested_tensor_storage_offsets(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_storage_offsets(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1089"><span class="ln">1089 </span></a><span class="s1">at::Tensor _nested_from_padded_and_nested_example(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; padded, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nt_example); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_from_padded_and_nested_example(Tensor padded, Tensor nt_example) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1090"><span class="ln">1090 </span></a><span class="s1">at::Tensor _nested_view_from_buffer(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nested_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nested_strides, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_view_from_buffer(Tensor(a) self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1091"><span class="ln">1091 </span></a><span class="s1">at::Tensor _nested_view_from_buffer_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nested_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nested_strides, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_view_from_buffer_copy(Tensor self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1092"><span class="ln">1092 </span></a><span class="s1">at::Tensor _nested_view_from_jagged(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dummy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; lengths, int64_t ragged_idx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min_seqlen, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max_seqlen); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_view_from_jagged(Tensor(a) self, Tensor offsets, Tensor dummy, Tensor? lengths=None, int ragged_idx=1, Tensor? min_seqlen=None, Tensor? max_seqlen=None) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1093"><span class="ln">1093 </span></a><span class="s1">at::Tensor _nested_view_from_jagged_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dummy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; lengths, int64_t ragged_idx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min_seqlen, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max_seqlen); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_view_from_jagged_copy(Tensor self, Tensor offsets, Tensor dummy, Tensor? lengths=None, int ragged_idx=1, Tensor? min_seqlen=None, Tensor? max_seqlen=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1094"><span class="ln">1094 </span></a><span class="s1">at::Tensor _nested_get_values(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_values(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1095"><span class="ln">1095 </span></a><span class="s1">at::Tensor _nested_get_values_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_values_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1096"><span class="ln">1096 </span></a><span class="s1">at::Tensor _nested_get_offsets(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_offsets(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1097"><span class="ln">1097 </span></a><span class="s1">at::Tensor _nested_get_lengths(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_lengths(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1098"><span class="ln">1098 </span></a><span class="s1">int64_t _nested_get_ragged_idx(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_ragged_idx(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1099"><span class="ln">1099 </span></a><span class="s1">at::Tensor _nested_get_min_seqlen(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_min_seqlen(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1100"><span class="ln">1100 </span></a><span class="s1">at::Tensor _nested_get_max_seqlen(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_max_seqlen(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1101"><span class="ln">1101 </span></a><span class="s1">at::Tensor _nested_get_jagged_dummy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; any); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_jagged_dummy(Tensor any) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1102"><span class="ln">1102 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _nested_compute_contiguous_strides_offsets(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; nested_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_compute_contiguous_strides_offsets(Tensor nested_size) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1103"><span class="ln">1103 </span></a><span class="s1">at::Tensor _trilinear(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; i1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; i2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1104"><span class="ln">1104 </span></a><span class="s1">at::Tensor triplet_margin_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; anchor, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; positive, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; negative, </span><span class="s2">double </span><span class="s1">margin, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">swap, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1105"><span class="ln">1105 </span></a><span class="s1">at::Tensor trunc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trunc(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1106"><span class="ln">1106 </span></a><span class="s1">at::Tensor &amp; trunc_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trunc_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1107"><span class="ln">1107 </span></a><span class="s1">at::Tensor &amp; trunc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trunc.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1108"><span class="ln">1108 </span></a><span class="s1">at::Tensor fix(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fix(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1109"><span class="ln">1109 </span></a><span class="s1">at::Tensor &amp; fix_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fix_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1110"><span class="ln">1110 </span></a><span class="s1">at::Tensor &amp; fix_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fix.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1111"><span class="ln">1111 </span></a><span class="s1">at::Tensor type_as(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::type_as(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1112"><span class="ln">1112 </span></a><span class="s2">bool </span><span class="s1">_has_compatible_shallow_copy_type(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; from); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1113"><span class="ln">1113 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _unique(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">sorted, </span><span class="s2">bool </span><span class="s1">return_inverse); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unique(Tensor self, bool sorted=True, bool return_inverse=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1114"><span class="ln">1114 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; unique_dim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">sorted, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1115"><span class="ln">1115 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; unique_consecutive(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts, ::std::optional&lt;int64_t&gt; dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1116"><span class="ln">1116 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; unique_dim_consecutive(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1117"><span class="ln">1117 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _unique2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">sorted, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1118"><span class="ln">1118 </span></a><span class="s1">at::Tensor _unsafe_view(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unsafe_view(Tensor self, SymInt[] size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1119"><span class="ln">1119 </span></a><span class="s1">at::Tensor unsqueeze(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsqueeze(Tensor(a) self, int dim) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1120"><span class="ln">1120 </span></a><span class="s1">at::Tensor &amp; unsqueeze_(at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsqueeze_(Tensor(a!) self, int dim) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1121"><span class="ln">1121 </span></a><span class="s1">at::Tensor vander(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, ::std::optional&lt;int64_t&gt; N, </span><span class="s2">bool </span><span class="s1">increasing); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::vander(Tensor x, int? N=None, bool increasing=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1122"><span class="ln">1122 </span></a><span class="s1">at::Tensor var(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">unbiased); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var(Tensor self, bool unbiased=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1123"><span class="ln">1123 </span></a><span class="s1">at::Tensor var(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1124"><span class="ln">1124 </span></a><span class="s1">at::Tensor var(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1125"><span class="ln">1125 </span></a><span class="s1">at::Tensor &amp; var_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1126"><span class="ln">1126 </span></a><span class="s1">at::Tensor &amp; var_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1127"><span class="ln">1127 </span></a><span class="s1">at::Tensor var(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1128"><span class="ln">1128 </span></a><span class="s1">at::Tensor &amp; var_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1129"><span class="ln">1129 </span></a><span class="s1">at::Tensor var(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1130"><span class="ln">1130 </span></a><span class="s1">at::Tensor &amp; var_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1131"><span class="ln">1131 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; var_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">unbiased); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var_mean(Tensor self, bool unbiased=True) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1132"><span class="ln">1132 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; var_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1133"><span class="ln">1133 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; var_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1134"><span class="ln">1134 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; var_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">unbiased, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1135"><span class="ln">1135 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; var_mean(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::DimnameList dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1136"><span class="ln">1136 </span></a><span class="s1">at::Tensor view_as(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_as(Tensor(a) self, Tensor other) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1137"><span class="ln">1137 </span></a><span class="s1">at::Tensor where(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; condition, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::where.self(Tensor condition, Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1138"><span class="ln">1138 </span></a><span class="s1">at::Tensor &amp; where_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; condition, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1139"><span class="ln">1139 </span></a><span class="s1">at::Tensor where(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; condition, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1140"><span class="ln">1140 </span></a><span class="s1">at::Tensor where(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; condition, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1141"><span class="ln">1141 </span></a><span class="s1">at::Tensor where(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; condition, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1142"><span class="ln">1142 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; where(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; condition); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::where(Tensor condition) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1143"><span class="ln">1143 </span></a><span class="s1">at::Tensor norm_except_dim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; v, int64_t pow, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1144"><span class="ln">1144 </span></a><span class="s1">at::Tensor _weight_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; v, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; g, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_norm(Tensor v, Tensor g, int dim=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1145"><span class="ln">1145 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _weight_norm_interface(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; v, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; g, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_norm_interface(Tensor v, Tensor g, int dim=0) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1146"><span class="ln">1146 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _weight_norm_interface_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_w, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_v, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_g, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_norms, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1147"><span class="ln">1147 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _weight_norm_differentiable_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_w, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_v, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_g, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_norms, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1148"><span class="ln">1148 </span></a><span class="s1">at::Tensor zeros(at::IntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1149"><span class="ln">1149 </span></a><span class="s1">at::Tensor _efficientzerotensor(c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_efficientzerotensor(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1150"><span class="ln">1150 </span></a><span class="s1">at::Tensor zeros(c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1151"><span class="ln">1151 </span></a><span class="s1">at::Tensor &amp; zeros_out(c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1152"><span class="ln">1152 </span></a><span class="s1">at::Tensor zeros_like(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1153"><span class="ln">1153 </span></a><span class="s1">at::Tensor _standard_gamma_grad(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_standard_gamma_grad(Tensor self, Tensor output) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1154"><span class="ln">1154 </span></a><span class="s1">at::Tensor _standard_gamma(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_standard_gamma(Tensor self, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1155"><span class="ln">1155 </span></a><span class="s1">at::Tensor _dirichlet_grad(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; total); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1156"><span class="ln">1156 </span></a><span class="s1">at::Tensor _sample_dirichlet(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sample_dirichlet(Tensor self, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1157"><span class="ln">1157 </span></a><span class="s1">at::Tensor poisson(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::poisson(Tensor self, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1158"><span class="ln">1158 </span></a><span class="s1">at::Tensor binomial(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; count, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; prob, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1159"><span class="ln">1159 </span></a><span class="s1">at::Tensor native_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_norm(Tensor self, Scalar p=2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1160"><span class="ln">1160 </span></a><span class="s1">at::Tensor native_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1161"><span class="ln">1161 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _batch_norm_with_update(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, at::Tensor &amp; running_mean, at::Tensor &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_batch_norm_with_update(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1162"><span class="ln">1162 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _batch_norm_with_update_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, at::Tensor &amp; running_mean, at::Tensor &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out, at::Tensor &amp; save_mean, at::Tensor &amp; save_invstd, at::Tensor &amp; reserve); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_batch_norm_with_update.out(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, float momentum, float eps, *, Tensor(d!) out, Tensor(e!) save_mean, Tensor(f!) save_invstd, Tensor(g!) reserve) -&gt; (Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1163"><span class="ln">1163 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _batch_norm_no_update(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_batch_norm_no_update(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1164"><span class="ln">1164 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; batch_norm_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_var, </span><span class="s2">bool </span><span class="s1">update, </span><span class="s2">double </span><span class="s1">eps, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reserve); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_backward(Tensor grad_out, Tensor input, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, bool update, float eps, bool[3] output_mask, Tensor reserve) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1165"><span class="ln">1165 </span></a><span class="s1">at::Tensor _sparse_sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sum(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1166"><span class="ln">1166 </span></a><span class="s1">at::Tensor _sparse_sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sum.dtype(Tensor self, *, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1167"><span class="ln">1167 </span></a><span class="s1">at::Tensor _sparse_sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sum.dim(Tensor self, int[1] dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1168"><span class="ln">1168 </span></a><span class="s1">at::Tensor _sparse_sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1169"><span class="ln">1169 </span></a><span class="s1">at::Tensor _sparse_sum_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1170"><span class="ln">1170 </span></a><span class="s1">at::Tensor _sparse_csr_sum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_csr_sum.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1171"><span class="ln">1171 </span></a><span class="s1">at::Tensor _sparse_csr_prod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_csr_prod.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1172"><span class="ln">1172 </span></a><span class="s1">at::Tensor _sparse_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1173"><span class="ln">1173 </span></a><span class="s1">at::Tensor _sparse_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1174"><span class="ln">1174 </span></a><span class="s1">at::Tensor _sparse_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1175"><span class="ln">1175 </span></a><span class="s1">at::Tensor _sparse_softmax_backward_data(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1176"><span class="ln">1176 </span></a><span class="s1">at::Tensor _sparse_log_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1177"><span class="ln">1177 </span></a><span class="s1">at::Tensor _sparse_log_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1178"><span class="ln">1178 </span></a><span class="s1">at::Tensor _sparse_log_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1179"><span class="ln">1179 </span></a><span class="s1">at::Tensor _sparse_log_softmax_backward_data(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1180"><span class="ln">1180 </span></a><span class="s1">at::Tensor _spdiags(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; diagonals, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, at::IntArrayRef shape, ::std::optional&lt;at::Layout&gt; layout); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_spdiags(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1181"><span class="ln">1181 </span></a><span class="s1">at::Tensor norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1182"><span class="ln">1182 </span></a><span class="s1">at::Tensor norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.Scalar(Tensor self, Scalar p=2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1183"><span class="ln">1183 </span></a><span class="s1">at::Tensor norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1184"><span class="ln">1184 </span></a><span class="s1">at::Tensor norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1185"><span class="ln">1185 </span></a><span class="s1">at::Tensor &amp; norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::ScalarType dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1186"><span class="ln">1186 </span></a><span class="s1">at::Tensor &amp; norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1187"><span class="ln">1187 </span></a><span class="s1">at::Tensor norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1188"><span class="ln">1188 </span></a><span class="s1">at::Tensor norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1189"><span class="ln">1189 </span></a><span class="s1">at::Tensor &amp; norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::ScalarType dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1190"><span class="ln">1190 </span></a><span class="s1">at::Tensor &amp; norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::DimnameList dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1191"><span class="ln">1191 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; frexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::frexp.Tensor(Tensor self) -&gt; (Tensor mantissa, Tensor exponent)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1192"><span class="ln">1192 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; frexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; mantissa, at::Tensor &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -&gt; (Tensor(a!) mantissa, Tensor(b!) exponent)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1193"><span class="ln">1193 </span></a><span class="s1">at::Tensor frobenius_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1194"><span class="ln">1194 </span></a><span class="s1">at::Tensor &amp; frobenius_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1195"><span class="ln">1195 </span></a><span class="s1">at::Tensor nuclear_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nuclear_norm(Tensor self, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1196"><span class="ln">1196 </span></a><span class="s1">at::Tensor &amp; nuclear_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1197"><span class="ln">1197 </span></a><span class="s1">at::Tensor nuclear_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1198"><span class="ln">1198 </span></a><span class="s1">at::Tensor &amp; nuclear_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1199"><span class="ln">1199 </span></a><span class="s1">at::Tensor clone(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1200"><span class="ln">1200 </span></a><span class="s1">at::Tensor positive(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::positive(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1201"><span class="ln">1201 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; resize_as_(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; the_template, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1202"><span class="ln">1202 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; resize_as_sparse_(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; the_template); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1203"><span class="ln">1203 </span></a><span class="s1">at::Tensor &amp; zero_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zero_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1204"><span class="ln">1204 </span></a><span class="s1">at::Tensor &amp; sub_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1205"><span class="ln">1205 </span></a><span class="s1">at::Tensor sub(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1206"><span class="ln">1206 </span></a><span class="s1">at::Tensor &amp; sub_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1207"><span class="ln">1207 </span></a><span class="s1">at::Tensor sub(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1208"><span class="ln">1208 </span></a><span class="s1">at::Tensor &amp; sub_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1209"><span class="ln">1209 </span></a><span class="s1">at::Tensor &amp; subtract_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1210"><span class="ln">1210 </span></a><span class="s1">at::Tensor subtract(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1211"><span class="ln">1211 </span></a><span class="s1">at::Tensor &amp; subtract_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1212"><span class="ln">1212 </span></a><span class="s1">at::Tensor subtract(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1213"><span class="ln">1213 </span></a><span class="s1">at::Tensor &amp; subtract_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1214"><span class="ln">1214 </span></a><span class="s1">at::Tensor rsub(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1215"><span class="ln">1215 </span></a><span class="s1">at::Tensor &amp; heaviside_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1216"><span class="ln">1216 </span></a><span class="s1">at::Tensor heaviside(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::heaviside(Tensor self, Tensor values) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1217"><span class="ln">1217 </span></a><span class="s1">at::Tensor &amp; heaviside_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::heaviside_(Tensor(a!) self, Tensor values) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1218"><span class="ln">1218 </span></a><span class="s1">at::Tensor rsub(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1219"><span class="ln">1219 </span></a><span class="s1">at::Tensor _sparse_addmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1220"><span class="ln">1220 </span></a><span class="s1">at::Tensor &amp; sparse_sampled_addmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1221"><span class="ln">1221 </span></a><span class="s1">at::Tensor sparse_sampled_addmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1222"><span class="ln">1222 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _sparse_mm_reduce_impl(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, c10::string_view reduce); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_mm_reduce_impl(Tensor self, Tensor other, str reduce) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1223"><span class="ln">1223 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _sparse_mm_reduce_impl_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::string_view reduce, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; arg_out, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_mm_reduce_impl_backward(Tensor self, Tensor grad_out, Tensor weight, str reduce, Tensor arg_out, bool[2] output_mask) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1224"><span class="ln">1224 </span></a><span class="s1">at::Tensor &amp; addmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1225"><span class="ln">1225 </span></a><span class="s1">at::Tensor addmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1226"><span class="ln">1226 </span></a><span class="s1">at::Tensor addmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::ScalarType out_dtype, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmm.dtype(Tensor self, Tensor mat1, Tensor mat2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1227"><span class="ln">1227 </span></a><span class="s1">at::Tensor &amp; addmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::ScalarType out_dtype, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmm.dtype_out(Tensor self, Tensor mat1, Tensor mat2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1228"><span class="ln">1228 </span></a><span class="s1">at::Tensor &amp; addmm_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1229"><span class="ln">1229 </span></a><span class="s1">at::Tensor &amp; _addmm_activation_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">bool </span><span class="s1">use_gelu, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_addmm_activation.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1230"><span class="ln">1230 </span></a><span class="s1">at::Tensor _addmm_activation(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">bool </span><span class="s1">use_gelu); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_addmm_activation(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1231"><span class="ln">1231 </span></a><span class="s1">at::Tensor _scaled_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale_a, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale_b, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; scale_result, ::std::optional&lt;at::ScalarType&gt; out_dtype, </span><span class="s2">bool </span><span class="s1">use_fast_accum); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_mm(Tensor self, Tensor mat2, Tensor scale_a, Tensor scale_b, Tensor? bias=None, Tensor? scale_result=None, ScalarType? out_dtype=None, bool use_fast_accum=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1232"><span class="ln">1232 </span></a><span class="s1">at::Tensor &amp; _scaled_mm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale_a, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale_b, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; scale_result, ::std::optional&lt;at::ScalarType&gt; out_dtype, </span><span class="s2">bool </span><span class="s1">use_fast_accum, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_mm.out(Tensor self, Tensor mat2, Tensor scale_a, Tensor scale_b, Tensor? bias=None, Tensor? scale_result=None, ScalarType? out_dtype=None, bool use_fast_accum=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1233"><span class="ln">1233 </span></a><span class="s1">at::Tensor _scaled_grouped_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale_a, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale_b, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; offs, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; scale_result, ::std::optional&lt;at::ScalarType&gt; out_dtype, </span><span class="s2">bool </span><span class="s1">use_fast_accum); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_grouped_mm(Tensor self, Tensor mat2, Tensor scale_a, Tensor scale_b, Tensor? offs=None, Tensor? bias=None, Tensor? scale_result=None, ScalarType? out_dtype=None, bool use_fast_accum=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1234"><span class="ln">1234 </span></a><span class="s1">at::Tensor _grouped_mm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; offs, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, ::std::optional&lt;at::ScalarType&gt; out_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_grouped_mm(Tensor self, Tensor mat2, Tensor? offs=None, Tensor? bias=None, ScalarType? out_dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1235"><span class="ln">1235 </span></a><span class="s1">at::Tensor _sparse_compressed_tensor_with_dims(int64_t nnz, int64_t dense_dim, at::IntArrayRef size, at::IntArrayRef blocksize, at::ScalarType index_dtype, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_compressed_tensor_with_dims(int nnz, int dense_dim, int[] size, int[] blocksize, ScalarType index_dtype, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1236"><span class="ln">1236 </span></a><span class="s1">at::Tensor sparse_compressed_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; compressed_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; plain_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_compressed_tensor.comp_plain_value_size(Tensor compressed_indices, Tensor plain_indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1237"><span class="ln">1237 </span></a><span class="s1">at::Tensor sparse_csr_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1238"><span class="ln">1238 </span></a><span class="s1">at::Tensor sparse_csc_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_csc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1239"><span class="ln">1239 </span></a><span class="s1">at::Tensor sparse_bsr_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_bsr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1240"><span class="ln">1240 </span></a><span class="s1">at::Tensor sparse_bsc_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_bsc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1241"><span class="ln">1241 </span></a><span class="s1">at::Tensor sparse_compressed_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; compressed_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; plain_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_compressed_tensor.comp_plain_value(Tensor compressed_indices, Tensor plain_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1242"><span class="ln">1242 </span></a><span class="s1">at::Tensor sparse_csr_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1243"><span class="ln">1243 </span></a><span class="s1">at::Tensor sparse_csc_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_csc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1244"><span class="ln">1244 </span></a><span class="s1">at::Tensor sparse_bsr_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_bsr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1245"><span class="ln">1245 </span></a><span class="s1">at::Tensor sparse_bsc_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_bsc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1246"><span class="ln">1246 </span></a><span class="s1">at::Tensor _sparse_compressed_tensor_unsafe(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; compressed_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; plain_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_compressed_tensor_unsafe(Tensor compressed_indices, Tensor plain_indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1247"><span class="ln">1247 </span></a><span class="s1">at::Tensor _sparse_csr_tensor_unsafe(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1248"><span class="ln">1248 </span></a><span class="s1">at::Tensor _sparse_csc_tensor_unsafe(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_csc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1249"><span class="ln">1249 </span></a><span class="s1">at::Tensor _sparse_bsr_tensor_unsafe(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1250"><span class="ln">1250 </span></a><span class="s1">at::Tensor _sparse_bsc_tensor_unsafe(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1251"><span class="ln">1251 </span></a><span class="s1">at::Tensor sparse_coo_tensor(at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1252"><span class="ln">1252 </span></a><span class="s1">at::Tensor sparse_coo_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; is_coalesced); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1253"><span class="ln">1253 </span></a><span class="s1">at::Tensor sparse_coo_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; is_coalesced); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1254"><span class="ln">1254 </span></a><span class="s1">at::Tensor _sparse_coo_tensor_unsafe(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, c10::SymIntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; is_coalesced); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1255"><span class="ln">1255 </span></a><span class="s2">void </span><span class="s1">_validate_sparse_coo_tensor_args(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; is_coalesced, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; check_pinning); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size, bool? is_coalesced=None, bool? check_pinning=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1256"><span class="ln">1256 </span></a><span class="s2">void </span><span class="s1">_validate_sparse_compressed_tensor_args(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; compressed_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; plain_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, at::Layout layout, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; check_pinning); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_validate_sparse_compressed_tensor_args(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, Layout layout, bool? check_pinning=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1257"><span class="ln">1257 </span></a><span class="s2">void </span><span class="s1">_validate_sparse_csr_tensor_args(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; check_pinning); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, bool? check_pinning=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1258"><span class="ln">1258 </span></a><span class="s2">void </span><span class="s1">_validate_sparse_csc_tensor_args(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; check_pinning); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_validate_sparse_csc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, bool? check_pinning=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1259"><span class="ln">1259 </span></a><span class="s2">void </span><span class="s1">_validate_sparse_bsr_tensor_args(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; check_pinning); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_validate_sparse_bsr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, bool? check_pinning=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1260"><span class="ln">1260 </span></a><span class="s2">void </span><span class="s1">_validate_sparse_bsc_tensor_args(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; ccol_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; row_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::IntArrayRef size, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; check_pinning); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_validate_sparse_bsc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, bool? check_pinning=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1261"><span class="ln">1261 </span></a><span class="s1">at::Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1262"><span class="ln">1262 </span></a><span class="s1">at::Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; is_coalesced); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? is_coalesced=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1263"><span class="ln">1263 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; sparse_resize_(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1264"><span class="ln">1264 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; sparse_resize_and_clear_(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1265"><span class="ln">1265 </span></a><span class="s1">at::Tensor sparse_mask(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_mask(Tensor self, Tensor mask) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1266"><span class="ln">1266 </span></a><span class="s1">at::Tensor _sparse_mask_projection(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">bool </span><span class="s1">accumulate_matches); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_mask_projection(Tensor self, Tensor mask, bool accumulate_matches=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1267"><span class="ln">1267 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _to_cpu(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_cpu(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1268"><span class="ln">1268 </span></a><span class="s1">at::Tensor to_dense(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; masked_grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_dense(Tensor self, ScalarType? dtype=None, *, bool? masked_grad=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1269"><span class="ln">1269 </span></a><span class="s1">at::Tensor _to_dense(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; masked_grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_dense(Tensor self, ScalarType? dtype=None, bool? masked_grad=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1270"><span class="ln">1270 </span></a><span class="s1">at::Tensor to_dense_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; masked_grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1271"><span class="ln">1271 </span></a><span class="s1">int64_t sparse_dim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_dim(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1272"><span class="ln">1272 </span></a><span class="s1">int64_t _dimI(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_dimI(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1273"><span class="ln">1273 </span></a><span class="s1">int64_t dense_dim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dense_dim(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1274"><span class="ln">1274 </span></a><span class="s1">int64_t _dimV(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_dimV(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1275"><span class="ln">1275 </span></a><span class="s1">int64_t _nnz(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nnz(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1276"><span class="ln">1276 </span></a><span class="s1">at::Tensor coalesce(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::coalesce(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1277"><span class="ln">1277 </span></a><span class="s1">at::Tensor _coalesce(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_coalesce(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1278"><span class="ln">1278 </span></a><span class="s2">bool </span><span class="s1">is_coalesced(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_coalesced(Tensor self) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1279"><span class="ln">1279 </span></a><span class="s1">at::Tensor _indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_indices(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1280"><span class="ln">1280 </span></a><span class="s1">at::Tensor _values(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_values(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1281"><span class="ln">1281 </span></a><span class="s1">at::Tensor &amp; _coalesced_(at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">coalesced); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_coalesced_(Tensor(a!) self, bool coalesced) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1282"><span class="ln">1282 </span></a><span class="s1">at::Tensor indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::indices(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1283"><span class="ln">1283 </span></a><span class="s1">at::Tensor values(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::values(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1284"><span class="ln">1284 </span></a><span class="s1">at::Tensor crow_indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::crow_indices(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1285"><span class="ln">1285 </span></a><span class="s1">at::Tensor col_indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::col_indices(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1286"><span class="ln">1286 </span></a><span class="s1">at::Tensor ccol_indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ccol_indices(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1287"><span class="ln">1287 </span></a><span class="s1">at::Tensor row_indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::row_indices(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1288"><span class="ln">1288 </span></a><span class="s1">at::Tensor &amp; hspmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1289"><span class="ln">1289 </span></a><span class="s1">at::Tensor hspmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hspmm(Tensor mat1, Tensor mat2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1290"><span class="ln">1290 </span></a><span class="s1">at::Tensor &amp; copy_sparse_to_sparse_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1291"><span class="ln">1291 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; unbind(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unbind.int(Tensor(a -&gt; *) self, int dim=0) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1292"><span class="ln">1292 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; unbind(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unbind.Dimname(Tensor(a -&gt; *) self, Dimname dim) -&gt; Tensor(a)[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1293"><span class="ln">1293 </span></a><span class="s1">at::Tensor to_sparse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t sparse_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1294"><span class="ln">1294 </span></a><span class="s1">at::Tensor _to_sparse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t sparse_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse.sparse_dim(Tensor self, int sparse_dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1295"><span class="ln">1295 </span></a><span class="s1">at::Tensor to_sparse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Layout&gt; layout, at::OptionalIntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1296"><span class="ln">1296 </span></a><span class="s1">at::Tensor _to_sparse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Layout&gt; layout, at::OptionalIntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1297"><span class="ln">1297 </span></a><span class="s1">at::Tensor to_sparse_csr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_sparse_csr(Tensor self, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1298"><span class="ln">1298 </span></a><span class="s1">at::Tensor _to_sparse_csr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_csr(Tensor self, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1299"><span class="ln">1299 </span></a><span class="s1">at::Tensor to_sparse_csc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_sparse_csc(Tensor self, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1300"><span class="ln">1300 </span></a><span class="s1">at::Tensor _to_sparse_csc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_csc(Tensor self, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1301"><span class="ln">1301 </span></a><span class="s1">at::Tensor to_sparse_bsr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1302"><span class="ln">1302 </span></a><span class="s1">at::Tensor _to_sparse_bsr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1303"><span class="ln">1303 </span></a><span class="s1">at::Tensor to_sparse_bsc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1304"><span class="ln">1304 </span></a><span class="s1">at::Tensor _to_sparse_bsc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1305"><span class="ln">1305 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _to_sparse_semi_structured(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; dense); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_semi_structured(Tensor dense) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1306"><span class="ln">1306 </span></a><span class="s1">at::Tensor to_mkldnn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1307"><span class="ln">1307 </span></a><span class="s1">at::Tensor mkldnn_reorder_conv2d_weight(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_reorder_conv2d_weight(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1308"><span class="ln">1308 </span></a><span class="s1">at::Tensor mkldnn_reorder_conv3d_weight(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_reorder_conv3d_weight(Tensor self, SymInt[3] padding=0, SymInt[3] stride=1, SymInt[3] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1309"><span class="ln">1309 </span></a><span class="s1">at::Tensor to_mkldnn_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_mkldnn_backward(Tensor grad, Tensor input) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1310"><span class="ln">1310 </span></a><span class="s1">at::Tensor quantize_per_tensor_dynamic(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ScalarType dtype, </span><span class="s2">bool </span><span class="s1">reduce_range); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor_dynamic(Tensor self, ScalarType dtype, bool reduce_range) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1311"><span class="ln">1311 </span></a><span class="s1">at::Tensor quantize_per_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1312"><span class="ln">1312 </span></a><span class="s1">at::Tensor quantize_per_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1313"><span class="ln">1313 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; quantize_per_tensor(at::TensorList tensors, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_points, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1314"><span class="ln">1314 </span></a><span class="s1">at::Tensor quantize_per_channel(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_points, int64_t axis, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1315"><span class="ln">1315 </span></a><span class="s1">at::Tensor dequantize(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dequantize.self(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1316"><span class="ln">1316 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; dequantize(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dequantize.tensors(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1317"><span class="ln">1317 </span></a><span class="s2">double </span><span class="s1">q_scale(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::q_scale(Tensor self) -&gt; float&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1318"><span class="ln">1318 </span></a><span class="s1">int64_t q_zero_point(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::q_zero_point(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1319"><span class="ln">1319 </span></a><span class="s1">at::Tensor q_per_channel_scales(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::q_per_channel_scales(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1320"><span class="ln">1320 </span></a><span class="s1">at::Tensor q_per_channel_zero_points(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::q_per_channel_zero_points(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1321"><span class="ln">1321 </span></a><span class="s1">int64_t q_per_channel_axis(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::q_per_channel_axis(Tensor self) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1322"><span class="ln">1322 </span></a><span class="s1">at::Tensor int_repr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::int_repr(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1323"><span class="ln">1323 </span></a><span class="s1">at::Tensor _make_per_tensor_quantized_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1324"><span class="ln">1324 </span></a><span class="s1">at::Tensor _make_per_channel_quantized_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1325"><span class="ln">1325 </span></a><span class="s1">at::QScheme qscheme(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::qscheme(Tensor self) -&gt; QScheme&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1326"><span class="ln">1326 </span></a><span class="s1">at::Tensor fake_quantize_per_tensor_affine(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, int64_t quant_min, int64_t quant_max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1327"><span class="ln">1327 </span></a><span class="s1">at::Tensor fake_quantize_per_tensor_affine(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t quant_min, int64_t quant_max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1328"><span class="ln">1328 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; fake_quantize_per_tensor_affine_cachemask(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, int64_t quant_min, int64_t quant_max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -&gt; (Tensor output, Tensor mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1329"><span class="ln">1329 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; fake_quant_enabled, int64_t quant_min, int64_t quant_max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -&gt; (Tensor output, Tensor mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1330"><span class="ln">1330 </span></a><span class="s1">at::Tensor fake_quantize_per_tensor_affine_cachemask_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1331"><span class="ln">1331 </span></a><span class="s1">at::Tensor _fake_quantize_learnable_per_tensor_affine(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t quant_min, int64_t quant_max, </span><span class="s2">double </span><span class="s1">grad_factor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1332"><span class="ln">1332 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _fake_quantize_learnable_per_tensor_affine_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t quant_min, int64_t quant_max, </span><span class="s2">double </span><span class="s1">grad_factor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1333"><span class="ln">1333 </span></a><span class="s1">at::Tensor fake_quantize_per_channel_affine(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis, int64_t quant_min, int64_t quant_max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1334"><span class="ln">1334 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; fake_quantize_per_channel_affine_cachemask(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis, int64_t quant_min, int64_t quant_max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -&gt; (Tensor output, Tensor mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1335"><span class="ln">1335 </span></a><span class="s1">at::Tensor fake_quantize_per_channel_affine_cachemask_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1336"><span class="ln">1336 </span></a><span class="s1">at::Tensor _fake_quantize_learnable_per_channel_affine(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, </span><span class="s2">double </span><span class="s1">grad_factor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1337"><span class="ln">1337 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _fake_quantize_learnable_per_channel_affine_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, </span><span class="s2">double </span><span class="s1">grad_factor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1338"><span class="ln">1338 </span></a><span class="s1">at::Tensor fused_moving_avg_obs_fake_quant(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; observer_on, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; fake_quant_on, at::Tensor &amp; running_min, at::Tensor &amp; running_max, at::Tensor &amp; scale, at::Tensor &amp; zero_point, </span><span class="s2">double </span><span class="s1">averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, </span><span class="s2">bool </span><span class="s1">per_row_fake_quant, </span><span class="s2">bool </span><span class="s1">symmetric_quant); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1339"><span class="ln">1339 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _fused_moving_avg_obs_fq_helper(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; observer_on, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; fake_quant_on, at::Tensor &amp; running_min, at::Tensor &amp; running_max, at::Tensor &amp; scale, at::Tensor &amp; zero_point, </span><span class="s2">double </span><span class="s1">averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, </span><span class="s2">bool </span><span class="s1">per_row_fake_quant, </span><span class="s2">bool </span><span class="s1">symmetric_quant); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -&gt; (Tensor output, Tensor mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1340"><span class="ln">1340 </span></a><span class="s1">::std::tuple&lt;</span><span class="s2">double</span><span class="s1">,int64_t&gt; _choose_qparams_per_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">reduce_range); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -&gt; (float, int)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1341"><span class="ln">1341 </span></a><span class="s1">at::Tensor _saturate_weight_to_fp16(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_saturate_weight_to_fp16(Tensor weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1342"><span class="ln">1342 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; choose_qparams_optimized(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, int64_t numel, int64_t n_bins, </span><span class="s2">double </span><span class="s1">ratio, int64_t bit_width); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1343"><span class="ln">1343 </span></a><span class="s1">at::Tensor _autocast_to_reduced_precision(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">cuda_enabled, </span><span class="s2">bool </span><span class="s1">cpu_enabled, at::ScalarType cuda_dtype, at::ScalarType cpu_dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_autocast_to_reduced_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled, ScalarType cuda_dtype, ScalarType cpu_dtype) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1344"><span class="ln">1344 </span></a><span class="s1">at::Tensor _autocast_to_full_precision(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">cuda_enabled, </span><span class="s2">bool </span><span class="s1">cpu_enabled); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1345"><span class="ln">1345 </span></a><span class="s1">at::Tensor _to_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, </span><span class="s2">bool </span><span class="s1">non_blocking, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1346"><span class="ln">1346 </span></a><span class="s1">at::Tensor to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, </span><span class="s2">bool </span><span class="s1">non_blocking, </span><span class="s2">bool </span><span class="s1">copy, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1347"><span class="ln">1347 </span></a><span class="s1">at::Tensor to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Device device, at::ScalarType dtype, </span><span class="s2">bool </span><span class="s1">non_blocking, </span><span class="s2">bool </span><span class="s1">copy, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1348"><span class="ln">1348 </span></a><span class="s1">at::Tensor to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ScalarType dtype, </span><span class="s2">bool </span><span class="s1">non_blocking, </span><span class="s2">bool </span><span class="s1">copy, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1349"><span class="ln">1349 </span></a><span class="s1">at::Tensor to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">bool </span><span class="s1">non_blocking, </span><span class="s2">bool </span><span class="s1">copy, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1350"><span class="ln">1350 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; meshgrid(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::meshgrid(Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1351"><span class="ln">1351 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; meshgrid(at::TensorList tensors, c10::string_view indexing); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1352"><span class="ln">1352 </span></a><span class="s1">at::Tensor cartesian_prod(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cartesian_prod(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1353"><span class="ln">1353 </span></a><span class="s1">at::Tensor combinations(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t r, </span><span class="s2">bool </span><span class="s1">with_replacement); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::combinations(Tensor self, int r=2, bool with_replacement=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1354"><span class="ln">1354 </span></a><span class="s1">at::Scalar item(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::item(Tensor self) -&gt; Scalar&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1355"><span class="ln">1355 </span></a><span class="s1">at::ScalarType result_type(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::result_type.Tensor(Tensor tensor, Tensor other) -&gt; ScalarType&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1356"><span class="ln">1356 </span></a><span class="s1">at::ScalarType result_type(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::result_type.Scalar(Tensor tensor, Scalar other) -&gt; ScalarType&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1357"><span class="ln">1357 </span></a><span class="s1">at::ScalarType result_type(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -&gt; ScalarType&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1358"><span class="ln">1358 </span></a><span class="s1">at::ScalarType result_type(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar1, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -&gt; ScalarType&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1359"><span class="ln">1359 </span></a><span class="s2">bool </span><span class="s1">can_cast(at::ScalarType from_, at::ScalarType to); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::can_cast(ScalarType from_, ScalarType to) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1360"><span class="ln">1360 </span></a><span class="s1">at::ScalarType promote_types(at::ScalarType type1, at::ScalarType type2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::promote_types(ScalarType type1, ScalarType type2) -&gt; ScalarType&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1361"><span class="ln">1361 </span></a><span class="s1">at::Scalar _local_scalar_dense(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_local_scalar_dense(Tensor self) -&gt; Scalar&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1362"><span class="ln">1362 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _lstm_mps(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_lstm_mps(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1363"><span class="ln">1363 </span></a><span class="s1">::std::tuple&lt;at::Tensor,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; lstm_mps_backward(</span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_y, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; z_state, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cell_state_fwd, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; layersOutputs, at::TensorList hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lstm_mps_backward(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -&gt; (Tensor, Tensor[], Tensor[])&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1364"><span class="ln">1364 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _thnn_fused_lstm_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hidden_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; input_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; hidden_bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1365"><span class="ln">1365 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _thnn_fused_lstm_cell_backward_impl(</span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; workspace, </span><span class="s2">bool </span><span class="s1">has_bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_lstm_cell_backward_impl(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1366"><span class="ln">1366 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _thnn_fused_lstm_cell_backward(</span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; workspace, </span><span class="s2">bool </span><span class="s1">has_bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1367"><span class="ln">1367 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _thnn_differentiable_lstm_cell_backward(</span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hidden_gates, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; input_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; hidden_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cy); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1368"><span class="ln">1368 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _thnn_fused_gru_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hidden_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; input_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; hidden_bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1369"><span class="ln">1369 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _thnn_fused_gru_cell_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_hy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; workspace, </span><span class="s2">bool </span><span class="s1">has_bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1370"><span class="ln">1370 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _thnn_differentiable_gru_cell_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_hy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hidden_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; input_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; hidden_bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1371"><span class="ln">1371 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; lstm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1372"><span class="ln">1372 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; lstm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch_sizes, at::TensorList hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1373"><span class="ln">1373 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; gru(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1374"><span class="ln">1374 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; gru(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch_sizes, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1375"><span class="ln">1375 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; rnn_tanh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1376"><span class="ln">1376 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; rnn_tanh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch_sizes, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1377"><span class="ln">1377 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; rnn_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1378"><span class="ln">1378 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; rnn_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch_sizes, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1379"><span class="ln">1379 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; lstm_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_ih, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1380"><span class="ln">1380 </span></a><span class="s1">at::Tensor gru_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_ih, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1381"><span class="ln">1381 </span></a><span class="s1">at::Tensor rnn_tanh_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_ih, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1382"><span class="ln">1382 </span></a><span class="s1">at::Tensor rnn_relu_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_ih, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; b_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1383"><span class="ln">1383 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; quantized_lstm_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1384"><span class="ln">1384 </span></a><span class="s1">at::Tensor quantized_gru_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1385"><span class="ln">1385 </span></a><span class="s1">at::Tensor quantized_rnn_relu_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1386"><span class="ln">1386 </span></a><span class="s1">at::Tensor quantized_rnn_tanh_cell(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; w_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; packed_hh, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_ih, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_offsets_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale_hh, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_ih, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; zero_point_hh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1387"><span class="ln">1387 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _pack_padded_sequence(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lengths, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1388"><span class="ln">1388 </span></a><span class="s1">at::Tensor _pack_padded_sequence_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, c10::SymIntArrayRef input_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch_sizes, </span><span class="s2">bool </span><span class="s1">batch_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pack_padded_sequence_backward(Tensor grad, SymInt[] input_size, Tensor batch_sizes, bool batch_first) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1389"><span class="ln">1389 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _pad_packed_sequence(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch_sizes, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; padding_value, int64_t total_length); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1390"><span class="ln">1390 </span></a><span class="s1">at::Tensor &amp; set_(at::Tensor &amp; self, at::Storage source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set_.source_Storage(Tensor(a!) self, Storage source) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1391"><span class="ln">1391 </span></a><span class="s1">at::Tensor &amp; set_(at::Tensor &amp; self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1392"><span class="ln">1392 </span></a><span class="s1">at::Tensor &amp; set_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1393"><span class="ln">1393 </span></a><span class="s1">at::Tensor &amp; set_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1394"><span class="ln">1394 </span></a><span class="s1">at::Tensor &amp; set_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1395"><span class="ln">1395 </span></a><span class="s1">at::Tensor lift(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lift(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1396"><span class="ln">1396 </span></a><span class="s1">at::Tensor lift_fresh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lift_fresh(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1397"><span class="ln">1397 </span></a><span class="s1">at::Tensor lift_fresh_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lift_fresh_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1398"><span class="ln">1398 </span></a><span class="s2">bool </span><span class="s1">is_set_to(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::is_set_to(Tensor self, Tensor tensor) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1399"><span class="ln">1399 </span></a><span class="s1">at::Tensor &amp; masked_fill_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1400"><span class="ln">1400 </span></a><span class="s1">at::Tensor masked_fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1401"><span class="ln">1401 </span></a><span class="s1">at::Tensor &amp; masked_fill_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1402"><span class="ln">1402 </span></a><span class="s1">at::Tensor masked_fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1403"><span class="ln">1403 </span></a><span class="s1">at::Tensor &amp; masked_scatter_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1404"><span class="ln">1404 </span></a><span class="s1">at::Tensor masked_scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1405"><span class="ln">1405 </span></a><span class="s1">at::Tensor masked_scatter_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, c10::SymIntArrayRef sizes); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_scatter_backward(Tensor grad_output, Tensor mask, SymInt[] sizes) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1406"><span class="ln">1406 </span></a><span class="s1">at::Tensor _masked_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, ::std::optional&lt;int64_t&gt; dim, ::std::optional&lt;int64_t&gt; mask_type); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1407"><span class="ln">1407 </span></a><span class="s1">at::Tensor _masked_softmax_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, ::std::optional&lt;int64_t&gt; dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_masked_softmax_backward(Tensor grad_output, Tensor output, Tensor mask, int? dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1408"><span class="ln">1408 </span></a><span class="s1">at::Tensor view(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1409"><span class="ln">1409 </span></a><span class="s1">at::Tensor view(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view.dtype(Tensor(a) self, ScalarType dtype) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1410"><span class="ln">1410 </span></a><span class="s1">at::Tensor &amp; put_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, </span><span class="s2">bool </span><span class="s1">accumulate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1411"><span class="ln">1411 </span></a><span class="s1">at::Tensor put(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, </span><span class="s2">bool </span><span class="s1">accumulate); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1412"><span class="ln">1412 </span></a><span class="s1">at::Tensor &amp; index_add_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1413"><span class="ln">1413 </span></a><span class="s1">at::Tensor &amp; index_add_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1414"><span class="ln">1414 </span></a><span class="s1">at::Tensor index_add(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1415"><span class="ln">1415 </span></a><span class="s1">at::Tensor index_add(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1416"><span class="ln">1416 </span></a><span class="s1">at::Tensor &amp; index_reduce_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, c10::string_view reduce, </span><span class="s2">bool </span><span class="s1">include_self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1417"><span class="ln">1417 </span></a><span class="s1">at::Tensor &amp; index_reduce_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, c10::string_view reduce, </span><span class="s2">bool </span><span class="s1">include_self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1418"><span class="ln">1418 </span></a><span class="s1">at::Tensor index_reduce(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, c10::string_view reduce, </span><span class="s2">bool </span><span class="s1">include_self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1419"><span class="ln">1419 </span></a><span class="s1">at::Tensor &amp; index_fill_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1420"><span class="ln">1420 </span></a><span class="s1">at::Tensor index_fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1421"><span class="ln">1421 </span></a><span class="s1">at::Tensor &amp; index_fill_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1422"><span class="ln">1422 </span></a><span class="s1">at::Tensor index_fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1423"><span class="ln">1423 </span></a><span class="s1">at::Tensor &amp; index_fill_(at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1424"><span class="ln">1424 </span></a><span class="s1">at::Tensor &amp; index_fill_(at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1425"><span class="ln">1425 </span></a><span class="s1">at::Tensor index_fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1426"><span class="ln">1426 </span></a><span class="s1">at::Tensor index_fill(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1427"><span class="ln">1427 </span></a><span class="s1">at::Tensor scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1428"><span class="ln">1428 </span></a><span class="s1">at::Tensor &amp; scatter_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1429"><span class="ln">1429 </span></a><span class="s1">at::Tensor &amp; scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1430"><span class="ln">1430 </span></a><span class="s1">at::Tensor scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1431"><span class="ln">1431 </span></a><span class="s1">at::Tensor &amp; scatter_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1432"><span class="ln">1432 </span></a><span class="s1">at::Tensor &amp; scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1433"><span class="ln">1433 </span></a><span class="s1">at::Tensor scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::string_view reduce); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1434"><span class="ln">1434 </span></a><span class="s1">at::Tensor &amp; scatter_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::string_view reduce); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1435"><span class="ln">1435 </span></a><span class="s1">at::Tensor &amp; scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::string_view reduce, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1436"><span class="ln">1436 </span></a><span class="s1">at::Tensor scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, c10::string_view reduce); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1437"><span class="ln">1437 </span></a><span class="s1">at::Tensor &amp; scatter_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, c10::string_view reduce); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1438"><span class="ln">1438 </span></a><span class="s1">at::Tensor &amp; scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, c10::string_view reduce, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1439"><span class="ln">1439 </span></a><span class="s1">at::Tensor scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1440"><span class="ln">1440 </span></a><span class="s1">at::Tensor scatter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1441"><span class="ln">1441 </span></a><span class="s1">at::Tensor scatter_add(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1442"><span class="ln">1442 </span></a><span class="s1">at::Tensor &amp; scatter_add_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1443"><span class="ln">1443 </span></a><span class="s1">at::Tensor &amp; scatter_add_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1444"><span class="ln">1444 </span></a><span class="s1">at::Tensor scatter_add(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1445"><span class="ln">1445 </span></a><span class="s1">at::Tensor scatter_reduce(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::string_view reduce, </span><span class="s2">bool </span><span class="s1">include_self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1446"><span class="ln">1446 </span></a><span class="s1">at::Tensor &amp; scatter_reduce_(at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::string_view reduce, </span><span class="s2">bool </span><span class="s1">include_self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_reduce_.two(Tensor(a!) self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1447"><span class="ln">1447 </span></a><span class="s1">at::Tensor &amp; scatter_reduce_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::string_view reduce, </span><span class="s2">bool </span><span class="s1">include_self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scatter_reduce.two_out(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1448"><span class="ln">1448 </span></a><span class="s1">at::Tensor &amp; eq_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eq_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1449"><span class="ln">1449 </span></a><span class="s1">at::Tensor &amp; eq_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eq_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1450"><span class="ln">1450 </span></a><span class="s1">at::Tensor &amp; bitwise_and_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1451"><span class="ln">1451 </span></a><span class="s1">at::Tensor &amp; bitwise_and_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1452"><span class="ln">1452 </span></a><span class="s1">at::Tensor bitwise_and(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1453"><span class="ln">1453 </span></a><span class="s1">at::Tensor bitwise_and(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and.Scalar_Tensor(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1454"><span class="ln">1454 </span></a><span class="s1">at::Tensor bitwise_and(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1455"><span class="ln">1455 </span></a><span class="s1">at::Tensor &amp; bitwise_and_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1456"><span class="ln">1456 </span></a><span class="s1">at::Tensor &amp; bitwise_and_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1457"><span class="ln">1457 </span></a><span class="s1">at::Tensor __and__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__and__.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1458"><span class="ln">1458 </span></a><span class="s1">at::Tensor __and__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__and__.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1459"><span class="ln">1459 </span></a><span class="s1">at::Tensor &amp; __iand__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1460"><span class="ln">1460 </span></a><span class="s1">at::Tensor &amp; __iand__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1461"><span class="ln">1461 </span></a><span class="s1">at::Tensor &amp; bitwise_or_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1462"><span class="ln">1462 </span></a><span class="s1">at::Tensor &amp; bitwise_or_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1463"><span class="ln">1463 </span></a><span class="s1">at::Tensor bitwise_or(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1464"><span class="ln">1464 </span></a><span class="s1">at::Tensor bitwise_or(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or.Scalar_Tensor(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1465"><span class="ln">1465 </span></a><span class="s1">at::Tensor bitwise_or(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1466"><span class="ln">1466 </span></a><span class="s1">at::Tensor &amp; bitwise_or_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1467"><span class="ln">1467 </span></a><span class="s1">at::Tensor &amp; bitwise_or_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1468"><span class="ln">1468 </span></a><span class="s1">at::Tensor __or__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__or__.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1469"><span class="ln">1469 </span></a><span class="s1">at::Tensor __or__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__or__.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1470"><span class="ln">1470 </span></a><span class="s1">at::Tensor &amp; __ior__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1471"><span class="ln">1471 </span></a><span class="s1">at::Tensor &amp; __ior__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1472"><span class="ln">1472 </span></a><span class="s1">at::Tensor &amp; bitwise_xor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1473"><span class="ln">1473 </span></a><span class="s1">at::Tensor &amp; bitwise_xor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1474"><span class="ln">1474 </span></a><span class="s1">at::Tensor bitwise_xor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1475"><span class="ln">1475 </span></a><span class="s1">at::Tensor bitwise_xor(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor.Scalar_Tensor(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1476"><span class="ln">1476 </span></a><span class="s1">at::Tensor bitwise_xor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1477"><span class="ln">1477 </span></a><span class="s1">at::Tensor &amp; bitwise_xor_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1478"><span class="ln">1478 </span></a><span class="s1">at::Tensor &amp; bitwise_xor_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1479"><span class="ln">1479 </span></a><span class="s1">at::Tensor __xor__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__xor__.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1480"><span class="ln">1480 </span></a><span class="s1">at::Tensor __xor__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__xor__.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1481"><span class="ln">1481 </span></a><span class="s1">at::Tensor &amp; __ixor__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1482"><span class="ln">1482 </span></a><span class="s1">at::Tensor &amp; __ixor__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1483"><span class="ln">1483 </span></a><span class="s1">at::Tensor __lshift__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__lshift__.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1484"><span class="ln">1484 </span></a><span class="s1">at::Tensor __lshift__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__lshift__.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1485"><span class="ln">1485 </span></a><span class="s1">at::Tensor &amp; __ilshift__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1486"><span class="ln">1486 </span></a><span class="s1">at::Tensor &amp; __ilshift__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1487"><span class="ln">1487 </span></a><span class="s1">at::Tensor bitwise_left_shift(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1488"><span class="ln">1488 </span></a><span class="s1">at::Tensor &amp; bitwise_left_shift_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1489"><span class="ln">1489 </span></a><span class="s1">at::Tensor &amp; bitwise_left_shift_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1490"><span class="ln">1490 </span></a><span class="s1">at::Tensor bitwise_left_shift(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1491"><span class="ln">1491 </span></a><span class="s1">at::Tensor &amp; bitwise_left_shift_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1492"><span class="ln">1492 </span></a><span class="s1">at::Tensor &amp; bitwise_left_shift_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1493"><span class="ln">1493 </span></a><span class="s1">at::Tensor bitwise_left_shift(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1494"><span class="ln">1494 </span></a><span class="s1">at::Tensor __rshift__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__rshift__.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1495"><span class="ln">1495 </span></a><span class="s1">at::Tensor __rshift__(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__rshift__.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1496"><span class="ln">1496 </span></a><span class="s1">at::Tensor &amp; __irshift__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1497"><span class="ln">1497 </span></a><span class="s1">at::Tensor &amp; __irshift__(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1498"><span class="ln">1498 </span></a><span class="s1">at::Tensor bitwise_right_shift(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1499"><span class="ln">1499 </span></a><span class="s1">at::Tensor &amp; bitwise_right_shift_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1500"><span class="ln">1500 </span></a><span class="s1">at::Tensor &amp; bitwise_right_shift_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1501"><span class="ln">1501 </span></a><span class="s1">at::Tensor bitwise_right_shift(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1502"><span class="ln">1502 </span></a><span class="s1">at::Tensor &amp; bitwise_right_shift_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1503"><span class="ln">1503 </span></a><span class="s1">at::Tensor &amp; bitwise_right_shift_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1504"><span class="ln">1504 </span></a><span class="s1">at::Tensor bitwise_right_shift(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1505"><span class="ln">1505 </span></a><span class="s1">at::Tensor &amp; tril_(at::Tensor &amp; self, int64_t diagonal); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tril_(Tensor(a!) self, int diagonal=0) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1506"><span class="ln">1506 </span></a><span class="s1">at::Tensor &amp; triu_(at::Tensor &amp; self, int64_t diagonal); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triu_(Tensor(a!) self, int diagonal=0) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1507"><span class="ln">1507 </span></a><span class="s1">at::Tensor &amp; digamma_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::digamma_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1508"><span class="ln">1508 </span></a><span class="s1">at::Tensor &amp; lerp_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1509"><span class="ln">1509 </span></a><span class="s1">at::Tensor &amp; lerp_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1510"><span class="ln">1510 </span></a><span class="s1">at::Tensor &amp; addbmm_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1511"><span class="ln">1511 </span></a><span class="s1">at::Tensor &amp; addbmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1512"><span class="ln">1512 </span></a><span class="s1">at::Tensor addbmm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; batch2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1513"><span class="ln">1513 </span></a><span class="s1">at::Tensor &amp; random_(at::Tensor &amp; self, int64_t from, ::std::optional&lt;int64_t&gt; to, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1514"><span class="ln">1514 </span></a><span class="s1">at::Tensor &amp; random_(at::Tensor &amp; self, int64_t to, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1515"><span class="ln">1515 </span></a><span class="s1">at::Tensor &amp; random_(at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random_(Tensor(a!) self, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1516"><span class="ln">1516 </span></a><span class="s1">at::Tensor &amp; uniform_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">from, </span><span class="s2">double </span><span class="s1">to, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1517"><span class="ln">1517 </span></a><span class="s1">at::Tensor &amp; cauchy_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">median, </span><span class="s2">double </span><span class="s1">sigma, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1518"><span class="ln">1518 </span></a><span class="s1">at::Tensor &amp; log_normal_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1519"><span class="ln">1519 </span></a><span class="s1">at::Tensor &amp; exponential_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">lambd, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1520"><span class="ln">1520 </span></a><span class="s1">at::Tensor &amp; geometric_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1521"><span class="ln">1521 </span></a><span class="s1">at::Tensor &amp; diag_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t diagonal, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1522"><span class="ln">1522 </span></a><span class="s1">at::Tensor diag(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t diagonal); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diag(Tensor self, int diagonal=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1523"><span class="ln">1523 </span></a><span class="s1">at::Tensor &amp; cross_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;int64_t&gt; dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1524"><span class="ln">1524 </span></a><span class="s1">at::Tensor cross(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::optional&lt;int64_t&gt; dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cross(Tensor self, Tensor other, int? dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1525"><span class="ln">1525 </span></a><span class="s1">at::Tensor &amp; triu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t diagonal, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1526"><span class="ln">1526 </span></a><span class="s1">at::Tensor triu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t diagonal); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triu(Tensor self, int diagonal=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1527"><span class="ln">1527 </span></a><span class="s1">at::Tensor &amp; tril_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t diagonal, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1528"><span class="ln">1528 </span></a><span class="s1">at::Tensor tril(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t diagonal); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tril(Tensor self, int diagonal=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1529"><span class="ln">1529 </span></a><span class="s1">at::Tensor tril_indices(int64_t row, int64_t col, int64_t offset, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1530"><span class="ln">1530 </span></a><span class="s1">at::Tensor triu_indices(int64_t row, int64_t col, int64_t offset, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1531"><span class="ln">1531 </span></a><span class="s1">at::Tensor trace(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trace(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1532"><span class="ln">1532 </span></a><span class="s1">at::Tensor trace_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, c10::SymIntArrayRef sizes); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trace_backward(Tensor grad, SymInt[] sizes) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1533"><span class="ln">1533 </span></a><span class="s1">at::Tensor &amp; ne_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1534"><span class="ln">1534 </span></a><span class="s1">at::Tensor ne(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ne.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1535"><span class="ln">1535 </span></a><span class="s1">at::Tensor &amp; ne_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1536"><span class="ln">1536 </span></a><span class="s1">at::Tensor ne(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ne.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1537"><span class="ln">1537 </span></a><span class="s1">at::Tensor &amp; ne_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ne_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1538"><span class="ln">1538 </span></a><span class="s1">at::Tensor &amp; ne_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ne_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1539"><span class="ln">1539 </span></a><span class="s1">at::Tensor &amp; not_equal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1540"><span class="ln">1540 </span></a><span class="s1">at::Tensor not_equal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::not_equal.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1541"><span class="ln">1541 </span></a><span class="s1">at::Tensor &amp; not_equal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1542"><span class="ln">1542 </span></a><span class="s1">at::Tensor not_equal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::not_equal.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1543"><span class="ln">1543 </span></a><span class="s1">at::Tensor &amp; not_equal_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1544"><span class="ln">1544 </span></a><span class="s1">at::Tensor &amp; not_equal_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1545"><span class="ln">1545 </span></a><span class="s1">at::Tensor &amp; eq_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1546"><span class="ln">1546 </span></a><span class="s1">at::Tensor eq(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eq.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1547"><span class="ln">1547 </span></a><span class="s1">at::Tensor &amp; eq_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1548"><span class="ln">1548 </span></a><span class="s1">at::Tensor eq(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::eq.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1549"><span class="ln">1549 </span></a><span class="s1">at::Tensor &amp; ge_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1550"><span class="ln">1550 </span></a><span class="s1">at::Tensor ge(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ge.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1551"><span class="ln">1551 </span></a><span class="s1">at::Tensor &amp; ge_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1552"><span class="ln">1552 </span></a><span class="s1">at::Tensor ge(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ge.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1553"><span class="ln">1553 </span></a><span class="s1">at::Tensor &amp; ge_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ge_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1554"><span class="ln">1554 </span></a><span class="s1">at::Tensor &amp; ge_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ge_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1555"><span class="ln">1555 </span></a><span class="s1">at::Tensor &amp; greater_equal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1556"><span class="ln">1556 </span></a><span class="s1">at::Tensor greater_equal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_equal.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1557"><span class="ln">1557 </span></a><span class="s1">at::Tensor &amp; greater_equal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1558"><span class="ln">1558 </span></a><span class="s1">at::Tensor greater_equal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_equal.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1559"><span class="ln">1559 </span></a><span class="s1">at::Tensor &amp; greater_equal_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1560"><span class="ln">1560 </span></a><span class="s1">at::Tensor &amp; greater_equal_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1561"><span class="ln">1561 </span></a><span class="s1">at::Tensor &amp; le_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1562"><span class="ln">1562 </span></a><span class="s1">at::Tensor le(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::le.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1563"><span class="ln">1563 </span></a><span class="s1">at::Tensor &amp; le_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1564"><span class="ln">1564 </span></a><span class="s1">at::Tensor le(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::le.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1565"><span class="ln">1565 </span></a><span class="s1">at::Tensor &amp; le_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::le_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1566"><span class="ln">1566 </span></a><span class="s1">at::Tensor &amp; le_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::le_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1567"><span class="ln">1567 </span></a><span class="s1">at::Tensor &amp; less_equal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1568"><span class="ln">1568 </span></a><span class="s1">at::Tensor less_equal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_equal.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1569"><span class="ln">1569 </span></a><span class="s1">at::Tensor &amp; less_equal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1570"><span class="ln">1570 </span></a><span class="s1">at::Tensor less_equal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_equal.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1571"><span class="ln">1571 </span></a><span class="s1">at::Tensor &amp; less_equal_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1572"><span class="ln">1572 </span></a><span class="s1">at::Tensor &amp; less_equal_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1573"><span class="ln">1573 </span></a><span class="s1">at::Tensor &amp; gt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1574"><span class="ln">1574 </span></a><span class="s1">at::Tensor gt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gt.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1575"><span class="ln">1575 </span></a><span class="s1">at::Tensor &amp; gt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1576"><span class="ln">1576 </span></a><span class="s1">at::Tensor gt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gt.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1577"><span class="ln">1577 </span></a><span class="s1">at::Tensor &amp; gt_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gt_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1578"><span class="ln">1578 </span></a><span class="s1">at::Tensor &amp; gt_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gt_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1579"><span class="ln">1579 </span></a><span class="s1">at::Tensor &amp; greater_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1580"><span class="ln">1580 </span></a><span class="s1">at::Tensor greater(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1581"><span class="ln">1581 </span></a><span class="s1">at::Tensor &amp; greater_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1582"><span class="ln">1582 </span></a><span class="s1">at::Tensor greater(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1583"><span class="ln">1583 </span></a><span class="s1">at::Tensor &amp; greater_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1584"><span class="ln">1584 </span></a><span class="s1">at::Tensor &amp; greater_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::greater_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1585"><span class="ln">1585 </span></a><span class="s1">at::Tensor &amp; lt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1586"><span class="ln">1586 </span></a><span class="s1">at::Tensor lt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lt.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1587"><span class="ln">1587 </span></a><span class="s1">at::Tensor &amp; lt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1588"><span class="ln">1588 </span></a><span class="s1">at::Tensor lt(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lt.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1589"><span class="ln">1589 </span></a><span class="s1">at::Tensor &amp; lt_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lt_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1590"><span class="ln">1590 </span></a><span class="s1">at::Tensor &amp; lt_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lt_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1591"><span class="ln">1591 </span></a><span class="s1">at::Tensor &amp; less_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1592"><span class="ln">1592 </span></a><span class="s1">at::Tensor less(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1593"><span class="ln">1593 </span></a><span class="s1">at::Tensor &amp; less_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1594"><span class="ln">1594 </span></a><span class="s1">at::Tensor less(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1595"><span class="ln">1595 </span></a><span class="s1">at::Tensor &amp; less_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1596"><span class="ln">1596 </span></a><span class="s1">at::Tensor &amp; less_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::less_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1597"><span class="ln">1597 </span></a><span class="s1">at::Tensor &amp; take_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1598"><span class="ln">1598 </span></a><span class="s1">at::Tensor take(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::take(Tensor self, Tensor index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1599"><span class="ln">1599 </span></a><span class="s1">at::Tensor &amp; take_along_dim_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, ::std::optional&lt;int64_t&gt; dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1600"><span class="ln">1600 </span></a><span class="s1">at::Tensor take_along_dim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, ::std::optional&lt;int64_t&gt; dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1601"><span class="ln">1601 </span></a><span class="s1">at::Tensor &amp; index_select_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1602"><span class="ln">1602 </span></a><span class="s1">at::Tensor index_select(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_select(Tensor self, int dim, Tensor index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1603"><span class="ln">1603 </span></a><span class="s1">at::Tensor &amp; index_select_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1604"><span class="ln">1604 </span></a><span class="s1">at::Tensor index_select(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1605"><span class="ln">1605 </span></a><span class="s1">at::Tensor index_select_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, c10::SymIntArrayRef self_sizes, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_select_backward(Tensor grad, SymInt[] self_sizes, int dim, Tensor index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1606"><span class="ln">1606 </span></a><span class="s1">at::Tensor &amp; masked_select_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1607"><span class="ln">1607 </span></a><span class="s1">at::Tensor masked_select(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_select(Tensor self, Tensor mask) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1608"><span class="ln">1608 </span></a><span class="s1">at::Tensor masked_select_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_select_backward(Tensor grad, Tensor input, Tensor mask) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1609"><span class="ln">1609 </span></a><span class="s1">at::Tensor &amp; nonzero_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nonzero.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1610"><span class="ln">1610 </span></a><span class="s1">at::Tensor nonzero(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nonzero(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1611"><span class="ln">1611 </span></a><span class="s1">at::Tensor &amp; nonzero_static_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt size, int64_t fill_value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nonzero_static.out(Tensor self, *, SymInt size, int fill_value=-1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1612"><span class="ln">1612 </span></a><span class="s1">at::Tensor nonzero_static(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt size, int64_t fill_value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nonzero_static(Tensor self, *, SymInt size, int fill_value=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1613"><span class="ln">1613 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; nonzero_numpy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nonzero_numpy(Tensor self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1614"><span class="ln">1614 </span></a><span class="s1">at::Tensor argwhere(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argwhere(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1615"><span class="ln">1615 </span></a><span class="s1">at::Tensor &amp; gather_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">bool </span><span class="s1">sparse_grad, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1616"><span class="ln">1616 </span></a><span class="s1">at::Tensor gather(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">bool </span><span class="s1">sparse_grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1617"><span class="ln">1617 </span></a><span class="s1">at::Tensor gather_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">bool </span><span class="s1">sparse_grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1618"><span class="ln">1618 </span></a><span class="s1">at::Tensor &amp; gather_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">bool </span><span class="s1">sparse_grad, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1619"><span class="ln">1619 </span></a><span class="s1">at::Tensor gather(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">bool </span><span class="s1">sparse_grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1620"><span class="ln">1620 </span></a><span class="s1">at::Tensor _gather_sparse_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1621"><span class="ln">1621 </span></a><span class="s1">at::Tensor &amp; addcmul_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1622"><span class="ln">1622 </span></a><span class="s1">at::Tensor addcmul(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1623"><span class="ln">1623 </span></a><span class="s1">at::Tensor &amp; addcmul_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1624"><span class="ln">1624 </span></a><span class="s1">at::Tensor &amp; addcdiv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1625"><span class="ln">1625 </span></a><span class="s1">at::Tensor addcdiv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1626"><span class="ln">1626 </span></a><span class="s1">at::Tensor &amp; addcdiv_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1627"><span class="ln">1627 </span></a><span class="s1">at::Tensor cross_entropy_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, </span><span class="s2">double </span><span class="s1">label_smoothing); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1628"><span class="ln">1628 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; triangular_solve_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">upper, </span><span class="s2">bool </span><span class="s1">transpose, </span><span class="s2">bool </span><span class="s1">unitriangular, at::Tensor &amp; X, at::Tensor &amp; M); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -&gt; (Tensor(a!) solution, Tensor(b!) cloned_coefficient)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1629"><span class="ln">1629 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; triangular_solve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">upper, </span><span class="s2">bool </span><span class="s1">transpose, </span><span class="s2">bool </span><span class="s1">unitriangular); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -&gt; (Tensor solution, Tensor cloned_coefficient)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1630"><span class="ln">1630 </span></a><span class="s2">void </span><span class="s1">_linalg_check_errors(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; info, c10::string_view api_name, </span><span class="s2">bool </span><span class="s1">is_matrix); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_check_errors(Tensor info, str api_name, *, bool is_matrix) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1631"><span class="ln">1631 </span></a><span class="s1">at::Tensor &amp; linalg_solve_triangular_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">upper, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">unitriangular, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1632"><span class="ln">1632 </span></a><span class="s1">at::Tensor linalg_solve_triangular(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">upper, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">unitriangular); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1633"><span class="ln">1633 </span></a><span class="s1">at::Tensor linalg_vander(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, ::std::optional&lt;c10::SymInt&gt; N); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_vander(Tensor x, *, SymInt? N=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1634"><span class="ln">1634 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; svd_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">some, </span><span class="s2">bool </span><span class="s1">compute_uv, at::Tensor &amp; U, at::Tensor &amp; S, at::Tensor &amp; V); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -&gt; (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1635"><span class="ln">1635 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; svd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">some, </span><span class="s2">bool </span><span class="s1">compute_uv); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::svd(Tensor self, bool some=True, bool compute_uv=True) -&gt; (Tensor U, Tensor S, Tensor V)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1636"><span class="ln">1636 </span></a><span class="s1">at::Tensor swapaxes(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t axis0, int64_t axis1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::swapaxes(Tensor(a) self, int axis0, int axis1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1637"><span class="ln">1637 </span></a><span class="s1">at::Tensor &amp; swapaxes_(at::Tensor &amp; self, int64_t axis0, int64_t axis1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1638"><span class="ln">1638 </span></a><span class="s1">at::Tensor swapdims(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim0, int64_t dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::swapdims(Tensor(a) self, int dim0, int dim1) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1639"><span class="ln">1639 </span></a><span class="s1">at::Tensor &amp; swapdims_(at::Tensor &amp; self, int64_t dim0, int64_t dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1640"><span class="ln">1640 </span></a><span class="s1">at::Tensor &amp; cholesky_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1641"><span class="ln">1641 </span></a><span class="s1">at::Tensor cholesky(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cholesky(Tensor self, bool upper=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1642"><span class="ln">1642 </span></a><span class="s1">at::Tensor &amp; cholesky_solve_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, </span><span class="s2">bool </span><span class="s1">upper, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1643"><span class="ln">1643 </span></a><span class="s1">at::Tensor cholesky_solve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, </span><span class="s2">bool </span><span class="s1">upper); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1644"><span class="ln">1644 </span></a><span class="s1">at::Tensor _cholesky_solve_helper(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">upper); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1645"><span class="ln">1645 </span></a><span class="s1">at::Tensor cholesky_inverse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cholesky_inverse(Tensor self, bool upper=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1646"><span class="ln">1646 </span></a><span class="s1">at::Tensor &amp; cholesky_inverse_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1647"><span class="ln">1647 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; qr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">some, at::Tensor &amp; Q, at::Tensor &amp; R); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -&gt; (Tensor(a!) Q, Tensor(b!) R)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1648"><span class="ln">1648 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; qr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">some); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::qr(Tensor self, bool some=True) -&gt; (Tensor Q, Tensor R)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1649"><span class="ln">1649 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; geqrf_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; a, at::Tensor &amp; tau); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -&gt; (Tensor(a!) a, Tensor(b!) tau)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1650"><span class="ln">1650 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; geqrf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::geqrf(Tensor self) -&gt; (Tensor a, Tensor tau)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1651"><span class="ln">1651 </span></a><span class="s1">at::Tensor orgqr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::orgqr(Tensor self, Tensor input2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1652"><span class="ln">1652 </span></a><span class="s1">at::Tensor &amp; orgqr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1653"><span class="ln">1653 </span></a><span class="s1">at::Tensor &amp; ormqr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input3, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">transpose, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1654"><span class="ln">1654 </span></a><span class="s1">at::Tensor ormqr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input3, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">transpose); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1655"><span class="ln">1655 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _lu_with_info(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">pivot, </span><span class="s2">bool </span><span class="s1">check_errors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -&gt; (Tensor LU, Tensor pivots, Tensor info)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1656"><span class="ln">1656 </span></a><span class="s1">at::Tensor &amp; lu_solve_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_pivots, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1657"><span class="ln">1657 </span></a><span class="s1">at::Tensor lu_solve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_pivots); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1658"><span class="ln">1658 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; lu_unpack(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_pivots, </span><span class="s2">bool </span><span class="s1">unpack_data, </span><span class="s2">bool </span><span class="s1">unpack_pivots); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -&gt; (Tensor P, Tensor L, Tensor U)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1659"><span class="ln">1659 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; lu_unpack_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_data, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU_pivots, </span><span class="s2">bool </span><span class="s1">unpack_data, </span><span class="s2">bool </span><span class="s1">unpack_pivots, at::Tensor &amp; P, at::Tensor &amp; L, at::Tensor &amp; U); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -&gt; (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1660"><span class="ln">1660 </span></a><span class="s1">at::Tensor &amp; multinomial_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt num_samples, </span><span class="s2">bool </span><span class="s1">replacement, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multinomial.out(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1661"><span class="ln">1661 </span></a><span class="s1">at::Tensor multinomial(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt num_samples, </span><span class="s2">bool </span><span class="s1">replacement, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multinomial(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1662"><span class="ln">1662 </span></a><span class="s1">at::Tensor &amp; lgamma_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lgamma.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1663"><span class="ln">1663 </span></a><span class="s1">at::Tensor &amp; lgamma_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lgamma_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1664"><span class="ln">1664 </span></a><span class="s1">at::Tensor lgamma(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lgamma(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1665"><span class="ln">1665 </span></a><span class="s1">at::Tensor &amp; digamma_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::digamma.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1666"><span class="ln">1666 </span></a><span class="s1">at::Tensor digamma(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::digamma(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1667"><span class="ln">1667 </span></a><span class="s1">at::Tensor &amp; polygamma_out(int64_t n, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1668"><span class="ln">1668 </span></a><span class="s1">at::Tensor polygamma(int64_t n, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::polygamma(int n, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1669"><span class="ln">1669 </span></a><span class="s1">at::Tensor &amp; polygamma_(at::Tensor &amp; self, int64_t n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::polygamma_(Tensor(a!) self, int n) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1670"><span class="ln">1670 </span></a><span class="s1">at::Tensor erfinv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erfinv(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1671"><span class="ln">1671 </span></a><span class="s1">at::Tensor &amp; erfinv_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erfinv_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1672"><span class="ln">1672 </span></a><span class="s1">at::Tensor &amp; erfinv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::erfinv.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1673"><span class="ln">1673 </span></a><span class="s1">at::Tensor i0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::i0(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1674"><span class="ln">1674 </span></a><span class="s1">at::Tensor &amp; i0_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::i0_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1675"><span class="ln">1675 </span></a><span class="s1">at::Tensor &amp; i0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::i0.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1676"><span class="ln">1676 </span></a><span class="s1">at::Tensor sign(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sign(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1677"><span class="ln">1677 </span></a><span class="s1">at::Tensor &amp; sign_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sign_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1678"><span class="ln">1678 </span></a><span class="s1">at::Tensor &amp; sign_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sign.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1679"><span class="ln">1679 </span></a><span class="s1">at::Tensor signbit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::signbit(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1680"><span class="ln">1680 </span></a><span class="s1">at::Tensor &amp; signbit_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::signbit.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1681"><span class="ln">1681 </span></a><span class="s1">at::Tensor dist(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dist(Tensor self, Tensor other, Scalar p=2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1682"><span class="ln">1682 </span></a><span class="s1">at::Tensor &amp; atan2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1683"><span class="ln">1683 </span></a><span class="s1">at::Tensor &amp; atan2_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atan2_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1684"><span class="ln">1684 </span></a><span class="s1">at::Tensor atan2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::atan2(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1685"><span class="ln">1685 </span></a><span class="s1">at::Tensor arctan2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctan2(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1686"><span class="ln">1686 </span></a><span class="s1">at::Tensor &amp; arctan2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1687"><span class="ln">1687 </span></a><span class="s1">at::Tensor &amp; arctan2_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::arctan2_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1688"><span class="ln">1688 </span></a><span class="s1">at::Tensor &amp; lerp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1689"><span class="ln">1689 </span></a><span class="s1">at::Tensor &amp; lerp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1690"><span class="ln">1690 </span></a><span class="s1">at::Tensor lerp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1691"><span class="ln">1691 </span></a><span class="s1">at::Tensor lerp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; end, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1692"><span class="ln">1692 </span></a><span class="s1">at::Tensor &amp; histc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t bins, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1693"><span class="ln">1693 </span></a><span class="s1">at::Tensor histc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t bins, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1694"><span class="ln">1694 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; histogram_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bins, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density, at::Tensor &amp; hist, at::Tensor &amp; bin_edges); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -&gt; (Tensor(a!) hist, Tensor(b!) bin_edges)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1695"><span class="ln">1695 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; histogram(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bins, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -&gt; (Tensor hist, Tensor bin_edges)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1696"><span class="ln">1696 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; histogram_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density, at::Tensor &amp; hist, at::Tensor &amp; bin_edges); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -&gt; (Tensor(a!) hist, Tensor(b!) bin_edges)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1697"><span class="ln">1697 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; histogram(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -&gt; (Tensor hist, Tensor bin_edges)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1698"><span class="ln">1698 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _histogramdd_bin_edges(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_histogramdd_bin_edges(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1699"><span class="ln">1699 </span></a><span class="s1">at::Tensor _histogramdd_from_bin_cts(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_histogramdd_from_bin_cts(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1700"><span class="ln">1700 </span></a><span class="s1">at::Tensor _histogramdd_from_bin_tensors(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::TensorList bins, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_histogramdd_from_bin_tensors(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1701"><span class="ln">1701 </span></a><span class="s1">::std::tuple&lt;at::Tensor,::std::vector&lt;at::Tensor&gt;&gt; histogramdd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histogramdd(Tensor self, int[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -&gt; (Tensor hist, Tensor[] bin_edges)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1702"><span class="ln">1702 </span></a><span class="s1">::std::tuple&lt;at::Tensor,::std::vector&lt;at::Tensor&gt;&gt; histogramdd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histogramdd.int_bins(Tensor self, int bins, float[]? range=None, Tensor? weight=None, bool density=False) -&gt; (Tensor hist, Tensor[] bin_edges)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1703"><span class="ln">1703 </span></a><span class="s1">::std::tuple&lt;at::Tensor,::std::vector&lt;at::Tensor&gt;&gt; histogramdd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::TensorList bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::histogramdd.TensorList_bins(Tensor self, Tensor[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -&gt; (Tensor hist, Tensor[] bin_edges)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1704"><span class="ln">1704 </span></a><span class="s1">at::Tensor &amp; fmod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1705"><span class="ln">1705 </span></a><span class="s1">at::Tensor fmod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmod.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1706"><span class="ln">1706 </span></a><span class="s1">at::Tensor &amp; fmod_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1707"><span class="ln">1707 </span></a><span class="s1">at::Tensor &amp; fmod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1708"><span class="ln">1708 </span></a><span class="s1">at::Tensor fmod(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmod.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1709"><span class="ln">1709 </span></a><span class="s1">at::Tensor &amp; fmod_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1710"><span class="ln">1710 </span></a><span class="s1">at::Tensor &amp; hypot_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1711"><span class="ln">1711 </span></a><span class="s1">at::Tensor hypot(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hypot(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1712"><span class="ln">1712 </span></a><span class="s1">at::Tensor &amp; hypot_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hypot_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1713"><span class="ln">1713 </span></a><span class="s1">at::Tensor &amp; igamma_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1714"><span class="ln">1714 </span></a><span class="s1">at::Tensor igamma(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::igamma(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1715"><span class="ln">1715 </span></a><span class="s1">at::Tensor &amp; igamma_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::igamma_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1716"><span class="ln">1716 </span></a><span class="s1">at::Tensor &amp; igammac_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1717"><span class="ln">1717 </span></a><span class="s1">at::Tensor igammac(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::igammac(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1718"><span class="ln">1718 </span></a><span class="s1">at::Tensor &amp; igammac_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::igammac_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1719"><span class="ln">1719 </span></a><span class="s1">at::Tensor &amp; nextafter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1720"><span class="ln">1720 </span></a><span class="s1">at::Tensor nextafter(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nextafter(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1721"><span class="ln">1721 </span></a><span class="s1">at::Tensor &amp; nextafter_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nextafter_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1722"><span class="ln">1722 </span></a><span class="s1">at::Tensor &amp; remainder_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1723"><span class="ln">1723 </span></a><span class="s1">at::Tensor remainder(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder.Scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1724"><span class="ln">1724 </span></a><span class="s1">at::Tensor &amp; remainder_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1725"><span class="ln">1725 </span></a><span class="s1">at::Tensor &amp; remainder_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1726"><span class="ln">1726 </span></a><span class="s1">at::Tensor remainder(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder.Tensor(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1727"><span class="ln">1727 </span></a><span class="s1">at::Tensor &amp; remainder_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1728"><span class="ln">1728 </span></a><span class="s1">at::Tensor remainder(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1729"><span class="ln">1729 </span></a><span class="s1">at::Tensor min(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1730"><span class="ln">1730 </span></a><span class="s1">at::Tensor &amp; min_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min.unary_out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1731"><span class="ln">1731 </span></a><span class="s1">at::Tensor fmin(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmin(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1732"><span class="ln">1732 </span></a><span class="s1">at::Tensor &amp; fmin_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1733"><span class="ln">1733 </span></a><span class="s1">at::Tensor max(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1734"><span class="ln">1734 </span></a><span class="s1">at::Tensor fmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmax(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1735"><span class="ln">1735 </span></a><span class="s1">at::Tensor &amp; fmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1736"><span class="ln">1736 </span></a><span class="s1">at::Tensor maximum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::maximum(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1737"><span class="ln">1737 </span></a><span class="s1">at::Tensor &amp; maximum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1738"><span class="ln">1738 </span></a><span class="s1">at::Tensor max(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max.other(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1739"><span class="ln">1739 </span></a><span class="s1">at::Tensor &amp; max_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1740"><span class="ln">1740 </span></a><span class="s1">at::Tensor &amp; max_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max.unary_out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1741"><span class="ln">1741 </span></a><span class="s1">at::Tensor minimum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::minimum(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1742"><span class="ln">1742 </span></a><span class="s1">at::Tensor &amp; minimum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1743"><span class="ln">1743 </span></a><span class="s1">at::Tensor &amp; min_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1744"><span class="ln">1744 </span></a><span class="s1">at::Tensor min(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::min.other(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1745"><span class="ln">1745 </span></a><span class="s1">at::Tensor quantile(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1746"><span class="ln">1746 </span></a><span class="s1">at::Tensor &amp; quantile_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1747"><span class="ln">1747 </span></a><span class="s1">at::Tensor quantile(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1748"><span class="ln">1748 </span></a><span class="s1">at::Tensor &amp; quantile_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1749"><span class="ln">1749 </span></a><span class="s1">at::Tensor nanquantile(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1750"><span class="ln">1750 </span></a><span class="s1">at::Tensor &amp; nanquantile_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1751"><span class="ln">1751 </span></a><span class="s1">at::Tensor nanquantile(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1752"><span class="ln">1752 </span></a><span class="s1">at::Tensor &amp; nanquantile_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">q, ::std::optional&lt;int64_t&gt; dim, </span><span class="s2">bool </span><span class="s1">keepdim, c10::string_view interpolation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1753"><span class="ln">1753 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; sort_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">descending, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1754"><span class="ln">1754 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; sort_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; stable, int64_t dim, </span><span class="s2">bool </span><span class="s1">descending, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1755"><span class="ln">1755 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; sort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">descending); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort(Tensor self, int dim=-1, bool descending=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1756"><span class="ln">1756 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; sort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; stable, int64_t dim, </span><span class="s2">bool </span><span class="s1">descending); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1757"><span class="ln">1757 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; sort_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">descending, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1758"><span class="ln">1758 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; sort_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; stable, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">descending, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1759"><span class="ln">1759 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; sort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">descending); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1760"><span class="ln">1760 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; sort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; stable, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">descending); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1761"><span class="ln">1761 </span></a><span class="s1">at::Tensor &amp; msort_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::msort.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1762"><span class="ln">1762 </span></a><span class="s1">at::Tensor msort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::msort(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1763"><span class="ln">1763 </span></a><span class="s1">at::Tensor argsort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">descending); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argsort(Tensor self, int dim=-1, bool descending=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1764"><span class="ln">1764 </span></a><span class="s1">at::Tensor argsort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">stable, int64_t dim, </span><span class="s2">bool </span><span class="s1">descending); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1765"><span class="ln">1765 </span></a><span class="s1">at::Tensor &amp; argsort_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">stable, int64_t dim, </span><span class="s2">bool </span><span class="s1">descending, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1766"><span class="ln">1766 </span></a><span class="s1">at::Tensor argsort(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Dimname dim, </span><span class="s2">bool </span><span class="s1">descending); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1767"><span class="ln">1767 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; topk_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt k, int64_t dim, </span><span class="s2">bool </span><span class="s1">largest, </span><span class="s2">bool </span><span class="s1">sorted, at::Tensor &amp; values, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -&gt; (Tensor(a!) values, Tensor(b!) indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1768"><span class="ln">1768 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; topk(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt k, int64_t dim, </span><span class="s2">bool </span><span class="s1">largest, </span><span class="s2">bool </span><span class="s1">sorted); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -&gt; (Tensor values, Tensor indices)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1769"><span class="ln">1769 </span></a><span class="s1">at::Tensor all(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1770"><span class="ln">1770 </span></a><span class="s1">at::Tensor &amp; all_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::all.all_out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1771"><span class="ln">1771 </span></a><span class="s1">at::Tensor any(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1772"><span class="ln">1772 </span></a><span class="s1">at::Tensor &amp; any_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::any.all_out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1773"><span class="ln">1773 </span></a><span class="s1">at::Tensor &amp; renorm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; maxnorm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1774"><span class="ln">1774 </span></a><span class="s1">at::Tensor renorm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; maxnorm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1775"><span class="ln">1775 </span></a><span class="s1">at::Tensor &amp; renorm_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; maxnorm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1776"><span class="ln">1776 </span></a><span class="s1">at::Tensor unfold(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dimension, int64_t size, int64_t step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unfold(Tensor(a) self, int dimension, int size, int step) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1777"><span class="ln">1777 </span></a><span class="s1">at::Tensor unfold_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_in, c10::SymIntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1778"><span class="ln">1778 </span></a><span class="s2">bool </span><span class="s1">equal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::equal(Tensor self, Tensor other) -&gt; bool&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1779"><span class="ln">1779 </span></a><span class="s1">at::Tensor &amp; pow_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1780"><span class="ln">1780 </span></a><span class="s1">at::Tensor pow(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1781"><span class="ln">1781 </span></a><span class="s1">at::Tensor &amp; pow_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1782"><span class="ln">1782 </span></a><span class="s1">at::Tensor pow(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow.Scalar(Scalar self, Tensor exponent) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1783"><span class="ln">1783 </span></a><span class="s1">at::Tensor &amp; pow_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1784"><span class="ln">1784 </span></a><span class="s1">at::Tensor pow(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1785"><span class="ln">1785 </span></a><span class="s1">at::Tensor &amp; pow_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1786"><span class="ln">1786 </span></a><span class="s1">at::Tensor &amp; pow_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1787"><span class="ln">1787 </span></a><span class="s1">at::Tensor &amp; float_power_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1788"><span class="ln">1788 </span></a><span class="s1">at::Tensor float_power(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1789"><span class="ln">1789 </span></a><span class="s1">at::Tensor &amp; float_power_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1790"><span class="ln">1790 </span></a><span class="s1">at::Tensor float_power(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power.Scalar(Scalar self, Tensor exponent) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1791"><span class="ln">1791 </span></a><span class="s1">at::Tensor &amp; float_power_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1792"><span class="ln">1792 </span></a><span class="s1">at::Tensor float_power(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1793"><span class="ln">1793 </span></a><span class="s1">at::Tensor &amp; float_power_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1794"><span class="ln">1794 </span></a><span class="s1">at::Tensor &amp; float_power_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1795"><span class="ln">1795 </span></a><span class="s1">at::Tensor &amp; normal_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1796"><span class="ln">1796 </span></a><span class="s1">at::Tensor normal_functional(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1797"><span class="ln">1797 </span></a><span class="s1">at::Tensor &amp; normal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1798"><span class="ln">1798 </span></a><span class="s1">at::Tensor normal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1799"><span class="ln">1799 </span></a><span class="s1">at::Tensor &amp; normal_out(</span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; std, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1800"><span class="ln">1800 </span></a><span class="s1">at::Tensor normal(</span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; std, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1801"><span class="ln">1801 </span></a><span class="s1">at::Tensor &amp; normal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; std, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1802"><span class="ln">1802 </span></a><span class="s1">at::Tensor normal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; std, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1803"><span class="ln">1803 </span></a><span class="s1">at::Tensor normal(</span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1804"><span class="ln">1804 </span></a><span class="s1">at::Tensor &amp; normal_out(</span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1805"><span class="ln">1805 </span></a><span class="s1">at::Tensor alias(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::alias(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1806"><span class="ln">1806 </span></a><span class="s2">void </span><span class="s1">_amp_foreach_non_finite_check_and_unscale_(at::TensorList self, at::Tensor &amp; found_inf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; inv_scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1807"><span class="ln">1807 </span></a><span class="s1">at::Tensor &amp; _amp_update_scale_(at::Tensor &amp; self, at::Tensor &amp; growth_tracker, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; found_inf, </span><span class="s2">double </span><span class="s1">scale_growth_factor, </span><span class="s2">double </span><span class="s1">scale_backoff_factor, int64_t growth_interval); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1808"><span class="ln">1808 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_add(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1809"><span class="ln">1809 </span></a><span class="s2">void </span><span class="s1">_foreach_add_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1810"><span class="ln">1810 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_add(at::TensorList self, at::TensorList other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1811"><span class="ln">1811 </span></a><span class="s2">void </span><span class="s1">_foreach_add_(at::TensorList self, at::TensorList other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1812"><span class="ln">1812 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_add(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1813"><span class="ln">1813 </span></a><span class="s2">void </span><span class="s1">_foreach_add_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1814"><span class="ln">1814 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_add(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.Tensor(Tensor[] self, Tensor other, *, Scalar alpha=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1815"><span class="ln">1815 </span></a><span class="s2">void </span><span class="s1">_foreach_add_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add_.Tensor(Tensor(a!)[] self, Tensor other, *, Scalar alpha=1) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1816"><span class="ln">1816 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sub(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1817"><span class="ln">1817 </span></a><span class="s2">void </span><span class="s1">_foreach_sub_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1818"><span class="ln">1818 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sub(at::TensorList self, at::TensorList other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1819"><span class="ln">1819 </span></a><span class="s2">void </span><span class="s1">_foreach_sub_(at::TensorList self, at::TensorList other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1820"><span class="ln">1820 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sub(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1821"><span class="ln">1821 </span></a><span class="s2">void </span><span class="s1">_foreach_sub_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1822"><span class="ln">1822 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_mul(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1823"><span class="ln">1823 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1824"><span class="ln">1824 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_mul(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.List(Tensor[] self, Tensor[] other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1825"><span class="ln">1825 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1826"><span class="ln">1826 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_mul(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1827"><span class="ln">1827 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1828"><span class="ln">1828 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_mul(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.Tensor(Tensor[] self, Tensor other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1829"><span class="ln">1829 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul_.Tensor(Tensor(a!)[] self, Tensor other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1830"><span class="ln">1830 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_div(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1831"><span class="ln">1831 </span></a><span class="s2">void </span><span class="s1">_foreach_div_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1832"><span class="ln">1832 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_div(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.List(Tensor[] self, Tensor[] other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1833"><span class="ln">1833 </span></a><span class="s2">void </span><span class="s1">_foreach_div_(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1834"><span class="ln">1834 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_div(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1835"><span class="ln">1835 </span></a><span class="s2">void </span><span class="s1">_foreach_div_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1836"><span class="ln">1836 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_div(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.Tensor(Tensor[] self, Tensor other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1837"><span class="ln">1837 </span></a><span class="s2">void </span><span class="s1">_foreach_div_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div_.Tensor(Tensor(a!)[] self, Tensor other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1838"><span class="ln">1838 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_clamp_max(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1839"><span class="ln">1839 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_max_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1840"><span class="ln">1840 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_clamp_max(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max.List(Tensor[] self, Tensor[] other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1841"><span class="ln">1841 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_max_(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max_.List(Tensor(a!)[] self, Tensor[] other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1842"><span class="ln">1842 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_clamp_max(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1843"><span class="ln">1843 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_max_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1844"><span class="ln">1844 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_clamp_min(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1845"><span class="ln">1845 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_min_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1846"><span class="ln">1846 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_clamp_min(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min.List(Tensor[] self, Tensor[] other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1847"><span class="ln">1847 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_min_(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min_.List(Tensor(a!)[] self, Tensor[] other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1848"><span class="ln">1848 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_clamp_min(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1849"><span class="ln">1849 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_min_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1850"><span class="ln">1850 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_maximum(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1851"><span class="ln">1851 </span></a><span class="s2">void </span><span class="s1">_foreach_maximum_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1852"><span class="ln">1852 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_maximum(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum.List(Tensor[] self, Tensor[] other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1853"><span class="ln">1853 </span></a><span class="s2">void </span><span class="s1">_foreach_maximum_(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1854"><span class="ln">1854 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_maximum(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1855"><span class="ln">1855 </span></a><span class="s2">void </span><span class="s1">_foreach_maximum_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1856"><span class="ln">1856 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_minimum(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum.Scalar(Tensor[] self, Scalar scalar) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1857"><span class="ln">1857 </span></a><span class="s2">void </span><span class="s1">_foreach_minimum_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum_.Scalar(Tensor(a!)[] self, Scalar scalar) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1858"><span class="ln">1858 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_minimum(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum.List(Tensor[] self, Tensor[] other) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1859"><span class="ln">1859 </span></a><span class="s2">void </span><span class="s1">_foreach_minimum_(at::TensorList self, at::TensorList other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum_.List(Tensor(a!)[] self, Tensor[] other) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1860"><span class="ln">1860 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_minimum(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum.ScalarList(Tensor[] self, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1861"><span class="ln">1861 </span></a><span class="s2">void </span><span class="s1">_foreach_minimum_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1862"><span class="ln">1862 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_addcdiv(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1863"><span class="ln">1863 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_addcdiv(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1864"><span class="ln">1864 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_addcdiv(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1865"><span class="ln">1865 </span></a><span class="s2">void </span><span class="s1">_foreach_addcdiv_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1866"><span class="ln">1866 </span></a><span class="s2">void </span><span class="s1">_foreach_addcdiv_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1867"><span class="ln">1867 </span></a><span class="s2">void </span><span class="s1">_foreach_addcdiv_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1868"><span class="ln">1868 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_addcmul(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1869"><span class="ln">1869 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_addcmul(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1870"><span class="ln">1870 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_addcmul(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1871"><span class="ln">1871 </span></a><span class="s2">void </span><span class="s1">_foreach_addcmul_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1872"><span class="ln">1872 </span></a><span class="s2">void </span><span class="s1">_foreach_addcmul_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef&lt;at::Scalar&gt; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1873"><span class="ln">1873 </span></a><span class="s2">void </span><span class="s1">_foreach_addcmul_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scalars); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1874"><span class="ln">1874 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_abs(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_abs(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1875"><span class="ln">1875 </span></a><span class="s2">void </span><span class="s1">_foreach_abs_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_abs_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1876"><span class="ln">1876 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_acos(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_acos(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1877"><span class="ln">1877 </span></a><span class="s2">void </span><span class="s1">_foreach_acos_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_acos_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1878"><span class="ln">1878 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_asin(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_asin(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1879"><span class="ln">1879 </span></a><span class="s2">void </span><span class="s1">_foreach_asin_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_asin_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1880"><span class="ln">1880 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_atan(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_atan(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1881"><span class="ln">1881 </span></a><span class="s2">void </span><span class="s1">_foreach_atan_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_atan_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1882"><span class="ln">1882 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_ceil(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_ceil(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1883"><span class="ln">1883 </span></a><span class="s2">void </span><span class="s1">_foreach_ceil_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_ceil_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1884"><span class="ln">1884 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_cos(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_cos(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1885"><span class="ln">1885 </span></a><span class="s2">void </span><span class="s1">_foreach_cos_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_cos_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1886"><span class="ln">1886 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_cosh(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_cosh(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1887"><span class="ln">1887 </span></a><span class="s2">void </span><span class="s1">_foreach_cosh_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_cosh_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1888"><span class="ln">1888 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_erf(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_erf(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1889"><span class="ln">1889 </span></a><span class="s2">void </span><span class="s1">_foreach_erf_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_erf_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1890"><span class="ln">1890 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_erfc(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_erfc(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1891"><span class="ln">1891 </span></a><span class="s2">void </span><span class="s1">_foreach_erfc_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_erfc_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1892"><span class="ln">1892 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_exp(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_exp(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1893"><span class="ln">1893 </span></a><span class="s2">void </span><span class="s1">_foreach_exp_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_exp_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1894"><span class="ln">1894 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_expm1(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_expm1(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1895"><span class="ln">1895 </span></a><span class="s2">void </span><span class="s1">_foreach_expm1_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_expm1_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1896"><span class="ln">1896 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_floor(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_floor(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1897"><span class="ln">1897 </span></a><span class="s2">void </span><span class="s1">_foreach_floor_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_floor_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1898"><span class="ln">1898 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_frac(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_frac(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1899"><span class="ln">1899 </span></a><span class="s2">void </span><span class="s1">_foreach_frac_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_frac_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1900"><span class="ln">1900 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_lerp(at::TensorList self, at::TensorList tensors1, at::TensorList weights); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp.List(Tensor[] self, Tensor[] tensors1, Tensor[] weights) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1901"><span class="ln">1901 </span></a><span class="s2">void </span><span class="s1">_foreach_lerp_(at::TensorList self, at::TensorList tensors1, at::TensorList weights); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp_.List(Tensor(a!)[] self, Tensor[] tensors1, Tensor[] weights) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1902"><span class="ln">1902 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_lerp(at::TensorList self, at::TensorList tensors1, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp.Scalar(Tensor[] self, Tensor[] tensors1, Scalar weight) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1903"><span class="ln">1903 </span></a><span class="s2">void </span><span class="s1">_foreach_lerp_(at::TensorList self, at::TensorList tensors1, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp_.Scalar(Tensor(a!)[] self, Tensor[] tensors1, Scalar weight) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1904"><span class="ln">1904 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_lerp(at::TensorList self, at::TensorList tensors1, at::ArrayRef&lt;at::Scalar&gt; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp.ScalarList(Tensor[] self, Tensor[] tensors1, Scalar[] weight) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1905"><span class="ln">1905 </span></a><span class="s2">void </span><span class="s1">_foreach_lerp_(at::TensorList self, at::TensorList tensors1, at::ArrayRef&lt;at::Scalar&gt; weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp_.ScalarList(Tensor(a!)[] self, Tensor[] tensors1, Scalar[] weight) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1906"><span class="ln">1906 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_lgamma(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lgamma(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1907"><span class="ln">1907 </span></a><span class="s2">void </span><span class="s1">_foreach_lgamma_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lgamma_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1908"><span class="ln">1908 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_log(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1909"><span class="ln">1909 </span></a><span class="s2">void </span><span class="s1">_foreach_log_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1910"><span class="ln">1910 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_log10(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log10(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1911"><span class="ln">1911 </span></a><span class="s2">void </span><span class="s1">_foreach_log10_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log10_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1912"><span class="ln">1912 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_log1p(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log1p(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1913"><span class="ln">1913 </span></a><span class="s2">void </span><span class="s1">_foreach_log1p_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log1p_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1914"><span class="ln">1914 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_log2(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log2(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1915"><span class="ln">1915 </span></a><span class="s2">void </span><span class="s1">_foreach_log2_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log2_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1916"><span class="ln">1916 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_max(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_max(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1917"><span class="ln">1917 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_neg(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_neg(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1918"><span class="ln">1918 </span></a><span class="s2">void </span><span class="s1">_foreach_neg_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_neg_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1919"><span class="ln">1919 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_norm(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; ord, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_norm.Scalar(Tensor[] self, Scalar ord=2, ScalarType? dtype=None) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1920"><span class="ln">1920 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_pow(at::TensorList self, at::TensorList exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow.List(Tensor[] self, Tensor[] exponent) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1921"><span class="ln">1921 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_pow(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow.Scalar(Tensor[] self, Scalar exponent) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1922"><span class="ln">1922 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_pow(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow.ScalarList(Tensor[] self, Scalar[] exponent) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1923"><span class="ln">1923 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_pow(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, at::TensorList exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow.ScalarAndTensor(Scalar self, Tensor[] exponent) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1924"><span class="ln">1924 </span></a><span class="s2">void </span><span class="s1">_foreach_pow_(at::TensorList self, at::TensorList exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow_.List(Tensor(a!)[] self, Tensor[] exponent) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1925"><span class="ln">1925 </span></a><span class="s2">void </span><span class="s1">_foreach_pow_(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow_.Scalar(Tensor(a!)[] self, Scalar exponent) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1926"><span class="ln">1926 </span></a><span class="s2">void </span><span class="s1">_foreach_pow_(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; exponent); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow_.ScalarList(Tensor(a!)[] self, Scalar[] exponent) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1927"><span class="ln">1927 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_reciprocal(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_reciprocal(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1928"><span class="ln">1928 </span></a><span class="s2">void </span><span class="s1">_foreach_reciprocal_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_reciprocal_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1929"><span class="ln">1929 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_round(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_round(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1930"><span class="ln">1930 </span></a><span class="s2">void </span><span class="s1">_foreach_round_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_round_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1931"><span class="ln">1931 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_rsqrt(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_rsqrt(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1932"><span class="ln">1932 </span></a><span class="s2">void </span><span class="s1">_foreach_rsqrt_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_rsqrt_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1933"><span class="ln">1933 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sigmoid(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sigmoid(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1934"><span class="ln">1934 </span></a><span class="s2">void </span><span class="s1">_foreach_sigmoid_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sigmoid_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1935"><span class="ln">1935 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sign(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sign(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1936"><span class="ln">1936 </span></a><span class="s2">void </span><span class="s1">_foreach_sign_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sign_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1937"><span class="ln">1937 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sin(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sin(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1938"><span class="ln">1938 </span></a><span class="s2">void </span><span class="s1">_foreach_sin_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sin_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1939"><span class="ln">1939 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sinh(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sinh(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1940"><span class="ln">1940 </span></a><span class="s2">void </span><span class="s1">_foreach_sinh_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sinh_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1941"><span class="ln">1941 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_sqrt(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sqrt(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1942"><span class="ln">1942 </span></a><span class="s2">void </span><span class="s1">_foreach_sqrt_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sqrt_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1943"><span class="ln">1943 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_tan(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_tan(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1944"><span class="ln">1944 </span></a><span class="s2">void </span><span class="s1">_foreach_tan_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_tan_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1945"><span class="ln">1945 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_tanh(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_tanh(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1946"><span class="ln">1946 </span></a><span class="s2">void </span><span class="s1">_foreach_tanh_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_tanh_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1947"><span class="ln">1947 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_trunc(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_trunc(Tensor[] self) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1948"><span class="ln">1948 </span></a><span class="s2">void </span><span class="s1">_foreach_trunc_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_trunc_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1949"><span class="ln">1949 </span></a><span class="s2">void </span><span class="s1">_foreach_zero_(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_zero_(Tensor(a!)[] self) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1950"><span class="ln">1950 </span></a><span class="s2">void </span><span class="s1">_foreach_copy_(at::TensorList self, at::TensorList src, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_copy_(Tensor(a!)[] self, Tensor[] src, bool non_blocking=False) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1951"><span class="ln">1951 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_copy(at::TensorList self, at::TensorList src, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_copy(Tensor[] self, Tensor[] src, bool non_blocking=False) -&gt; Tensor[] self_out&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1952"><span class="ln">1952 </span></a><span class="s1">at::Tensor bucketize(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; boundaries, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1953"><span class="ln">1953 </span></a><span class="s1">at::Tensor &amp; bucketize_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; boundaries, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1954"><span class="ln">1954 </span></a><span class="s1">at::Tensor bucketize(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; boundaries, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1955"><span class="ln">1955 </span></a><span class="s1">at::Tensor searchsorted(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; sorted_sequence, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right, ::std::optional&lt;c10::string_view&gt; side, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; sorter); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1956"><span class="ln">1956 </span></a><span class="s1">at::Tensor &amp; searchsorted_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; sorted_sequence, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right, ::std::optional&lt;c10::string_view&gt; side, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; sorter, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1957"><span class="ln">1957 </span></a><span class="s1">at::Tensor searchsorted(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; sorted_sequence, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right, ::std::optional&lt;c10::string_view&gt; side, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; sorter); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1958"><span class="ln">1958 </span></a><span class="s1">at::Tensor &amp; searchsorted_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; sorted_sequence, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right, ::std::optional&lt;c10::string_view&gt; side, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; sorter, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::searchsorted.Scalar_out(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1959"><span class="ln">1959 </span></a><span class="s1">at::Tensor _convert_indices_from_coo_to_csr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t size, </span><span class="s2">bool </span><span class="s1">out_int32); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1960"><span class="ln">1960 </span></a><span class="s1">at::Tensor &amp; _convert_indices_from_coo_to_csr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t size, </span><span class="s2">bool </span><span class="s1">out_int32, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1961"><span class="ln">1961 </span></a><span class="s1">at::Tensor _convert_indices_from_csr_to_coo(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">transpose); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convert_indices_from_csr_to_coo(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1962"><span class="ln">1962 </span></a><span class="s1">at::Tensor &amp; _convert_indices_from_csr_to_coo_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; crow_indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; col_indices, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">transpose, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1963"><span class="ln">1963 </span></a><span class="s1">at::Tensor &amp; mse_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1964"><span class="ln">1964 </span></a><span class="s1">at::Tensor mse_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1965"><span class="ln">1965 </span></a><span class="s1">at::Tensor &amp; mse_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1966"><span class="ln">1966 </span></a><span class="s1">at::Tensor mse_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1967"><span class="ln">1967 </span></a><span class="s1">at::Tensor l1_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1968"><span class="ln">1968 </span></a><span class="s1">at::Tensor &amp; multi_margin_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; margin, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1969"><span class="ln">1969 </span></a><span class="s1">at::Tensor multi_margin_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; margin, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1970"><span class="ln">1970 </span></a><span class="s1">at::Tensor &amp; multi_margin_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; margin, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1971"><span class="ln">1971 </span></a><span class="s1">at::Tensor multi_margin_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; margin, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1972"><span class="ln">1972 </span></a><span class="s1">at::Tensor &amp; multilabel_margin_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1973"><span class="ln">1973 </span></a><span class="s1">at::Tensor multilabel_margin_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1974"><span class="ln">1974 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; multilabel_margin_loss_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, at::Tensor &amp; output, at::Tensor &amp; is_target); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1975"><span class="ln">1975 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; multilabel_margin_loss_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -&gt; (Tensor output, Tensor is_target)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1976"><span class="ln">1976 </span></a><span class="s1">at::Tensor &amp; multilabel_margin_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; is_target, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1977"><span class="ln">1977 </span></a><span class="s1">at::Tensor multilabel_margin_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; is_target); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1978"><span class="ln">1978 </span></a><span class="s1">at::Tensor &amp; nll_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1979"><span class="ln">1979 </span></a><span class="s1">at::Tensor nll_loss_nd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1980"><span class="ln">1980 </span></a><span class="s1">at::Tensor nll_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1981"><span class="ln">1981 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; nll_loss_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor &amp; output, at::Tensor &amp; total_weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1982"><span class="ln">1982 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; nll_loss_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -&gt; (Tensor output, Tensor total_weight)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1983"><span class="ln">1983 </span></a><span class="s1">at::Tensor &amp; nll_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; total_weight, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1984"><span class="ln">1984 </span></a><span class="s1">at::Tensor nll_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; total_weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1985"><span class="ln">1985 </span></a><span class="s1">at::Tensor &amp; nll_loss2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1986"><span class="ln">1986 </span></a><span class="s1">at::Tensor nll_loss2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1987"><span class="ln">1987 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; nll_loss2d_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor &amp; output, at::Tensor &amp; total_weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1988"><span class="ln">1988 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; nll_loss2d_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -&gt; (Tensor output, Tensor total_weight)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1989"><span class="ln">1989 </span></a><span class="s1">at::Tensor &amp; nll_loss2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; total_weight, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1990"><span class="ln">1990 </span></a><span class="s1">at::Tensor nll_loss2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, int64_t reduction, c10::SymInt ignore_index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; total_weight); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1991"><span class="ln">1991 </span></a><span class="s1">at::Tensor &amp; smooth_l1_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">beta, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1992"><span class="ln">1992 </span></a><span class="s1">at::Tensor smooth_l1_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">beta); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1993"><span class="ln">1993 </span></a><span class="s1">at::Tensor &amp; smooth_l1_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">beta, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1994"><span class="ln">1994 </span></a><span class="s1">at::Tensor smooth_l1_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">beta); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1995"><span class="ln">1995 </span></a><span class="s1">at::Tensor &amp; huber_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">delta, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1996"><span class="ln">1996 </span></a><span class="s1">at::Tensor huber_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">delta); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1997"><span class="ln">1997 </span></a><span class="s1">at::Tensor &amp; huber_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">delta, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l1998"><span class="ln">1998 </span></a><span class="s1">at::Tensor huber_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, </span><span class="s2">double </span><span class="s1">delta); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l1999"><span class="ln">1999 </span></a><span class="s1">at::Tensor &amp; soft_margin_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2000"><span class="ln">2000 </span></a><span class="s1">at::Tensor soft_margin_loss(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2001"><span class="ln">2001 </span></a><span class="s1">at::Tensor &amp; soft_margin_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2002"><span class="ln">2002 </span></a><span class="s1">at::Tensor soft_margin_loss_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, int64_t reduction); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2003"><span class="ln">2003 </span></a><span class="s1">at::Tensor &amp; elu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; input_scale, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2004"><span class="ln">2004 </span></a><span class="s1">at::Tensor elu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; input_scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2005"><span class="ln">2005 </span></a><span class="s1">at::Tensor &amp; elu_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; input_scale, </span><span class="s2">bool </span><span class="s1">is_result, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self_or_result, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2006"><span class="ln">2006 </span></a><span class="s1">at::Tensor elu_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; input_scale, </span><span class="s2">bool </span><span class="s1">is_result, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self_or_result); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2007"><span class="ln">2007 </span></a><span class="s1">at::Tensor &amp; elu_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scale, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; input_scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2008"><span class="ln">2008 </span></a><span class="s1">at::Tensor &amp; glu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2009"><span class="ln">2009 </span></a><span class="s1">at::Tensor glu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu(Tensor self, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2010"><span class="ln">2010 </span></a><span class="s1">at::Tensor &amp; glu_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2011"><span class="ln">2011 </span></a><span class="s1">at::Tensor glu_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu_backward(Tensor grad_output, Tensor self, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2012"><span class="ln">2012 </span></a><span class="s1">at::Tensor glu_jvp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; glu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dx, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu_jvp(Tensor glu, Tensor x, Tensor dx, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2013"><span class="ln">2013 </span></a><span class="s1">at::Tensor glu_backward_jvp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_glu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dgrad_glu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dx, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2014"><span class="ln">2014 </span></a><span class="s1">at::Tensor &amp; hardsigmoid_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2015"><span class="ln">2015 </span></a><span class="s1">at::Tensor hardsigmoid(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardsigmoid(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2016"><span class="ln">2016 </span></a><span class="s1">at::Tensor &amp; hardsigmoid_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardsigmoid_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2017"><span class="ln">2017 </span></a><span class="s1">at::Tensor &amp; hardsigmoid_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2018"><span class="ln">2018 </span></a><span class="s1">at::Tensor hardsigmoid_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardsigmoid_backward(Tensor grad_output, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2019"><span class="ln">2019 </span></a><span class="s1">at::Tensor &amp; hardtanh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min_val, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max_val, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2020"><span class="ln">2020 </span></a><span class="s1">at::Tensor hardtanh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min_val, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max_val); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2021"><span class="ln">2021 </span></a><span class="s1">at::Tensor &amp; hardtanh_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min_val, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max_val, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2022"><span class="ln">2022 </span></a><span class="s1">at::Tensor hardtanh_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min_val, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max_val); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2023"><span class="ln">2023 </span></a><span class="s1">at::Tensor &amp; hardtanh_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; min_val, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; max_val); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2024"><span class="ln">2024 </span></a><span class="s1">at::Tensor &amp; hardswish_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardswish.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2025"><span class="ln">2025 </span></a><span class="s1">at::Tensor hardswish(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardswish(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2026"><span class="ln">2026 </span></a><span class="s1">at::Tensor &amp; hardswish_(at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardswish_(Tensor(a!) self) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2027"><span class="ln">2027 </span></a><span class="s1">at::Tensor hardswish_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardswish_backward(Tensor grad_output, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2028"><span class="ln">2028 </span></a><span class="s1">at::Tensor &amp; leaky_relu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; negative_slope, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2029"><span class="ln">2029 </span></a><span class="s1">at::Tensor leaky_relu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; negative_slope); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2030"><span class="ln">2030 </span></a><span class="s1">at::Tensor &amp; leaky_relu_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; negative_slope, </span><span class="s2">bool </span><span class="s1">self_is_result, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2031"><span class="ln">2031 </span></a><span class="s1">at::Tensor leaky_relu_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; negative_slope, </span><span class="s2">bool </span><span class="s1">self_is_result); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2032"><span class="ln">2032 </span></a><span class="s1">at::Tensor &amp; leaky_relu_(at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; negative_slope); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2033"><span class="ln">2033 </span></a><span class="s1">at::Tensor &amp; log_sigmoid_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2034"><span class="ln">2034 </span></a><span class="s1">at::Tensor log_sigmoid(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_sigmoid(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2035"><span class="ln">2035 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; log_sigmoid_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; output, at::Tensor &amp; buffer); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2036"><span class="ln">2036 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; log_sigmoid_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_sigmoid_forward(Tensor self) -&gt; (Tensor output, Tensor buffer)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2037"><span class="ln">2037 </span></a><span class="s1">at::Tensor &amp; log_sigmoid_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; buffer, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2038"><span class="ln">2038 </span></a><span class="s1">at::Tensor log_sigmoid_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; buffer); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2039"><span class="ln">2039 </span></a><span class="s1">at::Tensor &amp; rrelu_with_noise_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; noise, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu_with_noise.out(Tensor self, Tensor(b!) noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2040"><span class="ln">2040 </span></a><span class="s1">at::Tensor rrelu_with_noise(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; noise, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu_with_noise(Tensor self, Tensor(b!) noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2041"><span class="ln">2041 </span></a><span class="s1">at::Tensor rrelu_with_noise_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; noise, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">bool </span><span class="s1">self_is_result); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2042"><span class="ln">2042 </span></a><span class="s1">at::Tensor &amp; rrelu_with_noise_(at::Tensor &amp; self, at::Tensor &amp; noise, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu_with_noise_(Tensor(a!) self, Tensor(b!) noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2043"><span class="ln">2043 </span></a><span class="s1">at::Tensor &amp; softplus_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2044"><span class="ln">2044 </span></a><span class="s1">at::Tensor softplus(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2045"><span class="ln">2045 </span></a><span class="s1">at::Tensor &amp; softplus_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2046"><span class="ln">2046 </span></a><span class="s1">at::Tensor softplus_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; threshold); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2047"><span class="ln">2047 </span></a><span class="s1">at::Tensor &amp; softshrink_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2048"><span class="ln">2048 </span></a><span class="s1">at::Tensor softshrink(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softshrink(Tensor self, Scalar lambd=0.5) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2049"><span class="ln">2049 </span></a><span class="s1">at::Tensor &amp; softshrink_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2050"><span class="ln">2050 </span></a><span class="s1">at::Tensor softshrink_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lambd); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2051"><span class="ln">2051 </span></a><span class="s1">at::Tensor &amp; adaptive_avg_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2052"><span class="ln">2052 </span></a><span class="s1">at::Tensor adaptive_avg_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2053"><span class="ln">2053 </span></a><span class="s1">at::Tensor mkldnn_adaptive_avg_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2054"><span class="ln">2054 </span></a><span class="s1">at::Tensor &amp; mkldnn_adaptive_avg_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2055"><span class="ln">2055 </span></a><span class="s1">at::Tensor mkldnn_adaptive_avg_pool2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2056"><span class="ln">2056 </span></a><span class="s1">at::Tensor _adaptive_avg_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2057"><span class="ln">2057 </span></a><span class="s1">at::Tensor _adaptive_avg_pool2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2058"><span class="ln">2058 </span></a><span class="s1">at::Tensor &amp; adaptive_avg_pool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2059"><span class="ln">2059 </span></a><span class="s1">at::Tensor adaptive_avg_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2060"><span class="ln">2060 </span></a><span class="s1">at::Tensor _adaptive_avg_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2061"><span class="ln">2061 </span></a><span class="s1">at::Tensor &amp; adaptive_avg_pool3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2062"><span class="ln">2062 </span></a><span class="s1">at::Tensor _adaptive_avg_pool3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2063"><span class="ln">2063 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; adaptive_max_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size, at::Tensor &amp; out, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2064"><span class="ln">2064 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; adaptive_max_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2065"><span class="ln">2065 </span></a><span class="s1">at::Tensor &amp; adaptive_max_pool2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2066"><span class="ln">2066 </span></a><span class="s1">at::Tensor adaptive_max_pool2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2067"><span class="ln">2067 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; adaptive_max_pool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size, at::Tensor &amp; out, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2068"><span class="ln">2068 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; adaptive_max_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2069"><span class="ln">2069 </span></a><span class="s1">at::Tensor &amp; adaptive_max_pool3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2070"><span class="ln">2070 </span></a><span class="s1">at::Tensor adaptive_max_pool3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2071"><span class="ln">2071 </span></a><span class="s1">at::Tensor &amp; avg_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2072"><span class="ln">2072 </span></a><span class="s1">at::Tensor avg_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2073"><span class="ln">2073 </span></a><span class="s1">at::Tensor &amp; avg_pool2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2074"><span class="ln">2074 </span></a><span class="s1">at::Tensor avg_pool2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2075"><span class="ln">2075 </span></a><span class="s1">at::Tensor &amp; avg_pool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2076"><span class="ln">2076 </span></a><span class="s1">at::Tensor avg_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2077"><span class="ln">2077 </span></a><span class="s1">at::Tensor &amp; avg_pool3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2078"><span class="ln">2078 </span></a><span class="s1">at::Tensor avg_pool3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, ::std::optional&lt;int64_t&gt; divisor_override); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2079"><span class="ln">2079 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; fractional_max_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; random_samples, at::Tensor &amp; output, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2080"><span class="ln">2080 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; fractional_max_pool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; random_samples); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2081"><span class="ln">2081 </span></a><span class="s1">at::Tensor &amp; fractional_max_pool2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2082"><span class="ln">2082 </span></a><span class="s1">at::Tensor fractional_max_pool2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2083"><span class="ln">2083 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; fractional_max_pool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; random_samples, at::Tensor &amp; output, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2084"><span class="ln">2084 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; fractional_max_pool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; random_samples); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2085"><span class="ln">2085 </span></a><span class="s1">at::Tensor &amp; fractional_max_pool3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2086"><span class="ln">2086 </span></a><span class="s1">at::Tensor fractional_max_pool3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2087"><span class="ln">2087 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; max_pool2d_with_indices_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2088"><span class="ln">2088 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; max_pool2d_with_indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2089"><span class="ln">2089 </span></a><span class="s1">at::Tensor &amp; max_pool2d_with_indices_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2090"><span class="ln">2090 </span></a><span class="s1">at::Tensor max_pool2d_with_indices_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2091"><span class="ln">2091 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; max_pool3d_with_indices_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out, at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2092"><span class="ln">2092 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; max_pool3d_with_indices(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2093"><span class="ln">2093 </span></a><span class="s1">at::Tensor &amp; max_pool3d_with_indices_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2094"><span class="ln">2094 </span></a><span class="s1">at::Tensor max_pool3d_with_indices_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2095"><span class="ln">2095 </span></a><span class="s1">at::Tensor &amp; max_unpool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymIntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2096"><span class="ln">2096 </span></a><span class="s1">at::Tensor max_unpool2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymIntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2097"><span class="ln">2097 </span></a><span class="s1">at::Tensor &amp; max_unpool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2098"><span class="ln">2098 </span></a><span class="s1">at::Tensor max_unpool3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2099"><span class="ln">2099 </span></a><span class="s1">at::Tensor &amp; reflection_pad1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2100"><span class="ln">2100 </span></a><span class="s1">at::Tensor reflection_pad1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad1d(Tensor self, SymInt[2] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2101"><span class="ln">2101 </span></a><span class="s1">at::Tensor &amp; reflection_pad1d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2102"><span class="ln">2102 </span></a><span class="s1">at::Tensor reflection_pad1d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2103"><span class="ln">2103 </span></a><span class="s1">at::Tensor &amp; reflection_pad2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2104"><span class="ln">2104 </span></a><span class="s1">at::Tensor reflection_pad2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad2d(Tensor self, SymInt[4] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2105"><span class="ln">2105 </span></a><span class="s1">at::Tensor &amp; reflection_pad2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2106"><span class="ln">2106 </span></a><span class="s1">at::Tensor reflection_pad2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2107"><span class="ln">2107 </span></a><span class="s1">at::Tensor &amp; reflection_pad3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2108"><span class="ln">2108 </span></a><span class="s1">at::Tensor reflection_pad3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad3d(Tensor self, SymInt[6] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2109"><span class="ln">2109 </span></a><span class="s1">at::Tensor &amp; reflection_pad3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2110"><span class="ln">2110 </span></a><span class="s1">at::Tensor reflection_pad3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2111"><span class="ln">2111 </span></a><span class="s1">at::Tensor &amp; replication_pad1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2112"><span class="ln">2112 </span></a><span class="s1">at::Tensor replication_pad1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad1d(Tensor self, SymInt[2] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2113"><span class="ln">2113 </span></a><span class="s1">at::Tensor &amp; replication_pad1d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2114"><span class="ln">2114 </span></a><span class="s1">at::Tensor replication_pad1d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2115"><span class="ln">2115 </span></a><span class="s1">at::Tensor &amp; replication_pad2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2116"><span class="ln">2116 </span></a><span class="s1">at::Tensor replication_pad2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad2d(Tensor self, SymInt[4] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2117"><span class="ln">2117 </span></a><span class="s1">at::Tensor &amp; replication_pad2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2118"><span class="ln">2118 </span></a><span class="s1">at::Tensor replication_pad2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2119"><span class="ln">2119 </span></a><span class="s1">at::Tensor &amp; replication_pad3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2120"><span class="ln">2120 </span></a><span class="s1">at::Tensor replication_pad3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad3d(Tensor self, SymInt[6] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2121"><span class="ln">2121 </span></a><span class="s1">at::Tensor &amp; replication_pad3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2122"><span class="ln">2122 </span></a><span class="s1">at::Tensor replication_pad3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2123"><span class="ln">2123 </span></a><span class="s1">at::Tensor _pad_circular(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef pad); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pad_circular(Tensor self, SymInt[] pad) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2124"><span class="ln">2124 </span></a><span class="s1">at::Tensor _pad_enum(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef pad, int64_t mode, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pad_enum(Tensor self, SymInt[] pad, int mode, float? value=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2125"><span class="ln">2125 </span></a><span class="s1">at::Tensor pad(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef pad, c10::string_view mode, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pad(Tensor self, SymInt[] pad, str mode=\&quot;constant\&quot;, float? value=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2126"><span class="ln">2126 </span></a><span class="s1">at::Tensor upsample_linear1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2127"><span class="ln">2127 </span></a><span class="s1">at::Tensor upsample_bilinear2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2128"><span class="ln">2128 </span></a><span class="s1">at::Tensor _upsample_bilinear2d_aa(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bilinear2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2129"><span class="ln">2129 </span></a><span class="s1">at::Tensor upsample_trilinear3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2130"><span class="ln">2130 </span></a><span class="s1">at::Tensor upsample_bicubic2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2131"><span class="ln">2131 </span></a><span class="s1">at::Tensor _upsample_bicubic2d_aa(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bicubic2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2132"><span class="ln">2132 </span></a><span class="s1">at::Tensor upsample_nearest1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2133"><span class="ln">2133 </span></a><span class="s1">at::Tensor _upsample_nearest_exact1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2134"><span class="ln">2134 </span></a><span class="s1">at::Tensor upsample_nearest2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2135"><span class="ln">2135 </span></a><span class="s1">at::Tensor _upsample_nearest_exact2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2136"><span class="ln">2136 </span></a><span class="s1">at::Tensor upsample_nearest3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2137"><span class="ln">2137 </span></a><span class="s1">at::Tensor _upsample_nearest_exact3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2138"><span class="ln">2138 </span></a><span class="s1">at::Tensor &amp; upsample_linear1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2139"><span class="ln">2139 </span></a><span class="s1">at::Tensor upsample_linear1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2140"><span class="ln">2140 </span></a><span class="s1">at::Tensor &amp; upsample_linear1d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2141"><span class="ln">2141 </span></a><span class="s1">at::Tensor upsample_linear1d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2142"><span class="ln">2142 </span></a><span class="s1">at::Tensor &amp; upsample_bilinear2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2143"><span class="ln">2143 </span></a><span class="s1">at::Tensor upsample_bilinear2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2144"><span class="ln">2144 </span></a><span class="s1">at::Tensor &amp; upsample_bilinear2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2145"><span class="ln">2145 </span></a><span class="s1">at::Tensor upsample_bilinear2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2146"><span class="ln">2146 </span></a><span class="s1">at::Tensor &amp; _upsample_bilinear2d_aa_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2147"><span class="ln">2147 </span></a><span class="s1">at::Tensor _upsample_bilinear2d_aa(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bilinear2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2148"><span class="ln">2148 </span></a><span class="s1">at::Tensor &amp; _upsample_bilinear2d_aa_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2149"><span class="ln">2149 </span></a><span class="s1">at::Tensor _upsample_bilinear2d_aa_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bilinear2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2150"><span class="ln">2150 </span></a><span class="s1">at::Tensor &amp; upsample_bicubic2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2151"><span class="ln">2151 </span></a><span class="s1">at::Tensor upsample_bicubic2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2152"><span class="ln">2152 </span></a><span class="s1">at::Tensor &amp; upsample_bicubic2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2153"><span class="ln">2153 </span></a><span class="s1">at::Tensor upsample_bicubic2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2154"><span class="ln">2154 </span></a><span class="s1">at::Tensor &amp; _upsample_bicubic2d_aa_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2155"><span class="ln">2155 </span></a><span class="s1">at::Tensor _upsample_bicubic2d_aa(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bicubic2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2156"><span class="ln">2156 </span></a><span class="s1">at::Tensor &amp; _upsample_bicubic2d_aa_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2157"><span class="ln">2157 </span></a><span class="s1">at::Tensor _upsample_bicubic2d_aa_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_bicubic2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2158"><span class="ln">2158 </span></a><span class="s1">at::Tensor &amp; upsample_trilinear3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2159"><span class="ln">2159 </span></a><span class="s1">at::Tensor upsample_trilinear3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2160"><span class="ln">2160 </span></a><span class="s1">at::Tensor &amp; upsample_trilinear3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2161"><span class="ln">2161 </span></a><span class="s1">at::Tensor upsample_trilinear3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2162"><span class="ln">2162 </span></a><span class="s1">at::Tensor &amp; upsample_nearest1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2163"><span class="ln">2163 </span></a><span class="s1">at::Tensor &amp; _upsample_nearest_exact1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2164"><span class="ln">2164 </span></a><span class="s1">at::Tensor upsample_nearest1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2165"><span class="ln">2165 </span></a><span class="s1">at::Tensor _upsample_nearest_exact1d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2166"><span class="ln">2166 </span></a><span class="s1">at::Tensor &amp; upsample_nearest1d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2167"><span class="ln">2167 </span></a><span class="s1">at::Tensor &amp; _upsample_nearest_exact1d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2168"><span class="ln">2168 </span></a><span class="s1">at::Tensor upsample_nearest1d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2169"><span class="ln">2169 </span></a><span class="s1">at::Tensor _upsample_nearest_exact1d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2170"><span class="ln">2170 </span></a><span class="s1">at::Tensor &amp; upsample_nearest2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2171"><span class="ln">2171 </span></a><span class="s1">at::Tensor &amp; _upsample_nearest_exact2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2172"><span class="ln">2172 </span></a><span class="s1">at::Tensor upsample_nearest2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2173"><span class="ln">2173 </span></a><span class="s1">at::Tensor _upsample_nearest_exact2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2174"><span class="ln">2174 </span></a><span class="s1">at::Tensor &amp; upsample_nearest2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2175"><span class="ln">2175 </span></a><span class="s1">at::Tensor &amp; _upsample_nearest_exact2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2176"><span class="ln">2176 </span></a><span class="s1">at::Tensor upsample_nearest2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2177"><span class="ln">2177 </span></a><span class="s1">at::Tensor _upsample_nearest_exact2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2178"><span class="ln">2178 </span></a><span class="s1">at::Tensor &amp; upsample_nearest3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2179"><span class="ln">2179 </span></a><span class="s1">at::Tensor &amp; _upsample_nearest_exact3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2180"><span class="ln">2180 </span></a><span class="s1">at::Tensor upsample_nearest3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2181"><span class="ln">2181 </span></a><span class="s1">at::Tensor _upsample_nearest_exact3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2182"><span class="ln">2182 </span></a><span class="s1">at::Tensor &amp; upsample_nearest3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2183"><span class="ln">2183 </span></a><span class="s1">at::Tensor &amp; _upsample_nearest_exact3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2184"><span class="ln">2184 </span></a><span class="s1">at::Tensor upsample_nearest3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2185"><span class="ln">2185 </span></a><span class="s1">at::Tensor _upsample_nearest_exact3d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_d, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_h, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scales_w); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2186"><span class="ln">2186 </span></a><span class="s1">at::Tensor &amp; sigmoid_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2187"><span class="ln">2187 </span></a><span class="s1">at::Tensor sigmoid_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sigmoid_backward(Tensor grad_output, Tensor output) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2188"><span class="ln">2188 </span></a><span class="s1">at::Tensor &amp; logit_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2189"><span class="ln">2189 </span></a><span class="s1">at::Tensor logit_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2190"><span class="ln">2190 </span></a><span class="s1">at::Tensor &amp; tanh_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, at::Tensor &amp; grad_input); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2191"><span class="ln">2191 </span></a><span class="s1">at::Tensor tanh_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tanh_backward(Tensor grad_output, Tensor output) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2192"><span class="ln">2192 </span></a><span class="s1">at::Tensor &amp; slow_conv_transpose2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2193"><span class="ln">2193 </span></a><span class="s1">at::Tensor slow_conv_transpose2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_transpose2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2194"><span class="ln">2194 </span></a><span class="s1">at::Tensor &amp; slow_conv_transpose3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt[3] dilation=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2195"><span class="ln">2195 </span></a><span class="s1">at::Tensor slow_conv_transpose3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_transpose3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt[3] dilation=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2196"><span class="ln">2196 </span></a><span class="s1">at::Tensor &amp; thnn_conv2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::thnn_conv2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2197"><span class="ln">2197 </span></a><span class="s1">at::Tensor thnn_conv2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::thnn_conv2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2198"><span class="ln">2198 </span></a><span class="s1">at::Tensor &amp; _slow_conv2d_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor &amp; output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_slow_conv2d_forward.output(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) output) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2199"><span class="ln">2199 </span></a><span class="s1">at::Tensor _slow_conv2d_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_slow_conv2d_forward(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2200"><span class="ln">2200 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _slow_conv2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor &amp; grad_input, at::Tensor &amp; grad_weight, at::Tensor &amp; grad_bias); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2201"><span class="ln">2201 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _slow_conv2d_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask) -&gt; (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2202"><span class="ln">2202 </span></a><span class="s1">at::Tensor &amp; _conv_depthwise2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_conv_depthwise2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2203"><span class="ln">2203 </span></a><span class="s1">at::Tensor _conv_depthwise2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_conv_depthwise2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2204"><span class="ln">2204 </span></a><span class="s1">at::Tensor conv_depthwise3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_depthwise3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2205"><span class="ln">2205 </span></a><span class="s1">at::Tensor &amp; slow_conv3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2206"><span class="ln">2206 </span></a><span class="s1">at::Tensor slow_conv3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2207"><span class="ln">2207 </span></a><span class="s1">at::Tensor &amp; slow_conv3d_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor &amp; output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv3d_forward.output(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, *, Tensor(a!) output) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2208"><span class="ln">2208 </span></a><span class="s1">at::Tensor slow_conv3d_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv3d_forward(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2209"><span class="ln">2209 </span></a><span class="s1">at::Tensor slow_conv_dilated2d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_dilated2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2210"><span class="ln">2210 </span></a><span class="s1">at::Tensor slow_conv_dilated3d(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_dilated3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2211"><span class="ln">2211 </span></a><span class="s1">at::Tensor &amp; col2im_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2212"><span class="ln">2212 </span></a><span class="s1">at::Tensor col2im(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2213"><span class="ln">2213 </span></a><span class="s1">at::Tensor column_stack(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::column_stack(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2214"><span class="ln">2214 </span></a><span class="s1">at::Tensor &amp; column_stack_out(at::TensorList tensors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2215"><span class="ln">2215 </span></a><span class="s1">at::Tensor &amp; im2col_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2216"><span class="ln">2216 </span></a><span class="s1">at::Tensor im2col(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2217"><span class="ln">2217 </span></a><span class="s1">at::Tensor isfinite(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isfinite(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2218"><span class="ln">2218 </span></a><span class="s1">at::Tensor isinf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isinf(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2219"><span class="ln">2219 </span></a><span class="s2">void </span><span class="s1">record_stream(at::Tensor &amp; self, at::Stream s); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::record_stream(Tensor(a!) self, Stream s) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2220"><span class="ln">2220 </span></a><span class="s1">at::Tensor isposinf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isposinf(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2221"><span class="ln">2221 </span></a><span class="s1">at::Tensor &amp; isposinf_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isposinf.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2222"><span class="ln">2222 </span></a><span class="s1">at::Tensor isneginf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isneginf(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2223"><span class="ln">2223 </span></a><span class="s1">at::Tensor &amp; isneginf_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isneginf.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2224"><span class="ln">2224 </span></a><span class="s1">at::Tensor _add_batch_dim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t batch_dim, int64_t level); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_add_batch_dim(Tensor self, int batch_dim, int level) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2225"><span class="ln">2225 </span></a><span class="s1">at::Tensor _remove_batch_dim(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t level, c10::SymInt batch_size, int64_t out_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_remove_batch_dim(Tensor self, int level, SymInt batch_size, int out_dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2226"><span class="ln">2226 </span></a><span class="s1">at::Tensor special_entr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_entr(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2227"><span class="ln">2227 </span></a><span class="s1">at::Tensor &amp; special_entr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_entr.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2228"><span class="ln">2228 </span></a><span class="s1">at::Tensor special_ndtri(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_ndtri(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2229"><span class="ln">2229 </span></a><span class="s1">at::Tensor &amp; special_ndtri_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2230"><span class="ln">2230 </span></a><span class="s1">at::Tensor special_log_ndtr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_log_ndtr(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2231"><span class="ln">2231 </span></a><span class="s1">at::Tensor &amp; special_log_ndtr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2232"><span class="ln">2232 </span></a><span class="s1">at::Tensor special_expm1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_expm1(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2233"><span class="ln">2233 </span></a><span class="s1">at::Tensor &amp; special_expm1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2234"><span class="ln">2234 </span></a><span class="s1">at::Tensor special_exp2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_exp2(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2235"><span class="ln">2235 </span></a><span class="s1">at::Tensor &amp; special_exp2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2236"><span class="ln">2236 </span></a><span class="s1">at::Tensor special_psi(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_psi(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2237"><span class="ln">2237 </span></a><span class="s1">at::Tensor &amp; special_psi_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_psi.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2238"><span class="ln">2238 </span></a><span class="s1">at::Tensor special_digamma(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_digamma(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2239"><span class="ln">2239 </span></a><span class="s1">at::Tensor &amp; special_digamma_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2240"><span class="ln">2240 </span></a><span class="s1">at::Tensor special_gammaln(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_gammaln(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2241"><span class="ln">2241 </span></a><span class="s1">at::Tensor &amp; special_gammaln_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2242"><span class="ln">2242 </span></a><span class="s1">at::Tensor special_erf(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erf(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2243"><span class="ln">2243 </span></a><span class="s1">at::Tensor &amp; special_erf_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erf.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2244"><span class="ln">2244 </span></a><span class="s1">at::Tensor special_erfc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erfc(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2245"><span class="ln">2245 </span></a><span class="s1">at::Tensor &amp; special_erfc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2246"><span class="ln">2246 </span></a><span class="s1">at::Tensor special_erfcx(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erfcx(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2247"><span class="ln">2247 </span></a><span class="s1">at::Tensor &amp; special_erfcx_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2248"><span class="ln">2248 </span></a><span class="s1">at::Tensor special_erfinv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erfinv(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2249"><span class="ln">2249 </span></a><span class="s1">at::Tensor &amp; special_erfinv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2250"><span class="ln">2250 </span></a><span class="s1">at::Tensor special_ndtr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_ndtr(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2251"><span class="ln">2251 </span></a><span class="s1">at::Tensor &amp; special_ndtr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2252"><span class="ln">2252 </span></a><span class="s1">at::Tensor special_xlog1py(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlog1py(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2253"><span class="ln">2253 </span></a><span class="s1">at::Tensor special_xlog1py(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2254"><span class="ln">2254 </span></a><span class="s1">at::Tensor special_xlog1py(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2255"><span class="ln">2255 </span></a><span class="s1">at::Tensor &amp; special_xlog1py_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2256"><span class="ln">2256 </span></a><span class="s1">at::Tensor &amp; special_xlog1py_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2257"><span class="ln">2257 </span></a><span class="s1">at::Tensor &amp; special_xlog1py_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2258"><span class="ln">2258 </span></a><span class="s1">at::Tensor special_xlogy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlogy(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2259"><span class="ln">2259 </span></a><span class="s1">at::Tensor special_xlogy(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlogy.self_scalar(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2260"><span class="ln">2260 </span></a><span class="s1">at::Tensor special_xlogy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlogy.other_scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2261"><span class="ln">2261 </span></a><span class="s1">at::Tensor &amp; special_xlogy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2262"><span class="ln">2262 </span></a><span class="s1">at::Tensor &amp; special_xlogy_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2263"><span class="ln">2263 </span></a><span class="s1">at::Tensor &amp; special_xlogy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2264"><span class="ln">2264 </span></a><span class="s1">at::Tensor special_zeta(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_zeta(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2265"><span class="ln">2265 </span></a><span class="s1">at::Tensor special_zeta(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_zeta.self_scalar(Scalar self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2266"><span class="ln">2266 </span></a><span class="s1">at::Tensor special_zeta(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_zeta.other_scalar(Tensor self, Scalar other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2267"><span class="ln">2267 </span></a><span class="s1">at::Tensor &amp; special_zeta_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2268"><span class="ln">2268 </span></a><span class="s1">at::Tensor &amp; special_zeta_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2269"><span class="ln">2269 </span></a><span class="s1">at::Tensor &amp; special_zeta_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2270"><span class="ln">2270 </span></a><span class="s1">at::Tensor special_i0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i0(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2271"><span class="ln">2271 </span></a><span class="s1">at::Tensor &amp; special_i0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i0.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2272"><span class="ln">2272 </span></a><span class="s1">at::Tensor special_i0e(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i0e(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2273"><span class="ln">2273 </span></a><span class="s1">at::Tensor &amp; special_i0e_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2274"><span class="ln">2274 </span></a><span class="s1">at::Tensor special_i1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i1(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2275"><span class="ln">2275 </span></a><span class="s1">at::Tensor &amp; special_i1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i1.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2276"><span class="ln">2276 </span></a><span class="s1">at::Tensor special_i1e(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i1e(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2277"><span class="ln">2277 </span></a><span class="s1">at::Tensor &amp; special_i1e_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2278"><span class="ln">2278 </span></a><span class="s1">at::Tensor special_logit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_logit(Tensor self, float? eps=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2279"><span class="ln">2279 </span></a><span class="s1">at::Tensor &amp; special_logit_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; eps, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2280"><span class="ln">2280 </span></a><span class="s1">at::Tensor special_polygamma(int64_t n, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_polygamma(int n, Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2281"><span class="ln">2281 </span></a><span class="s1">at::Tensor &amp; special_polygamma_out(int64_t n, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2282"><span class="ln">2282 </span></a><span class="s1">at::Tensor special_logsumexp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2283"><span class="ln">2283 </span></a><span class="s1">at::Tensor &amp; special_logsumexp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2284"><span class="ln">2284 </span></a><span class="s1">at::Tensor special_expit(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_expit(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2285"><span class="ln">2285 </span></a><span class="s1">at::Tensor &amp; special_expit_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_expit.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2286"><span class="ln">2286 </span></a><span class="s1">at::Tensor special_sinc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_sinc(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2287"><span class="ln">2287 </span></a><span class="s1">at::Tensor &amp; special_sinc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2288"><span class="ln">2288 </span></a><span class="s1">at::Tensor special_round(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t decimals); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_round(Tensor self, *, int decimals=0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2289"><span class="ln">2289 </span></a><span class="s1">at::Tensor &amp; special_round_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t decimals, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_round.out(Tensor self, *, int decimals=0, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2290"><span class="ln">2290 </span></a><span class="s1">at::Tensor special_log1p(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_log1p(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2291"><span class="ln">2291 </span></a><span class="s1">at::Tensor &amp; special_log1p_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2292"><span class="ln">2292 </span></a><span class="s1">at::Tensor special_log_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2293"><span class="ln">2293 </span></a><span class="s1">at::Tensor &amp; special_gammainc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2294"><span class="ln">2294 </span></a><span class="s1">at::Tensor special_gammainc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_gammainc(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2295"><span class="ln">2295 </span></a><span class="s1">at::Tensor &amp; special_gammaincc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2296"><span class="ln">2296 </span></a><span class="s1">at::Tensor special_gammaincc(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_gammaincc(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2297"><span class="ln">2297 </span></a><span class="s1">at::Tensor special_multigammaln(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_multigammaln(Tensor self, int p) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2298"><span class="ln">2298 </span></a><span class="s1">at::Tensor &amp; special_multigammaln_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2299"><span class="ln">2299 </span></a><span class="s1">at::Tensor special_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_softmax(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2300"><span class="ln">2300 </span></a><span class="s1">at::Tensor fft_fft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2301"><span class="ln">2301 </span></a><span class="s1">at::Tensor &amp; fft_fft_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2302"><span class="ln">2302 </span></a><span class="s1">at::Tensor fft_ifft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ifft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2303"><span class="ln">2303 </span></a><span class="s1">at::Tensor &amp; fft_ifft_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ifft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2304"><span class="ln">2304 </span></a><span class="s1">at::Tensor fft_rfft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2305"><span class="ln">2305 </span></a><span class="s1">at::Tensor &amp; fft_rfft_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2306"><span class="ln">2306 </span></a><span class="s1">at::Tensor fft_irfft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2307"><span class="ln">2307 </span></a><span class="s1">at::Tensor &amp; fft_irfft_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2308"><span class="ln">2308 </span></a><span class="s1">at::Tensor fft_hfft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_hfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2309"><span class="ln">2309 </span></a><span class="s1">at::Tensor &amp; fft_hfft_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_hfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2310"><span class="ln">2310 </span></a><span class="s1">at::Tensor fft_ihfft(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ihfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2311"><span class="ln">2311 </span></a><span class="s1">at::Tensor &amp; fft_ihfft_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;c10::SymInt&gt; n, int64_t dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ihfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2312"><span class="ln">2312 </span></a><span class="s1">at::Tensor fft_fft2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2313"><span class="ln">2313 </span></a><span class="s1">at::Tensor &amp; fft_fft2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2314"><span class="ln">2314 </span></a><span class="s1">at::Tensor fft_ifft2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2315"><span class="ln">2315 </span></a><span class="s1">at::Tensor &amp; fft_ifft2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2316"><span class="ln">2316 </span></a><span class="s1">at::Tensor fft_rfft2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2317"><span class="ln">2317 </span></a><span class="s1">at::Tensor &amp; fft_rfft2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2318"><span class="ln">2318 </span></a><span class="s1">at::Tensor fft_irfft2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2319"><span class="ln">2319 </span></a><span class="s1">at::Tensor &amp; fft_irfft2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2320"><span class="ln">2320 </span></a><span class="s1">at::Tensor fft_hfft2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2321"><span class="ln">2321 </span></a><span class="s1">at::Tensor &amp; fft_hfft2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2322"><span class="ln">2322 </span></a><span class="s1">at::Tensor fft_ihfft2(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ihfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2323"><span class="ln">2323 </span></a><span class="s1">at::Tensor &amp; fft_ihfft2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ihfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2324"><span class="ln">2324 </span></a><span class="s1">at::Tensor fft_fftn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2325"><span class="ln">2325 </span></a><span class="s1">at::Tensor &amp; fft_fftn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2326"><span class="ln">2326 </span></a><span class="s1">at::Tensor fft_ifftn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2327"><span class="ln">2327 </span></a><span class="s1">at::Tensor &amp; fft_ifftn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2328"><span class="ln">2328 </span></a><span class="s1">at::Tensor fft_rfftn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2329"><span class="ln">2329 </span></a><span class="s1">at::Tensor &amp; fft_rfftn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2330"><span class="ln">2330 </span></a><span class="s1">at::Tensor fft_irfftn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_irfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2331"><span class="ln">2331 </span></a><span class="s1">at::Tensor &amp; fft_irfftn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_irfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2332"><span class="ln">2332 </span></a><span class="s1">at::Tensor fft_hfftn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_hfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2333"><span class="ln">2333 </span></a><span class="s1">at::Tensor &amp; fft_hfftn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_hfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2334"><span class="ln">2334 </span></a><span class="s1">at::Tensor fft_ihfftn(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2335"><span class="ln">2335 </span></a><span class="s1">at::Tensor &amp; fft_ihfftn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, ::std::optional&lt;c10::string_view&gt; norm, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2336"><span class="ln">2336 </span></a><span class="s1">at::Tensor fft_fftfreq(int64_t n, </span><span class="s2">double </span><span class="s1">d, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2337"><span class="ln">2337 </span></a><span class="s1">at::Tensor &amp; fft_fftfreq_out(int64_t n, </span><span class="s2">double </span><span class="s1">d, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2338"><span class="ln">2338 </span></a><span class="s1">at::Tensor fft_rfftfreq(int64_t n, </span><span class="s2">double </span><span class="s1">d, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2339"><span class="ln">2339 </span></a><span class="s1">at::Tensor &amp; fft_rfftfreq_out(int64_t n, </span><span class="s2">double </span><span class="s1">d, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2340"><span class="ln">2340 </span></a><span class="s1">at::Tensor fft_fftshift(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_fftshift(Tensor self, int[1]? dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2341"><span class="ln">2341 </span></a><span class="s1">at::Tensor fft_ifftshift(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fft_ifftshift(Tensor self, int[1]? dim=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2342"><span class="ln">2342 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_cholesky_ex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper, </span><span class="s2">bool </span><span class="s1">check_errors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -&gt; (Tensor L, Tensor info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2343"><span class="ln">2343 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_cholesky_ex_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper, </span><span class="s2">bool </span><span class="s1">check_errors, at::Tensor &amp; L, at::Tensor &amp; info); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -&gt; (Tensor(a!) L, Tensor(b!) info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2344"><span class="ln">2344 </span></a><span class="s1">at::Tensor linalg_cholesky(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cholesky(Tensor self, *, bool upper=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2345"><span class="ln">2345 </span></a><span class="s1">at::Tensor &amp; linalg_cholesky_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">upper, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2346"><span class="ln">2346 </span></a><span class="s1">at::Tensor linalg_cross(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cross(Tensor self, Tensor other, *, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2347"><span class="ln">2347 </span></a><span class="s1">at::Tensor &amp; linalg_cross_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2348"><span class="ln">2348 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_lu_factor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">pivot); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu_factor(Tensor A, *, bool pivot=True) -&gt; (Tensor LU, Tensor pivots)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2349"><span class="ln">2349 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_lu_factor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">pivot, at::Tensor &amp; LU, at::Tensor &amp; pivots); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu_factor.out(Tensor A, *, bool pivot=True, Tensor(a!) LU, Tensor(b!) pivots) -&gt; (Tensor(a!) LU, Tensor(b!) pivots)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2350"><span class="ln">2350 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; linalg_lu_factor_ex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">pivot, </span><span class="s2">bool </span><span class="s1">check_errors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -&gt; (Tensor LU, Tensor pivots, Tensor info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2351"><span class="ln">2351 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; linalg_lu_factor_ex_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">pivot, </span><span class="s2">bool </span><span class="s1">check_errors, at::Tensor &amp; LU, at::Tensor &amp; pivots, at::Tensor &amp; info); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -&gt; (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2352"><span class="ln">2352 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; linalg_lu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">pivot); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu(Tensor A, *, bool pivot=True) -&gt; (Tensor P, Tensor L, Tensor U)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2353"><span class="ln">2353 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; linalg_lu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">pivot, at::Tensor &amp; P, at::Tensor &amp; L, at::Tensor &amp; U); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu.out(Tensor A, *, bool pivot=True, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -&gt; (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2354"><span class="ln">2354 </span></a><span class="s1">at::Tensor linalg_lu_solve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; pivots, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">adjoint); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu_solve(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2355"><span class="ln">2355 </span></a><span class="s1">at::Tensor &amp; linalg_lu_solve_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; LU, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; pivots, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">adjoint, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lu_solve.out(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2356"><span class="ln">2356 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _linalg_det(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_det(Tensor A) -&gt; (Tensor result, Tensor LU, Tensor pivots)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2357"><span class="ln">2357 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _linalg_det_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, at::Tensor &amp; result, at::Tensor &amp; LU, at::Tensor &amp; pivots); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -&gt; (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2358"><span class="ln">2358 </span></a><span class="s1">at::Tensor linalg_det(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_det(Tensor A) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2359"><span class="ln">2359 </span></a><span class="s1">at::Tensor &amp; linalg_det_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_det.out(Tensor A, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2360"><span class="ln">2360 </span></a><span class="s1">at::Tensor det(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::det(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2361"><span class="ln">2361 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; linalg_ldl_factor_ex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">hermitian, </span><span class="s2">bool </span><span class="s1">check_errors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_ldl_factor_ex(Tensor self, *, bool hermitian=False, bool check_errors=False) -&gt; (Tensor LD, Tensor pivots, Tensor info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2362"><span class="ln">2362 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; linalg_ldl_factor_ex_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">hermitian, </span><span class="s2">bool </span><span class="s1">check_errors, at::Tensor &amp; LD, at::Tensor &amp; pivots, at::Tensor &amp; info); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_ldl_factor_ex.out(Tensor self, *, bool hermitian=False, bool check_errors=False, Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) -&gt; (Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2363"><span class="ln">2363 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_ldl_factor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_ldl_factor(Tensor self, *, bool hermitian=False) -&gt; (Tensor LD, Tensor pivots)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2364"><span class="ln">2364 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_ldl_factor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; LD, at::Tensor &amp; pivots); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -&gt; (Tensor(a!) LD, Tensor(b!) pivots)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2365"><span class="ln">2365 </span></a><span class="s1">at::Tensor linalg_ldl_solve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; LD, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; pivots, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_ldl_solve(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2366"><span class="ln">2366 </span></a><span class="s1">at::Tensor &amp; linalg_ldl_solve_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; LD, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; pivots, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_ldl_solve.out(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2367"><span class="ln">2367 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; linalg_lstsq(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; rcond, ::std::optional&lt;c10::string_view&gt; driver); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -&gt; (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2368"><span class="ln">2368 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; linalg_lstsq_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; b, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; rcond, ::std::optional&lt;c10::string_view&gt; driver, at::Tensor &amp; solution, at::Tensor &amp; residuals, at::Tensor &amp; rank, at::Tensor &amp; singular_values); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -&gt; (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2369"><span class="ln">2369 </span></a><span class="s1">at::Tensor linalg_matmul(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matmul(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2370"><span class="ln">2370 </span></a><span class="s1">at::Tensor &amp; linalg_matmul_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2371"><span class="ln">2371 </span></a><span class="s1">at::Tensor linalg_vecdot(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_vecdot(Tensor x, Tensor y, *, int dim=-1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2372"><span class="ln">2372 </span></a><span class="s1">at::Tensor &amp; linalg_vecdot_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; y, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_vecdot.out(Tensor x, Tensor y, *, int dim=-1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2373"><span class="ln">2373 </span></a><span class="s1">at::Tensor linalg_matrix_exp(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_exp(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2374"><span class="ln">2374 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _linalg_slogdet(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_slogdet(Tensor A) -&gt; (Tensor sign, Tensor logabsdet, Tensor LU, Tensor pivots)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2375"><span class="ln">2375 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _linalg_slogdet_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, at::Tensor &amp; sign, at::Tensor &amp; logabsdet, at::Tensor &amp; LU, at::Tensor &amp; pivots); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_slogdet.sign(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots) -&gt; (Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2376"><span class="ln">2376 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_slogdet(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_slogdet(Tensor A) -&gt; (Tensor sign, Tensor logabsdet)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2377"><span class="ln">2377 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_slogdet_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, at::Tensor &amp; sign, at::Tensor &amp; logabsdet); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -&gt; (Tensor(a!) sign, Tensor(b!) logabsdet)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2378"><span class="ln">2378 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; slogdet(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slogdet(Tensor self) -&gt; (Tensor sign, Tensor logabsdet)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2379"><span class="ln">2379 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; slogdet_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; sign, at::Tensor &amp; logabsdet); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -&gt; (Tensor(a!) sign, Tensor(b!) logabsdet)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2380"><span class="ln">2380 </span></a><span class="s1">at::Tensor logdet(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::logdet(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2381"><span class="ln">2381 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_eig(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eig(Tensor self) -&gt; (Tensor eigenvalues, Tensor eigenvectors)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2382"><span class="ln">2382 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_eig_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; eigenvalues, at::Tensor &amp; eigenvectors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -&gt; (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2383"><span class="ln">2383 </span></a><span class="s1">at::Tensor _linalg_eigvals(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_eigvals(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2384"><span class="ln">2384 </span></a><span class="s1">at::Tensor linalg_eigvals(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eigvals(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2385"><span class="ln">2385 </span></a><span class="s1">at::Tensor &amp; linalg_eigvals_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2386"><span class="ln">2386 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _linalg_eigh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, c10::string_view UPLO, </span><span class="s2">bool </span><span class="s1">compute_v); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_eigh(Tensor A, str UPLO=\&quot;L\&quot;, bool compute_v=True) -&gt; (Tensor eigenvalues, Tensor eigenvectors)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2387"><span class="ln">2387 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _linalg_eigh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, c10::string_view UPLO, </span><span class="s2">bool </span><span class="s1">compute_v, at::Tensor &amp; eigenvalues, at::Tensor &amp; eigenvectors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_eigh.eigenvalues(Tensor A, str UPLO=\&quot;L\&quot;, bool compute_v=True, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -&gt; (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2388"><span class="ln">2388 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_eigh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view UPLO); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eigh(Tensor self, str UPLO=\&quot;L\&quot;) -&gt; (Tensor eigenvalues, Tensor eigenvectors)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2389"><span class="ln">2389 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_eigh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view UPLO, at::Tensor &amp; eigvals, at::Tensor &amp; eigvecs); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eigh.eigvals(Tensor self, str UPLO=\&quot;L\&quot;, *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -&gt; (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2390"><span class="ln">2390 </span></a><span class="s1">at::Tensor linalg_eigvalsh(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view UPLO); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eigvalsh(Tensor self, str UPLO=\&quot;L\&quot;) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2391"><span class="ln">2391 </span></a><span class="s1">at::Tensor &amp; linalg_eigvalsh_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view UPLO, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_eigvalsh.out(Tensor self, str UPLO=\&quot;L\&quot;, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2392"><span class="ln">2392 </span></a><span class="s1">at::Tensor linalg_householder_product(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tau); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_householder_product(Tensor input, Tensor tau) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2393"><span class="ln">2393 </span></a><span class="s1">at::Tensor &amp; linalg_householder_product_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tau, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2394"><span class="ln">2394 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_inv_ex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">check_errors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_inv_ex(Tensor A, *, bool check_errors=False) -&gt; (Tensor inverse, Tensor info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2395"><span class="ln">2395 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_inv_ex_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">check_errors, at::Tensor &amp; inverse, at::Tensor &amp; info); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -&gt; (Tensor(a!) inverse, Tensor(b!) info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2396"><span class="ln">2396 </span></a><span class="s1">at::Tensor linalg_inv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_inv(Tensor A) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2397"><span class="ln">2397 </span></a><span class="s1">at::Tensor &amp; linalg_inv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_inv.out(Tensor A, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2398"><span class="ln">2398 </span></a><span class="s1">at::Tensor inverse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::inverse(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2399"><span class="ln">2399 </span></a><span class="s1">at::Tensor &amp; inverse_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::inverse.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2400"><span class="ln">2400 </span></a><span class="s1">at::Tensor inner(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::inner(Tensor self, Tensor other) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2401"><span class="ln">2401 </span></a><span class="s1">at::Tensor &amp; inner_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2402"><span class="ln">2402 </span></a><span class="s1">at::Tensor outer(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::outer(Tensor self, Tensor vec2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2403"><span class="ln">2403 </span></a><span class="s1">at::Tensor &amp; outer_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2404"><span class="ln">2404 </span></a><span class="s1">at::Tensor ger(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ger(Tensor self, Tensor vec2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2405"><span class="ln">2405 </span></a><span class="s1">at::Tensor &amp; ger_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; vec2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2406"><span class="ln">2406 </span></a><span class="s1">at::Tensor linalg_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; ord, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2407"><span class="ln">2407 </span></a><span class="s1">at::Tensor linalg_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view ord, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2408"><span class="ln">2408 </span></a><span class="s1">at::Tensor &amp; linalg_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; ord, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2409"><span class="ln">2409 </span></a><span class="s1">at::Tensor &amp; linalg_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view ord, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2410"><span class="ln">2410 </span></a><span class="s1">at::Tensor linalg_vector_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; ord, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2411"><span class="ln">2411 </span></a><span class="s1">at::Tensor &amp; linalg_vector_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; ord, at::OptionalIntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2412"><span class="ln">2412 </span></a><span class="s1">at::Tensor linalg_matrix_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; ord, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2413"><span class="ln">2413 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; ord, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2414"><span class="ln">2414 </span></a><span class="s1">at::Tensor linalg_matrix_norm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view ord, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2415"><span class="ln">2415 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view ord, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2416"><span class="ln">2416 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _linalg_svd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">full_matrices, </span><span class="s2">bool </span><span class="s1">compute_uv, ::std::optional&lt;c10::string_view&gt; driver); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_svd(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None) -&gt; (Tensor U, Tensor S, Tensor Vh)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2417"><span class="ln">2417 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _linalg_svd_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">full_matrices, </span><span class="s2">bool </span><span class="s1">compute_uv, ::std::optional&lt;c10::string_view&gt; driver, at::Tensor &amp; U, at::Tensor &amp; S, at::Tensor &amp; Vh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_svd.U(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -&gt; (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2418"><span class="ln">2418 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; linalg_svd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">full_matrices, ::std::optional&lt;c10::string_view&gt; driver); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_svd(Tensor A, bool full_matrices=True, *, str? driver=None) -&gt; (Tensor U, Tensor S, Tensor Vh)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2419"><span class="ln">2419 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; linalg_svd_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">full_matrices, ::std::optional&lt;c10::string_view&gt; driver, at::Tensor &amp; U, at::Tensor &amp; S, at::Tensor &amp; Vh); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_svd.U(Tensor A, bool full_matrices=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -&gt; (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2420"><span class="ln">2420 </span></a><span class="s1">at::Tensor linalg_svdvals(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, ::std::optional&lt;c10::string_view&gt; driver); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_svdvals(Tensor A, *, str? driver=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2421"><span class="ln">2421 </span></a><span class="s1">at::Tensor &amp; linalg_svdvals_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, ::std::optional&lt;c10::string_view&gt; driver, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_svdvals.out(Tensor A, *, str? driver=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2422"><span class="ln">2422 </span></a><span class="s1">at::Tensor linalg_cond(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cond(Tensor self, Scalar? p=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2423"><span class="ln">2423 </span></a><span class="s1">at::Tensor &amp; linalg_cond_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2424"><span class="ln">2424 </span></a><span class="s1">at::Tensor linalg_cond(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cond.p_str(Tensor self, str p) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2425"><span class="ln">2425 </span></a><span class="s1">at::Tensor &amp; linalg_cond_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::string_view p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2426"><span class="ln">2426 </span></a><span class="s1">at::Tensor linalg_pinv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; atol, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; rtol, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2427"><span class="ln">2427 </span></a><span class="s1">at::Tensor &amp; linalg_pinv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; atol, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; rtol, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2428"><span class="ln">2428 </span></a><span class="s1">at::Tensor linalg_pinv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; atol, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; rtol, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2429"><span class="ln">2429 </span></a><span class="s1">at::Tensor &amp; linalg_pinv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; atol, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; rtol, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2430"><span class="ln">2430 </span></a><span class="s1">at::Tensor linalg_pinv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">rcond, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv(Tensor self, float rcond, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2431"><span class="ln">2431 </span></a><span class="s1">at::Tensor linalg_pinv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; rcond, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2432"><span class="ln">2432 </span></a><span class="s1">at::Tensor &amp; linalg_pinv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">rcond, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2433"><span class="ln">2433 </span></a><span class="s1">at::Tensor &amp; linalg_pinv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; rcond, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2434"><span class="ln">2434 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _linalg_solve_ex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">check_errors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -&gt; (Tensor result, Tensor LU, Tensor pivots, Tensor info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2435"><span class="ln">2435 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _linalg_solve_ex_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">check_errors, at::Tensor &amp; result, at::Tensor &amp; LU, at::Tensor &amp; pivots, at::Tensor &amp; info); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_linalg_solve_ex.result(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info) -&gt; (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2436"><span class="ln">2436 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_solve_ex(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">check_errors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -&gt; (Tensor result, Tensor info)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2437"><span class="ln">2437 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_solve_ex_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left, </span><span class="s2">bool </span><span class="s1">check_errors, at::Tensor &amp; result, at::Tensor &amp; info); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -&gt; (Tensor(a!) result, Tensor(b!) info)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2438"><span class="ln">2438 </span></a><span class="s1">at::Tensor linalg_solve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_solve(Tensor A, Tensor B, *, bool left=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2439"><span class="ln">2439 </span></a><span class="s1">at::Tensor _spsolve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_spsolve(Tensor A, Tensor B, *, bool left=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2440"><span class="ln">2440 </span></a><span class="s1">at::Tensor &amp; linalg_solve_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; B, </span><span class="s2">bool </span><span class="s1">left, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2441"><span class="ln">2441 </span></a><span class="s1">at::Tensor linalg_tensorinv(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t ind); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_tensorinv(Tensor self, int ind=2) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2442"><span class="ln">2442 </span></a><span class="s1">at::Tensor &amp; linalg_tensorinv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t ind, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2443"><span class="ln">2443 </span></a><span class="s1">at::Tensor linalg_tensorsolve(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::OptionalIntArrayRef dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2444"><span class="ln">2444 </span></a><span class="s1">at::Tensor &amp; linalg_tensorsolve_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::OptionalIntArrayRef dims, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2445"><span class="ln">2445 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; linalg_qr(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, c10::string_view mode); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_qr(Tensor A, str mode='reduced') -&gt; (Tensor Q, Tensor R)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2446"><span class="ln">2446 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; linalg_qr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, c10::string_view mode, at::Tensor &amp; Q, at::Tensor &amp; R); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_qr.out(Tensor A, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -&gt; (Tensor(a!) Q, Tensor(b!) R)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2447"><span class="ln">2447 </span></a><span class="s1">at::Tensor linalg_matrix_power(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_power(Tensor self, int n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2448"><span class="ln">2448 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_power_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2449"><span class="ln">2449 </span></a><span class="s1">at::Tensor linalg_matrix_rank(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; atol, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; rtol, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2450"><span class="ln">2450 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_rank_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; atol, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; rtol, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2451"><span class="ln">2451 </span></a><span class="s1">at::Tensor linalg_matrix_rank(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; atol, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; rtol, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2452"><span class="ln">2452 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_rank_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; atol, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; rtol, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2453"><span class="ln">2453 </span></a><span class="s1">at::Tensor linalg_matrix_rank(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">tol, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2454"><span class="ln">2454 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_rank_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">tol, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2455"><span class="ln">2455 </span></a><span class="s1">at::Tensor linalg_matrix_rank(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tol, </span><span class="s2">bool </span><span class="s1">hermitian); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2456"><span class="ln">2456 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_rank_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tol, </span><span class="s2">bool </span><span class="s1">hermitian, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2457"><span class="ln">2457 </span></a><span class="s1">at::Tensor linalg_multi_dot(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_multi_dot(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2458"><span class="ln">2458 </span></a><span class="s1">at::Tensor &amp; linalg_multi_dot_out(at::TensorList tensors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2459"><span class="ln">2459 </span></a><span class="s1">at::Tensor nested_to_padded_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">padding, at::OptionalIntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nested_to_padded_tensor(Tensor self, float padding, int[]? output_size=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2460"><span class="ln">2460 </span></a><span class="s1">at::Tensor _test_serialization_subcmul(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2461"><span class="ln">2461 </span></a><span class="s1">at::Tensor _test_parallel_materialize(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t num_parallel, </span><span class="s2">bool </span><span class="s1">skip_first); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_parallel_materialize(Tensor self, int num_parallel, bool skip_first=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2462"><span class="ln">2462 </span></a><span class="s1">at::Tensor _test_optional_intlist(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::OptionalIntArrayRef addends); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_optional_intlist(Tensor values, int[]? addends) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2463"><span class="ln">2463 </span></a><span class="s1">at::Tensor _test_optional_filled_intlist(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::OptionalIntArrayRef addends); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2464"><span class="ln">2464 </span></a><span class="s1">at::Tensor _test_optional_floatlist(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; addends); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_optional_floatlist(Tensor values, float[]? addends) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2465"><span class="ln">2465 </span></a><span class="s1">at::Tensor _test_string_default(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; dummy, c10::string_view a, c10::string_view b); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_string_default(Tensor dummy, str a=\&quot;\\\&quot;'\\\\\&quot;, str b='\&quot;\\'\\\\') -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2466"><span class="ln">2466 </span></a><span class="s1">at::Tensor _test_ambiguous_defaults(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; dummy, int64_t a, int64_t b); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2467"><span class="ln">2467 </span></a><span class="s1">at::Tensor _test_ambiguous_defaults(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; dummy, int64_t a, c10::string_view b); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_ambiguous_defaults.b(Tensor dummy, int a=2, str b=\&quot;2\&quot;) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2468"><span class="ln">2468 </span></a><span class="s1">at::Tensor _test_warn_in_autograd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_warn_in_autograd(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2469"><span class="ln">2469 </span></a><span class="s1">at::Tensor _test_autograd_multiple_dispatch(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_autograd_multiple_dispatch.fullcoverage(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2470"><span class="ln">2470 </span></a><span class="s1">at::Tensor _test_autograd_multiple_dispatch(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">b); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_autograd_multiple_dispatch.ntonly(Tensor self, bool b) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2471"><span class="ln">2471 </span></a><span class="s1">at::Tensor _test_autograd_multiple_dispatch_view(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_autograd_multiple_dispatch_view(Tensor(a) self) -&gt; Tensor(a)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2472"><span class="ln">2472 </span></a><span class="s1">at::Tensor _test_autograd_multiple_dispatch_view_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_autograd_multiple_dispatch_view_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2473"><span class="ln">2473 </span></a><span class="s1">at::Tensor segment_reduce(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, c10::string_view reduce, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; lengths, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; offsets, int64_t axis, </span><span class="s2">bool </span><span class="s1">unsafe, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; initial); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2474"><span class="ln">2474 </span></a><span class="s1">at::Tensor _segment_reduce_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, c10::string_view reduce, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; lengths, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; offsets, int64_t axis, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; initial); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, Tensor? offsets=None, int axis=0, Scalar? initial=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2475"><span class="ln">2475 </span></a><span class="s1">at::Tensor pad_sequence(at::TensorList sequences, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">padding_value, c10::string_view padding_side); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0, str padding_side=\&quot;right\&quot;) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2476"><span class="ln">2476 </span></a><span class="s1">at::Tensor flatten_dense_tensors(at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flatten_dense_tensors(Tensor[] tensors) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2477"><span class="ln">2477 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; unflatten_dense_tensors(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; flat, at::TensorList tensors); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2478"><span class="ln">2478 </span></a><span class="s1">at::Tensor _nested_tensor_from_tensor_list(at::TensorList list, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_from_tensor_list(Tensor[] list, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2479"><span class="ln">2479 </span></a><span class="s1">at::Tensor _fw_primal_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t level); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fw_primal_copy(Tensor self, int level) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2480"><span class="ln">2480 </span></a><span class="s1">at::Tensor _make_dual_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; primal, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tangent, int64_t level); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_dual_copy(Tensor primal, Tensor tangent, int level) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2481"><span class="ln">2481 </span></a><span class="s1">at::Tensor view_as_real_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_as_real_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2482"><span class="ln">2482 </span></a><span class="s1">at::Tensor view_as_complex_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_as_complex_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2483"><span class="ln">2483 </span></a><span class="s1">at::Tensor _conj_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_conj_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2484"><span class="ln">2484 </span></a><span class="s1">at::Tensor _neg_view_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_neg_view_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2485"><span class="ln">2485 </span></a><span class="s1">at::Tensor as_strided_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;c10::SymInt&gt; storage_offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2486"><span class="ln">2486 </span></a><span class="s1">at::Tensor _sparse_broadcast_to_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_broadcast_to_copy(Tensor self, int[] size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2487"><span class="ln">2487 </span></a><span class="s1">at::Tensor diagonal_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t offset, int64_t dim1, int64_t dim2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal_copy(Tensor self, int offset=0, int dim1=0, int dim2=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2488"><span class="ln">2488 </span></a><span class="s1">at::Tensor expand_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, </span><span class="s2">bool </span><span class="s1">implicit); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::expand_copy(Tensor self, SymInt[] size, *, bool implicit=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2489"><span class="ln">2489 </span></a><span class="s1">at::Tensor permute_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dims); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::permute_copy(Tensor self, int[] dims) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2490"><span class="ln">2490 </span></a><span class="s1">at::Tensor _reshape_alias_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2491"><span class="ln">2491 </span></a><span class="s1">at::Tensor select_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymInt index); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select_copy.int(Tensor self, int dim, SymInt index) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2492"><span class="ln">2492 </span></a><span class="s1">at::Tensor detach_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::detach_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2493"><span class="ln">2493 </span></a><span class="s1">at::Tensor slice_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;c10::SymInt&gt; start, ::std::optional&lt;c10::SymInt&gt; end, c10::SymInt step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice_copy.Tensor(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2494"><span class="ln">2494 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; split_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt split_size, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2495"><span class="ln">2495 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; split_with_sizes_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef split_sizes, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::split_with_sizes_copy(Tensor self, SymInt[] split_sizes, int dim=0) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2496"><span class="ln">2496 </span></a><span class="s1">at::Tensor squeeze_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2497"><span class="ln">2497 </span></a><span class="s1">at::Tensor squeeze_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_copy.dim(Tensor self, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2498"><span class="ln">2498 </span></a><span class="s1">at::Tensor squeeze_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_copy.dims(Tensor self, int[] dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2499"><span class="ln">2499 </span></a><span class="s1">at::Tensor t_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::t_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2500"><span class="ln">2500 </span></a><span class="s1">at::Tensor transpose_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim0, int64_t dim1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::transpose_copy.int(Tensor self, int dim0, int dim1) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2501"><span class="ln">2501 </span></a><span class="s1">at::Tensor unsqueeze_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsqueeze_copy(Tensor self, int dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2502"><span class="ln">2502 </span></a><span class="s1">at::Tensor _indices_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_indices_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2503"><span class="ln">2503 </span></a><span class="s1">at::Tensor _values_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_values_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2504"><span class="ln">2504 </span></a><span class="s1">at::Tensor indices_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::indices_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2505"><span class="ln">2505 </span></a><span class="s1">at::Tensor values_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::values_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2506"><span class="ln">2506 </span></a><span class="s1">at::Tensor crow_indices_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::crow_indices_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2507"><span class="ln">2507 </span></a><span class="s1">at::Tensor col_indices_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::col_indices_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2508"><span class="ln">2508 </span></a><span class="s1">at::Tensor ccol_indices_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ccol_indices_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2509"><span class="ln">2509 </span></a><span class="s1">at::Tensor row_indices_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::row_indices_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2510"><span class="ln">2510 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; unbind_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unbind_copy.int(Tensor self, int dim=0) -&gt; Tensor[]&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2511"><span class="ln">2511 </span></a><span class="s2">void </span><span class="s1">unbind_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unbind_copy.int_out(Tensor self, int dim=0, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2512"><span class="ln">2512 </span></a><span class="s2">void </span><span class="s1">split_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt split_size, int64_t dim, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2513"><span class="ln">2513 </span></a><span class="s2">void </span><span class="s1">split_with_sizes_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2514"><span class="ln">2514 </span></a><span class="s1">at::Tensor view_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_copy(Tensor self, SymInt[] size) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2515"><span class="ln">2515 </span></a><span class="s1">at::Tensor view_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ScalarType dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_copy.dtype(Tensor self, ScalarType dtype) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2516"><span class="ln">2516 </span></a><span class="s1">at::Tensor unfold_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dimension, int64_t size, int64_t step); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unfold_copy(Tensor self, int dimension, int size, int step) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2517"><span class="ln">2517 </span></a><span class="s1">at::Tensor alias_copy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::alias_copy(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2518"><span class="ln">2518 </span></a><span class="s1">at::Tensor to_padded_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">padding, at::OptionalSymIntArrayRef output_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2519"><span class="ln">2519 </span></a><span class="s1">at::Tensor _jagged_to_padded_dense_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::TensorList offsets, c10::SymIntArrayRef max_lengths, </span><span class="s2">double </span><span class="s1">padding_value); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_jagged_to_padded_dense_forward(Tensor values, Tensor[] offsets, SymInt[] max_lengths, float padding_value=0.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2520"><span class="ln">2520 </span></a><span class="s1">at::Tensor _padded_dense_to_jagged_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; dense, at::TensorList offsets, ::std::optional&lt;c10::SymInt&gt; total_L); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_padded_dense_to_jagged_forward(Tensor dense, Tensor[] offsets, SymInt? total_L=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2521"><span class="ln">2521 </span></a><span class="s1">at::Tensor _nested_from_padded_tensor(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; padded, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dummy, int64_t ragged_idx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min_seqlen, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max_seqlen, ::std::optional&lt;c10::SymInt&gt; sum_S); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_from_padded_tensor(Tensor padded, Tensor offsets, Tensor dummy, int ragged_idx=1, Tensor? min_seqlen=None, Tensor? max_seqlen=None, SymInt? sum_S=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2522"><span class="ln">2522 </span></a><span class="s1">at::Tensor _nested_tensor_softmax_with_shape(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_softmax_with_shape(Tensor self, Tensor query) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2523"><span class="ln">2523 </span></a><span class="s1">at::Tensor _safe_softmax(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;at::ScalarType&gt; dtype); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_safe_softmax(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2524"><span class="ln">2524 </span></a><span class="s1">at::Tensor _transformer_encoder_layer_fwd(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t embed_dim, int64_t num_heads, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_bias, </span><span class="s2">bool </span><span class="s1">use_gelu, </span><span class="s2">bool </span><span class="s1">norm_first, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_weight_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_bias_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_weight_2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_bias_2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_weight_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_bias_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_weight_2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_bias_2, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; mask, ::std::optional&lt;int64_t&gt; mask_type); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2525"><span class="ln">2525 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _native_multi_head_attention(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, int64_t embed_dim, int64_t num_head, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; mask, </span><span class="s2">bool </span><span class="s1">need_weights, </span><span class="s2">bool </span><span class="s1">average_attn_weights, ::std::optional&lt;int64_t&gt; mask_type); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2526"><span class="ln">2526 </span></a><span class="s1">at::Tensor scaled_dot_product_attention(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_mask, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale, </span><span class="s2">bool </span><span class="s1">enable_gqa); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2527"><span class="ln">2527 </span></a><span class="s1">int64_t _fused_sdp_choice(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_mask, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale, </span><span class="s2">bool </span><span class="s1">enable_gqa); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_sdp_choice(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -&gt; int&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2528"><span class="ln">2528 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _scaled_dot_product_attention_math(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_mask, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale, </span><span class="s2">bool </span><span class="s1">enable_gqa); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_attention_math(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, Tensor? dropout_mask=None, *, float? scale=None, bool enable_gqa=False) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2529"><span class="ln">2529 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _scaled_dot_product_attention_math_for_mps(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_mask, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_attention_math_for_mps(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, Tensor? dropout_mask=None, *, float? scale=None) -&gt; (Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2530"><span class="ln">2530 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt,at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_flash_attention(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">bool </span><span class="s1">return_debug_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_flash_attention(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -&gt; (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor rng_state, Tensor unused, Tensor debug_attn_mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2531"><span class="ln">2531 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _scaled_dot_product_flash_attention_for_cpu(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_flash_attention_for_cpu(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, *, Tensor? attn_mask=None, float? scale=None) -&gt; (Tensor output, Tensor logsumexp)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2532"><span class="ln">2532 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt,at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_fused_attention_overrideable(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_bias, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">bool </span><span class="s1">return_debug_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_fused_attention_overrideable(Tensor query, Tensor key, Tensor value, Tensor? attn_bias=None, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -&gt; (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2533"><span class="ln">2533 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_flash_attention_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; logsumexp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_q, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_seed, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_offset, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -&gt; (Tensor grad_query, Tensor grad_key, Tensor grad_value)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2534"><span class="ln">2534 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_flash_attention_for_cpu_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; logsumexp, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_flash_attention_for_cpu_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, float dropout_p, bool is_causal, *, Tensor? attn_mask=None, float? scale=None) -&gt; (Tensor grad_query, Tensor grad_key, Tensor grad_value)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2535"><span class="ln">2535 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_fused_attention_overrideable_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; attn_bias, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">4</span><span class="s1">&gt; grad_input_mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; logsumexp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_q, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_seed, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_offset, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_fused_attention_overrideable_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor attn_bias, bool[4] grad_input_mask, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -&gt; (Tensor grad_query, Tensor grad_key, Tensor grad_value, Tensor grad_attn_bias)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2536"><span class="ln">2536 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_efficient_attention(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_bias, </span><span class="s2">bool </span><span class="s1">compute_log_sumexp, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -&gt; (Tensor output, Tensor log_sumexp, Tensor philox_seed, Tensor philox_offset)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2537"><span class="ln">2537 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_efficient_attention_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; attn_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; logsumexp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_seed, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_offset, </span><span class="s2">double </span><span class="s1">dropout_p, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">4</span><span class="s1">&gt; grad_input_mask, </span><span class="s2">bool </span><span class="s1">is_causal, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor attn_bias, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, float dropout_p, bool[4] grad_input_mask, bool is_causal=False, *, float? scale=None) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2538"><span class="ln">2538 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt,at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_cudnn_attention(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_bias, </span><span class="s2">bool </span><span class="s1">compute_log_sumexp, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">bool </span><span class="s1">return_debug_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_cudnn_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -&gt; (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2539"><span class="ln">2539 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _scaled_dot_product_cudnn_attention_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; logsumexp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_seed, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_offset, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; attn_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_q, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_scaled_dot_product_cudnn_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, Tensor attn_bias, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, *, float? scale=None) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2540"><span class="ln">2540 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _flash_attention_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cum_seq_q, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">bool </span><span class="s1">return_debug_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale, ::std::optional&lt;c10::SymInt&gt; window_size_left, ::std::optional&lt;c10::SymInt&gt; window_size_right, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; seqused_k, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; alibi_slopes); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_flash_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? cum_seq_q, Tensor? cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, bool return_debug_mask, *, float? scale=None, SymInt? window_size_left=None, SymInt? window_size_right=None, Tensor? seqused_k=None, Tensor? alibi_slopes=None) -&gt; (Tensor output, Tensor softmax_logsumexp, Tensor rng_state, Tensor unused, Tensor debug_attn_mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2541"><span class="ln">2541 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor&gt; _flash_attention_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; logsumexp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_q, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; rng_state, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; unused, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale, ::std::optional&lt;c10::SymInt&gt; window_size_left, ::std::optional&lt;c10::SymInt&gt; window_size_right); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor rng_state, Tensor unused, *, float? scale=None, SymInt? window_size_left=None, SymInt? window_size_right=None) -&gt; (Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2542"><span class="ln">2542 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt&gt; _efficient_attention_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cu_seqlens_q, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cu_seqlens_k, ::std::optional&lt;c10::SymInt&gt; max_seqlen_q, ::std::optional&lt;c10::SymInt&gt; max_seqlen_k, </span><span class="s2">double </span><span class="s1">dropout_p, int64_t custom_mask_type, </span><span class="s2">bool </span><span class="s1">compute_log_sumexp, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; seqlen_k, ::std::optional&lt;int64_t&gt; window_size); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_efficient_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt? max_seqlen_q, SymInt? max_seqlen_k, float dropout_p, int custom_mask_type, bool compute_log_sumexp=False, *, float? scale=None, Tensor? seqlen_k=None, int? window_size=None) -&gt; (Tensor output, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, SymInt max_seqlen_batch_q, SymInt max_seqlen_batch_k)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2543"><span class="ln">2543 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _efficient_attention_backward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cu_seqlens_q, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cu_seqlens_k, c10::SymInt max_seqlen_q, c10::SymInt max_seqlen_k, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; logsumexp, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_seed, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; philox_offset, int64_t custom_mask_type, </span><span class="s2">bool </span><span class="s1">bias_requires_grad, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale, ::std::optional&lt;int64_t&gt; num_splits_key, ::std::optional&lt;int64_t&gt; window_size, </span><span class="s2">bool </span><span class="s1">shared_storage_dqdkdv); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor out, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, Tensor logsumexp, float dropout_p, Tensor philox_seed, Tensor philox_offset, int custom_mask_type, bool bias_requires_grad, *, float? scale=None, int? num_splits_key=None, int? window_size=None, bool shared_storage_dqdkdv=False) -&gt; (Tensor, Tensor, Tensor, Tensor)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2544"><span class="ln">2544 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt,at::Tensor,at::Tensor,at::Tensor&gt; _cudnn_attention_forward(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; attn_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cum_seq_q, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, </span><span class="s2">bool </span><span class="s1">compute_log_sumexp, </span><span class="s2">double </span><span class="s1">dropout_p, </span><span class="s2">bool </span><span class="s1">is_causal, </span><span class="s2">bool </span><span class="s1">return_debug_mask, ::std::optional&lt;</span><span class="s2">double</span><span class="s1">&gt; scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, Tensor? cum_seq_q, Tensor? cum_seq_k, SymInt max_q, SymInt max_k, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -&gt; (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2545"><span class="ln">2545 </span></a><span class="s1">at::Tensor _triton_scaled_dot_attention(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; q, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; k, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; v, </span><span class="s2">double </span><span class="s1">dropout_p); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_triton_scaled_dot_attention(Tensor q, Tensor k, Tensor v, float dropout_p=0.0) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2546"><span class="ln">2546 </span></a><span class="s1">at::Tensor &amp; _fill_mem_eff_dropout_mask_(at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">dropout_p, int64_t seed, int64_t offset); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fill_mem_eff_dropout_mask_(Tensor(a!) self, float dropout_p, int seed, int offset) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2547"><span class="ln">2547 </span></a><span class="s1">at::Tensor _triton_multi_head_attention(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, int64_t embed_dim, int64_t num_head, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; mask); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_triton_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2548"><span class="ln">2548 </span></a><span class="s1">at::Tensor special_airy_ai(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_airy_ai(Tensor x) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2549"><span class="ln">2549 </span></a><span class="s1">at::Tensor &amp; special_airy_ai_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2550"><span class="ln">2550 </span></a><span class="s1">at::Tensor special_bessel_j0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_j0(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2551"><span class="ln">2551 </span></a><span class="s1">at::Tensor &amp; special_bessel_j0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_j0.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2552"><span class="ln">2552 </span></a><span class="s1">at::Tensor special_bessel_j1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_j1(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2553"><span class="ln">2553 </span></a><span class="s1">at::Tensor &amp; special_bessel_j1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2554"><span class="ln">2554 </span></a><span class="s1">at::Tensor special_bessel_y0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_y0(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2555"><span class="ln">2555 </span></a><span class="s1">at::Tensor &amp; special_bessel_y0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_y0.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2556"><span class="ln">2556 </span></a><span class="s1">at::Tensor special_bessel_y1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_y1(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2557"><span class="ln">2557 </span></a><span class="s1">at::Tensor &amp; special_bessel_y1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_bessel_y1.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2558"><span class="ln">2558 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_t(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_t(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2559"><span class="ln">2559 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_t(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2560"><span class="ln">2560 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_t(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2561"><span class="ln">2561 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_t_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2562"><span class="ln">2562 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_t_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2563"><span class="ln">2563 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_t_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2564"><span class="ln">2564 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_u(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_u(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2565"><span class="ln">2565 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_u(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2566"><span class="ln">2566 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_u(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2567"><span class="ln">2567 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_u_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2568"><span class="ln">2568 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_u_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2569"><span class="ln">2569 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_u_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2570"><span class="ln">2570 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_v(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_v(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2571"><span class="ln">2571 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_v(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2572"><span class="ln">2572 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_v(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2573"><span class="ln">2573 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_v_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2574"><span class="ln">2574 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_v_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2575"><span class="ln">2575 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_v_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2576"><span class="ln">2576 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_w(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_w(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2577"><span class="ln">2577 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_w(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2578"><span class="ln">2578 </span></a><span class="s1">at::Tensor special_chebyshev_polynomial_w(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2579"><span class="ln">2579 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_w_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2580"><span class="ln">2580 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_w_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2581"><span class="ln">2581 </span></a><span class="s1">at::Tensor &amp; special_chebyshev_polynomial_w_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2582"><span class="ln">2582 </span></a><span class="s1">at::Tensor special_hermite_polynomial_h(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_h(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2583"><span class="ln">2583 </span></a><span class="s1">at::Tensor special_hermite_polynomial_h(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2584"><span class="ln">2584 </span></a><span class="s1">at::Tensor special_hermite_polynomial_h(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2585"><span class="ln">2585 </span></a><span class="s1">at::Tensor &amp; special_hermite_polynomial_h_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2586"><span class="ln">2586 </span></a><span class="s1">at::Tensor &amp; special_hermite_polynomial_h_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2587"><span class="ln">2587 </span></a><span class="s1">at::Tensor &amp; special_hermite_polynomial_h_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2588"><span class="ln">2588 </span></a><span class="s1">at::Tensor special_hermite_polynomial_he(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_he(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2589"><span class="ln">2589 </span></a><span class="s1">at::Tensor special_hermite_polynomial_he(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_he.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2590"><span class="ln">2590 </span></a><span class="s1">at::Tensor special_hermite_polynomial_he(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_he.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2591"><span class="ln">2591 </span></a><span class="s1">at::Tensor &amp; special_hermite_polynomial_he_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_he.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2592"><span class="ln">2592 </span></a><span class="s1">at::Tensor &amp; special_hermite_polynomial_he_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_he.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2593"><span class="ln">2593 </span></a><span class="s1">at::Tensor &amp; special_hermite_polynomial_he_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_hermite_polynomial_he.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2594"><span class="ln">2594 </span></a><span class="s1">at::Tensor special_laguerre_polynomial_l(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_laguerre_polynomial_l(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2595"><span class="ln">2595 </span></a><span class="s1">at::Tensor special_laguerre_polynomial_l(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_laguerre_polynomial_l.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2596"><span class="ln">2596 </span></a><span class="s1">at::Tensor special_laguerre_polynomial_l(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_laguerre_polynomial_l.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2597"><span class="ln">2597 </span></a><span class="s1">at::Tensor &amp; special_laguerre_polynomial_l_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_laguerre_polynomial_l.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2598"><span class="ln">2598 </span></a><span class="s1">at::Tensor &amp; special_laguerre_polynomial_l_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_laguerre_polynomial_l.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2599"><span class="ln">2599 </span></a><span class="s1">at::Tensor &amp; special_laguerre_polynomial_l_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_laguerre_polynomial_l.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2600"><span class="ln">2600 </span></a><span class="s1">at::Tensor special_legendre_polynomial_p(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_legendre_polynomial_p(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2601"><span class="ln">2601 </span></a><span class="s1">at::Tensor special_legendre_polynomial_p(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_legendre_polynomial_p.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2602"><span class="ln">2602 </span></a><span class="s1">at::Tensor special_legendre_polynomial_p(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_legendre_polynomial_p.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2603"><span class="ln">2603 </span></a><span class="s1">at::Tensor &amp; special_legendre_polynomial_p_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_legendre_polynomial_p.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2604"><span class="ln">2604 </span></a><span class="s1">at::Tensor &amp; special_legendre_polynomial_p_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_legendre_polynomial_p.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2605"><span class="ln">2605 </span></a><span class="s1">at::Tensor &amp; special_legendre_polynomial_p_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_legendre_polynomial_p.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2606"><span class="ln">2606 </span></a><span class="s1">at::Tensor special_modified_bessel_i0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_i0(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2607"><span class="ln">2607 </span></a><span class="s1">at::Tensor &amp; special_modified_bessel_i0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2608"><span class="ln">2608 </span></a><span class="s1">at::Tensor special_modified_bessel_i1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_i1(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2609"><span class="ln">2609 </span></a><span class="s1">at::Tensor &amp; special_modified_bessel_i1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_i1.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2610"><span class="ln">2610 </span></a><span class="s1">at::Tensor special_modified_bessel_k0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_k0(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2611"><span class="ln">2611 </span></a><span class="s1">at::Tensor &amp; special_modified_bessel_k0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2612"><span class="ln">2612 </span></a><span class="s1">at::Tensor special_modified_bessel_k1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_k1(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2613"><span class="ln">2613 </span></a><span class="s1">at::Tensor &amp; special_modified_bessel_k1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2614"><span class="ln">2614 </span></a><span class="s1">at::Tensor special_scaled_modified_bessel_k0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_scaled_modified_bessel_k0(Tensor x) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2615"><span class="ln">2615 </span></a><span class="s1">at::Tensor &amp; special_scaled_modified_bessel_k0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_scaled_modified_bessel_k0.out(Tensor x, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2616"><span class="ln">2616 </span></a><span class="s1">at::Tensor special_scaled_modified_bessel_k1(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_scaled_modified_bessel_k1(Tensor x) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2617"><span class="ln">2617 </span></a><span class="s1">at::Tensor &amp; special_scaled_modified_bessel_k1_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_scaled_modified_bessel_k1.out(Tensor x, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2618"><span class="ln">2618 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_t(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2619"><span class="ln">2619 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_t(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2620"><span class="ln">2620 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_t(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2621"><span class="ln">2621 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_t_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2622"><span class="ln">2622 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_t_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2623"><span class="ln">2623 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_t_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2624"><span class="ln">2624 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_u(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_u(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2625"><span class="ln">2625 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_u(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2626"><span class="ln">2626 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_u(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2627"><span class="ln">2627 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_u_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2628"><span class="ln">2628 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_u_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2629"><span class="ln">2629 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_u_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2630"><span class="ln">2630 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_v(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_v(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2631"><span class="ln">2631 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_v(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2632"><span class="ln">2632 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_v(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2633"><span class="ln">2633 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_v_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2634"><span class="ln">2634 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_v_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2635"><span class="ln">2635 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_v_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2636"><span class="ln">2636 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_w(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_w(Tensor x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2637"><span class="ln">2637 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_w(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2638"><span class="ln">2638 </span></a><span class="s1">at::Tensor special_shifted_chebyshev_polynomial_w(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2639"><span class="ln">2639 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_w_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2640"><span class="ln">2640 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_w_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2641"><span class="ln">2641 </span></a><span class="s1">at::Tensor &amp; special_shifted_chebyshev_polynomial_w_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; n, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_shifted_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2642"><span class="ln">2642 </span></a><span class="s1">at::Tensor special_spherical_bessel_j0(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_spherical_bessel_j0(Tensor x) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2643"><span class="ln">2643 </span></a><span class="s1">at::Tensor &amp; special_spherical_bessel_j0_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::special_spherical_bessel_j0.out(Tensor x, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2644"><span class="ln">2644 </span></a><span class="s1">at::Tensor _foobar(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">arg1, </span><span class="s2">bool </span><span class="s1">arg2, </span><span class="s2">bool </span><span class="s1">arg3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foobar(Tensor self, bool arg1=True, bool arg2=True, *, bool arg3=True) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2645"><span class="ln">2645 </span></a><span class="s2">void </span><span class="s1">_fused_adam_(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adam_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2646"><span class="ln">2646 </span></a><span class="s2">void </span><span class="s1">_fused_adam_(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adam_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2647"><span class="ln">2647 </span></a><span class="s2">void </span><span class="s1">_fused_adamw_(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2648"><span class="ln">2648 </span></a><span class="s2">void </span><span class="s1">_fused_adamw_(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adamw_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2649"><span class="ln">2649 </span></a><span class="s2">void </span><span class="s1">_fused_sgd_(at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">dampening, </span><span class="s2">bool </span><span class="s1">nesterov, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">bool </span><span class="s1">is_first_step, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_sgd_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] momentum_buffer_list, *, float weight_decay, float momentum, float lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2650"><span class="ln">2650 </span></a><span class="s2">void </span><span class="s1">_fused_sgd_(at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">dampening, </span><span class="s2">bool </span><span class="s1">nesterov, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">bool </span><span class="s1">is_first_step, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_sgd_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] momentum_buffer_list, *, float weight_decay, float momentum, Tensor lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2651"><span class="ln">2651 </span></a><span class="s2">void </span><span class="s1">_fused_adagrad_(at::TensorList self, at::TensorList grads, at::TensorList state_sums, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">lr_decay, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adagrad_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] state_sums, Tensor(d!)[] state_steps, *, float lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2652"><span class="ln">2652 </span></a><span class="s2">void </span><span class="s1">_fused_adagrad_(at::TensorList self, at::TensorList grads, at::TensorList state_sums, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">lr_decay, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adagrad_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] state_sums, Tensor[] state_steps, *, Tensor lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;False&quot;}</span>
<a name="l2653"><span class="ln">2653 </span></a><span class="s2">void </span><span class="s1">_propagate_xla_data(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_propagate_xla_data(Tensor input, Tensor output) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;False&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2654"><span class="ln">2654 </span></a><span class="s1">at::Tensor &amp; _new_zeros_with_same_feature_meta_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, int64_t self_num_batch_dims, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_new_zeros_with_same_feature_meta.out(Tensor self, Tensor other, *, int self_num_batch_dims=0, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2655"><span class="ln">2655 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _cudnn_ctc_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">zero_infinity, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_ctc_loss.out(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2656"><span class="ln">2656 </span></a><span class="s1">at::Tensor &amp; _cudnn_rnn_flatten_weight_out(at::TensorList weight_arr, int64_t weight_stride0, c10::SymInt input_size, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">bool </span><span class="s1">bidirectional, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2657"><span class="ln">2657 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _cudnn_rnn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight_buf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, c10::SymIntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3, at::Tensor &amp; out4); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2658"><span class="ln">2658 </span></a><span class="s2">void </span><span class="s1">_cudnn_rnn_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight_buf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, c10::SymIntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reserve, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">4</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::TensorList out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2659"><span class="ln">2659 </span></a><span class="s1">at::Tensor &amp; _cudnn_init_dropout_state_out(</span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, int64_t dropout_seed, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cudnn_init_dropout_state.out(float dropout, bool train, int dropout_seed, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2660"><span class="ln">2660 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _fused_dropout_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_dropout.out(Tensor self, float p, Generator? generator=None, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2661"><span class="ln">2661 </span></a><span class="s1">at::Tensor &amp; _masked_scale_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">double </span><span class="s1">scale, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_masked_scale.out(Tensor self, Tensor mask, float scale, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2662"><span class="ln">2662 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; native_dropout_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; train, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_dropout.out(Tensor input, float p, bool? train, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2663"><span class="ln">2663 </span></a><span class="s1">at::Tensor &amp; native_dropout_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">double </span><span class="s1">scale, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2664"><span class="ln">2664 </span></a><span class="s1">at::Tensor &amp; _conj_physical_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_conj_physical.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2665"><span class="ln">2665 </span></a><span class="s1">at::Tensor &amp; avg_pool1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, </span><span class="s2">bool </span><span class="s1">ceil_mode, </span><span class="s2">bool </span><span class="s1">count_include_pad, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::avg_pool1d.out(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2666"><span class="ln">2666 </span></a><span class="s1">at::Tensor &amp; adaptive_avg_pool1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::adaptive_avg_pool1d.out(Tensor self, int[1] output_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2667"><span class="ln">2667 </span></a><span class="s1">at::Tensor &amp; _add_relu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2668"><span class="ln">2668 </span></a><span class="s1">at::Tensor &amp; add_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2669"><span class="ln">2669 </span></a><span class="s1">at::Tensor &amp; affine_grid_generator_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; theta, c10::SymIntArrayRef size, </span><span class="s2">bool </span><span class="s1">align_corners, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2670"><span class="ln">2670 </span></a><span class="s1">at::Tensor &amp; _test_functorch_fallback_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_functorch_fallback.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2671"><span class="ln">2671 </span></a><span class="s1">at::Tensor &amp; bartlett_window_out(int64_t window_length, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2672"><span class="ln">2672 </span></a><span class="s1">at::Tensor &amp; bartlett_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2673"><span class="ln">2673 </span></a><span class="s1">at::Tensor &amp; quantized_batch_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; var, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">double </span><span class="s1">output_scale, int64_t output_zero_point, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2674"><span class="ln">2674 </span></a><span class="s1">at::Tensor &amp; bernoulli_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; p, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli.Tensor_out(Tensor self, Tensor p, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2675"><span class="ln">2675 </span></a><span class="s1">at::Tensor bernoulli(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; p, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli.Tensor(Tensor self, Tensor p, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2676"><span class="ln">2676 </span></a><span class="s1">at::Tensor &amp; bernoulli_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bernoulli.float_out(Tensor self, float p=0.5, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2677"><span class="ln">2677 </span></a><span class="s1">at::Tensor &amp; binary_cross_entropy_with_logits_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; pos_weight, int64_t reduction, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binary_cross_entropy_with_logits.out(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2678"><span class="ln">2678 </span></a><span class="s1">at::Tensor &amp; bincount_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weights, c10::SymInt minlength, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bincount.out(Tensor self, Tensor? weights=None, SymInt minlength=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2679"><span class="ln">2679 </span></a><span class="s1">at::Tensor &amp; blackman_window_out(int64_t window_length, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::blackman_window.out(int window_length, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2680"><span class="ln">2680 </span></a><span class="s1">at::Tensor &amp; blackman_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::blackman_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2681"><span class="ln">2681 </span></a><span class="s1">at::Tensor &amp; block_diag_out(at::TensorList tensors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::block_diag.out(Tensor[] tensors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2682"><span class="ln">2682 </span></a><span class="s1">at::Tensor &amp; constant_pad_nd_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef pad, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2683"><span class="ln">2683 </span></a><span class="s1">at::Tensor &amp; convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2684"><span class="ln">2684 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; convolution_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, at::OptionalSymIntArrayRef bias_sizes, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2685"><span class="ln">2685 </span></a><span class="s1">at::Tensor &amp; convolution_overrideable_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution_overrideable.out(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2686"><span class="ln">2686 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; convolution_backward_overrideable_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::convolution_backward_overrideable.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2687"><span class="ln">2687 </span></a><span class="s1">at::Tensor &amp; _convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">cudnn_enabled, </span><span class="s2">bool </span><span class="s1">allow_tf32, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2688"><span class="ln">2688 </span></a><span class="s1">at::Tensor &amp; conv_tbc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bias, int64_t pad, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2689"><span class="ln">2689 </span></a><span class="s1">at::Tensor &amp; copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, </span><span class="s2">bool </span><span class="s1">non_blocking, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2690"><span class="ln">2690 </span></a><span class="s1">at::Tensor &amp; _copy_from_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dst, </span><span class="s2">bool </span><span class="s1">non_blocking, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_copy_from.out(Tensor self, Tensor dst, bool non_blocking=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2691"><span class="ln">2691 </span></a><span class="s1">at::Tensor &amp; _copy_from_and_resize_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dst, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_copy_from_and_resize.out(Tensor self, Tensor dst, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2692"><span class="ln">2692 </span></a><span class="s1">at::Tensor &amp; count_nonzero_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::count_nonzero.dim_IntList_out(Tensor self, int[] dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2693"><span class="ln">2693 </span></a><span class="s1">at::Tensor &amp; count_nonzero_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::count_nonzero.out(Tensor self, int? dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2694"><span class="ln">2694 </span></a><span class="s1">at::Tensor &amp; cudnn_affine_grid_generator_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; theta, int64_t N, int64_t C, int64_t H, int64_t W, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_affine_grid_generator.out(Tensor theta, int N, int C, int H, int W, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2695"><span class="ln">2695 </span></a><span class="s1">at::Tensor &amp; cudnn_affine_grid_generator_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, int64_t N, int64_t C, int64_t H, int64_t W, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2696"><span class="ln">2696 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; cudnn_batch_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">exponential_average_factor, </span><span class="s2">double </span><span class="s1">epsilon, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2697"><span class="ln">2697 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; cudnn_batch_norm_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_var, </span><span class="s2">double </span><span class="s1">epsilon, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reserveSpace, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2698"><span class="ln">2698 </span></a><span class="s1">at::Tensor &amp; cudnn_convolution_transpose_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, </span><span class="s2">bool </span><span class="s1">allow_tf32, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution_transpose.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2699"><span class="ln">2699 </span></a><span class="s1">at::Tensor &amp; _mps_convolution_transpose_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mps_convolution_transpose.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2700"><span class="ln">2700 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; mps_convolution_transpose_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mps_convolution_transpose_backward.out(Tensor self, Tensor grad_output, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2701"><span class="ln">2701 </span></a><span class="s1">at::Tensor &amp; cudnn_convolution_relu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2702"><span class="ln">2702 </span></a><span class="s1">at::Tensor &amp; cudnn_convolution_add_relu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; z, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; alpha, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_convolution_add_relu.out(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2703"><span class="ln">2703 </span></a><span class="s1">at::Tensor &amp; cudnn_grid_sampler_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2704"><span class="ln">2704 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; cudnn_grid_sampler_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cudnn_grid_sampler_backward.out(Tensor self, Tensor grid, Tensor grad_output, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2705"><span class="ln">2705 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _ctc_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, </span><span class="s2">bool </span><span class="s1">zero_infinity, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_ctc_loss.out(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2706"><span class="ln">2706 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _ctc_loss_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; target_lengths, int64_t blank, </span><span class="s2">bool </span><span class="s1">zero_infinity, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_ctc_loss.Tensor_out(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2707"><span class="ln">2707 </span></a><span class="s1">at::Tensor &amp; _ctc_loss_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_probs, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; neg_log_likelihood, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; log_alpha, int64_t blank, </span><span class="s2">bool </span><span class="s1">zero_infinity, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_ctc_loss_backward.out(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2708"><span class="ln">2708 </span></a><span class="s1">at::Tensor &amp; diag_embed_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2709"><span class="ln">2709 </span></a><span class="s1">at::Tensor &amp; diagonal_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2710"><span class="ln">2710 </span></a><span class="s1">at::Tensor &amp; div_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2711"><span class="ln">2711 </span></a><span class="s1">at::Tensor &amp; div_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, ::std::optional&lt;c10::string_view&gt; rounding_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2712"><span class="ln">2712 </span></a><span class="s1">at::Tensor &amp; embedding_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymInt padding_idx, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, </span><span class="s2">bool </span><span class="s1">sparse, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2713"><span class="ln">2713 </span></a><span class="s1">at::Tensor &amp; embedding_dense_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, c10::SymInt num_weights, c10::SymInt padding_idx, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2714"><span class="ln">2714 </span></a><span class="s1">at::Tensor &amp; embedding_renorm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">double </span><span class="s1">max_norm, </span><span class="s2">double </span><span class="s1">norm_type, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_renorm.out(Tensor self, Tensor indices, float max_norm, float norm_type, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2715"><span class="ln">2715 </span></a><span class="s1">at::Tensor embedding_renorm(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">double </span><span class="s1">max_norm, </span><span class="s2">double </span><span class="s1">norm_type); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::embedding_renorm(Tensor self, Tensor indices, float max_norm, float norm_type) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2716"><span class="ln">2716 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _embedding_bag_forward_only_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">bool </span><span class="s1">sparse, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, </span><span class="s2">bool </span><span class="s1">include_last_offset, int64_t padding_idx, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_forward_only.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2717"><span class="ln">2717 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _embedding_bag_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">bool </span><span class="s1">sparse, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, </span><span class="s2">bool </span><span class="s1">include_last_offset, int64_t padding_idx, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2718"><span class="ln">2718 </span></a><span class="s1">at::Tensor &amp; _embedding_bag_dense_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offset2bag, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; bag_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; maximum_indices, c10::SymInt num_weights, </span><span class="s2">bool </span><span class="s1">scale_grad_by_freq, int64_t mode, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; per_sample_weights, int64_t padding_idx, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_dense_backward.out(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2719"><span class="ln">2719 </span></a><span class="s1">at::Tensor &amp; _embedding_bag_per_sample_weights_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offset2bag, int64_t mode, int64_t padding_idx, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_embedding_bag_per_sample_weights_backward.out(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2720"><span class="ln">2720 </span></a><span class="s1">at::Tensor &amp; empty_out(at::IntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty.names_out(int[] size, *, Dimname[]? names, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2721"><span class="ln">2721 </span></a><span class="s1">at::Tensor &amp; empty_permuted_out(c10::SymIntArrayRef size, at::IntArrayRef physical_layout, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2722"><span class="ln">2722 </span></a><span class="s1">at::Tensor &amp; new_empty_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2723"><span class="ln">2723 </span></a><span class="s1">at::Tensor &amp; new_empty_strided_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2724"><span class="ln">2724 </span></a><span class="s1">at::Tensor &amp; new_full_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2725"><span class="ln">2725 </span></a><span class="s1">at::Tensor &amp; new_zeros_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2726"><span class="ln">2726 </span></a><span class="s1">at::Tensor &amp; new_ones_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2727"><span class="ln">2727 </span></a><span class="s1">at::Tensor &amp; _empty_affine_quantized_out(c10::SymIntArrayRef size, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_empty_affine_quantized.out(SymInt[] size, *, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2728"><span class="ln">2728 </span></a><span class="s1">at::Tensor &amp; _empty_per_channel_affine_quantized_out(c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_points, int64_t axis, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_empty_per_channel_affine_quantized.out(SymInt[] size, *, Tensor scales, Tensor zero_points, int axis, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2729"><span class="ln">2729 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; resize_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, ::std::optional&lt;at::MemoryFormat&gt; memory_format, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2730"><span class="ln">2730 </span></a><span class="s1">at::Tensor resize(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2731"><span class="ln">2731 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; _resize_output_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Device device, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_resize_output.out(Tensor self, SymInt[] size, Device device, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2732"><span class="ln">2732 </span></a><span class="s1">at::Tensor _resize_output(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Device device); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_resize_output(Tensor self, SymInt[] size, Device device) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2733"><span class="ln">2733 </span></a><span class="s1">at::Tensor &amp; empty_quantized_out(at::IntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qtensor, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_quantized.out(int[] size, Tensor qtensor, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2734"><span class="ln">2734 </span></a><span class="s1">at::Tensor &amp; empty_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2735"><span class="ln">2735 </span></a><span class="s1">at::Tensor &amp; empty_strided_out(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2736"><span class="ln">2736 </span></a><span class="s1">at::Tensor &amp; fill_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fill.Scalar_out(Tensor self, Scalar value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2737"><span class="ln">2737 </span></a><span class="s1">at::Tensor &amp; fill_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fill.Tensor_out(Tensor self, Tensor value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2738"><span class="ln">2738 </span></a><span class="s1">at::Tensor &amp; floor_divide_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::floor_divide.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2739"><span class="ln">2739 </span></a><span class="s1">at::Tensor &amp; full_out(at::IntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, ::std::optional&lt;at::DimnameList&gt; names, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::full.names_out(int[] size, Scalar fill_value, *, Dimname[]? names, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2740"><span class="ln">2740 </span></a><span class="s1">at::Tensor &amp; full_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; fill_value, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::full_like.out(Tensor self, Scalar fill_value, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2741"><span class="ln">2741 </span></a><span class="s1">at::Tensor &amp; from_file_out(c10::string_view filename, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; shared, ::std::optional&lt;int64_t&gt; size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::from_file.out(str filename, bool? shared=None, int? size=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2742"><span class="ln">2742 </span></a><span class="s1">at::Tensor &amp; grid_sampler_2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_2d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2743"><span class="ln">2743 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; grid_sampler_2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_2d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2744"><span class="ln">2744 </span></a><span class="s1">at::Tensor &amp; _grid_sampler_2d_cpu_fallback_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_grid_sampler_2d_cpu_fallback.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2745"><span class="ln">2745 </span></a><span class="s1">at::Tensor &amp; grid_sampler_3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2746"><span class="ln">2746 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; grid_sampler_3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grid, int64_t interpolation_mode, int64_t padding_mode, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2747"><span class="ln">2747 </span></a><span class="s1">at::Tensor &amp; hann_window_out(int64_t window_length, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hann_window.out(int window_length, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2748"><span class="ln">2748 </span></a><span class="s1">at::Tensor &amp; hann_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2749"><span class="ln">2749 </span></a><span class="s1">at::Tensor &amp; hamming_window_out(int64_t window_length, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window.out(int window_length, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2750"><span class="ln">2750 </span></a><span class="s1">at::Tensor &amp; hamming_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2751"><span class="ln">2751 </span></a><span class="s1">at::Tensor &amp; hamming_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, </span><span class="s2">double </span><span class="s1">alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2752"><span class="ln">2752 </span></a><span class="s1">at::Tensor &amp; hamming_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, </span><span class="s2">double </span><span class="s1">alpha, </span><span class="s2">double </span><span class="s1">beta, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2753"><span class="ln">2753 </span></a><span class="s1">at::Tensor &amp; kaiser_window_out(int64_t window_length, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kaiser_window.out(int window_length, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2754"><span class="ln">2754 </span></a><span class="s1">at::Tensor &amp; kaiser_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kaiser_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2755"><span class="ln">2755 </span></a><span class="s1">at::Tensor &amp; kaiser_window_out(int64_t window_length, </span><span class="s2">bool </span><span class="s1">periodic, </span><span class="s2">double </span><span class="s1">beta, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::kaiser_window.beta_out(int window_length, bool periodic, float beta, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2756"><span class="ln">2756 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; native_group_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2757"><span class="ln">2757 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; native_group_norm_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; rstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2758"><span class="ln">2758 </span></a><span class="s1">at::Tensor &amp; index_put_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, </span><span class="s2">bool </span><span class="s1">accumulate, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_put.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2759"><span class="ln">2759 </span></a><span class="s1">at::Tensor &amp; _index_put_impl_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, </span><span class="s2">bool </span><span class="s1">accumulate, </span><span class="s2">bool </span><span class="s1">unsafe, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_index_put_impl.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2760"><span class="ln">2760 </span></a><span class="s1">at::Tensor _index_put_impl(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">c10::List&lt;::std::optional&lt;at::Tensor&gt;&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, </span><span class="s2">bool </span><span class="s1">accumulate, </span><span class="s2">bool </span><span class="s1">unsafe); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_index_put_impl(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2761"><span class="ln">2761 </span></a><span class="s1">at::Tensor &amp; isnan_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isnan.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2762"><span class="ln">2762 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; native_layer_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, c10::SymIntArrayRef normalized_shape, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2763"><span class="ln">2763 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; native_layer_norm_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, c10::SymIntArrayRef normalized_shape, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; rstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2764"><span class="ln">2764 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; linear_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2765"><span class="ln">2765 </span></a><span class="s1">at::Tensor &amp; mkldnn_linear_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear.out(Tensor self, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2766"><span class="ln">2766 </span></a><span class="s1">at::Tensor &amp; mkldnn_linear_backward_input_out(at::IntArrayRef input_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2767"><span class="ln">2767 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; mkldnn_linear_backward_weights_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">bool </span><span class="s1">bias_defined, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear_backward_weights.out(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2768"><span class="ln">2768 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; mkldnn_linear_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2769"><span class="ln">2769 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; matmul_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">2</span><span class="s1">&gt; mask, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::matmul_backward.out(Tensor grad, Tensor self, Tensor other, bool[2] mask, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2770"><span class="ln">2770 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _aminmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_aminmax.out(Tensor self, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2771"><span class="ln">2771 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _aminmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_aminmax.dim_out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2772"><span class="ln">2772 </span></a><span class="s1">at::Tensor &amp; max_pool2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::max_pool2d_backward.out(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2773"><span class="ln">2773 </span></a><span class="s1">at::Tensor &amp; mkldnn_max_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2774"><span class="ln">2774 </span></a><span class="s1">at::Tensor &amp; mkldnn_max_pool2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool2d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2775"><span class="ln">2775 </span></a><span class="s1">at::Tensor &amp; mkldnn_max_pool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2776"><span class="ln">2776 </span></a><span class="s1">at::Tensor &amp; mkldnn_max_pool3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_max_pool3d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2777"><span class="ln">2777 </span></a><span class="s1">at::Tensor &amp; quantized_max_pool1d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_max_pool1d.out(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2778"><span class="ln">2778 </span></a><span class="s1">at::Tensor &amp; quantized_max_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2779"><span class="ln">2779 </span></a><span class="s1">at::Tensor &amp; quantized_max_pool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, </span><span class="s2">bool </span><span class="s1">ceil_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantized_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2780"><span class="ln">2780 </span></a><span class="s1">at::Tensor &amp; median_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::median.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2781"><span class="ln">2781 </span></a><span class="s1">at::Tensor &amp; nanmedian_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::nanmedian.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2782"><span class="ln">2782 </span></a><span class="s1">at::Tensor &amp; _mps_convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mps_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2783"><span class="ln">2783 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; mps_convolution_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mps_convolution_backward.out(Tensor self, Tensor grad_output, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2784"><span class="ln">2784 </span></a><span class="s1">at::Tensor &amp; mkldnn_convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2785"><span class="ln">2785 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; mkldnn_rnn_layer_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight3, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx_, </span><span class="s2">bool </span><span class="s1">reverse, at::IntArrayRef batch_sizes, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">has_biases, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">bool </span><span class="s1">train, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_rnn_layer.out(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2786"><span class="ln">2786 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; mkldnn_rnn_layer_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight3, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight4, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx_tmp, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hy_, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cy_, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">bool </span><span class="s1">reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">has_biases, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, at::IntArrayRef batch_sizes, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; workspace, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3, at::Tensor &amp; out4, at::Tensor &amp; out5, at::Tensor &amp; out6); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2787"><span class="ln">2787 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; miopen_batch_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">exponential_average_factor, </span><span class="s2">double </span><span class="s1">epsilon, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2788"><span class="ln">2788 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; miopen_batch_norm_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_var, </span><span class="s2">double </span><span class="s1">epsilon, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2789"><span class="ln">2789 </span></a><span class="s1">at::Tensor &amp; miopen_convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2790"><span class="ln">2790 </span></a><span class="s1">at::Tensor &amp; miopen_convolution_transpose_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2791"><span class="ln">2791 </span></a><span class="s1">at::Tensor &amp; miopen_depthwise_convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, </span><span class="s2">bool </span><span class="s1">benchmark, </span><span class="s2">bool </span><span class="s1">deterministic, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2792"><span class="ln">2792 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; miopen_rnn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, at::IntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3, at::Tensor &amp; out4); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2793"><span class="ln">2793 </span></a><span class="s2">void </span><span class="s1">miopen_rnn_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList weight, int64_t weight_stride0, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight_buf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_output, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, </span><span class="s2">bool </span><span class="s1">batch_first, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, at::IntArrayRef batch_sizes, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; dropout_state, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; reserve, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">4</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::TensorList out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::miopen_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2794"><span class="ln">2794 </span></a><span class="s1">at::Tensor &amp; _sparse_sparse_matmul_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sparse_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2795"><span class="ln">2795 </span></a><span class="s1">at::Tensor &amp; mul_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2796"><span class="ln">2796 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _native_batch_norm_legit_functional(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_var, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_batch_norm_legit_functional(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor, Tensor running_mean_out, Tensor running_var_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2797"><span class="ln">2797 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _native_batch_norm_legit_no_training_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_batch_norm_legit_no_training.out(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2798"><span class="ln">2798 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; batch_norm_stats_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2799"><span class="ln">2799 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; batch_norm_gather_stats_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, int64_t count, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2800"><span class="ln">2800 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; batch_norm_gather_stats_with_counts_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; counts, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2801"><span class="ln">2801 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; native_batch_norm_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; save_invstd, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">double </span><span class="s1">eps, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2802"><span class="ln">2802 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; batch_norm_backward_reduce_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">input_g, </span><span class="s2">bool </span><span class="s1">weight_g, </span><span class="s2">bool </span><span class="s1">bias_g, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2803"><span class="ln">2803 </span></a><span class="s1">at::Tensor &amp; batch_norm_backward_elemt_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_out, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; invstd, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; sum_dy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; sum_dy_xmu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; count, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_backward_elemt.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2804"><span class="ln">2804 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; batch_norm_update_stats_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::batch_norm_update_stats.out(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2805"><span class="ln">2805 </span></a><span class="s1">at::Tensor &amp; _nnpack_spatial_convolution_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2806"><span class="ln">2806 </span></a><span class="s1">at::Tensor &amp; ones_out(at::IntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2807"><span class="ln">2807 </span></a><span class="s1">at::Tensor &amp; ones_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ones_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2808"><span class="ln">2808 </span></a><span class="s1">at::Tensor &amp; _euclidean_dist_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_euclidean_dist.out(Tensor x1, Tensor x2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2809"><span class="ln">2809 </span></a><span class="s1">at::Tensor &amp; _cdist_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;int64_t&gt; compute_mode, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2810"><span class="ln">2810 </span></a><span class="s1">at::Tensor &amp; _cdist_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x2, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cdist, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cdist_backward.out(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2811"><span class="ln">2811 </span></a><span class="s1">at::Tensor &amp; _pdist_forward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pdist_forward.out(Tensor self, float p=2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2812"><span class="ln">2812 </span></a><span class="s1">at::Tensor &amp; _pdist_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; pdist, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pdist_backward.out(Tensor grad, Tensor self, float p, Tensor pdist, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2813"><span class="ln">2813 </span></a><span class="s1">at::Tensor &amp; pixel_shuffle_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t upscale_factor, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2814"><span class="ln">2814 </span></a><span class="s1">at::Tensor &amp; pixel_unshuffle_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t downscale_factor, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::pixel_unshuffle.out(Tensor self, int downscale_factor, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2815"><span class="ln">2815 </span></a><span class="s1">at::Tensor &amp; channel_shuffle_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt groups, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::channel_shuffle.out(Tensor self, SymInt groups, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2816"><span class="ln">2816 </span></a><span class="s1">at::Tensor &amp; _pin_memory_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Device&gt; device, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pin_memory.out(Tensor self, Device? device=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2817"><span class="ln">2817 </span></a><span class="s1">at::Tensor &amp; scalar_tensor_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; s, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::scalar_tensor.out(Scalar s, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2818"><span class="ln">2818 </span></a><span class="s1">at::Tensor &amp; rand_out(c10::SymIntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2819"><span class="ln">2819 </span></a><span class="s1">at::Tensor &amp; rand_out(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::DimnameList&gt; names, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2820"><span class="ln">2820 </span></a><span class="s1">at::Tensor &amp; rand_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2821"><span class="ln">2821 </span></a><span class="s1">at::Tensor &amp; randint_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt high, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2822"><span class="ln">2822 </span></a><span class="s1">at::Tensor &amp; randint_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; high, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint_like.Tensor_out(Tensor self, Tensor high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2823"><span class="ln">2823 </span></a><span class="s1">at::Tensor &amp; randint_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt low, c10::SymInt high, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2824"><span class="ln">2824 </span></a><span class="s1">at::Tensor &amp; randn_out(c10::SymIntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2825"><span class="ln">2825 </span></a><span class="s1">at::Tensor &amp; randn_out(c10::SymIntArrayRef size, ::std::optional&lt;at::Generator&gt; generator, ::std::optional&lt;at::DimnameList&gt; names, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2826"><span class="ln">2826 </span></a><span class="s1">at::Tensor &amp; randn_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::randn_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2827"><span class="ln">2827 </span></a><span class="s1">at::Tensor &amp; repeat_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef repeats, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2828"><span class="ln">2828 </span></a><span class="s1">at::Tensor &amp; repeat_interleave_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; repeats, ::std::optional&lt;c10::SymInt&gt; output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::repeat_interleave.Tensor_out(Tensor repeats, *, SymInt? output_size=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2829"><span class="ln">2829 </span></a><span class="s1">at::Tensor &amp; _mkldnn_reshape_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef shape, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mkldnn_reshape.out(Tensor self, int[] shape, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2830"><span class="ln">2830 </span></a><span class="s1">at::Tensor &amp; relu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::relu.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2831"><span class="ln">2831 </span></a><span class="s1">at::Tensor &amp; select_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2832"><span class="ln">2832 </span></a><span class="s1">at::Tensor &amp; celu_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2833"><span class="ln">2833 </span></a><span class="s1">at::Tensor &amp; slice_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2834"><span class="ln">2834 </span></a><span class="s1">at::Tensor &amp; slice_scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t dim, ::std::optional&lt;c10::SymInt&gt; start, ::std::optional&lt;c10::SymInt&gt; end, c10::SymInt step, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2835"><span class="ln">2835 </span></a><span class="s1">at::Tensor &amp; select_scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t dim, c10::SymInt index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2836"><span class="ln">2836 </span></a><span class="s1">at::Tensor &amp; diagonal_scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal_scatter.out(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2837"><span class="ln">2837 </span></a><span class="s1">at::Tensor &amp; as_strided_scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;c10::SymInt&gt; storage_offset, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2838"><span class="ln">2838 </span></a><span class="s2">void </span><span class="s1">unsafe_split_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymInt split_size, int64_t dim, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2839"><span class="ln">2839 </span></a><span class="s2">void </span><span class="s1">unsafe_split_with_sizes_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2840"><span class="ln">2840 </span></a><span class="s1">at::Tensor &amp; sum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sum.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2841"><span class="ln">2841 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; std_mean_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2842"><span class="ln">2842 </span></a><span class="s1">at::Tensor &amp; prod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::prod.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2843"><span class="ln">2843 </span></a><span class="s1">at::Tensor &amp; _mkldnn_transpose_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim0, int64_t dim1, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_mkldnn_transpose.out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2844"><span class="ln">2844 </span></a><span class="s1">at::Tensor &amp; flip_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dims, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2845"><span class="ln">2845 </span></a><span class="s1">at::Tensor &amp; roll_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef shifts, at::IntArrayRef dims, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2846"><span class="ln">2846 </span></a><span class="s1">at::Tensor &amp; rot90_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t k, at::IntArrayRef dims, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rot90.out(Tensor self, int k=1, int[] dims=[0,1], *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2847"><span class="ln">2847 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _transform_bias_rescale_qkv_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, int64_t num_heads, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_transform_bias_rescale_qkv.out(Tensor qkv, Tensor qkv_bias, int num_heads, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2848"><span class="ln">2848 </span></a><span class="s1">at::Tensor &amp; _nested_tensor_from_mask_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; t, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">bool </span><span class="s1">mask_check, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_from_mask.out(Tensor t, Tensor mask, bool mask_check=True, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2849"><span class="ln">2849 </span></a><span class="s1">at::Tensor &amp; _nested_from_padded_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; padded, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cpu_nested_shape_example, </span><span class="s2">bool </span><span class="s1">fuse_transform_0213, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_from_padded.out(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2850"><span class="ln">2850 </span></a><span class="s1">at::Tensor &amp; _nested_tensor_size_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_size.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2851"><span class="ln">2851 </span></a><span class="s1">at::Tensor &amp; _nested_tensor_strides_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_strides.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2852"><span class="ln">2852 </span></a><span class="s1">at::Tensor &amp; _nested_tensor_storage_offsets_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_storage_offsets.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2853"><span class="ln">2853 </span></a><span class="s1">at::Tensor &amp; _nested_from_padded_and_nested_example_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; padded, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nt_example, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_from_padded_and_nested_example.out(Tensor padded, Tensor nt_example, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2854"><span class="ln">2854 </span></a><span class="s1">at::Tensor &amp; _nested_view_from_buffer_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nested_size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; nested_strides, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_view_from_buffer_copy.out(Tensor self, Tensor nested_size, Tensor nested_strides, Tensor offsets, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2855"><span class="ln">2855 </span></a><span class="s1">at::Tensor &amp; _nested_view_from_jagged_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dummy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; lengths, int64_t ragged_idx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; min_seqlen, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; max_seqlen, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_view_from_jagged_copy.out(Tensor self, Tensor offsets, Tensor dummy, Tensor? lengths=None, int ragged_idx=1, Tensor? min_seqlen=None, Tensor? max_seqlen=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2856"><span class="ln">2856 </span></a><span class="s1">at::Tensor &amp; _nested_get_values_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_get_values_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2857"><span class="ln">2857 </span></a><span class="s1">at::Tensor &amp; _trilinear_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; i1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; i2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_trilinear.out(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2858"><span class="ln">2858 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _unique_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">sorted, </span><span class="s2">bool </span><span class="s1">return_inverse, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unique.out(Tensor self, bool sorted=True, bool return_inverse=False, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2859"><span class="ln">2859 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; unique_dim_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">sorted, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2860"><span class="ln">2860 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; unique_consecutive_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts, ::std::optional&lt;int64_t&gt; dim, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2861"><span class="ln">2861 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; unique_dim_consecutive_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unique_dim_consecutive.out(Tensor self, int dim, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2862"><span class="ln">2862 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _unique2_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">sorted, </span><span class="s2">bool </span><span class="s1">return_inverse, </span><span class="s2">bool </span><span class="s1">return_counts, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2863"><span class="ln">2863 </span></a><span class="s1">at::Tensor &amp; _unsafe_view_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_unsafe_view.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2864"><span class="ln">2864 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; var_mean_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::OptionalIntArrayRef dim, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; correction, </span><span class="s2">bool </span><span class="s1">keepdim, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::var_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2865"><span class="ln">2865 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _weight_norm_interface_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; v, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; g, int64_t dim, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_norm_interface.out(Tensor v, Tensor g, int dim=0, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2866"><span class="ln">2866 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _weight_norm_interface_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_w, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_v, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_g, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; saved_norms, int64_t dim, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2867"><span class="ln">2867 </span></a><span class="s1">at::Tensor &amp; zeros_out(at::IntArrayRef size, ::std::optional&lt;at::DimnameList&gt; names, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zeros.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2868"><span class="ln">2868 </span></a><span class="s1">at::Tensor &amp; _efficientzerotensor_out(c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_efficientzerotensor.out(SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2869"><span class="ln">2869 </span></a><span class="s1">at::Tensor &amp; zeros_like_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2870"><span class="ln">2870 </span></a><span class="s1">at::Tensor &amp; _standard_gamma_grad_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_standard_gamma_grad.out(Tensor self, Tensor output, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2871"><span class="ln">2871 </span></a><span class="s1">at::Tensor &amp; _standard_gamma_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_standard_gamma.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2872"><span class="ln">2872 </span></a><span class="s1">at::Tensor &amp; _dirichlet_grad_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; alpha, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; total, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_dirichlet_grad.out(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2873"><span class="ln">2873 </span></a><span class="s1">at::Tensor &amp; _sample_dirichlet_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sample_dirichlet.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2874"><span class="ln">2874 </span></a><span class="s1">at::Tensor &amp; poisson_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::poisson.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2875"><span class="ln">2875 </span></a><span class="s1">at::Tensor &amp; binomial_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; count, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; prob, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::binomial.out(Tensor count, Tensor prob, Generator? generator=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2876"><span class="ln">2876 </span></a><span class="s1">at::Tensor &amp; native_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_norm.out(Tensor self, Scalar p=2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2877"><span class="ln">2877 </span></a><span class="s1">at::Tensor &amp; native_norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::native_norm.ScalarOpt_dim_dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2878"><span class="ln">2878 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _batch_norm_with_update_functional(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_mean, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_batch_norm_with_update_functional(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor running_mean_out, Tensor running_var_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2879"><span class="ln">2879 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _batch_norm_no_update_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_mean, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; running_var, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">eps, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_batch_norm_no_update.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2880"><span class="ln">2880 </span></a><span class="s1">at::Tensor &amp; _sparse_sum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sum.dim_out(Tensor self, int[1] dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2881"><span class="ln">2881 </span></a><span class="s1">at::Tensor &amp; _sparse_sum_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_sum_backward.out(Tensor grad, Tensor self, int[] dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2882"><span class="ln">2882 </span></a><span class="s1">at::Tensor &amp; _sparse_csr_sum_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_csr_sum.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2883"><span class="ln">2883 </span></a><span class="s1">at::Tensor &amp; _sparse_csr_prod_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, </span><span class="s2">bool </span><span class="s1">keepdim, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_csr_prod.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2884"><span class="ln">2884 </span></a><span class="s1">at::Tensor &amp; _sparse_softmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2885"><span class="ln">2885 </span></a><span class="s1">at::Tensor &amp; _sparse_softmax_backward_data_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2886"><span class="ln">2886 </span></a><span class="s1">at::Tensor &amp; _sparse_log_softmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">bool </span><span class="s1">half_to_float, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2887"><span class="ln">2887 </span></a><span class="s1">at::Tensor &amp; _sparse_log_softmax_backward_data_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2888"><span class="ln">2888 </span></a><span class="s1">at::Tensor &amp; _spdiags_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; diagonals, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; offsets, at::IntArrayRef shape, ::std::optional&lt;at::Layout&gt; layout, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_spdiags.out(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2889"><span class="ln">2889 </span></a><span class="s1">at::Tensor &amp; norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; p, at::ScalarType dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.ScalarOpt_dtype_out(Tensor self, Scalar? p, *, ScalarType dtype, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2890"><span class="ln">2890 </span></a><span class="s1">at::Tensor &amp; norm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::norm.Scalar_out(Tensor self, Scalar p=2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2891"><span class="ln">2891 </span></a><span class="s1">at::Tensor &amp; clone_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2892"><span class="ln">2892 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; resize_as_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; the_template, ::std::optional&lt;at::MemoryFormat&gt; memory_format, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2893"><span class="ln">2893 </span></a><span class="s1">at::Tensor resize_as(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; the_template, ::std::optional&lt;at::MemoryFormat&gt; memory_format); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize_as(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2894"><span class="ln">2894 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; resize_as_sparse_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; the_template, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2895"><span class="ln">2895 </span></a><span class="s1">at::Tensor resize_as_sparse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; the_template); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::resize_as_sparse(Tensor self, Tensor the_template) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2896"><span class="ln">2896 </span></a><span class="s1">at::Tensor &amp; zero_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zero.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2897"><span class="ln">2897 </span></a><span class="s1">at::Tensor zero(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::zero(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2898"><span class="ln">2898 </span></a><span class="s1">at::Tensor &amp; sub_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2899"><span class="ln">2899 </span></a><span class="s1">at::Tensor &amp; rsub_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2900"><span class="ln">2900 </span></a><span class="s1">at::Tensor &amp; rsub_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2901"><span class="ln">2901 </span></a><span class="s1">at::Tensor &amp; _sparse_addmm_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mat2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; beta, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2902"><span class="ln">2902 </span></a><span class="s1">at::Tensor &amp; sparse_coo_tensor_out(at::IntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_coo_tensor.size_out(int[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2903"><span class="ln">2903 </span></a><span class="s1">at::Tensor &amp; _sparse_coo_tensor_with_dims_out(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_coo_tensor_with_dims.out(int sparse_dim, int dense_dim, int[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2904"><span class="ln">2904 </span></a><span class="s1">at::Tensor &amp; _sparse_coo_tensor_with_dims_and_tensors_out(int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; indices, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; is_coalesced, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_coo_tensor_with_dims_and_tensors.out(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, bool? is_coalesced=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2905"><span class="ln">2905 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; sparse_resize_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_resize.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2906"><span class="ln">2906 </span></a><span class="s1">at::Tensor sparse_resize(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_resize(Tensor self, int[] size, int sparse_dim, int dense_dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2907"><span class="ln">2907 </span></a><span class="s2">const </span><span class="s1">at::Tensor &amp; sparse_resize_and_clear_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_resize_and_clear.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2908"><span class="ln">2908 </span></a><span class="s1">at::Tensor sparse_resize_and_clear(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_resize_and_clear(Tensor self, int[] size, int sparse_dim, int dense_dim) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2909"><span class="ln">2909 </span></a><span class="s1">at::Tensor &amp; sparse_mask_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::sparse_mask.out(Tensor self, Tensor mask, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2910"><span class="ln">2910 </span></a><span class="s1">at::Tensor &amp; _sparse_mask_projection_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">bool </span><span class="s1">accumulate_matches, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_mask_projection.out(Tensor self, Tensor mask, bool accumulate_matches=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2911"><span class="ln">2911 </span></a><span class="s1">at::Tensor &amp; _to_dense_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; masked_grad, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_dense.out(Tensor self, ScalarType? dtype=None, bool? masked_grad=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2912"><span class="ln">2912 </span></a><span class="s1">at::Tensor &amp; _coalesce_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_coalesce.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2913"><span class="ln">2913 </span></a><span class="s1">at::Tensor &amp; _coalesced_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">coalesced, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_coalesced.out(Tensor self, bool coalesced, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2914"><span class="ln">2914 </span></a><span class="s1">at::Tensor _coalesced(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">coalesced); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_coalesced(Tensor self, bool coalesced) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2915"><span class="ln">2915 </span></a><span class="s1">at::Tensor &amp; copy_sparse_to_sparse_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, </span><span class="s2">bool </span><span class="s1">non_blocking, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copy_sparse_to_sparse.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2916"><span class="ln">2916 </span></a><span class="s1">at::Tensor copy_sparse_to_sparse(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, </span><span class="s2">bool </span><span class="s1">non_blocking); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::copy_sparse_to_sparse(Tensor self, Tensor src, bool non_blocking=False) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2917"><span class="ln">2917 </span></a><span class="s1">at::Tensor &amp; _to_sparse_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t sparse_dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse.sparse_dim_out(Tensor self, int sparse_dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2918"><span class="ln">2918 </span></a><span class="s1">at::Tensor &amp; _to_sparse_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Layout&gt; layout, at::OptionalIntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse.out(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2919"><span class="ln">2919 </span></a><span class="s1">at::Tensor &amp; _to_sparse_csr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dense_dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_csr.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2920"><span class="ln">2920 </span></a><span class="s1">at::Tensor &amp; _to_sparse_csc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;int64_t&gt; dense_dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_csc.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2921"><span class="ln">2921 </span></a><span class="s1">at::Tensor &amp; _to_sparse_bsr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_bsr.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2922"><span class="ln">2922 </span></a><span class="s1">at::Tensor &amp; _to_sparse_bsc_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef blocksize, ::std::optional&lt;int64_t&gt; dense_dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_sparse_bsc.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2923"><span class="ln">2923 </span></a><span class="s1">at::Tensor &amp; to_mkldnn_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::ScalarType&gt; dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_mkldnn.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2924"><span class="ln">2924 </span></a><span class="s1">at::Tensor &amp; mkldnn_reorder_conv2d_weight_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_reorder_conv2d_weight.out(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2925"><span class="ln">2925 </span></a><span class="s1">at::Tensor &amp; mkldnn_reorder_conv3d_weight_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_reorder_conv3d_weight.out(Tensor self, SymInt[3] padding=0, SymInt[3] stride=1, SymInt[3] dilation=1, SymInt groups=1, SymInt[]? input_size=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2926"><span class="ln">2926 </span></a><span class="s1">at::Tensor &amp; quantize_per_tensor_dynamic_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ScalarType dtype, </span><span class="s2">bool </span><span class="s1">reduce_range, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor_dynamic.out(Tensor self, ScalarType dtype, bool reduce_range, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2927"><span class="ln">2927 </span></a><span class="s1">at::Tensor &amp; quantize_per_tensor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, at::ScalarType dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor.out(Tensor self, float scale, int zero_point, ScalarType dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2928"><span class="ln">2928 </span></a><span class="s1">at::Tensor &amp; quantize_per_tensor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, at::ScalarType dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor.tensor_qparams_out(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2929"><span class="ln">2929 </span></a><span class="s2">void </span><span class="s1">quantize_per_tensor_out(at::TensorList tensors, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_points, at::ScalarType dtype, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_tensor.tensors_out(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2930"><span class="ln">2930 </span></a><span class="s1">at::Tensor &amp; quantize_per_channel_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scales, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_points, int64_t axis, at::ScalarType dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2931"><span class="ln">2931 </span></a><span class="s1">at::Tensor &amp; dequantize_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2932"><span class="ln">2932 </span></a><span class="s2">void </span><span class="s1">dequantize_out(at::TensorList tensors, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2933"><span class="ln">2933 </span></a><span class="s1">at::Tensor &amp; q_per_channel_scales_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2934"><span class="ln">2934 </span></a><span class="s1">at::Tensor &amp; q_per_channel_zero_points_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2935"><span class="ln">2935 </span></a><span class="s1">at::Tensor &amp; int_repr_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::int_repr.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2936"><span class="ln">2936 </span></a><span class="s1">at::Tensor &amp; _make_per_tensor_quantized_tensor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_per_tensor_quantized_tensor.out(Tensor self, float scale, int zero_point, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2937"><span class="ln">2937 </span></a><span class="s1">at::Tensor &amp; _make_per_channel_quantized_tensor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_per_channel_quantized_tensor.out(Tensor self, Tensor scale, Tensor zero_point, int axis, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2938"><span class="ln">2938 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; fake_quantize_per_tensor_affine_cachemask_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_tensor_affine_cachemask.out(Tensor self, float scale, int zero_point, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2939"><span class="ln">2939 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; fake_quant_enabled, int64_t quant_min, int64_t quant_max, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams.out(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2940"><span class="ln">2940 </span></a><span class="s1">at::Tensor &amp; _fake_quantize_learnable_per_tensor_affine_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t quant_min, int64_t quant_max, </span><span class="s2">double </span><span class="s1">grad_factor, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_learnable_per_tensor_affine.out(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2941"><span class="ln">2941 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; fake_quantize_per_channel_affine_cachemask_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::fake_quantize_per_channel_affine_cachemask.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2942"><span class="ln">2942 </span></a><span class="s1">at::Tensor &amp; _fake_quantize_learnable_per_channel_affine_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, </span><span class="s2">double </span><span class="s1">grad_factor, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fake_quantize_learnable_per_channel_affine.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2943"><span class="ln">2943 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _fused_moving_avg_obs_fq_helper_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; observer_on, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; fake_quant_on, at::Tensor &amp; running_min, at::Tensor &amp; running_max, at::Tensor &amp; scale, at::Tensor &amp; zero_point, </span><span class="s2">double </span><span class="s1">averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, </span><span class="s2">bool </span><span class="s1">per_row_fake_quant, </span><span class="s2">bool </span><span class="s1">symmetric_quant, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_moving_avg_obs_fq_helper.out(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False, *, Tensor(e!) out0, Tensor(f!) out1) -&gt; (Tensor(e!), Tensor(f!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2944"><span class="ln">2944 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor&gt; _fused_moving_avg_obs_fq_helper_functional(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; observer_on, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; fake_quant_on, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_min, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; running_max, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scale, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; zero_point, </span><span class="s2">double </span><span class="s1">averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, </span><span class="s2">bool </span><span class="s1">per_row_fake_quant, </span><span class="s2">bool </span><span class="s1">symmetric_quant); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_moving_avg_obs_fq_helper_functional(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor running_min, Tensor running_max, Tensor scale, Tensor zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -&gt; (Tensor output, Tensor mask, Tensor running_min_out, Tensor running_max_out, Tensor scale_out, Tensor zero_point_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2945"><span class="ln">2945 </span></a><span class="s1">at::Tensor &amp; _to_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">non_blocking, ::std::optional&lt;at::MemoryFormat&gt; memory_format, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_to_copy.out(Tensor self, *, bool non_blocking=False, MemoryFormat? memory_format=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2946"><span class="ln">2946 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _lstm_mps_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::TensorList hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3, at::Tensor &amp; out4, at::Tensor &amp; out5); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_lstm_mps.out(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2947"><span class="ln">2947 </span></a><span class="s2">void </span><span class="s1">lstm_mps_backward_out(</span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_y, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; z_state, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cell_state_fwd, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; layersOutputs, at::TensorList hx, at::TensorList params, </span><span class="s2">bool </span><span class="s1">has_biases, int64_t num_layers, </span><span class="s2">double </span><span class="s1">dropout, </span><span class="s2">bool </span><span class="s1">train, </span><span class="s2">bool </span><span class="s1">bidirectional, </span><span class="s2">bool </span><span class="s1">batch_first, at::Tensor &amp; out0, at::TensorList out1, at::TensorList out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lstm_mps_backward.out(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2948"><span class="ln">2948 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _thnn_fused_lstm_cell_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hidden_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; input_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; hidden_bias, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2949"><span class="ln">2949 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _thnn_fused_lstm_cell_backward_impl_out(</span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_hy, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cx, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; cy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; workspace, </span><span class="s2">bool </span><span class="s1">has_bias, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_lstm_cell_backward_impl.out(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2950"><span class="ln">2950 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _thnn_fused_gru_cell_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hidden_gates, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; hx, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; input_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; hidden_bias, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2951"><span class="ln">2951 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _thnn_fused_gru_cell_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_hy, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; workspace, </span><span class="s2">bool </span><span class="s1">has_bias, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2, at::Tensor &amp; out3, at::Tensor &amp; out4); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_thnn_fused_gru_cell_backward.out(Tensor grad_hy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2952"><span class="ln">2952 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _pack_padded_sequence_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lengths, </span><span class="s2">bool </span><span class="s1">batch_first, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2953"><span class="ln">2953 </span></a><span class="s1">at::Tensor &amp; set_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Storage source, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set.source_Storage_out(Tensor self, Storage source, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2954"><span class="ln">2954 </span></a><span class="s1">at::Tensor set(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Storage source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set.source_Storage(Tensor self, Storage source) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2955"><span class="ln">2955 </span></a><span class="s1">at::Tensor &amp; set_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2956"><span class="ln">2956 </span></a><span class="s1">at::Tensor set(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set.source_Storage_storage_offset(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2957"><span class="ln">2957 </span></a><span class="s1">at::Tensor &amp; set_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set.source_Tensor_out(Tensor self, Tensor source, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2958"><span class="ln">2958 </span></a><span class="s1">at::Tensor set(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set.source_Tensor(Tensor self, Tensor source) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2959"><span class="ln">2959 </span></a><span class="s1">at::Tensor &amp; set_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2960"><span class="ln">2960 </span></a><span class="s1">at::Tensor set(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::set(Tensor self) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2961"><span class="ln">2961 </span></a><span class="s1">at::Tensor &amp; lift_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lift.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2962"><span class="ln">2962 </span></a><span class="s1">at::Tensor &amp; lift_fresh_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2963"><span class="ln">2963 </span></a><span class="s1">at::Tensor &amp; masked_fill_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_fill.Scalar_out(Tensor self, Tensor mask, Scalar value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2964"><span class="ln">2964 </span></a><span class="s1">at::Tensor &amp; masked_fill_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_fill.Tensor_out(Tensor self, Tensor mask, Tensor value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2965"><span class="ln">2965 </span></a><span class="s1">at::Tensor &amp; masked_scatter_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::masked_scatter.out(Tensor self, Tensor mask, Tensor source, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2966"><span class="ln">2966 </span></a><span class="s1">at::Tensor &amp; _masked_softmax_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, ::std::optional&lt;int64_t&gt; dim, ::std::optional&lt;int64_t&gt; mask_type, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_masked_softmax.out(Tensor self, Tensor mask, int? dim=None, int? mask_type=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2967"><span class="ln">2967 </span></a><span class="s1">at::Tensor &amp; _masked_softmax_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; mask, ::std::optional&lt;int64_t&gt; dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_masked_softmax_backward.out(Tensor grad_output, Tensor output, Tensor mask, int? dim=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2968"><span class="ln">2968 </span></a><span class="s1">at::Tensor &amp; put_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; source, </span><span class="s2">bool </span><span class="s1">accumulate, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::put.out(Tensor self, Tensor index, Tensor source, bool accumulate=False, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2969"><span class="ln">2969 </span></a><span class="s1">at::Tensor &amp; index_fill_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2970"><span class="ln">2970 </span></a><span class="s1">at::Tensor &amp; index_fill_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; index, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2971"><span class="ln">2971 </span></a><span class="s1">at::Tensor &amp; bitwise_and_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_and.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2972"><span class="ln">2972 </span></a><span class="s1">at::Tensor &amp; bitwise_or_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_or.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2973"><span class="ln">2973 </span></a><span class="s1">at::Tensor &amp; bitwise_xor_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_xor.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2974"><span class="ln">2974 </span></a><span class="s1">at::Tensor &amp; __lshift___out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__lshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2975"><span class="ln">2975 </span></a><span class="s1">at::Tensor &amp; __lshift___out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__lshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2976"><span class="ln">2976 </span></a><span class="s1">at::Tensor &amp; bitwise_left_shift_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_left_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2977"><span class="ln">2977 </span></a><span class="s1">at::Tensor &amp; __rshift___out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__rshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2978"><span class="ln">2978 </span></a><span class="s1">at::Tensor &amp; __rshift___out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::__rshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2979"><span class="ln">2979 </span></a><span class="s1">at::Tensor &amp; bitwise_right_shift_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bitwise_right_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2980"><span class="ln">2980 </span></a><span class="s1">at::Tensor &amp; random_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t from, ::std::optional&lt;int64_t&gt; to, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2981"><span class="ln">2981 </span></a><span class="s1">at::Tensor random(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t from, ::std::optional&lt;int64_t&gt; to, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random.from(Tensor self, int from, int? to, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2982"><span class="ln">2982 </span></a><span class="s1">at::Tensor &amp; random_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t to, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2983"><span class="ln">2983 </span></a><span class="s1">at::Tensor random(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t to, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random.to(Tensor self, int to, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2984"><span class="ln">2984 </span></a><span class="s1">at::Tensor &amp; random_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2985"><span class="ln">2985 </span></a><span class="s1">at::Tensor random(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::random(Tensor self, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2986"><span class="ln">2986 </span></a><span class="s1">at::Tensor &amp; uniform_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">from, </span><span class="s2">double </span><span class="s1">to, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::uniform.out(Tensor self, float from=0, float to=1, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2987"><span class="ln">2987 </span></a><span class="s1">at::Tensor uniform(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">from, </span><span class="s2">double </span><span class="s1">to, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::uniform(Tensor self, float from=0, float to=1, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2988"><span class="ln">2988 </span></a><span class="s1">at::Tensor &amp; cauchy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">median, </span><span class="s2">double </span><span class="s1">sigma, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2989"><span class="ln">2989 </span></a><span class="s1">at::Tensor cauchy(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">median, </span><span class="s2">double </span><span class="s1">sigma, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::cauchy(Tensor self, float median=0, float sigma=1, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2990"><span class="ln">2990 </span></a><span class="s1">at::Tensor &amp; log_normal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2991"><span class="ln">2991 </span></a><span class="s1">at::Tensor log_normal(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::log_normal(Tensor self, float mean=1, float std=2, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2992"><span class="ln">2992 </span></a><span class="s1">at::Tensor &amp; exponential_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">lambd, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exponential.out(Tensor self, float lambd=1, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2993"><span class="ln">2993 </span></a><span class="s1">at::Tensor exponential(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">lambd, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::exponential(Tensor self, float lambd=1, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2994"><span class="ln">2994 </span></a><span class="s1">at::Tensor &amp; geometric_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::geometric.out(Tensor self, float p, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2995"><span class="ln">2995 </span></a><span class="s1">at::Tensor geometric(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">p, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::geometric(Tensor self, float p, *, Generator? generator=None) -&gt; Tensor&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2996"><span class="ln">2996 </span></a><span class="s1">at::Tensor &amp; tril_indices_out(int64_t row, int64_t col, int64_t offset, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::tril_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2997"><span class="ln">2997 </span></a><span class="s1">at::Tensor &amp; triu_indices_out(int64_t row, int64_t col, int64_t offset, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::triu_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2998"><span class="ln">2998 </span></a><span class="s1">at::Tensor &amp; trace_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::trace.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l2999"><span class="ln">2999 </span></a><span class="s1">at::Tensor &amp; _cholesky_solve_helper_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; A, </span><span class="s2">bool </span><span class="s1">upper, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_cholesky_solve_helper.out(Tensor self, Tensor A, bool upper, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3000"><span class="ln">3000 </span></a><span class="s1">at::Tensor &amp; dist_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::dist.out(Tensor self, Tensor other, Scalar p=2, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3001"><span class="ln">3001 </span></a><span class="s2">void </span><span class="s1">_histogramdd_bin_edges_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_histogramdd_bin_edges.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3002"><span class="ln">3002 </span></a><span class="s1">at::Tensor &amp; _histogramdd_from_bin_cts_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef bins, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; range, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_histogramdd_from_bin_cts.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3003"><span class="ln">3003 </span></a><span class="s1">at::Tensor &amp; _histogramdd_from_bin_tensors_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::TensorList bins, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; weight, </span><span class="s2">bool </span><span class="s1">density, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_histogramdd_from_bin_tensors.out(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3004"><span class="ln">3004 </span></a><span class="s1">at::Tensor &amp; remainder_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::remainder.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3005"><span class="ln">3005 </span></a><span class="s1">at::Tensor &amp; unfold_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_in, c10::SymIntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3006"><span class="ln">3006 </span></a><span class="s1">at::Tensor &amp; normal_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">mean, </span><span class="s2">double </span><span class="s1">std, ::std::optional&lt;at::Generator&gt; generator, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3007"><span class="ln">3007 </span></a><span class="s2">void </span><span class="s1">_amp_foreach_non_finite_check_and_unscale_out(at::TensorList self, at::Tensor &amp; found_inf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; inv_scale, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_amp_foreach_non_finite_check_and_unscale.out(Tensor[] self, Tensor(b!) found_inf, Tensor inv_scale, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3008"><span class="ln">3008 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,at::Tensor&gt; _amp_foreach_non_finite_check_and_unscale(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; found_inf, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; inv_scale); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_amp_foreach_non_finite_check_and_unscale(Tensor[] self, Tensor found_inf, Tensor inv_scale) -&gt; (Tensor[] self_out, Tensor found_inf_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3009"><span class="ln">3009 </span></a><span class="s1">at::Tensor &amp; _amp_update_scale_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; growth_tracker, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; found_inf, </span><span class="s2">double </span><span class="s1">scale_growth_factor, </span><span class="s2">double </span><span class="s1">scale_backoff_factor, int64_t growth_interval, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_amp_update_scale.out(Tensor self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3010"><span class="ln">3010 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; _amp_update_scale(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; growth_tracker, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; found_inf, </span><span class="s2">double </span><span class="s1">scale_growth_factor, </span><span class="s2">double </span><span class="s1">scale_backoff_factor, int64_t growth_interval); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_amp_update_scale(Tensor self, Tensor growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -&gt; (Tensor, Tensor growth_tracker_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3011"><span class="ln">3011 </span></a><span class="s2">void </span><span class="s1">_foreach_add_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3012"><span class="ln">3012 </span></a><span class="s2">void </span><span class="s1">_foreach_add_out(at::TensorList self, at::TensorList other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3013"><span class="ln">3013 </span></a><span class="s2">void </span><span class="s1">_foreach_add_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3014"><span class="ln">3014 </span></a><span class="s2">void </span><span class="s1">_foreach_add_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_add.Tensor_out(Tensor[] self, Tensor other, *, Scalar alpha=1, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3015"><span class="ln">3015 </span></a><span class="s2">void </span><span class="s1">_foreach_sub_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3016"><span class="ln">3016 </span></a><span class="s2">void </span><span class="s1">_foreach_sub_out(at::TensorList self, at::TensorList other, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; alpha, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3017"><span class="ln">3017 </span></a><span class="s2">void </span><span class="s1">_foreach_sub_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sub.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3018"><span class="ln">3018 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3019"><span class="ln">3019 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_out(at::TensorList self, at::TensorList other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3020"><span class="ln">3020 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3021"><span class="ln">3021 </span></a><span class="s2">void </span><span class="s1">_foreach_mul_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_mul.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3022"><span class="ln">3022 </span></a><span class="s2">void </span><span class="s1">_foreach_div_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3023"><span class="ln">3023 </span></a><span class="s2">void </span><span class="s1">_foreach_div_out(at::TensorList self, at::TensorList other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3024"><span class="ln">3024 </span></a><span class="s2">void </span><span class="s1">_foreach_div_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3025"><span class="ln">3025 </span></a><span class="s2">void </span><span class="s1">_foreach_div_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_div.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3026"><span class="ln">3026 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_max_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3027"><span class="ln">3027 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_max_out(at::TensorList self, at::TensorList other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3028"><span class="ln">3028 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_max_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_max.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3029"><span class="ln">3029 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_min_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3030"><span class="ln">3030 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_min_out(at::TensorList self, at::TensorList other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3031"><span class="ln">3031 </span></a><span class="s2">void </span><span class="s1">_foreach_clamp_min_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_clamp_min.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3032"><span class="ln">3032 </span></a><span class="s2">void </span><span class="s1">_foreach_maximum_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3033"><span class="ln">3033 </span></a><span class="s2">void </span><span class="s1">_foreach_maximum_out(at::TensorList self, at::TensorList other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3034"><span class="ln">3034 </span></a><span class="s2">void </span><span class="s1">_foreach_maximum_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_maximum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3035"><span class="ln">3035 </span></a><span class="s2">void </span><span class="s1">_foreach_minimum_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; scalar, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3036"><span class="ln">3036 </span></a><span class="s2">void </span><span class="s1">_foreach_minimum_out(at::TensorList self, at::TensorList other, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3037"><span class="ln">3037 </span></a><span class="s2">void </span><span class="s1">_foreach_minimum_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_minimum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3038"><span class="ln">3038 </span></a><span class="s2">void </span><span class="s1">_foreach_addcdiv_out(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3039"><span class="ln">3039 </span></a><span class="s2">void </span><span class="s1">_foreach_addcdiv_out(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3040"><span class="ln">3040 </span></a><span class="s2">void </span><span class="s1">_foreach_addcdiv_out(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcdiv.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3041"><span class="ln">3041 </span></a><span class="s2">void </span><span class="s1">_foreach_addcmul_out(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; value, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3042"><span class="ln">3042 </span></a><span class="s2">void </span><span class="s1">_foreach_addcmul_out(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef&lt;at::Scalar&gt; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3043"><span class="ln">3043 </span></a><span class="s2">void </span><span class="s1">_foreach_addcmul_out(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; scalars, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_addcmul.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3044"><span class="ln">3044 </span></a><span class="s2">void </span><span class="s1">_foreach_abs_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_abs.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3045"><span class="ln">3045 </span></a><span class="s2">void </span><span class="s1">_foreach_acos_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_acos.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3046"><span class="ln">3046 </span></a><span class="s2">void </span><span class="s1">_foreach_asin_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_asin.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3047"><span class="ln">3047 </span></a><span class="s2">void </span><span class="s1">_foreach_atan_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_atan.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3048"><span class="ln">3048 </span></a><span class="s2">void </span><span class="s1">_foreach_ceil_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_ceil.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3049"><span class="ln">3049 </span></a><span class="s2">void </span><span class="s1">_foreach_cos_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_cos.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3050"><span class="ln">3050 </span></a><span class="s2">void </span><span class="s1">_foreach_cosh_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_cosh.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3051"><span class="ln">3051 </span></a><span class="s2">void </span><span class="s1">_foreach_erf_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_erf.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3052"><span class="ln">3052 </span></a><span class="s2">void </span><span class="s1">_foreach_erfc_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_erfc.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3053"><span class="ln">3053 </span></a><span class="s2">void </span><span class="s1">_foreach_exp_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_exp.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3054"><span class="ln">3054 </span></a><span class="s2">void </span><span class="s1">_foreach_expm1_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_expm1.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3055"><span class="ln">3055 </span></a><span class="s2">void </span><span class="s1">_foreach_floor_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_floor.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3056"><span class="ln">3056 </span></a><span class="s2">void </span><span class="s1">_foreach_frac_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_frac.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3057"><span class="ln">3057 </span></a><span class="s2">void </span><span class="s1">_foreach_lerp_out(at::TensorList self, at::TensorList tensors1, at::TensorList weights, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp.List_out(Tensor[] self, Tensor[] tensors1, Tensor[] weights, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3058"><span class="ln">3058 </span></a><span class="s2">void </span><span class="s1">_foreach_lerp_out(at::TensorList self, at::TensorList tensors1, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; weight, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp.Scalar_out(Tensor[] self, Tensor[] tensors1, Scalar weight, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3059"><span class="ln">3059 </span></a><span class="s2">void </span><span class="s1">_foreach_lerp_out(at::TensorList self, at::TensorList tensors1, at::ArrayRef&lt;at::Scalar&gt; weight, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lerp.ScalarList_out(Tensor[] self, Tensor[] tensors1, Scalar[] weight, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3060"><span class="ln">3060 </span></a><span class="s2">void </span><span class="s1">_foreach_lgamma_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_lgamma.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3061"><span class="ln">3061 </span></a><span class="s2">void </span><span class="s1">_foreach_log_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3062"><span class="ln">3062 </span></a><span class="s2">void </span><span class="s1">_foreach_log10_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log10.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3063"><span class="ln">3063 </span></a><span class="s2">void </span><span class="s1">_foreach_log1p_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log1p.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3064"><span class="ln">3064 </span></a><span class="s2">void </span><span class="s1">_foreach_log2_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_log2.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3065"><span class="ln">3065 </span></a><span class="s2">void </span><span class="s1">_foreach_max_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_max.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3066"><span class="ln">3066 </span></a><span class="s2">void </span><span class="s1">_foreach_neg_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_neg.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3067"><span class="ln">3067 </span></a><span class="s2">void </span><span class="s1">_foreach_norm_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; ord, ::std::optional&lt;at::ScalarType&gt; dtype, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_norm.Scalar_out(Tensor[] self, Scalar ord=2, ScalarType? dtype=None, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3068"><span class="ln">3068 </span></a><span class="s2">void </span><span class="s1">_foreach_pow_out(at::TensorList self, at::TensorList exponent, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow.List_out(Tensor[] self, Tensor[] exponent, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3069"><span class="ln">3069 </span></a><span class="s2">void </span><span class="s1">_foreach_pow_out(at::TensorList self, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; exponent, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow.Scalar_out(Tensor[] self, Scalar exponent, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3070"><span class="ln">3070 </span></a><span class="s2">void </span><span class="s1">_foreach_pow_out(at::TensorList self, at::ArrayRef&lt;at::Scalar&gt; exponent, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_pow.ScalarList_out(Tensor[] self, Scalar[] exponent, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3071"><span class="ln">3071 </span></a><span class="s2">void </span><span class="s1">_foreach_reciprocal_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_reciprocal.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3072"><span class="ln">3072 </span></a><span class="s2">void </span><span class="s1">_foreach_round_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_round.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3073"><span class="ln">3073 </span></a><span class="s2">void </span><span class="s1">_foreach_rsqrt_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_rsqrt.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3074"><span class="ln">3074 </span></a><span class="s2">void </span><span class="s1">_foreach_sigmoid_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sigmoid.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3075"><span class="ln">3075 </span></a><span class="s2">void </span><span class="s1">_foreach_sign_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sign.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3076"><span class="ln">3076 </span></a><span class="s2">void </span><span class="s1">_foreach_sin_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sin.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3077"><span class="ln">3077 </span></a><span class="s2">void </span><span class="s1">_foreach_sinh_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sinh.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3078"><span class="ln">3078 </span></a><span class="s2">void </span><span class="s1">_foreach_sqrt_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_sqrt.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3079"><span class="ln">3079 </span></a><span class="s2">void </span><span class="s1">_foreach_tan_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_tan.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3080"><span class="ln">3080 </span></a><span class="s2">void </span><span class="s1">_foreach_tanh_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_tanh.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3081"><span class="ln">3081 </span></a><span class="s2">void </span><span class="s1">_foreach_trunc_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_trunc.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3082"><span class="ln">3082 </span></a><span class="s2">void </span><span class="s1">_foreach_zero_out(at::TensorList self, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_zero.out(Tensor[] self, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3083"><span class="ln">3083 </span></a><span class="s1">::std::vector&lt;at::Tensor&gt; _foreach_zero(at::TensorList self); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_zero(Tensor[] self) -&gt; Tensor[] self_out&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3084"><span class="ln">3084 </span></a><span class="s2">void </span><span class="s1">_foreach_copy_out(at::TensorList self, at::TensorList src, </span><span class="s2">bool </span><span class="s1">non_blocking, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foreach_copy.out(Tensor[] self, Tensor[] src, bool non_blocking=False, *, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3085"><span class="ln">3085 </span></a><span class="s1">at::Tensor &amp; bucketize_out(</span><span class="s2">const </span><span class="s1">at::Scalar &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; boundaries, </span><span class="s2">bool </span><span class="s1">out_int32, </span><span class="s2">bool </span><span class="s1">right, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3086"><span class="ln">3086 </span></a><span class="s1">at::Tensor &amp; glu_jvp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; glu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dx, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu_jvp.out(Tensor glu, Tensor x, Tensor dx, int dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3087"><span class="ln">3087 </span></a><span class="s1">at::Tensor &amp; glu_backward_jvp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_glu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; x, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dgrad_glu, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; dx, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3088"><span class="ln">3088 </span></a><span class="s1">at::Tensor &amp; hardswish_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3089"><span class="ln">3089 </span></a><span class="s1">::std::tuple&lt;at::Tensor,at::Tensor&gt; rrelu_with_noise_functional(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; noise, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, ::std::optional&lt;at::Generator&gt; generator); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu_with_noise_functional(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -&gt; (Tensor, Tensor noise_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3090"><span class="ln">3090 </span></a><span class="s1">at::Tensor &amp; rrelu_with_noise_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; noise, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; lower, </span><span class="s2">const </span><span class="s1">at::Scalar &amp; upper, </span><span class="s2">bool </span><span class="s1">training, </span><span class="s2">bool </span><span class="s1">self_is_result, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::rrelu_with_noise_backward.out(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3091"><span class="ln">3091 </span></a><span class="s1">at::Tensor &amp; mkldnn_adaptive_avg_pool2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::mkldnn_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3092"><span class="ln">3092 </span></a><span class="s1">at::Tensor &amp; _adaptive_avg_pool2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3093"><span class="ln">3093 </span></a><span class="s1">at::Tensor &amp; _adaptive_avg_pool2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3094"><span class="ln">3094 </span></a><span class="s1">at::Tensor &amp; _adaptive_avg_pool3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3095"><span class="ln">3095 </span></a><span class="s1">at::Tensor &amp; _adaptive_avg_pool3d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_adaptive_avg_pool3d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3096"><span class="ln">3096 </span></a><span class="s1">at::Tensor &amp; upsample_bilinear2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, </span><span class="s2">bool </span><span class="s1">align_corners, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_bilinear2d.vec_out(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3097"><span class="ln">3097 </span></a><span class="s1">at::Tensor &amp; upsample_nearest2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; input, at::OptionalSymIntArrayRef output_size, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; scale_factors, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::upsample_nearest2d.vec_out(Tensor input, SymInt[]? output_size, float[]? scale_factors, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3098"><span class="ln">3098 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;,at::Tensor &amp;&gt; _slow_conv2d_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad_output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array&lt;</span><span class="s2">bool</span><span class="s1">,</span><span class="s3">3</span><span class="s1">&gt; output_mask, at::Tensor &amp; out0, at::Tensor &amp; out1, at::Tensor &amp; out2); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_slow_conv2d_backward.output_mask_out(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -&gt; (Tensor(a!), Tensor(b!), Tensor(c!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3099"><span class="ln">3099 </span></a><span class="s1">at::Tensor &amp; conv_depthwise3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::conv_depthwise3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3100"><span class="ln">3100 </span></a><span class="s1">at::Tensor &amp; slow_conv_dilated2d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3101"><span class="ln">3101 </span></a><span class="s1">at::Tensor &amp; slow_conv_dilated3d_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; weight, c10::SymIntArrayRef kernel_size, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3102"><span class="ln">3102 </span></a><span class="s1">at::Tensor &amp; isinf_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::isinf.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3103"><span class="ln">3103 </span></a><span class="s1">at::Tensor &amp; linalg_matrix_exp_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::linalg_matrix_exp.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3104"><span class="ln">3104 </span></a><span class="s1">at::Tensor &amp; _test_optional_intlist_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::OptionalIntArrayRef addends, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_optional_intlist.out(Tensor values, int[]? addends, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3105"><span class="ln">3105 </span></a><span class="s1">at::Tensor &amp; _test_optional_filled_intlist_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, at::OptionalIntArrayRef addends, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_optional_filled_intlist.out(Tensor values, int[2]? addends, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3106"><span class="ln">3106 </span></a><span class="s1">at::Tensor &amp; _test_optional_floatlist_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; values, ::std::optional&lt;at::ArrayRef&lt;</span><span class="s2">double</span><span class="s1">&gt;&gt; addends, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_optional_floatlist.out(Tensor values, float[]? addends, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3107"><span class="ln">3107 </span></a><span class="s1">at::Tensor &amp; _test_warn_in_autograd_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_warn_in_autograd.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3108"><span class="ln">3108 </span></a><span class="s1">at::Tensor &amp; _test_autograd_multiple_dispatch_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_autograd_multiple_dispatch.fullcoverage_out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3109"><span class="ln">3109 </span></a><span class="s1">at::Tensor &amp; _test_autograd_multiple_dispatch_view_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_test_autograd_multiple_dispatch_view_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3110"><span class="ln">3110 </span></a><span class="s1">at::Tensor &amp; segment_reduce_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, c10::string_view reduce, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; lengths, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; indices, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; offsets, int64_t axis, </span><span class="s2">bool </span><span class="s1">unsafe, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; initial, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::segment_reduce.out(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3111"><span class="ln">3111 </span></a><span class="s1">at::Tensor &amp; _segment_reduce_backward_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; grad, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; output, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; data, c10::string_view reduce, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; lengths, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; offsets, int64_t axis, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Scalar&gt; &amp; initial, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_segment_reduce_backward.out(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, Tensor? offsets=None, int axis=0, Scalar? initial=None, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3112"><span class="ln">3112 </span></a><span class="s1">at::Tensor &amp; _nested_tensor_from_tensor_list_out(at::TensorList list, ::std::optional&lt;at::ScalarType&gt; dtype, ::std::optional&lt;at::Layout&gt; layout, ::std::optional&lt;at::Device&gt; device, ::std::optional&lt;</span><span class="s2">bool</span><span class="s1">&gt; pin_memory, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_nested_tensor_from_tensor_list.out(Tensor[] list, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3113"><span class="ln">3113 </span></a><span class="s1">at::Tensor &amp; _fw_primal_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t level, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fw_primal_copy.out(Tensor self, int level, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3114"><span class="ln">3114 </span></a><span class="s1">at::Tensor &amp; _make_dual_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; primal, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; tangent, int64_t level, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_make_dual_copy.out(Tensor primal, Tensor tangent, int level, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3115"><span class="ln">3115 </span></a><span class="s1">at::Tensor &amp; view_as_real_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3116"><span class="ln">3116 </span></a><span class="s1">at::Tensor &amp; view_as_complex_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_as_complex_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3117"><span class="ln">3117 </span></a><span class="s1">at::Tensor &amp; _conj_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_conj_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3118"><span class="ln">3118 </span></a><span class="s1">at::Tensor &amp; _neg_view_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_neg_view_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3119"><span class="ln">3119 </span></a><span class="s1">at::Tensor &amp; as_strided_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional&lt;c10::SymInt&gt; storage_offset, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3120"><span class="ln">3120 </span></a><span class="s1">at::Tensor &amp; _sparse_broadcast_to_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_sparse_broadcast_to_copy.out(Tensor self, int[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3121"><span class="ln">3121 </span></a><span class="s1">at::Tensor &amp; diagonal_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::diagonal_copy.out(Tensor self, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3122"><span class="ln">3122 </span></a><span class="s1">at::Tensor &amp; expand_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, </span><span class="s2">bool </span><span class="s1">implicit, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3123"><span class="ln">3123 </span></a><span class="s1">at::Tensor &amp; permute_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dims, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::permute_copy.out(Tensor self, int[] dims, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3124"><span class="ln">3124 </span></a><span class="s1">at::Tensor &amp; _reshape_alias_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3125"><span class="ln">3125 </span></a><span class="s1">at::Tensor &amp; select_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, c10::SymInt index, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3126"><span class="ln">3126 </span></a><span class="s1">at::Tensor &amp; detach_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3127"><span class="ln">3127 </span></a><span class="s1">at::Tensor &amp; slice_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, ::std::optional&lt;c10::SymInt&gt; start, ::std::optional&lt;c10::SymInt&gt; end, c10::SymInt step, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3128"><span class="ln">3128 </span></a><span class="s1">at::Tensor &amp; squeeze_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3129"><span class="ln">3129 </span></a><span class="s1">at::Tensor &amp; squeeze_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3130"><span class="ln">3130 </span></a><span class="s1">at::Tensor &amp; squeeze_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::IntArrayRef dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3131"><span class="ln">3131 </span></a><span class="s1">at::Tensor &amp; t_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::t_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3132"><span class="ln">3132 </span></a><span class="s1">at::Tensor &amp; transpose_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim0, int64_t dim1, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::transpose_copy.int_out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3133"><span class="ln">3133 </span></a><span class="s1">at::Tensor &amp; unsqueeze_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dim, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unsqueeze_copy.out(Tensor self, int dim, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3134"><span class="ln">3134 </span></a><span class="s1">at::Tensor &amp; _indices_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_indices_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3135"><span class="ln">3135 </span></a><span class="s1">at::Tensor &amp; _values_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_values_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3136"><span class="ln">3136 </span></a><span class="s1">at::Tensor &amp; indices_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3137"><span class="ln">3137 </span></a><span class="s1">at::Tensor &amp; values_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::values_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3138"><span class="ln">3138 </span></a><span class="s1">at::Tensor &amp; crow_indices_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::crow_indices_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3139"><span class="ln">3139 </span></a><span class="s1">at::Tensor &amp; col_indices_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::col_indices_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3140"><span class="ln">3140 </span></a><span class="s1">at::Tensor &amp; ccol_indices_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3141"><span class="ln">3141 </span></a><span class="s1">at::Tensor &amp; row_indices_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3142"><span class="ln">3142 </span></a><span class="s1">at::Tensor &amp; view_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, c10::SymIntArrayRef size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3143"><span class="ln">3143 </span></a><span class="s1">at::Tensor &amp; view_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::ScalarType dtype, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::view_copy.dtype_out(Tensor self, ScalarType dtype, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3144"><span class="ln">3144 </span></a><span class="s1">at::Tensor &amp; unfold_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, int64_t dimension, int64_t size, int64_t step, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::unfold_copy.out(Tensor self, int dimension, int size, int step, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3145"><span class="ln">3145 </span></a><span class="s1">at::Tensor &amp; alias_copy_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::alias_copy.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3146"><span class="ln">3146 </span></a><span class="s1">at::Tensor &amp; to_padded_tensor_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">double </span><span class="s1">padding, at::OptionalSymIntArrayRef output_size, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3147"><span class="ln">3147 </span></a><span class="s1">at::Tensor &amp; _transformer_encoder_layer_fwd_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; src, int64_t embed_dim, int64_t num_heads, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_bias, </span><span class="s2">bool </span><span class="s1">use_gelu, </span><span class="s2">bool </span><span class="s1">norm_first, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_weight_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_bias_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_weight_2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; norm_bias_2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_weight_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_bias_1, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_weight_2, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; ffn_bias_2, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; mask, ::std::optional&lt;int64_t&gt; mask_type, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3148"><span class="ln">3148 </span></a><span class="s1">::std::tuple&lt;at::Tensor &amp;,at::Tensor &amp;&gt; _native_multi_head_attention_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, int64_t embed_dim, int64_t num_head, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; mask, </span><span class="s2">bool </span><span class="s1">need_weights, </span><span class="s2">bool </span><span class="s1">average_attn_weights, ::std::optional&lt;int64_t&gt; mask_type, at::Tensor &amp; out0, at::Tensor &amp; out1); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -&gt; (Tensor(a!), Tensor(b!))&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3149"><span class="ln">3149 </span></a><span class="s1">at::Tensor &amp; _triton_scaled_dot_attention_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; q, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; k, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; v, </span><span class="s2">double </span><span class="s1">dropout_p, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_triton_scaled_dot_attention.out(Tensor q, Tensor k, Tensor v, float dropout_p=0.0, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3150"><span class="ln">3150 </span></a><span class="s1">at::Tensor &amp; _triton_multi_head_attention_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; query, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; key, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; value, int64_t embed_dim, int64_t num_head, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; qkv_bias, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_weight, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; proj_bias, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; mask, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_triton_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3151"><span class="ln">3151 </span></a><span class="s1">at::Tensor &amp; _foobar_out(</span><span class="s2">const </span><span class="s1">at::Tensor &amp; self, </span><span class="s2">bool </span><span class="s1">arg1, </span><span class="s2">bool </span><span class="s1">arg2, </span><span class="s2">bool </span><span class="s1">arg3, at::Tensor &amp; out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_foobar.out(Tensor self, bool arg1=True, bool arg2=True, *, bool arg3=True, Tensor(a!) out) -&gt; Tensor(a!)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3152"><span class="ln">3152 </span></a><span class="s2">void </span><span class="s1">_fused_adam_out(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adam.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3153"><span class="ln">3153 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_adam(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adam(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3154"><span class="ln">3154 </span></a><span class="s2">void </span><span class="s1">_fused_adam_out(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adam.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3155"><span class="ln">3155 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_adam(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adam.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3156"><span class="ln">3156 </span></a><span class="s2">void </span><span class="s1">_fused_adamw_out(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adamw.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3157"><span class="ln">3157 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_adamw(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adamw(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3158"><span class="ln">3158 </span></a><span class="s2">void </span><span class="s1">_fused_adamw_out(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adamw.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3159"><span class="ln">3159 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_adamw(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">beta1, </span><span class="s2">double </span><span class="s1">beta2, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">amsgrad, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adamw.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3160"><span class="ln">3160 </span></a><span class="s2">void </span><span class="s1">_fused_sgd_out(at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">dampening, </span><span class="s2">bool </span><span class="s1">nesterov, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">bool </span><span class="s1">is_first_step, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_sgd.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] momentum_buffer_list, *, float weight_decay, float momentum, float lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3161"><span class="ln">3161 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_sgd(at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">dampening, </span><span class="s2">bool </span><span class="s1">nesterov, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">bool </span><span class="s1">is_first_step, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_sgd(Tensor[] self, Tensor[] grads, Tensor[] momentum_buffer_list, *, float weight_decay, float momentum, float lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] momentum_buffer_list_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3162"><span class="ln">3162 </span></a><span class="s2">void </span><span class="s1">_fused_sgd_out(at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">dampening, </span><span class="s2">bool </span><span class="s1">nesterov, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">bool </span><span class="s1">is_first_step, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_sgd.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] momentum_buffer_list, *, float weight_decay, float momentum, Tensor lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3163"><span class="ln">3163 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_sgd(at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">momentum, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">dampening, </span><span class="s2">bool </span><span class="s1">nesterov, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">bool </span><span class="s1">is_first_step, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_sgd.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] momentum_buffer_list, *, float weight_decay, float momentum, Tensor lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] momentum_buffer_list_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3164"><span class="ln">3164 </span></a><span class="s2">void </span><span class="s1">_fused_adagrad_out(at::TensorList self, at::TensorList grads, at::TensorList state_sums, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">lr_decay, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adagrad.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] state_sums, Tensor(d!)[] state_steps, *, float lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3165"><span class="ln">3165 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_adagrad(at::TensorList self, at::TensorList grads, at::TensorList state_sums, at::TensorList state_steps, </span><span class="s2">double </span><span class="s1">lr, </span><span class="s2">double </span><span class="s1">lr_decay, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adagrad(Tensor[] self, Tensor[] grads, Tensor[] state_sums, Tensor[] state_steps, *, float lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] state_sums_out, Tensor[] state_steps_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3166"><span class="ln">3166 </span></a><span class="s2">void </span><span class="s1">_fused_adagrad_out(at::TensorList self, at::TensorList grads, at::TensorList state_sums, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">lr_decay, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf, at::TensorList out); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adagrad.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] state_sums, Tensor[] state_steps, *, Tensor lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -&gt; ()&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3167"><span class="ln">3167 </span></a><span class="s1">::std::tuple&lt;::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;,::std::vector&lt;at::Tensor&gt;&gt; _fused_adagrad(at::TensorList self, at::TensorList grads, at::TensorList state_sums, at::TensorList state_steps, </span><span class="s2">const </span><span class="s1">at::Tensor &amp; lr, </span><span class="s2">double </span><span class="s1">lr_decay, </span><span class="s2">double </span><span class="s1">weight_decay, </span><span class="s2">double </span><span class="s1">eps, </span><span class="s2">bool </span><span class="s1">maximize, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; grad_scale, </span><span class="s2">const </span><span class="s1">::std::optional&lt;at::Tensor&gt; &amp; found_inf); </span><span class="s0">// {&quot;schema&quot;: &quot;aten::_fused_adagrad.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] state_sums, Tensor[] state_steps, *, Tensor lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; (Tensor[] self_out, Tensor[] grads_out, Tensor[] state_sums_out)&quot;, &quot;dispatch&quot;: &quot;True&quot;, &quot;default&quot;: &quot;True&quot;}</span>
<a name="l3168"><span class="ln">3168 </span></a></pre>
</body>
</html>