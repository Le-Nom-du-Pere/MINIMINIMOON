<html>
<head>
<title>_VF.pyi</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #969896;}
.s1 { color: #333333;}
.s2 { color: #a71d5d;}
.s3 { color: #63a35c;}
.s4 { color: #183691;}
.s5 { color: #0086b3;}
.ln { color: #333333; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_VF.pyi</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0"># @generated by tools/pyi/gen_pyi.py from torch/_C/_VariableFunctions.pyi.in</span>
<a name="l2"><span class="ln">2    </span></a><span class="s0"># mypy: disable-error-code=&quot;type-arg&quot;</span>
<a name="l3"><span class="ln">3    </span></a><span class="s0"># mypy: allow-untyped-defs</span>
<a name="l4"><span class="ln">4    </span></a><span class="s0"># ruff: noqa: F401,PYI054</span>
<a name="l5"><span class="ln">5    </span></a>
<a name="l6"><span class="ln">6    </span></a><span class="s2">from </span><span class="s1">collections</span><span class="s3">.</span><span class="s1">abc </span><span class="s2">import </span><span class="s1">Sequence</span>
<a name="l7"><span class="ln">7    </span></a><span class="s2">from </span><span class="s1">types </span><span class="s2">import </span><span class="s1">EllipsisType</span>
<a name="l8"><span class="ln">8    </span></a><span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Literal</span><span class="s3">, </span><span class="s1">overload</span><span class="s3">, </span><span class="s1">TypeVar</span>
<a name="l9"><span class="ln">9    </span></a>
<a name="l10"><span class="ln">10   </span></a><span class="s2">import </span><span class="s1">torch</span>
<a name="l11"><span class="ln">11   </span></a><span class="s2">from </span><span class="s1">torch </span><span class="s2">import </span><span class="s3">(</span>
<a name="l12"><span class="ln">12   </span></a>    <span class="s1">contiguous_format</span><span class="s3">,</span>
<a name="l13"><span class="ln">13   </span></a>    <span class="s1">Generator</span><span class="s3">,</span>
<a name="l14"><span class="ln">14   </span></a>    <span class="s1">inf</span><span class="s3">,</span>
<a name="l15"><span class="ln">15   </span></a>    <span class="s1">memory_format</span><span class="s3">,</span>
<a name="l16"><span class="ln">16   </span></a>    <span class="s1">strided</span><span class="s3">,</span>
<a name="l17"><span class="ln">17   </span></a>    <span class="s1">SymInt</span><span class="s3">,</span>
<a name="l18"><span class="ln">18   </span></a>    <span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19"><span class="ln">19   </span></a><span class="s3">)</span>
<a name="l20"><span class="ln">20   </span></a><span class="s2">from </span><span class="s1">torch</span><span class="s3">.</span><span class="s1">_prims_common </span><span class="s2">import </span><span class="s1">DeviceLikeType</span>
<a name="l21"><span class="ln">21   </span></a><span class="s2">from </span><span class="s1">torch</span><span class="s3">.</span><span class="s1">types </span><span class="s2">import </span><span class="s3">(</span>
<a name="l22"><span class="ln">22   </span></a>    <span class="s1">_bool</span><span class="s3">,</span>
<a name="l23"><span class="ln">23   </span></a>    <span class="s1">_complex</span><span class="s3">,</span>
<a name="l24"><span class="ln">24   </span></a>    <span class="s1">_device</span><span class="s3">,</span>
<a name="l25"><span class="ln">25   </span></a>    <span class="s1">_dtype</span><span class="s3">,</span>
<a name="l26"><span class="ln">26   </span></a>    <span class="s1">_float</span><span class="s3">,</span>
<a name="l27"><span class="ln">27   </span></a>    <span class="s1">_int</span><span class="s3">,</span>
<a name="l28"><span class="ln">28   </span></a>    <span class="s1">_layout</span><span class="s3">,</span>
<a name="l29"><span class="ln">29   </span></a>    <span class="s1">_qscheme</span><span class="s3">,</span>
<a name="l30"><span class="ln">30   </span></a>    <span class="s1">_size</span><span class="s3">,</span>
<a name="l31"><span class="ln">31   </span></a>    <span class="s1">Device</span><span class="s3">,</span>
<a name="l32"><span class="ln">32   </span></a>    <span class="s1">Number</span><span class="s3">,</span>
<a name="l33"><span class="ln">33   </span></a><span class="s3">)</span>
<a name="l34"><span class="ln">34   </span></a>
<a name="l35"><span class="ln">35   </span></a><span class="s1">__all__ </span><span class="s2">= </span><span class="s3">[</span>
<a name="l36"><span class="ln">36   </span></a>    <span class="s4">&quot;__and__&quot;</span><span class="s3">,</span>
<a name="l37"><span class="ln">37   </span></a>    <span class="s4">&quot;__lshift__&quot;</span><span class="s3">,</span>
<a name="l38"><span class="ln">38   </span></a>    <span class="s4">&quot;__or__&quot;</span><span class="s3">,</span>
<a name="l39"><span class="ln">39   </span></a>    <span class="s4">&quot;__rshift__&quot;</span><span class="s3">,</span>
<a name="l40"><span class="ln">40   </span></a>    <span class="s4">&quot;__xor__&quot;</span><span class="s3">,</span>
<a name="l41"><span class="ln">41   </span></a>    <span class="s4">&quot;_adaptive_avg_pool2d&quot;</span><span class="s3">,</span>
<a name="l42"><span class="ln">42   </span></a>    <span class="s4">&quot;_adaptive_avg_pool3d&quot;</span><span class="s3">,</span>
<a name="l43"><span class="ln">43   </span></a>    <span class="s4">&quot;_add_batch_dim&quot;</span><span class="s3">,</span>
<a name="l44"><span class="ln">44   </span></a>    <span class="s4">&quot;_add_relu&quot;</span><span class="s3">,</span>
<a name="l45"><span class="ln">45   </span></a>    <span class="s4">&quot;_add_relu_&quot;</span><span class="s3">,</span>
<a name="l46"><span class="ln">46   </span></a>    <span class="s4">&quot;_addmm_activation&quot;</span><span class="s3">,</span>
<a name="l47"><span class="ln">47   </span></a>    <span class="s4">&quot;_aminmax&quot;</span><span class="s3">,</span>
<a name="l48"><span class="ln">48   </span></a>    <span class="s4">&quot;_amp_foreach_non_finite_check_and_unscale_&quot;</span><span class="s3">,</span>
<a name="l49"><span class="ln">49   </span></a>    <span class="s4">&quot;_amp_update_scale_&quot;</span><span class="s3">,</span>
<a name="l50"><span class="ln">50   </span></a>    <span class="s4">&quot;_assert_async&quot;</span><span class="s3">,</span>
<a name="l51"><span class="ln">51   </span></a>    <span class="s4">&quot;_assert_scalar&quot;</span><span class="s3">,</span>
<a name="l52"><span class="ln">52   </span></a>    <span class="s4">&quot;_assert_tensor_metadata&quot;</span><span class="s3">,</span>
<a name="l53"><span class="ln">53   </span></a>    <span class="s4">&quot;_batch_norm_impl_index&quot;</span><span class="s3">,</span>
<a name="l54"><span class="ln">54   </span></a>    <span class="s4">&quot;_cast_Byte&quot;</span><span class="s3">,</span>
<a name="l55"><span class="ln">55   </span></a>    <span class="s4">&quot;_cast_Char&quot;</span><span class="s3">,</span>
<a name="l56"><span class="ln">56   </span></a>    <span class="s4">&quot;_cast_Double&quot;</span><span class="s3">,</span>
<a name="l57"><span class="ln">57   </span></a>    <span class="s4">&quot;_cast_Float&quot;</span><span class="s3">,</span>
<a name="l58"><span class="ln">58   </span></a>    <span class="s4">&quot;_cast_Half&quot;</span><span class="s3">,</span>
<a name="l59"><span class="ln">59   </span></a>    <span class="s4">&quot;_cast_Int&quot;</span><span class="s3">,</span>
<a name="l60"><span class="ln">60   </span></a>    <span class="s4">&quot;_cast_Long&quot;</span><span class="s3">,</span>
<a name="l61"><span class="ln">61   </span></a>    <span class="s4">&quot;_cast_Short&quot;</span><span class="s3">,</span>
<a name="l62"><span class="ln">62   </span></a>    <span class="s4">&quot;_choose_qparams_per_tensor&quot;</span><span class="s3">,</span>
<a name="l63"><span class="ln">63   </span></a>    <span class="s4">&quot;_chunk_cat&quot;</span><span class="s3">,</span>
<a name="l64"><span class="ln">64   </span></a>    <span class="s4">&quot;_coalesce&quot;</span><span class="s3">,</span>
<a name="l65"><span class="ln">65   </span></a>    <span class="s4">&quot;_compute_linear_combination&quot;</span><span class="s3">,</span>
<a name="l66"><span class="ln">66   </span></a>    <span class="s4">&quot;_conj&quot;</span><span class="s3">,</span>
<a name="l67"><span class="ln">67   </span></a>    <span class="s4">&quot;_conj_copy&quot;</span><span class="s3">,</span>
<a name="l68"><span class="ln">68   </span></a>    <span class="s4">&quot;_conj_physical&quot;</span><span class="s3">,</span>
<a name="l69"><span class="ln">69   </span></a>    <span class="s4">&quot;_convert_indices_from_coo_to_csr&quot;</span><span class="s3">,</span>
<a name="l70"><span class="ln">70   </span></a>    <span class="s4">&quot;_convert_indices_from_csr_to_coo&quot;</span><span class="s3">,</span>
<a name="l71"><span class="ln">71   </span></a>    <span class="s4">&quot;_convert_weight_to_int4pack&quot;</span><span class="s3">,</span>
<a name="l72"><span class="ln">72   </span></a>    <span class="s4">&quot;_convert_weight_to_int4pack_for_cpu&quot;</span><span class="s3">,</span>
<a name="l73"><span class="ln">73   </span></a>    <span class="s4">&quot;_convolution&quot;</span><span class="s3">,</span>
<a name="l74"><span class="ln">74   </span></a>    <span class="s4">&quot;_convolution_mode&quot;</span><span class="s3">,</span>
<a name="l75"><span class="ln">75   </span></a>    <span class="s4">&quot;_copy_from&quot;</span><span class="s3">,</span>
<a name="l76"><span class="ln">76   </span></a>    <span class="s4">&quot;_copy_from_and_resize&quot;</span><span class="s3">,</span>
<a name="l77"><span class="ln">77   </span></a>    <span class="s4">&quot;_cslt_compress&quot;</span><span class="s3">,</span>
<a name="l78"><span class="ln">78   </span></a>    <span class="s4">&quot;_cslt_sparse_mm&quot;</span><span class="s3">,</span>
<a name="l79"><span class="ln">79   </span></a>    <span class="s4">&quot;_cslt_sparse_mm_search&quot;</span><span class="s3">,</span>
<a name="l80"><span class="ln">80   </span></a>    <span class="s4">&quot;_ctc_loss&quot;</span><span class="s3">,</span>
<a name="l81"><span class="ln">81   </span></a>    <span class="s4">&quot;_cudnn_ctc_loss&quot;</span><span class="s3">,</span>
<a name="l82"><span class="ln">82   </span></a>    <span class="s4">&quot;_cudnn_init_dropout_state&quot;</span><span class="s3">,</span>
<a name="l83"><span class="ln">83   </span></a>    <span class="s4">&quot;_cudnn_rnn&quot;</span><span class="s3">,</span>
<a name="l84"><span class="ln">84   </span></a>    <span class="s4">&quot;_cudnn_rnn_flatten_weight&quot;</span><span class="s3">,</span>
<a name="l85"><span class="ln">85   </span></a>    <span class="s4">&quot;_cufft_clear_plan_cache&quot;</span><span class="s3">,</span>
<a name="l86"><span class="ln">86   </span></a>    <span class="s4">&quot;_cufft_get_plan_cache_max_size&quot;</span><span class="s3">,</span>
<a name="l87"><span class="ln">87   </span></a>    <span class="s4">&quot;_cufft_get_plan_cache_size&quot;</span><span class="s3">,</span>
<a name="l88"><span class="ln">88   </span></a>    <span class="s4">&quot;_cufft_set_plan_cache_max_size&quot;</span><span class="s3">,</span>
<a name="l89"><span class="ln">89   </span></a>    <span class="s4">&quot;_cummax_helper&quot;</span><span class="s3">,</span>
<a name="l90"><span class="ln">90   </span></a>    <span class="s4">&quot;_cummin_helper&quot;</span><span class="s3">,</span>
<a name="l91"><span class="ln">91   </span></a>    <span class="s4">&quot;_debug_has_internal_overlap&quot;</span><span class="s3">,</span>
<a name="l92"><span class="ln">92   </span></a>    <span class="s4">&quot;_dim_arange&quot;</span><span class="s3">,</span>
<a name="l93"><span class="ln">93   </span></a>    <span class="s4">&quot;_dirichlet_grad&quot;</span><span class="s3">,</span>
<a name="l94"><span class="ln">94   </span></a>    <span class="s4">&quot;_disable_functionalization&quot;</span><span class="s3">,</span>
<a name="l95"><span class="ln">95   </span></a>    <span class="s4">&quot;_dyn_quant_matmul_4bit&quot;</span><span class="s3">,</span>
<a name="l96"><span class="ln">96   </span></a>    <span class="s4">&quot;_dyn_quant_pack_4bit_weight&quot;</span><span class="s3">,</span>
<a name="l97"><span class="ln">97   </span></a>    <span class="s4">&quot;_efficientzerotensor&quot;</span><span class="s3">,</span>
<a name="l98"><span class="ln">98   </span></a>    <span class="s4">&quot;_embedding_bag&quot;</span><span class="s3">,</span>
<a name="l99"><span class="ln">99   </span></a>    <span class="s4">&quot;_embedding_bag_forward_only&quot;</span><span class="s3">,</span>
<a name="l100"><span class="ln">100  </span></a>    <span class="s4">&quot;_empty_affine_quantized&quot;</span><span class="s3">,</span>
<a name="l101"><span class="ln">101  </span></a>    <span class="s4">&quot;_empty_per_channel_affine_quantized&quot;</span><span class="s3">,</span>
<a name="l102"><span class="ln">102  </span></a>    <span class="s4">&quot;_enable_functionalization&quot;</span><span class="s3">,</span>
<a name="l103"><span class="ln">103  </span></a>    <span class="s4">&quot;_euclidean_dist&quot;</span><span class="s3">,</span>
<a name="l104"><span class="ln">104  </span></a>    <span class="s4">&quot;_fake_quantize_learnable_per_channel_affine&quot;</span><span class="s3">,</span>
<a name="l105"><span class="ln">105  </span></a>    <span class="s4">&quot;_fake_quantize_learnable_per_tensor_affine&quot;</span><span class="s3">,</span>
<a name="l106"><span class="ln">106  </span></a>    <span class="s4">&quot;_fake_quantize_per_tensor_affine_cachemask_tensor_qparams&quot;</span><span class="s3">,</span>
<a name="l107"><span class="ln">107  </span></a>    <span class="s4">&quot;_fake_quantize_per_tensor_affine_cachemask_tensor_qparams&quot;</span><span class="s3">,</span>
<a name="l108"><span class="ln">108  </span></a>    <span class="s4">&quot;_fft_c2c&quot;</span><span class="s3">,</span>
<a name="l109"><span class="ln">109  </span></a>    <span class="s4">&quot;_fft_c2r&quot;</span><span class="s3">,</span>
<a name="l110"><span class="ln">110  </span></a>    <span class="s4">&quot;_fft_r2c&quot;</span><span class="s3">,</span>
<a name="l111"><span class="ln">111  </span></a>    <span class="s4">&quot;_fill_mem_eff_dropout_mask_&quot;</span><span class="s3">,</span>
<a name="l112"><span class="ln">112  </span></a>    <span class="s4">&quot;_foobar&quot;</span><span class="s3">,</span>
<a name="l113"><span class="ln">113  </span></a>    <span class="s4">&quot;_foreach_abs&quot;</span><span class="s3">,</span>
<a name="l114"><span class="ln">114  </span></a>    <span class="s4">&quot;_foreach_abs_&quot;</span><span class="s3">,</span>
<a name="l115"><span class="ln">115  </span></a>    <span class="s4">&quot;_foreach_acos&quot;</span><span class="s3">,</span>
<a name="l116"><span class="ln">116  </span></a>    <span class="s4">&quot;_foreach_acos_&quot;</span><span class="s3">,</span>
<a name="l117"><span class="ln">117  </span></a>    <span class="s4">&quot;_foreach_add&quot;</span><span class="s3">,</span>
<a name="l118"><span class="ln">118  </span></a>    <span class="s4">&quot;_foreach_add_&quot;</span><span class="s3">,</span>
<a name="l119"><span class="ln">119  </span></a>    <span class="s4">&quot;_foreach_addcdiv&quot;</span><span class="s3">,</span>
<a name="l120"><span class="ln">120  </span></a>    <span class="s4">&quot;_foreach_addcdiv_&quot;</span><span class="s3">,</span>
<a name="l121"><span class="ln">121  </span></a>    <span class="s4">&quot;_foreach_addcmul&quot;</span><span class="s3">,</span>
<a name="l122"><span class="ln">122  </span></a>    <span class="s4">&quot;_foreach_addcmul_&quot;</span><span class="s3">,</span>
<a name="l123"><span class="ln">123  </span></a>    <span class="s4">&quot;_foreach_asin&quot;</span><span class="s3">,</span>
<a name="l124"><span class="ln">124  </span></a>    <span class="s4">&quot;_foreach_asin_&quot;</span><span class="s3">,</span>
<a name="l125"><span class="ln">125  </span></a>    <span class="s4">&quot;_foreach_atan&quot;</span><span class="s3">,</span>
<a name="l126"><span class="ln">126  </span></a>    <span class="s4">&quot;_foreach_atan_&quot;</span><span class="s3">,</span>
<a name="l127"><span class="ln">127  </span></a>    <span class="s4">&quot;_foreach_ceil&quot;</span><span class="s3">,</span>
<a name="l128"><span class="ln">128  </span></a>    <span class="s4">&quot;_foreach_ceil_&quot;</span><span class="s3">,</span>
<a name="l129"><span class="ln">129  </span></a>    <span class="s4">&quot;_foreach_clamp_max&quot;</span><span class="s3">,</span>
<a name="l130"><span class="ln">130  </span></a>    <span class="s4">&quot;_foreach_clamp_max_&quot;</span><span class="s3">,</span>
<a name="l131"><span class="ln">131  </span></a>    <span class="s4">&quot;_foreach_clamp_min&quot;</span><span class="s3">,</span>
<a name="l132"><span class="ln">132  </span></a>    <span class="s4">&quot;_foreach_clamp_min_&quot;</span><span class="s3">,</span>
<a name="l133"><span class="ln">133  </span></a>    <span class="s4">&quot;_foreach_copy_&quot;</span><span class="s3">,</span>
<a name="l134"><span class="ln">134  </span></a>    <span class="s4">&quot;_foreach_cos&quot;</span><span class="s3">,</span>
<a name="l135"><span class="ln">135  </span></a>    <span class="s4">&quot;_foreach_cos_&quot;</span><span class="s3">,</span>
<a name="l136"><span class="ln">136  </span></a>    <span class="s4">&quot;_foreach_cosh&quot;</span><span class="s3">,</span>
<a name="l137"><span class="ln">137  </span></a>    <span class="s4">&quot;_foreach_cosh_&quot;</span><span class="s3">,</span>
<a name="l138"><span class="ln">138  </span></a>    <span class="s4">&quot;_foreach_div&quot;</span><span class="s3">,</span>
<a name="l139"><span class="ln">139  </span></a>    <span class="s4">&quot;_foreach_div_&quot;</span><span class="s3">,</span>
<a name="l140"><span class="ln">140  </span></a>    <span class="s4">&quot;_foreach_erf&quot;</span><span class="s3">,</span>
<a name="l141"><span class="ln">141  </span></a>    <span class="s4">&quot;_foreach_erf_&quot;</span><span class="s3">,</span>
<a name="l142"><span class="ln">142  </span></a>    <span class="s4">&quot;_foreach_erfc&quot;</span><span class="s3">,</span>
<a name="l143"><span class="ln">143  </span></a>    <span class="s4">&quot;_foreach_erfc_&quot;</span><span class="s3">,</span>
<a name="l144"><span class="ln">144  </span></a>    <span class="s4">&quot;_foreach_exp&quot;</span><span class="s3">,</span>
<a name="l145"><span class="ln">145  </span></a>    <span class="s4">&quot;_foreach_exp_&quot;</span><span class="s3">,</span>
<a name="l146"><span class="ln">146  </span></a>    <span class="s4">&quot;_foreach_expm1&quot;</span><span class="s3">,</span>
<a name="l147"><span class="ln">147  </span></a>    <span class="s4">&quot;_foreach_expm1_&quot;</span><span class="s3">,</span>
<a name="l148"><span class="ln">148  </span></a>    <span class="s4">&quot;_foreach_floor&quot;</span><span class="s3">,</span>
<a name="l149"><span class="ln">149  </span></a>    <span class="s4">&quot;_foreach_floor_&quot;</span><span class="s3">,</span>
<a name="l150"><span class="ln">150  </span></a>    <span class="s4">&quot;_foreach_frac&quot;</span><span class="s3">,</span>
<a name="l151"><span class="ln">151  </span></a>    <span class="s4">&quot;_foreach_frac_&quot;</span><span class="s3">,</span>
<a name="l152"><span class="ln">152  </span></a>    <span class="s4">&quot;_foreach_lerp&quot;</span><span class="s3">,</span>
<a name="l153"><span class="ln">153  </span></a>    <span class="s4">&quot;_foreach_lerp_&quot;</span><span class="s3">,</span>
<a name="l154"><span class="ln">154  </span></a>    <span class="s4">&quot;_foreach_lgamma&quot;</span><span class="s3">,</span>
<a name="l155"><span class="ln">155  </span></a>    <span class="s4">&quot;_foreach_lgamma_&quot;</span><span class="s3">,</span>
<a name="l156"><span class="ln">156  </span></a>    <span class="s4">&quot;_foreach_log&quot;</span><span class="s3">,</span>
<a name="l157"><span class="ln">157  </span></a>    <span class="s4">&quot;_foreach_log10&quot;</span><span class="s3">,</span>
<a name="l158"><span class="ln">158  </span></a>    <span class="s4">&quot;_foreach_log10_&quot;</span><span class="s3">,</span>
<a name="l159"><span class="ln">159  </span></a>    <span class="s4">&quot;_foreach_log1p&quot;</span><span class="s3">,</span>
<a name="l160"><span class="ln">160  </span></a>    <span class="s4">&quot;_foreach_log1p_&quot;</span><span class="s3">,</span>
<a name="l161"><span class="ln">161  </span></a>    <span class="s4">&quot;_foreach_log2&quot;</span><span class="s3">,</span>
<a name="l162"><span class="ln">162  </span></a>    <span class="s4">&quot;_foreach_log2_&quot;</span><span class="s3">,</span>
<a name="l163"><span class="ln">163  </span></a>    <span class="s4">&quot;_foreach_log_&quot;</span><span class="s3">,</span>
<a name="l164"><span class="ln">164  </span></a>    <span class="s4">&quot;_foreach_max&quot;</span><span class="s3">,</span>
<a name="l165"><span class="ln">165  </span></a>    <span class="s4">&quot;_foreach_maximum&quot;</span><span class="s3">,</span>
<a name="l166"><span class="ln">166  </span></a>    <span class="s4">&quot;_foreach_maximum_&quot;</span><span class="s3">,</span>
<a name="l167"><span class="ln">167  </span></a>    <span class="s4">&quot;_foreach_minimum&quot;</span><span class="s3">,</span>
<a name="l168"><span class="ln">168  </span></a>    <span class="s4">&quot;_foreach_minimum_&quot;</span><span class="s3">,</span>
<a name="l169"><span class="ln">169  </span></a>    <span class="s4">&quot;_foreach_mul&quot;</span><span class="s3">,</span>
<a name="l170"><span class="ln">170  </span></a>    <span class="s4">&quot;_foreach_mul_&quot;</span><span class="s3">,</span>
<a name="l171"><span class="ln">171  </span></a>    <span class="s4">&quot;_foreach_neg&quot;</span><span class="s3">,</span>
<a name="l172"><span class="ln">172  </span></a>    <span class="s4">&quot;_foreach_neg_&quot;</span><span class="s3">,</span>
<a name="l173"><span class="ln">173  </span></a>    <span class="s4">&quot;_foreach_norm&quot;</span><span class="s3">,</span>
<a name="l174"><span class="ln">174  </span></a>    <span class="s4">&quot;_foreach_pow&quot;</span><span class="s3">,</span>
<a name="l175"><span class="ln">175  </span></a>    <span class="s4">&quot;_foreach_pow_&quot;</span><span class="s3">,</span>
<a name="l176"><span class="ln">176  </span></a>    <span class="s4">&quot;_foreach_reciprocal&quot;</span><span class="s3">,</span>
<a name="l177"><span class="ln">177  </span></a>    <span class="s4">&quot;_foreach_reciprocal_&quot;</span><span class="s3">,</span>
<a name="l178"><span class="ln">178  </span></a>    <span class="s4">&quot;_foreach_round&quot;</span><span class="s3">,</span>
<a name="l179"><span class="ln">179  </span></a>    <span class="s4">&quot;_foreach_round_&quot;</span><span class="s3">,</span>
<a name="l180"><span class="ln">180  </span></a>    <span class="s4">&quot;_foreach_rsqrt&quot;</span><span class="s3">,</span>
<a name="l181"><span class="ln">181  </span></a>    <span class="s4">&quot;_foreach_rsqrt_&quot;</span><span class="s3">,</span>
<a name="l182"><span class="ln">182  </span></a>    <span class="s4">&quot;_foreach_sigmoid&quot;</span><span class="s3">,</span>
<a name="l183"><span class="ln">183  </span></a>    <span class="s4">&quot;_foreach_sigmoid_&quot;</span><span class="s3">,</span>
<a name="l184"><span class="ln">184  </span></a>    <span class="s4">&quot;_foreach_sign&quot;</span><span class="s3">,</span>
<a name="l185"><span class="ln">185  </span></a>    <span class="s4">&quot;_foreach_sign_&quot;</span><span class="s3">,</span>
<a name="l186"><span class="ln">186  </span></a>    <span class="s4">&quot;_foreach_sin&quot;</span><span class="s3">,</span>
<a name="l187"><span class="ln">187  </span></a>    <span class="s4">&quot;_foreach_sin_&quot;</span><span class="s3">,</span>
<a name="l188"><span class="ln">188  </span></a>    <span class="s4">&quot;_foreach_sinh&quot;</span><span class="s3">,</span>
<a name="l189"><span class="ln">189  </span></a>    <span class="s4">&quot;_foreach_sinh_&quot;</span><span class="s3">,</span>
<a name="l190"><span class="ln">190  </span></a>    <span class="s4">&quot;_foreach_sqrt&quot;</span><span class="s3">,</span>
<a name="l191"><span class="ln">191  </span></a>    <span class="s4">&quot;_foreach_sqrt_&quot;</span><span class="s3">,</span>
<a name="l192"><span class="ln">192  </span></a>    <span class="s4">&quot;_foreach_sub&quot;</span><span class="s3">,</span>
<a name="l193"><span class="ln">193  </span></a>    <span class="s4">&quot;_foreach_sub_&quot;</span><span class="s3">,</span>
<a name="l194"><span class="ln">194  </span></a>    <span class="s4">&quot;_foreach_tan&quot;</span><span class="s3">,</span>
<a name="l195"><span class="ln">195  </span></a>    <span class="s4">&quot;_foreach_tan_&quot;</span><span class="s3">,</span>
<a name="l196"><span class="ln">196  </span></a>    <span class="s4">&quot;_foreach_tanh&quot;</span><span class="s3">,</span>
<a name="l197"><span class="ln">197  </span></a>    <span class="s4">&quot;_foreach_tanh_&quot;</span><span class="s3">,</span>
<a name="l198"><span class="ln">198  </span></a>    <span class="s4">&quot;_foreach_trunc&quot;</span><span class="s3">,</span>
<a name="l199"><span class="ln">199  </span></a>    <span class="s4">&quot;_foreach_trunc_&quot;</span><span class="s3">,</span>
<a name="l200"><span class="ln">200  </span></a>    <span class="s4">&quot;_foreach_zero_&quot;</span><span class="s3">,</span>
<a name="l201"><span class="ln">201  </span></a>    <span class="s4">&quot;_from_functional_tensor&quot;</span><span class="s3">,</span>
<a name="l202"><span class="ln">202  </span></a>    <span class="s4">&quot;_functional_assert_async&quot;</span><span class="s3">,</span>
<a name="l203"><span class="ln">203  </span></a>    <span class="s4">&quot;_functional_assert_scalar&quot;</span><span class="s3">,</span>
<a name="l204"><span class="ln">204  </span></a>    <span class="s4">&quot;_functional_sym_constrain_range&quot;</span><span class="s3">,</span>
<a name="l205"><span class="ln">205  </span></a>    <span class="s4">&quot;_functional_sym_constrain_range_for_size&quot;</span><span class="s3">,</span>
<a name="l206"><span class="ln">206  </span></a>    <span class="s4">&quot;_functionalize_apply_view_metas&quot;</span><span class="s3">,</span>
<a name="l207"><span class="ln">207  </span></a>    <span class="s4">&quot;_functionalize_are_all_mutations_hidden_from_autograd&quot;</span><span class="s3">,</span>
<a name="l208"><span class="ln">208  </span></a>    <span class="s4">&quot;_functionalize_are_all_mutations_under_no_grad_or_inference_mode&quot;</span><span class="s3">,</span>
<a name="l209"><span class="ln">209  </span></a>    <span class="s4">&quot;_functionalize_commit_update&quot;</span><span class="s3">,</span>
<a name="l210"><span class="ln">210  </span></a>    <span class="s4">&quot;_functionalize_has_metadata_mutation&quot;</span><span class="s3">,</span>
<a name="l211"><span class="ln">211  </span></a>    <span class="s4">&quot;_functionalize_is_symbolic&quot;</span><span class="s3">,</span>
<a name="l212"><span class="ln">212  </span></a>    <span class="s4">&quot;_functionalize_mark_mutation_hidden_from_autograd&quot;</span><span class="s3">,</span>
<a name="l213"><span class="ln">213  </span></a>    <span class="s4">&quot;_functionalize_replace&quot;</span><span class="s3">,</span>
<a name="l214"><span class="ln">214  </span></a>    <span class="s4">&quot;_functionalize_set_storage_changed&quot;</span><span class="s3">,</span>
<a name="l215"><span class="ln">215  </span></a>    <span class="s4">&quot;_functionalize_sync&quot;</span><span class="s3">,</span>
<a name="l216"><span class="ln">216  </span></a>    <span class="s4">&quot;_functionalize_unsafe_set&quot;</span><span class="s3">,</span>
<a name="l217"><span class="ln">217  </span></a>    <span class="s4">&quot;_functionalize_was_inductor_storage_resized&quot;</span><span class="s3">,</span>
<a name="l218"><span class="ln">218  </span></a>    <span class="s4">&quot;_functionalize_was_storage_changed&quot;</span><span class="s3">,</span>
<a name="l219"><span class="ln">219  </span></a>    <span class="s4">&quot;_fused_adagrad_&quot;</span><span class="s3">,</span>
<a name="l220"><span class="ln">220  </span></a>    <span class="s4">&quot;_fused_adam_&quot;</span><span class="s3">,</span>
<a name="l221"><span class="ln">221  </span></a>    <span class="s4">&quot;_fused_adamw_&quot;</span><span class="s3">,</span>
<a name="l222"><span class="ln">222  </span></a>    <span class="s4">&quot;_fused_dropout&quot;</span><span class="s3">,</span>
<a name="l223"><span class="ln">223  </span></a>    <span class="s4">&quot;_fused_moving_avg_obs_fq_helper&quot;</span><span class="s3">,</span>
<a name="l224"><span class="ln">224  </span></a>    <span class="s4">&quot;_fused_moving_avg_obs_fq_helper&quot;</span><span class="s3">,</span>
<a name="l225"><span class="ln">225  </span></a>    <span class="s4">&quot;_fused_rms_norm&quot;</span><span class="s3">,</span>
<a name="l226"><span class="ln">226  </span></a>    <span class="s4">&quot;_fused_sdp_choice&quot;</span><span class="s3">,</span>
<a name="l227"><span class="ln">227  </span></a>    <span class="s4">&quot;_fused_sgd_&quot;</span><span class="s3">,</span>
<a name="l228"><span class="ln">228  </span></a>    <span class="s4">&quot;_fw_primal_copy&quot;</span><span class="s3">,</span>
<a name="l229"><span class="ln">229  </span></a>    <span class="s4">&quot;_grid_sampler_2d_cpu_fallback&quot;</span><span class="s3">,</span>
<a name="l230"><span class="ln">230  </span></a>    <span class="s4">&quot;_grouped_mm&quot;</span><span class="s3">,</span>
<a name="l231"><span class="ln">231  </span></a>    <span class="s4">&quot;_has_compatible_shallow_copy_type&quot;</span><span class="s3">,</span>
<a name="l232"><span class="ln">232  </span></a>    <span class="s4">&quot;_histogramdd_bin_edges&quot;</span><span class="s3">,</span>
<a name="l233"><span class="ln">233  </span></a>    <span class="s4">&quot;_histogramdd_from_bin_cts&quot;</span><span class="s3">,</span>
<a name="l234"><span class="ln">234  </span></a>    <span class="s4">&quot;_histogramdd_from_bin_tensors&quot;</span><span class="s3">,</span>
<a name="l235"><span class="ln">235  </span></a>    <span class="s4">&quot;_index_put_impl_&quot;</span><span class="s3">,</span>
<a name="l236"><span class="ln">236  </span></a>    <span class="s4">&quot;_indices_copy&quot;</span><span class="s3">,</span>
<a name="l237"><span class="ln">237  </span></a>    <span class="s4">&quot;_int_mm&quot;</span><span class="s3">,</span>
<a name="l238"><span class="ln">238  </span></a>    <span class="s4">&quot;_is_all_true&quot;</span><span class="s3">,</span>
<a name="l239"><span class="ln">239  </span></a>    <span class="s4">&quot;_is_any_true&quot;</span><span class="s3">,</span>
<a name="l240"><span class="ln">240  </span></a>    <span class="s4">&quot;_is_functional_tensor&quot;</span><span class="s3">,</span>
<a name="l241"><span class="ln">241  </span></a>    <span class="s4">&quot;_is_functional_tensor_base&quot;</span><span class="s3">,</span>
<a name="l242"><span class="ln">242  </span></a>    <span class="s4">&quot;_is_zerotensor&quot;</span><span class="s3">,</span>
<a name="l243"><span class="ln">243  </span></a>    <span class="s4">&quot;_lazy_clone&quot;</span><span class="s3">,</span>
<a name="l244"><span class="ln">244  </span></a>    <span class="s4">&quot;_linalg_check_errors&quot;</span><span class="s3">,</span>
<a name="l245"><span class="ln">245  </span></a>    <span class="s4">&quot;_linalg_det&quot;</span><span class="s3">,</span>
<a name="l246"><span class="ln">246  </span></a>    <span class="s4">&quot;_linalg_det&quot;</span><span class="s3">,</span>
<a name="l247"><span class="ln">247  </span></a>    <span class="s4">&quot;_linalg_eigh&quot;</span><span class="s3">,</span>
<a name="l248"><span class="ln">248  </span></a>    <span class="s4">&quot;_linalg_eigh&quot;</span><span class="s3">,</span>
<a name="l249"><span class="ln">249  </span></a>    <span class="s4">&quot;_linalg_slogdet&quot;</span><span class="s3">,</span>
<a name="l250"><span class="ln">250  </span></a>    <span class="s4">&quot;_linalg_slogdet&quot;</span><span class="s3">,</span>
<a name="l251"><span class="ln">251  </span></a>    <span class="s4">&quot;_linalg_solve_ex&quot;</span><span class="s3">,</span>
<a name="l252"><span class="ln">252  </span></a>    <span class="s4">&quot;_linalg_solve_ex&quot;</span><span class="s3">,</span>
<a name="l253"><span class="ln">253  </span></a>    <span class="s4">&quot;_linalg_svd&quot;</span><span class="s3">,</span>
<a name="l254"><span class="ln">254  </span></a>    <span class="s4">&quot;_linalg_svd&quot;</span><span class="s3">,</span>
<a name="l255"><span class="ln">255  </span></a>    <span class="s4">&quot;_log_softmax&quot;</span><span class="s3">,</span>
<a name="l256"><span class="ln">256  </span></a>    <span class="s4">&quot;_log_softmax_backward_data&quot;</span><span class="s3">,</span>
<a name="l257"><span class="ln">257  </span></a>    <span class="s4">&quot;_logcumsumexp&quot;</span><span class="s3">,</span>
<a name="l258"><span class="ln">258  </span></a>    <span class="s4">&quot;_lstm_mps&quot;</span><span class="s3">,</span>
<a name="l259"><span class="ln">259  </span></a>    <span class="s4">&quot;_lu_with_info&quot;</span><span class="s3">,</span>
<a name="l260"><span class="ln">260  </span></a>    <span class="s4">&quot;_lu_with_info&quot;</span><span class="s3">,</span>
<a name="l261"><span class="ln">261  </span></a>    <span class="s4">&quot;_make_dep_token&quot;</span><span class="s3">,</span>
<a name="l262"><span class="ln">262  </span></a>    <span class="s4">&quot;_make_dual&quot;</span><span class="s3">,</span>
<a name="l263"><span class="ln">263  </span></a>    <span class="s4">&quot;_make_dual_copy&quot;</span><span class="s3">,</span>
<a name="l264"><span class="ln">264  </span></a>    <span class="s4">&quot;_make_per_channel_quantized_tensor&quot;</span><span class="s3">,</span>
<a name="l265"><span class="ln">265  </span></a>    <span class="s4">&quot;_make_per_tensor_quantized_tensor&quot;</span><span class="s3">,</span>
<a name="l266"><span class="ln">266  </span></a>    <span class="s4">&quot;_masked_scale&quot;</span><span class="s3">,</span>
<a name="l267"><span class="ln">267  </span></a>    <span class="s4">&quot;_masked_softmax&quot;</span><span class="s3">,</span>
<a name="l268"><span class="ln">268  </span></a>    <span class="s4">&quot;_mixed_dtypes_linear&quot;</span><span class="s3">,</span>
<a name="l269"><span class="ln">269  </span></a>    <span class="s4">&quot;_mkldnn_reshape&quot;</span><span class="s3">,</span>
<a name="l270"><span class="ln">270  </span></a>    <span class="s4">&quot;_mkldnn_transpose&quot;</span><span class="s3">,</span>
<a name="l271"><span class="ln">271  </span></a>    <span class="s4">&quot;_mkldnn_transpose_&quot;</span><span class="s3">,</span>
<a name="l272"><span class="ln">272  </span></a>    <span class="s4">&quot;_mps_convolution&quot;</span><span class="s3">,</span>
<a name="l273"><span class="ln">273  </span></a>    <span class="s4">&quot;_mps_convolution_transpose&quot;</span><span class="s3">,</span>
<a name="l274"><span class="ln">274  </span></a>    <span class="s4">&quot;_native_batch_norm_legit&quot;</span><span class="s3">,</span>
<a name="l275"><span class="ln">275  </span></a>    <span class="s4">&quot;_native_batch_norm_legit_no_training&quot;</span><span class="s3">,</span>
<a name="l276"><span class="ln">276  </span></a>    <span class="s4">&quot;_native_multi_head_attention&quot;</span><span class="s3">,</span>
<a name="l277"><span class="ln">277  </span></a>    <span class="s4">&quot;_neg_view&quot;</span><span class="s3">,</span>
<a name="l278"><span class="ln">278  </span></a>    <span class="s4">&quot;_neg_view_copy&quot;</span><span class="s3">,</span>
<a name="l279"><span class="ln">279  </span></a>    <span class="s4">&quot;_nested_compute_contiguous_strides_offsets&quot;</span><span class="s3">,</span>
<a name="l280"><span class="ln">280  </span></a>    <span class="s4">&quot;_nested_from_padded&quot;</span><span class="s3">,</span>
<a name="l281"><span class="ln">281  </span></a>    <span class="s4">&quot;_nested_from_padded_and_nested_example&quot;</span><span class="s3">,</span>
<a name="l282"><span class="ln">282  </span></a>    <span class="s4">&quot;_nested_from_padded_tensor&quot;</span><span class="s3">,</span>
<a name="l283"><span class="ln">283  </span></a>    <span class="s4">&quot;_nested_get_jagged_dummy&quot;</span><span class="s3">,</span>
<a name="l284"><span class="ln">284  </span></a>    <span class="s4">&quot;_nested_get_lengths&quot;</span><span class="s3">,</span>
<a name="l285"><span class="ln">285  </span></a>    <span class="s4">&quot;_nested_get_max_seqlen&quot;</span><span class="s3">,</span>
<a name="l286"><span class="ln">286  </span></a>    <span class="s4">&quot;_nested_get_min_seqlen&quot;</span><span class="s3">,</span>
<a name="l287"><span class="ln">287  </span></a>    <span class="s4">&quot;_nested_get_offsets&quot;</span><span class="s3">,</span>
<a name="l288"><span class="ln">288  </span></a>    <span class="s4">&quot;_nested_get_ragged_idx&quot;</span><span class="s3">,</span>
<a name="l289"><span class="ln">289  </span></a>    <span class="s4">&quot;_nested_get_values&quot;</span><span class="s3">,</span>
<a name="l290"><span class="ln">290  </span></a>    <span class="s4">&quot;_nested_get_values_copy&quot;</span><span class="s3">,</span>
<a name="l291"><span class="ln">291  </span></a>    <span class="s4">&quot;_nested_tensor_from_mask&quot;</span><span class="s3">,</span>
<a name="l292"><span class="ln">292  </span></a>    <span class="s4">&quot;_nested_tensor_from_mask_left_aligned&quot;</span><span class="s3">,</span>
<a name="l293"><span class="ln">293  </span></a>    <span class="s4">&quot;_nested_tensor_from_tensor_list&quot;</span><span class="s3">,</span>
<a name="l294"><span class="ln">294  </span></a>    <span class="s4">&quot;_nested_tensor_softmax_with_shape&quot;</span><span class="s3">,</span>
<a name="l295"><span class="ln">295  </span></a>    <span class="s4">&quot;_nested_view_from_buffer&quot;</span><span class="s3">,</span>
<a name="l296"><span class="ln">296  </span></a>    <span class="s4">&quot;_nested_view_from_buffer_copy&quot;</span><span class="s3">,</span>
<a name="l297"><span class="ln">297  </span></a>    <span class="s4">&quot;_nested_view_from_jagged&quot;</span><span class="s3">,</span>
<a name="l298"><span class="ln">298  </span></a>    <span class="s4">&quot;_nested_view_from_jagged_copy&quot;</span><span class="s3">,</span>
<a name="l299"><span class="ln">299  </span></a>    <span class="s4">&quot;_nnpack_available&quot;</span><span class="s3">,</span>
<a name="l300"><span class="ln">300  </span></a>    <span class="s4">&quot;_nnpack_spatial_convolution&quot;</span><span class="s3">,</span>
<a name="l301"><span class="ln">301  </span></a>    <span class="s4">&quot;_pack_padded_sequence&quot;</span><span class="s3">,</span>
<a name="l302"><span class="ln">302  </span></a>    <span class="s4">&quot;_pad_packed_sequence&quot;</span><span class="s3">,</span>
<a name="l303"><span class="ln">303  </span></a>    <span class="s4">&quot;_pin_memory&quot;</span><span class="s3">,</span>
<a name="l304"><span class="ln">304  </span></a>    <span class="s4">&quot;_prelu_kernel&quot;</span><span class="s3">,</span>
<a name="l305"><span class="ln">305  </span></a>    <span class="s4">&quot;_print&quot;</span><span class="s3">,</span>
<a name="l306"><span class="ln">306  </span></a>    <span class="s4">&quot;_propagate_xla_data&quot;</span><span class="s3">,</span>
<a name="l307"><span class="ln">307  </span></a>    <span class="s4">&quot;_remove_batch_dim&quot;</span><span class="s3">,</span>
<a name="l308"><span class="ln">308  </span></a>    <span class="s4">&quot;_reshape_alias_copy&quot;</span><span class="s3">,</span>
<a name="l309"><span class="ln">309  </span></a>    <span class="s4">&quot;_reshape_from_tensor&quot;</span><span class="s3">,</span>
<a name="l310"><span class="ln">310  </span></a>    <span class="s4">&quot;_resize_output_&quot;</span><span class="s3">,</span>
<a name="l311"><span class="ln">311  </span></a>    <span class="s4">&quot;_rowwise_prune&quot;</span><span class="s3">,</span>
<a name="l312"><span class="ln">312  </span></a>    <span class="s4">&quot;_safe_softmax&quot;</span><span class="s3">,</span>
<a name="l313"><span class="ln">313  </span></a>    <span class="s4">&quot;_sample_dirichlet&quot;</span><span class="s3">,</span>
<a name="l314"><span class="ln">314  </span></a>    <span class="s4">&quot;_saturate_weight_to_fp16&quot;</span><span class="s3">,</span>
<a name="l315"><span class="ln">315  </span></a>    <span class="s4">&quot;_scaled_dot_product_attention_math&quot;</span><span class="s3">,</span>
<a name="l316"><span class="ln">316  </span></a>    <span class="s4">&quot;_scaled_dot_product_attention_math_for_mps&quot;</span><span class="s3">,</span>
<a name="l317"><span class="ln">317  </span></a>    <span class="s4">&quot;_scaled_dot_product_cudnn_attention&quot;</span><span class="s3">,</span>
<a name="l318"><span class="ln">318  </span></a>    <span class="s4">&quot;_scaled_dot_product_cudnn_attention&quot;</span><span class="s3">,</span>
<a name="l319"><span class="ln">319  </span></a>    <span class="s4">&quot;_scaled_dot_product_efficient_attention&quot;</span><span class="s3">,</span>
<a name="l320"><span class="ln">320  </span></a>    <span class="s4">&quot;_scaled_dot_product_efficient_attention&quot;</span><span class="s3">,</span>
<a name="l321"><span class="ln">321  </span></a>    <span class="s4">&quot;_scaled_dot_product_flash_attention&quot;</span><span class="s3">,</span>
<a name="l322"><span class="ln">322  </span></a>    <span class="s4">&quot;_scaled_dot_product_flash_attention&quot;</span><span class="s3">,</span>
<a name="l323"><span class="ln">323  </span></a>    <span class="s4">&quot;_scaled_dot_product_flash_attention_for_cpu&quot;</span><span class="s3">,</span>
<a name="l324"><span class="ln">324  </span></a>    <span class="s4">&quot;_scaled_dot_product_flash_attention_for_cpu&quot;</span><span class="s3">,</span>
<a name="l325"><span class="ln">325  </span></a>    <span class="s4">&quot;_scaled_grouped_mm&quot;</span><span class="s3">,</span>
<a name="l326"><span class="ln">326  </span></a>    <span class="s4">&quot;_scaled_mm&quot;</span><span class="s3">,</span>
<a name="l327"><span class="ln">327  </span></a>    <span class="s4">&quot;_shape_as_tensor&quot;</span><span class="s3">,</span>
<a name="l328"><span class="ln">328  </span></a>    <span class="s4">&quot;_sobol_engine_draw&quot;</span><span class="s3">,</span>
<a name="l329"><span class="ln">329  </span></a>    <span class="s4">&quot;_sobol_engine_ff_&quot;</span><span class="s3">,</span>
<a name="l330"><span class="ln">330  </span></a>    <span class="s4">&quot;_sobol_engine_initialize_state_&quot;</span><span class="s3">,</span>
<a name="l331"><span class="ln">331  </span></a>    <span class="s4">&quot;_sobol_engine_scramble_&quot;</span><span class="s3">,</span>
<a name="l332"><span class="ln">332  </span></a>    <span class="s4">&quot;_softmax&quot;</span><span class="s3">,</span>
<a name="l333"><span class="ln">333  </span></a>    <span class="s4">&quot;_softmax_backward_data&quot;</span><span class="s3">,</span>
<a name="l334"><span class="ln">334  </span></a>    <span class="s4">&quot;_sparse_broadcast_to&quot;</span><span class="s3">,</span>
<a name="l335"><span class="ln">335  </span></a>    <span class="s4">&quot;_sparse_broadcast_to_copy&quot;</span><span class="s3">,</span>
<a name="l336"><span class="ln">336  </span></a>    <span class="s4">&quot;_sparse_csr_prod&quot;</span><span class="s3">,</span>
<a name="l337"><span class="ln">337  </span></a>    <span class="s4">&quot;_sparse_csr_sum&quot;</span><span class="s3">,</span>
<a name="l338"><span class="ln">338  </span></a>    <span class="s4">&quot;_sparse_log_softmax_backward_data&quot;</span><span class="s3">,</span>
<a name="l339"><span class="ln">339  </span></a>    <span class="s4">&quot;_sparse_semi_structured_addmm&quot;</span><span class="s3">,</span>
<a name="l340"><span class="ln">340  </span></a>    <span class="s4">&quot;_sparse_semi_structured_apply&quot;</span><span class="s3">,</span>
<a name="l341"><span class="ln">341  </span></a>    <span class="s4">&quot;_sparse_semi_structured_apply_dense&quot;</span><span class="s3">,</span>
<a name="l342"><span class="ln">342  </span></a>    <span class="s4">&quot;_sparse_semi_structured_linear&quot;</span><span class="s3">,</span>
<a name="l343"><span class="ln">343  </span></a>    <span class="s4">&quot;_sparse_semi_structured_mm&quot;</span><span class="s3">,</span>
<a name="l344"><span class="ln">344  </span></a>    <span class="s4">&quot;_sparse_semi_structured_tile&quot;</span><span class="s3">,</span>
<a name="l345"><span class="ln">345  </span></a>    <span class="s4">&quot;_sparse_softmax_backward_data&quot;</span><span class="s3">,</span>
<a name="l346"><span class="ln">346  </span></a>    <span class="s4">&quot;_sparse_sparse_matmul&quot;</span><span class="s3">,</span>
<a name="l347"><span class="ln">347  </span></a>    <span class="s4">&quot;_sparse_sum&quot;</span><span class="s3">,</span>
<a name="l348"><span class="ln">348  </span></a>    <span class="s4">&quot;_stack&quot;</span><span class="s3">,</span>
<a name="l349"><span class="ln">349  </span></a>    <span class="s4">&quot;_standard_gamma&quot;</span><span class="s3">,</span>
<a name="l350"><span class="ln">350  </span></a>    <span class="s4">&quot;_standard_gamma_grad&quot;</span><span class="s3">,</span>
<a name="l351"><span class="ln">351  </span></a>    <span class="s4">&quot;_sync&quot;</span><span class="s3">,</span>
<a name="l352"><span class="ln">352  </span></a>    <span class="s4">&quot;_test_autograd_multiple_dispatch&quot;</span><span class="s3">,</span>
<a name="l353"><span class="ln">353  </span></a>    <span class="s4">&quot;_test_autograd_multiple_dispatch_view&quot;</span><span class="s3">,</span>
<a name="l354"><span class="ln">354  </span></a>    <span class="s4">&quot;_test_autograd_multiple_dispatch_view_copy&quot;</span><span class="s3">,</span>
<a name="l355"><span class="ln">355  </span></a>    <span class="s4">&quot;_test_check_tensor&quot;</span><span class="s3">,</span>
<a name="l356"><span class="ln">356  </span></a>    <span class="s4">&quot;_test_functorch_fallback&quot;</span><span class="s3">,</span>
<a name="l357"><span class="ln">357  </span></a>    <span class="s4">&quot;_test_parallel_materialize&quot;</span><span class="s3">,</span>
<a name="l358"><span class="ln">358  </span></a>    <span class="s4">&quot;_test_serialization_subcmul&quot;</span><span class="s3">,</span>
<a name="l359"><span class="ln">359  </span></a>    <span class="s4">&quot;_to_cpu&quot;</span><span class="s3">,</span>
<a name="l360"><span class="ln">360  </span></a>    <span class="s4">&quot;_to_functional_tensor&quot;</span><span class="s3">,</span>
<a name="l361"><span class="ln">361  </span></a>    <span class="s4">&quot;_to_sparse_semi_structured&quot;</span><span class="s3">,</span>
<a name="l362"><span class="ln">362  </span></a>    <span class="s4">&quot;_transform_bias_rescale_qkv&quot;</span><span class="s3">,</span>
<a name="l363"><span class="ln">363  </span></a>    <span class="s4">&quot;_transformer_encoder_layer_fwd&quot;</span><span class="s3">,</span>
<a name="l364"><span class="ln">364  </span></a>    <span class="s4">&quot;_trilinear&quot;</span><span class="s3">,</span>
<a name="l365"><span class="ln">365  </span></a>    <span class="s4">&quot;_triton_multi_head_attention&quot;</span><span class="s3">,</span>
<a name="l366"><span class="ln">366  </span></a>    <span class="s4">&quot;_triton_scaled_dot_attention&quot;</span><span class="s3">,</span>
<a name="l367"><span class="ln">367  </span></a>    <span class="s4">&quot;_unique&quot;</span><span class="s3">,</span>
<a name="l368"><span class="ln">368  </span></a>    <span class="s4">&quot;_unique2&quot;</span><span class="s3">,</span>
<a name="l369"><span class="ln">369  </span></a>    <span class="s4">&quot;_unpack_dual&quot;</span><span class="s3">,</span>
<a name="l370"><span class="ln">370  </span></a>    <span class="s4">&quot;_unpack_dual&quot;</span><span class="s3">,</span>
<a name="l371"><span class="ln">371  </span></a>    <span class="s4">&quot;_unsafe_index&quot;</span><span class="s3">,</span>
<a name="l372"><span class="ln">372  </span></a>    <span class="s4">&quot;_unsafe_index_put&quot;</span><span class="s3">,</span>
<a name="l373"><span class="ln">373  </span></a>    <span class="s4">&quot;_unsafe_masked_index&quot;</span><span class="s3">,</span>
<a name="l374"><span class="ln">374  </span></a>    <span class="s4">&quot;_unsafe_masked_index_put_accumulate&quot;</span><span class="s3">,</span>
<a name="l375"><span class="ln">375  </span></a>    <span class="s4">&quot;_use_cudnn_ctc_loss&quot;</span><span class="s3">,</span>
<a name="l376"><span class="ln">376  </span></a>    <span class="s4">&quot;_use_cudnn_rnn_flatten_weight&quot;</span><span class="s3">,</span>
<a name="l377"><span class="ln">377  </span></a>    <span class="s4">&quot;_validate_compressed_sparse_indices&quot;</span><span class="s3">,</span>
<a name="l378"><span class="ln">378  </span></a>    <span class="s4">&quot;_validate_sparse_bsc_tensor_args&quot;</span><span class="s3">,</span>
<a name="l379"><span class="ln">379  </span></a>    <span class="s4">&quot;_validate_sparse_bsr_tensor_args&quot;</span><span class="s3">,</span>
<a name="l380"><span class="ln">380  </span></a>    <span class="s4">&quot;_validate_sparse_compressed_tensor_args&quot;</span><span class="s3">,</span>
<a name="l381"><span class="ln">381  </span></a>    <span class="s4">&quot;_validate_sparse_coo_tensor_args&quot;</span><span class="s3">,</span>
<a name="l382"><span class="ln">382  </span></a>    <span class="s4">&quot;_validate_sparse_csc_tensor_args&quot;</span><span class="s3">,</span>
<a name="l383"><span class="ln">383  </span></a>    <span class="s4">&quot;_validate_sparse_csr_tensor_args&quot;</span><span class="s3">,</span>
<a name="l384"><span class="ln">384  </span></a>    <span class="s4">&quot;_values_copy&quot;</span><span class="s3">,</span>
<a name="l385"><span class="ln">385  </span></a>    <span class="s4">&quot;_weight_int4pack_mm&quot;</span><span class="s3">,</span>
<a name="l386"><span class="ln">386  </span></a>    <span class="s4">&quot;_weight_int4pack_mm_for_cpu&quot;</span><span class="s3">,</span>
<a name="l387"><span class="ln">387  </span></a>    <span class="s4">&quot;_weight_int4pack_mm_with_scales_and_zeros&quot;</span><span class="s3">,</span>
<a name="l388"><span class="ln">388  </span></a>    <span class="s4">&quot;_weight_int8pack_mm&quot;</span><span class="s3">,</span>
<a name="l389"><span class="ln">389  </span></a>    <span class="s4">&quot;_weight_norm&quot;</span><span class="s3">,</span>
<a name="l390"><span class="ln">390  </span></a>    <span class="s4">&quot;_weight_norm_interface&quot;</span><span class="s3">,</span>
<a name="l391"><span class="ln">391  </span></a>    <span class="s4">&quot;_wrapped_linear_prepack&quot;</span><span class="s3">,</span>
<a name="l392"><span class="ln">392  </span></a>    <span class="s4">&quot;_wrapped_quantized_linear_prepacked&quot;</span><span class="s3">,</span>
<a name="l393"><span class="ln">393  </span></a>    <span class="s4">&quot;abs&quot;</span><span class="s3">,</span>
<a name="l394"><span class="ln">394  </span></a>    <span class="s4">&quot;abs_&quot;</span><span class="s3">,</span>
<a name="l395"><span class="ln">395  </span></a>    <span class="s4">&quot;absolute&quot;</span><span class="s3">,</span>
<a name="l396"><span class="ln">396  </span></a>    <span class="s4">&quot;acos&quot;</span><span class="s3">,</span>
<a name="l397"><span class="ln">397  </span></a>    <span class="s4">&quot;acos_&quot;</span><span class="s3">,</span>
<a name="l398"><span class="ln">398  </span></a>    <span class="s4">&quot;acosh&quot;</span><span class="s3">,</span>
<a name="l399"><span class="ln">399  </span></a>    <span class="s4">&quot;acosh_&quot;</span><span class="s3">,</span>
<a name="l400"><span class="ln">400  </span></a>    <span class="s4">&quot;adaptive_avg_pool1d&quot;</span><span class="s3">,</span>
<a name="l401"><span class="ln">401  </span></a>    <span class="s4">&quot;adaptive_max_pool1d&quot;</span><span class="s3">,</span>
<a name="l402"><span class="ln">402  </span></a>    <span class="s4">&quot;add&quot;</span><span class="s3">,</span>
<a name="l403"><span class="ln">403  </span></a>    <span class="s4">&quot;addbmm&quot;</span><span class="s3">,</span>
<a name="l404"><span class="ln">404  </span></a>    <span class="s4">&quot;addcdiv&quot;</span><span class="s3">,</span>
<a name="l405"><span class="ln">405  </span></a>    <span class="s4">&quot;addcmul&quot;</span><span class="s3">,</span>
<a name="l406"><span class="ln">406  </span></a>    <span class="s4">&quot;addmm&quot;</span><span class="s3">,</span>
<a name="l407"><span class="ln">407  </span></a>    <span class="s4">&quot;addmv&quot;</span><span class="s3">,</span>
<a name="l408"><span class="ln">408  </span></a>    <span class="s4">&quot;addmv_&quot;</span><span class="s3">,</span>
<a name="l409"><span class="ln">409  </span></a>    <span class="s4">&quot;addr&quot;</span><span class="s3">,</span>
<a name="l410"><span class="ln">410  </span></a>    <span class="s4">&quot;adjoint&quot;</span><span class="s3">,</span>
<a name="l411"><span class="ln">411  </span></a>    <span class="s4">&quot;affine_grid_generator&quot;</span><span class="s3">,</span>
<a name="l412"><span class="ln">412  </span></a>    <span class="s4">&quot;alias_copy&quot;</span><span class="s3">,</span>
<a name="l413"><span class="ln">413  </span></a>    <span class="s4">&quot;all&quot;</span><span class="s3">,</span>
<a name="l414"><span class="ln">414  </span></a>    <span class="s4">&quot;allclose&quot;</span><span class="s3">,</span>
<a name="l415"><span class="ln">415  </span></a>    <span class="s4">&quot;alpha_dropout&quot;</span><span class="s3">,</span>
<a name="l416"><span class="ln">416  </span></a>    <span class="s4">&quot;alpha_dropout_&quot;</span><span class="s3">,</span>
<a name="l417"><span class="ln">417  </span></a>    <span class="s4">&quot;amax&quot;</span><span class="s3">,</span>
<a name="l418"><span class="ln">418  </span></a>    <span class="s4">&quot;amin&quot;</span><span class="s3">,</span>
<a name="l419"><span class="ln">419  </span></a>    <span class="s4">&quot;aminmax&quot;</span><span class="s3">,</span>
<a name="l420"><span class="ln">420  </span></a>    <span class="s4">&quot;aminmax&quot;</span><span class="s3">,</span>
<a name="l421"><span class="ln">421  </span></a>    <span class="s4">&quot;angle&quot;</span><span class="s3">,</span>
<a name="l422"><span class="ln">422  </span></a>    <span class="s4">&quot;any&quot;</span><span class="s3">,</span>
<a name="l423"><span class="ln">423  </span></a>    <span class="s4">&quot;arange&quot;</span><span class="s3">,</span>
<a name="l424"><span class="ln">424  </span></a>    <span class="s4">&quot;arccos&quot;</span><span class="s3">,</span>
<a name="l425"><span class="ln">425  </span></a>    <span class="s4">&quot;arccos_&quot;</span><span class="s3">,</span>
<a name="l426"><span class="ln">426  </span></a>    <span class="s4">&quot;arccosh&quot;</span><span class="s3">,</span>
<a name="l427"><span class="ln">427  </span></a>    <span class="s4">&quot;arccosh_&quot;</span><span class="s3">,</span>
<a name="l428"><span class="ln">428  </span></a>    <span class="s4">&quot;arcsin&quot;</span><span class="s3">,</span>
<a name="l429"><span class="ln">429  </span></a>    <span class="s4">&quot;arcsin_&quot;</span><span class="s3">,</span>
<a name="l430"><span class="ln">430  </span></a>    <span class="s4">&quot;arcsinh&quot;</span><span class="s3">,</span>
<a name="l431"><span class="ln">431  </span></a>    <span class="s4">&quot;arcsinh_&quot;</span><span class="s3">,</span>
<a name="l432"><span class="ln">432  </span></a>    <span class="s4">&quot;arctan&quot;</span><span class="s3">,</span>
<a name="l433"><span class="ln">433  </span></a>    <span class="s4">&quot;arctan2&quot;</span><span class="s3">,</span>
<a name="l434"><span class="ln">434  </span></a>    <span class="s4">&quot;arctan_&quot;</span><span class="s3">,</span>
<a name="l435"><span class="ln">435  </span></a>    <span class="s4">&quot;arctanh&quot;</span><span class="s3">,</span>
<a name="l436"><span class="ln">436  </span></a>    <span class="s4">&quot;arctanh_&quot;</span><span class="s3">,</span>
<a name="l437"><span class="ln">437  </span></a>    <span class="s4">&quot;argmax&quot;</span><span class="s3">,</span>
<a name="l438"><span class="ln">438  </span></a>    <span class="s4">&quot;argmin&quot;</span><span class="s3">,</span>
<a name="l439"><span class="ln">439  </span></a>    <span class="s4">&quot;argsort&quot;</span><span class="s3">,</span>
<a name="l440"><span class="ln">440  </span></a>    <span class="s4">&quot;argwhere&quot;</span><span class="s3">,</span>
<a name="l441"><span class="ln">441  </span></a>    <span class="s4">&quot;as_strided&quot;</span><span class="s3">,</span>
<a name="l442"><span class="ln">442  </span></a>    <span class="s4">&quot;as_strided_&quot;</span><span class="s3">,</span>
<a name="l443"><span class="ln">443  </span></a>    <span class="s4">&quot;as_strided_copy&quot;</span><span class="s3">,</span>
<a name="l444"><span class="ln">444  </span></a>    <span class="s4">&quot;as_strided_scatter&quot;</span><span class="s3">,</span>
<a name="l445"><span class="ln">445  </span></a>    <span class="s4">&quot;as_tensor&quot;</span><span class="s3">,</span>
<a name="l446"><span class="ln">446  </span></a>    <span class="s4">&quot;asarray&quot;</span><span class="s3">,</span>
<a name="l447"><span class="ln">447  </span></a>    <span class="s4">&quot;asin&quot;</span><span class="s3">,</span>
<a name="l448"><span class="ln">448  </span></a>    <span class="s4">&quot;asin_&quot;</span><span class="s3">,</span>
<a name="l449"><span class="ln">449  </span></a>    <span class="s4">&quot;asinh&quot;</span><span class="s3">,</span>
<a name="l450"><span class="ln">450  </span></a>    <span class="s4">&quot;asinh_&quot;</span><span class="s3">,</span>
<a name="l451"><span class="ln">451  </span></a>    <span class="s4">&quot;atan&quot;</span><span class="s3">,</span>
<a name="l452"><span class="ln">452  </span></a>    <span class="s4">&quot;atan2&quot;</span><span class="s3">,</span>
<a name="l453"><span class="ln">453  </span></a>    <span class="s4">&quot;atan_&quot;</span><span class="s3">,</span>
<a name="l454"><span class="ln">454  </span></a>    <span class="s4">&quot;atanh&quot;</span><span class="s3">,</span>
<a name="l455"><span class="ln">455  </span></a>    <span class="s4">&quot;atanh_&quot;</span><span class="s3">,</span>
<a name="l456"><span class="ln">456  </span></a>    <span class="s4">&quot;avg_pool1d&quot;</span><span class="s3">,</span>
<a name="l457"><span class="ln">457  </span></a>    <span class="s4">&quot;baddbmm&quot;</span><span class="s3">,</span>
<a name="l458"><span class="ln">458  </span></a>    <span class="s4">&quot;bartlett_window&quot;</span><span class="s3">,</span>
<a name="l459"><span class="ln">459  </span></a>    <span class="s4">&quot;batch_norm&quot;</span><span class="s3">,</span>
<a name="l460"><span class="ln">460  </span></a>    <span class="s4">&quot;batch_norm_backward_elemt&quot;</span><span class="s3">,</span>
<a name="l461"><span class="ln">461  </span></a>    <span class="s4">&quot;batch_norm_backward_reduce&quot;</span><span class="s3">,</span>
<a name="l462"><span class="ln">462  </span></a>    <span class="s4">&quot;batch_norm_elemt&quot;</span><span class="s3">,</span>
<a name="l463"><span class="ln">463  </span></a>    <span class="s4">&quot;batch_norm_gather_stats&quot;</span><span class="s3">,</span>
<a name="l464"><span class="ln">464  </span></a>    <span class="s4">&quot;batch_norm_gather_stats_with_counts&quot;</span><span class="s3">,</span>
<a name="l465"><span class="ln">465  </span></a>    <span class="s4">&quot;batch_norm_stats&quot;</span><span class="s3">,</span>
<a name="l466"><span class="ln">466  </span></a>    <span class="s4">&quot;batch_norm_update_stats&quot;</span><span class="s3">,</span>
<a name="l467"><span class="ln">467  </span></a>    <span class="s4">&quot;bernoulli&quot;</span><span class="s3">,</span>
<a name="l468"><span class="ln">468  </span></a>    <span class="s4">&quot;bilinear&quot;</span><span class="s3">,</span>
<a name="l469"><span class="ln">469  </span></a>    <span class="s4">&quot;binary_cross_entropy_with_logits&quot;</span><span class="s3">,</span>
<a name="l470"><span class="ln">470  </span></a>    <span class="s4">&quot;bincount&quot;</span><span class="s3">,</span>
<a name="l471"><span class="ln">471  </span></a>    <span class="s4">&quot;binomial&quot;</span><span class="s3">,</span>
<a name="l472"><span class="ln">472  </span></a>    <span class="s4">&quot;bitwise_and&quot;</span><span class="s3">,</span>
<a name="l473"><span class="ln">473  </span></a>    <span class="s4">&quot;bitwise_left_shift&quot;</span><span class="s3">,</span>
<a name="l474"><span class="ln">474  </span></a>    <span class="s4">&quot;bitwise_not&quot;</span><span class="s3">,</span>
<a name="l475"><span class="ln">475  </span></a>    <span class="s4">&quot;bitwise_or&quot;</span><span class="s3">,</span>
<a name="l476"><span class="ln">476  </span></a>    <span class="s4">&quot;bitwise_right_shift&quot;</span><span class="s3">,</span>
<a name="l477"><span class="ln">477  </span></a>    <span class="s4">&quot;bitwise_xor&quot;</span><span class="s3">,</span>
<a name="l478"><span class="ln">478  </span></a>    <span class="s4">&quot;blackman_window&quot;</span><span class="s3">,</span>
<a name="l479"><span class="ln">479  </span></a>    <span class="s4">&quot;bmm&quot;</span><span class="s3">,</span>
<a name="l480"><span class="ln">480  </span></a>    <span class="s4">&quot;broadcast_to&quot;</span><span class="s3">,</span>
<a name="l481"><span class="ln">481  </span></a>    <span class="s4">&quot;bucketize&quot;</span><span class="s3">,</span>
<a name="l482"><span class="ln">482  </span></a>    <span class="s4">&quot;can_cast&quot;</span><span class="s3">,</span>
<a name="l483"><span class="ln">483  </span></a>    <span class="s4">&quot;cat&quot;</span><span class="s3">,</span>
<a name="l484"><span class="ln">484  </span></a>    <span class="s4">&quot;ccol_indices_copy&quot;</span><span class="s3">,</span>
<a name="l485"><span class="ln">485  </span></a>    <span class="s4">&quot;ceil&quot;</span><span class="s3">,</span>
<a name="l486"><span class="ln">486  </span></a>    <span class="s4">&quot;ceil_&quot;</span><span class="s3">,</span>
<a name="l487"><span class="ln">487  </span></a>    <span class="s4">&quot;celu&quot;</span><span class="s3">,</span>
<a name="l488"><span class="ln">488  </span></a>    <span class="s4">&quot;celu_&quot;</span><span class="s3">,</span>
<a name="l489"><span class="ln">489  </span></a>    <span class="s4">&quot;channel_shuffle&quot;</span><span class="s3">,</span>
<a name="l490"><span class="ln">490  </span></a>    <span class="s4">&quot;cholesky&quot;</span><span class="s3">,</span>
<a name="l491"><span class="ln">491  </span></a>    <span class="s4">&quot;cholesky_inverse&quot;</span><span class="s3">,</span>
<a name="l492"><span class="ln">492  </span></a>    <span class="s4">&quot;cholesky_solve&quot;</span><span class="s3">,</span>
<a name="l493"><span class="ln">493  </span></a>    <span class="s4">&quot;choose_qparams_optimized&quot;</span><span class="s3">,</span>
<a name="l494"><span class="ln">494  </span></a>    <span class="s4">&quot;chunk&quot;</span><span class="s3">,</span>
<a name="l495"><span class="ln">495  </span></a>    <span class="s4">&quot;clamp&quot;</span><span class="s3">,</span>
<a name="l496"><span class="ln">496  </span></a>    <span class="s4">&quot;clamp_&quot;</span><span class="s3">,</span>
<a name="l497"><span class="ln">497  </span></a>    <span class="s4">&quot;clamp_max&quot;</span><span class="s3">,</span>
<a name="l498"><span class="ln">498  </span></a>    <span class="s4">&quot;clamp_max_&quot;</span><span class="s3">,</span>
<a name="l499"><span class="ln">499  </span></a>    <span class="s4">&quot;clamp_min&quot;</span><span class="s3">,</span>
<a name="l500"><span class="ln">500  </span></a>    <span class="s4">&quot;clamp_min_&quot;</span><span class="s3">,</span>
<a name="l501"><span class="ln">501  </span></a>    <span class="s4">&quot;clip&quot;</span><span class="s3">,</span>
<a name="l502"><span class="ln">502  </span></a>    <span class="s4">&quot;clip_&quot;</span><span class="s3">,</span>
<a name="l503"><span class="ln">503  </span></a>    <span class="s4">&quot;clone&quot;</span><span class="s3">,</span>
<a name="l504"><span class="ln">504  </span></a>    <span class="s4">&quot;col_indices_copy&quot;</span><span class="s3">,</span>
<a name="l505"><span class="ln">505  </span></a>    <span class="s4">&quot;column_stack&quot;</span><span class="s3">,</span>
<a name="l506"><span class="ln">506  </span></a>    <span class="s4">&quot;combinations&quot;</span><span class="s3">,</span>
<a name="l507"><span class="ln">507  </span></a>    <span class="s4">&quot;complex&quot;</span><span class="s3">,</span>
<a name="l508"><span class="ln">508  </span></a>    <span class="s4">&quot;concat&quot;</span><span class="s3">,</span>
<a name="l509"><span class="ln">509  </span></a>    <span class="s4">&quot;concatenate&quot;</span><span class="s3">,</span>
<a name="l510"><span class="ln">510  </span></a>    <span class="s4">&quot;conj&quot;</span><span class="s3">,</span>
<a name="l511"><span class="ln">511  </span></a>    <span class="s4">&quot;conj_physical&quot;</span><span class="s3">,</span>
<a name="l512"><span class="ln">512  </span></a>    <span class="s4">&quot;conj_physical_&quot;</span><span class="s3">,</span>
<a name="l513"><span class="ln">513  </span></a>    <span class="s4">&quot;constant_pad_nd&quot;</span><span class="s3">,</span>
<a name="l514"><span class="ln">514  </span></a>    <span class="s4">&quot;conv1d&quot;</span><span class="s3">,</span>
<a name="l515"><span class="ln">515  </span></a>    <span class="s4">&quot;conv2d&quot;</span><span class="s3">,</span>
<a name="l516"><span class="ln">516  </span></a>    <span class="s4">&quot;conv3d&quot;</span><span class="s3">,</span>
<a name="l517"><span class="ln">517  </span></a>    <span class="s4">&quot;conv_tbc&quot;</span><span class="s3">,</span>
<a name="l518"><span class="ln">518  </span></a>    <span class="s4">&quot;conv_transpose1d&quot;</span><span class="s3">,</span>
<a name="l519"><span class="ln">519  </span></a>    <span class="s4">&quot;conv_transpose2d&quot;</span><span class="s3">,</span>
<a name="l520"><span class="ln">520  </span></a>    <span class="s4">&quot;conv_transpose3d&quot;</span><span class="s3">,</span>
<a name="l521"><span class="ln">521  </span></a>    <span class="s4">&quot;convolution&quot;</span><span class="s3">,</span>
<a name="l522"><span class="ln">522  </span></a>    <span class="s4">&quot;copysign&quot;</span><span class="s3">,</span>
<a name="l523"><span class="ln">523  </span></a>    <span class="s4">&quot;corrcoef&quot;</span><span class="s3">,</span>
<a name="l524"><span class="ln">524  </span></a>    <span class="s4">&quot;cos&quot;</span><span class="s3">,</span>
<a name="l525"><span class="ln">525  </span></a>    <span class="s4">&quot;cos_&quot;</span><span class="s3">,</span>
<a name="l526"><span class="ln">526  </span></a>    <span class="s4">&quot;cosh&quot;</span><span class="s3">,</span>
<a name="l527"><span class="ln">527  </span></a>    <span class="s4">&quot;cosh_&quot;</span><span class="s3">,</span>
<a name="l528"><span class="ln">528  </span></a>    <span class="s4">&quot;cosine_embedding_loss&quot;</span><span class="s3">,</span>
<a name="l529"><span class="ln">529  </span></a>    <span class="s4">&quot;cosine_similarity&quot;</span><span class="s3">,</span>
<a name="l530"><span class="ln">530  </span></a>    <span class="s4">&quot;count_nonzero&quot;</span><span class="s3">,</span>
<a name="l531"><span class="ln">531  </span></a>    <span class="s4">&quot;cov&quot;</span><span class="s3">,</span>
<a name="l532"><span class="ln">532  </span></a>    <span class="s4">&quot;cross&quot;</span><span class="s3">,</span>
<a name="l533"><span class="ln">533  </span></a>    <span class="s4">&quot;crow_indices_copy&quot;</span><span class="s3">,</span>
<a name="l534"><span class="ln">534  </span></a>    <span class="s4">&quot;ctc_loss&quot;</span><span class="s3">,</span>
<a name="l535"><span class="ln">535  </span></a>    <span class="s4">&quot;cudnn_affine_grid_generator&quot;</span><span class="s3">,</span>
<a name="l536"><span class="ln">536  </span></a>    <span class="s4">&quot;cudnn_batch_norm&quot;</span><span class="s3">,</span>
<a name="l537"><span class="ln">537  </span></a>    <span class="s4">&quot;cudnn_convolution&quot;</span><span class="s3">,</span>
<a name="l538"><span class="ln">538  </span></a>    <span class="s4">&quot;cudnn_convolution_add_relu&quot;</span><span class="s3">,</span>
<a name="l539"><span class="ln">539  </span></a>    <span class="s4">&quot;cudnn_convolution_relu&quot;</span><span class="s3">,</span>
<a name="l540"><span class="ln">540  </span></a>    <span class="s4">&quot;cudnn_convolution_transpose&quot;</span><span class="s3">,</span>
<a name="l541"><span class="ln">541  </span></a>    <span class="s4">&quot;cudnn_grid_sampler&quot;</span><span class="s3">,</span>
<a name="l542"><span class="ln">542  </span></a>    <span class="s4">&quot;cudnn_is_acceptable&quot;</span><span class="s3">,</span>
<a name="l543"><span class="ln">543  </span></a>    <span class="s4">&quot;cummax&quot;</span><span class="s3">,</span>
<a name="l544"><span class="ln">544  </span></a>    <span class="s4">&quot;cummax&quot;</span><span class="s3">,</span>
<a name="l545"><span class="ln">545  </span></a>    <span class="s4">&quot;cummin&quot;</span><span class="s3">,</span>
<a name="l546"><span class="ln">546  </span></a>    <span class="s4">&quot;cummin&quot;</span><span class="s3">,</span>
<a name="l547"><span class="ln">547  </span></a>    <span class="s4">&quot;cumprod&quot;</span><span class="s3">,</span>
<a name="l548"><span class="ln">548  </span></a>    <span class="s4">&quot;cumsum&quot;</span><span class="s3">,</span>
<a name="l549"><span class="ln">549  </span></a>    <span class="s4">&quot;cumulative_trapezoid&quot;</span><span class="s3">,</span>
<a name="l550"><span class="ln">550  </span></a>    <span class="s4">&quot;deg2rad&quot;</span><span class="s3">,</span>
<a name="l551"><span class="ln">551  </span></a>    <span class="s4">&quot;deg2rad_&quot;</span><span class="s3">,</span>
<a name="l552"><span class="ln">552  </span></a>    <span class="s4">&quot;dequantize&quot;</span><span class="s3">,</span>
<a name="l553"><span class="ln">553  </span></a>    <span class="s4">&quot;det&quot;</span><span class="s3">,</span>
<a name="l554"><span class="ln">554  </span></a>    <span class="s4">&quot;detach&quot;</span><span class="s3">,</span>
<a name="l555"><span class="ln">555  </span></a>    <span class="s4">&quot;detach_&quot;</span><span class="s3">,</span>
<a name="l556"><span class="ln">556  </span></a>    <span class="s4">&quot;detach_copy&quot;</span><span class="s3">,</span>
<a name="l557"><span class="ln">557  </span></a>    <span class="s4">&quot;diag&quot;</span><span class="s3">,</span>
<a name="l558"><span class="ln">558  </span></a>    <span class="s4">&quot;diag_embed&quot;</span><span class="s3">,</span>
<a name="l559"><span class="ln">559  </span></a>    <span class="s4">&quot;diagflat&quot;</span><span class="s3">,</span>
<a name="l560"><span class="ln">560  </span></a>    <span class="s4">&quot;diagonal&quot;</span><span class="s3">,</span>
<a name="l561"><span class="ln">561  </span></a>    <span class="s4">&quot;diagonal_copy&quot;</span><span class="s3">,</span>
<a name="l562"><span class="ln">562  </span></a>    <span class="s4">&quot;diagonal_scatter&quot;</span><span class="s3">,</span>
<a name="l563"><span class="ln">563  </span></a>    <span class="s4">&quot;diff&quot;</span><span class="s3">,</span>
<a name="l564"><span class="ln">564  </span></a>    <span class="s4">&quot;digamma&quot;</span><span class="s3">,</span>
<a name="l565"><span class="ln">565  </span></a>    <span class="s4">&quot;dist&quot;</span><span class="s3">,</span>
<a name="l566"><span class="ln">566  </span></a>    <span class="s4">&quot;div&quot;</span><span class="s3">,</span>
<a name="l567"><span class="ln">567  </span></a>    <span class="s4">&quot;divide&quot;</span><span class="s3">,</span>
<a name="l568"><span class="ln">568  </span></a>    <span class="s4">&quot;dot&quot;</span><span class="s3">,</span>
<a name="l569"><span class="ln">569  </span></a>    <span class="s4">&quot;dropout&quot;</span><span class="s3">,</span>
<a name="l570"><span class="ln">570  </span></a>    <span class="s4">&quot;dropout_&quot;</span><span class="s3">,</span>
<a name="l571"><span class="ln">571  </span></a>    <span class="s4">&quot;dsmm&quot;</span><span class="s3">,</span>
<a name="l572"><span class="ln">572  </span></a>    <span class="s4">&quot;dsplit&quot;</span><span class="s3">,</span>
<a name="l573"><span class="ln">573  </span></a>    <span class="s4">&quot;dstack&quot;</span><span class="s3">,</span>
<a name="l574"><span class="ln">574  </span></a>    <span class="s4">&quot;embedding&quot;</span><span class="s3">,</span>
<a name="l575"><span class="ln">575  </span></a>    <span class="s4">&quot;embedding_bag&quot;</span><span class="s3">,</span>
<a name="l576"><span class="ln">576  </span></a>    <span class="s4">&quot;embedding_renorm_&quot;</span><span class="s3">,</span>
<a name="l577"><span class="ln">577  </span></a>    <span class="s4">&quot;empty&quot;</span><span class="s3">,</span>
<a name="l578"><span class="ln">578  </span></a>    <span class="s4">&quot;empty_like&quot;</span><span class="s3">,</span>
<a name="l579"><span class="ln">579  </span></a>    <span class="s4">&quot;empty_permuted&quot;</span><span class="s3">,</span>
<a name="l580"><span class="ln">580  </span></a>    <span class="s4">&quot;empty_quantized&quot;</span><span class="s3">,</span>
<a name="l581"><span class="ln">581  </span></a>    <span class="s4">&quot;empty_strided&quot;</span><span class="s3">,</span>
<a name="l582"><span class="ln">582  </span></a>    <span class="s4">&quot;eq&quot;</span><span class="s3">,</span>
<a name="l583"><span class="ln">583  </span></a>    <span class="s4">&quot;equal&quot;</span><span class="s3">,</span>
<a name="l584"><span class="ln">584  </span></a>    <span class="s4">&quot;erf&quot;</span><span class="s3">,</span>
<a name="l585"><span class="ln">585  </span></a>    <span class="s4">&quot;erf_&quot;</span><span class="s3">,</span>
<a name="l586"><span class="ln">586  </span></a>    <span class="s4">&quot;erfc&quot;</span><span class="s3">,</span>
<a name="l587"><span class="ln">587  </span></a>    <span class="s4">&quot;erfc_&quot;</span><span class="s3">,</span>
<a name="l588"><span class="ln">588  </span></a>    <span class="s4">&quot;erfinv&quot;</span><span class="s3">,</span>
<a name="l589"><span class="ln">589  </span></a>    <span class="s4">&quot;exp&quot;</span><span class="s3">,</span>
<a name="l590"><span class="ln">590  </span></a>    <span class="s4">&quot;exp2&quot;</span><span class="s3">,</span>
<a name="l591"><span class="ln">591  </span></a>    <span class="s4">&quot;exp2_&quot;</span><span class="s3">,</span>
<a name="l592"><span class="ln">592  </span></a>    <span class="s4">&quot;exp_&quot;</span><span class="s3">,</span>
<a name="l593"><span class="ln">593  </span></a>    <span class="s4">&quot;expand_copy&quot;</span><span class="s3">,</span>
<a name="l594"><span class="ln">594  </span></a>    <span class="s4">&quot;expm1&quot;</span><span class="s3">,</span>
<a name="l595"><span class="ln">595  </span></a>    <span class="s4">&quot;expm1_&quot;</span><span class="s3">,</span>
<a name="l596"><span class="ln">596  </span></a>    <span class="s4">&quot;eye&quot;</span><span class="s3">,</span>
<a name="l597"><span class="ln">597  </span></a>    <span class="s4">&quot;fake_quantize_per_channel_affine&quot;</span><span class="s3">,</span>
<a name="l598"><span class="ln">598  </span></a>    <span class="s4">&quot;fake_quantize_per_tensor_affine&quot;</span><span class="s3">,</span>
<a name="l599"><span class="ln">599  </span></a>    <span class="s4">&quot;fbgemm_linear_fp16_weight&quot;</span><span class="s3">,</span>
<a name="l600"><span class="ln">600  </span></a>    <span class="s4">&quot;fbgemm_linear_fp16_weight_fp32_activation&quot;</span><span class="s3">,</span>
<a name="l601"><span class="ln">601  </span></a>    <span class="s4">&quot;fbgemm_linear_int8_weight&quot;</span><span class="s3">,</span>
<a name="l602"><span class="ln">602  </span></a>    <span class="s4">&quot;fbgemm_linear_int8_weight_fp32_activation&quot;</span><span class="s3">,</span>
<a name="l603"><span class="ln">603  </span></a>    <span class="s4">&quot;fbgemm_linear_quantize_weight&quot;</span><span class="s3">,</span>
<a name="l604"><span class="ln">604  </span></a>    <span class="s4">&quot;fbgemm_pack_gemm_matrix_fp16&quot;</span><span class="s3">,</span>
<a name="l605"><span class="ln">605  </span></a>    <span class="s4">&quot;fbgemm_pack_quantized_matrix&quot;</span><span class="s3">,</span>
<a name="l606"><span class="ln">606  </span></a>    <span class="s4">&quot;feature_alpha_dropout&quot;</span><span class="s3">,</span>
<a name="l607"><span class="ln">607  </span></a>    <span class="s4">&quot;feature_alpha_dropout_&quot;</span><span class="s3">,</span>
<a name="l608"><span class="ln">608  </span></a>    <span class="s4">&quot;feature_dropout&quot;</span><span class="s3">,</span>
<a name="l609"><span class="ln">609  </span></a>    <span class="s4">&quot;feature_dropout_&quot;</span><span class="s3">,</span>
<a name="l610"><span class="ln">610  </span></a>    <span class="s4">&quot;fill&quot;</span><span class="s3">,</span>
<a name="l611"><span class="ln">611  </span></a>    <span class="s4">&quot;fill_&quot;</span><span class="s3">,</span>
<a name="l612"><span class="ln">612  </span></a>    <span class="s4">&quot;fix&quot;</span><span class="s3">,</span>
<a name="l613"><span class="ln">613  </span></a>    <span class="s4">&quot;fix_&quot;</span><span class="s3">,</span>
<a name="l614"><span class="ln">614  </span></a>    <span class="s4">&quot;flatten&quot;</span><span class="s3">,</span>
<a name="l615"><span class="ln">615  </span></a>    <span class="s4">&quot;flip&quot;</span><span class="s3">,</span>
<a name="l616"><span class="ln">616  </span></a>    <span class="s4">&quot;fliplr&quot;</span><span class="s3">,</span>
<a name="l617"><span class="ln">617  </span></a>    <span class="s4">&quot;flipud&quot;</span><span class="s3">,</span>
<a name="l618"><span class="ln">618  </span></a>    <span class="s4">&quot;float_power&quot;</span><span class="s3">,</span>
<a name="l619"><span class="ln">619  </span></a>    <span class="s4">&quot;floor&quot;</span><span class="s3">,</span>
<a name="l620"><span class="ln">620  </span></a>    <span class="s4">&quot;floor_&quot;</span><span class="s3">,</span>
<a name="l621"><span class="ln">621  </span></a>    <span class="s4">&quot;floor_divide&quot;</span><span class="s3">,</span>
<a name="l622"><span class="ln">622  </span></a>    <span class="s4">&quot;fmax&quot;</span><span class="s3">,</span>
<a name="l623"><span class="ln">623  </span></a>    <span class="s4">&quot;fmin&quot;</span><span class="s3">,</span>
<a name="l624"><span class="ln">624  </span></a>    <span class="s4">&quot;fmod&quot;</span><span class="s3">,</span>
<a name="l625"><span class="ln">625  </span></a>    <span class="s4">&quot;frac&quot;</span><span class="s3">,</span>
<a name="l626"><span class="ln">626  </span></a>    <span class="s4">&quot;frac_&quot;</span><span class="s3">,</span>
<a name="l627"><span class="ln">627  </span></a>    <span class="s4">&quot;frexp&quot;</span><span class="s3">,</span>
<a name="l628"><span class="ln">628  </span></a>    <span class="s4">&quot;frexp&quot;</span><span class="s3">,</span>
<a name="l629"><span class="ln">629  </span></a>    <span class="s4">&quot;frobenius_norm&quot;</span><span class="s3">,</span>
<a name="l630"><span class="ln">630  </span></a>    <span class="s4">&quot;from_file&quot;</span><span class="s3">,</span>
<a name="l631"><span class="ln">631  </span></a>    <span class="s4">&quot;from_numpy&quot;</span><span class="s3">,</span>
<a name="l632"><span class="ln">632  </span></a>    <span class="s4">&quot;frombuffer&quot;</span><span class="s3">,</span>
<a name="l633"><span class="ln">633  </span></a>    <span class="s4">&quot;full&quot;</span><span class="s3">,</span>
<a name="l634"><span class="ln">634  </span></a>    <span class="s4">&quot;full_like&quot;</span><span class="s3">,</span>
<a name="l635"><span class="ln">635  </span></a>    <span class="s4">&quot;fused_moving_avg_obs_fake_quant&quot;</span><span class="s3">,</span>
<a name="l636"><span class="ln">636  </span></a>    <span class="s4">&quot;gather&quot;</span><span class="s3">,</span>
<a name="l637"><span class="ln">637  </span></a>    <span class="s4">&quot;gcd&quot;</span><span class="s3">,</span>
<a name="l638"><span class="ln">638  </span></a>    <span class="s4">&quot;gcd_&quot;</span><span class="s3">,</span>
<a name="l639"><span class="ln">639  </span></a>    <span class="s4">&quot;ge&quot;</span><span class="s3">,</span>
<a name="l640"><span class="ln">640  </span></a>    <span class="s4">&quot;geqrf&quot;</span><span class="s3">,</span>
<a name="l641"><span class="ln">641  </span></a>    <span class="s4">&quot;geqrf&quot;</span><span class="s3">,</span>
<a name="l642"><span class="ln">642  </span></a>    <span class="s4">&quot;ger&quot;</span><span class="s3">,</span>
<a name="l643"><span class="ln">643  </span></a>    <span class="s4">&quot;get_default_dtype&quot;</span><span class="s3">,</span>
<a name="l644"><span class="ln">644  </span></a>    <span class="s4">&quot;get_num_interop_threads&quot;</span><span class="s3">,</span>
<a name="l645"><span class="ln">645  </span></a>    <span class="s4">&quot;get_num_threads&quot;</span><span class="s3">,</span>
<a name="l646"><span class="ln">646  </span></a>    <span class="s4">&quot;gradient&quot;</span><span class="s3">,</span>
<a name="l647"><span class="ln">647  </span></a>    <span class="s4">&quot;greater&quot;</span><span class="s3">,</span>
<a name="l648"><span class="ln">648  </span></a>    <span class="s4">&quot;greater_equal&quot;</span><span class="s3">,</span>
<a name="l649"><span class="ln">649  </span></a>    <span class="s4">&quot;grid_sampler&quot;</span><span class="s3">,</span>
<a name="l650"><span class="ln">650  </span></a>    <span class="s4">&quot;grid_sampler_2d&quot;</span><span class="s3">,</span>
<a name="l651"><span class="ln">651  </span></a>    <span class="s4">&quot;grid_sampler_3d&quot;</span><span class="s3">,</span>
<a name="l652"><span class="ln">652  </span></a>    <span class="s4">&quot;group_norm&quot;</span><span class="s3">,</span>
<a name="l653"><span class="ln">653  </span></a>    <span class="s4">&quot;gru&quot;</span><span class="s3">,</span>
<a name="l654"><span class="ln">654  </span></a>    <span class="s4">&quot;gru_cell&quot;</span><span class="s3">,</span>
<a name="l655"><span class="ln">655  </span></a>    <span class="s4">&quot;gt&quot;</span><span class="s3">,</span>
<a name="l656"><span class="ln">656  </span></a>    <span class="s4">&quot;hamming_window&quot;</span><span class="s3">,</span>
<a name="l657"><span class="ln">657  </span></a>    <span class="s4">&quot;hann_window&quot;</span><span class="s3">,</span>
<a name="l658"><span class="ln">658  </span></a>    <span class="s4">&quot;hardshrink&quot;</span><span class="s3">,</span>
<a name="l659"><span class="ln">659  </span></a>    <span class="s4">&quot;heaviside&quot;</span><span class="s3">,</span>
<a name="l660"><span class="ln">660  </span></a>    <span class="s4">&quot;hinge_embedding_loss&quot;</span><span class="s3">,</span>
<a name="l661"><span class="ln">661  </span></a>    <span class="s4">&quot;histc&quot;</span><span class="s3">,</span>
<a name="l662"><span class="ln">662  </span></a>    <span class="s4">&quot;histogram&quot;</span><span class="s3">,</span>
<a name="l663"><span class="ln">663  </span></a>    <span class="s4">&quot;histogram&quot;</span><span class="s3">,</span>
<a name="l664"><span class="ln">664  </span></a>    <span class="s4">&quot;histogramdd&quot;</span><span class="s3">,</span>
<a name="l665"><span class="ln">665  </span></a>    <span class="s4">&quot;histogramdd&quot;</span><span class="s3">,</span>
<a name="l666"><span class="ln">666  </span></a>    <span class="s4">&quot;hsmm&quot;</span><span class="s3">,</span>
<a name="l667"><span class="ln">667  </span></a>    <span class="s4">&quot;hsplit&quot;</span><span class="s3">,</span>
<a name="l668"><span class="ln">668  </span></a>    <span class="s4">&quot;hspmm&quot;</span><span class="s3">,</span>
<a name="l669"><span class="ln">669  </span></a>    <span class="s4">&quot;hstack&quot;</span><span class="s3">,</span>
<a name="l670"><span class="ln">670  </span></a>    <span class="s4">&quot;hypot&quot;</span><span class="s3">,</span>
<a name="l671"><span class="ln">671  </span></a>    <span class="s4">&quot;i0&quot;</span><span class="s3">,</span>
<a name="l672"><span class="ln">672  </span></a>    <span class="s4">&quot;i0_&quot;</span><span class="s3">,</span>
<a name="l673"><span class="ln">673  </span></a>    <span class="s4">&quot;igamma&quot;</span><span class="s3">,</span>
<a name="l674"><span class="ln">674  </span></a>    <span class="s4">&quot;igammac&quot;</span><span class="s3">,</span>
<a name="l675"><span class="ln">675  </span></a>    <span class="s4">&quot;imag&quot;</span><span class="s3">,</span>
<a name="l676"><span class="ln">676  </span></a>    <span class="s4">&quot;index_add&quot;</span><span class="s3">,</span>
<a name="l677"><span class="ln">677  </span></a>    <span class="s4">&quot;index_copy&quot;</span><span class="s3">,</span>
<a name="l678"><span class="ln">678  </span></a>    <span class="s4">&quot;index_fill&quot;</span><span class="s3">,</span>
<a name="l679"><span class="ln">679  </span></a>    <span class="s4">&quot;index_put&quot;</span><span class="s3">,</span>
<a name="l680"><span class="ln">680  </span></a>    <span class="s4">&quot;index_put_&quot;</span><span class="s3">,</span>
<a name="l681"><span class="ln">681  </span></a>    <span class="s4">&quot;index_reduce&quot;</span><span class="s3">,</span>
<a name="l682"><span class="ln">682  </span></a>    <span class="s4">&quot;index_select&quot;</span><span class="s3">,</span>
<a name="l683"><span class="ln">683  </span></a>    <span class="s4">&quot;indices_copy&quot;</span><span class="s3">,</span>
<a name="l684"><span class="ln">684  </span></a>    <span class="s4">&quot;init_num_threads&quot;</span><span class="s3">,</span>
<a name="l685"><span class="ln">685  </span></a>    <span class="s4">&quot;inner&quot;</span><span class="s3">,</span>
<a name="l686"><span class="ln">686  </span></a>    <span class="s4">&quot;instance_norm&quot;</span><span class="s3">,</span>
<a name="l687"><span class="ln">687  </span></a>    <span class="s4">&quot;int_repr&quot;</span><span class="s3">,</span>
<a name="l688"><span class="ln">688  </span></a>    <span class="s4">&quot;inverse&quot;</span><span class="s3">,</span>
<a name="l689"><span class="ln">689  </span></a>    <span class="s4">&quot;is_complex&quot;</span><span class="s3">,</span>
<a name="l690"><span class="ln">690  </span></a>    <span class="s4">&quot;is_conj&quot;</span><span class="s3">,</span>
<a name="l691"><span class="ln">691  </span></a>    <span class="s4">&quot;is_distributed&quot;</span><span class="s3">,</span>
<a name="l692"><span class="ln">692  </span></a>    <span class="s4">&quot;is_floating_point&quot;</span><span class="s3">,</span>
<a name="l693"><span class="ln">693  </span></a>    <span class="s4">&quot;is_grad_enabled&quot;</span><span class="s3">,</span>
<a name="l694"><span class="ln">694  </span></a>    <span class="s4">&quot;is_inference&quot;</span><span class="s3">,</span>
<a name="l695"><span class="ln">695  </span></a>    <span class="s4">&quot;is_inference_mode_enabled&quot;</span><span class="s3">,</span>
<a name="l696"><span class="ln">696  </span></a>    <span class="s4">&quot;is_neg&quot;</span><span class="s3">,</span>
<a name="l697"><span class="ln">697  </span></a>    <span class="s4">&quot;is_nonzero&quot;</span><span class="s3">,</span>
<a name="l698"><span class="ln">698  </span></a>    <span class="s4">&quot;is_same_size&quot;</span><span class="s3">,</span>
<a name="l699"><span class="ln">699  </span></a>    <span class="s4">&quot;is_signed&quot;</span><span class="s3">,</span>
<a name="l700"><span class="ln">700  </span></a>    <span class="s4">&quot;is_vulkan_available&quot;</span><span class="s3">,</span>
<a name="l701"><span class="ln">701  </span></a>    <span class="s4">&quot;isclose&quot;</span><span class="s3">,</span>
<a name="l702"><span class="ln">702  </span></a>    <span class="s4">&quot;isfinite&quot;</span><span class="s3">,</span>
<a name="l703"><span class="ln">703  </span></a>    <span class="s4">&quot;isin&quot;</span><span class="s3">,</span>
<a name="l704"><span class="ln">704  </span></a>    <span class="s4">&quot;isinf&quot;</span><span class="s3">,</span>
<a name="l705"><span class="ln">705  </span></a>    <span class="s4">&quot;isnan&quot;</span><span class="s3">,</span>
<a name="l706"><span class="ln">706  </span></a>    <span class="s4">&quot;isneginf&quot;</span><span class="s3">,</span>
<a name="l707"><span class="ln">707  </span></a>    <span class="s4">&quot;isposinf&quot;</span><span class="s3">,</span>
<a name="l708"><span class="ln">708  </span></a>    <span class="s4">&quot;isreal&quot;</span><span class="s3">,</span>
<a name="l709"><span class="ln">709  </span></a>    <span class="s4">&quot;istft&quot;</span><span class="s3">,</span>
<a name="l710"><span class="ln">710  </span></a>    <span class="s4">&quot;kaiser_window&quot;</span><span class="s3">,</span>
<a name="l711"><span class="ln">711  </span></a>    <span class="s4">&quot;kl_div&quot;</span><span class="s3">,</span>
<a name="l712"><span class="ln">712  </span></a>    <span class="s4">&quot;kron&quot;</span><span class="s3">,</span>
<a name="l713"><span class="ln">713  </span></a>    <span class="s4">&quot;kthvalue&quot;</span><span class="s3">,</span>
<a name="l714"><span class="ln">714  </span></a>    <span class="s4">&quot;kthvalue&quot;</span><span class="s3">,</span>
<a name="l715"><span class="ln">715  </span></a>    <span class="s4">&quot;layer_norm&quot;</span><span class="s3">,</span>
<a name="l716"><span class="ln">716  </span></a>    <span class="s4">&quot;lcm&quot;</span><span class="s3">,</span>
<a name="l717"><span class="ln">717  </span></a>    <span class="s4">&quot;lcm_&quot;</span><span class="s3">,</span>
<a name="l718"><span class="ln">718  </span></a>    <span class="s4">&quot;ldexp&quot;</span><span class="s3">,</span>
<a name="l719"><span class="ln">719  </span></a>    <span class="s4">&quot;ldexp_&quot;</span><span class="s3">,</span>
<a name="l720"><span class="ln">720  </span></a>    <span class="s4">&quot;le&quot;</span><span class="s3">,</span>
<a name="l721"><span class="ln">721  </span></a>    <span class="s4">&quot;lerp&quot;</span><span class="s3">,</span>
<a name="l722"><span class="ln">722  </span></a>    <span class="s4">&quot;less&quot;</span><span class="s3">,</span>
<a name="l723"><span class="ln">723  </span></a>    <span class="s4">&quot;less_equal&quot;</span><span class="s3">,</span>
<a name="l724"><span class="ln">724  </span></a>    <span class="s4">&quot;lgamma&quot;</span><span class="s3">,</span>
<a name="l725"><span class="ln">725  </span></a>    <span class="s4">&quot;linspace&quot;</span><span class="s3">,</span>
<a name="l726"><span class="ln">726  </span></a>    <span class="s4">&quot;log&quot;</span><span class="s3">,</span>
<a name="l727"><span class="ln">727  </span></a>    <span class="s4">&quot;log10&quot;</span><span class="s3">,</span>
<a name="l728"><span class="ln">728  </span></a>    <span class="s4">&quot;log10_&quot;</span><span class="s3">,</span>
<a name="l729"><span class="ln">729  </span></a>    <span class="s4">&quot;log1p&quot;</span><span class="s3">,</span>
<a name="l730"><span class="ln">730  </span></a>    <span class="s4">&quot;log1p_&quot;</span><span class="s3">,</span>
<a name="l731"><span class="ln">731  </span></a>    <span class="s4">&quot;log2&quot;</span><span class="s3">,</span>
<a name="l732"><span class="ln">732  </span></a>    <span class="s4">&quot;log2_&quot;</span><span class="s3">,</span>
<a name="l733"><span class="ln">733  </span></a>    <span class="s4">&quot;log_&quot;</span><span class="s3">,</span>
<a name="l734"><span class="ln">734  </span></a>    <span class="s4">&quot;log_softmax&quot;</span><span class="s3">,</span>
<a name="l735"><span class="ln">735  </span></a>    <span class="s4">&quot;logaddexp&quot;</span><span class="s3">,</span>
<a name="l736"><span class="ln">736  </span></a>    <span class="s4">&quot;logaddexp2&quot;</span><span class="s3">,</span>
<a name="l737"><span class="ln">737  </span></a>    <span class="s4">&quot;logcumsumexp&quot;</span><span class="s3">,</span>
<a name="l738"><span class="ln">738  </span></a>    <span class="s4">&quot;logdet&quot;</span><span class="s3">,</span>
<a name="l739"><span class="ln">739  </span></a>    <span class="s4">&quot;logical_and&quot;</span><span class="s3">,</span>
<a name="l740"><span class="ln">740  </span></a>    <span class="s4">&quot;logical_not&quot;</span><span class="s3">,</span>
<a name="l741"><span class="ln">741  </span></a>    <span class="s4">&quot;logical_or&quot;</span><span class="s3">,</span>
<a name="l742"><span class="ln">742  </span></a>    <span class="s4">&quot;logical_xor&quot;</span><span class="s3">,</span>
<a name="l743"><span class="ln">743  </span></a>    <span class="s4">&quot;logit&quot;</span><span class="s3">,</span>
<a name="l744"><span class="ln">744  </span></a>    <span class="s4">&quot;logit_&quot;</span><span class="s3">,</span>
<a name="l745"><span class="ln">745  </span></a>    <span class="s4">&quot;logspace&quot;</span><span class="s3">,</span>
<a name="l746"><span class="ln">746  </span></a>    <span class="s4">&quot;logsumexp&quot;</span><span class="s3">,</span>
<a name="l747"><span class="ln">747  </span></a>    <span class="s4">&quot;lstm&quot;</span><span class="s3">,</span>
<a name="l748"><span class="ln">748  </span></a>    <span class="s4">&quot;lstm_cell&quot;</span><span class="s3">,</span>
<a name="l749"><span class="ln">749  </span></a>    <span class="s4">&quot;lt&quot;</span><span class="s3">,</span>
<a name="l750"><span class="ln">750  </span></a>    <span class="s4">&quot;lu_solve&quot;</span><span class="s3">,</span>
<a name="l751"><span class="ln">751  </span></a>    <span class="s4">&quot;lu_unpack&quot;</span><span class="s3">,</span>
<a name="l752"><span class="ln">752  </span></a>    <span class="s4">&quot;lu_unpack&quot;</span><span class="s3">,</span>
<a name="l753"><span class="ln">753  </span></a>    <span class="s4">&quot;margin_ranking_loss&quot;</span><span class="s3">,</span>
<a name="l754"><span class="ln">754  </span></a>    <span class="s4">&quot;masked_fill&quot;</span><span class="s3">,</span>
<a name="l755"><span class="ln">755  </span></a>    <span class="s4">&quot;masked_scatter&quot;</span><span class="s3">,</span>
<a name="l756"><span class="ln">756  </span></a>    <span class="s4">&quot;masked_select&quot;</span><span class="s3">,</span>
<a name="l757"><span class="ln">757  </span></a>    <span class="s4">&quot;matmul&quot;</span><span class="s3">,</span>
<a name="l758"><span class="ln">758  </span></a>    <span class="s4">&quot;matrix_exp&quot;</span><span class="s3">,</span>
<a name="l759"><span class="ln">759  </span></a>    <span class="s4">&quot;matrix_power&quot;</span><span class="s3">,</span>
<a name="l760"><span class="ln">760  </span></a>    <span class="s4">&quot;max&quot;</span><span class="s3">,</span>
<a name="l761"><span class="ln">761  </span></a>    <span class="s4">&quot;max&quot;</span><span class="s3">,</span>
<a name="l762"><span class="ln">762  </span></a>    <span class="s4">&quot;max_pool1d&quot;</span><span class="s3">,</span>
<a name="l763"><span class="ln">763  </span></a>    <span class="s4">&quot;max_pool1d_with_indices&quot;</span><span class="s3">,</span>
<a name="l764"><span class="ln">764  </span></a>    <span class="s4">&quot;max_pool2d&quot;</span><span class="s3">,</span>
<a name="l765"><span class="ln">765  </span></a>    <span class="s4">&quot;max_pool3d&quot;</span><span class="s3">,</span>
<a name="l766"><span class="ln">766  </span></a>    <span class="s4">&quot;maximum&quot;</span><span class="s3">,</span>
<a name="l767"><span class="ln">767  </span></a>    <span class="s4">&quot;mean&quot;</span><span class="s3">,</span>
<a name="l768"><span class="ln">768  </span></a>    <span class="s4">&quot;median&quot;</span><span class="s3">,</span>
<a name="l769"><span class="ln">769  </span></a>    <span class="s4">&quot;median&quot;</span><span class="s3">,</span>
<a name="l770"><span class="ln">770  </span></a>    <span class="s4">&quot;min&quot;</span><span class="s3">,</span>
<a name="l771"><span class="ln">771  </span></a>    <span class="s4">&quot;min&quot;</span><span class="s3">,</span>
<a name="l772"><span class="ln">772  </span></a>    <span class="s4">&quot;minimum&quot;</span><span class="s3">,</span>
<a name="l773"><span class="ln">773  </span></a>    <span class="s4">&quot;miopen_batch_norm&quot;</span><span class="s3">,</span>
<a name="l774"><span class="ln">774  </span></a>    <span class="s4">&quot;miopen_convolution&quot;</span><span class="s3">,</span>
<a name="l775"><span class="ln">775  </span></a>    <span class="s4">&quot;miopen_convolution_add_relu&quot;</span><span class="s3">,</span>
<a name="l776"><span class="ln">776  </span></a>    <span class="s4">&quot;miopen_convolution_relu&quot;</span><span class="s3">,</span>
<a name="l777"><span class="ln">777  </span></a>    <span class="s4">&quot;miopen_convolution_transpose&quot;</span><span class="s3">,</span>
<a name="l778"><span class="ln">778  </span></a>    <span class="s4">&quot;miopen_depthwise_convolution&quot;</span><span class="s3">,</span>
<a name="l779"><span class="ln">779  </span></a>    <span class="s4">&quot;miopen_rnn&quot;</span><span class="s3">,</span>
<a name="l780"><span class="ln">780  </span></a>    <span class="s4">&quot;mkldnn_adaptive_avg_pool2d&quot;</span><span class="s3">,</span>
<a name="l781"><span class="ln">781  </span></a>    <span class="s4">&quot;mkldnn_convolution&quot;</span><span class="s3">,</span>
<a name="l782"><span class="ln">782  </span></a>    <span class="s4">&quot;mkldnn_linear_backward_weights&quot;</span><span class="s3">,</span>
<a name="l783"><span class="ln">783  </span></a>    <span class="s4">&quot;mkldnn_max_pool2d&quot;</span><span class="s3">,</span>
<a name="l784"><span class="ln">784  </span></a>    <span class="s4">&quot;mkldnn_max_pool3d&quot;</span><span class="s3">,</span>
<a name="l785"><span class="ln">785  </span></a>    <span class="s4">&quot;mkldnn_rnn_layer&quot;</span><span class="s3">,</span>
<a name="l786"><span class="ln">786  </span></a>    <span class="s4">&quot;mm&quot;</span><span class="s3">,</span>
<a name="l787"><span class="ln">787  </span></a>    <span class="s4">&quot;mode&quot;</span><span class="s3">,</span>
<a name="l788"><span class="ln">788  </span></a>    <span class="s4">&quot;mode&quot;</span><span class="s3">,</span>
<a name="l789"><span class="ln">789  </span></a>    <span class="s4">&quot;moveaxis&quot;</span><span class="s3">,</span>
<a name="l790"><span class="ln">790  </span></a>    <span class="s4">&quot;movedim&quot;</span><span class="s3">,</span>
<a name="l791"><span class="ln">791  </span></a>    <span class="s4">&quot;msort&quot;</span><span class="s3">,</span>
<a name="l792"><span class="ln">792  </span></a>    <span class="s4">&quot;mul&quot;</span><span class="s3">,</span>
<a name="l793"><span class="ln">793  </span></a>    <span class="s4">&quot;multinomial&quot;</span><span class="s3">,</span>
<a name="l794"><span class="ln">794  </span></a>    <span class="s4">&quot;multiply&quot;</span><span class="s3">,</span>
<a name="l795"><span class="ln">795  </span></a>    <span class="s4">&quot;mv&quot;</span><span class="s3">,</span>
<a name="l796"><span class="ln">796  </span></a>    <span class="s4">&quot;mvlgamma&quot;</span><span class="s3">,</span>
<a name="l797"><span class="ln">797  </span></a>    <span class="s4">&quot;nan_to_num&quot;</span><span class="s3">,</span>
<a name="l798"><span class="ln">798  </span></a>    <span class="s4">&quot;nan_to_num_&quot;</span><span class="s3">,</span>
<a name="l799"><span class="ln">799  </span></a>    <span class="s4">&quot;nanmean&quot;</span><span class="s3">,</span>
<a name="l800"><span class="ln">800  </span></a>    <span class="s4">&quot;nanmedian&quot;</span><span class="s3">,</span>
<a name="l801"><span class="ln">801  </span></a>    <span class="s4">&quot;nanmedian&quot;</span><span class="s3">,</span>
<a name="l802"><span class="ln">802  </span></a>    <span class="s4">&quot;nanquantile&quot;</span><span class="s3">,</span>
<a name="l803"><span class="ln">803  </span></a>    <span class="s4">&quot;nansum&quot;</span><span class="s3">,</span>
<a name="l804"><span class="ln">804  </span></a>    <span class="s4">&quot;narrow&quot;</span><span class="s3">,</span>
<a name="l805"><span class="ln">805  </span></a>    <span class="s4">&quot;narrow_copy&quot;</span><span class="s3">,</span>
<a name="l806"><span class="ln">806  </span></a>    <span class="s4">&quot;native_batch_norm&quot;</span><span class="s3">,</span>
<a name="l807"><span class="ln">807  </span></a>    <span class="s4">&quot;native_channel_shuffle&quot;</span><span class="s3">,</span>
<a name="l808"><span class="ln">808  </span></a>    <span class="s4">&quot;native_dropout&quot;</span><span class="s3">,</span>
<a name="l809"><span class="ln">809  </span></a>    <span class="s4">&quot;native_group_norm&quot;</span><span class="s3">,</span>
<a name="l810"><span class="ln">810  </span></a>    <span class="s4">&quot;native_layer_norm&quot;</span><span class="s3">,</span>
<a name="l811"><span class="ln">811  </span></a>    <span class="s4">&quot;native_norm&quot;</span><span class="s3">,</span>
<a name="l812"><span class="ln">812  </span></a>    <span class="s4">&quot;ne&quot;</span><span class="s3">,</span>
<a name="l813"><span class="ln">813  </span></a>    <span class="s4">&quot;neg&quot;</span><span class="s3">,</span>
<a name="l814"><span class="ln">814  </span></a>    <span class="s4">&quot;neg_&quot;</span><span class="s3">,</span>
<a name="l815"><span class="ln">815  </span></a>    <span class="s4">&quot;negative&quot;</span><span class="s3">,</span>
<a name="l816"><span class="ln">816  </span></a>    <span class="s4">&quot;negative_&quot;</span><span class="s3">,</span>
<a name="l817"><span class="ln">817  </span></a>    <span class="s4">&quot;nextafter&quot;</span><span class="s3">,</span>
<a name="l818"><span class="ln">818  </span></a>    <span class="s4">&quot;nonzero&quot;</span><span class="s3">,</span>
<a name="l819"><span class="ln">819  </span></a>    <span class="s4">&quot;nonzero_static&quot;</span><span class="s3">,</span>
<a name="l820"><span class="ln">820  </span></a>    <span class="s4">&quot;norm_except_dim&quot;</span><span class="s3">,</span>
<a name="l821"><span class="ln">821  </span></a>    <span class="s4">&quot;normal&quot;</span><span class="s3">,</span>
<a name="l822"><span class="ln">822  </span></a>    <span class="s4">&quot;not_equal&quot;</span><span class="s3">,</span>
<a name="l823"><span class="ln">823  </span></a>    <span class="s4">&quot;nuclear_norm&quot;</span><span class="s3">,</span>
<a name="l824"><span class="ln">824  </span></a>    <span class="s4">&quot;numel&quot;</span><span class="s3">,</span>
<a name="l825"><span class="ln">825  </span></a>    <span class="s4">&quot;ones&quot;</span><span class="s3">,</span>
<a name="l826"><span class="ln">826  </span></a>    <span class="s4">&quot;ones_like&quot;</span><span class="s3">,</span>
<a name="l827"><span class="ln">827  </span></a>    <span class="s4">&quot;orgqr&quot;</span><span class="s3">,</span>
<a name="l828"><span class="ln">828  </span></a>    <span class="s4">&quot;ormqr&quot;</span><span class="s3">,</span>
<a name="l829"><span class="ln">829  </span></a>    <span class="s4">&quot;outer&quot;</span><span class="s3">,</span>
<a name="l830"><span class="ln">830  </span></a>    <span class="s4">&quot;pairwise_distance&quot;</span><span class="s3">,</span>
<a name="l831"><span class="ln">831  </span></a>    <span class="s4">&quot;pdist&quot;</span><span class="s3">,</span>
<a name="l832"><span class="ln">832  </span></a>    <span class="s4">&quot;permute&quot;</span><span class="s3">,</span>
<a name="l833"><span class="ln">833  </span></a>    <span class="s4">&quot;permute_copy&quot;</span><span class="s3">,</span>
<a name="l834"><span class="ln">834  </span></a>    <span class="s4">&quot;pinverse&quot;</span><span class="s3">,</span>
<a name="l835"><span class="ln">835  </span></a>    <span class="s4">&quot;pixel_shuffle&quot;</span><span class="s3">,</span>
<a name="l836"><span class="ln">836  </span></a>    <span class="s4">&quot;pixel_unshuffle&quot;</span><span class="s3">,</span>
<a name="l837"><span class="ln">837  </span></a>    <span class="s4">&quot;poisson&quot;</span><span class="s3">,</span>
<a name="l838"><span class="ln">838  </span></a>    <span class="s4">&quot;poisson_nll_loss&quot;</span><span class="s3">,</span>
<a name="l839"><span class="ln">839  </span></a>    <span class="s4">&quot;polar&quot;</span><span class="s3">,</span>
<a name="l840"><span class="ln">840  </span></a>    <span class="s4">&quot;polygamma&quot;</span><span class="s3">,</span>
<a name="l841"><span class="ln">841  </span></a>    <span class="s4">&quot;positive&quot;</span><span class="s3">,</span>
<a name="l842"><span class="ln">842  </span></a>    <span class="s4">&quot;pow&quot;</span><span class="s3">,</span>
<a name="l843"><span class="ln">843  </span></a>    <span class="s4">&quot;prelu&quot;</span><span class="s3">,</span>
<a name="l844"><span class="ln">844  </span></a>    <span class="s4">&quot;prod&quot;</span><span class="s3">,</span>
<a name="l845"><span class="ln">845  </span></a>    <span class="s4">&quot;promote_types&quot;</span><span class="s3">,</span>
<a name="l846"><span class="ln">846  </span></a>    <span class="s4">&quot;put&quot;</span><span class="s3">,</span>
<a name="l847"><span class="ln">847  </span></a>    <span class="s4">&quot;q_per_channel_axis&quot;</span><span class="s3">,</span>
<a name="l848"><span class="ln">848  </span></a>    <span class="s4">&quot;q_per_channel_scales&quot;</span><span class="s3">,</span>
<a name="l849"><span class="ln">849  </span></a>    <span class="s4">&quot;q_per_channel_zero_points&quot;</span><span class="s3">,</span>
<a name="l850"><span class="ln">850  </span></a>    <span class="s4">&quot;q_scale&quot;</span><span class="s3">,</span>
<a name="l851"><span class="ln">851  </span></a>    <span class="s4">&quot;q_zero_point&quot;</span><span class="s3">,</span>
<a name="l852"><span class="ln">852  </span></a>    <span class="s4">&quot;qr&quot;</span><span class="s3">,</span>
<a name="l853"><span class="ln">853  </span></a>    <span class="s4">&quot;qr&quot;</span><span class="s3">,</span>
<a name="l854"><span class="ln">854  </span></a>    <span class="s4">&quot;quantile&quot;</span><span class="s3">,</span>
<a name="l855"><span class="ln">855  </span></a>    <span class="s4">&quot;quantize_per_channel&quot;</span><span class="s3">,</span>
<a name="l856"><span class="ln">856  </span></a>    <span class="s4">&quot;quantize_per_tensor&quot;</span><span class="s3">,</span>
<a name="l857"><span class="ln">857  </span></a>    <span class="s4">&quot;quantize_per_tensor_dynamic&quot;</span><span class="s3">,</span>
<a name="l858"><span class="ln">858  </span></a>    <span class="s4">&quot;quantized_batch_norm&quot;</span><span class="s3">,</span>
<a name="l859"><span class="ln">859  </span></a>    <span class="s4">&quot;quantized_gru_cell&quot;</span><span class="s3">,</span>
<a name="l860"><span class="ln">860  </span></a>    <span class="s4">&quot;quantized_lstm_cell&quot;</span><span class="s3">,</span>
<a name="l861"><span class="ln">861  </span></a>    <span class="s4">&quot;quantized_max_pool1d&quot;</span><span class="s3">,</span>
<a name="l862"><span class="ln">862  </span></a>    <span class="s4">&quot;quantized_max_pool2d&quot;</span><span class="s3">,</span>
<a name="l863"><span class="ln">863  </span></a>    <span class="s4">&quot;quantized_max_pool3d&quot;</span><span class="s3">,</span>
<a name="l864"><span class="ln">864  </span></a>    <span class="s4">&quot;quantized_rnn_relu_cell&quot;</span><span class="s3">,</span>
<a name="l865"><span class="ln">865  </span></a>    <span class="s4">&quot;quantized_rnn_tanh_cell&quot;</span><span class="s3">,</span>
<a name="l866"><span class="ln">866  </span></a>    <span class="s4">&quot;rad2deg&quot;</span><span class="s3">,</span>
<a name="l867"><span class="ln">867  </span></a>    <span class="s4">&quot;rad2deg_&quot;</span><span class="s3">,</span>
<a name="l868"><span class="ln">868  </span></a>    <span class="s4">&quot;rand&quot;</span><span class="s3">,</span>
<a name="l869"><span class="ln">869  </span></a>    <span class="s4">&quot;rand_like&quot;</span><span class="s3">,</span>
<a name="l870"><span class="ln">870  </span></a>    <span class="s4">&quot;randint&quot;</span><span class="s3">,</span>
<a name="l871"><span class="ln">871  </span></a>    <span class="s4">&quot;randint_like&quot;</span><span class="s3">,</span>
<a name="l872"><span class="ln">872  </span></a>    <span class="s4">&quot;randn&quot;</span><span class="s3">,</span>
<a name="l873"><span class="ln">873  </span></a>    <span class="s4">&quot;randn_like&quot;</span><span class="s3">,</span>
<a name="l874"><span class="ln">874  </span></a>    <span class="s4">&quot;randperm&quot;</span><span class="s3">,</span>
<a name="l875"><span class="ln">875  </span></a>    <span class="s4">&quot;range&quot;</span><span class="s3">,</span>
<a name="l876"><span class="ln">876  </span></a>    <span class="s4">&quot;ravel&quot;</span><span class="s3">,</span>
<a name="l877"><span class="ln">877  </span></a>    <span class="s4">&quot;real&quot;</span><span class="s3">,</span>
<a name="l878"><span class="ln">878  </span></a>    <span class="s4">&quot;reciprocal&quot;</span><span class="s3">,</span>
<a name="l879"><span class="ln">879  </span></a>    <span class="s4">&quot;reciprocal_&quot;</span><span class="s3">,</span>
<a name="l880"><span class="ln">880  </span></a>    <span class="s4">&quot;relu&quot;</span><span class="s3">,</span>
<a name="l881"><span class="ln">881  </span></a>    <span class="s4">&quot;relu_&quot;</span><span class="s3">,</span>
<a name="l882"><span class="ln">882  </span></a>    <span class="s4">&quot;remainder&quot;</span><span class="s3">,</span>
<a name="l883"><span class="ln">883  </span></a>    <span class="s4">&quot;renorm&quot;</span><span class="s3">,</span>
<a name="l884"><span class="ln">884  </span></a>    <span class="s4">&quot;repeat_interleave&quot;</span><span class="s3">,</span>
<a name="l885"><span class="ln">885  </span></a>    <span class="s4">&quot;reshape&quot;</span><span class="s3">,</span>
<a name="l886"><span class="ln">886  </span></a>    <span class="s4">&quot;resize_as_&quot;</span><span class="s3">,</span>
<a name="l887"><span class="ln">887  </span></a>    <span class="s4">&quot;resize_as_sparse_&quot;</span><span class="s3">,</span>
<a name="l888"><span class="ln">888  </span></a>    <span class="s4">&quot;resolve_conj&quot;</span><span class="s3">,</span>
<a name="l889"><span class="ln">889  </span></a>    <span class="s4">&quot;resolve_neg&quot;</span><span class="s3">,</span>
<a name="l890"><span class="ln">890  </span></a>    <span class="s4">&quot;result_type&quot;</span><span class="s3">,</span>
<a name="l891"><span class="ln">891  </span></a>    <span class="s4">&quot;rms_norm&quot;</span><span class="s3">,</span>
<a name="l892"><span class="ln">892  </span></a>    <span class="s4">&quot;rnn_relu&quot;</span><span class="s3">,</span>
<a name="l893"><span class="ln">893  </span></a>    <span class="s4">&quot;rnn_relu_cell&quot;</span><span class="s3">,</span>
<a name="l894"><span class="ln">894  </span></a>    <span class="s4">&quot;rnn_tanh&quot;</span><span class="s3">,</span>
<a name="l895"><span class="ln">895  </span></a>    <span class="s4">&quot;rnn_tanh_cell&quot;</span><span class="s3">,</span>
<a name="l896"><span class="ln">896  </span></a>    <span class="s4">&quot;roll&quot;</span><span class="s3">,</span>
<a name="l897"><span class="ln">897  </span></a>    <span class="s4">&quot;rot90&quot;</span><span class="s3">,</span>
<a name="l898"><span class="ln">898  </span></a>    <span class="s4">&quot;round&quot;</span><span class="s3">,</span>
<a name="l899"><span class="ln">899  </span></a>    <span class="s4">&quot;round_&quot;</span><span class="s3">,</span>
<a name="l900"><span class="ln">900  </span></a>    <span class="s4">&quot;row_indices_copy&quot;</span><span class="s3">,</span>
<a name="l901"><span class="ln">901  </span></a>    <span class="s4">&quot;row_stack&quot;</span><span class="s3">,</span>
<a name="l902"><span class="ln">902  </span></a>    <span class="s4">&quot;rrelu&quot;</span><span class="s3">,</span>
<a name="l903"><span class="ln">903  </span></a>    <span class="s4">&quot;rrelu_&quot;</span><span class="s3">,</span>
<a name="l904"><span class="ln">904  </span></a>    <span class="s4">&quot;rsqrt&quot;</span><span class="s3">,</span>
<a name="l905"><span class="ln">905  </span></a>    <span class="s4">&quot;rsqrt_&quot;</span><span class="s3">,</span>
<a name="l906"><span class="ln">906  </span></a>    <span class="s4">&quot;rsub&quot;</span><span class="s3">,</span>
<a name="l907"><span class="ln">907  </span></a>    <span class="s4">&quot;saddmm&quot;</span><span class="s3">,</span>
<a name="l908"><span class="ln">908  </span></a>    <span class="s4">&quot;scalar_tensor&quot;</span><span class="s3">,</span>
<a name="l909"><span class="ln">909  </span></a>    <span class="s4">&quot;scatter&quot;</span><span class="s3">,</span>
<a name="l910"><span class="ln">910  </span></a>    <span class="s4">&quot;scatter_add&quot;</span><span class="s3">,</span>
<a name="l911"><span class="ln">911  </span></a>    <span class="s4">&quot;scatter_reduce&quot;</span><span class="s3">,</span>
<a name="l912"><span class="ln">912  </span></a>    <span class="s4">&quot;searchsorted&quot;</span><span class="s3">,</span>
<a name="l913"><span class="ln">913  </span></a>    <span class="s4">&quot;segment_reduce&quot;</span><span class="s3">,</span>
<a name="l914"><span class="ln">914  </span></a>    <span class="s4">&quot;select&quot;</span><span class="s3">,</span>
<a name="l915"><span class="ln">915  </span></a>    <span class="s4">&quot;select_copy&quot;</span><span class="s3">,</span>
<a name="l916"><span class="ln">916  </span></a>    <span class="s4">&quot;select_scatter&quot;</span><span class="s3">,</span>
<a name="l917"><span class="ln">917  </span></a>    <span class="s4">&quot;selu&quot;</span><span class="s3">,</span>
<a name="l918"><span class="ln">918  </span></a>    <span class="s4">&quot;selu_&quot;</span><span class="s3">,</span>
<a name="l919"><span class="ln">919  </span></a>    <span class="s4">&quot;set_flush_denormal&quot;</span><span class="s3">,</span>
<a name="l920"><span class="ln">920  </span></a>    <span class="s4">&quot;set_num_interop_threads&quot;</span><span class="s3">,</span>
<a name="l921"><span class="ln">921  </span></a>    <span class="s4">&quot;set_num_threads&quot;</span><span class="s3">,</span>
<a name="l922"><span class="ln">922  </span></a>    <span class="s4">&quot;sgn&quot;</span><span class="s3">,</span>
<a name="l923"><span class="ln">923  </span></a>    <span class="s4">&quot;sigmoid&quot;</span><span class="s3">,</span>
<a name="l924"><span class="ln">924  </span></a>    <span class="s4">&quot;sigmoid_&quot;</span><span class="s3">,</span>
<a name="l925"><span class="ln">925  </span></a>    <span class="s4">&quot;sign&quot;</span><span class="s3">,</span>
<a name="l926"><span class="ln">926  </span></a>    <span class="s4">&quot;signbit&quot;</span><span class="s3">,</span>
<a name="l927"><span class="ln">927  </span></a>    <span class="s4">&quot;sin&quot;</span><span class="s3">,</span>
<a name="l928"><span class="ln">928  </span></a>    <span class="s4">&quot;sin_&quot;</span><span class="s3">,</span>
<a name="l929"><span class="ln">929  </span></a>    <span class="s4">&quot;sinc&quot;</span><span class="s3">,</span>
<a name="l930"><span class="ln">930  </span></a>    <span class="s4">&quot;sinc_&quot;</span><span class="s3">,</span>
<a name="l931"><span class="ln">931  </span></a>    <span class="s4">&quot;sinh&quot;</span><span class="s3">,</span>
<a name="l932"><span class="ln">932  </span></a>    <span class="s4">&quot;sinh_&quot;</span><span class="s3">,</span>
<a name="l933"><span class="ln">933  </span></a>    <span class="s4">&quot;slice_copy&quot;</span><span class="s3">,</span>
<a name="l934"><span class="ln">934  </span></a>    <span class="s4">&quot;slice_inverse&quot;</span><span class="s3">,</span>
<a name="l935"><span class="ln">935  </span></a>    <span class="s4">&quot;slice_scatter&quot;</span><span class="s3">,</span>
<a name="l936"><span class="ln">936  </span></a>    <span class="s4">&quot;slogdet&quot;</span><span class="s3">,</span>
<a name="l937"><span class="ln">937  </span></a>    <span class="s4">&quot;slogdet&quot;</span><span class="s3">,</span>
<a name="l938"><span class="ln">938  </span></a>    <span class="s4">&quot;smm&quot;</span><span class="s3">,</span>
<a name="l939"><span class="ln">939  </span></a>    <span class="s4">&quot;softmax&quot;</span><span class="s3">,</span>
<a name="l940"><span class="ln">940  </span></a>    <span class="s4">&quot;sort&quot;</span><span class="s3">,</span>
<a name="l941"><span class="ln">941  </span></a>    <span class="s4">&quot;sort&quot;</span><span class="s3">,</span>
<a name="l942"><span class="ln">942  </span></a>    <span class="s4">&quot;sparse_bsc_tensor&quot;</span><span class="s3">,</span>
<a name="l943"><span class="ln">943  </span></a>    <span class="s4">&quot;sparse_bsr_tensor&quot;</span><span class="s3">,</span>
<a name="l944"><span class="ln">944  </span></a>    <span class="s4">&quot;sparse_compressed_tensor&quot;</span><span class="s3">,</span>
<a name="l945"><span class="ln">945  </span></a>    <span class="s4">&quot;sparse_coo_tensor&quot;</span><span class="s3">,</span>
<a name="l946"><span class="ln">946  </span></a>    <span class="s4">&quot;sparse_csc_tensor&quot;</span><span class="s3">,</span>
<a name="l947"><span class="ln">947  </span></a>    <span class="s4">&quot;sparse_csr_tensor&quot;</span><span class="s3">,</span>
<a name="l948"><span class="ln">948  </span></a>    <span class="s4">&quot;split_copy&quot;</span><span class="s3">,</span>
<a name="l949"><span class="ln">949  </span></a>    <span class="s4">&quot;split_with_sizes&quot;</span><span class="s3">,</span>
<a name="l950"><span class="ln">950  </span></a>    <span class="s4">&quot;split_with_sizes_copy&quot;</span><span class="s3">,</span>
<a name="l951"><span class="ln">951  </span></a>    <span class="s4">&quot;spmm&quot;</span><span class="s3">,</span>
<a name="l952"><span class="ln">952  </span></a>    <span class="s4">&quot;sqrt&quot;</span><span class="s3">,</span>
<a name="l953"><span class="ln">953  </span></a>    <span class="s4">&quot;sqrt_&quot;</span><span class="s3">,</span>
<a name="l954"><span class="ln">954  </span></a>    <span class="s4">&quot;square&quot;</span><span class="s3">,</span>
<a name="l955"><span class="ln">955  </span></a>    <span class="s4">&quot;square_&quot;</span><span class="s3">,</span>
<a name="l956"><span class="ln">956  </span></a>    <span class="s4">&quot;squeeze&quot;</span><span class="s3">,</span>
<a name="l957"><span class="ln">957  </span></a>    <span class="s4">&quot;squeeze_copy&quot;</span><span class="s3">,</span>
<a name="l958"><span class="ln">958  </span></a>    <span class="s4">&quot;sspaddmm&quot;</span><span class="s3">,</span>
<a name="l959"><span class="ln">959  </span></a>    <span class="s4">&quot;stack&quot;</span><span class="s3">,</span>
<a name="l960"><span class="ln">960  </span></a>    <span class="s4">&quot;std&quot;</span><span class="s3">,</span>
<a name="l961"><span class="ln">961  </span></a>    <span class="s4">&quot;std_mean&quot;</span><span class="s3">,</span>
<a name="l962"><span class="ln">962  </span></a>    <span class="s4">&quot;sub&quot;</span><span class="s3">,</span>
<a name="l963"><span class="ln">963  </span></a>    <span class="s4">&quot;subtract&quot;</span><span class="s3">,</span>
<a name="l964"><span class="ln">964  </span></a>    <span class="s4">&quot;sum&quot;</span><span class="s3">,</span>
<a name="l965"><span class="ln">965  </span></a>    <span class="s4">&quot;svd&quot;</span><span class="s3">,</span>
<a name="l966"><span class="ln">966  </span></a>    <span class="s4">&quot;svd&quot;</span><span class="s3">,</span>
<a name="l967"><span class="ln">967  </span></a>    <span class="s4">&quot;swapaxes&quot;</span><span class="s3">,</span>
<a name="l968"><span class="ln">968  </span></a>    <span class="s4">&quot;swapdims&quot;</span><span class="s3">,</span>
<a name="l969"><span class="ln">969  </span></a>    <span class="s4">&quot;sym_constrain_range&quot;</span><span class="s3">,</span>
<a name="l970"><span class="ln">970  </span></a>    <span class="s4">&quot;sym_constrain_range_for_size&quot;</span><span class="s3">,</span>
<a name="l971"><span class="ln">971  </span></a>    <span class="s4">&quot;t&quot;</span><span class="s3">,</span>
<a name="l972"><span class="ln">972  </span></a>    <span class="s4">&quot;t_copy&quot;</span><span class="s3">,</span>
<a name="l973"><span class="ln">973  </span></a>    <span class="s4">&quot;take&quot;</span><span class="s3">,</span>
<a name="l974"><span class="ln">974  </span></a>    <span class="s4">&quot;take_along_dim&quot;</span><span class="s3">,</span>
<a name="l975"><span class="ln">975  </span></a>    <span class="s4">&quot;tan&quot;</span><span class="s3">,</span>
<a name="l976"><span class="ln">976  </span></a>    <span class="s4">&quot;tan_&quot;</span><span class="s3">,</span>
<a name="l977"><span class="ln">977  </span></a>    <span class="s4">&quot;tanh&quot;</span><span class="s3">,</span>
<a name="l978"><span class="ln">978  </span></a>    <span class="s4">&quot;tanh_&quot;</span><span class="s3">,</span>
<a name="l979"><span class="ln">979  </span></a>    <span class="s4">&quot;tensor&quot;</span><span class="s3">,</span>
<a name="l980"><span class="ln">980  </span></a>    <span class="s4">&quot;tensor_split&quot;</span><span class="s3">,</span>
<a name="l981"><span class="ln">981  </span></a>    <span class="s4">&quot;threshold&quot;</span><span class="s3">,</span>
<a name="l982"><span class="ln">982  </span></a>    <span class="s4">&quot;threshold_&quot;</span><span class="s3">,</span>
<a name="l983"><span class="ln">983  </span></a>    <span class="s4">&quot;tile&quot;</span><span class="s3">,</span>
<a name="l984"><span class="ln">984  </span></a>    <span class="s4">&quot;topk&quot;</span><span class="s3">,</span>
<a name="l985"><span class="ln">985  </span></a>    <span class="s4">&quot;topk&quot;</span><span class="s3">,</span>
<a name="l986"><span class="ln">986  </span></a>    <span class="s4">&quot;trace&quot;</span><span class="s3">,</span>
<a name="l987"><span class="ln">987  </span></a>    <span class="s4">&quot;transpose&quot;</span><span class="s3">,</span>
<a name="l988"><span class="ln">988  </span></a>    <span class="s4">&quot;transpose_copy&quot;</span><span class="s3">,</span>
<a name="l989"><span class="ln">989  </span></a>    <span class="s4">&quot;trapezoid&quot;</span><span class="s3">,</span>
<a name="l990"><span class="ln">990  </span></a>    <span class="s4">&quot;trapz&quot;</span><span class="s3">,</span>
<a name="l991"><span class="ln">991  </span></a>    <span class="s4">&quot;triangular_solve&quot;</span><span class="s3">,</span>
<a name="l992"><span class="ln">992  </span></a>    <span class="s4">&quot;triangular_solve&quot;</span><span class="s3">,</span>
<a name="l993"><span class="ln">993  </span></a>    <span class="s4">&quot;tril&quot;</span><span class="s3">,</span>
<a name="l994"><span class="ln">994  </span></a>    <span class="s4">&quot;tril_indices&quot;</span><span class="s3">,</span>
<a name="l995"><span class="ln">995  </span></a>    <span class="s4">&quot;triplet_margin_loss&quot;</span><span class="s3">,</span>
<a name="l996"><span class="ln">996  </span></a>    <span class="s4">&quot;triu&quot;</span><span class="s3">,</span>
<a name="l997"><span class="ln">997  </span></a>    <span class="s4">&quot;triu_indices&quot;</span><span class="s3">,</span>
<a name="l998"><span class="ln">998  </span></a>    <span class="s4">&quot;true_divide&quot;</span><span class="s3">,</span>
<a name="l999"><span class="ln">999  </span></a>    <span class="s4">&quot;trunc&quot;</span><span class="s3">,</span>
<a name="l1000"><span class="ln">1000 </span></a>    <span class="s4">&quot;trunc_&quot;</span><span class="s3">,</span>
<a name="l1001"><span class="ln">1001 </span></a>    <span class="s4">&quot;unbind&quot;</span><span class="s3">,</span>
<a name="l1002"><span class="ln">1002 </span></a>    <span class="s4">&quot;unbind_copy&quot;</span><span class="s3">,</span>
<a name="l1003"><span class="ln">1003 </span></a>    <span class="s4">&quot;unflatten&quot;</span><span class="s3">,</span>
<a name="l1004"><span class="ln">1004 </span></a>    <span class="s4">&quot;unfold_copy&quot;</span><span class="s3">,</span>
<a name="l1005"><span class="ln">1005 </span></a>    <span class="s4">&quot;unique_dim&quot;</span><span class="s3">,</span>
<a name="l1006"><span class="ln">1006 </span></a>    <span class="s4">&quot;unsafe_chunk&quot;</span><span class="s3">,</span>
<a name="l1007"><span class="ln">1007 </span></a>    <span class="s4">&quot;unsafe_split&quot;</span><span class="s3">,</span>
<a name="l1008"><span class="ln">1008 </span></a>    <span class="s4">&quot;unsafe_split_with_sizes&quot;</span><span class="s3">,</span>
<a name="l1009"><span class="ln">1009 </span></a>    <span class="s4">&quot;unsqueeze&quot;</span><span class="s3">,</span>
<a name="l1010"><span class="ln">1010 </span></a>    <span class="s4">&quot;unsqueeze_copy&quot;</span><span class="s3">,</span>
<a name="l1011"><span class="ln">1011 </span></a>    <span class="s4">&quot;values_copy&quot;</span><span class="s3">,</span>
<a name="l1012"><span class="ln">1012 </span></a>    <span class="s4">&quot;vander&quot;</span><span class="s3">,</span>
<a name="l1013"><span class="ln">1013 </span></a>    <span class="s4">&quot;var&quot;</span><span class="s3">,</span>
<a name="l1014"><span class="ln">1014 </span></a>    <span class="s4">&quot;var_mean&quot;</span><span class="s3">,</span>
<a name="l1015"><span class="ln">1015 </span></a>    <span class="s4">&quot;vdot&quot;</span><span class="s3">,</span>
<a name="l1016"><span class="ln">1016 </span></a>    <span class="s4">&quot;view_as_complex&quot;</span><span class="s3">,</span>
<a name="l1017"><span class="ln">1017 </span></a>    <span class="s4">&quot;view_as_complex_copy&quot;</span><span class="s3">,</span>
<a name="l1018"><span class="ln">1018 </span></a>    <span class="s4">&quot;view_as_real&quot;</span><span class="s3">,</span>
<a name="l1019"><span class="ln">1019 </span></a>    <span class="s4">&quot;view_as_real_copy&quot;</span><span class="s3">,</span>
<a name="l1020"><span class="ln">1020 </span></a>    <span class="s4">&quot;view_copy&quot;</span><span class="s3">,</span>
<a name="l1021"><span class="ln">1021 </span></a>    <span class="s4">&quot;vsplit&quot;</span><span class="s3">,</span>
<a name="l1022"><span class="ln">1022 </span></a>    <span class="s4">&quot;vstack&quot;</span><span class="s3">,</span>
<a name="l1023"><span class="ln">1023 </span></a>    <span class="s4">&quot;where&quot;</span><span class="s3">,</span>
<a name="l1024"><span class="ln">1024 </span></a>    <span class="s4">&quot;xlogy&quot;</span><span class="s3">,</span>
<a name="l1025"><span class="ln">1025 </span></a>    <span class="s4">&quot;xlogy_&quot;</span><span class="s3">,</span>
<a name="l1026"><span class="ln">1026 </span></a>    <span class="s4">&quot;zero_&quot;</span><span class="s3">,</span>
<a name="l1027"><span class="ln">1027 </span></a>    <span class="s4">&quot;zeros&quot;</span><span class="s3">,</span>
<a name="l1028"><span class="ln">1028 </span></a>    <span class="s4">&quot;zeros_like&quot;</span><span class="s3">,</span>
<a name="l1029"><span class="ln">1029 </span></a><span class="s3">]</span>
<a name="l1030"><span class="ln">1030 </span></a>
<a name="l1031"><span class="ln">1031 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1032"><span class="ln">1032 </span></a><span class="s2">def </span><span class="s1">__and__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1033"><span class="ln">1033 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1034"><span class="ln">1034 </span></a><span class="s2">def </span><span class="s1">__and__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1035"><span class="ln">1035 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1036"><span class="ln">1036 </span></a><span class="s2">def </span><span class="s1">__lshift__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1037"><span class="ln">1037 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1038"><span class="ln">1038 </span></a><span class="s2">def </span><span class="s1">__lshift__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1039"><span class="ln">1039 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1040"><span class="ln">1040 </span></a><span class="s2">def </span><span class="s1">__or__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1041"><span class="ln">1041 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1042"><span class="ln">1042 </span></a><span class="s2">def </span><span class="s1">__or__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1043"><span class="ln">1043 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1044"><span class="ln">1044 </span></a><span class="s2">def </span><span class="s1">__rshift__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1045"><span class="ln">1045 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1046"><span class="ln">1046 </span></a><span class="s2">def </span><span class="s1">__rshift__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1047"><span class="ln">1047 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1048"><span class="ln">1048 </span></a><span class="s2">def </span><span class="s1">__xor__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1049"><span class="ln">1049 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1050"><span class="ln">1050 </span></a><span class="s2">def </span><span class="s1">__xor__</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1051"><span class="ln">1051 </span></a><span class="s2">def </span><span class="s1">_adaptive_avg_pool2d</span><span class="s3">(</span>
<a name="l1052"><span class="ln">1052 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1053"><span class="ln">1053 </span></a>    <span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1054"><span class="ln">1054 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1055"><span class="ln">1055 </span></a><span class="s2">def </span><span class="s1">_adaptive_avg_pool3d</span><span class="s3">(</span>
<a name="l1056"><span class="ln">1056 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1057"><span class="ln">1057 </span></a>    <span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1058"><span class="ln">1058 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1059"><span class="ln">1059 </span></a><span class="s2">def </span><span class="s1">_add_batch_dim</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">batch_dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">level</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1060"><span class="ln">1060 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1061"><span class="ln">1061 </span></a><span class="s2">def </span><span class="s1">_add_relu</span><span class="s3">(</span>
<a name="l1062"><span class="ln">1062 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1063"><span class="ln">1063 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1064"><span class="ln">1064 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1065"><span class="ln">1065 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1066"><span class="ln">1066 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1067"><span class="ln">1067 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1068"><span class="ln">1068 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1069"><span class="ln">1069 </span></a><span class="s2">def </span><span class="s1">_add_relu</span><span class="s3">(</span>
<a name="l1070"><span class="ln">1070 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1071"><span class="ln">1071 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1072"><span class="ln">1072 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1073"><span class="ln">1073 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1074"><span class="ln">1074 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1075"><span class="ln">1075 </span></a><span class="s2">def </span><span class="s1">_add_relu_</span><span class="s3">(</span>
<a name="l1076"><span class="ln">1076 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1077"><span class="ln">1077 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1078"><span class="ln">1078 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1079"><span class="ln">1079 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1080"><span class="ln">1080 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1081"><span class="ln">1081 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1082"><span class="ln">1082 </span></a><span class="s2">def </span><span class="s1">_add_relu_</span><span class="s3">(</span>
<a name="l1083"><span class="ln">1083 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1084"><span class="ln">1084 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1085"><span class="ln">1085 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1086"><span class="ln">1086 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1087"><span class="ln">1087 </span></a><span class="s2">def </span><span class="s1">_addmm_activation</span><span class="s3">(</span>
<a name="l1088"><span class="ln">1088 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1089"><span class="ln">1089 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1090"><span class="ln">1090 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1091"><span class="ln">1091 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1092"><span class="ln">1092 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1093"><span class="ln">1093 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1094"><span class="ln">1094 </span></a>    <span class="s1">use_gelu</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1095"><span class="ln">1095 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1096"><span class="ln">1096 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1097"><span class="ln">1097 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1098"><span class="ln">1098 </span></a><span class="s2">def </span><span class="s1">_aminmax</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1099"><span class="ln">1099 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1100"><span class="ln">1100 </span></a><span class="s2">def </span><span class="s1">_aminmax</span><span class="s3">(</span>
<a name="l1101"><span class="ln">1101 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1102"><span class="ln">1102 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1103"><span class="ln">1103 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1104"><span class="ln">1104 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1105"><span class="ln">1105 </span></a><span class="s2">def </span><span class="s1">_amp_foreach_non_finite_check_and_unscale_</span><span class="s3">(</span>
<a name="l1106"><span class="ln">1106 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1107"><span class="ln">1107 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1108"><span class="ln">1108 </span></a>    <span class="s1">inv_scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1109"><span class="ln">1109 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1110"><span class="ln">1110 </span></a><span class="s2">def </span><span class="s1">_amp_update_scale_</span><span class="s3">(</span>
<a name="l1111"><span class="ln">1111 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1112"><span class="ln">1112 </span></a>    <span class="s1">growth_tracker</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1113"><span class="ln">1113 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1114"><span class="ln">1114 </span></a>    <span class="s1">scale_growth_factor</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l1115"><span class="ln">1115 </span></a>    <span class="s1">scale_backoff_factor</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l1116"><span class="ln">1116 </span></a>    <span class="s1">growth_interval</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1117"><span class="ln">1117 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1118"><span class="ln">1118 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1119"><span class="ln">1119 </span></a><span class="s2">def </span><span class="s1">_assert_async</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1120"><span class="ln">1120 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1121"><span class="ln">1121 </span></a>    _assert_async(tensor) -&gt; void 
<a name="l1122"><span class="ln">1122 </span></a> 
<a name="l1123"><span class="ln">1123 </span></a>    Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors, 
<a name="l1124"><span class="ln">1124 </span></a>    this is equivalent to ``assert tensor`` or ``assert tensor.is_nonzero()``; for 
<a name="l1125"><span class="ln">1125 </span></a>    CUDA tensors, we DO NOT synchronize and you may only find out the assertion 
<a name="l1126"><span class="ln">1126 </span></a>    failed at a later CUDA kernel launch.  Asynchronous assertion can be helpful for 
<a name="l1127"><span class="ln">1127 </span></a>    testing invariants in CUDA tensors without giving up performance.  This function 
<a name="l1128"><span class="ln">1128 </span></a>    is NOT intended to be used for regular error checking, as it will trash your CUDA 
<a name="l1129"><span class="ln">1129 </span></a>    context if the assert fails (forcing you to restart your PyTorch process.) 
<a name="l1130"><span class="ln">1130 </span></a> 
<a name="l1131"><span class="ln">1131 </span></a>    Args: 
<a name="l1132"><span class="ln">1132 </span></a>        tensor (Tensor): a one element tensor to test to see if it is nonzero.  Zero 
<a name="l1133"><span class="ln">1133 </span></a>            elements (including False for boolean tensors) cause an assertion failure 
<a name="l1134"><span class="ln">1134 </span></a>            to be raised. 
<a name="l1135"><span class="ln">1135 </span></a>    &quot;&quot;&quot;</span>
<a name="l1136"><span class="ln">1136 </span></a>
<a name="l1137"><span class="ln">1137 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1138"><span class="ln">1138 </span></a><span class="s2">def </span><span class="s1">_assert_async</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">assert_msg</span><span class="s2">: </span><span class="s1">str</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1139"><span class="ln">1139 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1140"><span class="ln">1140 </span></a>    _assert_async(tensor) -&gt; void 
<a name="l1141"><span class="ln">1141 </span></a> 
<a name="l1142"><span class="ln">1142 </span></a>    Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors, 
<a name="l1143"><span class="ln">1143 </span></a>    this is equivalent to ``assert tensor`` or ``assert tensor.is_nonzero()``; for 
<a name="l1144"><span class="ln">1144 </span></a>    CUDA tensors, we DO NOT synchronize and you may only find out the assertion 
<a name="l1145"><span class="ln">1145 </span></a>    failed at a later CUDA kernel launch.  Asynchronous assertion can be helpful for 
<a name="l1146"><span class="ln">1146 </span></a>    testing invariants in CUDA tensors without giving up performance.  This function 
<a name="l1147"><span class="ln">1147 </span></a>    is NOT intended to be used for regular error checking, as it will trash your CUDA 
<a name="l1148"><span class="ln">1148 </span></a>    context if the assert fails (forcing you to restart your PyTorch process.) 
<a name="l1149"><span class="ln">1149 </span></a> 
<a name="l1150"><span class="ln">1150 </span></a>    Args: 
<a name="l1151"><span class="ln">1151 </span></a>        tensor (Tensor): a one element tensor to test to see if it is nonzero.  Zero 
<a name="l1152"><span class="ln">1152 </span></a>            elements (including False for boolean tensors) cause an assertion failure 
<a name="l1153"><span class="ln">1153 </span></a>            to be raised. 
<a name="l1154"><span class="ln">1154 </span></a>    &quot;&quot;&quot;</span>
<a name="l1155"><span class="ln">1155 </span></a>
<a name="l1156"><span class="ln">1156 </span></a><span class="s2">def </span><span class="s1">_assert_scalar</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">assert_msg</span><span class="s2">: </span><span class="s1">str</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1157"><span class="ln">1157 </span></a><span class="s2">def </span><span class="s1">_assert_tensor_metadata</span><span class="s3">(</span>
<a name="l1158"><span class="ln">1158 </span></a>    <span class="s1">a</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1159"><span class="ln">1159 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1160"><span class="ln">1160 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1161"><span class="ln">1161 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1162"><span class="ln">1162 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1163"><span class="ln">1163 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1164"><span class="ln">1164 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1165"><span class="ln">1165 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1166"><span class="ln">1166 </span></a><span class="s2">def </span><span class="s1">_batch_norm_impl_index</span><span class="s3">(</span>
<a name="l1167"><span class="ln">1167 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1168"><span class="ln">1168 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1169"><span class="ln">1169 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1170"><span class="ln">1170 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1171"><span class="ln">1171 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1172"><span class="ln">1172 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1173"><span class="ln">1173 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l1174"><span class="ln">1174 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l1175"><span class="ln">1175 </span></a>    <span class="s1">cudnn_enabled</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1176"><span class="ln">1176 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">_int</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1177"><span class="ln">1177 </span></a><span class="s2">def </span><span class="s1">_cast_Byte</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1178"><span class="ln">1178 </span></a><span class="s2">def </span><span class="s1">_cast_Char</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1179"><span class="ln">1179 </span></a><span class="s2">def </span><span class="s1">_cast_Double</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1180"><span class="ln">1180 </span></a><span class="s2">def </span><span class="s1">_cast_Float</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1181"><span class="ln">1181 </span></a><span class="s2">def </span><span class="s1">_cast_Half</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1182"><span class="ln">1182 </span></a><span class="s2">def </span><span class="s1">_cast_Int</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1183"><span class="ln">1183 </span></a><span class="s2">def </span><span class="s1">_cast_Long</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1184"><span class="ln">1184 </span></a><span class="s2">def </span><span class="s1">_cast_Short</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1185"><span class="ln">1185 </span></a><span class="s2">def </span><span class="s1">_choose_qparams_per_tensor</span><span class="s3">(</span>
<a name="l1186"><span class="ln">1186 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1187"><span class="ln">1187 </span></a>    <span class="s1">reduce_range</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1188"><span class="ln">1188 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">_float</span><span class="s3">, </span><span class="s1">_int</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1189"><span class="ln">1189 </span></a><span class="s2">def </span><span class="s1">_chunk_cat</span><span class="s3">(</span>
<a name="l1190"><span class="ln">1190 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1191"><span class="ln">1191 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1192"><span class="ln">1192 </span></a>    <span class="s1">num_chunks</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1193"><span class="ln">1193 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1194"><span class="ln">1194 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1195"><span class="ln">1195 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1196"><span class="ln">1196 </span></a><span class="s2">def </span><span class="s1">_coalesce</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1197"><span class="ln">1197 </span></a><span class="s2">def </span><span class="s1">_compute_linear_combination</span><span class="s3">(</span>
<a name="l1198"><span class="ln">1198 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1199"><span class="ln">1199 </span></a>    <span class="s1">coefficients</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1200"><span class="ln">1200 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1201"><span class="ln">1201 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1202"><span class="ln">1202 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1203"><span class="ln">1203 </span></a><span class="s2">def </span><span class="s1">_conj</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1204"><span class="ln">1204 </span></a><span class="s2">def </span><span class="s1">_conj_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1205"><span class="ln">1205 </span></a><span class="s2">def </span><span class="s1">_conj_physical</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1206"><span class="ln">1206 </span></a><span class="s2">def </span><span class="s1">_convert_indices_from_coo_to_csr</span><span class="s3">(</span>
<a name="l1207"><span class="ln">1207 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1208"><span class="ln">1208 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1209"><span class="ln">1209 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1210"><span class="ln">1210 </span></a>    <span class="s1">out_int32</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1211"><span class="ln">1211 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1212"><span class="ln">1212 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1213"><span class="ln">1213 </span></a><span class="s2">def </span><span class="s1">_convert_indices_from_csr_to_coo</span><span class="s3">(</span>
<a name="l1214"><span class="ln">1214 </span></a>    <span class="s1">crow_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1215"><span class="ln">1215 </span></a>    <span class="s1">col_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1216"><span class="ln">1216 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1217"><span class="ln">1217 </span></a>    <span class="s1">out_int32</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1218"><span class="ln">1218 </span></a>    <span class="s1">transpose</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1219"><span class="ln">1219 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1220"><span class="ln">1220 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1221"><span class="ln">1221 </span></a><span class="s2">def </span><span class="s1">_convert_weight_to_int4pack</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">innerKTiles</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1222"><span class="ln">1222 </span></a><span class="s2">def </span><span class="s1">_convert_weight_to_int4pack_for_cpu</span><span class="s3">(</span>
<a name="l1223"><span class="ln">1223 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1224"><span class="ln">1224 </span></a>    <span class="s1">innerKTiles</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1225"><span class="ln">1225 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1226"><span class="ln">1226 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1227"><span class="ln">1227 </span></a><span class="s2">def </span><span class="s1">_convolution</span><span class="s3">(</span>
<a name="l1228"><span class="ln">1228 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1229"><span class="ln">1229 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1230"><span class="ln">1230 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1231"><span class="ln">1231 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1232"><span class="ln">1232 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1233"><span class="ln">1233 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1234"><span class="ln">1234 </span></a>    <span class="s1">transposed</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1235"><span class="ln">1235 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l1236"><span class="ln">1236 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1237"><span class="ln">1237 </span></a>    <span class="s1">benchmark</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1238"><span class="ln">1238 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1239"><span class="ln">1239 </span></a>    <span class="s1">cudnn_enabled</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1240"><span class="ln">1240 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1241"><span class="ln">1241 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1242"><span class="ln">1242 </span></a><span class="s2">def </span><span class="s1">_convolution</span><span class="s3">(</span>
<a name="l1243"><span class="ln">1243 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1244"><span class="ln">1244 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1245"><span class="ln">1245 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1246"><span class="ln">1246 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1247"><span class="ln">1247 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1248"><span class="ln">1248 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1249"><span class="ln">1249 </span></a>    <span class="s1">transposed</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1250"><span class="ln">1250 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1251"><span class="ln">1251 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1252"><span class="ln">1252 </span></a>    <span class="s1">benchmark</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1253"><span class="ln">1253 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1254"><span class="ln">1254 </span></a>    <span class="s1">cudnn_enabled</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1255"><span class="ln">1255 </span></a>    <span class="s1">allow_tf32</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1256"><span class="ln">1256 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1257"><span class="ln">1257 </span></a><span class="s2">def </span><span class="s1">_convolution_mode</span><span class="s3">(</span>
<a name="l1258"><span class="ln">1258 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1259"><span class="ln">1259 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1260"><span class="ln">1260 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1261"><span class="ln">1261 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1262"><span class="ln">1262 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l1263"><span class="ln">1263 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1264"><span class="ln">1264 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1265"><span class="ln">1265 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1266"><span class="ln">1266 </span></a><span class="s2">def </span><span class="s1">_copy_from</span><span class="s3">(</span>
<a name="l1267"><span class="ln">1267 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1268"><span class="ln">1268 </span></a>    <span class="s1">dst</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1269"><span class="ln">1269 </span></a>    <span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1270"><span class="ln">1270 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1271"><span class="ln">1271 </span></a><span class="s2">def </span><span class="s1">_copy_from_and_resize</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dst</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1272"><span class="ln">1272 </span></a><span class="s2">def </span><span class="s1">_cslt_compress</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1273"><span class="ln">1273 </span></a><span class="s2">def </span><span class="s1">_cslt_sparse_mm</span><span class="s3">(</span>
<a name="l1274"><span class="ln">1274 </span></a>    <span class="s1">compressed_A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1275"><span class="ln">1275 </span></a>    <span class="s1">dense_B</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1276"><span class="ln">1276 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1277"><span class="ln">1277 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1278"><span class="ln">1278 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1279"><span class="ln">1279 </span></a>    <span class="s1">transpose_result</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1280"><span class="ln">1280 </span></a>    <span class="s1">alg_id</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l1281"><span class="ln">1281 </span></a>    <span class="s1">split_k</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1282"><span class="ln">1282 </span></a>    <span class="s1">split_k_mode</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l1283"><span class="ln">1283 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1284"><span class="ln">1284 </span></a><span class="s2">def </span><span class="s1">_cslt_sparse_mm_search</span><span class="s3">(</span>
<a name="l1285"><span class="ln">1285 </span></a>    <span class="s1">compressed_A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1286"><span class="ln">1286 </span></a>    <span class="s1">dense_B</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1287"><span class="ln">1287 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1288"><span class="ln">1288 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1289"><span class="ln">1289 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1290"><span class="ln">1290 </span></a>    <span class="s1">transpose_result</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1291"><span class="ln">1291 </span></a><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1292"><span class="ln">1292 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1293"><span class="ln">1293 </span></a><span class="s2">def </span><span class="s1">_ctc_loss</span><span class="s3">(</span>
<a name="l1294"><span class="ln">1294 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1295"><span class="ln">1295 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1296"><span class="ln">1296 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l1297"><span class="ln">1297 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l1298"><span class="ln">1298 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l1299"><span class="ln">1299 </span></a>    <span class="s1">zero_infinity</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1300"><span class="ln">1300 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1301"><span class="ln">1301 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1302"><span class="ln">1302 </span></a><span class="s2">def </span><span class="s1">_ctc_loss</span><span class="s3">(</span>
<a name="l1303"><span class="ln">1303 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1304"><span class="ln">1304 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1305"><span class="ln">1305 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1306"><span class="ln">1306 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1307"><span class="ln">1307 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l1308"><span class="ln">1308 </span></a>    <span class="s1">zero_infinity</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1309"><span class="ln">1309 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1310"><span class="ln">1310 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1311"><span class="ln">1311 </span></a><span class="s2">def </span><span class="s1">_cudnn_ctc_loss</span><span class="s3">(</span>
<a name="l1312"><span class="ln">1312 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1313"><span class="ln">1313 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1314"><span class="ln">1314 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l1315"><span class="ln">1315 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l1316"><span class="ln">1316 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1317"><span class="ln">1317 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1318"><span class="ln">1318 </span></a>    <span class="s1">zero_infinity</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1319"><span class="ln">1319 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1320"><span class="ln">1320 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1321"><span class="ln">1321 </span></a><span class="s2">def </span><span class="s1">_cudnn_ctc_loss</span><span class="s3">(</span>
<a name="l1322"><span class="ln">1322 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1323"><span class="ln">1323 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1324"><span class="ln">1324 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1325"><span class="ln">1325 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1326"><span class="ln">1326 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1327"><span class="ln">1327 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1328"><span class="ln">1328 </span></a>    <span class="s1">zero_infinity</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1329"><span class="ln">1329 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1330"><span class="ln">1330 </span></a><span class="s2">def </span><span class="s1">_cudnn_init_dropout_state</span><span class="s3">(</span>
<a name="l1331"><span class="ln">1331 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l1332"><span class="ln">1332 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1333"><span class="ln">1333 </span></a>    <span class="s1">dropout_seed</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1334"><span class="ln">1334 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1335"><span class="ln">1335 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1336"><span class="ln">1336 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1337"><span class="ln">1337 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1338"><span class="ln">1338 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1339"><span class="ln">1339 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1340"><span class="ln">1340 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1341"><span class="ln">1341 </span></a><span class="s2">def </span><span class="s1">_cudnn_rnn</span><span class="s3">(</span>
<a name="l1342"><span class="ln">1342 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1343"><span class="ln">1343 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1344"><span class="ln">1344 </span></a>    <span class="s1">weight_stride0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1345"><span class="ln">1345 </span></a>    <span class="s1">weight_buf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1346"><span class="ln">1346 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1347"><span class="ln">1347 </span></a>    <span class="s1">cx</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1348"><span class="ln">1348 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1349"><span class="ln">1349 </span></a>    <span class="s1">hidden_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1350"><span class="ln">1350 </span></a>    <span class="s1">proj_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1351"><span class="ln">1351 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1352"><span class="ln">1352 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1353"><span class="ln">1353 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l1354"><span class="ln">1354 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1355"><span class="ln">1355 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1356"><span class="ln">1356 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1357"><span class="ln">1357 </span></a>    <span class="s1">dropout_state</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1358"><span class="ln">1358 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1359"><span class="ln">1359 </span></a><span class="s2">def </span><span class="s1">_cudnn_rnn_flatten_weight</span><span class="s3">(</span>
<a name="l1360"><span class="ln">1360 </span></a>    <span class="s1">weight_arr</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1361"><span class="ln">1361 </span></a>    <span class="s1">weight_stride0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1362"><span class="ln">1362 </span></a>    <span class="s1">input_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1363"><span class="ln">1363 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1364"><span class="ln">1364 </span></a>    <span class="s1">hidden_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1365"><span class="ln">1365 </span></a>    <span class="s1">proj_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1366"><span class="ln">1366 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1367"><span class="ln">1367 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1368"><span class="ln">1368 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1369"><span class="ln">1369 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1370"><span class="ln">1370 </span></a><span class="s2">def </span><span class="s1">_cufft_clear_plan_cache</span><span class="s3">(</span><span class="s1">device_index</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1371"><span class="ln">1371 </span></a><span class="s2">def </span><span class="s1">_cufft_get_plan_cache_max_size</span><span class="s3">(</span><span class="s1">device_index</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1372"><span class="ln">1372 </span></a><span class="s2">def </span><span class="s1">_cufft_get_plan_cache_size</span><span class="s3">(</span><span class="s1">device_index</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1373"><span class="ln">1373 </span></a><span class="s2">def </span><span class="s1">_cufft_set_plan_cache_max_size</span><span class="s3">(</span>
<a name="l1374"><span class="ln">1374 </span></a>    <span class="s1">device_index</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1375"><span class="ln">1375 </span></a>    <span class="s1">max_size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1376"><span class="ln">1376 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1377"><span class="ln">1377 </span></a><span class="s2">def </span><span class="s1">_cummax_helper</span><span class="s3">(</span>
<a name="l1378"><span class="ln">1378 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1379"><span class="ln">1379 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1380"><span class="ln">1380 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1381"><span class="ln">1381 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1382"><span class="ln">1382 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1383"><span class="ln">1383 </span></a><span class="s2">def </span><span class="s1">_cummin_helper</span><span class="s3">(</span>
<a name="l1384"><span class="ln">1384 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1385"><span class="ln">1385 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1386"><span class="ln">1386 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1387"><span class="ln">1387 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1388"><span class="ln">1388 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1389"><span class="ln">1389 </span></a><span class="s2">def </span><span class="s1">_debug_has_internal_overlap</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1390"><span class="ln">1390 </span></a><span class="s2">def </span><span class="s1">_dim_arange</span><span class="s3">(</span><span class="s1">like</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1391"><span class="ln">1391 </span></a><span class="s2">def </span><span class="s1">_dirichlet_grad</span><span class="s3">(</span><span class="s1">x</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">alpha</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">total</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1392"><span class="ln">1392 </span></a><span class="s2">def </span><span class="s1">_disable_functionalization</span><span class="s3">()</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1393"><span class="ln">1393 </span></a><span class="s2">def </span><span class="s1">_dyn_quant_matmul_4bit</span><span class="s3">(</span>
<a name="l1394"><span class="ln">1394 </span></a>    <span class="s1">inp</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1395"><span class="ln">1395 </span></a>    <span class="s1">packed_weights</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1396"><span class="ln">1396 </span></a>    <span class="s1">block_size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1397"><span class="ln">1397 </span></a>    <span class="s1">in_features</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1398"><span class="ln">1398 </span></a>    <span class="s1">out_features</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1399"><span class="ln">1399 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1400"><span class="ln">1400 </span></a><span class="s2">def </span><span class="s1">_dyn_quant_pack_4bit_weight</span><span class="s3">(</span>
<a name="l1401"><span class="ln">1401 </span></a>    <span class="s1">weights</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1402"><span class="ln">1402 </span></a>    <span class="s1">scales_zeros</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1403"><span class="ln">1403 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1404"><span class="ln">1404 </span></a>    <span class="s1">block_size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1405"><span class="ln">1405 </span></a>    <span class="s1">in_features</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1406"><span class="ln">1406 </span></a>    <span class="s1">out_features</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1407"><span class="ln">1407 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1408"><span class="ln">1408 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1409"><span class="ln">1409 </span></a><span class="s2">def </span><span class="s1">_efficientzerotensor</span><span class="s3">(</span>
<a name="l1410"><span class="ln">1410 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1411"><span class="ln">1411 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1412"><span class="ln">1412 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1413"><span class="ln">1413 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1414"><span class="ln">1414 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1415"><span class="ln">1415 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1416"><span class="ln">1416 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1417"><span class="ln">1417 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1418"><span class="ln">1418 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1419"><span class="ln">1419 </span></a><span class="s2">def </span><span class="s1">_efficientzerotensor</span><span class="s3">(</span>
<a name="l1420"><span class="ln">1420 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1421"><span class="ln">1421 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1422"><span class="ln">1422 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1423"><span class="ln">1423 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1424"><span class="ln">1424 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1425"><span class="ln">1425 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1426"><span class="ln">1426 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1427"><span class="ln">1427 </span></a><span class="s2">def </span><span class="s1">_embedding_bag</span><span class="s3">(</span>
<a name="l1428"><span class="ln">1428 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1429"><span class="ln">1429 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1430"><span class="ln">1430 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1431"><span class="ln">1431 </span></a>    <span class="s1">scale_grad_by_freq</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1432"><span class="ln">1432 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l1433"><span class="ln">1433 </span></a>    <span class="s1">sparse</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1434"><span class="ln">1434 </span></a>    <span class="s1">per_sample_weights</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1435"><span class="ln">1435 </span></a>    <span class="s1">include_last_offset</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1436"><span class="ln">1436 </span></a>    <span class="s1">padding_idx</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l1437"><span class="ln">1437 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1438"><span class="ln">1438 </span></a><span class="s2">def </span><span class="s1">_embedding_bag_forward_only</span><span class="s3">(</span>
<a name="l1439"><span class="ln">1439 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1440"><span class="ln">1440 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1441"><span class="ln">1441 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1442"><span class="ln">1442 </span></a>    <span class="s1">scale_grad_by_freq</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1443"><span class="ln">1443 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l1444"><span class="ln">1444 </span></a>    <span class="s1">sparse</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1445"><span class="ln">1445 </span></a>    <span class="s1">per_sample_weights</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1446"><span class="ln">1446 </span></a>    <span class="s1">include_last_offset</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1447"><span class="ln">1447 </span></a>    <span class="s1">padding_idx</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l1448"><span class="ln">1448 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1449"><span class="ln">1449 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1450"><span class="ln">1450 </span></a><span class="s2">def </span><span class="s1">_empty_affine_quantized</span><span class="s3">(</span>
<a name="l1451"><span class="ln">1451 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1452"><span class="ln">1452 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1453"><span class="ln">1453 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1454"><span class="ln">1454 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l1455"><span class="ln">1455 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = </span><span class="s1">contiguous_format</span><span class="s3">,</span>
<a name="l1456"><span class="ln">1456 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1457"><span class="ln">1457 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1458"><span class="ln">1458 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1459"><span class="ln">1459 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1460"><span class="ln">1460 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1461"><span class="ln">1461 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1462"><span class="ln">1462 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1463"><span class="ln">1463 </span></a><span class="s2">def </span><span class="s1">_empty_affine_quantized</span><span class="s3">(</span>
<a name="l1464"><span class="ln">1464 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1465"><span class="ln">1465 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1466"><span class="ln">1466 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l1467"><span class="ln">1467 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = </span><span class="s1">contiguous_format</span><span class="s3">,</span>
<a name="l1468"><span class="ln">1468 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1469"><span class="ln">1469 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1470"><span class="ln">1470 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1471"><span class="ln">1471 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1472"><span class="ln">1472 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1473"><span class="ln">1473 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1474"><span class="ln">1474 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1475"><span class="ln">1475 </span></a><span class="s2">def </span><span class="s1">_empty_per_channel_affine_quantized</span><span class="s3">(</span>
<a name="l1476"><span class="ln">1476 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1477"><span class="ln">1477 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1478"><span class="ln">1478 </span></a>    <span class="s1">scales</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1479"><span class="ln">1479 </span></a>    <span class="s1">zero_points</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1480"><span class="ln">1480 </span></a>    <span class="s1">axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1481"><span class="ln">1481 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = </span><span class="s1">contiguous_format</span><span class="s3">,</span>
<a name="l1482"><span class="ln">1482 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1483"><span class="ln">1483 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1484"><span class="ln">1484 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1485"><span class="ln">1485 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1486"><span class="ln">1486 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1487"><span class="ln">1487 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1488"><span class="ln">1488 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1489"><span class="ln">1489 </span></a><span class="s2">def </span><span class="s1">_empty_per_channel_affine_quantized</span><span class="s3">(</span>
<a name="l1490"><span class="ln">1490 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1491"><span class="ln">1491 </span></a>    <span class="s1">scales</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1492"><span class="ln">1492 </span></a>    <span class="s1">zero_points</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1493"><span class="ln">1493 </span></a>    <span class="s1">axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1494"><span class="ln">1494 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = </span><span class="s1">contiguous_format</span><span class="s3">,</span>
<a name="l1495"><span class="ln">1495 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1496"><span class="ln">1496 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1497"><span class="ln">1497 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1498"><span class="ln">1498 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1499"><span class="ln">1499 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l1500"><span class="ln">1500 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1501"><span class="ln">1501 </span></a><span class="s2">def </span><span class="s1">_enable_functionalization</span><span class="s3">(</span><span class="s2">*</span><span class="s3">, </span><span class="s1">reapply_views</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1502"><span class="ln">1502 </span></a><span class="s2">def </span><span class="s1">_euclidean_dist</span><span class="s3">(</span><span class="s1">x1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">x2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1503"><span class="ln">1503 </span></a><span class="s2">def </span><span class="s1">_fake_quantize_learnable_per_channel_affine</span><span class="s3">(</span>
<a name="l1504"><span class="ln">1504 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1505"><span class="ln">1505 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1506"><span class="ln">1506 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1507"><span class="ln">1507 </span></a>    <span class="s1">axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1508"><span class="ln">1508 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1509"><span class="ln">1509 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1510"><span class="ln">1510 </span></a>    <span class="s1">grad_factor</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1.0</span><span class="s3">,</span>
<a name="l1511"><span class="ln">1511 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1512"><span class="ln">1512 </span></a><span class="s2">def </span><span class="s1">_fake_quantize_learnable_per_tensor_affine</span><span class="s3">(</span>
<a name="l1513"><span class="ln">1513 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1514"><span class="ln">1514 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1515"><span class="ln">1515 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1516"><span class="ln">1516 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1517"><span class="ln">1517 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1518"><span class="ln">1518 </span></a>    <span class="s1">grad_factor</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1.0</span><span class="s3">,</span>
<a name="l1519"><span class="ln">1519 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1520"><span class="ln">1520 </span></a><span class="s2">def </span><span class="s1">_fake_quantize_per_tensor_affine_cachemask_tensor_qparams</span><span class="s3">(</span>
<a name="l1521"><span class="ln">1521 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1522"><span class="ln">1522 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1523"><span class="ln">1523 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1524"><span class="ln">1524 </span></a>    <span class="s1">fake_quant_enabled</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1525"><span class="ln">1525 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1526"><span class="ln">1526 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1527"><span class="ln">1527 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_fake_quantize_per_tensor_affine_cachemask_tensor_qparams</span><span class="s2">:  </span><span class="s0"># fmt: skip</span>
<a name="l1528"><span class="ln">1528 </span></a>    <span class="s3">...</span>
<a name="l1529"><span class="ln">1529 </span></a><span class="s2">def </span><span class="s1">_fft_c2c</span><span class="s3">(</span>
<a name="l1530"><span class="ln">1530 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1531"><span class="ln">1531 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l1532"><span class="ln">1532 </span></a>    <span class="s1">normalization</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1533"><span class="ln">1533 </span></a>    <span class="s1">forward</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1534"><span class="ln">1534 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1535"><span class="ln">1535 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1536"><span class="ln">1536 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1537"><span class="ln">1537 </span></a><span class="s2">def </span><span class="s1">_fft_c2r</span><span class="s3">(</span>
<a name="l1538"><span class="ln">1538 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1539"><span class="ln">1539 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l1540"><span class="ln">1540 </span></a>    <span class="s1">normalization</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1541"><span class="ln">1541 </span></a>    <span class="s1">last_dim_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l1542"><span class="ln">1542 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1543"><span class="ln">1543 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1544"><span class="ln">1544 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1545"><span class="ln">1545 </span></a><span class="s2">def </span><span class="s1">_fft_r2c</span><span class="s3">(</span>
<a name="l1546"><span class="ln">1546 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1547"><span class="ln">1547 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l1548"><span class="ln">1548 </span></a>    <span class="s1">normalization</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1549"><span class="ln">1549 </span></a>    <span class="s1">onesided</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l1550"><span class="ln">1550 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1551"><span class="ln">1551 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l1552"><span class="ln">1552 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1553"><span class="ln">1553 </span></a><span class="s2">def </span><span class="s1">_fill_mem_eff_dropout_mask_</span><span class="s3">(</span>
<a name="l1554"><span class="ln">1554 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1555"><span class="ln">1555 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l1556"><span class="ln">1556 </span></a>    <span class="s1">seed</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1557"><span class="ln">1557 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l1558"><span class="ln">1558 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1559"><span class="ln">1559 </span></a><span class="s2">def </span><span class="s1">_foobar</span><span class="s3">(</span>
<a name="l1560"><span class="ln">1560 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1561"><span class="ln">1561 </span></a>    <span class="s1">arg1</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l1562"><span class="ln">1562 </span></a>    <span class="s1">arg2</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l1563"><span class="ln">1563 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1564"><span class="ln">1564 </span></a>    <span class="s1">arg3</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l1565"><span class="ln">1565 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1566"><span class="ln">1566 </span></a><span class="s2">def </span><span class="s1">_foreach_abs</span><span class="s3">(</span>
<a name="l1567"><span class="ln">1567 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1568"><span class="ln">1568 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1569"><span class="ln">1569 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1570"><span class="ln">1570 </span></a>    _foreach_abs(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1571"><span class="ln">1571 </span></a> 
<a name="l1572"><span class="ln">1572 </span></a>    Apply :func:`torch.abs` to each Tensor of the input list. 
<a name="l1573"><span class="ln">1573 </span></a>    &quot;&quot;&quot;</span>
<a name="l1574"><span class="ln">1574 </span></a>
<a name="l1575"><span class="ln">1575 </span></a><span class="s2">def </span><span class="s1">_foreach_abs_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1576"><span class="ln">1576 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1577"><span class="ln">1577 </span></a>    _foreach_abs_(self: List[Tensor]) -&gt; None 
<a name="l1578"><span class="ln">1578 </span></a> 
<a name="l1579"><span class="ln">1579 </span></a>    Apply :func:`torch.abs` to each Tensor of the input list. 
<a name="l1580"><span class="ln">1580 </span></a>    &quot;&quot;&quot;</span>
<a name="l1581"><span class="ln">1581 </span></a>
<a name="l1582"><span class="ln">1582 </span></a><span class="s2">def </span><span class="s1">_foreach_acos</span><span class="s3">(</span>
<a name="l1583"><span class="ln">1583 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1584"><span class="ln">1584 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1585"><span class="ln">1585 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1586"><span class="ln">1586 </span></a>    _foreach_acos(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1587"><span class="ln">1587 </span></a> 
<a name="l1588"><span class="ln">1588 </span></a>    Apply :func:`torch.acos` to each Tensor of the input list. 
<a name="l1589"><span class="ln">1589 </span></a>    &quot;&quot;&quot;</span>
<a name="l1590"><span class="ln">1590 </span></a>
<a name="l1591"><span class="ln">1591 </span></a><span class="s2">def </span><span class="s1">_foreach_acos_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1592"><span class="ln">1592 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1593"><span class="ln">1593 </span></a>    _foreach_acos_(self: List[Tensor]) -&gt; None 
<a name="l1594"><span class="ln">1594 </span></a> 
<a name="l1595"><span class="ln">1595 </span></a>    Apply :func:`torch.acos` to each Tensor of the input list. 
<a name="l1596"><span class="ln">1596 </span></a>    &quot;&quot;&quot;</span>
<a name="l1597"><span class="ln">1597 </span></a>
<a name="l1598"><span class="ln">1598 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1599"><span class="ln">1599 </span></a><span class="s2">def </span><span class="s1">_foreach_add</span><span class="s3">(</span>
<a name="l1600"><span class="ln">1600 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1601"><span class="ln">1601 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1602"><span class="ln">1602 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1603"><span class="ln">1603 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1604"><span class="ln">1604 </span></a><span class="s2">def </span><span class="s1">_foreach_add</span><span class="s3">(</span>
<a name="l1605"><span class="ln">1605 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1606"><span class="ln">1606 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1607"><span class="ln">1607 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1608"><span class="ln">1608 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1609"><span class="ln">1609 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1610"><span class="ln">1610 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1611"><span class="ln">1611 </span></a><span class="s2">def </span><span class="s1">_foreach_add</span><span class="s3">(</span>
<a name="l1612"><span class="ln">1612 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1613"><span class="ln">1613 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1614"><span class="ln">1614 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1615"><span class="ln">1615 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1616"><span class="ln">1616 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1617"><span class="ln">1617 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1618"><span class="ln">1618 </span></a><span class="s2">def </span><span class="s1">_foreach_add</span><span class="s3">(</span>
<a name="l1619"><span class="ln">1619 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1620"><span class="ln">1620 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1621"><span class="ln">1621 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1622"><span class="ln">1622 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1623"><span class="ln">1623 </span></a><span class="s2">def </span><span class="s1">_foreach_add_</span><span class="s3">(</span>
<a name="l1624"><span class="ln">1624 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1625"><span class="ln">1625 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1626"><span class="ln">1626 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1627"><span class="ln">1627 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1628"><span class="ln">1628 </span></a><span class="s2">def </span><span class="s1">_foreach_add_</span><span class="s3">(</span>
<a name="l1629"><span class="ln">1629 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1630"><span class="ln">1630 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1631"><span class="ln">1631 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1632"><span class="ln">1632 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1633"><span class="ln">1633 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1634"><span class="ln">1634 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1635"><span class="ln">1635 </span></a><span class="s2">def </span><span class="s1">_foreach_add_</span><span class="s3">(</span>
<a name="l1636"><span class="ln">1636 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1637"><span class="ln">1637 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1638"><span class="ln">1638 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l1639"><span class="ln">1639 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1640"><span class="ln">1640 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1641"><span class="ln">1641 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1642"><span class="ln">1642 </span></a><span class="s2">def </span><span class="s1">_foreach_add_</span><span class="s3">(</span>
<a name="l1643"><span class="ln">1643 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1644"><span class="ln">1644 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1645"><span class="ln">1645 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1646"><span class="ln">1646 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1647"><span class="ln">1647 </span></a><span class="s2">def </span><span class="s1">_foreach_addcdiv</span><span class="s3">(</span>
<a name="l1648"><span class="ln">1648 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1649"><span class="ln">1649 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1650"><span class="ln">1650 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1651"><span class="ln">1651 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1652"><span class="ln">1652 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1653"><span class="ln">1653 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1654"><span class="ln">1654 </span></a><span class="s2">def </span><span class="s1">_foreach_addcdiv</span><span class="s3">(</span>
<a name="l1655"><span class="ln">1655 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1656"><span class="ln">1656 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1657"><span class="ln">1657 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1658"><span class="ln">1658 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1659"><span class="ln">1659 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1660"><span class="ln">1660 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1661"><span class="ln">1661 </span></a><span class="s2">def </span><span class="s1">_foreach_addcdiv</span><span class="s3">(</span>
<a name="l1662"><span class="ln">1662 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1663"><span class="ln">1663 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1664"><span class="ln">1664 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1665"><span class="ln">1665 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1666"><span class="ln">1666 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1667"><span class="ln">1667 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1668"><span class="ln">1668 </span></a><span class="s2">def </span><span class="s1">_foreach_addcdiv_</span><span class="s3">(</span>
<a name="l1669"><span class="ln">1669 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1670"><span class="ln">1670 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1671"><span class="ln">1671 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1672"><span class="ln">1672 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1673"><span class="ln">1673 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1674"><span class="ln">1674 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1675"><span class="ln">1675 </span></a><span class="s2">def </span><span class="s1">_foreach_addcdiv_</span><span class="s3">(</span>
<a name="l1676"><span class="ln">1676 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1677"><span class="ln">1677 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1678"><span class="ln">1678 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1679"><span class="ln">1679 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1680"><span class="ln">1680 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1681"><span class="ln">1681 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1682"><span class="ln">1682 </span></a><span class="s2">def </span><span class="s1">_foreach_addcdiv_</span><span class="s3">(</span>
<a name="l1683"><span class="ln">1683 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1684"><span class="ln">1684 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1685"><span class="ln">1685 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1686"><span class="ln">1686 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1687"><span class="ln">1687 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1688"><span class="ln">1688 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1689"><span class="ln">1689 </span></a><span class="s2">def </span><span class="s1">_foreach_addcmul</span><span class="s3">(</span>
<a name="l1690"><span class="ln">1690 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1691"><span class="ln">1691 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1692"><span class="ln">1692 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1693"><span class="ln">1693 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1694"><span class="ln">1694 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1695"><span class="ln">1695 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1696"><span class="ln">1696 </span></a><span class="s2">def </span><span class="s1">_foreach_addcmul</span><span class="s3">(</span>
<a name="l1697"><span class="ln">1697 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1698"><span class="ln">1698 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1699"><span class="ln">1699 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1700"><span class="ln">1700 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1701"><span class="ln">1701 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1702"><span class="ln">1702 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1703"><span class="ln">1703 </span></a><span class="s2">def </span><span class="s1">_foreach_addcmul</span><span class="s3">(</span>
<a name="l1704"><span class="ln">1704 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1705"><span class="ln">1705 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1706"><span class="ln">1706 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1707"><span class="ln">1707 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1708"><span class="ln">1708 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1709"><span class="ln">1709 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1710"><span class="ln">1710 </span></a><span class="s2">def </span><span class="s1">_foreach_addcmul_</span><span class="s3">(</span>
<a name="l1711"><span class="ln">1711 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1712"><span class="ln">1712 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1713"><span class="ln">1713 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1714"><span class="ln">1714 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1715"><span class="ln">1715 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1716"><span class="ln">1716 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1717"><span class="ln">1717 </span></a><span class="s2">def </span><span class="s1">_foreach_addcmul_</span><span class="s3">(</span>
<a name="l1718"><span class="ln">1718 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1719"><span class="ln">1719 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1720"><span class="ln">1720 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1721"><span class="ln">1721 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1722"><span class="ln">1722 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1723"><span class="ln">1723 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1724"><span class="ln">1724 </span></a><span class="s2">def </span><span class="s1">_foreach_addcmul_</span><span class="s3">(</span>
<a name="l1725"><span class="ln">1725 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1726"><span class="ln">1726 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1727"><span class="ln">1727 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1728"><span class="ln">1728 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l1729"><span class="ln">1729 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1730"><span class="ln">1730 </span></a><span class="s2">def </span><span class="s1">_foreach_asin</span><span class="s3">(</span>
<a name="l1731"><span class="ln">1731 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1732"><span class="ln">1732 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1733"><span class="ln">1733 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1734"><span class="ln">1734 </span></a>    _foreach_asin(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1735"><span class="ln">1735 </span></a> 
<a name="l1736"><span class="ln">1736 </span></a>    Apply :func:`torch.asin` to each Tensor of the input list. 
<a name="l1737"><span class="ln">1737 </span></a>    &quot;&quot;&quot;</span>
<a name="l1738"><span class="ln">1738 </span></a>
<a name="l1739"><span class="ln">1739 </span></a><span class="s2">def </span><span class="s1">_foreach_asin_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1740"><span class="ln">1740 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1741"><span class="ln">1741 </span></a>    _foreach_asin_(self: List[Tensor]) -&gt; None 
<a name="l1742"><span class="ln">1742 </span></a> 
<a name="l1743"><span class="ln">1743 </span></a>    Apply :func:`torch.asin` to each Tensor of the input list. 
<a name="l1744"><span class="ln">1744 </span></a>    &quot;&quot;&quot;</span>
<a name="l1745"><span class="ln">1745 </span></a>
<a name="l1746"><span class="ln">1746 </span></a><span class="s2">def </span><span class="s1">_foreach_atan</span><span class="s3">(</span>
<a name="l1747"><span class="ln">1747 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1748"><span class="ln">1748 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1749"><span class="ln">1749 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1750"><span class="ln">1750 </span></a>    _foreach_atan(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1751"><span class="ln">1751 </span></a> 
<a name="l1752"><span class="ln">1752 </span></a>    Apply :func:`torch.atan` to each Tensor of the input list. 
<a name="l1753"><span class="ln">1753 </span></a>    &quot;&quot;&quot;</span>
<a name="l1754"><span class="ln">1754 </span></a>
<a name="l1755"><span class="ln">1755 </span></a><span class="s2">def </span><span class="s1">_foreach_atan_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1756"><span class="ln">1756 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1757"><span class="ln">1757 </span></a>    _foreach_atan_(self: List[Tensor]) -&gt; None 
<a name="l1758"><span class="ln">1758 </span></a> 
<a name="l1759"><span class="ln">1759 </span></a>    Apply :func:`torch.atan` to each Tensor of the input list. 
<a name="l1760"><span class="ln">1760 </span></a>    &quot;&quot;&quot;</span>
<a name="l1761"><span class="ln">1761 </span></a>
<a name="l1762"><span class="ln">1762 </span></a><span class="s2">def </span><span class="s1">_foreach_ceil</span><span class="s3">(</span>
<a name="l1763"><span class="ln">1763 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1764"><span class="ln">1764 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1765"><span class="ln">1765 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1766"><span class="ln">1766 </span></a>    _foreach_ceil(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1767"><span class="ln">1767 </span></a> 
<a name="l1768"><span class="ln">1768 </span></a>    Apply :func:`torch.ceil` to each Tensor of the input list. 
<a name="l1769"><span class="ln">1769 </span></a>    &quot;&quot;&quot;</span>
<a name="l1770"><span class="ln">1770 </span></a>
<a name="l1771"><span class="ln">1771 </span></a><span class="s2">def </span><span class="s1">_foreach_ceil_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1772"><span class="ln">1772 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1773"><span class="ln">1773 </span></a>    _foreach_ceil_(self: List[Tensor]) -&gt; None 
<a name="l1774"><span class="ln">1774 </span></a> 
<a name="l1775"><span class="ln">1775 </span></a>    Apply :func:`torch.ceil` to each Tensor of the input list. 
<a name="l1776"><span class="ln">1776 </span></a>    &quot;&quot;&quot;</span>
<a name="l1777"><span class="ln">1777 </span></a>
<a name="l1778"><span class="ln">1778 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1779"><span class="ln">1779 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_max</span><span class="s3">(</span>
<a name="l1780"><span class="ln">1780 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1781"><span class="ln">1781 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1782"><span class="ln">1782 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1783"><span class="ln">1783 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1784"><span class="ln">1784 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_max</span><span class="s3">(</span>
<a name="l1785"><span class="ln">1785 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1786"><span class="ln">1786 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1787"><span class="ln">1787 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1788"><span class="ln">1788 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1789"><span class="ln">1789 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_max</span><span class="s3">(</span>
<a name="l1790"><span class="ln">1790 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1791"><span class="ln">1791 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1792"><span class="ln">1792 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1793"><span class="ln">1793 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1794"><span class="ln">1794 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_max_</span><span class="s3">(</span>
<a name="l1795"><span class="ln">1795 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1796"><span class="ln">1796 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1797"><span class="ln">1797 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1798"><span class="ln">1798 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1799"><span class="ln">1799 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_max_</span><span class="s3">(</span>
<a name="l1800"><span class="ln">1800 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1801"><span class="ln">1801 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1802"><span class="ln">1802 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1803"><span class="ln">1803 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1804"><span class="ln">1804 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_max_</span><span class="s3">(</span>
<a name="l1805"><span class="ln">1805 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1806"><span class="ln">1806 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1807"><span class="ln">1807 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1808"><span class="ln">1808 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1809"><span class="ln">1809 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_min</span><span class="s3">(</span>
<a name="l1810"><span class="ln">1810 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1811"><span class="ln">1811 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1812"><span class="ln">1812 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1813"><span class="ln">1813 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1814"><span class="ln">1814 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_min</span><span class="s3">(</span>
<a name="l1815"><span class="ln">1815 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1816"><span class="ln">1816 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1817"><span class="ln">1817 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1818"><span class="ln">1818 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1819"><span class="ln">1819 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_min</span><span class="s3">(</span>
<a name="l1820"><span class="ln">1820 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1821"><span class="ln">1821 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1822"><span class="ln">1822 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1823"><span class="ln">1823 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1824"><span class="ln">1824 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_min_</span><span class="s3">(</span>
<a name="l1825"><span class="ln">1825 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1826"><span class="ln">1826 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1827"><span class="ln">1827 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1828"><span class="ln">1828 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1829"><span class="ln">1829 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_min_</span><span class="s3">(</span>
<a name="l1830"><span class="ln">1830 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1831"><span class="ln">1831 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1832"><span class="ln">1832 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1833"><span class="ln">1833 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1834"><span class="ln">1834 </span></a><span class="s2">def </span><span class="s1">_foreach_clamp_min_</span><span class="s3">(</span>
<a name="l1835"><span class="ln">1835 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1836"><span class="ln">1836 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1837"><span class="ln">1837 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1838"><span class="ln">1838 </span></a><span class="s2">def </span><span class="s1">_foreach_copy_</span><span class="s3">(</span>
<a name="l1839"><span class="ln">1839 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1840"><span class="ln">1840 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1841"><span class="ln">1841 </span></a>    <span class="s1">non_blocking</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l1842"><span class="ln">1842 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1843"><span class="ln">1843 </span></a><span class="s2">def </span><span class="s1">_foreach_cos</span><span class="s3">(</span>
<a name="l1844"><span class="ln">1844 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1845"><span class="ln">1845 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1846"><span class="ln">1846 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1847"><span class="ln">1847 </span></a>    _foreach_cos(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1848"><span class="ln">1848 </span></a> 
<a name="l1849"><span class="ln">1849 </span></a>    Apply :func:`torch.cos` to each Tensor of the input list. 
<a name="l1850"><span class="ln">1850 </span></a>    &quot;&quot;&quot;</span>
<a name="l1851"><span class="ln">1851 </span></a>
<a name="l1852"><span class="ln">1852 </span></a><span class="s2">def </span><span class="s1">_foreach_cos_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1853"><span class="ln">1853 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1854"><span class="ln">1854 </span></a>    _foreach_cos_(self: List[Tensor]) -&gt; None 
<a name="l1855"><span class="ln">1855 </span></a> 
<a name="l1856"><span class="ln">1856 </span></a>    Apply :func:`torch.cos` to each Tensor of the input list. 
<a name="l1857"><span class="ln">1857 </span></a>    &quot;&quot;&quot;</span>
<a name="l1858"><span class="ln">1858 </span></a>
<a name="l1859"><span class="ln">1859 </span></a><span class="s2">def </span><span class="s1">_foreach_cosh</span><span class="s3">(</span>
<a name="l1860"><span class="ln">1860 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1861"><span class="ln">1861 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1862"><span class="ln">1862 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1863"><span class="ln">1863 </span></a>    _foreach_cosh(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1864"><span class="ln">1864 </span></a> 
<a name="l1865"><span class="ln">1865 </span></a>    Apply :func:`torch.cosh` to each Tensor of the input list. 
<a name="l1866"><span class="ln">1866 </span></a>    &quot;&quot;&quot;</span>
<a name="l1867"><span class="ln">1867 </span></a>
<a name="l1868"><span class="ln">1868 </span></a><span class="s2">def </span><span class="s1">_foreach_cosh_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1869"><span class="ln">1869 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1870"><span class="ln">1870 </span></a>    _foreach_cosh_(self: List[Tensor]) -&gt; None 
<a name="l1871"><span class="ln">1871 </span></a> 
<a name="l1872"><span class="ln">1872 </span></a>    Apply :func:`torch.cosh` to each Tensor of the input list. 
<a name="l1873"><span class="ln">1873 </span></a>    &quot;&quot;&quot;</span>
<a name="l1874"><span class="ln">1874 </span></a>
<a name="l1875"><span class="ln">1875 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1876"><span class="ln">1876 </span></a><span class="s2">def </span><span class="s1">_foreach_div</span><span class="s3">(</span>
<a name="l1877"><span class="ln">1877 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1878"><span class="ln">1878 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1879"><span class="ln">1879 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1880"><span class="ln">1880 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1881"><span class="ln">1881 </span></a><span class="s2">def </span><span class="s1">_foreach_div</span><span class="s3">(</span>
<a name="l1882"><span class="ln">1882 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1883"><span class="ln">1883 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1884"><span class="ln">1884 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1885"><span class="ln">1885 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1886"><span class="ln">1886 </span></a><span class="s2">def </span><span class="s1">_foreach_div</span><span class="s3">(</span>
<a name="l1887"><span class="ln">1887 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1888"><span class="ln">1888 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1889"><span class="ln">1889 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1890"><span class="ln">1890 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1891"><span class="ln">1891 </span></a><span class="s2">def </span><span class="s1">_foreach_div</span><span class="s3">(</span>
<a name="l1892"><span class="ln">1892 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1893"><span class="ln">1893 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1894"><span class="ln">1894 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l1895"><span class="ln">1895 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1896"><span class="ln">1896 </span></a><span class="s2">def </span><span class="s1">_foreach_div_</span><span class="s3">(</span>
<a name="l1897"><span class="ln">1897 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1898"><span class="ln">1898 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l1899"><span class="ln">1899 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1900"><span class="ln">1900 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1901"><span class="ln">1901 </span></a><span class="s2">def </span><span class="s1">_foreach_div_</span><span class="s3">(</span>
<a name="l1902"><span class="ln">1902 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1903"><span class="ln">1903 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l1904"><span class="ln">1904 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1905"><span class="ln">1905 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1906"><span class="ln">1906 </span></a><span class="s2">def </span><span class="s1">_foreach_div_</span><span class="s3">(</span>
<a name="l1907"><span class="ln">1907 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1908"><span class="ln">1908 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l1909"><span class="ln">1909 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1910"><span class="ln">1910 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l1911"><span class="ln">1911 </span></a><span class="s2">def </span><span class="s1">_foreach_div_</span><span class="s3">(</span>
<a name="l1912"><span class="ln">1912 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1913"><span class="ln">1913 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1914"><span class="ln">1914 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l1915"><span class="ln">1915 </span></a><span class="s2">def </span><span class="s1">_foreach_erf</span><span class="s3">(</span>
<a name="l1916"><span class="ln">1916 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1917"><span class="ln">1917 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1918"><span class="ln">1918 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1919"><span class="ln">1919 </span></a>    _foreach_erf(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1920"><span class="ln">1920 </span></a> 
<a name="l1921"><span class="ln">1921 </span></a>    Apply :func:`torch.erf` to each Tensor of the input list. 
<a name="l1922"><span class="ln">1922 </span></a>    &quot;&quot;&quot;</span>
<a name="l1923"><span class="ln">1923 </span></a>
<a name="l1924"><span class="ln">1924 </span></a><span class="s2">def </span><span class="s1">_foreach_erf_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1925"><span class="ln">1925 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1926"><span class="ln">1926 </span></a>    _foreach_erf_(self: List[Tensor]) -&gt; None 
<a name="l1927"><span class="ln">1927 </span></a> 
<a name="l1928"><span class="ln">1928 </span></a>    Apply :func:`torch.erf` to each Tensor of the input list. 
<a name="l1929"><span class="ln">1929 </span></a>    &quot;&quot;&quot;</span>
<a name="l1930"><span class="ln">1930 </span></a>
<a name="l1931"><span class="ln">1931 </span></a><span class="s2">def </span><span class="s1">_foreach_erfc</span><span class="s3">(</span>
<a name="l1932"><span class="ln">1932 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1933"><span class="ln">1933 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1934"><span class="ln">1934 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1935"><span class="ln">1935 </span></a>    _foreach_erfc(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1936"><span class="ln">1936 </span></a> 
<a name="l1937"><span class="ln">1937 </span></a>    Apply :func:`torch.erfc` to each Tensor of the input list. 
<a name="l1938"><span class="ln">1938 </span></a>    &quot;&quot;&quot;</span>
<a name="l1939"><span class="ln">1939 </span></a>
<a name="l1940"><span class="ln">1940 </span></a><span class="s2">def </span><span class="s1">_foreach_erfc_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1941"><span class="ln">1941 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1942"><span class="ln">1942 </span></a>    _foreach_erfc_(self: List[Tensor]) -&gt; None 
<a name="l1943"><span class="ln">1943 </span></a> 
<a name="l1944"><span class="ln">1944 </span></a>    Apply :func:`torch.erfc` to each Tensor of the input list. 
<a name="l1945"><span class="ln">1945 </span></a>    &quot;&quot;&quot;</span>
<a name="l1946"><span class="ln">1946 </span></a>
<a name="l1947"><span class="ln">1947 </span></a><span class="s2">def </span><span class="s1">_foreach_exp</span><span class="s3">(</span>
<a name="l1948"><span class="ln">1948 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1949"><span class="ln">1949 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1950"><span class="ln">1950 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1951"><span class="ln">1951 </span></a>    _foreach_exp(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1952"><span class="ln">1952 </span></a> 
<a name="l1953"><span class="ln">1953 </span></a>    Apply :func:`torch.exp` to each Tensor of the input list. 
<a name="l1954"><span class="ln">1954 </span></a>    &quot;&quot;&quot;</span>
<a name="l1955"><span class="ln">1955 </span></a>
<a name="l1956"><span class="ln">1956 </span></a><span class="s2">def </span><span class="s1">_foreach_exp_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1957"><span class="ln">1957 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1958"><span class="ln">1958 </span></a>    _foreach_exp_(self: List[Tensor]) -&gt; None 
<a name="l1959"><span class="ln">1959 </span></a> 
<a name="l1960"><span class="ln">1960 </span></a>    Apply :func:`torch.exp` to each Tensor of the input list. 
<a name="l1961"><span class="ln">1961 </span></a>    &quot;&quot;&quot;</span>
<a name="l1962"><span class="ln">1962 </span></a>
<a name="l1963"><span class="ln">1963 </span></a><span class="s2">def </span><span class="s1">_foreach_expm1</span><span class="s3">(</span>
<a name="l1964"><span class="ln">1964 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1965"><span class="ln">1965 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1966"><span class="ln">1966 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1967"><span class="ln">1967 </span></a>    _foreach_expm1(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1968"><span class="ln">1968 </span></a> 
<a name="l1969"><span class="ln">1969 </span></a>    Apply :func:`torch.expm1` to each Tensor of the input list. 
<a name="l1970"><span class="ln">1970 </span></a>    &quot;&quot;&quot;</span>
<a name="l1971"><span class="ln">1971 </span></a>
<a name="l1972"><span class="ln">1972 </span></a><span class="s2">def </span><span class="s1">_foreach_expm1_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1973"><span class="ln">1973 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1974"><span class="ln">1974 </span></a>    _foreach_expm1_(self: List[Tensor]) -&gt; None 
<a name="l1975"><span class="ln">1975 </span></a> 
<a name="l1976"><span class="ln">1976 </span></a>    Apply :func:`torch.expm1` to each Tensor of the input list. 
<a name="l1977"><span class="ln">1977 </span></a>    &quot;&quot;&quot;</span>
<a name="l1978"><span class="ln">1978 </span></a>
<a name="l1979"><span class="ln">1979 </span></a><span class="s2">def </span><span class="s1">_foreach_floor</span><span class="s3">(</span>
<a name="l1980"><span class="ln">1980 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1981"><span class="ln">1981 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1982"><span class="ln">1982 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1983"><span class="ln">1983 </span></a>    _foreach_floor(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l1984"><span class="ln">1984 </span></a> 
<a name="l1985"><span class="ln">1985 </span></a>    Apply :func:`torch.floor` to each Tensor of the input list. 
<a name="l1986"><span class="ln">1986 </span></a>    &quot;&quot;&quot;</span>
<a name="l1987"><span class="ln">1987 </span></a>
<a name="l1988"><span class="ln">1988 </span></a><span class="s2">def </span><span class="s1">_foreach_floor_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l1989"><span class="ln">1989 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1990"><span class="ln">1990 </span></a>    _foreach_floor_(self: List[Tensor]) -&gt; None 
<a name="l1991"><span class="ln">1991 </span></a> 
<a name="l1992"><span class="ln">1992 </span></a>    Apply :func:`torch.floor` to each Tensor of the input list. 
<a name="l1993"><span class="ln">1993 </span></a>    &quot;&quot;&quot;</span>
<a name="l1994"><span class="ln">1994 </span></a>
<a name="l1995"><span class="ln">1995 </span></a><span class="s2">def </span><span class="s1">_foreach_frac</span><span class="s3">(</span>
<a name="l1996"><span class="ln">1996 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l1997"><span class="ln">1997 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l1998"><span class="ln">1998 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l1999"><span class="ln">1999 </span></a>    _foreach_frac(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2000"><span class="ln">2000 </span></a> 
<a name="l2001"><span class="ln">2001 </span></a>    Apply :func:`torch.frac` to each Tensor of the input list. 
<a name="l2002"><span class="ln">2002 </span></a>    &quot;&quot;&quot;</span>
<a name="l2003"><span class="ln">2003 </span></a>
<a name="l2004"><span class="ln">2004 </span></a><span class="s2">def </span><span class="s1">_foreach_frac_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2005"><span class="ln">2005 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2006"><span class="ln">2006 </span></a>    _foreach_frac_(self: List[Tensor]) -&gt; None 
<a name="l2007"><span class="ln">2007 </span></a> 
<a name="l2008"><span class="ln">2008 </span></a>    Apply :func:`torch.frac` to each Tensor of the input list. 
<a name="l2009"><span class="ln">2009 </span></a>    &quot;&quot;&quot;</span>
<a name="l2010"><span class="ln">2010 </span></a>
<a name="l2011"><span class="ln">2011 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2012"><span class="ln">2012 </span></a><span class="s2">def </span><span class="s1">_foreach_lerp</span><span class="s3">(</span>
<a name="l2013"><span class="ln">2013 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2014"><span class="ln">2014 </span></a>    <span class="s1">tensors1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2015"><span class="ln">2015 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2016"><span class="ln">2016 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2017"><span class="ln">2017 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2018"><span class="ln">2018 </span></a><span class="s2">def </span><span class="s1">_foreach_lerp</span><span class="s3">(</span>
<a name="l2019"><span class="ln">2019 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2020"><span class="ln">2020 </span></a>    <span class="s1">tensors1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2021"><span class="ln">2021 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2022"><span class="ln">2022 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2023"><span class="ln">2023 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2024"><span class="ln">2024 </span></a><span class="s2">def </span><span class="s1">_foreach_lerp</span><span class="s3">(</span>
<a name="l2025"><span class="ln">2025 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2026"><span class="ln">2026 </span></a>    <span class="s1">tensors1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2027"><span class="ln">2027 </span></a>    <span class="s1">weights</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2028"><span class="ln">2028 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2029"><span class="ln">2029 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2030"><span class="ln">2030 </span></a><span class="s2">def </span><span class="s1">_foreach_lerp_</span><span class="s3">(</span>
<a name="l2031"><span class="ln">2031 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2032"><span class="ln">2032 </span></a>    <span class="s1">tensors1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2033"><span class="ln">2033 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2034"><span class="ln">2034 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2035"><span class="ln">2035 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2036"><span class="ln">2036 </span></a><span class="s2">def </span><span class="s1">_foreach_lerp_</span><span class="s3">(</span>
<a name="l2037"><span class="ln">2037 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2038"><span class="ln">2038 </span></a>    <span class="s1">tensors1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2039"><span class="ln">2039 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2040"><span class="ln">2040 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2041"><span class="ln">2041 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2042"><span class="ln">2042 </span></a><span class="s2">def </span><span class="s1">_foreach_lerp_</span><span class="s3">(</span>
<a name="l2043"><span class="ln">2043 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2044"><span class="ln">2044 </span></a>    <span class="s1">tensors1</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2045"><span class="ln">2045 </span></a>    <span class="s1">weights</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2046"><span class="ln">2046 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2047"><span class="ln">2047 </span></a><span class="s2">def </span><span class="s1">_foreach_lgamma</span><span class="s3">(</span>
<a name="l2048"><span class="ln">2048 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2049"><span class="ln">2049 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2050"><span class="ln">2050 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2051"><span class="ln">2051 </span></a>    _foreach_lgamma(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2052"><span class="ln">2052 </span></a> 
<a name="l2053"><span class="ln">2053 </span></a>    Apply :func:`torch.lgamma` to each Tensor of the input list. 
<a name="l2054"><span class="ln">2054 </span></a>    &quot;&quot;&quot;</span>
<a name="l2055"><span class="ln">2055 </span></a>
<a name="l2056"><span class="ln">2056 </span></a><span class="s2">def </span><span class="s1">_foreach_lgamma_</span><span class="s3">(</span>
<a name="l2057"><span class="ln">2057 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2058"><span class="ln">2058 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2059"><span class="ln">2059 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2060"><span class="ln">2060 </span></a>    _foreach_lgamma_(self: List[Tensor]) -&gt; None 
<a name="l2061"><span class="ln">2061 </span></a> 
<a name="l2062"><span class="ln">2062 </span></a>    Apply :func:`torch.lgamma` to each Tensor of the input list. 
<a name="l2063"><span class="ln">2063 </span></a>    &quot;&quot;&quot;</span>
<a name="l2064"><span class="ln">2064 </span></a>
<a name="l2065"><span class="ln">2065 </span></a><span class="s2">def </span><span class="s1">_foreach_log</span><span class="s3">(</span>
<a name="l2066"><span class="ln">2066 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2067"><span class="ln">2067 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2068"><span class="ln">2068 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2069"><span class="ln">2069 </span></a>    _foreach_log(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2070"><span class="ln">2070 </span></a> 
<a name="l2071"><span class="ln">2071 </span></a>    Apply :func:`torch.log` to each Tensor of the input list. 
<a name="l2072"><span class="ln">2072 </span></a>    &quot;&quot;&quot;</span>
<a name="l2073"><span class="ln">2073 </span></a>
<a name="l2074"><span class="ln">2074 </span></a><span class="s2">def </span><span class="s1">_foreach_log10</span><span class="s3">(</span>
<a name="l2075"><span class="ln">2075 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2076"><span class="ln">2076 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2077"><span class="ln">2077 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2078"><span class="ln">2078 </span></a>    _foreach_log10(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2079"><span class="ln">2079 </span></a> 
<a name="l2080"><span class="ln">2080 </span></a>    Apply :func:`torch.log10` to each Tensor of the input list. 
<a name="l2081"><span class="ln">2081 </span></a>    &quot;&quot;&quot;</span>
<a name="l2082"><span class="ln">2082 </span></a>
<a name="l2083"><span class="ln">2083 </span></a><span class="s2">def </span><span class="s1">_foreach_log10_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2084"><span class="ln">2084 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2085"><span class="ln">2085 </span></a>    _foreach_log10_(self: List[Tensor]) -&gt; None 
<a name="l2086"><span class="ln">2086 </span></a> 
<a name="l2087"><span class="ln">2087 </span></a>    Apply :func:`torch.log10` to each Tensor of the input list. 
<a name="l2088"><span class="ln">2088 </span></a>    &quot;&quot;&quot;</span>
<a name="l2089"><span class="ln">2089 </span></a>
<a name="l2090"><span class="ln">2090 </span></a><span class="s2">def </span><span class="s1">_foreach_log1p</span><span class="s3">(</span>
<a name="l2091"><span class="ln">2091 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2092"><span class="ln">2092 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2093"><span class="ln">2093 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2094"><span class="ln">2094 </span></a>    _foreach_log1p(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2095"><span class="ln">2095 </span></a> 
<a name="l2096"><span class="ln">2096 </span></a>    Apply :func:`torch.log1p` to each Tensor of the input list. 
<a name="l2097"><span class="ln">2097 </span></a>    &quot;&quot;&quot;</span>
<a name="l2098"><span class="ln">2098 </span></a>
<a name="l2099"><span class="ln">2099 </span></a><span class="s2">def </span><span class="s1">_foreach_log1p_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2100"><span class="ln">2100 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2101"><span class="ln">2101 </span></a>    _foreach_log1p_(self: List[Tensor]) -&gt; None 
<a name="l2102"><span class="ln">2102 </span></a> 
<a name="l2103"><span class="ln">2103 </span></a>    Apply :func:`torch.log1p` to each Tensor of the input list. 
<a name="l2104"><span class="ln">2104 </span></a>    &quot;&quot;&quot;</span>
<a name="l2105"><span class="ln">2105 </span></a>
<a name="l2106"><span class="ln">2106 </span></a><span class="s2">def </span><span class="s1">_foreach_log2</span><span class="s3">(</span>
<a name="l2107"><span class="ln">2107 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2108"><span class="ln">2108 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2109"><span class="ln">2109 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2110"><span class="ln">2110 </span></a>    _foreach_log2(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2111"><span class="ln">2111 </span></a> 
<a name="l2112"><span class="ln">2112 </span></a>    Apply :func:`torch.log2` to each Tensor of the input list. 
<a name="l2113"><span class="ln">2113 </span></a>    &quot;&quot;&quot;</span>
<a name="l2114"><span class="ln">2114 </span></a>
<a name="l2115"><span class="ln">2115 </span></a><span class="s2">def </span><span class="s1">_foreach_log2_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2116"><span class="ln">2116 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2117"><span class="ln">2117 </span></a>    _foreach_log2_(self: List[Tensor]) -&gt; None 
<a name="l2118"><span class="ln">2118 </span></a> 
<a name="l2119"><span class="ln">2119 </span></a>    Apply :func:`torch.log2` to each Tensor of the input list. 
<a name="l2120"><span class="ln">2120 </span></a>    &quot;&quot;&quot;</span>
<a name="l2121"><span class="ln">2121 </span></a>
<a name="l2122"><span class="ln">2122 </span></a><span class="s2">def </span><span class="s1">_foreach_log_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2123"><span class="ln">2123 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2124"><span class="ln">2124 </span></a>    _foreach_log_(self: List[Tensor]) -&gt; None 
<a name="l2125"><span class="ln">2125 </span></a> 
<a name="l2126"><span class="ln">2126 </span></a>    Apply :func:`torch.log` to each Tensor of the input list. 
<a name="l2127"><span class="ln">2127 </span></a>    &quot;&quot;&quot;</span>
<a name="l2128"><span class="ln">2128 </span></a>
<a name="l2129"><span class="ln">2129 </span></a><span class="s2">def </span><span class="s1">_foreach_max</span><span class="s3">(</span>
<a name="l2130"><span class="ln">2130 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2131"><span class="ln">2131 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2132"><span class="ln">2132 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2133"><span class="ln">2133 </span></a><span class="s2">def </span><span class="s1">_foreach_maximum</span><span class="s3">(</span>
<a name="l2134"><span class="ln">2134 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2135"><span class="ln">2135 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2136"><span class="ln">2136 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2137"><span class="ln">2137 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2138"><span class="ln">2138 </span></a><span class="s2">def </span><span class="s1">_foreach_maximum</span><span class="s3">(</span>
<a name="l2139"><span class="ln">2139 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2140"><span class="ln">2140 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2141"><span class="ln">2141 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2142"><span class="ln">2142 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2143"><span class="ln">2143 </span></a><span class="s2">def </span><span class="s1">_foreach_maximum</span><span class="s3">(</span>
<a name="l2144"><span class="ln">2144 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2145"><span class="ln">2145 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2146"><span class="ln">2146 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2147"><span class="ln">2147 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2148"><span class="ln">2148 </span></a><span class="s2">def </span><span class="s1">_foreach_maximum_</span><span class="s3">(</span>
<a name="l2149"><span class="ln">2149 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2150"><span class="ln">2150 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2151"><span class="ln">2151 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2152"><span class="ln">2152 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2153"><span class="ln">2153 </span></a><span class="s2">def </span><span class="s1">_foreach_maximum_</span><span class="s3">(</span>
<a name="l2154"><span class="ln">2154 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2155"><span class="ln">2155 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2156"><span class="ln">2156 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2157"><span class="ln">2157 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2158"><span class="ln">2158 </span></a><span class="s2">def </span><span class="s1">_foreach_maximum_</span><span class="s3">(</span>
<a name="l2159"><span class="ln">2159 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2160"><span class="ln">2160 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2161"><span class="ln">2161 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2162"><span class="ln">2162 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2163"><span class="ln">2163 </span></a><span class="s2">def </span><span class="s1">_foreach_minimum</span><span class="s3">(</span>
<a name="l2164"><span class="ln">2164 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2165"><span class="ln">2165 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2166"><span class="ln">2166 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2167"><span class="ln">2167 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2168"><span class="ln">2168 </span></a><span class="s2">def </span><span class="s1">_foreach_minimum</span><span class="s3">(</span>
<a name="l2169"><span class="ln">2169 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2170"><span class="ln">2170 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2171"><span class="ln">2171 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2172"><span class="ln">2172 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2173"><span class="ln">2173 </span></a><span class="s2">def </span><span class="s1">_foreach_minimum</span><span class="s3">(</span>
<a name="l2174"><span class="ln">2174 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2175"><span class="ln">2175 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2176"><span class="ln">2176 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2177"><span class="ln">2177 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2178"><span class="ln">2178 </span></a><span class="s2">def </span><span class="s1">_foreach_minimum_</span><span class="s3">(</span>
<a name="l2179"><span class="ln">2179 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2180"><span class="ln">2180 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2181"><span class="ln">2181 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2182"><span class="ln">2182 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2183"><span class="ln">2183 </span></a><span class="s2">def </span><span class="s1">_foreach_minimum_</span><span class="s3">(</span>
<a name="l2184"><span class="ln">2184 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2185"><span class="ln">2185 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2186"><span class="ln">2186 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2187"><span class="ln">2187 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2188"><span class="ln">2188 </span></a><span class="s2">def </span><span class="s1">_foreach_minimum_</span><span class="s3">(</span>
<a name="l2189"><span class="ln">2189 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2190"><span class="ln">2190 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2191"><span class="ln">2191 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2192"><span class="ln">2192 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2193"><span class="ln">2193 </span></a><span class="s2">def </span><span class="s1">_foreach_mul</span><span class="s3">(</span>
<a name="l2194"><span class="ln">2194 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2195"><span class="ln">2195 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2196"><span class="ln">2196 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2197"><span class="ln">2197 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2198"><span class="ln">2198 </span></a><span class="s2">def </span><span class="s1">_foreach_mul</span><span class="s3">(</span>
<a name="l2199"><span class="ln">2199 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2200"><span class="ln">2200 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2201"><span class="ln">2201 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2202"><span class="ln">2202 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2203"><span class="ln">2203 </span></a><span class="s2">def </span><span class="s1">_foreach_mul</span><span class="s3">(</span>
<a name="l2204"><span class="ln">2204 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2205"><span class="ln">2205 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2206"><span class="ln">2206 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2207"><span class="ln">2207 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2208"><span class="ln">2208 </span></a><span class="s2">def </span><span class="s1">_foreach_mul</span><span class="s3">(</span>
<a name="l2209"><span class="ln">2209 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2210"><span class="ln">2210 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2211"><span class="ln">2211 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2212"><span class="ln">2212 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2213"><span class="ln">2213 </span></a><span class="s2">def </span><span class="s1">_foreach_mul_</span><span class="s3">(</span>
<a name="l2214"><span class="ln">2214 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2215"><span class="ln">2215 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2216"><span class="ln">2216 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2217"><span class="ln">2217 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2218"><span class="ln">2218 </span></a><span class="s2">def </span><span class="s1">_foreach_mul_</span><span class="s3">(</span>
<a name="l2219"><span class="ln">2219 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2220"><span class="ln">2220 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2221"><span class="ln">2221 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2222"><span class="ln">2222 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2223"><span class="ln">2223 </span></a><span class="s2">def </span><span class="s1">_foreach_mul_</span><span class="s3">(</span>
<a name="l2224"><span class="ln">2224 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2225"><span class="ln">2225 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2226"><span class="ln">2226 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2227"><span class="ln">2227 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2228"><span class="ln">2228 </span></a><span class="s2">def </span><span class="s1">_foreach_mul_</span><span class="s3">(</span>
<a name="l2229"><span class="ln">2229 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2230"><span class="ln">2230 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2231"><span class="ln">2231 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2232"><span class="ln">2232 </span></a><span class="s2">def </span><span class="s1">_foreach_neg</span><span class="s3">(</span>
<a name="l2233"><span class="ln">2233 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2234"><span class="ln">2234 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2235"><span class="ln">2235 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2236"><span class="ln">2236 </span></a>    _foreach_neg(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2237"><span class="ln">2237 </span></a> 
<a name="l2238"><span class="ln">2238 </span></a>    Apply :func:`torch.neg` to each Tensor of the input list. 
<a name="l2239"><span class="ln">2239 </span></a>    &quot;&quot;&quot;</span>
<a name="l2240"><span class="ln">2240 </span></a>
<a name="l2241"><span class="ln">2241 </span></a><span class="s2">def </span><span class="s1">_foreach_neg_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2242"><span class="ln">2242 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2243"><span class="ln">2243 </span></a>    _foreach_neg_(self: List[Tensor]) -&gt; None 
<a name="l2244"><span class="ln">2244 </span></a> 
<a name="l2245"><span class="ln">2245 </span></a>    Apply :func:`torch.neg` to each Tensor of the input list. 
<a name="l2246"><span class="ln">2246 </span></a>    &quot;&quot;&quot;</span>
<a name="l2247"><span class="ln">2247 </span></a>
<a name="l2248"><span class="ln">2248 </span></a><span class="s2">def </span><span class="s1">_foreach_norm</span><span class="s3">(</span>
<a name="l2249"><span class="ln">2249 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2250"><span class="ln">2250 </span></a>    <span class="s1">ord</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">2</span><span class="s3">,</span>
<a name="l2251"><span class="ln">2251 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2252"><span class="ln">2252 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2253"><span class="ln">2253 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2254"><span class="ln">2254 </span></a><span class="s2">def </span><span class="s1">_foreach_pow</span><span class="s3">(</span>
<a name="l2255"><span class="ln">2255 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2256"><span class="ln">2256 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2257"><span class="ln">2257 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2258"><span class="ln">2258 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2259"><span class="ln">2259 </span></a><span class="s2">def </span><span class="s1">_foreach_pow</span><span class="s3">(</span>
<a name="l2260"><span class="ln">2260 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2261"><span class="ln">2261 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2262"><span class="ln">2262 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2263"><span class="ln">2263 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2264"><span class="ln">2264 </span></a><span class="s2">def </span><span class="s1">_foreach_pow</span><span class="s3">(</span>
<a name="l2265"><span class="ln">2265 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2266"><span class="ln">2266 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2267"><span class="ln">2267 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2268"><span class="ln">2268 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2269"><span class="ln">2269 </span></a><span class="s2">def </span><span class="s1">_foreach_pow</span><span class="s3">(</span>
<a name="l2270"><span class="ln">2270 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2271"><span class="ln">2271 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2272"><span class="ln">2272 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2273"><span class="ln">2273 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2274"><span class="ln">2274 </span></a><span class="s2">def </span><span class="s1">_foreach_pow_</span><span class="s3">(</span>
<a name="l2275"><span class="ln">2275 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2276"><span class="ln">2276 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2277"><span class="ln">2277 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2278"><span class="ln">2278 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2279"><span class="ln">2279 </span></a><span class="s2">def </span><span class="s1">_foreach_pow_</span><span class="s3">(</span>
<a name="l2280"><span class="ln">2280 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2281"><span class="ln">2281 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2282"><span class="ln">2282 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2283"><span class="ln">2283 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2284"><span class="ln">2284 </span></a><span class="s2">def </span><span class="s1">_foreach_pow_</span><span class="s3">(</span>
<a name="l2285"><span class="ln">2285 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2286"><span class="ln">2286 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2287"><span class="ln">2287 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2288"><span class="ln">2288 </span></a><span class="s2">def </span><span class="s1">_foreach_reciprocal</span><span class="s3">(</span>
<a name="l2289"><span class="ln">2289 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2290"><span class="ln">2290 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2291"><span class="ln">2291 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2292"><span class="ln">2292 </span></a>    _foreach_reciprocal(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2293"><span class="ln">2293 </span></a> 
<a name="l2294"><span class="ln">2294 </span></a>    Apply :func:`torch.reciprocal` to each Tensor of the input list. 
<a name="l2295"><span class="ln">2295 </span></a>    &quot;&quot;&quot;</span>
<a name="l2296"><span class="ln">2296 </span></a>
<a name="l2297"><span class="ln">2297 </span></a><span class="s2">def </span><span class="s1">_foreach_reciprocal_</span><span class="s3">(</span>
<a name="l2298"><span class="ln">2298 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2299"><span class="ln">2299 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2300"><span class="ln">2300 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2301"><span class="ln">2301 </span></a>    _foreach_reciprocal_(self: List[Tensor]) -&gt; None 
<a name="l2302"><span class="ln">2302 </span></a> 
<a name="l2303"><span class="ln">2303 </span></a>    Apply :func:`torch.reciprocal` to each Tensor of the input list. 
<a name="l2304"><span class="ln">2304 </span></a>    &quot;&quot;&quot;</span>
<a name="l2305"><span class="ln">2305 </span></a>
<a name="l2306"><span class="ln">2306 </span></a><span class="s2">def </span><span class="s1">_foreach_round</span><span class="s3">(</span>
<a name="l2307"><span class="ln">2307 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2308"><span class="ln">2308 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2309"><span class="ln">2309 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2310"><span class="ln">2310 </span></a>    _foreach_round(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2311"><span class="ln">2311 </span></a> 
<a name="l2312"><span class="ln">2312 </span></a>    Apply :func:`torch.round` to each Tensor of the input list. 
<a name="l2313"><span class="ln">2313 </span></a>    &quot;&quot;&quot;</span>
<a name="l2314"><span class="ln">2314 </span></a>
<a name="l2315"><span class="ln">2315 </span></a><span class="s2">def </span><span class="s1">_foreach_round_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2316"><span class="ln">2316 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2317"><span class="ln">2317 </span></a>    _foreach_round_(self: List[Tensor]) -&gt; None 
<a name="l2318"><span class="ln">2318 </span></a> 
<a name="l2319"><span class="ln">2319 </span></a>    Apply :func:`torch.round` to each Tensor of the input list. 
<a name="l2320"><span class="ln">2320 </span></a>    &quot;&quot;&quot;</span>
<a name="l2321"><span class="ln">2321 </span></a>
<a name="l2322"><span class="ln">2322 </span></a><span class="s2">def </span><span class="s1">_foreach_rsqrt</span><span class="s3">(</span>
<a name="l2323"><span class="ln">2323 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2324"><span class="ln">2324 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2325"><span class="ln">2325 </span></a><span class="s2">def </span><span class="s1">_foreach_rsqrt_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2326"><span class="ln">2326 </span></a><span class="s2">def </span><span class="s1">_foreach_sigmoid</span><span class="s3">(</span>
<a name="l2327"><span class="ln">2327 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2328"><span class="ln">2328 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2329"><span class="ln">2329 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2330"><span class="ln">2330 </span></a>    _foreach_sigmoid(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2331"><span class="ln">2331 </span></a> 
<a name="l2332"><span class="ln">2332 </span></a>    Apply :func:`torch.sigmoid` to each Tensor of the input list. 
<a name="l2333"><span class="ln">2333 </span></a>    &quot;&quot;&quot;</span>
<a name="l2334"><span class="ln">2334 </span></a>
<a name="l2335"><span class="ln">2335 </span></a><span class="s2">def </span><span class="s1">_foreach_sigmoid_</span><span class="s3">(</span>
<a name="l2336"><span class="ln">2336 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2337"><span class="ln">2337 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2338"><span class="ln">2338 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2339"><span class="ln">2339 </span></a>    _foreach_sigmoid_(self: List[Tensor]) -&gt; None 
<a name="l2340"><span class="ln">2340 </span></a> 
<a name="l2341"><span class="ln">2341 </span></a>    Apply :func:`torch.sigmoid` to each Tensor of the input list. 
<a name="l2342"><span class="ln">2342 </span></a>    &quot;&quot;&quot;</span>
<a name="l2343"><span class="ln">2343 </span></a>
<a name="l2344"><span class="ln">2344 </span></a><span class="s2">def </span><span class="s1">_foreach_sign</span><span class="s3">(</span>
<a name="l2345"><span class="ln">2345 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2346"><span class="ln">2346 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2347"><span class="ln">2347 </span></a><span class="s2">def </span><span class="s1">_foreach_sign_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2348"><span class="ln">2348 </span></a><span class="s2">def </span><span class="s1">_foreach_sin</span><span class="s3">(</span>
<a name="l2349"><span class="ln">2349 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2350"><span class="ln">2350 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2351"><span class="ln">2351 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2352"><span class="ln">2352 </span></a>    _foreach_sin(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2353"><span class="ln">2353 </span></a> 
<a name="l2354"><span class="ln">2354 </span></a>    Apply :func:`torch.sin` to each Tensor of the input list. 
<a name="l2355"><span class="ln">2355 </span></a>    &quot;&quot;&quot;</span>
<a name="l2356"><span class="ln">2356 </span></a>
<a name="l2357"><span class="ln">2357 </span></a><span class="s2">def </span><span class="s1">_foreach_sin_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2358"><span class="ln">2358 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2359"><span class="ln">2359 </span></a>    _foreach_sin_(self: List[Tensor]) -&gt; None 
<a name="l2360"><span class="ln">2360 </span></a> 
<a name="l2361"><span class="ln">2361 </span></a>    Apply :func:`torch.sin` to each Tensor of the input list. 
<a name="l2362"><span class="ln">2362 </span></a>    &quot;&quot;&quot;</span>
<a name="l2363"><span class="ln">2363 </span></a>
<a name="l2364"><span class="ln">2364 </span></a><span class="s2">def </span><span class="s1">_foreach_sinh</span><span class="s3">(</span>
<a name="l2365"><span class="ln">2365 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2366"><span class="ln">2366 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2367"><span class="ln">2367 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2368"><span class="ln">2368 </span></a>    _foreach_sinh(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2369"><span class="ln">2369 </span></a> 
<a name="l2370"><span class="ln">2370 </span></a>    Apply :func:`torch.sinh` to each Tensor of the input list. 
<a name="l2371"><span class="ln">2371 </span></a>    &quot;&quot;&quot;</span>
<a name="l2372"><span class="ln">2372 </span></a>
<a name="l2373"><span class="ln">2373 </span></a><span class="s2">def </span><span class="s1">_foreach_sinh_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2374"><span class="ln">2374 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2375"><span class="ln">2375 </span></a>    _foreach_sinh_(self: List[Tensor]) -&gt; None 
<a name="l2376"><span class="ln">2376 </span></a> 
<a name="l2377"><span class="ln">2377 </span></a>    Apply :func:`torch.sinh` to each Tensor of the input list. 
<a name="l2378"><span class="ln">2378 </span></a>    &quot;&quot;&quot;</span>
<a name="l2379"><span class="ln">2379 </span></a>
<a name="l2380"><span class="ln">2380 </span></a><span class="s2">def </span><span class="s1">_foreach_sqrt</span><span class="s3">(</span>
<a name="l2381"><span class="ln">2381 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2382"><span class="ln">2382 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2383"><span class="ln">2383 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2384"><span class="ln">2384 </span></a>    _foreach_sqrt(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2385"><span class="ln">2385 </span></a> 
<a name="l2386"><span class="ln">2386 </span></a>    Apply :func:`torch.sqrt` to each Tensor of the input list. 
<a name="l2387"><span class="ln">2387 </span></a>    &quot;&quot;&quot;</span>
<a name="l2388"><span class="ln">2388 </span></a>
<a name="l2389"><span class="ln">2389 </span></a><span class="s2">def </span><span class="s1">_foreach_sqrt_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2390"><span class="ln">2390 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2391"><span class="ln">2391 </span></a>    _foreach_sqrt_(self: List[Tensor]) -&gt; None 
<a name="l2392"><span class="ln">2392 </span></a> 
<a name="l2393"><span class="ln">2393 </span></a>    Apply :func:`torch.sqrt` to each Tensor of the input list. 
<a name="l2394"><span class="ln">2394 </span></a>    &quot;&quot;&quot;</span>
<a name="l2395"><span class="ln">2395 </span></a>
<a name="l2396"><span class="ln">2396 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2397"><span class="ln">2397 </span></a><span class="s2">def </span><span class="s1">_foreach_sub</span><span class="s3">(</span>
<a name="l2398"><span class="ln">2398 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2399"><span class="ln">2399 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2400"><span class="ln">2400 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2401"><span class="ln">2401 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2402"><span class="ln">2402 </span></a><span class="s2">def </span><span class="s1">_foreach_sub</span><span class="s3">(</span>
<a name="l2403"><span class="ln">2403 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2404"><span class="ln">2404 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2405"><span class="ln">2405 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2406"><span class="ln">2406 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l2407"><span class="ln">2407 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2408"><span class="ln">2408 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2409"><span class="ln">2409 </span></a><span class="s2">def </span><span class="s1">_foreach_sub</span><span class="s3">(</span>
<a name="l2410"><span class="ln">2410 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2411"><span class="ln">2411 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2412"><span class="ln">2412 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2413"><span class="ln">2413 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2414"><span class="ln">2414 </span></a><span class="s2">def </span><span class="s1">_foreach_sub_</span><span class="s3">(</span>
<a name="l2415"><span class="ln">2415 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2416"><span class="ln">2416 </span></a>    <span class="s1">scalars</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l2417"><span class="ln">2417 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2418"><span class="ln">2418 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2419"><span class="ln">2419 </span></a><span class="s2">def </span><span class="s1">_foreach_sub_</span><span class="s3">(</span>
<a name="l2420"><span class="ln">2420 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2421"><span class="ln">2421 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2422"><span class="ln">2422 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2423"><span class="ln">2423 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l2424"><span class="ln">2424 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2425"><span class="ln">2425 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2426"><span class="ln">2426 </span></a><span class="s2">def </span><span class="s1">_foreach_sub_</span><span class="s3">(</span>
<a name="l2427"><span class="ln">2427 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2428"><span class="ln">2428 </span></a>    <span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2429"><span class="ln">2429 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2430"><span class="ln">2430 </span></a><span class="s2">def </span><span class="s1">_foreach_tan</span><span class="s3">(</span>
<a name="l2431"><span class="ln">2431 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2432"><span class="ln">2432 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2433"><span class="ln">2433 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2434"><span class="ln">2434 </span></a>    _foreach_tan(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2435"><span class="ln">2435 </span></a> 
<a name="l2436"><span class="ln">2436 </span></a>    Apply :func:`torch.tan` to each Tensor of the input list. 
<a name="l2437"><span class="ln">2437 </span></a>    &quot;&quot;&quot;</span>
<a name="l2438"><span class="ln">2438 </span></a>
<a name="l2439"><span class="ln">2439 </span></a><span class="s2">def </span><span class="s1">_foreach_tan_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2440"><span class="ln">2440 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2441"><span class="ln">2441 </span></a>    _foreach_tan_(self: List[Tensor]) -&gt; None 
<a name="l2442"><span class="ln">2442 </span></a> 
<a name="l2443"><span class="ln">2443 </span></a>    Apply :func:`torch.tan` to each Tensor of the input list. 
<a name="l2444"><span class="ln">2444 </span></a>    &quot;&quot;&quot;</span>
<a name="l2445"><span class="ln">2445 </span></a>
<a name="l2446"><span class="ln">2446 </span></a><span class="s2">def </span><span class="s1">_foreach_tanh</span><span class="s3">(</span>
<a name="l2447"><span class="ln">2447 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2448"><span class="ln">2448 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2449"><span class="ln">2449 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2450"><span class="ln">2450 </span></a>    _foreach_tanh(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2451"><span class="ln">2451 </span></a> 
<a name="l2452"><span class="ln">2452 </span></a>    Apply :func:`torch.tanh` to each Tensor of the input list. 
<a name="l2453"><span class="ln">2453 </span></a>    &quot;&quot;&quot;</span>
<a name="l2454"><span class="ln">2454 </span></a>
<a name="l2455"><span class="ln">2455 </span></a><span class="s2">def </span><span class="s1">_foreach_tanh_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2456"><span class="ln">2456 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2457"><span class="ln">2457 </span></a>    _foreach_tanh_(self: List[Tensor]) -&gt; None 
<a name="l2458"><span class="ln">2458 </span></a> 
<a name="l2459"><span class="ln">2459 </span></a>    Apply :func:`torch.tanh` to each Tensor of the input list. 
<a name="l2460"><span class="ln">2460 </span></a>    &quot;&quot;&quot;</span>
<a name="l2461"><span class="ln">2461 </span></a>
<a name="l2462"><span class="ln">2462 </span></a><span class="s2">def </span><span class="s1">_foreach_trunc</span><span class="s3">(</span>
<a name="l2463"><span class="ln">2463 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2464"><span class="ln">2464 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l2465"><span class="ln">2465 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2466"><span class="ln">2466 </span></a>    _foreach_trunc(self: List[Tensor]) -&gt; List[Tensor] 
<a name="l2467"><span class="ln">2467 </span></a> 
<a name="l2468"><span class="ln">2468 </span></a>    Apply :func:`torch.trunc` to each Tensor of the input list. 
<a name="l2469"><span class="ln">2469 </span></a>    &quot;&quot;&quot;</span>
<a name="l2470"><span class="ln">2470 </span></a>
<a name="l2471"><span class="ln">2471 </span></a><span class="s2">def </span><span class="s1">_foreach_trunc_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2472"><span class="ln">2472 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2473"><span class="ln">2473 </span></a>    _foreach_trunc_(self: List[Tensor]) -&gt; None 
<a name="l2474"><span class="ln">2474 </span></a> 
<a name="l2475"><span class="ln">2475 </span></a>    Apply :func:`torch.trunc` to each Tensor of the input list. 
<a name="l2476"><span class="ln">2476 </span></a>    &quot;&quot;&quot;</span>
<a name="l2477"><span class="ln">2477 </span></a>
<a name="l2478"><span class="ln">2478 </span></a><span class="s2">def </span><span class="s1">_foreach_zero_</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l2479"><span class="ln">2479 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l2480"><span class="ln">2480 </span></a>    _foreach_zero_(self: List[Tensor]) -&gt; None 
<a name="l2481"><span class="ln">2481 </span></a> 
<a name="l2482"><span class="ln">2482 </span></a>    Apply :func:`torch.zero` to each Tensor of the input list. 
<a name="l2483"><span class="ln">2483 </span></a>    &quot;&quot;&quot;</span>
<a name="l2484"><span class="ln">2484 </span></a>
<a name="l2485"><span class="ln">2485 </span></a><span class="s2">def </span><span class="s1">_from_functional_tensor</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2486"><span class="ln">2486 </span></a><span class="s2">def </span><span class="s1">_functional_assert_async</span><span class="s3">(</span>
<a name="l2487"><span class="ln">2487 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2488"><span class="ln">2488 </span></a>    <span class="s1">assert_msg</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l2489"><span class="ln">2489 </span></a>    <span class="s1">dep_token</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2490"><span class="ln">2490 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2491"><span class="ln">2491 </span></a><span class="s2">def </span><span class="s1">_functional_assert_scalar</span><span class="s3">(</span>
<a name="l2492"><span class="ln">2492 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2493"><span class="ln">2493 </span></a>    <span class="s1">assert_msg</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l2494"><span class="ln">2494 </span></a>    <span class="s1">dep_token</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2495"><span class="ln">2495 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2496"><span class="ln">2496 </span></a><span class="s2">def </span><span class="s1">_functional_sym_constrain_range</span><span class="s3">(</span>
<a name="l2497"><span class="ln">2497 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2498"><span class="ln">2498 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2499"><span class="ln">2499 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2500"><span class="ln">2500 </span></a>    <span class="s1">dep_token</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2501"><span class="ln">2501 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2502"><span class="ln">2502 </span></a><span class="s2">def </span><span class="s1">_functional_sym_constrain_range_for_size</span><span class="s3">(</span>
<a name="l2503"><span class="ln">2503 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l2504"><span class="ln">2504 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2505"><span class="ln">2505 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2506"><span class="ln">2506 </span></a>    <span class="s1">dep_token</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2507"><span class="ln">2507 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2508"><span class="ln">2508 </span></a><span class="s2">def </span><span class="s1">_functionalize_apply_view_metas</span><span class="s3">(</span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">base</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2509"><span class="ln">2509 </span></a><span class="s2">def </span><span class="s1">_functionalize_are_all_mutations_hidden_from_autograd</span><span class="s3">(</span>
<a name="l2510"><span class="ln">2510 </span></a>    <span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2511"><span class="ln">2511 </span></a><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2512"><span class="ln">2512 </span></a><span class="s2">def </span><span class="s1">_functionalize_are_all_mutations_under_no_grad_or_inference_mode</span><span class="s3">(</span>
<a name="l2513"><span class="ln">2513 </span></a>    <span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2514"><span class="ln">2514 </span></a><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2515"><span class="ln">2515 </span></a><span class="s2">def </span><span class="s1">_functionalize_commit_update</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2516"><span class="ln">2516 </span></a><span class="s2">def </span><span class="s1">_functionalize_has_metadata_mutation</span><span class="s3">(</span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2517"><span class="ln">2517 </span></a><span class="s2">def </span><span class="s1">_functionalize_is_symbolic</span><span class="s3">(</span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2518"><span class="ln">2518 </span></a><span class="s2">def </span><span class="s1">_functionalize_mark_mutation_hidden_from_autograd</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2519"><span class="ln">2519 </span></a><span class="s2">def </span><span class="s1">_functionalize_replace</span><span class="s3">(</span><span class="s1">self_</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2520"><span class="ln">2520 </span></a><span class="s2">def </span><span class="s1">_functionalize_set_storage_changed</span><span class="s3">(</span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2521"><span class="ln">2521 </span></a><span class="s2">def </span><span class="s1">_functionalize_sync</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2522"><span class="ln">2522 </span></a><span class="s2">def </span><span class="s1">_functionalize_unsafe_set</span><span class="s3">(</span><span class="s1">dst</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2523"><span class="ln">2523 </span></a><span class="s2">def </span><span class="s1">_functionalize_was_inductor_storage_resized</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2524"><span class="ln">2524 </span></a><span class="s2">def </span><span class="s1">_functionalize_was_storage_changed</span><span class="s3">(</span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2525"><span class="ln">2525 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2526"><span class="ln">2526 </span></a><span class="s2">def </span><span class="s1">_fused_adagrad_</span><span class="s3">(</span>
<a name="l2527"><span class="ln">2527 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2528"><span class="ln">2528 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2529"><span class="ln">2529 </span></a>    <span class="s1">state_sums</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2530"><span class="ln">2530 </span></a>    <span class="s1">state_steps</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2531"><span class="ln">2531 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2532"><span class="ln">2532 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2533"><span class="ln">2533 </span></a>    <span class="s1">lr_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2534"><span class="ln">2534 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2535"><span class="ln">2535 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2536"><span class="ln">2536 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2537"><span class="ln">2537 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2538"><span class="ln">2538 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2539"><span class="ln">2539 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2540"><span class="ln">2540 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2541"><span class="ln">2541 </span></a><span class="s2">def </span><span class="s1">_fused_adagrad_</span><span class="s3">(</span>
<a name="l2542"><span class="ln">2542 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2543"><span class="ln">2543 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2544"><span class="ln">2544 </span></a>    <span class="s1">state_sums</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2545"><span class="ln">2545 </span></a>    <span class="s1">state_steps</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2546"><span class="ln">2546 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2547"><span class="ln">2547 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2548"><span class="ln">2548 </span></a>    <span class="s1">lr_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2549"><span class="ln">2549 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2550"><span class="ln">2550 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2551"><span class="ln">2551 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2552"><span class="ln">2552 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2553"><span class="ln">2553 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2554"><span class="ln">2554 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2555"><span class="ln">2555 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2556"><span class="ln">2556 </span></a><span class="s2">def </span><span class="s1">_fused_adam_</span><span class="s3">(</span>
<a name="l2557"><span class="ln">2557 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2558"><span class="ln">2558 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2559"><span class="ln">2559 </span></a>    <span class="s1">exp_avgs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2560"><span class="ln">2560 </span></a>    <span class="s1">exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2561"><span class="ln">2561 </span></a>    <span class="s1">max_exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2562"><span class="ln">2562 </span></a>    <span class="s1">state_steps</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2563"><span class="ln">2563 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2564"><span class="ln">2564 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2565"><span class="ln">2565 </span></a>    <span class="s1">beta1</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2566"><span class="ln">2566 </span></a>    <span class="s1">beta2</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2567"><span class="ln">2567 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2568"><span class="ln">2568 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2569"><span class="ln">2569 </span></a>    <span class="s1">amsgrad</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2570"><span class="ln">2570 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2571"><span class="ln">2571 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2572"><span class="ln">2572 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2573"><span class="ln">2573 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2574"><span class="ln">2574 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2575"><span class="ln">2575 </span></a><span class="s2">def </span><span class="s1">_fused_adam_</span><span class="s3">(</span>
<a name="l2576"><span class="ln">2576 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2577"><span class="ln">2577 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2578"><span class="ln">2578 </span></a>    <span class="s1">exp_avgs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2579"><span class="ln">2579 </span></a>    <span class="s1">exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2580"><span class="ln">2580 </span></a>    <span class="s1">max_exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2581"><span class="ln">2581 </span></a>    <span class="s1">state_steps</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2582"><span class="ln">2582 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2583"><span class="ln">2583 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2584"><span class="ln">2584 </span></a>    <span class="s1">beta1</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2585"><span class="ln">2585 </span></a>    <span class="s1">beta2</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2586"><span class="ln">2586 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2587"><span class="ln">2587 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2588"><span class="ln">2588 </span></a>    <span class="s1">amsgrad</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2589"><span class="ln">2589 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2590"><span class="ln">2590 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2591"><span class="ln">2591 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2592"><span class="ln">2592 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2593"><span class="ln">2593 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2594"><span class="ln">2594 </span></a><span class="s2">def </span><span class="s1">_fused_adamw_</span><span class="s3">(</span>
<a name="l2595"><span class="ln">2595 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2596"><span class="ln">2596 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2597"><span class="ln">2597 </span></a>    <span class="s1">exp_avgs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2598"><span class="ln">2598 </span></a>    <span class="s1">exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2599"><span class="ln">2599 </span></a>    <span class="s1">max_exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2600"><span class="ln">2600 </span></a>    <span class="s1">state_steps</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2601"><span class="ln">2601 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2602"><span class="ln">2602 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2603"><span class="ln">2603 </span></a>    <span class="s1">beta1</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2604"><span class="ln">2604 </span></a>    <span class="s1">beta2</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2605"><span class="ln">2605 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2606"><span class="ln">2606 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2607"><span class="ln">2607 </span></a>    <span class="s1">amsgrad</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2608"><span class="ln">2608 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2609"><span class="ln">2609 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2610"><span class="ln">2610 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2611"><span class="ln">2611 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2612"><span class="ln">2612 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2613"><span class="ln">2613 </span></a><span class="s2">def </span><span class="s1">_fused_adamw_</span><span class="s3">(</span>
<a name="l2614"><span class="ln">2614 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2615"><span class="ln">2615 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2616"><span class="ln">2616 </span></a>    <span class="s1">exp_avgs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2617"><span class="ln">2617 </span></a>    <span class="s1">exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2618"><span class="ln">2618 </span></a>    <span class="s1">max_exp_avg_sqs</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2619"><span class="ln">2619 </span></a>    <span class="s1">state_steps</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2620"><span class="ln">2620 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2621"><span class="ln">2621 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2622"><span class="ln">2622 </span></a>    <span class="s1">beta1</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2623"><span class="ln">2623 </span></a>    <span class="s1">beta2</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2624"><span class="ln">2624 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2625"><span class="ln">2625 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2626"><span class="ln">2626 </span></a>    <span class="s1">amsgrad</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2627"><span class="ln">2627 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2628"><span class="ln">2628 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2629"><span class="ln">2629 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2630"><span class="ln">2630 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2631"><span class="ln">2631 </span></a><span class="s2">def </span><span class="s1">_fused_dropout</span><span class="s3">(</span>
<a name="l2632"><span class="ln">2632 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2633"><span class="ln">2633 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2634"><span class="ln">2634 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2635"><span class="ln">2635 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2636"><span class="ln">2636 </span></a><span class="s2">def </span><span class="s1">_fused_moving_avg_obs_fq_helper</span><span class="s3">(</span>
<a name="l2637"><span class="ln">2637 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2638"><span class="ln">2638 </span></a>    <span class="s1">observer_on</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2639"><span class="ln">2639 </span></a>    <span class="s1">fake_quant_on</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2640"><span class="ln">2640 </span></a>    <span class="s1">running_min</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2641"><span class="ln">2641 </span></a>    <span class="s1">running_max</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2642"><span class="ln">2642 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2643"><span class="ln">2643 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2644"><span class="ln">2644 </span></a>    <span class="s1">averaging_const</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2645"><span class="ln">2645 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2646"><span class="ln">2646 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2647"><span class="ln">2647 </span></a>    <span class="s1">ch_axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2648"><span class="ln">2648 </span></a>    <span class="s1">per_row_fake_quant</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2649"><span class="ln">2649 </span></a>    <span class="s1">symmetric_quant</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2650"><span class="ln">2650 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_fused_moving_avg_obs_fq_helper</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2651"><span class="ln">2651 </span></a><span class="s2">def </span><span class="s1">_fused_rms_norm</span><span class="s3">(</span>
<a name="l2652"><span class="ln">2652 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2653"><span class="ln">2653 </span></a>    <span class="s1">normalized_shape_ndim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2654"><span class="ln">2654 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2655"><span class="ln">2655 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2656"><span class="ln">2656 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2657"><span class="ln">2657 </span></a><span class="s2">def </span><span class="s1">_fused_sdp_choice</span><span class="s3">(</span>
<a name="l2658"><span class="ln">2658 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2659"><span class="ln">2659 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2660"><span class="ln">2660 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2661"><span class="ln">2661 </span></a>    <span class="s1">attn_mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2662"><span class="ln">2662 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l2663"><span class="ln">2663 </span></a>    <span class="s1">is_causal</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2664"><span class="ln">2664 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2665"><span class="ln">2665 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2666"><span class="ln">2666 </span></a>    <span class="s1">enable_gqa</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2667"><span class="ln">2667 </span></a><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2668"><span class="ln">2668 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2669"><span class="ln">2669 </span></a><span class="s2">def </span><span class="s1">_fused_sgd_</span><span class="s3">(</span>
<a name="l2670"><span class="ln">2670 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2671"><span class="ln">2671 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2672"><span class="ln">2672 </span></a>    <span class="s1">momentum_buffer_list</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2673"><span class="ln">2673 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2674"><span class="ln">2674 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2675"><span class="ln">2675 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2676"><span class="ln">2676 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2677"><span class="ln">2677 </span></a>    <span class="s1">dampening</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2678"><span class="ln">2678 </span></a>    <span class="s1">nesterov</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2679"><span class="ln">2679 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2680"><span class="ln">2680 </span></a>    <span class="s1">is_first_step</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2681"><span class="ln">2681 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2682"><span class="ln">2682 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2683"><span class="ln">2683 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2684"><span class="ln">2684 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2685"><span class="ln">2685 </span></a><span class="s2">def </span><span class="s1">_fused_sgd_</span><span class="s3">(</span>
<a name="l2686"><span class="ln">2686 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2687"><span class="ln">2687 </span></a>    <span class="s1">grads</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2688"><span class="ln">2688 </span></a>    <span class="s1">momentum_buffer_list</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2689"><span class="ln">2689 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2690"><span class="ln">2690 </span></a>    <span class="s1">weight_decay</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2691"><span class="ln">2691 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2692"><span class="ln">2692 </span></a>    <span class="s1">lr</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2693"><span class="ln">2693 </span></a>    <span class="s1">dampening</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2694"><span class="ln">2694 </span></a>    <span class="s1">nesterov</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2695"><span class="ln">2695 </span></a>    <span class="s1">maximize</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2696"><span class="ln">2696 </span></a>    <span class="s1">is_first_step</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2697"><span class="ln">2697 </span></a>    <span class="s1">grad_scale</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2698"><span class="ln">2698 </span></a>    <span class="s1">found_inf</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2699"><span class="ln">2699 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2700"><span class="ln">2700 </span></a><span class="s2">def </span><span class="s1">_fw_primal_copy</span><span class="s3">(</span>
<a name="l2701"><span class="ln">2701 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2702"><span class="ln">2702 </span></a>    <span class="s1">level</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2703"><span class="ln">2703 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2704"><span class="ln">2704 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2705"><span class="ln">2705 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2706"><span class="ln">2706 </span></a><span class="s2">def </span><span class="s1">_grid_sampler_2d_cpu_fallback</span><span class="s3">(</span>
<a name="l2707"><span class="ln">2707 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2708"><span class="ln">2708 </span></a>    <span class="s1">grid</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2709"><span class="ln">2709 </span></a>    <span class="s1">interpolation_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2710"><span class="ln">2710 </span></a>    <span class="s1">padding_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2711"><span class="ln">2711 </span></a>    <span class="s1">align_corners</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2712"><span class="ln">2712 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2713"><span class="ln">2713 </span></a><span class="s2">def </span><span class="s1">_grouped_mm</span><span class="s3">(</span>
<a name="l2714"><span class="ln">2714 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2715"><span class="ln">2715 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2716"><span class="ln">2716 </span></a>    <span class="s1">offs</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2717"><span class="ln">2717 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2718"><span class="ln">2718 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2719"><span class="ln">2719 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2720"><span class="ln">2720 </span></a><span class="s2">def </span><span class="s1">_has_compatible_shallow_copy_type</span><span class="s3">(</span>
<a name="l2721"><span class="ln">2721 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2722"><span class="ln">2722 </span></a>    <span class="s1">from_</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2723"><span class="ln">2723 </span></a><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2724"><span class="ln">2724 </span></a><span class="s2">def </span><span class="s1">_histogramdd_bin_edges</span><span class="s3">(</span>
<a name="l2725"><span class="ln">2725 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2726"><span class="ln">2726 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l2727"><span class="ln">2727 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2728"><span class="ln">2728 </span></a>    <span class="s1">range</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_float</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2729"><span class="ln">2729 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2730"><span class="ln">2730 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2731"><span class="ln">2731 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2732"><span class="ln">2732 </span></a><span class="s2">def </span><span class="s1">_histogramdd_from_bin_cts</span><span class="s3">(</span>
<a name="l2733"><span class="ln">2733 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2734"><span class="ln">2734 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l2735"><span class="ln">2735 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2736"><span class="ln">2736 </span></a>    <span class="s1">range</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_float</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2737"><span class="ln">2737 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2738"><span class="ln">2738 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2739"><span class="ln">2739 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2740"><span class="ln">2740 </span></a><span class="s2">def </span><span class="s1">_histogramdd_from_bin_tensors</span><span class="s3">(</span>
<a name="l2741"><span class="ln">2741 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2742"><span class="ln">2742 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2743"><span class="ln">2743 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2744"><span class="ln">2744 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2745"><span class="ln">2745 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2746"><span class="ln">2746 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2747"><span class="ln">2747 </span></a><span class="s2">def </span><span class="s1">_index_put_impl_</span><span class="s3">(</span>
<a name="l2748"><span class="ln">2748 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2749"><span class="ln">2749 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2750"><span class="ln">2750 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2751"><span class="ln">2751 </span></a>    <span class="s1">accumulate</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2752"><span class="ln">2752 </span></a>    <span class="s1">unsafe</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2753"><span class="ln">2753 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2754"><span class="ln">2754 </span></a><span class="s2">def </span><span class="s1">_indices_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2755"><span class="ln">2755 </span></a><span class="s2">def </span><span class="s1">_int_mm</span><span class="s3">(</span>
<a name="l2756"><span class="ln">2756 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2757"><span class="ln">2757 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2758"><span class="ln">2758 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2759"><span class="ln">2759 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2760"><span class="ln">2760 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2761"><span class="ln">2761 </span></a><span class="s2">def </span><span class="s1">_is_all_true</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2762"><span class="ln">2762 </span></a><span class="s2">def </span><span class="s1">_is_any_true</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2763"><span class="ln">2763 </span></a><span class="s2">def </span><span class="s1">_is_functional_tensor</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2764"><span class="ln">2764 </span></a><span class="s2">def </span><span class="s1">_is_functional_tensor_base</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2765"><span class="ln">2765 </span></a><span class="s2">def </span><span class="s1">_is_zerotensor</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2766"><span class="ln">2766 </span></a><span class="s2">def </span><span class="s1">_lazy_clone</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2767"><span class="ln">2767 </span></a><span class="s2">def </span><span class="s1">_linalg_check_errors</span><span class="s3">(</span>
<a name="l2768"><span class="ln">2768 </span></a>    <span class="s1">info</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2769"><span class="ln">2769 </span></a>    <span class="s1">api_name</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l2770"><span class="ln">2770 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2771"><span class="ln">2771 </span></a>    <span class="s1">is_matrix</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2772"><span class="ln">2772 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l2773"><span class="ln">2773 </span></a><span class="s2">def </span><span class="s1">_linalg_det</span><span class="s3">(</span>
<a name="l2774"><span class="ln">2774 </span></a>    <span class="s1">A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2775"><span class="ln">2775 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2776"><span class="ln">2776 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2777"><span class="ln">2777 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_linalg_det</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2778"><span class="ln">2778 </span></a><span class="s2">def </span><span class="s1">_linalg_eigh</span><span class="s3">(</span>
<a name="l2779"><span class="ln">2779 </span></a>    <span class="s1">A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2780"><span class="ln">2780 </span></a>    <span class="s1">UPLO</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;L&quot;</span><span class="s3">,</span>
<a name="l2781"><span class="ln">2781 </span></a>    <span class="s1">compute_v</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2782"><span class="ln">2782 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2783"><span class="ln">2783 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2784"><span class="ln">2784 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_linalg_eigh</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2785"><span class="ln">2785 </span></a><span class="s2">def </span><span class="s1">_linalg_slogdet</span><span class="s3">(</span>
<a name="l2786"><span class="ln">2786 </span></a>    <span class="s1">A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2787"><span class="ln">2787 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2788"><span class="ln">2788 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2789"><span class="ln">2789 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_linalg_slogdet</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2790"><span class="ln">2790 </span></a><span class="s2">def </span><span class="s1">_linalg_solve_ex</span><span class="s3">(</span>
<a name="l2791"><span class="ln">2791 </span></a>    <span class="s1">A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2792"><span class="ln">2792 </span></a>    <span class="s1">B</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2793"><span class="ln">2793 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2794"><span class="ln">2794 </span></a>    <span class="s1">left</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2795"><span class="ln">2795 </span></a>    <span class="s1">check_errors</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2796"><span class="ln">2796 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2797"><span class="ln">2797 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_linalg_solve_ex</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2798"><span class="ln">2798 </span></a><span class="s2">def </span><span class="s1">_linalg_svd</span><span class="s3">(</span>
<a name="l2799"><span class="ln">2799 </span></a>    <span class="s1">A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2800"><span class="ln">2800 </span></a>    <span class="s1">full_matrices</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2801"><span class="ln">2801 </span></a>    <span class="s1">compute_uv</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2802"><span class="ln">2802 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2803"><span class="ln">2803 </span></a>    <span class="s1">driver</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2804"><span class="ln">2804 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2805"><span class="ln">2805 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_linalg_svd</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2806"><span class="ln">2806 </span></a><span class="s2">def </span><span class="s1">_log_softmax</span><span class="s3">(</span>
<a name="l2807"><span class="ln">2807 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2808"><span class="ln">2808 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2809"><span class="ln">2809 </span></a>    <span class="s1">half_to_float</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2810"><span class="ln">2810 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2811"><span class="ln">2811 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2812"><span class="ln">2812 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2813"><span class="ln">2813 </span></a><span class="s2">def </span><span class="s1">_log_softmax_backward_data</span><span class="s3">(</span>
<a name="l2814"><span class="ln">2814 </span></a>    <span class="s1">grad_output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2815"><span class="ln">2815 </span></a>    <span class="s1">output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2816"><span class="ln">2816 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2817"><span class="ln">2817 </span></a>    <span class="s1">input_dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l2818"><span class="ln">2818 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2819"><span class="ln">2819 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2820"><span class="ln">2820 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2821"><span class="ln">2821 </span></a><span class="s2">def </span><span class="s1">_logcumsumexp</span><span class="s3">(</span>
<a name="l2822"><span class="ln">2822 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2823"><span class="ln">2823 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2824"><span class="ln">2824 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2825"><span class="ln">2825 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2826"><span class="ln">2826 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2827"><span class="ln">2827 </span></a><span class="s2">def </span><span class="s1">_lstm_mps</span><span class="s3">(</span>
<a name="l2828"><span class="ln">2828 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2829"><span class="ln">2829 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2830"><span class="ln">2830 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2831"><span class="ln">2831 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2832"><span class="ln">2832 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2833"><span class="ln">2833 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2834"><span class="ln">2834 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2835"><span class="ln">2835 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2836"><span class="ln">2836 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2837"><span class="ln">2837 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2838"><span class="ln">2838 </span></a><span class="s2">def </span><span class="s1">_lu_with_info</span><span class="s3">(</span>
<a name="l2839"><span class="ln">2839 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2840"><span class="ln">2840 </span></a>    <span class="s1">pivot</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2841"><span class="ln">2841 </span></a>    <span class="s1">check_errors</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2842"><span class="ln">2842 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_lu_with_info</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2843"><span class="ln">2843 </span></a><span class="s2">def </span><span class="s1">_make_dep_token</span><span class="s3">(</span>
<a name="l2844"><span class="ln">2844 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2845"><span class="ln">2845 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2846"><span class="ln">2846 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2847"><span class="ln">2847 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2848"><span class="ln">2848 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2849"><span class="ln">2849 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l2850"><span class="ln">2850 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l2851"><span class="ln">2851 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2852"><span class="ln">2852 </span></a><span class="s2">def </span><span class="s1">_make_dual</span><span class="s3">(</span><span class="s1">primal</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">tangent</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">level</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2853"><span class="ln">2853 </span></a><span class="s2">def </span><span class="s1">_make_dual_copy</span><span class="s3">(</span>
<a name="l2854"><span class="ln">2854 </span></a>    <span class="s1">primal</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2855"><span class="ln">2855 </span></a>    <span class="s1">tangent</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2856"><span class="ln">2856 </span></a>    <span class="s1">level</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2857"><span class="ln">2857 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2858"><span class="ln">2858 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2859"><span class="ln">2859 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2860"><span class="ln">2860 </span></a><span class="s2">def </span><span class="s1">_make_per_channel_quantized_tensor</span><span class="s3">(</span>
<a name="l2861"><span class="ln">2861 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2862"><span class="ln">2862 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2863"><span class="ln">2863 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2864"><span class="ln">2864 </span></a>    <span class="s1">axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2865"><span class="ln">2865 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2866"><span class="ln">2866 </span></a><span class="s2">def </span><span class="s1">_make_per_tensor_quantized_tensor</span><span class="s3">(</span>
<a name="l2867"><span class="ln">2867 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2868"><span class="ln">2868 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2869"><span class="ln">2869 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2870"><span class="ln">2870 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2871"><span class="ln">2871 </span></a><span class="s2">def </span><span class="s1">_masked_scale</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">scale</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2872"><span class="ln">2872 </span></a><span class="s2">def </span><span class="s1">_masked_softmax</span><span class="s3">(</span>
<a name="l2873"><span class="ln">2873 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2874"><span class="ln">2874 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2875"><span class="ln">2875 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2876"><span class="ln">2876 </span></a>    <span class="s1">mask_type</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2877"><span class="ln">2877 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2878"><span class="ln">2878 </span></a><span class="s2">def </span><span class="s1">_mixed_dtypes_linear</span><span class="s3">(</span>
<a name="l2879"><span class="ln">2879 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2880"><span class="ln">2880 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2881"><span class="ln">2881 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2882"><span class="ln">2882 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2883"><span class="ln">2883 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2884"><span class="ln">2884 </span></a>    <span class="s1">activation</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2885"><span class="ln">2885 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2886"><span class="ln">2886 </span></a><span class="s2">def </span><span class="s1">_mkldnn_reshape</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">shape</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2887"><span class="ln">2887 </span></a><span class="s2">def </span><span class="s1">_mkldnn_transpose</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2888"><span class="ln">2888 </span></a><span class="s2">def </span><span class="s1">_mkldnn_transpose_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2889"><span class="ln">2889 </span></a><span class="s2">def </span><span class="s1">_mps_convolution</span><span class="s3">(</span>
<a name="l2890"><span class="ln">2890 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2891"><span class="ln">2891 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2892"><span class="ln">2892 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2893"><span class="ln">2893 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l2894"><span class="ln">2894 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l2895"><span class="ln">2895 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l2896"><span class="ln">2896 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l2897"><span class="ln">2897 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2898"><span class="ln">2898 </span></a><span class="s2">def </span><span class="s1">_mps_convolution_transpose</span><span class="s3">(</span>
<a name="l2899"><span class="ln">2899 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2900"><span class="ln">2900 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2901"><span class="ln">2901 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l2902"><span class="ln">2902 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l2903"><span class="ln">2903 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l2904"><span class="ln">2904 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l2905"><span class="ln">2905 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l2906"><span class="ln">2906 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2907"><span class="ln">2907 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2908"><span class="ln">2908 </span></a><span class="s2">def </span><span class="s1">_native_batch_norm_legit</span><span class="s3">(</span>
<a name="l2909"><span class="ln">2909 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2910"><span class="ln">2910 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2911"><span class="ln">2911 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2912"><span class="ln">2912 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2913"><span class="ln">2913 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2914"><span class="ln">2914 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2915"><span class="ln">2915 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2916"><span class="ln">2916 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2917"><span class="ln">2917 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2918"><span class="ln">2918 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2919"><span class="ln">2919 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2920"><span class="ln">2920 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l2921"><span class="ln">2921 </span></a><span class="s2">def </span><span class="s1">_native_batch_norm_legit</span><span class="s3">(</span>
<a name="l2922"><span class="ln">2922 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2923"><span class="ln">2923 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2924"><span class="ln">2924 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2925"><span class="ln">2925 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l2926"><span class="ln">2926 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2927"><span class="ln">2927 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2928"><span class="ln">2928 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2929"><span class="ln">2929 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2930"><span class="ln">2930 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2931"><span class="ln">2931 </span></a><span class="s2">def </span><span class="s1">_native_batch_norm_legit_no_training</span><span class="s3">(</span>
<a name="l2932"><span class="ln">2932 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2933"><span class="ln">2933 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2934"><span class="ln">2934 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2935"><span class="ln">2935 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2936"><span class="ln">2936 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2937"><span class="ln">2937 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2938"><span class="ln">2938 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l2939"><span class="ln">2939 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2940"><span class="ln">2940 </span></a><span class="s2">def </span><span class="s1">_native_multi_head_attention</span><span class="s3">(</span>
<a name="l2941"><span class="ln">2941 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2942"><span class="ln">2942 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2943"><span class="ln">2943 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2944"><span class="ln">2944 </span></a>    <span class="s1">embed_dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2945"><span class="ln">2945 </span></a>    <span class="s1">num_head</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l2946"><span class="ln">2946 </span></a>    <span class="s1">qkv_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2947"><span class="ln">2947 </span></a>    <span class="s1">qkv_bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2948"><span class="ln">2948 </span></a>    <span class="s1">proj_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2949"><span class="ln">2949 </span></a>    <span class="s1">proj_bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2950"><span class="ln">2950 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2951"><span class="ln">2951 </span></a>    <span class="s1">need_weights</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2952"><span class="ln">2952 </span></a>    <span class="s1">average_attn_weights</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2953"><span class="ln">2953 </span></a>    <span class="s1">mask_type</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2954"><span class="ln">2954 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2955"><span class="ln">2955 </span></a><span class="s2">def </span><span class="s1">_neg_view</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2956"><span class="ln">2956 </span></a><span class="s2">def </span><span class="s1">_neg_view_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2957"><span class="ln">2957 </span></a><span class="s2">def </span><span class="s1">_nested_compute_contiguous_strides_offsets</span><span class="s3">(</span>
<a name="l2958"><span class="ln">2958 </span></a>    <span class="s1">nested_size</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2959"><span class="ln">2959 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2960"><span class="ln">2960 </span></a><span class="s2">def </span><span class="s1">_nested_from_padded</span><span class="s3">(</span>
<a name="l2961"><span class="ln">2961 </span></a>    <span class="s1">padded</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2962"><span class="ln">2962 </span></a>    <span class="s1">cpu_nested_shape_example</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2963"><span class="ln">2963 </span></a>    <span class="s1">fuse_transform_0213</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l2964"><span class="ln">2964 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2965"><span class="ln">2965 </span></a><span class="s2">def </span><span class="s1">_nested_from_padded_and_nested_example</span><span class="s3">(</span>
<a name="l2966"><span class="ln">2966 </span></a>    <span class="s1">padded</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2967"><span class="ln">2967 </span></a>    <span class="s1">nt_example</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2968"><span class="ln">2968 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2969"><span class="ln">2969 </span></a><span class="s2">def </span><span class="s1">_nested_from_padded_tensor</span><span class="s3">(</span>
<a name="l2970"><span class="ln">2970 </span></a>    <span class="s1">padded</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2971"><span class="ln">2971 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2972"><span class="ln">2972 </span></a>    <span class="s1">dummy</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2973"><span class="ln">2973 </span></a>    <span class="s1">ragged_idx</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l2974"><span class="ln">2974 </span></a>    <span class="s1">min_seqlen</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2975"><span class="ln">2975 </span></a>    <span class="s1">max_seqlen</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2976"><span class="ln">2976 </span></a>    <span class="s1">sum_S</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2977"><span class="ln">2977 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2978"><span class="ln">2978 </span></a><span class="s2">def </span><span class="s1">_nested_get_jagged_dummy</span><span class="s3">(</span><span class="s1">any</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2979"><span class="ln">2979 </span></a><span class="s2">def </span><span class="s1">_nested_get_lengths</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2980"><span class="ln">2980 </span></a><span class="s2">def </span><span class="s1">_nested_get_max_seqlen</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2981"><span class="ln">2981 </span></a><span class="s2">def </span><span class="s1">_nested_get_min_seqlen</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2982"><span class="ln">2982 </span></a><span class="s2">def </span><span class="s1">_nested_get_offsets</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2983"><span class="ln">2983 </span></a><span class="s2">def </span><span class="s1">_nested_get_ragged_idx</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2984"><span class="ln">2984 </span></a><span class="s2">def </span><span class="s1">_nested_get_values</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2985"><span class="ln">2985 </span></a><span class="s2">def </span><span class="s1">_nested_get_values_copy</span><span class="s3">(</span>
<a name="l2986"><span class="ln">2986 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2987"><span class="ln">2987 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l2988"><span class="ln">2988 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2989"><span class="ln">2989 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2990"><span class="ln">2990 </span></a><span class="s2">def </span><span class="s1">_nested_tensor_from_mask</span><span class="s3">(</span>
<a name="l2991"><span class="ln">2991 </span></a>    <span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2992"><span class="ln">2992 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l2993"><span class="ln">2993 </span></a>    <span class="s1">mask_check</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l2994"><span class="ln">2994 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2995"><span class="ln">2995 </span></a><span class="s2">def </span><span class="s1">_nested_tensor_from_mask_left_aligned</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l2996"><span class="ln">2996 </span></a><span class="s2">def </span><span class="s1">_nested_tensor_from_tensor_list</span><span class="s3">(</span>
<a name="l2997"><span class="ln">2997 </span></a>    <span class="s1">list</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l2998"><span class="ln">2998 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l2999"><span class="ln">2999 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3000"><span class="ln">3000 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3001"><span class="ln">3001 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3002"><span class="ln">3002 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3003"><span class="ln">3003 </span></a><span class="s2">def </span><span class="s1">_nested_tensor_softmax_with_shape</span><span class="s3">(</span>
<a name="l3004"><span class="ln">3004 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3005"><span class="ln">3005 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3006"><span class="ln">3006 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3007"><span class="ln">3007 </span></a><span class="s2">def </span><span class="s1">_nested_view_from_buffer</span><span class="s3">(</span>
<a name="l3008"><span class="ln">3008 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3009"><span class="ln">3009 </span></a>    <span class="s1">nested_size</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3010"><span class="ln">3010 </span></a>    <span class="s1">nested_strides</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3011"><span class="ln">3011 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3012"><span class="ln">3012 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3013"><span class="ln">3013 </span></a><span class="s2">def </span><span class="s1">_nested_view_from_buffer_copy</span><span class="s3">(</span>
<a name="l3014"><span class="ln">3014 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3015"><span class="ln">3015 </span></a>    <span class="s1">nested_size</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3016"><span class="ln">3016 </span></a>    <span class="s1">nested_strides</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3017"><span class="ln">3017 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3018"><span class="ln">3018 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3019"><span class="ln">3019 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3020"><span class="ln">3020 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3021"><span class="ln">3021 </span></a><span class="s2">def </span><span class="s1">_nested_view_from_jagged</span><span class="s3">(</span>
<a name="l3022"><span class="ln">3022 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3023"><span class="ln">3023 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3024"><span class="ln">3024 </span></a>    <span class="s1">dummy</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3025"><span class="ln">3025 </span></a>    <span class="s1">lengths</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3026"><span class="ln">3026 </span></a>    <span class="s1">ragged_idx</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3027"><span class="ln">3027 </span></a>    <span class="s1">min_seqlen</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3028"><span class="ln">3028 </span></a>    <span class="s1">max_seqlen</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3029"><span class="ln">3029 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3030"><span class="ln">3030 </span></a><span class="s2">def </span><span class="s1">_nested_view_from_jagged_copy</span><span class="s3">(</span>
<a name="l3031"><span class="ln">3031 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3032"><span class="ln">3032 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3033"><span class="ln">3033 </span></a>    <span class="s1">dummy</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3034"><span class="ln">3034 </span></a>    <span class="s1">lengths</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3035"><span class="ln">3035 </span></a>    <span class="s1">ragged_idx</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3036"><span class="ln">3036 </span></a>    <span class="s1">min_seqlen</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3037"><span class="ln">3037 </span></a>    <span class="s1">max_seqlen</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3038"><span class="ln">3038 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3039"><span class="ln">3039 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3040"><span class="ln">3040 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3041"><span class="ln">3041 </span></a><span class="s2">def </span><span class="s1">_nnpack_available</span><span class="s3">() </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3042"><span class="ln">3042 </span></a><span class="s2">def </span><span class="s1">_nnpack_spatial_convolution</span><span class="s3">(</span>
<a name="l3043"><span class="ln">3043 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3044"><span class="ln">3044 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3045"><span class="ln">3045 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3046"><span class="ln">3046 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l3047"><span class="ln">3047 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3048"><span class="ln">3048 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3049"><span class="ln">3049 </span></a><span class="s2">def </span><span class="s1">_pack_padded_sequence</span><span class="s3">(</span>
<a name="l3050"><span class="ln">3050 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3051"><span class="ln">3051 </span></a>    <span class="s1">lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3052"><span class="ln">3052 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3053"><span class="ln">3053 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3054"><span class="ln">3054 </span></a><span class="s2">def </span><span class="s1">_pad_packed_sequence</span><span class="s3">(</span>
<a name="l3055"><span class="ln">3055 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3056"><span class="ln">3056 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3057"><span class="ln">3057 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3058"><span class="ln">3058 </span></a>    <span class="s1">padding_value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3059"><span class="ln">3059 </span></a>    <span class="s1">total_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3060"><span class="ln">3060 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3061"><span class="ln">3061 </span></a><span class="s2">def </span><span class="s1">_pin_memory</span><span class="s3">(</span>
<a name="l3062"><span class="ln">3062 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3063"><span class="ln">3063 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3064"><span class="ln">3064 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3065"><span class="ln">3065 </span></a><span class="s2">def </span><span class="s1">_prelu_kernel</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3066"><span class="ln">3066 </span></a><span class="s2">def </span><span class="s1">_print</span><span class="s3">(</span><span class="s1">s</span><span class="s2">: </span><span class="s1">str</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3067"><span class="ln">3067 </span></a><span class="s2">def </span><span class="s1">_propagate_xla_data</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3068"><span class="ln">3068 </span></a><span class="s2">def </span><span class="s1">_remove_batch_dim</span><span class="s3">(</span>
<a name="l3069"><span class="ln">3069 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3070"><span class="ln">3070 </span></a>    <span class="s1">level</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3071"><span class="ln">3071 </span></a>    <span class="s1">batch_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l3072"><span class="ln">3072 </span></a>    <span class="s1">out_dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3073"><span class="ln">3073 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3074"><span class="ln">3074 </span></a><span class="s2">def </span><span class="s1">_reshape_alias_copy</span><span class="s3">(</span>
<a name="l3075"><span class="ln">3075 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3076"><span class="ln">3076 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l3077"><span class="ln">3077 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l3078"><span class="ln">3078 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3079"><span class="ln">3079 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3080"><span class="ln">3080 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3081"><span class="ln">3081 </span></a><span class="s2">def </span><span class="s1">_reshape_from_tensor</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">shape</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3082"><span class="ln">3082 </span></a><span class="s2">def </span><span class="s1">_resize_output_</span><span class="s3">(</span>
<a name="l3083"><span class="ln">3083 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3084"><span class="ln">3084 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l3085"><span class="ln">3085 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3086"><span class="ln">3086 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3087"><span class="ln">3087 </span></a><span class="s2">def </span><span class="s1">_rowwise_prune</span><span class="s3">(</span>
<a name="l3088"><span class="ln">3088 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3089"><span class="ln">3089 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3090"><span class="ln">3090 </span></a>    <span class="s1">compressed_indices_dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l3091"><span class="ln">3091 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3092"><span class="ln">3092 </span></a><span class="s2">def </span><span class="s1">_safe_softmax</span><span class="s3">(</span>
<a name="l3093"><span class="ln">3093 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3094"><span class="ln">3094 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3095"><span class="ln">3095 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3096"><span class="ln">3096 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3097"><span class="ln">3097 </span></a><span class="s2">def </span><span class="s1">_sample_dirichlet</span><span class="s3">(</span>
<a name="l3098"><span class="ln">3098 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3099"><span class="ln">3099 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3100"><span class="ln">3100 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3101"><span class="ln">3101 </span></a><span class="s2">def </span><span class="s1">_saturate_weight_to_fp16</span><span class="s3">(</span><span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3102"><span class="ln">3102 </span></a><span class="s2">def </span><span class="s1">_scaled_dot_product_attention_math</span><span class="s3">(</span>
<a name="l3103"><span class="ln">3103 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3104"><span class="ln">3104 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3105"><span class="ln">3105 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3106"><span class="ln">3106 </span></a>    <span class="s1">attn_mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3107"><span class="ln">3107 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l3108"><span class="ln">3108 </span></a>    <span class="s1">is_causal</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3109"><span class="ln">3109 </span></a>    <span class="s1">dropout_mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3110"><span class="ln">3110 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3111"><span class="ln">3111 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3112"><span class="ln">3112 </span></a>    <span class="s1">enable_gqa</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3113"><span class="ln">3113 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3114"><span class="ln">3114 </span></a><span class="s2">def </span><span class="s1">_scaled_dot_product_attention_math_for_mps</span><span class="s3">(</span>
<a name="l3115"><span class="ln">3115 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3116"><span class="ln">3116 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3117"><span class="ln">3117 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3118"><span class="ln">3118 </span></a>    <span class="s1">attn_mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3119"><span class="ln">3119 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l3120"><span class="ln">3120 </span></a>    <span class="s1">is_causal</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3121"><span class="ln">3121 </span></a>    <span class="s1">dropout_mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3122"><span class="ln">3122 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3123"><span class="ln">3123 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3124"><span class="ln">3124 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3125"><span class="ln">3125 </span></a><span class="s2">def </span><span class="s1">_scaled_dot_product_cudnn_attention</span><span class="s3">(</span>
<a name="l3126"><span class="ln">3126 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3127"><span class="ln">3127 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3128"><span class="ln">3128 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3129"><span class="ln">3129 </span></a>    <span class="s1">attn_bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3130"><span class="ln">3130 </span></a>    <span class="s1">compute_log_sumexp</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3131"><span class="ln">3131 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l3132"><span class="ln">3132 </span></a>    <span class="s1">is_causal</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3133"><span class="ln">3133 </span></a>    <span class="s1">return_debug_mask</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3134"><span class="ln">3134 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3135"><span class="ln">3135 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3136"><span class="ln">3136 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_scaled_dot_product_cudnn_attention</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3137"><span class="ln">3137 </span></a><span class="s2">def </span><span class="s1">_scaled_dot_product_efficient_attention</span><span class="s3">(</span>
<a name="l3138"><span class="ln">3138 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3139"><span class="ln">3139 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3140"><span class="ln">3140 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3141"><span class="ln">3141 </span></a>    <span class="s1">attn_bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3142"><span class="ln">3142 </span></a>    <span class="s1">compute_log_sumexp</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3143"><span class="ln">3143 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l3144"><span class="ln">3144 </span></a>    <span class="s1">is_causal</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3145"><span class="ln">3145 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3146"><span class="ln">3146 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3147"><span class="ln">3147 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_scaled_dot_product_efficient_attention</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3148"><span class="ln">3148 </span></a><span class="s2">def </span><span class="s1">_scaled_dot_product_flash_attention</span><span class="s3">(</span>
<a name="l3149"><span class="ln">3149 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3150"><span class="ln">3150 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3151"><span class="ln">3151 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3152"><span class="ln">3152 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l3153"><span class="ln">3153 </span></a>    <span class="s1">is_causal</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3154"><span class="ln">3154 </span></a>    <span class="s1">return_debug_mask</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3155"><span class="ln">3155 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3156"><span class="ln">3156 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3157"><span class="ln">3157 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_scaled_dot_product_flash_attention</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3158"><span class="ln">3158 </span></a><span class="s2">def </span><span class="s1">_scaled_dot_product_flash_attention_for_cpu</span><span class="s3">(</span>
<a name="l3159"><span class="ln">3159 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3160"><span class="ln">3160 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3161"><span class="ln">3161 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3162"><span class="ln">3162 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l3163"><span class="ln">3163 </span></a>    <span class="s1">is_causal</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3164"><span class="ln">3164 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3165"><span class="ln">3165 </span></a>    <span class="s1">attn_mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3166"><span class="ln">3166 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3167"><span class="ln">3167 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_scaled_dot_product_flash_attention_for_cpu</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3168"><span class="ln">3168 </span></a><span class="s2">def </span><span class="s1">_scaled_grouped_mm</span><span class="s3">(</span>
<a name="l3169"><span class="ln">3169 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3170"><span class="ln">3170 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3171"><span class="ln">3171 </span></a>    <span class="s1">scale_a</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3172"><span class="ln">3172 </span></a>    <span class="s1">scale_b</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3173"><span class="ln">3173 </span></a>    <span class="s1">offs</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3174"><span class="ln">3174 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3175"><span class="ln">3175 </span></a>    <span class="s1">scale_result</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3176"><span class="ln">3176 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3177"><span class="ln">3177 </span></a>    <span class="s1">use_fast_accum</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3178"><span class="ln">3178 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3179"><span class="ln">3179 </span></a><span class="s2">def </span><span class="s1">_scaled_mm</span><span class="s3">(</span>
<a name="l3180"><span class="ln">3180 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3181"><span class="ln">3181 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3182"><span class="ln">3182 </span></a>    <span class="s1">scale_a</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3183"><span class="ln">3183 </span></a>    <span class="s1">scale_b</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3184"><span class="ln">3184 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3185"><span class="ln">3185 </span></a>    <span class="s1">scale_result</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3186"><span class="ln">3186 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3187"><span class="ln">3187 </span></a>    <span class="s1">use_fast_accum</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3188"><span class="ln">3188 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3189"><span class="ln">3189 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3190"><span class="ln">3190 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3191"><span class="ln">3191 </span></a><span class="s2">def </span><span class="s1">_shape_as_tensor</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3192"><span class="ln">3192 </span></a><span class="s2">def </span><span class="s1">_sobol_engine_draw</span><span class="s3">(</span>
<a name="l3193"><span class="ln">3193 </span></a>    <span class="s1">quasi</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3194"><span class="ln">3194 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3195"><span class="ln">3195 </span></a>    <span class="s1">sobolstate</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3196"><span class="ln">3196 </span></a>    <span class="s1">dimension</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3197"><span class="ln">3197 </span></a>    <span class="s1">num_generated</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3198"><span class="ln">3198 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3199"><span class="ln">3199 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3200"><span class="ln">3200 </span></a><span class="s2">def </span><span class="s1">_sobol_engine_ff_</span><span class="s3">(</span>
<a name="l3201"><span class="ln">3201 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3202"><span class="ln">3202 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3203"><span class="ln">3203 </span></a>    <span class="s1">sobolstate</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3204"><span class="ln">3204 </span></a>    <span class="s1">dimension</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3205"><span class="ln">3205 </span></a>    <span class="s1">num_generated</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3206"><span class="ln">3206 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3207"><span class="ln">3207 </span></a><span class="s2">def </span><span class="s1">_sobol_engine_initialize_state_</span><span class="s3">(</span>
<a name="l3208"><span class="ln">3208 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3209"><span class="ln">3209 </span></a>    <span class="s1">dimension</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3210"><span class="ln">3210 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3211"><span class="ln">3211 </span></a><span class="s2">def </span><span class="s1">_sobol_engine_scramble_</span><span class="s3">(</span>
<a name="l3212"><span class="ln">3212 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3213"><span class="ln">3213 </span></a>    <span class="s1">ltm</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3214"><span class="ln">3214 </span></a>    <span class="s1">dimension</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3215"><span class="ln">3215 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3216"><span class="ln">3216 </span></a><span class="s2">def </span><span class="s1">_softmax</span><span class="s3">(</span>
<a name="l3217"><span class="ln">3217 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3218"><span class="ln">3218 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3219"><span class="ln">3219 </span></a>    <span class="s1">half_to_float</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3220"><span class="ln">3220 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3221"><span class="ln">3221 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3222"><span class="ln">3222 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3223"><span class="ln">3223 </span></a><span class="s2">def </span><span class="s1">_softmax_backward_data</span><span class="s3">(</span>
<a name="l3224"><span class="ln">3224 </span></a>    <span class="s1">grad_output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3225"><span class="ln">3225 </span></a>    <span class="s1">output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3226"><span class="ln">3226 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3227"><span class="ln">3227 </span></a>    <span class="s1">input_dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l3228"><span class="ln">3228 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3229"><span class="ln">3229 </span></a>    <span class="s1">grad_input</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3230"><span class="ln">3230 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3231"><span class="ln">3231 </span></a><span class="s2">def </span><span class="s1">_sparse_broadcast_to</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3232"><span class="ln">3232 </span></a><span class="s2">def </span><span class="s1">_sparse_broadcast_to_copy</span><span class="s3">(</span>
<a name="l3233"><span class="ln">3233 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3234"><span class="ln">3234 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3235"><span class="ln">3235 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3236"><span class="ln">3236 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3237"><span class="ln">3237 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3238"><span class="ln">3238 </span></a><span class="s2">def </span><span class="s1">_sparse_csr_prod</span><span class="s3">(</span>
<a name="l3239"><span class="ln">3239 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3240"><span class="ln">3240 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3241"><span class="ln">3241 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3242"><span class="ln">3242 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3243"><span class="ln">3243 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3244"><span class="ln">3244 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3245"><span class="ln">3245 </span></a><span class="s2">def </span><span class="s1">_sparse_csr_sum</span><span class="s3">(</span>
<a name="l3246"><span class="ln">3246 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3247"><span class="ln">3247 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3248"><span class="ln">3248 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3249"><span class="ln">3249 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3250"><span class="ln">3250 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3251"><span class="ln">3251 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3252"><span class="ln">3252 </span></a><span class="s2">def </span><span class="s1">_sparse_log_softmax_backward_data</span><span class="s3">(</span>
<a name="l3253"><span class="ln">3253 </span></a>    <span class="s1">grad_output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3254"><span class="ln">3254 </span></a>    <span class="s1">output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3255"><span class="ln">3255 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3256"><span class="ln">3256 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3257"><span class="ln">3257 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3258"><span class="ln">3258 </span></a><span class="s2">def </span><span class="s1">_sparse_semi_structured_addmm</span><span class="s3">(</span>
<a name="l3259"><span class="ln">3259 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3260"><span class="ln">3260 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3261"><span class="ln">3261 </span></a>    <span class="s1">mat1_meta</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3262"><span class="ln">3262 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3263"><span class="ln">3263 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3264"><span class="ln">3264 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3265"><span class="ln">3265 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3266"><span class="ln">3266 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3267"><span class="ln">3267 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3268"><span class="ln">3268 </span></a><span class="s2">def </span><span class="s1">_sparse_semi_structured_apply</span><span class="s3">(</span>
<a name="l3269"><span class="ln">3269 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3270"><span class="ln">3270 </span></a>    <span class="s1">thread_masks</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3271"><span class="ln">3271 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3272"><span class="ln">3272 </span></a><span class="s2">def </span><span class="s1">_sparse_semi_structured_apply_dense</span><span class="s3">(</span>
<a name="l3273"><span class="ln">3273 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3274"><span class="ln">3274 </span></a>    <span class="s1">thread_masks</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3275"><span class="ln">3275 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3276"><span class="ln">3276 </span></a><span class="s2">def </span><span class="s1">_sparse_semi_structured_linear</span><span class="s3">(</span>
<a name="l3277"><span class="ln">3277 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3278"><span class="ln">3278 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3279"><span class="ln">3279 </span></a>    <span class="s1">meta</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3280"><span class="ln">3280 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3281"><span class="ln">3281 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3282"><span class="ln">3282 </span></a>    <span class="s1">activation</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3283"><span class="ln">3283 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3284"><span class="ln">3284 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3285"><span class="ln">3285 </span></a><span class="s2">def </span><span class="s1">_sparse_semi_structured_mm</span><span class="s3">(</span>
<a name="l3286"><span class="ln">3286 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3287"><span class="ln">3287 </span></a>    <span class="s1">mat1_meta</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3288"><span class="ln">3288 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3289"><span class="ln">3289 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3290"><span class="ln">3290 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3291"><span class="ln">3291 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3292"><span class="ln">3292 </span></a><span class="s2">def </span><span class="s1">_sparse_semi_structured_tile</span><span class="s3">(</span>
<a name="l3293"><span class="ln">3293 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3294"><span class="ln">3294 </span></a>    <span class="s1">algorithm</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
<a name="l3295"><span class="ln">3295 </span></a>    <span class="s1">use_cutlass</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l3296"><span class="ln">3296 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3297"><span class="ln">3297 </span></a><span class="s2">def </span><span class="s1">_sparse_softmax_backward_data</span><span class="s3">(</span>
<a name="l3298"><span class="ln">3298 </span></a>    <span class="s1">grad_output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3299"><span class="ln">3299 </span></a>    <span class="s1">output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3300"><span class="ln">3300 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3301"><span class="ln">3301 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3302"><span class="ln">3302 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3303"><span class="ln">3303 </span></a><span class="s2">def </span><span class="s1">_sparse_sparse_matmul</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3304"><span class="ln">3304 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3305"><span class="ln">3305 </span></a><span class="s2">def </span><span class="s1">_sparse_sum</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3306"><span class="ln">3306 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3307"><span class="ln">3307 </span></a><span class="s2">def </span><span class="s1">_sparse_sum</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3308"><span class="ln">3308 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3309"><span class="ln">3309 </span></a><span class="s2">def </span><span class="s1">_sparse_sum</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3310"><span class="ln">3310 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3311"><span class="ln">3311 </span></a><span class="s2">def </span><span class="s1">_sparse_sum</span><span class="s3">(</span>
<a name="l3312"><span class="ln">3312 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3313"><span class="ln">3313 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3314"><span class="ln">3314 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3315"><span class="ln">3315 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l3316"><span class="ln">3316 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3317"><span class="ln">3317 </span></a><span class="s2">def </span><span class="s1">_stack</span><span class="s3">(</span>
<a name="l3318"><span class="ln">3318 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3319"><span class="ln">3319 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l3320"><span class="ln">3320 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3321"><span class="ln">3321 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3322"><span class="ln">3322 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3323"><span class="ln">3323 </span></a><span class="s2">def </span><span class="s1">_standard_gamma</span><span class="s3">(</span>
<a name="l3324"><span class="ln">3324 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3325"><span class="ln">3325 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3326"><span class="ln">3326 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3327"><span class="ln">3327 </span></a><span class="s2">def </span><span class="s1">_standard_gamma_grad</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3328"><span class="ln">3328 </span></a><span class="s2">def </span><span class="s1">_sync</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3329"><span class="ln">3329 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3330"><span class="ln">3330 </span></a><span class="s2">def </span><span class="s1">_test_autograd_multiple_dispatch</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3331"><span class="ln">3331 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3332"><span class="ln">3332 </span></a><span class="s2">def </span><span class="s1">_test_autograd_multiple_dispatch</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">b</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3333"><span class="ln">3333 </span></a><span class="s2">def </span><span class="s1">_test_autograd_multiple_dispatch_view</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3334"><span class="ln">3334 </span></a><span class="s2">def </span><span class="s1">_test_autograd_multiple_dispatch_view_copy</span><span class="s3">(</span>
<a name="l3335"><span class="ln">3335 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3336"><span class="ln">3336 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3337"><span class="ln">3337 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3338"><span class="ln">3338 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3339"><span class="ln">3339 </span></a><span class="s2">def </span><span class="s1">_test_check_tensor</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3340"><span class="ln">3340 </span></a><span class="s2">def </span><span class="s1">_test_functorch_fallback</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3341"><span class="ln">3341 </span></a><span class="s2">def </span><span class="s1">_test_parallel_materialize</span><span class="s3">(</span>
<a name="l3342"><span class="ln">3342 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3343"><span class="ln">3343 </span></a>    <span class="s1">num_parallel</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3344"><span class="ln">3344 </span></a>    <span class="s1">skip_first</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3345"><span class="ln">3345 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3346"><span class="ln">3346 </span></a><span class="s2">def </span><span class="s1">_test_serialization_subcmul</span><span class="s3">(</span>
<a name="l3347"><span class="ln">3347 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3348"><span class="ln">3348 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3349"><span class="ln">3349 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3350"><span class="ln">3350 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3351"><span class="ln">3351 </span></a><span class="s2">def </span><span class="s1">_to_cpu</span><span class="s3">(</span>
<a name="l3352"><span class="ln">3352 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3353"><span class="ln">3353 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3354"><span class="ln">3354 </span></a><span class="s2">def </span><span class="s1">_to_functional_tensor</span><span class="s3">(</span><span class="s1">t</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3355"><span class="ln">3355 </span></a><span class="s2">def </span><span class="s1">_to_sparse_semi_structured</span><span class="s3">(</span><span class="s1">dense</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3356"><span class="ln">3356 </span></a><span class="s2">def </span><span class="s1">_transform_bias_rescale_qkv</span><span class="s3">(</span>
<a name="l3357"><span class="ln">3357 </span></a>    <span class="s1">qkv</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3358"><span class="ln">3358 </span></a>    <span class="s1">qkv_bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3359"><span class="ln">3359 </span></a>    <span class="s1">num_heads</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3360"><span class="ln">3360 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3361"><span class="ln">3361 </span></a><span class="s2">def </span><span class="s1">_transformer_encoder_layer_fwd</span><span class="s3">(</span>
<a name="l3362"><span class="ln">3362 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3363"><span class="ln">3363 </span></a>    <span class="s1">embed_dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3364"><span class="ln">3364 </span></a>    <span class="s1">num_heads</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3365"><span class="ln">3365 </span></a>    <span class="s1">qkv_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3366"><span class="ln">3366 </span></a>    <span class="s1">qkv_bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3367"><span class="ln">3367 </span></a>    <span class="s1">proj_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3368"><span class="ln">3368 </span></a>    <span class="s1">proj_bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3369"><span class="ln">3369 </span></a>    <span class="s1">use_gelu</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3370"><span class="ln">3370 </span></a>    <span class="s1">norm_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3371"><span class="ln">3371 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l3372"><span class="ln">3372 </span></a>    <span class="s1">norm_weight_1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3373"><span class="ln">3373 </span></a>    <span class="s1">norm_bias_1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3374"><span class="ln">3374 </span></a>    <span class="s1">norm_weight_2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3375"><span class="ln">3375 </span></a>    <span class="s1">norm_bias_2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3376"><span class="ln">3376 </span></a>    <span class="s1">ffn_weight_1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3377"><span class="ln">3377 </span></a>    <span class="s1">ffn_bias_1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3378"><span class="ln">3378 </span></a>    <span class="s1">ffn_weight_2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3379"><span class="ln">3379 </span></a>    <span class="s1">ffn_bias_2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3380"><span class="ln">3380 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3381"><span class="ln">3381 </span></a>    <span class="s1">mask_type</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3382"><span class="ln">3382 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3383"><span class="ln">3383 </span></a><span class="s2">def </span><span class="s1">_trilinear</span><span class="s3">(</span>
<a name="l3384"><span class="ln">3384 </span></a>    <span class="s1">i1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3385"><span class="ln">3385 </span></a>    <span class="s1">i2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3386"><span class="ln">3386 </span></a>    <span class="s1">i3</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3387"><span class="ln">3387 </span></a>    <span class="s1">expand1</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3388"><span class="ln">3388 </span></a>    <span class="s1">expand2</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3389"><span class="ln">3389 </span></a>    <span class="s1">expand3</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3390"><span class="ln">3390 </span></a>    <span class="s1">sumdim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3391"><span class="ln">3391 </span></a>    <span class="s1">unroll_dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3392"><span class="ln">3392 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3393"><span class="ln">3393 </span></a><span class="s2">def </span><span class="s1">_triton_multi_head_attention</span><span class="s3">(</span>
<a name="l3394"><span class="ln">3394 </span></a>    <span class="s1">query</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3395"><span class="ln">3395 </span></a>    <span class="s1">key</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3396"><span class="ln">3396 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3397"><span class="ln">3397 </span></a>    <span class="s1">embed_dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3398"><span class="ln">3398 </span></a>    <span class="s1">num_head</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3399"><span class="ln">3399 </span></a>    <span class="s1">qkv_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3400"><span class="ln">3400 </span></a>    <span class="s1">qkv_bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3401"><span class="ln">3401 </span></a>    <span class="s1">proj_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3402"><span class="ln">3402 </span></a>    <span class="s1">proj_bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3403"><span class="ln">3403 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3404"><span class="ln">3404 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3405"><span class="ln">3405 </span></a><span class="s2">def </span><span class="s1">_triton_scaled_dot_attention</span><span class="s3">(</span>
<a name="l3406"><span class="ln">3406 </span></a>    <span class="s1">q</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3407"><span class="ln">3407 </span></a>    <span class="s1">k</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3408"><span class="ln">3408 </span></a>    <span class="s1">v</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3409"><span class="ln">3409 </span></a>    <span class="s1">dropout_p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l3410"><span class="ln">3410 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3411"><span class="ln">3411 </span></a><span class="s2">def </span><span class="s1">_unique</span><span class="s3">(</span>
<a name="l3412"><span class="ln">3412 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3413"><span class="ln">3413 </span></a>    <span class="s1">sorted</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l3414"><span class="ln">3414 </span></a>    <span class="s1">return_inverse</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3415"><span class="ln">3415 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3416"><span class="ln">3416 </span></a><span class="s2">def </span><span class="s1">_unique2</span><span class="s3">(</span>
<a name="l3417"><span class="ln">3417 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3418"><span class="ln">3418 </span></a>    <span class="s1">sorted</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l3419"><span class="ln">3419 </span></a>    <span class="s1">return_inverse</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3420"><span class="ln">3420 </span></a>    <span class="s1">return_counts</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3421"><span class="ln">3421 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3422"><span class="ln">3422 </span></a><span class="s2">def </span><span class="s1">_unpack_dual</span><span class="s3">(</span>
<a name="l3423"><span class="ln">3423 </span></a>    <span class="s1">dual</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3424"><span class="ln">3424 </span></a>    <span class="s1">level</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3425"><span class="ln">3425 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">_unpack_dual</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3426"><span class="ln">3426 </span></a><span class="s2">def </span><span class="s1">_unsafe_index</span><span class="s3">(</span>
<a name="l3427"><span class="ln">3427 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3428"><span class="ln">3428 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3429"><span class="ln">3429 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3430"><span class="ln">3430 </span></a><span class="s2">def </span><span class="s1">_unsafe_index_put</span><span class="s3">(</span>
<a name="l3431"><span class="ln">3431 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3432"><span class="ln">3432 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3433"><span class="ln">3433 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3434"><span class="ln">3434 </span></a>    <span class="s1">accumulate</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l3435"><span class="ln">3435 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3436"><span class="ln">3436 </span></a><span class="s2">def </span><span class="s1">_unsafe_masked_index</span><span class="s3">(</span>
<a name="l3437"><span class="ln">3437 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3438"><span class="ln">3438 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3439"><span class="ln">3439 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3440"><span class="ln">3440 </span></a>    <span class="s1">fill</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3441"><span class="ln">3441 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3442"><span class="ln">3442 </span></a><span class="s2">def </span><span class="s1">_unsafe_masked_index_put_accumulate</span><span class="s3">(</span>
<a name="l3443"><span class="ln">3443 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3444"><span class="ln">3444 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3445"><span class="ln">3445 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l3446"><span class="ln">3446 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3447"><span class="ln">3447 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3448"><span class="ln">3448 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3449"><span class="ln">3449 </span></a><span class="s2">def </span><span class="s1">_use_cudnn_ctc_loss</span><span class="s3">(</span>
<a name="l3450"><span class="ln">3450 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3451"><span class="ln">3451 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3452"><span class="ln">3452 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3453"><span class="ln">3453 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3454"><span class="ln">3454 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3455"><span class="ln">3455 </span></a><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3456"><span class="ln">3456 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3457"><span class="ln">3457 </span></a><span class="s2">def </span><span class="s1">_use_cudnn_ctc_loss</span><span class="s3">(</span>
<a name="l3458"><span class="ln">3458 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3459"><span class="ln">3459 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3460"><span class="ln">3460 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3461"><span class="ln">3461 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3462"><span class="ln">3462 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3463"><span class="ln">3463 </span></a><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3464"><span class="ln">3464 </span></a><span class="s2">def </span><span class="s1">_use_cudnn_rnn_flatten_weight</span><span class="s3">() </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3465"><span class="ln">3465 </span></a><span class="s2">def </span><span class="s1">_validate_compressed_sparse_indices</span><span class="s3">(</span>
<a name="l3466"><span class="ln">3466 </span></a>    <span class="s1">is_crow</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l3467"><span class="ln">3467 </span></a>    <span class="s1">compressed_idx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3468"><span class="ln">3468 </span></a>    <span class="s1">plain_idx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3469"><span class="ln">3469 </span></a>    <span class="s1">cdim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3470"><span class="ln">3470 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3471"><span class="ln">3471 </span></a>    <span class="s1">nnz</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3472"><span class="ln">3472 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3473"><span class="ln">3473 </span></a><span class="s2">def </span><span class="s1">_validate_sparse_bsc_tensor_args</span><span class="s3">(</span>
<a name="l3474"><span class="ln">3474 </span></a>    <span class="s1">ccol_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3475"><span class="ln">3475 </span></a>    <span class="s1">row_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3476"><span class="ln">3476 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3477"><span class="ln">3477 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3478"><span class="ln">3478 </span></a>    <span class="s1">check_pinning</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3479"><span class="ln">3479 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3480"><span class="ln">3480 </span></a><span class="s2">def </span><span class="s1">_validate_sparse_bsr_tensor_args</span><span class="s3">(</span>
<a name="l3481"><span class="ln">3481 </span></a>    <span class="s1">crow_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3482"><span class="ln">3482 </span></a>    <span class="s1">col_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3483"><span class="ln">3483 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3484"><span class="ln">3484 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3485"><span class="ln">3485 </span></a>    <span class="s1">check_pinning</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3486"><span class="ln">3486 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3487"><span class="ln">3487 </span></a><span class="s2">def </span><span class="s1">_validate_sparse_compressed_tensor_args</span><span class="s3">(</span>
<a name="l3488"><span class="ln">3488 </span></a>    <span class="s1">compressed_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3489"><span class="ln">3489 </span></a>    <span class="s1">plain_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3490"><span class="ln">3490 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3491"><span class="ln">3491 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3492"><span class="ln">3492 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout</span><span class="s3">,</span>
<a name="l3493"><span class="ln">3493 </span></a>    <span class="s1">check_pinning</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3494"><span class="ln">3494 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3495"><span class="ln">3495 </span></a><span class="s2">def </span><span class="s1">_validate_sparse_coo_tensor_args</span><span class="s3">(</span>
<a name="l3496"><span class="ln">3496 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3497"><span class="ln">3497 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3498"><span class="ln">3498 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3499"><span class="ln">3499 </span></a>    <span class="s1">is_coalesced</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3500"><span class="ln">3500 </span></a>    <span class="s1">check_pinning</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3501"><span class="ln">3501 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3502"><span class="ln">3502 </span></a><span class="s2">def </span><span class="s1">_validate_sparse_csc_tensor_args</span><span class="s3">(</span>
<a name="l3503"><span class="ln">3503 </span></a>    <span class="s1">ccol_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3504"><span class="ln">3504 </span></a>    <span class="s1">row_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3505"><span class="ln">3505 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3506"><span class="ln">3506 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3507"><span class="ln">3507 </span></a>    <span class="s1">check_pinning</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3508"><span class="ln">3508 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3509"><span class="ln">3509 </span></a><span class="s2">def </span><span class="s1">_validate_sparse_csr_tensor_args</span><span class="s3">(</span>
<a name="l3510"><span class="ln">3510 </span></a>    <span class="s1">crow_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3511"><span class="ln">3511 </span></a>    <span class="s1">col_indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3512"><span class="ln">3512 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3513"><span class="ln">3513 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3514"><span class="ln">3514 </span></a>    <span class="s1">check_pinning</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3515"><span class="ln">3515 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l3516"><span class="ln">3516 </span></a><span class="s2">def </span><span class="s1">_values_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3517"><span class="ln">3517 </span></a><span class="s2">def </span><span class="s1">_weight_int4pack_mm</span><span class="s3">(</span>
<a name="l3518"><span class="ln">3518 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3519"><span class="ln">3519 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3520"><span class="ln">3520 </span></a>    <span class="s1">qGroupSize</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3521"><span class="ln">3521 </span></a>    <span class="s1">qScaleAndZeros</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3522"><span class="ln">3522 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3523"><span class="ln">3523 </span></a><span class="s2">def </span><span class="s1">_weight_int4pack_mm_for_cpu</span><span class="s3">(</span>
<a name="l3524"><span class="ln">3524 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3525"><span class="ln">3525 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3526"><span class="ln">3526 </span></a>    <span class="s1">qGroupSize</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3527"><span class="ln">3527 </span></a>    <span class="s1">qScaleAndZeros</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3528"><span class="ln">3528 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3529"><span class="ln">3529 </span></a><span class="s2">def </span><span class="s1">_weight_int4pack_mm_with_scales_and_zeros</span><span class="s3">(</span>
<a name="l3530"><span class="ln">3530 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3531"><span class="ln">3531 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3532"><span class="ln">3532 </span></a>    <span class="s1">qGroupSize</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3533"><span class="ln">3533 </span></a>    <span class="s1">qScale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3534"><span class="ln">3534 </span></a>    <span class="s1">qZeros</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3535"><span class="ln">3535 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3536"><span class="ln">3536 </span></a><span class="s2">def </span><span class="s1">_weight_int8pack_mm</span><span class="s3">(</span>
<a name="l3537"><span class="ln">3537 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3538"><span class="ln">3538 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3539"><span class="ln">3539 </span></a>    <span class="s1">scales</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3540"><span class="ln">3540 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3541"><span class="ln">3541 </span></a><span class="s2">def </span><span class="s1">_weight_norm</span><span class="s3">(</span><span class="s1">v</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">g</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3542"><span class="ln">3542 </span></a><span class="s2">def </span><span class="s1">_weight_norm_interface</span><span class="s3">(</span>
<a name="l3543"><span class="ln">3543 </span></a>    <span class="s1">v</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3544"><span class="ln">3544 </span></a>    <span class="s1">g</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3545"><span class="ln">3545 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l3546"><span class="ln">3546 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3547"><span class="ln">3547 </span></a><span class="s2">def </span><span class="s1">_wrapped_linear_prepack</span><span class="s3">(</span>
<a name="l3548"><span class="ln">3548 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3549"><span class="ln">3549 </span></a>    <span class="s1">weight_scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3550"><span class="ln">3550 </span></a>    <span class="s1">weight_zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3551"><span class="ln">3551 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3552"><span class="ln">3552 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3553"><span class="ln">3553 </span></a><span class="s2">def </span><span class="s1">_wrapped_quantized_linear_prepacked</span><span class="s3">(</span>
<a name="l3554"><span class="ln">3554 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3555"><span class="ln">3555 </span></a>    <span class="s1">input_scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3556"><span class="ln">3556 </span></a>    <span class="s1">input_zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3557"><span class="ln">3557 </span></a>    <span class="s1">packed_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3558"><span class="ln">3558 </span></a>    <span class="s1">output_scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3559"><span class="ln">3559 </span></a>    <span class="s1">output_zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3560"><span class="ln">3560 </span></a>    <span class="s1">out_channel</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l3561"><span class="ln">3561 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3562"><span class="ln">3562 </span></a><span class="s2">def </span><span class="s1">abs</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3563"><span class="ln">3563 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3564"><span class="ln">3564 </span></a>    abs(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l3565"><span class="ln">3565 </span></a> 
<a name="l3566"><span class="ln">3566 </span></a>    Computes the absolute value of each element in :attr:`input`. 
<a name="l3567"><span class="ln">3567 </span></a> 
<a name="l3568"><span class="ln">3568 </span></a>    .. math:: 
<a name="l3569"><span class="ln">3569 </span></a>        \text{out}_{i} = |\text{input}_{i}| 
<a name="l3570"><span class="ln">3570 </span></a> 
<a name="l3571"><span class="ln">3571 </span></a>    Args: 
<a name="l3572"><span class="ln">3572 </span></a>        input (Tensor): the input tensor. 
<a name="l3573"><span class="ln">3573 </span></a> 
<a name="l3574"><span class="ln">3574 </span></a>    Keyword args: 
<a name="l3575"><span class="ln">3575 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3576"><span class="ln">3576 </span></a> 
<a name="l3577"><span class="ln">3577 </span></a>    Example:: 
<a name="l3578"><span class="ln">3578 </span></a> 
<a name="l3579"><span class="ln">3579 </span></a>        &gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3])) 
<a name="l3580"><span class="ln">3580 </span></a>        tensor([ 1,  2,  3]) 
<a name="l3581"><span class="ln">3581 </span></a>    &quot;&quot;&quot;</span>
<a name="l3582"><span class="ln">3582 </span></a>
<a name="l3583"><span class="ln">3583 </span></a><span class="s2">def </span><span class="s1">abs_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3584"><span class="ln">3584 </span></a><span class="s2">def </span><span class="s1">absolute</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3585"><span class="ln">3585 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3586"><span class="ln">3586 </span></a>    absolute(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l3587"><span class="ln">3587 </span></a> 
<a name="l3588"><span class="ln">3588 </span></a>    Alias for :func:`torch.abs` 
<a name="l3589"><span class="ln">3589 </span></a>    &quot;&quot;&quot;</span>
<a name="l3590"><span class="ln">3590 </span></a>
<a name="l3591"><span class="ln">3591 </span></a><span class="s2">def </span><span class="s1">acos</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3592"><span class="ln">3592 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3593"><span class="ln">3593 </span></a>    acos(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l3594"><span class="ln">3594 </span></a> 
<a name="l3595"><span class="ln">3595 </span></a>    Computes the inverse cosine of each element in :attr:`input`. 
<a name="l3596"><span class="ln">3596 </span></a> 
<a name="l3597"><span class="ln">3597 </span></a>    .. math:: 
<a name="l3598"><span class="ln">3598 </span></a>        \text{out}_{i} = \cos^{-1}(\text{input}_{i}) 
<a name="l3599"><span class="ln">3599 </span></a> 
<a name="l3600"><span class="ln">3600 </span></a>    Args: 
<a name="l3601"><span class="ln">3601 </span></a>        input (Tensor): the input tensor. 
<a name="l3602"><span class="ln">3602 </span></a> 
<a name="l3603"><span class="ln">3603 </span></a>    Keyword args: 
<a name="l3604"><span class="ln">3604 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3605"><span class="ln">3605 </span></a> 
<a name="l3606"><span class="ln">3606 </span></a>    Example:: 
<a name="l3607"><span class="ln">3607 </span></a> 
<a name="l3608"><span class="ln">3608 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l3609"><span class="ln">3609 </span></a>        &gt;&gt;&gt; a 
<a name="l3610"><span class="ln">3610 </span></a>        tensor([ 0.3348, -0.5889,  0.2005, -0.1584]) 
<a name="l3611"><span class="ln">3611 </span></a>        &gt;&gt;&gt; torch.acos(a) 
<a name="l3612"><span class="ln">3612 </span></a>        tensor([ 1.2294,  2.2004,  1.3690,  1.7298]) 
<a name="l3613"><span class="ln">3613 </span></a>    &quot;&quot;&quot;</span>
<a name="l3614"><span class="ln">3614 </span></a>
<a name="l3615"><span class="ln">3615 </span></a><span class="s2">def </span><span class="s1">acos_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3616"><span class="ln">3616 </span></a><span class="s2">def </span><span class="s1">acosh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3617"><span class="ln">3617 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3618"><span class="ln">3618 </span></a>    acosh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l3619"><span class="ln">3619 </span></a> 
<a name="l3620"><span class="ln">3620 </span></a>    Returns a new tensor with the inverse hyperbolic cosine of the elements of :attr:`input`. 
<a name="l3621"><span class="ln">3621 </span></a> 
<a name="l3622"><span class="ln">3622 </span></a>    .. math:: 
<a name="l3623"><span class="ln">3623 </span></a>        \text{out}_{i} = \cosh^{-1}(\text{input}_{i}) 
<a name="l3624"><span class="ln">3624 </span></a> 
<a name="l3625"><span class="ln">3625 </span></a>    Note: 
<a name="l3626"><span class="ln">3626 </span></a>        The domain of the inverse hyperbolic cosine is `[1, inf)` and values outside this range 
<a name="l3627"><span class="ln">3627 </span></a>        will be mapped to ``NaN``, except for `+ INF` for which the output is mapped to `+ INF`. 
<a name="l3628"><span class="ln">3628 </span></a> 
<a name="l3629"><span class="ln">3629 </span></a>    Args: 
<a name="l3630"><span class="ln">3630 </span></a>        input (Tensor): the input tensor. 
<a name="l3631"><span class="ln">3631 </span></a> 
<a name="l3632"><span class="ln">3632 </span></a>    Keyword arguments: 
<a name="l3633"><span class="ln">3633 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3634"><span class="ln">3634 </span></a> 
<a name="l3635"><span class="ln">3635 </span></a>    Example:: 
<a name="l3636"><span class="ln">3636 </span></a> 
<a name="l3637"><span class="ln">3637 </span></a>        &gt;&gt;&gt; a = torch.randn(4).uniform_(1, 2) 
<a name="l3638"><span class="ln">3638 </span></a>        &gt;&gt;&gt; a 
<a name="l3639"><span class="ln">3639 </span></a>        tensor([ 1.3192, 1.9915, 1.9674, 1.7151 ]) 
<a name="l3640"><span class="ln">3640 </span></a>        &gt;&gt;&gt; torch.acosh(a) 
<a name="l3641"><span class="ln">3641 </span></a>        tensor([ 0.7791, 1.3120, 1.2979, 1.1341 ]) 
<a name="l3642"><span class="ln">3642 </span></a>    &quot;&quot;&quot;</span>
<a name="l3643"><span class="ln">3643 </span></a>
<a name="l3644"><span class="ln">3644 </span></a><span class="s2">def </span><span class="s1">acosh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3645"><span class="ln">3645 </span></a><span class="s2">def </span><span class="s1">adaptive_avg_pool1d</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3646"><span class="ln">3646 </span></a><span class="s2">def </span><span class="s1">adaptive_max_pool1d</span><span class="s3">(</span>
<a name="l3647"><span class="ln">3647 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3648"><span class="ln">3648 </span></a>    <span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l3649"><span class="ln">3649 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l3650"><span class="ln">3650 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3651"><span class="ln">3651 </span></a><span class="s2">def </span><span class="s1">add</span><span class="s3">(</span>
<a name="l3652"><span class="ln">3652 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3653"><span class="ln">3653 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3654"><span class="ln">3654 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3655"><span class="ln">3655 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3656"><span class="ln">3656 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3657"><span class="ln">3657 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3658"><span class="ln">3658 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3659"><span class="ln">3659 </span></a>    add(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l3660"><span class="ln">3660 </span></a> 
<a name="l3661"><span class="ln">3661 </span></a>    Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`. 
<a name="l3662"><span class="ln">3662 </span></a> 
<a name="l3663"><span class="ln">3663 </span></a>    .. math:: 
<a name="l3664"><span class="ln">3664 </span></a>        \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i 
<a name="l3665"><span class="ln">3665 </span></a> 
<a name="l3666"><span class="ln">3666 </span></a> 
<a name="l3667"><span class="ln">3667 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l3668"><span class="ln">3668 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l3669"><span class="ln">3669 </span></a> 
<a name="l3670"><span class="ln">3670 </span></a>    Args: 
<a name="l3671"><span class="ln">3671 </span></a>        input (Tensor): the input tensor. 
<a name="l3672"><span class="ln">3672 </span></a>        other (Tensor or Number): the tensor or number to add to :attr:`input`. 
<a name="l3673"><span class="ln">3673 </span></a> 
<a name="l3674"><span class="ln">3674 </span></a>    Keyword arguments: 
<a name="l3675"><span class="ln">3675 </span></a>        alpha (Number): the multiplier for :attr:`other`. 
<a name="l3676"><span class="ln">3676 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3677"><span class="ln">3677 </span></a> 
<a name="l3678"><span class="ln">3678 </span></a>    Examples:: 
<a name="l3679"><span class="ln">3679 </span></a> 
<a name="l3680"><span class="ln">3680 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l3681"><span class="ln">3681 </span></a>        &gt;&gt;&gt; a 
<a name="l3682"><span class="ln">3682 </span></a>        tensor([ 0.0202,  1.0985,  1.3506, -0.6056]) 
<a name="l3683"><span class="ln">3683 </span></a>        &gt;&gt;&gt; torch.add(a, 20) 
<a name="l3684"><span class="ln">3684 </span></a>        tensor([ 20.0202,  21.0985,  21.3506,  19.3944]) 
<a name="l3685"><span class="ln">3685 </span></a> 
<a name="l3686"><span class="ln">3686 </span></a>        &gt;&gt;&gt; b = torch.randn(4) 
<a name="l3687"><span class="ln">3687 </span></a>        &gt;&gt;&gt; b 
<a name="l3688"><span class="ln">3688 </span></a>        tensor([-0.9732, -0.3497,  0.6245,  0.4022]) 
<a name="l3689"><span class="ln">3689 </span></a>        &gt;&gt;&gt; c = torch.randn(4, 1) 
<a name="l3690"><span class="ln">3690 </span></a>        &gt;&gt;&gt; c 
<a name="l3691"><span class="ln">3691 </span></a>        tensor([[ 0.3743], 
<a name="l3692"><span class="ln">3692 </span></a>                [-1.7724], 
<a name="l3693"><span class="ln">3693 </span></a>                [-0.5811], 
<a name="l3694"><span class="ln">3694 </span></a>                [-0.8017]]) 
<a name="l3695"><span class="ln">3695 </span></a>        &gt;&gt;&gt; torch.add(b, c, alpha=10) 
<a name="l3696"><span class="ln">3696 </span></a>        tensor([[  2.7695,   3.3930,   4.3672,   4.1450], 
<a name="l3697"><span class="ln">3697 </span></a>                [-18.6971, -18.0736, -17.0994, -17.3216], 
<a name="l3698"><span class="ln">3698 </span></a>                [ -6.7845,  -6.1610,  -5.1868,  -5.4090], 
<a name="l3699"><span class="ln">3699 </span></a>                [ -8.9902,  -8.3667,  -7.3925,  -7.6147]]) 
<a name="l3700"><span class="ln">3700 </span></a>    &quot;&quot;&quot;</span>
<a name="l3701"><span class="ln">3701 </span></a>
<a name="l3702"><span class="ln">3702 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3703"><span class="ln">3703 </span></a><span class="s2">def </span><span class="s1">add</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3704"><span class="ln">3704 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3705"><span class="ln">3705 </span></a>    add(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l3706"><span class="ln">3706 </span></a> 
<a name="l3707"><span class="ln">3707 </span></a>    Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`. 
<a name="l3708"><span class="ln">3708 </span></a> 
<a name="l3709"><span class="ln">3709 </span></a>    .. math:: 
<a name="l3710"><span class="ln">3710 </span></a>        \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i 
<a name="l3711"><span class="ln">3711 </span></a> 
<a name="l3712"><span class="ln">3712 </span></a> 
<a name="l3713"><span class="ln">3713 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l3714"><span class="ln">3714 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l3715"><span class="ln">3715 </span></a> 
<a name="l3716"><span class="ln">3716 </span></a>    Args: 
<a name="l3717"><span class="ln">3717 </span></a>        input (Tensor): the input tensor. 
<a name="l3718"><span class="ln">3718 </span></a>        other (Tensor or Number): the tensor or number to add to :attr:`input`. 
<a name="l3719"><span class="ln">3719 </span></a> 
<a name="l3720"><span class="ln">3720 </span></a>    Keyword arguments: 
<a name="l3721"><span class="ln">3721 </span></a>        alpha (Number): the multiplier for :attr:`other`. 
<a name="l3722"><span class="ln">3722 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3723"><span class="ln">3723 </span></a> 
<a name="l3724"><span class="ln">3724 </span></a>    Examples:: 
<a name="l3725"><span class="ln">3725 </span></a> 
<a name="l3726"><span class="ln">3726 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l3727"><span class="ln">3727 </span></a>        &gt;&gt;&gt; a 
<a name="l3728"><span class="ln">3728 </span></a>        tensor([ 0.0202,  1.0985,  1.3506, -0.6056]) 
<a name="l3729"><span class="ln">3729 </span></a>        &gt;&gt;&gt; torch.add(a, 20) 
<a name="l3730"><span class="ln">3730 </span></a>        tensor([ 20.0202,  21.0985,  21.3506,  19.3944]) 
<a name="l3731"><span class="ln">3731 </span></a> 
<a name="l3732"><span class="ln">3732 </span></a>        &gt;&gt;&gt; b = torch.randn(4) 
<a name="l3733"><span class="ln">3733 </span></a>        &gt;&gt;&gt; b 
<a name="l3734"><span class="ln">3734 </span></a>        tensor([-0.9732, -0.3497,  0.6245,  0.4022]) 
<a name="l3735"><span class="ln">3735 </span></a>        &gt;&gt;&gt; c = torch.randn(4, 1) 
<a name="l3736"><span class="ln">3736 </span></a>        &gt;&gt;&gt; c 
<a name="l3737"><span class="ln">3737 </span></a>        tensor([[ 0.3743], 
<a name="l3738"><span class="ln">3738 </span></a>                [-1.7724], 
<a name="l3739"><span class="ln">3739 </span></a>                [-0.5811], 
<a name="l3740"><span class="ln">3740 </span></a>                [-0.8017]]) 
<a name="l3741"><span class="ln">3741 </span></a>        &gt;&gt;&gt; torch.add(b, c, alpha=10) 
<a name="l3742"><span class="ln">3742 </span></a>        tensor([[  2.7695,   3.3930,   4.3672,   4.1450], 
<a name="l3743"><span class="ln">3743 </span></a>                [-18.6971, -18.0736, -17.0994, -17.3216], 
<a name="l3744"><span class="ln">3744 </span></a>                [ -6.7845,  -6.1610,  -5.1868,  -5.4090], 
<a name="l3745"><span class="ln">3745 </span></a>                [ -8.9902,  -8.3667,  -7.3925,  -7.6147]]) 
<a name="l3746"><span class="ln">3746 </span></a>    &quot;&quot;&quot;</span>
<a name="l3747"><span class="ln">3747 </span></a>
<a name="l3748"><span class="ln">3748 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3749"><span class="ln">3749 </span></a><span class="s2">def </span><span class="s1">add</span><span class="s3">(</span>
<a name="l3750"><span class="ln">3750 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3751"><span class="ln">3751 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3752"><span class="ln">3752 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3753"><span class="ln">3753 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3754"><span class="ln">3754 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3755"><span class="ln">3755 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3756"><span class="ln">3756 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3757"><span class="ln">3757 </span></a>    add(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l3758"><span class="ln">3758 </span></a> 
<a name="l3759"><span class="ln">3759 </span></a>    Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`. 
<a name="l3760"><span class="ln">3760 </span></a> 
<a name="l3761"><span class="ln">3761 </span></a>    .. math:: 
<a name="l3762"><span class="ln">3762 </span></a>        \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i 
<a name="l3763"><span class="ln">3763 </span></a> 
<a name="l3764"><span class="ln">3764 </span></a> 
<a name="l3765"><span class="ln">3765 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l3766"><span class="ln">3766 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l3767"><span class="ln">3767 </span></a> 
<a name="l3768"><span class="ln">3768 </span></a>    Args: 
<a name="l3769"><span class="ln">3769 </span></a>        input (Tensor): the input tensor. 
<a name="l3770"><span class="ln">3770 </span></a>        other (Tensor or Number): the tensor or number to add to :attr:`input`. 
<a name="l3771"><span class="ln">3771 </span></a> 
<a name="l3772"><span class="ln">3772 </span></a>    Keyword arguments: 
<a name="l3773"><span class="ln">3773 </span></a>        alpha (Number): the multiplier for :attr:`other`. 
<a name="l3774"><span class="ln">3774 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3775"><span class="ln">3775 </span></a> 
<a name="l3776"><span class="ln">3776 </span></a>    Examples:: 
<a name="l3777"><span class="ln">3777 </span></a> 
<a name="l3778"><span class="ln">3778 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l3779"><span class="ln">3779 </span></a>        &gt;&gt;&gt; a 
<a name="l3780"><span class="ln">3780 </span></a>        tensor([ 0.0202,  1.0985,  1.3506, -0.6056]) 
<a name="l3781"><span class="ln">3781 </span></a>        &gt;&gt;&gt; torch.add(a, 20) 
<a name="l3782"><span class="ln">3782 </span></a>        tensor([ 20.0202,  21.0985,  21.3506,  19.3944]) 
<a name="l3783"><span class="ln">3783 </span></a> 
<a name="l3784"><span class="ln">3784 </span></a>        &gt;&gt;&gt; b = torch.randn(4) 
<a name="l3785"><span class="ln">3785 </span></a>        &gt;&gt;&gt; b 
<a name="l3786"><span class="ln">3786 </span></a>        tensor([-0.9732, -0.3497,  0.6245,  0.4022]) 
<a name="l3787"><span class="ln">3787 </span></a>        &gt;&gt;&gt; c = torch.randn(4, 1) 
<a name="l3788"><span class="ln">3788 </span></a>        &gt;&gt;&gt; c 
<a name="l3789"><span class="ln">3789 </span></a>        tensor([[ 0.3743], 
<a name="l3790"><span class="ln">3790 </span></a>                [-1.7724], 
<a name="l3791"><span class="ln">3791 </span></a>                [-0.5811], 
<a name="l3792"><span class="ln">3792 </span></a>                [-0.8017]]) 
<a name="l3793"><span class="ln">3793 </span></a>        &gt;&gt;&gt; torch.add(b, c, alpha=10) 
<a name="l3794"><span class="ln">3794 </span></a>        tensor([[  2.7695,   3.3930,   4.3672,   4.1450], 
<a name="l3795"><span class="ln">3795 </span></a>                [-18.6971, -18.0736, -17.0994, -17.3216], 
<a name="l3796"><span class="ln">3796 </span></a>                [ -6.7845,  -6.1610,  -5.1868,  -5.4090], 
<a name="l3797"><span class="ln">3797 </span></a>                [ -8.9902,  -8.3667,  -7.3925,  -7.6147]]) 
<a name="l3798"><span class="ln">3798 </span></a>    &quot;&quot;&quot;</span>
<a name="l3799"><span class="ln">3799 </span></a>
<a name="l3800"><span class="ln">3800 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3801"><span class="ln">3801 </span></a><span class="s2">def </span><span class="s1">addbmm</span><span class="s3">(</span>
<a name="l3802"><span class="ln">3802 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3803"><span class="ln">3803 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3804"><span class="ln">3804 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3805"><span class="ln">3805 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3806"><span class="ln">3806 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3807"><span class="ln">3807 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3808"><span class="ln">3808 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3809"><span class="ln">3809 </span></a>    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l3810"><span class="ln">3810 </span></a> 
<a name="l3811"><span class="ln">3811 </span></a>    Performs a batch matrix-matrix product of matrices stored 
<a name="l3812"><span class="ln">3812 </span></a>    in :attr:`batch1` and :attr:`batch2`, 
<a name="l3813"><span class="ln">3813 </span></a>    with a reduced add step (all matrix multiplications get accumulated 
<a name="l3814"><span class="ln">3814 </span></a>    along the first dimension). 
<a name="l3815"><span class="ln">3815 </span></a>    :attr:`input` is added to the final result. 
<a name="l3816"><span class="ln">3816 </span></a> 
<a name="l3817"><span class="ln">3817 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the 
<a name="l3818"><span class="ln">3818 </span></a>    same number of matrices. 
<a name="l3819"><span class="ln">3819 </span></a> 
<a name="l3820"><span class="ln">3820 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l3821"><span class="ln">3821 </span></a>    :math:`(b \times m \times p)` tensor, :attr:`input` must be 
<a name="l3822"><span class="ln">3822 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l3823"><span class="ln">3823 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l3824"><span class="ln">3824 </span></a> 
<a name="l3825"><span class="ln">3825 </span></a>    .. math:: 
<a name="l3826"><span class="ln">3826 </span></a>        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l3827"><span class="ln">3827 </span></a> 
<a name="l3828"><span class="ln">3828 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l3829"><span class="ln">3829 </span></a>    it will not be propagated. 
<a name="l3830"><span class="ln">3830 </span></a> 
<a name="l3831"><span class="ln">3831 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha` 
<a name="l3832"><span class="ln">3832 </span></a>    must be real numbers, otherwise they should be integers. 
<a name="l3833"><span class="ln">3833 </span></a> 
<a name="l3834"><span class="ln">3834 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l3835"><span class="ln">3835 </span></a> 
<a name="l3836"><span class="ln">3836 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l3837"><span class="ln">3837 </span></a> 
<a name="l3838"><span class="ln">3838 </span></a>    Args: 
<a name="l3839"><span class="ln">3839 </span></a>        input (Tensor): matrix to be added 
<a name="l3840"><span class="ln">3840 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l3841"><span class="ln">3841 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l3842"><span class="ln">3842 </span></a> 
<a name="l3843"><span class="ln">3843 </span></a>    Keyword args: 
<a name="l3844"><span class="ln">3844 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l3845"><span class="ln">3845 </span></a>        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`) 
<a name="l3846"><span class="ln">3846 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3847"><span class="ln">3847 </span></a> 
<a name="l3848"><span class="ln">3848 </span></a>    Example:: 
<a name="l3849"><span class="ln">3849 </span></a> 
<a name="l3850"><span class="ln">3850 </span></a>        &gt;&gt;&gt; M = torch.randn(3, 5) 
<a name="l3851"><span class="ln">3851 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l3852"><span class="ln">3852 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l3853"><span class="ln">3853 </span></a>        &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) 
<a name="l3854"><span class="ln">3854 </span></a>        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653], 
<a name="l3855"><span class="ln">3855 </span></a>                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743], 
<a name="l3856"><span class="ln">3856 </span></a>                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]]) 
<a name="l3857"><span class="ln">3857 </span></a>    &quot;&quot;&quot;</span>
<a name="l3858"><span class="ln">3858 </span></a>
<a name="l3859"><span class="ln">3859 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3860"><span class="ln">3860 </span></a><span class="s2">def </span><span class="s1">addbmm</span><span class="s3">(</span>
<a name="l3861"><span class="ln">3861 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3862"><span class="ln">3862 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3863"><span class="ln">3863 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3864"><span class="ln">3864 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3865"><span class="ln">3865 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3866"><span class="ln">3866 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3867"><span class="ln">3867 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3868"><span class="ln">3868 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3869"><span class="ln">3869 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3870"><span class="ln">3870 </span></a>    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l3871"><span class="ln">3871 </span></a> 
<a name="l3872"><span class="ln">3872 </span></a>    Performs a batch matrix-matrix product of matrices stored 
<a name="l3873"><span class="ln">3873 </span></a>    in :attr:`batch1` and :attr:`batch2`, 
<a name="l3874"><span class="ln">3874 </span></a>    with a reduced add step (all matrix multiplications get accumulated 
<a name="l3875"><span class="ln">3875 </span></a>    along the first dimension). 
<a name="l3876"><span class="ln">3876 </span></a>    :attr:`input` is added to the final result. 
<a name="l3877"><span class="ln">3877 </span></a> 
<a name="l3878"><span class="ln">3878 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the 
<a name="l3879"><span class="ln">3879 </span></a>    same number of matrices. 
<a name="l3880"><span class="ln">3880 </span></a> 
<a name="l3881"><span class="ln">3881 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l3882"><span class="ln">3882 </span></a>    :math:`(b \times m \times p)` tensor, :attr:`input` must be 
<a name="l3883"><span class="ln">3883 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l3884"><span class="ln">3884 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l3885"><span class="ln">3885 </span></a> 
<a name="l3886"><span class="ln">3886 </span></a>    .. math:: 
<a name="l3887"><span class="ln">3887 </span></a>        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l3888"><span class="ln">3888 </span></a> 
<a name="l3889"><span class="ln">3889 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l3890"><span class="ln">3890 </span></a>    it will not be propagated. 
<a name="l3891"><span class="ln">3891 </span></a> 
<a name="l3892"><span class="ln">3892 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha` 
<a name="l3893"><span class="ln">3893 </span></a>    must be real numbers, otherwise they should be integers. 
<a name="l3894"><span class="ln">3894 </span></a> 
<a name="l3895"><span class="ln">3895 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l3896"><span class="ln">3896 </span></a> 
<a name="l3897"><span class="ln">3897 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l3898"><span class="ln">3898 </span></a> 
<a name="l3899"><span class="ln">3899 </span></a>    Args: 
<a name="l3900"><span class="ln">3900 </span></a>        input (Tensor): matrix to be added 
<a name="l3901"><span class="ln">3901 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l3902"><span class="ln">3902 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l3903"><span class="ln">3903 </span></a> 
<a name="l3904"><span class="ln">3904 </span></a>    Keyword args: 
<a name="l3905"><span class="ln">3905 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l3906"><span class="ln">3906 </span></a>        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`) 
<a name="l3907"><span class="ln">3907 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3908"><span class="ln">3908 </span></a> 
<a name="l3909"><span class="ln">3909 </span></a>    Example:: 
<a name="l3910"><span class="ln">3910 </span></a> 
<a name="l3911"><span class="ln">3911 </span></a>        &gt;&gt;&gt; M = torch.randn(3, 5) 
<a name="l3912"><span class="ln">3912 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l3913"><span class="ln">3913 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l3914"><span class="ln">3914 </span></a>        &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) 
<a name="l3915"><span class="ln">3915 </span></a>        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653], 
<a name="l3916"><span class="ln">3916 </span></a>                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743], 
<a name="l3917"><span class="ln">3917 </span></a>                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]]) 
<a name="l3918"><span class="ln">3918 </span></a>    &quot;&quot;&quot;</span>
<a name="l3919"><span class="ln">3919 </span></a>
<a name="l3920"><span class="ln">3920 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3921"><span class="ln">3921 </span></a><span class="s2">def </span><span class="s1">addbmm</span><span class="s3">(</span>
<a name="l3922"><span class="ln">3922 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3923"><span class="ln">3923 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3924"><span class="ln">3924 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3925"><span class="ln">3925 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l3926"><span class="ln">3926 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3927"><span class="ln">3927 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l3928"><span class="ln">3928 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l3929"><span class="ln">3929 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3930"><span class="ln">3930 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3931"><span class="ln">3931 </span></a>    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l3932"><span class="ln">3932 </span></a> 
<a name="l3933"><span class="ln">3933 </span></a>    Performs a batch matrix-matrix product of matrices stored 
<a name="l3934"><span class="ln">3934 </span></a>    in :attr:`batch1` and :attr:`batch2`, 
<a name="l3935"><span class="ln">3935 </span></a>    with a reduced add step (all matrix multiplications get accumulated 
<a name="l3936"><span class="ln">3936 </span></a>    along the first dimension). 
<a name="l3937"><span class="ln">3937 </span></a>    :attr:`input` is added to the final result. 
<a name="l3938"><span class="ln">3938 </span></a> 
<a name="l3939"><span class="ln">3939 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the 
<a name="l3940"><span class="ln">3940 </span></a>    same number of matrices. 
<a name="l3941"><span class="ln">3941 </span></a> 
<a name="l3942"><span class="ln">3942 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l3943"><span class="ln">3943 </span></a>    :math:`(b \times m \times p)` tensor, :attr:`input` must be 
<a name="l3944"><span class="ln">3944 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l3945"><span class="ln">3945 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l3946"><span class="ln">3946 </span></a> 
<a name="l3947"><span class="ln">3947 </span></a>    .. math:: 
<a name="l3948"><span class="ln">3948 </span></a>        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l3949"><span class="ln">3949 </span></a> 
<a name="l3950"><span class="ln">3950 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l3951"><span class="ln">3951 </span></a>    it will not be propagated. 
<a name="l3952"><span class="ln">3952 </span></a> 
<a name="l3953"><span class="ln">3953 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha` 
<a name="l3954"><span class="ln">3954 </span></a>    must be real numbers, otherwise they should be integers. 
<a name="l3955"><span class="ln">3955 </span></a> 
<a name="l3956"><span class="ln">3956 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l3957"><span class="ln">3957 </span></a> 
<a name="l3958"><span class="ln">3958 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l3959"><span class="ln">3959 </span></a> 
<a name="l3960"><span class="ln">3960 </span></a>    Args: 
<a name="l3961"><span class="ln">3961 </span></a>        input (Tensor): matrix to be added 
<a name="l3962"><span class="ln">3962 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l3963"><span class="ln">3963 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l3964"><span class="ln">3964 </span></a> 
<a name="l3965"><span class="ln">3965 </span></a>    Keyword args: 
<a name="l3966"><span class="ln">3966 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l3967"><span class="ln">3967 </span></a>        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`) 
<a name="l3968"><span class="ln">3968 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l3969"><span class="ln">3969 </span></a> 
<a name="l3970"><span class="ln">3970 </span></a>    Example:: 
<a name="l3971"><span class="ln">3971 </span></a> 
<a name="l3972"><span class="ln">3972 </span></a>        &gt;&gt;&gt; M = torch.randn(3, 5) 
<a name="l3973"><span class="ln">3973 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l3974"><span class="ln">3974 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l3975"><span class="ln">3975 </span></a>        &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) 
<a name="l3976"><span class="ln">3976 </span></a>        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653], 
<a name="l3977"><span class="ln">3977 </span></a>                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743], 
<a name="l3978"><span class="ln">3978 </span></a>                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]]) 
<a name="l3979"><span class="ln">3979 </span></a>    &quot;&quot;&quot;</span>
<a name="l3980"><span class="ln">3980 </span></a>
<a name="l3981"><span class="ln">3981 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l3982"><span class="ln">3982 </span></a><span class="s2">def </span><span class="s1">addbmm</span><span class="s3">(</span>
<a name="l3983"><span class="ln">3983 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l3984"><span class="ln">3984 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3985"><span class="ln">3985 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3986"><span class="ln">3986 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l3987"><span class="ln">3987 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l3988"><span class="ln">3988 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l3989"><span class="ln">3989 </span></a>    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l3990"><span class="ln">3990 </span></a> 
<a name="l3991"><span class="ln">3991 </span></a>    Performs a batch matrix-matrix product of matrices stored 
<a name="l3992"><span class="ln">3992 </span></a>    in :attr:`batch1` and :attr:`batch2`, 
<a name="l3993"><span class="ln">3993 </span></a>    with a reduced add step (all matrix multiplications get accumulated 
<a name="l3994"><span class="ln">3994 </span></a>    along the first dimension). 
<a name="l3995"><span class="ln">3995 </span></a>    :attr:`input` is added to the final result. 
<a name="l3996"><span class="ln">3996 </span></a> 
<a name="l3997"><span class="ln">3997 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the 
<a name="l3998"><span class="ln">3998 </span></a>    same number of matrices. 
<a name="l3999"><span class="ln">3999 </span></a> 
<a name="l4000"><span class="ln">4000 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l4001"><span class="ln">4001 </span></a>    :math:`(b \times m \times p)` tensor, :attr:`input` must be 
<a name="l4002"><span class="ln">4002 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4003"><span class="ln">4003 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4004"><span class="ln">4004 </span></a> 
<a name="l4005"><span class="ln">4005 </span></a>    .. math:: 
<a name="l4006"><span class="ln">4006 </span></a>        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l4007"><span class="ln">4007 </span></a> 
<a name="l4008"><span class="ln">4008 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4009"><span class="ln">4009 </span></a>    it will not be propagated. 
<a name="l4010"><span class="ln">4010 </span></a> 
<a name="l4011"><span class="ln">4011 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha` 
<a name="l4012"><span class="ln">4012 </span></a>    must be real numbers, otherwise they should be integers. 
<a name="l4013"><span class="ln">4013 </span></a> 
<a name="l4014"><span class="ln">4014 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4015"><span class="ln">4015 </span></a> 
<a name="l4016"><span class="ln">4016 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4017"><span class="ln">4017 </span></a> 
<a name="l4018"><span class="ln">4018 </span></a>    Args: 
<a name="l4019"><span class="ln">4019 </span></a>        input (Tensor): matrix to be added 
<a name="l4020"><span class="ln">4020 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l4021"><span class="ln">4021 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l4022"><span class="ln">4022 </span></a> 
<a name="l4023"><span class="ln">4023 </span></a>    Keyword args: 
<a name="l4024"><span class="ln">4024 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4025"><span class="ln">4025 </span></a>        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`) 
<a name="l4026"><span class="ln">4026 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4027"><span class="ln">4027 </span></a> 
<a name="l4028"><span class="ln">4028 </span></a>    Example:: 
<a name="l4029"><span class="ln">4029 </span></a> 
<a name="l4030"><span class="ln">4030 </span></a>        &gt;&gt;&gt; M = torch.randn(3, 5) 
<a name="l4031"><span class="ln">4031 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l4032"><span class="ln">4032 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l4033"><span class="ln">4033 </span></a>        &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) 
<a name="l4034"><span class="ln">4034 </span></a>        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653], 
<a name="l4035"><span class="ln">4035 </span></a>                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743], 
<a name="l4036"><span class="ln">4036 </span></a>                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]]) 
<a name="l4037"><span class="ln">4037 </span></a>    &quot;&quot;&quot;</span>
<a name="l4038"><span class="ln">4038 </span></a>
<a name="l4039"><span class="ln">4039 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4040"><span class="ln">4040 </span></a><span class="s2">def </span><span class="s1">addbmm</span><span class="s3">(</span>
<a name="l4041"><span class="ln">4041 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4042"><span class="ln">4042 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4043"><span class="ln">4043 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4044"><span class="ln">4044 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4045"><span class="ln">4045 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4046"><span class="ln">4046 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4047"><span class="ln">4047 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4048"><span class="ln">4048 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4049"><span class="ln">4049 </span></a>    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4050"><span class="ln">4050 </span></a> 
<a name="l4051"><span class="ln">4051 </span></a>    Performs a batch matrix-matrix product of matrices stored 
<a name="l4052"><span class="ln">4052 </span></a>    in :attr:`batch1` and :attr:`batch2`, 
<a name="l4053"><span class="ln">4053 </span></a>    with a reduced add step (all matrix multiplications get accumulated 
<a name="l4054"><span class="ln">4054 </span></a>    along the first dimension). 
<a name="l4055"><span class="ln">4055 </span></a>    :attr:`input` is added to the final result. 
<a name="l4056"><span class="ln">4056 </span></a> 
<a name="l4057"><span class="ln">4057 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the 
<a name="l4058"><span class="ln">4058 </span></a>    same number of matrices. 
<a name="l4059"><span class="ln">4059 </span></a> 
<a name="l4060"><span class="ln">4060 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l4061"><span class="ln">4061 </span></a>    :math:`(b \times m \times p)` tensor, :attr:`input` must be 
<a name="l4062"><span class="ln">4062 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4063"><span class="ln">4063 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4064"><span class="ln">4064 </span></a> 
<a name="l4065"><span class="ln">4065 </span></a>    .. math:: 
<a name="l4066"><span class="ln">4066 </span></a>        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l4067"><span class="ln">4067 </span></a> 
<a name="l4068"><span class="ln">4068 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4069"><span class="ln">4069 </span></a>    it will not be propagated. 
<a name="l4070"><span class="ln">4070 </span></a> 
<a name="l4071"><span class="ln">4071 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha` 
<a name="l4072"><span class="ln">4072 </span></a>    must be real numbers, otherwise they should be integers. 
<a name="l4073"><span class="ln">4073 </span></a> 
<a name="l4074"><span class="ln">4074 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4075"><span class="ln">4075 </span></a> 
<a name="l4076"><span class="ln">4076 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4077"><span class="ln">4077 </span></a> 
<a name="l4078"><span class="ln">4078 </span></a>    Args: 
<a name="l4079"><span class="ln">4079 </span></a>        input (Tensor): matrix to be added 
<a name="l4080"><span class="ln">4080 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l4081"><span class="ln">4081 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l4082"><span class="ln">4082 </span></a> 
<a name="l4083"><span class="ln">4083 </span></a>    Keyword args: 
<a name="l4084"><span class="ln">4084 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4085"><span class="ln">4085 </span></a>        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`) 
<a name="l4086"><span class="ln">4086 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4087"><span class="ln">4087 </span></a> 
<a name="l4088"><span class="ln">4088 </span></a>    Example:: 
<a name="l4089"><span class="ln">4089 </span></a> 
<a name="l4090"><span class="ln">4090 </span></a>        &gt;&gt;&gt; M = torch.randn(3, 5) 
<a name="l4091"><span class="ln">4091 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l4092"><span class="ln">4092 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l4093"><span class="ln">4093 </span></a>        &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) 
<a name="l4094"><span class="ln">4094 </span></a>        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653], 
<a name="l4095"><span class="ln">4095 </span></a>                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743], 
<a name="l4096"><span class="ln">4096 </span></a>                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]]) 
<a name="l4097"><span class="ln">4097 </span></a>    &quot;&quot;&quot;</span>
<a name="l4098"><span class="ln">4098 </span></a>
<a name="l4099"><span class="ln">4099 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4100"><span class="ln">4100 </span></a><span class="s2">def </span><span class="s1">addcdiv</span><span class="s3">(</span>
<a name="l4101"><span class="ln">4101 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4102"><span class="ln">4102 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4103"><span class="ln">4103 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4104"><span class="ln">4104 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4105"><span class="ln">4105 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4106"><span class="ln">4106 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4107"><span class="ln">4107 </span></a>    addcdiv(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l4108"><span class="ln">4108 </span></a> 
<a name="l4109"><span class="ln">4109 </span></a>    Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`, 
<a name="l4110"><span class="ln">4110 </span></a>    multiplies the result by the scalar :attr:`value` and adds it to :attr:`input`. 
<a name="l4111"><span class="ln">4111 </span></a> 
<a name="l4112"><span class="ln">4112 </span></a>    .. warning:: 
<a name="l4113"><span class="ln">4113 </span></a>        Integer division with addcdiv is no longer supported, and in a future 
<a name="l4114"><span class="ln">4114 </span></a>        release addcdiv will perform a true division of tensor1 and tensor2. 
<a name="l4115"><span class="ln">4115 </span></a>        The historic addcdiv behavior can be implemented as 
<a name="l4116"><span class="ln">4116 </span></a>        (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) 
<a name="l4117"><span class="ln">4117 </span></a>        for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. 
<a name="l4118"><span class="ln">4118 </span></a>        The future addcdiv behavior is just the latter implementation: 
<a name="l4119"><span class="ln">4119 </span></a>        (input + value * tensor1 / tensor2), for all dtypes. 
<a name="l4120"><span class="ln">4120 </span></a> 
<a name="l4121"><span class="ln">4121 </span></a>    .. math:: 
<a name="l4122"><span class="ln">4122 </span></a>        \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i} 
<a name="l4123"><span class="ln">4123 </span></a> 
<a name="l4124"><span class="ln">4124 </span></a> 
<a name="l4125"><span class="ln">4125 </span></a>    The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l4126"><span class="ln">4126 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l4127"><span class="ln">4127 </span></a> 
<a name="l4128"><span class="ln">4128 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l4129"><span class="ln">4129 </span></a>    a real number, otherwise an integer. 
<a name="l4130"><span class="ln">4130 </span></a> 
<a name="l4131"><span class="ln">4131 </span></a>    Args: 
<a name="l4132"><span class="ln">4132 </span></a>        input (Tensor): the tensor to be added 
<a name="l4133"><span class="ln">4133 </span></a>        tensor1 (Tensor): the numerator tensor 
<a name="l4134"><span class="ln">4134 </span></a>        tensor2 (Tensor): the denominator tensor 
<a name="l4135"><span class="ln">4135 </span></a> 
<a name="l4136"><span class="ln">4136 </span></a>    Keyword args: 
<a name="l4137"><span class="ln">4137 </span></a>        value (Number, optional): multiplier for :math:`\text{tensor1} / \text{tensor2}` 
<a name="l4138"><span class="ln">4138 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4139"><span class="ln">4139 </span></a> 
<a name="l4140"><span class="ln">4140 </span></a>    Example:: 
<a name="l4141"><span class="ln">4141 </span></a> 
<a name="l4142"><span class="ln">4142 </span></a>        &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l4143"><span class="ln">4143 </span></a>        &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l4144"><span class="ln">4144 </span></a>        &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l4145"><span class="ln">4145 </span></a>        &gt;&gt;&gt; torch.addcdiv(t, t1, t2, value=0.1) 
<a name="l4146"><span class="ln">4146 </span></a>        tensor([[-0.2312, -3.6496,  0.1312], 
<a name="l4147"><span class="ln">4147 </span></a>                [-1.0428,  3.4292, -0.1030], 
<a name="l4148"><span class="ln">4148 </span></a>                [-0.5369, -0.9829,  0.0430]]) 
<a name="l4149"><span class="ln">4149 </span></a>    &quot;&quot;&quot;</span>
<a name="l4150"><span class="ln">4150 </span></a>
<a name="l4151"><span class="ln">4151 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4152"><span class="ln">4152 </span></a><span class="s2">def </span><span class="s1">addcdiv</span><span class="s3">(</span>
<a name="l4153"><span class="ln">4153 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4154"><span class="ln">4154 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4155"><span class="ln">4155 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4156"><span class="ln">4156 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4157"><span class="ln">4157 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4158"><span class="ln">4158 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4159"><span class="ln">4159 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4160"><span class="ln">4160 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4161"><span class="ln">4161 </span></a>    addcdiv(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l4162"><span class="ln">4162 </span></a> 
<a name="l4163"><span class="ln">4163 </span></a>    Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`, 
<a name="l4164"><span class="ln">4164 </span></a>    multiplies the result by the scalar :attr:`value` and adds it to :attr:`input`. 
<a name="l4165"><span class="ln">4165 </span></a> 
<a name="l4166"><span class="ln">4166 </span></a>    .. warning:: 
<a name="l4167"><span class="ln">4167 </span></a>        Integer division with addcdiv is no longer supported, and in a future 
<a name="l4168"><span class="ln">4168 </span></a>        release addcdiv will perform a true division of tensor1 and tensor2. 
<a name="l4169"><span class="ln">4169 </span></a>        The historic addcdiv behavior can be implemented as 
<a name="l4170"><span class="ln">4170 </span></a>        (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) 
<a name="l4171"><span class="ln">4171 </span></a>        for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. 
<a name="l4172"><span class="ln">4172 </span></a>        The future addcdiv behavior is just the latter implementation: 
<a name="l4173"><span class="ln">4173 </span></a>        (input + value * tensor1 / tensor2), for all dtypes. 
<a name="l4174"><span class="ln">4174 </span></a> 
<a name="l4175"><span class="ln">4175 </span></a>    .. math:: 
<a name="l4176"><span class="ln">4176 </span></a>        \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i} 
<a name="l4177"><span class="ln">4177 </span></a> 
<a name="l4178"><span class="ln">4178 </span></a> 
<a name="l4179"><span class="ln">4179 </span></a>    The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l4180"><span class="ln">4180 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l4181"><span class="ln">4181 </span></a> 
<a name="l4182"><span class="ln">4182 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l4183"><span class="ln">4183 </span></a>    a real number, otherwise an integer. 
<a name="l4184"><span class="ln">4184 </span></a> 
<a name="l4185"><span class="ln">4185 </span></a>    Args: 
<a name="l4186"><span class="ln">4186 </span></a>        input (Tensor): the tensor to be added 
<a name="l4187"><span class="ln">4187 </span></a>        tensor1 (Tensor): the numerator tensor 
<a name="l4188"><span class="ln">4188 </span></a>        tensor2 (Tensor): the denominator tensor 
<a name="l4189"><span class="ln">4189 </span></a> 
<a name="l4190"><span class="ln">4190 </span></a>    Keyword args: 
<a name="l4191"><span class="ln">4191 </span></a>        value (Number, optional): multiplier for :math:`\text{tensor1} / \text{tensor2}` 
<a name="l4192"><span class="ln">4192 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4193"><span class="ln">4193 </span></a> 
<a name="l4194"><span class="ln">4194 </span></a>    Example:: 
<a name="l4195"><span class="ln">4195 </span></a> 
<a name="l4196"><span class="ln">4196 </span></a>        &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l4197"><span class="ln">4197 </span></a>        &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l4198"><span class="ln">4198 </span></a>        &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l4199"><span class="ln">4199 </span></a>        &gt;&gt;&gt; torch.addcdiv(t, t1, t2, value=0.1) 
<a name="l4200"><span class="ln">4200 </span></a>        tensor([[-0.2312, -3.6496,  0.1312], 
<a name="l4201"><span class="ln">4201 </span></a>                [-1.0428,  3.4292, -0.1030], 
<a name="l4202"><span class="ln">4202 </span></a>                [-0.5369, -0.9829,  0.0430]]) 
<a name="l4203"><span class="ln">4203 </span></a>    &quot;&quot;&quot;</span>
<a name="l4204"><span class="ln">4204 </span></a>
<a name="l4205"><span class="ln">4205 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4206"><span class="ln">4206 </span></a><span class="s2">def </span><span class="s1">addcdiv</span><span class="s3">(</span>
<a name="l4207"><span class="ln">4207 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4208"><span class="ln">4208 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4209"><span class="ln">4209 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4210"><span class="ln">4210 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4211"><span class="ln">4211 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4212"><span class="ln">4212 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l4213"><span class="ln">4213 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4214"><span class="ln">4214 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4215"><span class="ln">4215 </span></a>    addcdiv(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l4216"><span class="ln">4216 </span></a> 
<a name="l4217"><span class="ln">4217 </span></a>    Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`, 
<a name="l4218"><span class="ln">4218 </span></a>    multiplies the result by the scalar :attr:`value` and adds it to :attr:`input`. 
<a name="l4219"><span class="ln">4219 </span></a> 
<a name="l4220"><span class="ln">4220 </span></a>    .. warning:: 
<a name="l4221"><span class="ln">4221 </span></a>        Integer division with addcdiv is no longer supported, and in a future 
<a name="l4222"><span class="ln">4222 </span></a>        release addcdiv will perform a true division of tensor1 and tensor2. 
<a name="l4223"><span class="ln">4223 </span></a>        The historic addcdiv behavior can be implemented as 
<a name="l4224"><span class="ln">4224 </span></a>        (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) 
<a name="l4225"><span class="ln">4225 </span></a>        for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. 
<a name="l4226"><span class="ln">4226 </span></a>        The future addcdiv behavior is just the latter implementation: 
<a name="l4227"><span class="ln">4227 </span></a>        (input + value * tensor1 / tensor2), for all dtypes. 
<a name="l4228"><span class="ln">4228 </span></a> 
<a name="l4229"><span class="ln">4229 </span></a>    .. math:: 
<a name="l4230"><span class="ln">4230 </span></a>        \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i} 
<a name="l4231"><span class="ln">4231 </span></a> 
<a name="l4232"><span class="ln">4232 </span></a> 
<a name="l4233"><span class="ln">4233 </span></a>    The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l4234"><span class="ln">4234 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l4235"><span class="ln">4235 </span></a> 
<a name="l4236"><span class="ln">4236 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l4237"><span class="ln">4237 </span></a>    a real number, otherwise an integer. 
<a name="l4238"><span class="ln">4238 </span></a> 
<a name="l4239"><span class="ln">4239 </span></a>    Args: 
<a name="l4240"><span class="ln">4240 </span></a>        input (Tensor): the tensor to be added 
<a name="l4241"><span class="ln">4241 </span></a>        tensor1 (Tensor): the numerator tensor 
<a name="l4242"><span class="ln">4242 </span></a>        tensor2 (Tensor): the denominator tensor 
<a name="l4243"><span class="ln">4243 </span></a> 
<a name="l4244"><span class="ln">4244 </span></a>    Keyword args: 
<a name="l4245"><span class="ln">4245 </span></a>        value (Number, optional): multiplier for :math:`\text{tensor1} / \text{tensor2}` 
<a name="l4246"><span class="ln">4246 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4247"><span class="ln">4247 </span></a> 
<a name="l4248"><span class="ln">4248 </span></a>    Example:: 
<a name="l4249"><span class="ln">4249 </span></a> 
<a name="l4250"><span class="ln">4250 </span></a>        &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l4251"><span class="ln">4251 </span></a>        &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l4252"><span class="ln">4252 </span></a>        &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l4253"><span class="ln">4253 </span></a>        &gt;&gt;&gt; torch.addcdiv(t, t1, t2, value=0.1) 
<a name="l4254"><span class="ln">4254 </span></a>        tensor([[-0.2312, -3.6496,  0.1312], 
<a name="l4255"><span class="ln">4255 </span></a>                [-1.0428,  3.4292, -0.1030], 
<a name="l4256"><span class="ln">4256 </span></a>                [-0.5369, -0.9829,  0.0430]]) 
<a name="l4257"><span class="ln">4257 </span></a>    &quot;&quot;&quot;</span>
<a name="l4258"><span class="ln">4258 </span></a>
<a name="l4259"><span class="ln">4259 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4260"><span class="ln">4260 </span></a><span class="s2">def </span><span class="s1">addcmul</span><span class="s3">(</span>
<a name="l4261"><span class="ln">4261 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4262"><span class="ln">4262 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4263"><span class="ln">4263 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4264"><span class="ln">4264 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4265"><span class="ln">4265 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4266"><span class="ln">4266 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4267"><span class="ln">4267 </span></a>    addcmul(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l4268"><span class="ln">4268 </span></a> 
<a name="l4269"><span class="ln">4269 </span></a>    Performs the element-wise multiplication of :attr:`tensor1` 
<a name="l4270"><span class="ln">4270 </span></a>    by :attr:`tensor2`, multiplies the result by the scalar :attr:`value` 
<a name="l4271"><span class="ln">4271 </span></a>    and adds it to :attr:`input`. 
<a name="l4272"><span class="ln">4272 </span></a> 
<a name="l4273"><span class="ln">4273 </span></a>    .. math:: 
<a name="l4274"><span class="ln">4274 </span></a>        \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i 
<a name="l4275"><span class="ln">4275 </span></a> 
<a name="l4276"><span class="ln">4276 </span></a>    The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l4277"><span class="ln">4277 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l4278"><span class="ln">4278 </span></a> 
<a name="l4279"><span class="ln">4279 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l4280"><span class="ln">4280 </span></a>    a real number, otherwise an integer. 
<a name="l4281"><span class="ln">4281 </span></a> 
<a name="l4282"><span class="ln">4282 </span></a>    Args: 
<a name="l4283"><span class="ln">4283 </span></a>        input (Tensor): the tensor to be added 
<a name="l4284"><span class="ln">4284 </span></a>        tensor1 (Tensor): the tensor to be multiplied 
<a name="l4285"><span class="ln">4285 </span></a>        tensor2 (Tensor): the tensor to be multiplied 
<a name="l4286"><span class="ln">4286 </span></a> 
<a name="l4287"><span class="ln">4287 </span></a>    Keyword args: 
<a name="l4288"><span class="ln">4288 </span></a>        value (Number, optional): multiplier for :math:`tensor1 .* tensor2` 
<a name="l4289"><span class="ln">4289 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4290"><span class="ln">4290 </span></a> 
<a name="l4291"><span class="ln">4291 </span></a>    Example:: 
<a name="l4292"><span class="ln">4292 </span></a> 
<a name="l4293"><span class="ln">4293 </span></a>        &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l4294"><span class="ln">4294 </span></a>        &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l4295"><span class="ln">4295 </span></a>        &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l4296"><span class="ln">4296 </span></a>        &gt;&gt;&gt; torch.addcmul(t, t1, t2, value=0.1) 
<a name="l4297"><span class="ln">4297 </span></a>        tensor([[-0.8635, -0.6391,  1.6174], 
<a name="l4298"><span class="ln">4298 </span></a>                [-0.7617, -0.5879,  1.7388], 
<a name="l4299"><span class="ln">4299 </span></a>                [-0.8353, -0.6249,  1.6511]]) 
<a name="l4300"><span class="ln">4300 </span></a>    &quot;&quot;&quot;</span>
<a name="l4301"><span class="ln">4301 </span></a>
<a name="l4302"><span class="ln">4302 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4303"><span class="ln">4303 </span></a><span class="s2">def </span><span class="s1">addcmul</span><span class="s3">(</span>
<a name="l4304"><span class="ln">4304 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4305"><span class="ln">4305 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4306"><span class="ln">4306 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4307"><span class="ln">4307 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4308"><span class="ln">4308 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4309"><span class="ln">4309 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4310"><span class="ln">4310 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4311"><span class="ln">4311 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4312"><span class="ln">4312 </span></a>    addcmul(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l4313"><span class="ln">4313 </span></a> 
<a name="l4314"><span class="ln">4314 </span></a>    Performs the element-wise multiplication of :attr:`tensor1` 
<a name="l4315"><span class="ln">4315 </span></a>    by :attr:`tensor2`, multiplies the result by the scalar :attr:`value` 
<a name="l4316"><span class="ln">4316 </span></a>    and adds it to :attr:`input`. 
<a name="l4317"><span class="ln">4317 </span></a> 
<a name="l4318"><span class="ln">4318 </span></a>    .. math:: 
<a name="l4319"><span class="ln">4319 </span></a>        \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i 
<a name="l4320"><span class="ln">4320 </span></a> 
<a name="l4321"><span class="ln">4321 </span></a>    The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l4322"><span class="ln">4322 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l4323"><span class="ln">4323 </span></a> 
<a name="l4324"><span class="ln">4324 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l4325"><span class="ln">4325 </span></a>    a real number, otherwise an integer. 
<a name="l4326"><span class="ln">4326 </span></a> 
<a name="l4327"><span class="ln">4327 </span></a>    Args: 
<a name="l4328"><span class="ln">4328 </span></a>        input (Tensor): the tensor to be added 
<a name="l4329"><span class="ln">4329 </span></a>        tensor1 (Tensor): the tensor to be multiplied 
<a name="l4330"><span class="ln">4330 </span></a>        tensor2 (Tensor): the tensor to be multiplied 
<a name="l4331"><span class="ln">4331 </span></a> 
<a name="l4332"><span class="ln">4332 </span></a>    Keyword args: 
<a name="l4333"><span class="ln">4333 </span></a>        value (Number, optional): multiplier for :math:`tensor1 .* tensor2` 
<a name="l4334"><span class="ln">4334 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4335"><span class="ln">4335 </span></a> 
<a name="l4336"><span class="ln">4336 </span></a>    Example:: 
<a name="l4337"><span class="ln">4337 </span></a> 
<a name="l4338"><span class="ln">4338 </span></a>        &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l4339"><span class="ln">4339 </span></a>        &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l4340"><span class="ln">4340 </span></a>        &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l4341"><span class="ln">4341 </span></a>        &gt;&gt;&gt; torch.addcmul(t, t1, t2, value=0.1) 
<a name="l4342"><span class="ln">4342 </span></a>        tensor([[-0.8635, -0.6391,  1.6174], 
<a name="l4343"><span class="ln">4343 </span></a>                [-0.7617, -0.5879,  1.7388], 
<a name="l4344"><span class="ln">4344 </span></a>                [-0.8353, -0.6249,  1.6511]]) 
<a name="l4345"><span class="ln">4345 </span></a>    &quot;&quot;&quot;</span>
<a name="l4346"><span class="ln">4346 </span></a>
<a name="l4347"><span class="ln">4347 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4348"><span class="ln">4348 </span></a><span class="s2">def </span><span class="s1">addcmul</span><span class="s3">(</span>
<a name="l4349"><span class="ln">4349 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4350"><span class="ln">4350 </span></a>    <span class="s1">tensor1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4351"><span class="ln">4351 </span></a>    <span class="s1">tensor2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4352"><span class="ln">4352 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4353"><span class="ln">4353 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4354"><span class="ln">4354 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l4355"><span class="ln">4355 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4356"><span class="ln">4356 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4357"><span class="ln">4357 </span></a>    addcmul(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor 
<a name="l4358"><span class="ln">4358 </span></a> 
<a name="l4359"><span class="ln">4359 </span></a>    Performs the element-wise multiplication of :attr:`tensor1` 
<a name="l4360"><span class="ln">4360 </span></a>    by :attr:`tensor2`, multiplies the result by the scalar :attr:`value` 
<a name="l4361"><span class="ln">4361 </span></a>    and adds it to :attr:`input`. 
<a name="l4362"><span class="ln">4362 </span></a> 
<a name="l4363"><span class="ln">4363 </span></a>    .. math:: 
<a name="l4364"><span class="ln">4364 </span></a>        \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i 
<a name="l4365"><span class="ln">4365 </span></a> 
<a name="l4366"><span class="ln">4366 </span></a>    The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be 
<a name="l4367"><span class="ln">4367 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l4368"><span class="ln">4368 </span></a> 
<a name="l4369"><span class="ln">4369 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be 
<a name="l4370"><span class="ln">4370 </span></a>    a real number, otherwise an integer. 
<a name="l4371"><span class="ln">4371 </span></a> 
<a name="l4372"><span class="ln">4372 </span></a>    Args: 
<a name="l4373"><span class="ln">4373 </span></a>        input (Tensor): the tensor to be added 
<a name="l4374"><span class="ln">4374 </span></a>        tensor1 (Tensor): the tensor to be multiplied 
<a name="l4375"><span class="ln">4375 </span></a>        tensor2 (Tensor): the tensor to be multiplied 
<a name="l4376"><span class="ln">4376 </span></a> 
<a name="l4377"><span class="ln">4377 </span></a>    Keyword args: 
<a name="l4378"><span class="ln">4378 </span></a>        value (Number, optional): multiplier for :math:`tensor1 .* tensor2` 
<a name="l4379"><span class="ln">4379 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4380"><span class="ln">4380 </span></a> 
<a name="l4381"><span class="ln">4381 </span></a>    Example:: 
<a name="l4382"><span class="ln">4382 </span></a> 
<a name="l4383"><span class="ln">4383 </span></a>        &gt;&gt;&gt; t = torch.randn(1, 3) 
<a name="l4384"><span class="ln">4384 </span></a>        &gt;&gt;&gt; t1 = torch.randn(3, 1) 
<a name="l4385"><span class="ln">4385 </span></a>        &gt;&gt;&gt; t2 = torch.randn(1, 3) 
<a name="l4386"><span class="ln">4386 </span></a>        &gt;&gt;&gt; torch.addcmul(t, t1, t2, value=0.1) 
<a name="l4387"><span class="ln">4387 </span></a>        tensor([[-0.8635, -0.6391,  1.6174], 
<a name="l4388"><span class="ln">4388 </span></a>                [-0.7617, -0.5879,  1.7388], 
<a name="l4389"><span class="ln">4389 </span></a>                [-0.8353, -0.6249,  1.6511]]) 
<a name="l4390"><span class="ln">4390 </span></a>    &quot;&quot;&quot;</span>
<a name="l4391"><span class="ln">4391 </span></a>
<a name="l4392"><span class="ln">4392 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4393"><span class="ln">4393 </span></a><span class="s2">def </span><span class="s1">addmm</span><span class="s3">(</span>
<a name="l4394"><span class="ln">4394 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4395"><span class="ln">4395 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4396"><span class="ln">4396 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4397"><span class="ln">4397 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4398"><span class="ln">4398 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4399"><span class="ln">4399 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4400"><span class="ln">4400 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4401"><span class="ln">4401 </span></a>    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4402"><span class="ln">4402 </span></a> 
<a name="l4403"><span class="ln">4403 </span></a>    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`. 
<a name="l4404"><span class="ln">4404 </span></a>    The matrix :attr:`input` is added to the final result. 
<a name="l4405"><span class="ln">4405 </span></a> 
<a name="l4406"><span class="ln">4406 </span></a>    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l4407"><span class="ln">4407 </span></a>    :math:`(m \times p)` tensor, then :attr:`input` must be 
<a name="l4408"><span class="ln">4408 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4409"><span class="ln">4409 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4410"><span class="ln">4410 </span></a> 
<a name="l4411"><span class="ln">4411 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4412"><span class="ln">4412 </span></a>    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively. 
<a name="l4413"><span class="ln">4413 </span></a> 
<a name="l4414"><span class="ln">4414 </span></a>    .. math:: 
<a name="l4415"><span class="ln">4415 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i) 
<a name="l4416"><span class="ln">4416 </span></a> 
<a name="l4417"><span class="ln">4417 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4418"><span class="ln">4418 </span></a>    it will not be propagated. 
<a name="l4419"><span class="ln">4419 </span></a> 
<a name="l4420"><span class="ln">4420 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4421"><span class="ln">4421 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4422"><span class="ln">4422 </span></a> 
<a name="l4423"><span class="ln">4423 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. If 
<a name="l4424"><span class="ln">4424 </span></a>    :attr:`input` is sparse the result will have the same layout and if :attr:`out` 
<a name="l4425"><span class="ln">4425 </span></a>    is provided it must have the same layout as :attr:`input`. 
<a name="l4426"><span class="ln">4426 </span></a> 
<a name="l4427"><span class="ln">4427 </span></a> 
<a name="l4428"><span class="ln">4428 </span></a>    .. warning:: 
<a name="l4429"><span class="ln">4429 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l4430"><span class="ln">4430 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l4431"><span class="ln">4431 </span></a>        open a feature request. 
<a name="l4432"><span class="ln">4432 </span></a> 
<a name="l4433"><span class="ln">4433 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4434"><span class="ln">4434 </span></a> 
<a name="l4435"><span class="ln">4435 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4436"><span class="ln">4436 </span></a> 
<a name="l4437"><span class="ln">4437 </span></a>    Args: 
<a name="l4438"><span class="ln">4438 </span></a>        input (Tensor): matrix to be added 
<a name="l4439"><span class="ln">4439 </span></a>        mat1 (Tensor): the first matrix to be matrix multiplied 
<a name="l4440"><span class="ln">4440 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l4441"><span class="ln">4441 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l4442"><span class="ln">4442 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l4443"><span class="ln">4443 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l4444"><span class="ln">4444 </span></a> 
<a name="l4445"><span class="ln">4445 </span></a>    Keyword args: 
<a name="l4446"><span class="ln">4446 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4447"><span class="ln">4447 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l4448"><span class="ln">4448 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4449"><span class="ln">4449 </span></a> 
<a name="l4450"><span class="ln">4450 </span></a>    Example:: 
<a name="l4451"><span class="ln">4451 </span></a> 
<a name="l4452"><span class="ln">4452 </span></a>        &gt;&gt;&gt; M = torch.randn(2, 3) 
<a name="l4453"><span class="ln">4453 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l4454"><span class="ln">4454 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l4455"><span class="ln">4455 </span></a>        &gt;&gt;&gt; torch.addmm(M, mat1, mat2) 
<a name="l4456"><span class="ln">4456 </span></a>        tensor([[-4.8716,  1.4671, -1.3746], 
<a name="l4457"><span class="ln">4457 </span></a>                [ 0.7573, -3.9555, -2.8681]]) 
<a name="l4458"><span class="ln">4458 </span></a>    &quot;&quot;&quot;</span>
<a name="l4459"><span class="ln">4459 </span></a>
<a name="l4460"><span class="ln">4460 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4461"><span class="ln">4461 </span></a><span class="s2">def </span><span class="s1">addmm</span><span class="s3">(</span>
<a name="l4462"><span class="ln">4462 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4463"><span class="ln">4463 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4464"><span class="ln">4464 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4465"><span class="ln">4465 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4466"><span class="ln">4466 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4467"><span class="ln">4467 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4468"><span class="ln">4468 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4469"><span class="ln">4469 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4470"><span class="ln">4470 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4471"><span class="ln">4471 </span></a>    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4472"><span class="ln">4472 </span></a> 
<a name="l4473"><span class="ln">4473 </span></a>    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`. 
<a name="l4474"><span class="ln">4474 </span></a>    The matrix :attr:`input` is added to the final result. 
<a name="l4475"><span class="ln">4475 </span></a> 
<a name="l4476"><span class="ln">4476 </span></a>    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l4477"><span class="ln">4477 </span></a>    :math:`(m \times p)` tensor, then :attr:`input` must be 
<a name="l4478"><span class="ln">4478 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4479"><span class="ln">4479 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4480"><span class="ln">4480 </span></a> 
<a name="l4481"><span class="ln">4481 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4482"><span class="ln">4482 </span></a>    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively. 
<a name="l4483"><span class="ln">4483 </span></a> 
<a name="l4484"><span class="ln">4484 </span></a>    .. math:: 
<a name="l4485"><span class="ln">4485 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i) 
<a name="l4486"><span class="ln">4486 </span></a> 
<a name="l4487"><span class="ln">4487 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4488"><span class="ln">4488 </span></a>    it will not be propagated. 
<a name="l4489"><span class="ln">4489 </span></a> 
<a name="l4490"><span class="ln">4490 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4491"><span class="ln">4491 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4492"><span class="ln">4492 </span></a> 
<a name="l4493"><span class="ln">4493 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. If 
<a name="l4494"><span class="ln">4494 </span></a>    :attr:`input` is sparse the result will have the same layout and if :attr:`out` 
<a name="l4495"><span class="ln">4495 </span></a>    is provided it must have the same layout as :attr:`input`. 
<a name="l4496"><span class="ln">4496 </span></a> 
<a name="l4497"><span class="ln">4497 </span></a> 
<a name="l4498"><span class="ln">4498 </span></a>    .. warning:: 
<a name="l4499"><span class="ln">4499 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l4500"><span class="ln">4500 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l4501"><span class="ln">4501 </span></a>        open a feature request. 
<a name="l4502"><span class="ln">4502 </span></a> 
<a name="l4503"><span class="ln">4503 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4504"><span class="ln">4504 </span></a> 
<a name="l4505"><span class="ln">4505 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4506"><span class="ln">4506 </span></a> 
<a name="l4507"><span class="ln">4507 </span></a>    Args: 
<a name="l4508"><span class="ln">4508 </span></a>        input (Tensor): matrix to be added 
<a name="l4509"><span class="ln">4509 </span></a>        mat1 (Tensor): the first matrix to be matrix multiplied 
<a name="l4510"><span class="ln">4510 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l4511"><span class="ln">4511 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l4512"><span class="ln">4512 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l4513"><span class="ln">4513 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l4514"><span class="ln">4514 </span></a> 
<a name="l4515"><span class="ln">4515 </span></a>    Keyword args: 
<a name="l4516"><span class="ln">4516 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4517"><span class="ln">4517 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l4518"><span class="ln">4518 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4519"><span class="ln">4519 </span></a> 
<a name="l4520"><span class="ln">4520 </span></a>    Example:: 
<a name="l4521"><span class="ln">4521 </span></a> 
<a name="l4522"><span class="ln">4522 </span></a>        &gt;&gt;&gt; M = torch.randn(2, 3) 
<a name="l4523"><span class="ln">4523 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l4524"><span class="ln">4524 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l4525"><span class="ln">4525 </span></a>        &gt;&gt;&gt; torch.addmm(M, mat1, mat2) 
<a name="l4526"><span class="ln">4526 </span></a>        tensor([[-4.8716,  1.4671, -1.3746], 
<a name="l4527"><span class="ln">4527 </span></a>                [ 0.7573, -3.9555, -2.8681]]) 
<a name="l4528"><span class="ln">4528 </span></a>    &quot;&quot;&quot;</span>
<a name="l4529"><span class="ln">4529 </span></a>
<a name="l4530"><span class="ln">4530 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4531"><span class="ln">4531 </span></a><span class="s2">def </span><span class="s1">addmm</span><span class="s3">(</span>
<a name="l4532"><span class="ln">4532 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4533"><span class="ln">4533 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4534"><span class="ln">4534 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4535"><span class="ln">4535 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4536"><span class="ln">4536 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4537"><span class="ln">4537 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4538"><span class="ln">4538 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l4539"><span class="ln">4539 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4540"><span class="ln">4540 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4541"><span class="ln">4541 </span></a>    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4542"><span class="ln">4542 </span></a> 
<a name="l4543"><span class="ln">4543 </span></a>    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`. 
<a name="l4544"><span class="ln">4544 </span></a>    The matrix :attr:`input` is added to the final result. 
<a name="l4545"><span class="ln">4545 </span></a> 
<a name="l4546"><span class="ln">4546 </span></a>    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l4547"><span class="ln">4547 </span></a>    :math:`(m \times p)` tensor, then :attr:`input` must be 
<a name="l4548"><span class="ln">4548 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4549"><span class="ln">4549 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4550"><span class="ln">4550 </span></a> 
<a name="l4551"><span class="ln">4551 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4552"><span class="ln">4552 </span></a>    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively. 
<a name="l4553"><span class="ln">4553 </span></a> 
<a name="l4554"><span class="ln">4554 </span></a>    .. math:: 
<a name="l4555"><span class="ln">4555 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i) 
<a name="l4556"><span class="ln">4556 </span></a> 
<a name="l4557"><span class="ln">4557 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4558"><span class="ln">4558 </span></a>    it will not be propagated. 
<a name="l4559"><span class="ln">4559 </span></a> 
<a name="l4560"><span class="ln">4560 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4561"><span class="ln">4561 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4562"><span class="ln">4562 </span></a> 
<a name="l4563"><span class="ln">4563 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. If 
<a name="l4564"><span class="ln">4564 </span></a>    :attr:`input` is sparse the result will have the same layout and if :attr:`out` 
<a name="l4565"><span class="ln">4565 </span></a>    is provided it must have the same layout as :attr:`input`. 
<a name="l4566"><span class="ln">4566 </span></a> 
<a name="l4567"><span class="ln">4567 </span></a> 
<a name="l4568"><span class="ln">4568 </span></a>    .. warning:: 
<a name="l4569"><span class="ln">4569 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l4570"><span class="ln">4570 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l4571"><span class="ln">4571 </span></a>        open a feature request. 
<a name="l4572"><span class="ln">4572 </span></a> 
<a name="l4573"><span class="ln">4573 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4574"><span class="ln">4574 </span></a> 
<a name="l4575"><span class="ln">4575 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4576"><span class="ln">4576 </span></a> 
<a name="l4577"><span class="ln">4577 </span></a>    Args: 
<a name="l4578"><span class="ln">4578 </span></a>        input (Tensor): matrix to be added 
<a name="l4579"><span class="ln">4579 </span></a>        mat1 (Tensor): the first matrix to be matrix multiplied 
<a name="l4580"><span class="ln">4580 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l4581"><span class="ln">4581 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l4582"><span class="ln">4582 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l4583"><span class="ln">4583 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l4584"><span class="ln">4584 </span></a> 
<a name="l4585"><span class="ln">4585 </span></a>    Keyword args: 
<a name="l4586"><span class="ln">4586 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4587"><span class="ln">4587 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l4588"><span class="ln">4588 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4589"><span class="ln">4589 </span></a> 
<a name="l4590"><span class="ln">4590 </span></a>    Example:: 
<a name="l4591"><span class="ln">4591 </span></a> 
<a name="l4592"><span class="ln">4592 </span></a>        &gt;&gt;&gt; M = torch.randn(2, 3) 
<a name="l4593"><span class="ln">4593 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l4594"><span class="ln">4594 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l4595"><span class="ln">4595 </span></a>        &gt;&gt;&gt; torch.addmm(M, mat1, mat2) 
<a name="l4596"><span class="ln">4596 </span></a>        tensor([[-4.8716,  1.4671, -1.3746], 
<a name="l4597"><span class="ln">4597 </span></a>                [ 0.7573, -3.9555, -2.8681]]) 
<a name="l4598"><span class="ln">4598 </span></a>    &quot;&quot;&quot;</span>
<a name="l4599"><span class="ln">4599 </span></a>
<a name="l4600"><span class="ln">4600 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4601"><span class="ln">4601 </span></a><span class="s2">def </span><span class="s1">addmm</span><span class="s3">(</span>
<a name="l4602"><span class="ln">4602 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4603"><span class="ln">4603 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4604"><span class="ln">4604 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4605"><span class="ln">4605 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l4606"><span class="ln">4606 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4607"><span class="ln">4607 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4608"><span class="ln">4608 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4609"><span class="ln">4609 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l4610"><span class="ln">4610 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4611"><span class="ln">4611 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4612"><span class="ln">4612 </span></a>    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4613"><span class="ln">4613 </span></a> 
<a name="l4614"><span class="ln">4614 </span></a>    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`. 
<a name="l4615"><span class="ln">4615 </span></a>    The matrix :attr:`input` is added to the final result. 
<a name="l4616"><span class="ln">4616 </span></a> 
<a name="l4617"><span class="ln">4617 </span></a>    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l4618"><span class="ln">4618 </span></a>    :math:`(m \times p)` tensor, then :attr:`input` must be 
<a name="l4619"><span class="ln">4619 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4620"><span class="ln">4620 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4621"><span class="ln">4621 </span></a> 
<a name="l4622"><span class="ln">4622 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4623"><span class="ln">4623 </span></a>    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively. 
<a name="l4624"><span class="ln">4624 </span></a> 
<a name="l4625"><span class="ln">4625 </span></a>    .. math:: 
<a name="l4626"><span class="ln">4626 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i) 
<a name="l4627"><span class="ln">4627 </span></a> 
<a name="l4628"><span class="ln">4628 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4629"><span class="ln">4629 </span></a>    it will not be propagated. 
<a name="l4630"><span class="ln">4630 </span></a> 
<a name="l4631"><span class="ln">4631 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4632"><span class="ln">4632 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4633"><span class="ln">4633 </span></a> 
<a name="l4634"><span class="ln">4634 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. If 
<a name="l4635"><span class="ln">4635 </span></a>    :attr:`input` is sparse the result will have the same layout and if :attr:`out` 
<a name="l4636"><span class="ln">4636 </span></a>    is provided it must have the same layout as :attr:`input`. 
<a name="l4637"><span class="ln">4637 </span></a> 
<a name="l4638"><span class="ln">4638 </span></a> 
<a name="l4639"><span class="ln">4639 </span></a>    .. warning:: 
<a name="l4640"><span class="ln">4640 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l4641"><span class="ln">4641 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l4642"><span class="ln">4642 </span></a>        open a feature request. 
<a name="l4643"><span class="ln">4643 </span></a> 
<a name="l4644"><span class="ln">4644 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4645"><span class="ln">4645 </span></a> 
<a name="l4646"><span class="ln">4646 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4647"><span class="ln">4647 </span></a> 
<a name="l4648"><span class="ln">4648 </span></a>    Args: 
<a name="l4649"><span class="ln">4649 </span></a>        input (Tensor): matrix to be added 
<a name="l4650"><span class="ln">4650 </span></a>        mat1 (Tensor): the first matrix to be matrix multiplied 
<a name="l4651"><span class="ln">4651 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l4652"><span class="ln">4652 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l4653"><span class="ln">4653 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l4654"><span class="ln">4654 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l4655"><span class="ln">4655 </span></a> 
<a name="l4656"><span class="ln">4656 </span></a>    Keyword args: 
<a name="l4657"><span class="ln">4657 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4658"><span class="ln">4658 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l4659"><span class="ln">4659 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4660"><span class="ln">4660 </span></a> 
<a name="l4661"><span class="ln">4661 </span></a>    Example:: 
<a name="l4662"><span class="ln">4662 </span></a> 
<a name="l4663"><span class="ln">4663 </span></a>        &gt;&gt;&gt; M = torch.randn(2, 3) 
<a name="l4664"><span class="ln">4664 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l4665"><span class="ln">4665 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l4666"><span class="ln">4666 </span></a>        &gt;&gt;&gt; torch.addmm(M, mat1, mat2) 
<a name="l4667"><span class="ln">4667 </span></a>        tensor([[-4.8716,  1.4671, -1.3746], 
<a name="l4668"><span class="ln">4668 </span></a>                [ 0.7573, -3.9555, -2.8681]]) 
<a name="l4669"><span class="ln">4669 </span></a>    &quot;&quot;&quot;</span>
<a name="l4670"><span class="ln">4670 </span></a>
<a name="l4671"><span class="ln">4671 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4672"><span class="ln">4672 </span></a><span class="s2">def </span><span class="s1">addmm</span><span class="s3">(</span>
<a name="l4673"><span class="ln">4673 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4674"><span class="ln">4674 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4675"><span class="ln">4675 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4676"><span class="ln">4676 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4677"><span class="ln">4677 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4678"><span class="ln">4678 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4679"><span class="ln">4679 </span></a>    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4680"><span class="ln">4680 </span></a> 
<a name="l4681"><span class="ln">4681 </span></a>    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`. 
<a name="l4682"><span class="ln">4682 </span></a>    The matrix :attr:`input` is added to the final result. 
<a name="l4683"><span class="ln">4683 </span></a> 
<a name="l4684"><span class="ln">4684 </span></a>    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l4685"><span class="ln">4685 </span></a>    :math:`(m \times p)` tensor, then :attr:`input` must be 
<a name="l4686"><span class="ln">4686 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4687"><span class="ln">4687 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4688"><span class="ln">4688 </span></a> 
<a name="l4689"><span class="ln">4689 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4690"><span class="ln">4690 </span></a>    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively. 
<a name="l4691"><span class="ln">4691 </span></a> 
<a name="l4692"><span class="ln">4692 </span></a>    .. math:: 
<a name="l4693"><span class="ln">4693 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i) 
<a name="l4694"><span class="ln">4694 </span></a> 
<a name="l4695"><span class="ln">4695 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4696"><span class="ln">4696 </span></a>    it will not be propagated. 
<a name="l4697"><span class="ln">4697 </span></a> 
<a name="l4698"><span class="ln">4698 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4699"><span class="ln">4699 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4700"><span class="ln">4700 </span></a> 
<a name="l4701"><span class="ln">4701 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. If 
<a name="l4702"><span class="ln">4702 </span></a>    :attr:`input` is sparse the result will have the same layout and if :attr:`out` 
<a name="l4703"><span class="ln">4703 </span></a>    is provided it must have the same layout as :attr:`input`. 
<a name="l4704"><span class="ln">4704 </span></a> 
<a name="l4705"><span class="ln">4705 </span></a> 
<a name="l4706"><span class="ln">4706 </span></a>    .. warning:: 
<a name="l4707"><span class="ln">4707 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l4708"><span class="ln">4708 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l4709"><span class="ln">4709 </span></a>        open a feature request. 
<a name="l4710"><span class="ln">4710 </span></a> 
<a name="l4711"><span class="ln">4711 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4712"><span class="ln">4712 </span></a> 
<a name="l4713"><span class="ln">4713 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4714"><span class="ln">4714 </span></a> 
<a name="l4715"><span class="ln">4715 </span></a>    Args: 
<a name="l4716"><span class="ln">4716 </span></a>        input (Tensor): matrix to be added 
<a name="l4717"><span class="ln">4717 </span></a>        mat1 (Tensor): the first matrix to be matrix multiplied 
<a name="l4718"><span class="ln">4718 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l4719"><span class="ln">4719 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l4720"><span class="ln">4720 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l4721"><span class="ln">4721 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l4722"><span class="ln">4722 </span></a> 
<a name="l4723"><span class="ln">4723 </span></a>    Keyword args: 
<a name="l4724"><span class="ln">4724 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4725"><span class="ln">4725 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l4726"><span class="ln">4726 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4727"><span class="ln">4727 </span></a> 
<a name="l4728"><span class="ln">4728 </span></a>    Example:: 
<a name="l4729"><span class="ln">4729 </span></a> 
<a name="l4730"><span class="ln">4730 </span></a>        &gt;&gt;&gt; M = torch.randn(2, 3) 
<a name="l4731"><span class="ln">4731 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l4732"><span class="ln">4732 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l4733"><span class="ln">4733 </span></a>        &gt;&gt;&gt; torch.addmm(M, mat1, mat2) 
<a name="l4734"><span class="ln">4734 </span></a>        tensor([[-4.8716,  1.4671, -1.3746], 
<a name="l4735"><span class="ln">4735 </span></a>                [ 0.7573, -3.9555, -2.8681]]) 
<a name="l4736"><span class="ln">4736 </span></a>    &quot;&quot;&quot;</span>
<a name="l4737"><span class="ln">4737 </span></a>
<a name="l4738"><span class="ln">4738 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4739"><span class="ln">4739 </span></a><span class="s2">def </span><span class="s1">addmm</span><span class="s3">(</span>
<a name="l4740"><span class="ln">4740 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4741"><span class="ln">4741 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4742"><span class="ln">4742 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4743"><span class="ln">4743 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4744"><span class="ln">4744 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4745"><span class="ln">4745 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4746"><span class="ln">4746 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4747"><span class="ln">4747 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4748"><span class="ln">4748 </span></a>    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4749"><span class="ln">4749 </span></a> 
<a name="l4750"><span class="ln">4750 </span></a>    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`. 
<a name="l4751"><span class="ln">4751 </span></a>    The matrix :attr:`input` is added to the final result. 
<a name="l4752"><span class="ln">4752 </span></a> 
<a name="l4753"><span class="ln">4753 </span></a>    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l4754"><span class="ln">4754 </span></a>    :math:`(m \times p)` tensor, then :attr:`input` must be 
<a name="l4755"><span class="ln">4755 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a :math:`(n \times p)` tensor 
<a name="l4756"><span class="ln">4756 </span></a>    and :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l4757"><span class="ln">4757 </span></a> 
<a name="l4758"><span class="ln">4758 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4759"><span class="ln">4759 </span></a>    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively. 
<a name="l4760"><span class="ln">4760 </span></a> 
<a name="l4761"><span class="ln">4761 </span></a>    .. math:: 
<a name="l4762"><span class="ln">4762 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i) 
<a name="l4763"><span class="ln">4763 </span></a> 
<a name="l4764"><span class="ln">4764 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4765"><span class="ln">4765 </span></a>    it will not be propagated. 
<a name="l4766"><span class="ln">4766 </span></a> 
<a name="l4767"><span class="ln">4767 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4768"><span class="ln">4768 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4769"><span class="ln">4769 </span></a> 
<a name="l4770"><span class="ln">4770 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. If 
<a name="l4771"><span class="ln">4771 </span></a>    :attr:`input` is sparse the result will have the same layout and if :attr:`out` 
<a name="l4772"><span class="ln">4772 </span></a>    is provided it must have the same layout as :attr:`input`. 
<a name="l4773"><span class="ln">4773 </span></a> 
<a name="l4774"><span class="ln">4774 </span></a> 
<a name="l4775"><span class="ln">4775 </span></a>    .. warning:: 
<a name="l4776"><span class="ln">4776 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l4777"><span class="ln">4777 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l4778"><span class="ln">4778 </span></a>        open a feature request. 
<a name="l4779"><span class="ln">4779 </span></a> 
<a name="l4780"><span class="ln">4780 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l4781"><span class="ln">4781 </span></a> 
<a name="l4782"><span class="ln">4782 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l4783"><span class="ln">4783 </span></a> 
<a name="l4784"><span class="ln">4784 </span></a>    Args: 
<a name="l4785"><span class="ln">4785 </span></a>        input (Tensor): matrix to be added 
<a name="l4786"><span class="ln">4786 </span></a>        mat1 (Tensor): the first matrix to be matrix multiplied 
<a name="l4787"><span class="ln">4787 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l4788"><span class="ln">4788 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l4789"><span class="ln">4789 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l4790"><span class="ln">4790 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l4791"><span class="ln">4791 </span></a> 
<a name="l4792"><span class="ln">4792 </span></a>    Keyword args: 
<a name="l4793"><span class="ln">4793 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4794"><span class="ln">4794 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l4795"><span class="ln">4795 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4796"><span class="ln">4796 </span></a> 
<a name="l4797"><span class="ln">4797 </span></a>    Example:: 
<a name="l4798"><span class="ln">4798 </span></a> 
<a name="l4799"><span class="ln">4799 </span></a>        &gt;&gt;&gt; M = torch.randn(2, 3) 
<a name="l4800"><span class="ln">4800 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l4801"><span class="ln">4801 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l4802"><span class="ln">4802 </span></a>        &gt;&gt;&gt; torch.addmm(M, mat1, mat2) 
<a name="l4803"><span class="ln">4803 </span></a>        tensor([[-4.8716,  1.4671, -1.3746], 
<a name="l4804"><span class="ln">4804 </span></a>                [ 0.7573, -3.9555, -2.8681]]) 
<a name="l4805"><span class="ln">4805 </span></a>    &quot;&quot;&quot;</span>
<a name="l4806"><span class="ln">4806 </span></a>
<a name="l4807"><span class="ln">4807 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4808"><span class="ln">4808 </span></a><span class="s2">def </span><span class="s1">addmv</span><span class="s3">(</span>
<a name="l4809"><span class="ln">4809 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4810"><span class="ln">4810 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4811"><span class="ln">4811 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4812"><span class="ln">4812 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4813"><span class="ln">4813 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4814"><span class="ln">4814 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4815"><span class="ln">4815 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4816"><span class="ln">4816 </span></a>    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4817"><span class="ln">4817 </span></a> 
<a name="l4818"><span class="ln">4818 </span></a>    Performs a matrix-vector product of the matrix :attr:`mat` and 
<a name="l4819"><span class="ln">4819 </span></a>    the vector :attr:`vec`. 
<a name="l4820"><span class="ln">4820 </span></a>    The vector :attr:`input` is added to the final result. 
<a name="l4821"><span class="ln">4821 </span></a> 
<a name="l4822"><span class="ln">4822 </span></a>    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l4823"><span class="ln">4823 </span></a>    size `m`, then :attr:`input` must be 
<a name="l4824"><span class="ln">4824 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 1-D tensor of size `n` and 
<a name="l4825"><span class="ln">4825 </span></a>    :attr:`out` will be 1-D tensor of size `n`. 
<a name="l4826"><span class="ln">4826 </span></a> 
<a name="l4827"><span class="ln">4827 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4828"><span class="ln">4828 </span></a>    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively. 
<a name="l4829"><span class="ln">4829 </span></a> 
<a name="l4830"><span class="ln">4830 </span></a>    .. math:: 
<a name="l4831"><span class="ln">4831 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec}) 
<a name="l4832"><span class="ln">4832 </span></a> 
<a name="l4833"><span class="ln">4833 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4834"><span class="ln">4834 </span></a>    it will not be propagated. 
<a name="l4835"><span class="ln">4835 </span></a> 
<a name="l4836"><span class="ln">4836 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4837"><span class="ln">4837 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4838"><span class="ln">4838 </span></a> 
<a name="l4839"><span class="ln">4839 </span></a>    Args: 
<a name="l4840"><span class="ln">4840 </span></a>        input (Tensor): vector to be added 
<a name="l4841"><span class="ln">4841 </span></a>        mat (Tensor): matrix to be matrix multiplied 
<a name="l4842"><span class="ln">4842 </span></a>        vec (Tensor): vector to be matrix multiplied 
<a name="l4843"><span class="ln">4843 </span></a> 
<a name="l4844"><span class="ln">4844 </span></a>    Keyword args: 
<a name="l4845"><span class="ln">4845 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4846"><span class="ln">4846 </span></a>        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`) 
<a name="l4847"><span class="ln">4847 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4848"><span class="ln">4848 </span></a> 
<a name="l4849"><span class="ln">4849 </span></a>    Example:: 
<a name="l4850"><span class="ln">4850 </span></a> 
<a name="l4851"><span class="ln">4851 </span></a>        &gt;&gt;&gt; M = torch.randn(2) 
<a name="l4852"><span class="ln">4852 </span></a>        &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l4853"><span class="ln">4853 </span></a>        &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l4854"><span class="ln">4854 </span></a>        &gt;&gt;&gt; torch.addmv(M, mat, vec) 
<a name="l4855"><span class="ln">4855 </span></a>        tensor([-0.3768, -5.5565]) 
<a name="l4856"><span class="ln">4856 </span></a>    &quot;&quot;&quot;</span>
<a name="l4857"><span class="ln">4857 </span></a>
<a name="l4858"><span class="ln">4858 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4859"><span class="ln">4859 </span></a><span class="s2">def </span><span class="s1">addmv</span><span class="s3">(</span>
<a name="l4860"><span class="ln">4860 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4861"><span class="ln">4861 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4862"><span class="ln">4862 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4863"><span class="ln">4863 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4864"><span class="ln">4864 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4865"><span class="ln">4865 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4866"><span class="ln">4866 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4867"><span class="ln">4867 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4868"><span class="ln">4868 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4869"><span class="ln">4869 </span></a>    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4870"><span class="ln">4870 </span></a> 
<a name="l4871"><span class="ln">4871 </span></a>    Performs a matrix-vector product of the matrix :attr:`mat` and 
<a name="l4872"><span class="ln">4872 </span></a>    the vector :attr:`vec`. 
<a name="l4873"><span class="ln">4873 </span></a>    The vector :attr:`input` is added to the final result. 
<a name="l4874"><span class="ln">4874 </span></a> 
<a name="l4875"><span class="ln">4875 </span></a>    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l4876"><span class="ln">4876 </span></a>    size `m`, then :attr:`input` must be 
<a name="l4877"><span class="ln">4877 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 1-D tensor of size `n` and 
<a name="l4878"><span class="ln">4878 </span></a>    :attr:`out` will be 1-D tensor of size `n`. 
<a name="l4879"><span class="ln">4879 </span></a> 
<a name="l4880"><span class="ln">4880 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4881"><span class="ln">4881 </span></a>    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively. 
<a name="l4882"><span class="ln">4882 </span></a> 
<a name="l4883"><span class="ln">4883 </span></a>    .. math:: 
<a name="l4884"><span class="ln">4884 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec}) 
<a name="l4885"><span class="ln">4885 </span></a> 
<a name="l4886"><span class="ln">4886 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4887"><span class="ln">4887 </span></a>    it will not be propagated. 
<a name="l4888"><span class="ln">4888 </span></a> 
<a name="l4889"><span class="ln">4889 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4890"><span class="ln">4890 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4891"><span class="ln">4891 </span></a> 
<a name="l4892"><span class="ln">4892 </span></a>    Args: 
<a name="l4893"><span class="ln">4893 </span></a>        input (Tensor): vector to be added 
<a name="l4894"><span class="ln">4894 </span></a>        mat (Tensor): matrix to be matrix multiplied 
<a name="l4895"><span class="ln">4895 </span></a>        vec (Tensor): vector to be matrix multiplied 
<a name="l4896"><span class="ln">4896 </span></a> 
<a name="l4897"><span class="ln">4897 </span></a>    Keyword args: 
<a name="l4898"><span class="ln">4898 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4899"><span class="ln">4899 </span></a>        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`) 
<a name="l4900"><span class="ln">4900 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4901"><span class="ln">4901 </span></a> 
<a name="l4902"><span class="ln">4902 </span></a>    Example:: 
<a name="l4903"><span class="ln">4903 </span></a> 
<a name="l4904"><span class="ln">4904 </span></a>        &gt;&gt;&gt; M = torch.randn(2) 
<a name="l4905"><span class="ln">4905 </span></a>        &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l4906"><span class="ln">4906 </span></a>        &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l4907"><span class="ln">4907 </span></a>        &gt;&gt;&gt; torch.addmv(M, mat, vec) 
<a name="l4908"><span class="ln">4908 </span></a>        tensor([-0.3768, -5.5565]) 
<a name="l4909"><span class="ln">4909 </span></a>    &quot;&quot;&quot;</span>
<a name="l4910"><span class="ln">4910 </span></a>
<a name="l4911"><span class="ln">4911 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4912"><span class="ln">4912 </span></a><span class="s2">def </span><span class="s1">addmv</span><span class="s3">(</span>
<a name="l4913"><span class="ln">4913 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4914"><span class="ln">4914 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4915"><span class="ln">4915 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4916"><span class="ln">4916 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l4917"><span class="ln">4917 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4918"><span class="ln">4918 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l4919"><span class="ln">4919 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l4920"><span class="ln">4920 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4921"><span class="ln">4921 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4922"><span class="ln">4922 </span></a>    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4923"><span class="ln">4923 </span></a> 
<a name="l4924"><span class="ln">4924 </span></a>    Performs a matrix-vector product of the matrix :attr:`mat` and 
<a name="l4925"><span class="ln">4925 </span></a>    the vector :attr:`vec`. 
<a name="l4926"><span class="ln">4926 </span></a>    The vector :attr:`input` is added to the final result. 
<a name="l4927"><span class="ln">4927 </span></a> 
<a name="l4928"><span class="ln">4928 </span></a>    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l4929"><span class="ln">4929 </span></a>    size `m`, then :attr:`input` must be 
<a name="l4930"><span class="ln">4930 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 1-D tensor of size `n` and 
<a name="l4931"><span class="ln">4931 </span></a>    :attr:`out` will be 1-D tensor of size `n`. 
<a name="l4932"><span class="ln">4932 </span></a> 
<a name="l4933"><span class="ln">4933 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4934"><span class="ln">4934 </span></a>    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively. 
<a name="l4935"><span class="ln">4935 </span></a> 
<a name="l4936"><span class="ln">4936 </span></a>    .. math:: 
<a name="l4937"><span class="ln">4937 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec}) 
<a name="l4938"><span class="ln">4938 </span></a> 
<a name="l4939"><span class="ln">4939 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4940"><span class="ln">4940 </span></a>    it will not be propagated. 
<a name="l4941"><span class="ln">4941 </span></a> 
<a name="l4942"><span class="ln">4942 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4943"><span class="ln">4943 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4944"><span class="ln">4944 </span></a> 
<a name="l4945"><span class="ln">4945 </span></a>    Args: 
<a name="l4946"><span class="ln">4946 </span></a>        input (Tensor): vector to be added 
<a name="l4947"><span class="ln">4947 </span></a>        mat (Tensor): matrix to be matrix multiplied 
<a name="l4948"><span class="ln">4948 </span></a>        vec (Tensor): vector to be matrix multiplied 
<a name="l4949"><span class="ln">4949 </span></a> 
<a name="l4950"><span class="ln">4950 </span></a>    Keyword args: 
<a name="l4951"><span class="ln">4951 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l4952"><span class="ln">4952 </span></a>        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`) 
<a name="l4953"><span class="ln">4953 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l4954"><span class="ln">4954 </span></a> 
<a name="l4955"><span class="ln">4955 </span></a>    Example:: 
<a name="l4956"><span class="ln">4956 </span></a> 
<a name="l4957"><span class="ln">4957 </span></a>        &gt;&gt;&gt; M = torch.randn(2) 
<a name="l4958"><span class="ln">4958 </span></a>        &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l4959"><span class="ln">4959 </span></a>        &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l4960"><span class="ln">4960 </span></a>        &gt;&gt;&gt; torch.addmv(M, mat, vec) 
<a name="l4961"><span class="ln">4961 </span></a>        tensor([-0.3768, -5.5565]) 
<a name="l4962"><span class="ln">4962 </span></a>    &quot;&quot;&quot;</span>
<a name="l4963"><span class="ln">4963 </span></a>
<a name="l4964"><span class="ln">4964 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l4965"><span class="ln">4965 </span></a><span class="s2">def </span><span class="s1">addmv</span><span class="s3">(</span>
<a name="l4966"><span class="ln">4966 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l4967"><span class="ln">4967 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4968"><span class="ln">4968 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4969"><span class="ln">4969 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l4970"><span class="ln">4970 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l4971"><span class="ln">4971 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l4972"><span class="ln">4972 </span></a>    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l4973"><span class="ln">4973 </span></a> 
<a name="l4974"><span class="ln">4974 </span></a>    Performs a matrix-vector product of the matrix :attr:`mat` and 
<a name="l4975"><span class="ln">4975 </span></a>    the vector :attr:`vec`. 
<a name="l4976"><span class="ln">4976 </span></a>    The vector :attr:`input` is added to the final result. 
<a name="l4977"><span class="ln">4977 </span></a> 
<a name="l4978"><span class="ln">4978 </span></a>    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l4979"><span class="ln">4979 </span></a>    size `m`, then :attr:`input` must be 
<a name="l4980"><span class="ln">4980 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 1-D tensor of size `n` and 
<a name="l4981"><span class="ln">4981 </span></a>    :attr:`out` will be 1-D tensor of size `n`. 
<a name="l4982"><span class="ln">4982 </span></a> 
<a name="l4983"><span class="ln">4983 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l4984"><span class="ln">4984 </span></a>    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively. 
<a name="l4985"><span class="ln">4985 </span></a> 
<a name="l4986"><span class="ln">4986 </span></a>    .. math:: 
<a name="l4987"><span class="ln">4987 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec}) 
<a name="l4988"><span class="ln">4988 </span></a> 
<a name="l4989"><span class="ln">4989 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l4990"><span class="ln">4990 </span></a>    it will not be propagated. 
<a name="l4991"><span class="ln">4991 </span></a> 
<a name="l4992"><span class="ln">4992 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l4993"><span class="ln">4993 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l4994"><span class="ln">4994 </span></a> 
<a name="l4995"><span class="ln">4995 </span></a>    Args: 
<a name="l4996"><span class="ln">4996 </span></a>        input (Tensor): vector to be added 
<a name="l4997"><span class="ln">4997 </span></a>        mat (Tensor): matrix to be matrix multiplied 
<a name="l4998"><span class="ln">4998 </span></a>        vec (Tensor): vector to be matrix multiplied 
<a name="l4999"><span class="ln">4999 </span></a> 
<a name="l5000"><span class="ln">5000 </span></a>    Keyword args: 
<a name="l5001"><span class="ln">5001 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l5002"><span class="ln">5002 </span></a>        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`) 
<a name="l5003"><span class="ln">5003 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5004"><span class="ln">5004 </span></a> 
<a name="l5005"><span class="ln">5005 </span></a>    Example:: 
<a name="l5006"><span class="ln">5006 </span></a> 
<a name="l5007"><span class="ln">5007 </span></a>        &gt;&gt;&gt; M = torch.randn(2) 
<a name="l5008"><span class="ln">5008 </span></a>        &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l5009"><span class="ln">5009 </span></a>        &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l5010"><span class="ln">5010 </span></a>        &gt;&gt;&gt; torch.addmv(M, mat, vec) 
<a name="l5011"><span class="ln">5011 </span></a>        tensor([-0.3768, -5.5565]) 
<a name="l5012"><span class="ln">5012 </span></a>    &quot;&quot;&quot;</span>
<a name="l5013"><span class="ln">5013 </span></a>
<a name="l5014"><span class="ln">5014 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5015"><span class="ln">5015 </span></a><span class="s2">def </span><span class="s1">addmv</span><span class="s3">(</span>
<a name="l5016"><span class="ln">5016 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5017"><span class="ln">5017 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5018"><span class="ln">5018 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5019"><span class="ln">5019 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5020"><span class="ln">5020 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5021"><span class="ln">5021 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5022"><span class="ln">5022 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5023"><span class="ln">5023 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5024"><span class="ln">5024 </span></a>    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l5025"><span class="ln">5025 </span></a> 
<a name="l5026"><span class="ln">5026 </span></a>    Performs a matrix-vector product of the matrix :attr:`mat` and 
<a name="l5027"><span class="ln">5027 </span></a>    the vector :attr:`vec`. 
<a name="l5028"><span class="ln">5028 </span></a>    The vector :attr:`input` is added to the final result. 
<a name="l5029"><span class="ln">5029 </span></a> 
<a name="l5030"><span class="ln">5030 </span></a>    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l5031"><span class="ln">5031 </span></a>    size `m`, then :attr:`input` must be 
<a name="l5032"><span class="ln">5032 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 1-D tensor of size `n` and 
<a name="l5033"><span class="ln">5033 </span></a>    :attr:`out` will be 1-D tensor of size `n`. 
<a name="l5034"><span class="ln">5034 </span></a> 
<a name="l5035"><span class="ln">5035 </span></a>    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between 
<a name="l5036"><span class="ln">5036 </span></a>    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively. 
<a name="l5037"><span class="ln">5037 </span></a> 
<a name="l5038"><span class="ln">5038 </span></a>    .. math:: 
<a name="l5039"><span class="ln">5039 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec}) 
<a name="l5040"><span class="ln">5040 </span></a> 
<a name="l5041"><span class="ln">5041 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l5042"><span class="ln">5042 </span></a>    it will not be propagated. 
<a name="l5043"><span class="ln">5043 </span></a> 
<a name="l5044"><span class="ln">5044 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l5045"><span class="ln">5045 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l5046"><span class="ln">5046 </span></a> 
<a name="l5047"><span class="ln">5047 </span></a>    Args: 
<a name="l5048"><span class="ln">5048 </span></a>        input (Tensor): vector to be added 
<a name="l5049"><span class="ln">5049 </span></a>        mat (Tensor): matrix to be matrix multiplied 
<a name="l5050"><span class="ln">5050 </span></a>        vec (Tensor): vector to be matrix multiplied 
<a name="l5051"><span class="ln">5051 </span></a> 
<a name="l5052"><span class="ln">5052 </span></a>    Keyword args: 
<a name="l5053"><span class="ln">5053 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l5054"><span class="ln">5054 </span></a>        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`) 
<a name="l5055"><span class="ln">5055 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5056"><span class="ln">5056 </span></a> 
<a name="l5057"><span class="ln">5057 </span></a>    Example:: 
<a name="l5058"><span class="ln">5058 </span></a> 
<a name="l5059"><span class="ln">5059 </span></a>        &gt;&gt;&gt; M = torch.randn(2) 
<a name="l5060"><span class="ln">5060 </span></a>        &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l5061"><span class="ln">5061 </span></a>        &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l5062"><span class="ln">5062 </span></a>        &gt;&gt;&gt; torch.addmv(M, mat, vec) 
<a name="l5063"><span class="ln">5063 </span></a>        tensor([-0.3768, -5.5565]) 
<a name="l5064"><span class="ln">5064 </span></a>    &quot;&quot;&quot;</span>
<a name="l5065"><span class="ln">5065 </span></a>
<a name="l5066"><span class="ln">5066 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5067"><span class="ln">5067 </span></a><span class="s2">def </span><span class="s1">addmv_</span><span class="s3">(</span>
<a name="l5068"><span class="ln">5068 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5069"><span class="ln">5069 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5070"><span class="ln">5070 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5071"><span class="ln">5071 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5072"><span class="ln">5072 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5073"><span class="ln">5073 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l5074"><span class="ln">5074 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5075"><span class="ln">5075 </span></a><span class="s2">def </span><span class="s1">addmv_</span><span class="s3">(</span>
<a name="l5076"><span class="ln">5076 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5077"><span class="ln">5077 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5078"><span class="ln">5078 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5079"><span class="ln">5079 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5080"><span class="ln">5080 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l5081"><span class="ln">5081 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l5082"><span class="ln">5082 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l5083"><span class="ln">5083 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5084"><span class="ln">5084 </span></a><span class="s2">def </span><span class="s1">addmv_</span><span class="s3">(</span>
<a name="l5085"><span class="ln">5085 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5086"><span class="ln">5086 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5087"><span class="ln">5087 </span></a>    <span class="s1">mat</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5088"><span class="ln">5088 </span></a>    <span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5089"><span class="ln">5089 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l5090"><span class="ln">5090 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5091"><span class="ln">5091 </span></a><span class="s2">def </span><span class="s1">addr</span><span class="s3">(</span>
<a name="l5092"><span class="ln">5092 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5093"><span class="ln">5093 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5094"><span class="ln">5094 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5095"><span class="ln">5095 </span></a>    <span class="s1">vec1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5096"><span class="ln">5096 </span></a>    <span class="s1">vec2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5097"><span class="ln">5097 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5098"><span class="ln">5098 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5099"><span class="ln">5099 </span></a>    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l5100"><span class="ln">5100 </span></a> 
<a name="l5101"><span class="ln">5101 </span></a>    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2` 
<a name="l5102"><span class="ln">5102 </span></a>    and adds it to the matrix :attr:`input`. 
<a name="l5103"><span class="ln">5103 </span></a> 
<a name="l5104"><span class="ln">5104 </span></a>    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the 
<a name="l5105"><span class="ln">5105 </span></a>    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix 
<a name="l5106"><span class="ln">5106 </span></a>    :attr:`input` respectively. 
<a name="l5107"><span class="ln">5107 </span></a> 
<a name="l5108"><span class="ln">5108 </span></a>    .. math:: 
<a name="l5109"><span class="ln">5109 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2}) 
<a name="l5110"><span class="ln">5110 </span></a> 
<a name="l5111"><span class="ln">5111 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l5112"><span class="ln">5112 </span></a>    it will not be propagated. 
<a name="l5113"><span class="ln">5113 </span></a> 
<a name="l5114"><span class="ln">5114 </span></a>    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector 
<a name="l5115"><span class="ln">5115 </span></a>    of size `m`, then :attr:`input` must be 
<a name="l5116"><span class="ln">5116 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a matrix of size 
<a name="l5117"><span class="ln">5117 </span></a>    :math:`(n \times m)` and :attr:`out` will be a matrix of size 
<a name="l5118"><span class="ln">5118 </span></a>    :math:`(n \times m)`. 
<a name="l5119"><span class="ln">5119 </span></a> 
<a name="l5120"><span class="ln">5120 </span></a>    Args: 
<a name="l5121"><span class="ln">5121 </span></a>        input (Tensor): matrix to be added 
<a name="l5122"><span class="ln">5122 </span></a>        vec1 (Tensor): the first vector of the outer product 
<a name="l5123"><span class="ln">5123 </span></a>        vec2 (Tensor): the second vector of the outer product 
<a name="l5124"><span class="ln">5124 </span></a> 
<a name="l5125"><span class="ln">5125 </span></a>    Keyword args: 
<a name="l5126"><span class="ln">5126 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l5127"><span class="ln">5127 </span></a>        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`) 
<a name="l5128"><span class="ln">5128 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5129"><span class="ln">5129 </span></a> 
<a name="l5130"><span class="ln">5130 </span></a>    Example:: 
<a name="l5131"><span class="ln">5131 </span></a> 
<a name="l5132"><span class="ln">5132 </span></a>        &gt;&gt;&gt; vec1 = torch.arange(1., 4.) 
<a name="l5133"><span class="ln">5133 </span></a>        &gt;&gt;&gt; vec2 = torch.arange(1., 3.) 
<a name="l5134"><span class="ln">5134 </span></a>        &gt;&gt;&gt; M = torch.zeros(3, 2) 
<a name="l5135"><span class="ln">5135 </span></a>        &gt;&gt;&gt; torch.addr(M, vec1, vec2) 
<a name="l5136"><span class="ln">5136 </span></a>        tensor([[ 1.,  2.], 
<a name="l5137"><span class="ln">5137 </span></a>                [ 2.,  4.], 
<a name="l5138"><span class="ln">5138 </span></a>                [ 3.,  6.]]) 
<a name="l5139"><span class="ln">5139 </span></a>    &quot;&quot;&quot;</span>
<a name="l5140"><span class="ln">5140 </span></a>
<a name="l5141"><span class="ln">5141 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5142"><span class="ln">5142 </span></a><span class="s2">def </span><span class="s1">addr</span><span class="s3">(</span>
<a name="l5143"><span class="ln">5143 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5144"><span class="ln">5144 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5145"><span class="ln">5145 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5146"><span class="ln">5146 </span></a>    <span class="s1">vec1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5147"><span class="ln">5147 </span></a>    <span class="s1">vec2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5148"><span class="ln">5148 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5149"><span class="ln">5149 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5150"><span class="ln">5150 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5151"><span class="ln">5151 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5152"><span class="ln">5152 </span></a>    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l5153"><span class="ln">5153 </span></a> 
<a name="l5154"><span class="ln">5154 </span></a>    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2` 
<a name="l5155"><span class="ln">5155 </span></a>    and adds it to the matrix :attr:`input`. 
<a name="l5156"><span class="ln">5156 </span></a> 
<a name="l5157"><span class="ln">5157 </span></a>    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the 
<a name="l5158"><span class="ln">5158 </span></a>    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix 
<a name="l5159"><span class="ln">5159 </span></a>    :attr:`input` respectively. 
<a name="l5160"><span class="ln">5160 </span></a> 
<a name="l5161"><span class="ln">5161 </span></a>    .. math:: 
<a name="l5162"><span class="ln">5162 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2}) 
<a name="l5163"><span class="ln">5163 </span></a> 
<a name="l5164"><span class="ln">5164 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l5165"><span class="ln">5165 </span></a>    it will not be propagated. 
<a name="l5166"><span class="ln">5166 </span></a> 
<a name="l5167"><span class="ln">5167 </span></a>    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector 
<a name="l5168"><span class="ln">5168 </span></a>    of size `m`, then :attr:`input` must be 
<a name="l5169"><span class="ln">5169 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a matrix of size 
<a name="l5170"><span class="ln">5170 </span></a>    :math:`(n \times m)` and :attr:`out` will be a matrix of size 
<a name="l5171"><span class="ln">5171 </span></a>    :math:`(n \times m)`. 
<a name="l5172"><span class="ln">5172 </span></a> 
<a name="l5173"><span class="ln">5173 </span></a>    Args: 
<a name="l5174"><span class="ln">5174 </span></a>        input (Tensor): matrix to be added 
<a name="l5175"><span class="ln">5175 </span></a>        vec1 (Tensor): the first vector of the outer product 
<a name="l5176"><span class="ln">5176 </span></a>        vec2 (Tensor): the second vector of the outer product 
<a name="l5177"><span class="ln">5177 </span></a> 
<a name="l5178"><span class="ln">5178 </span></a>    Keyword args: 
<a name="l5179"><span class="ln">5179 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l5180"><span class="ln">5180 </span></a>        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`) 
<a name="l5181"><span class="ln">5181 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5182"><span class="ln">5182 </span></a> 
<a name="l5183"><span class="ln">5183 </span></a>    Example:: 
<a name="l5184"><span class="ln">5184 </span></a> 
<a name="l5185"><span class="ln">5185 </span></a>        &gt;&gt;&gt; vec1 = torch.arange(1., 4.) 
<a name="l5186"><span class="ln">5186 </span></a>        &gt;&gt;&gt; vec2 = torch.arange(1., 3.) 
<a name="l5187"><span class="ln">5187 </span></a>        &gt;&gt;&gt; M = torch.zeros(3, 2) 
<a name="l5188"><span class="ln">5188 </span></a>        &gt;&gt;&gt; torch.addr(M, vec1, vec2) 
<a name="l5189"><span class="ln">5189 </span></a>        tensor([[ 1.,  2.], 
<a name="l5190"><span class="ln">5190 </span></a>                [ 2.,  4.], 
<a name="l5191"><span class="ln">5191 </span></a>                [ 3.,  6.]]) 
<a name="l5192"><span class="ln">5192 </span></a>    &quot;&quot;&quot;</span>
<a name="l5193"><span class="ln">5193 </span></a>
<a name="l5194"><span class="ln">5194 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5195"><span class="ln">5195 </span></a><span class="s2">def </span><span class="s1">addr</span><span class="s3">(</span>
<a name="l5196"><span class="ln">5196 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5197"><span class="ln">5197 </span></a>    <span class="s1">vec1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5198"><span class="ln">5198 </span></a>    <span class="s1">vec2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5199"><span class="ln">5199 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5200"><span class="ln">5200 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l5201"><span class="ln">5201 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l5202"><span class="ln">5202 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5203"><span class="ln">5203 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5204"><span class="ln">5204 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5205"><span class="ln">5205 </span></a>    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l5206"><span class="ln">5206 </span></a> 
<a name="l5207"><span class="ln">5207 </span></a>    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2` 
<a name="l5208"><span class="ln">5208 </span></a>    and adds it to the matrix :attr:`input`. 
<a name="l5209"><span class="ln">5209 </span></a> 
<a name="l5210"><span class="ln">5210 </span></a>    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the 
<a name="l5211"><span class="ln">5211 </span></a>    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix 
<a name="l5212"><span class="ln">5212 </span></a>    :attr:`input` respectively. 
<a name="l5213"><span class="ln">5213 </span></a> 
<a name="l5214"><span class="ln">5214 </span></a>    .. math:: 
<a name="l5215"><span class="ln">5215 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2}) 
<a name="l5216"><span class="ln">5216 </span></a> 
<a name="l5217"><span class="ln">5217 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l5218"><span class="ln">5218 </span></a>    it will not be propagated. 
<a name="l5219"><span class="ln">5219 </span></a> 
<a name="l5220"><span class="ln">5220 </span></a>    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector 
<a name="l5221"><span class="ln">5221 </span></a>    of size `m`, then :attr:`input` must be 
<a name="l5222"><span class="ln">5222 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a matrix of size 
<a name="l5223"><span class="ln">5223 </span></a>    :math:`(n \times m)` and :attr:`out` will be a matrix of size 
<a name="l5224"><span class="ln">5224 </span></a>    :math:`(n \times m)`. 
<a name="l5225"><span class="ln">5225 </span></a> 
<a name="l5226"><span class="ln">5226 </span></a>    Args: 
<a name="l5227"><span class="ln">5227 </span></a>        input (Tensor): matrix to be added 
<a name="l5228"><span class="ln">5228 </span></a>        vec1 (Tensor): the first vector of the outer product 
<a name="l5229"><span class="ln">5229 </span></a>        vec2 (Tensor): the second vector of the outer product 
<a name="l5230"><span class="ln">5230 </span></a> 
<a name="l5231"><span class="ln">5231 </span></a>    Keyword args: 
<a name="l5232"><span class="ln">5232 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l5233"><span class="ln">5233 </span></a>        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`) 
<a name="l5234"><span class="ln">5234 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5235"><span class="ln">5235 </span></a> 
<a name="l5236"><span class="ln">5236 </span></a>    Example:: 
<a name="l5237"><span class="ln">5237 </span></a> 
<a name="l5238"><span class="ln">5238 </span></a>        &gt;&gt;&gt; vec1 = torch.arange(1., 4.) 
<a name="l5239"><span class="ln">5239 </span></a>        &gt;&gt;&gt; vec2 = torch.arange(1., 3.) 
<a name="l5240"><span class="ln">5240 </span></a>        &gt;&gt;&gt; M = torch.zeros(3, 2) 
<a name="l5241"><span class="ln">5241 </span></a>        &gt;&gt;&gt; torch.addr(M, vec1, vec2) 
<a name="l5242"><span class="ln">5242 </span></a>        tensor([[ 1.,  2.], 
<a name="l5243"><span class="ln">5243 </span></a>                [ 2.,  4.], 
<a name="l5244"><span class="ln">5244 </span></a>                [ 3.,  6.]]) 
<a name="l5245"><span class="ln">5245 </span></a>    &quot;&quot;&quot;</span>
<a name="l5246"><span class="ln">5246 </span></a>
<a name="l5247"><span class="ln">5247 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5248"><span class="ln">5248 </span></a><span class="s2">def </span><span class="s1">addr</span><span class="s3">(</span>
<a name="l5249"><span class="ln">5249 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5250"><span class="ln">5250 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5251"><span class="ln">5251 </span></a>    <span class="s1">vec1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5252"><span class="ln">5252 </span></a>    <span class="s1">vec2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5253"><span class="ln">5253 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5254"><span class="ln">5254 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5255"><span class="ln">5255 </span></a>    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l5256"><span class="ln">5256 </span></a> 
<a name="l5257"><span class="ln">5257 </span></a>    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2` 
<a name="l5258"><span class="ln">5258 </span></a>    and adds it to the matrix :attr:`input`. 
<a name="l5259"><span class="ln">5259 </span></a> 
<a name="l5260"><span class="ln">5260 </span></a>    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the 
<a name="l5261"><span class="ln">5261 </span></a>    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix 
<a name="l5262"><span class="ln">5262 </span></a>    :attr:`input` respectively. 
<a name="l5263"><span class="ln">5263 </span></a> 
<a name="l5264"><span class="ln">5264 </span></a>    .. math:: 
<a name="l5265"><span class="ln">5265 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2}) 
<a name="l5266"><span class="ln">5266 </span></a> 
<a name="l5267"><span class="ln">5267 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l5268"><span class="ln">5268 </span></a>    it will not be propagated. 
<a name="l5269"><span class="ln">5269 </span></a> 
<a name="l5270"><span class="ln">5270 </span></a>    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector 
<a name="l5271"><span class="ln">5271 </span></a>    of size `m`, then :attr:`input` must be 
<a name="l5272"><span class="ln">5272 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a matrix of size 
<a name="l5273"><span class="ln">5273 </span></a>    :math:`(n \times m)` and :attr:`out` will be a matrix of size 
<a name="l5274"><span class="ln">5274 </span></a>    :math:`(n \times m)`. 
<a name="l5275"><span class="ln">5275 </span></a> 
<a name="l5276"><span class="ln">5276 </span></a>    Args: 
<a name="l5277"><span class="ln">5277 </span></a>        input (Tensor): matrix to be added 
<a name="l5278"><span class="ln">5278 </span></a>        vec1 (Tensor): the first vector of the outer product 
<a name="l5279"><span class="ln">5279 </span></a>        vec2 (Tensor): the second vector of the outer product 
<a name="l5280"><span class="ln">5280 </span></a> 
<a name="l5281"><span class="ln">5281 </span></a>    Keyword args: 
<a name="l5282"><span class="ln">5282 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l5283"><span class="ln">5283 </span></a>        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`) 
<a name="l5284"><span class="ln">5284 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5285"><span class="ln">5285 </span></a> 
<a name="l5286"><span class="ln">5286 </span></a>    Example:: 
<a name="l5287"><span class="ln">5287 </span></a> 
<a name="l5288"><span class="ln">5288 </span></a>        &gt;&gt;&gt; vec1 = torch.arange(1., 4.) 
<a name="l5289"><span class="ln">5289 </span></a>        &gt;&gt;&gt; vec2 = torch.arange(1., 3.) 
<a name="l5290"><span class="ln">5290 </span></a>        &gt;&gt;&gt; M = torch.zeros(3, 2) 
<a name="l5291"><span class="ln">5291 </span></a>        &gt;&gt;&gt; torch.addr(M, vec1, vec2) 
<a name="l5292"><span class="ln">5292 </span></a>        tensor([[ 1.,  2.], 
<a name="l5293"><span class="ln">5293 </span></a>                [ 2.,  4.], 
<a name="l5294"><span class="ln">5294 </span></a>                [ 3.,  6.]]) 
<a name="l5295"><span class="ln">5295 </span></a>    &quot;&quot;&quot;</span>
<a name="l5296"><span class="ln">5296 </span></a>
<a name="l5297"><span class="ln">5297 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5298"><span class="ln">5298 </span></a><span class="s2">def </span><span class="s1">addr</span><span class="s3">(</span>
<a name="l5299"><span class="ln">5299 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l5300"><span class="ln">5300 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5301"><span class="ln">5301 </span></a>    <span class="s1">vec1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5302"><span class="ln">5302 </span></a>    <span class="s1">vec2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5303"><span class="ln">5303 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5304"><span class="ln">5304 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5305"><span class="ln">5305 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5306"><span class="ln">5306 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5307"><span class="ln">5307 </span></a>    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l5308"><span class="ln">5308 </span></a> 
<a name="l5309"><span class="ln">5309 </span></a>    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2` 
<a name="l5310"><span class="ln">5310 </span></a>    and adds it to the matrix :attr:`input`. 
<a name="l5311"><span class="ln">5311 </span></a> 
<a name="l5312"><span class="ln">5312 </span></a>    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the 
<a name="l5313"><span class="ln">5313 </span></a>    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix 
<a name="l5314"><span class="ln">5314 </span></a>    :attr:`input` respectively. 
<a name="l5315"><span class="ln">5315 </span></a> 
<a name="l5316"><span class="ln">5316 </span></a>    .. math:: 
<a name="l5317"><span class="ln">5317 </span></a>        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2}) 
<a name="l5318"><span class="ln">5318 </span></a> 
<a name="l5319"><span class="ln">5319 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l5320"><span class="ln">5320 </span></a>    it will not be propagated. 
<a name="l5321"><span class="ln">5321 </span></a> 
<a name="l5322"><span class="ln">5322 </span></a>    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector 
<a name="l5323"><span class="ln">5323 </span></a>    of size `m`, then :attr:`input` must be 
<a name="l5324"><span class="ln">5324 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a matrix of size 
<a name="l5325"><span class="ln">5325 </span></a>    :math:`(n \times m)` and :attr:`out` will be a matrix of size 
<a name="l5326"><span class="ln">5326 </span></a>    :math:`(n \times m)`. 
<a name="l5327"><span class="ln">5327 </span></a> 
<a name="l5328"><span class="ln">5328 </span></a>    Args: 
<a name="l5329"><span class="ln">5329 </span></a>        input (Tensor): matrix to be added 
<a name="l5330"><span class="ln">5330 </span></a>        vec1 (Tensor): the first vector of the outer product 
<a name="l5331"><span class="ln">5331 </span></a>        vec2 (Tensor): the second vector of the outer product 
<a name="l5332"><span class="ln">5332 </span></a> 
<a name="l5333"><span class="ln">5333 </span></a>    Keyword args: 
<a name="l5334"><span class="ln">5334 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l5335"><span class="ln">5335 </span></a>        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`) 
<a name="l5336"><span class="ln">5336 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5337"><span class="ln">5337 </span></a> 
<a name="l5338"><span class="ln">5338 </span></a>    Example:: 
<a name="l5339"><span class="ln">5339 </span></a> 
<a name="l5340"><span class="ln">5340 </span></a>        &gt;&gt;&gt; vec1 = torch.arange(1., 4.) 
<a name="l5341"><span class="ln">5341 </span></a>        &gt;&gt;&gt; vec2 = torch.arange(1., 3.) 
<a name="l5342"><span class="ln">5342 </span></a>        &gt;&gt;&gt; M = torch.zeros(3, 2) 
<a name="l5343"><span class="ln">5343 </span></a>        &gt;&gt;&gt; torch.addr(M, vec1, vec2) 
<a name="l5344"><span class="ln">5344 </span></a>        tensor([[ 1.,  2.], 
<a name="l5345"><span class="ln">5345 </span></a>                [ 2.,  4.], 
<a name="l5346"><span class="ln">5346 </span></a>                [ 3.,  6.]]) 
<a name="l5347"><span class="ln">5347 </span></a>    &quot;&quot;&quot;</span>
<a name="l5348"><span class="ln">5348 </span></a>
<a name="l5349"><span class="ln">5349 </span></a><span class="s2">def </span><span class="s1">adjoint</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5350"><span class="ln">5350 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5351"><span class="ln">5351 </span></a>    adjoint(input: Tensor) -&gt; Tensor 
<a name="l5352"><span class="ln">5352 </span></a>    Returns a view of the tensor conjugated and with the last two dimensions transposed. 
<a name="l5353"><span class="ln">5353 </span></a> 
<a name="l5354"><span class="ln">5354 </span></a>    ``x.adjoint()`` is equivalent to ``x.transpose(-2, -1).conj()`` for complex tensors and 
<a name="l5355"><span class="ln">5355 </span></a>    to ``x.transpose(-2, -1)`` for real tensors. 
<a name="l5356"><span class="ln">5356 </span></a> 
<a name="l5357"><span class="ln">5357 </span></a>    Args: 
<a name="l5358"><span class="ln">5358 </span></a>        {input} 
<a name="l5359"><span class="ln">5359 </span></a> 
<a name="l5360"><span class="ln">5360 </span></a>    Example:: 
<a name="l5361"><span class="ln">5361 </span></a> 
<a name="l5362"><span class="ln">5362 </span></a>        &gt;&gt;&gt; x = torch.arange(4, dtype=torch.float) 
<a name="l5363"><span class="ln">5363 </span></a>        &gt;&gt;&gt; A = torch.complex(x, x).reshape(2, 2) 
<a name="l5364"><span class="ln">5364 </span></a>        &gt;&gt;&gt; A 
<a name="l5365"><span class="ln">5365 </span></a>        tensor([[0.+0.j, 1.+1.j], 
<a name="l5366"><span class="ln">5366 </span></a>                [2.+2.j, 3.+3.j]]) 
<a name="l5367"><span class="ln">5367 </span></a>        &gt;&gt;&gt; A.adjoint() 
<a name="l5368"><span class="ln">5368 </span></a>        tensor([[0.-0.j, 2.-2.j], 
<a name="l5369"><span class="ln">5369 </span></a>                [1.-1.j, 3.-3.j]]) 
<a name="l5370"><span class="ln">5370 </span></a>        &gt;&gt;&gt; (A.adjoint() == A.mH).all() 
<a name="l5371"><span class="ln">5371 </span></a>        tensor(True) 
<a name="l5372"><span class="ln">5372 </span></a>    &quot;&quot;&quot;</span>
<a name="l5373"><span class="ln">5373 </span></a>
<a name="l5374"><span class="ln">5374 </span></a><span class="s2">def </span><span class="s1">affine_grid_generator</span><span class="s3">(</span>
<a name="l5375"><span class="ln">5375 </span></a>    <span class="s1">theta</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5376"><span class="ln">5376 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l5377"><span class="ln">5377 </span></a>    <span class="s1">align_corners</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l5378"><span class="ln">5378 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l5379"><span class="ln">5379 </span></a><span class="s2">def </span><span class="s1">alias_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5380"><span class="ln">5380 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5381"><span class="ln">5381 </span></a>    Performs the same operation as :func:`torch.alias`, but all output tensors 
<a name="l5382"><span class="ln">5382 </span></a>    are freshly created instead of aliasing the input. 
<a name="l5383"><span class="ln">5383 </span></a>    &quot;&quot;&quot;</span>
<a name="l5384"><span class="ln">5384 </span></a>
<a name="l5385"><span class="ln">5385 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5386"><span class="ln">5386 </span></a><span class="s2">def </span><span class="s1">all</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5387"><span class="ln">5387 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5388"><span class="ln">5388 </span></a>    all(input: Tensor, *, out=None) -&gt; Tensor 
<a name="l5389"><span class="ln">5389 </span></a> 
<a name="l5390"><span class="ln">5390 </span></a>    Tests if all elements in :attr:`input` evaluate to `True`. 
<a name="l5391"><span class="ln">5391 </span></a> 
<a name="l5392"><span class="ln">5392 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l5393"><span class="ln">5393 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l5394"><span class="ln">5394 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l5395"><span class="ln">5395 </span></a> 
<a name="l5396"><span class="ln">5396 </span></a>    Args: 
<a name="l5397"><span class="ln">5397 </span></a>        input (Tensor): the input tensor. 
<a name="l5398"><span class="ln">5398 </span></a> 
<a name="l5399"><span class="ln">5399 </span></a>    Keyword args: 
<a name="l5400"><span class="ln">5400 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5401"><span class="ln">5401 </span></a> 
<a name="l5402"><span class="ln">5402 </span></a>    Example:: 
<a name="l5403"><span class="ln">5403 </span></a> 
<a name="l5404"><span class="ln">5404 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l5405"><span class="ln">5405 </span></a>        &gt;&gt;&gt; a 
<a name="l5406"><span class="ln">5406 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l5407"><span class="ln">5407 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5408"><span class="ln">5408 </span></a>        tensor(False, dtype=torch.bool) 
<a name="l5409"><span class="ln">5409 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l5410"><span class="ln">5410 </span></a>        &gt;&gt;&gt; a 
<a name="l5411"><span class="ln">5411 </span></a>        tensor([0, 1, 2]) 
<a name="l5412"><span class="ln">5412 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5413"><span class="ln">5413 </span></a>        tensor(False) 
<a name="l5414"><span class="ln">5414 </span></a> 
<a name="l5415"><span class="ln">5415 </span></a>    .. function:: all(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l5416"><span class="ln">5416 </span></a>       :noindex: 
<a name="l5417"><span class="ln">5417 </span></a> 
<a name="l5418"><span class="ln">5418 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l5419"><span class="ln">5419 </span></a>    returns `True` if all elements in the row evaluate to `True` and `False` otherwise. 
<a name="l5420"><span class="ln">5420 </span></a> 
<a name="l5421"><span class="ln">5421 </span></a> 
<a name="l5422"><span class="ln">5422 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l5423"><span class="ln">5423 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l5424"><span class="ln">5424 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l5425"><span class="ln">5425 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l5426"><span class="ln">5426 </span></a> 
<a name="l5427"><span class="ln">5427 </span></a> 
<a name="l5428"><span class="ln">5428 </span></a>    Args: 
<a name="l5429"><span class="ln">5429 </span></a>        input (Tensor): the input tensor. 
<a name="l5430"><span class="ln">5430 </span></a> 
<a name="l5431"><span class="ln">5431 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l5432"><span class="ln">5432 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l5433"><span class="ln">5433 </span></a> 
<a name="l5434"><span class="ln">5434 </span></a> 
<a name="l5435"><span class="ln">5435 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l5436"><span class="ln">5436 </span></a> 
<a name="l5437"><span class="ln">5437 </span></a> 
<a name="l5438"><span class="ln">5438 </span></a>    Keyword args: 
<a name="l5439"><span class="ln">5439 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5440"><span class="ln">5440 </span></a> 
<a name="l5441"><span class="ln">5441 </span></a>    Example:: 
<a name="l5442"><span class="ln">5442 </span></a> 
<a name="l5443"><span class="ln">5443 </span></a>        &gt;&gt;&gt; a = torch.rand(4, 2).bool() 
<a name="l5444"><span class="ln">5444 </span></a>        &gt;&gt;&gt; a 
<a name="l5445"><span class="ln">5445 </span></a>        tensor([[True, True], 
<a name="l5446"><span class="ln">5446 </span></a>                [True, False], 
<a name="l5447"><span class="ln">5447 </span></a>                [True, True], 
<a name="l5448"><span class="ln">5448 </span></a>                [True, True]], dtype=torch.bool) 
<a name="l5449"><span class="ln">5449 </span></a>        &gt;&gt;&gt; torch.all(a, dim=1) 
<a name="l5450"><span class="ln">5450 </span></a>        tensor([ True, False,  True,  True], dtype=torch.bool) 
<a name="l5451"><span class="ln">5451 </span></a>        &gt;&gt;&gt; torch.all(a, dim=0) 
<a name="l5452"><span class="ln">5452 </span></a>        tensor([ True, False], dtype=torch.bool) 
<a name="l5453"><span class="ln">5453 </span></a>    &quot;&quot;&quot;</span>
<a name="l5454"><span class="ln">5454 </span></a>
<a name="l5455"><span class="ln">5455 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5456"><span class="ln">5456 </span></a><span class="s2">def </span><span class="s1">all</span><span class="s3">(</span>
<a name="l5457"><span class="ln">5457 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5458"><span class="ln">5458 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5459"><span class="ln">5459 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5460"><span class="ln">5460 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5461"><span class="ln">5461 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5462"><span class="ln">5462 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5463"><span class="ln">5463 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5464"><span class="ln">5464 </span></a>    all(input: Tensor, *, out=None) -&gt; Tensor 
<a name="l5465"><span class="ln">5465 </span></a> 
<a name="l5466"><span class="ln">5466 </span></a>    Tests if all elements in :attr:`input` evaluate to `True`. 
<a name="l5467"><span class="ln">5467 </span></a> 
<a name="l5468"><span class="ln">5468 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l5469"><span class="ln">5469 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l5470"><span class="ln">5470 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l5471"><span class="ln">5471 </span></a> 
<a name="l5472"><span class="ln">5472 </span></a>    Args: 
<a name="l5473"><span class="ln">5473 </span></a>        input (Tensor): the input tensor. 
<a name="l5474"><span class="ln">5474 </span></a> 
<a name="l5475"><span class="ln">5475 </span></a>    Keyword args: 
<a name="l5476"><span class="ln">5476 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5477"><span class="ln">5477 </span></a> 
<a name="l5478"><span class="ln">5478 </span></a>    Example:: 
<a name="l5479"><span class="ln">5479 </span></a> 
<a name="l5480"><span class="ln">5480 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l5481"><span class="ln">5481 </span></a>        &gt;&gt;&gt; a 
<a name="l5482"><span class="ln">5482 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l5483"><span class="ln">5483 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5484"><span class="ln">5484 </span></a>        tensor(False, dtype=torch.bool) 
<a name="l5485"><span class="ln">5485 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l5486"><span class="ln">5486 </span></a>        &gt;&gt;&gt; a 
<a name="l5487"><span class="ln">5487 </span></a>        tensor([0, 1, 2]) 
<a name="l5488"><span class="ln">5488 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5489"><span class="ln">5489 </span></a>        tensor(False) 
<a name="l5490"><span class="ln">5490 </span></a> 
<a name="l5491"><span class="ln">5491 </span></a>    .. function:: all(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l5492"><span class="ln">5492 </span></a>       :noindex: 
<a name="l5493"><span class="ln">5493 </span></a> 
<a name="l5494"><span class="ln">5494 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l5495"><span class="ln">5495 </span></a>    returns `True` if all elements in the row evaluate to `True` and `False` otherwise. 
<a name="l5496"><span class="ln">5496 </span></a> 
<a name="l5497"><span class="ln">5497 </span></a> 
<a name="l5498"><span class="ln">5498 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l5499"><span class="ln">5499 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l5500"><span class="ln">5500 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l5501"><span class="ln">5501 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l5502"><span class="ln">5502 </span></a> 
<a name="l5503"><span class="ln">5503 </span></a> 
<a name="l5504"><span class="ln">5504 </span></a>    Args: 
<a name="l5505"><span class="ln">5505 </span></a>        input (Tensor): the input tensor. 
<a name="l5506"><span class="ln">5506 </span></a> 
<a name="l5507"><span class="ln">5507 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l5508"><span class="ln">5508 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l5509"><span class="ln">5509 </span></a> 
<a name="l5510"><span class="ln">5510 </span></a> 
<a name="l5511"><span class="ln">5511 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l5512"><span class="ln">5512 </span></a> 
<a name="l5513"><span class="ln">5513 </span></a> 
<a name="l5514"><span class="ln">5514 </span></a>    Keyword args: 
<a name="l5515"><span class="ln">5515 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5516"><span class="ln">5516 </span></a> 
<a name="l5517"><span class="ln">5517 </span></a>    Example:: 
<a name="l5518"><span class="ln">5518 </span></a> 
<a name="l5519"><span class="ln">5519 </span></a>        &gt;&gt;&gt; a = torch.rand(4, 2).bool() 
<a name="l5520"><span class="ln">5520 </span></a>        &gt;&gt;&gt; a 
<a name="l5521"><span class="ln">5521 </span></a>        tensor([[True, True], 
<a name="l5522"><span class="ln">5522 </span></a>                [True, False], 
<a name="l5523"><span class="ln">5523 </span></a>                [True, True], 
<a name="l5524"><span class="ln">5524 </span></a>                [True, True]], dtype=torch.bool) 
<a name="l5525"><span class="ln">5525 </span></a>        &gt;&gt;&gt; torch.all(a, dim=1) 
<a name="l5526"><span class="ln">5526 </span></a>        tensor([ True, False,  True,  True], dtype=torch.bool) 
<a name="l5527"><span class="ln">5527 </span></a>        &gt;&gt;&gt; torch.all(a, dim=0) 
<a name="l5528"><span class="ln">5528 </span></a>        tensor([ True, False], dtype=torch.bool) 
<a name="l5529"><span class="ln">5529 </span></a>    &quot;&quot;&quot;</span>
<a name="l5530"><span class="ln">5530 </span></a>
<a name="l5531"><span class="ln">5531 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5532"><span class="ln">5532 </span></a><span class="s2">def </span><span class="s1">all</span><span class="s3">(</span>
<a name="l5533"><span class="ln">5533 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5534"><span class="ln">5534 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l5535"><span class="ln">5535 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5536"><span class="ln">5536 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5537"><span class="ln">5537 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5538"><span class="ln">5538 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5539"><span class="ln">5539 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5540"><span class="ln">5540 </span></a>    all(input: Tensor, *, out=None) -&gt; Tensor 
<a name="l5541"><span class="ln">5541 </span></a> 
<a name="l5542"><span class="ln">5542 </span></a>    Tests if all elements in :attr:`input` evaluate to `True`. 
<a name="l5543"><span class="ln">5543 </span></a> 
<a name="l5544"><span class="ln">5544 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l5545"><span class="ln">5545 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l5546"><span class="ln">5546 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l5547"><span class="ln">5547 </span></a> 
<a name="l5548"><span class="ln">5548 </span></a>    Args: 
<a name="l5549"><span class="ln">5549 </span></a>        input (Tensor): the input tensor. 
<a name="l5550"><span class="ln">5550 </span></a> 
<a name="l5551"><span class="ln">5551 </span></a>    Keyword args: 
<a name="l5552"><span class="ln">5552 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5553"><span class="ln">5553 </span></a> 
<a name="l5554"><span class="ln">5554 </span></a>    Example:: 
<a name="l5555"><span class="ln">5555 </span></a> 
<a name="l5556"><span class="ln">5556 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l5557"><span class="ln">5557 </span></a>        &gt;&gt;&gt; a 
<a name="l5558"><span class="ln">5558 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l5559"><span class="ln">5559 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5560"><span class="ln">5560 </span></a>        tensor(False, dtype=torch.bool) 
<a name="l5561"><span class="ln">5561 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l5562"><span class="ln">5562 </span></a>        &gt;&gt;&gt; a 
<a name="l5563"><span class="ln">5563 </span></a>        tensor([0, 1, 2]) 
<a name="l5564"><span class="ln">5564 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5565"><span class="ln">5565 </span></a>        tensor(False) 
<a name="l5566"><span class="ln">5566 </span></a> 
<a name="l5567"><span class="ln">5567 </span></a>    .. function:: all(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l5568"><span class="ln">5568 </span></a>       :noindex: 
<a name="l5569"><span class="ln">5569 </span></a> 
<a name="l5570"><span class="ln">5570 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l5571"><span class="ln">5571 </span></a>    returns `True` if all elements in the row evaluate to `True` and `False` otherwise. 
<a name="l5572"><span class="ln">5572 </span></a> 
<a name="l5573"><span class="ln">5573 </span></a> 
<a name="l5574"><span class="ln">5574 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l5575"><span class="ln">5575 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l5576"><span class="ln">5576 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l5577"><span class="ln">5577 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l5578"><span class="ln">5578 </span></a> 
<a name="l5579"><span class="ln">5579 </span></a> 
<a name="l5580"><span class="ln">5580 </span></a>    Args: 
<a name="l5581"><span class="ln">5581 </span></a>        input (Tensor): the input tensor. 
<a name="l5582"><span class="ln">5582 </span></a> 
<a name="l5583"><span class="ln">5583 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l5584"><span class="ln">5584 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l5585"><span class="ln">5585 </span></a> 
<a name="l5586"><span class="ln">5586 </span></a> 
<a name="l5587"><span class="ln">5587 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l5588"><span class="ln">5588 </span></a> 
<a name="l5589"><span class="ln">5589 </span></a> 
<a name="l5590"><span class="ln">5590 </span></a>    Keyword args: 
<a name="l5591"><span class="ln">5591 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5592"><span class="ln">5592 </span></a> 
<a name="l5593"><span class="ln">5593 </span></a>    Example:: 
<a name="l5594"><span class="ln">5594 </span></a> 
<a name="l5595"><span class="ln">5595 </span></a>        &gt;&gt;&gt; a = torch.rand(4, 2).bool() 
<a name="l5596"><span class="ln">5596 </span></a>        &gt;&gt;&gt; a 
<a name="l5597"><span class="ln">5597 </span></a>        tensor([[True, True], 
<a name="l5598"><span class="ln">5598 </span></a>                [True, False], 
<a name="l5599"><span class="ln">5599 </span></a>                [True, True], 
<a name="l5600"><span class="ln">5600 </span></a>                [True, True]], dtype=torch.bool) 
<a name="l5601"><span class="ln">5601 </span></a>        &gt;&gt;&gt; torch.all(a, dim=1) 
<a name="l5602"><span class="ln">5602 </span></a>        tensor([ True, False,  True,  True], dtype=torch.bool) 
<a name="l5603"><span class="ln">5603 </span></a>        &gt;&gt;&gt; torch.all(a, dim=0) 
<a name="l5604"><span class="ln">5604 </span></a>        tensor([ True, False], dtype=torch.bool) 
<a name="l5605"><span class="ln">5605 </span></a>    &quot;&quot;&quot;</span>
<a name="l5606"><span class="ln">5606 </span></a>
<a name="l5607"><span class="ln">5607 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5608"><span class="ln">5608 </span></a><span class="s2">def </span><span class="s1">all</span><span class="s3">(</span>
<a name="l5609"><span class="ln">5609 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5610"><span class="ln">5610 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l5611"><span class="ln">5611 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5612"><span class="ln">5612 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5613"><span class="ln">5613 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5614"><span class="ln">5614 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5615"><span class="ln">5615 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5616"><span class="ln">5616 </span></a>    all(input: Tensor, *, out=None) -&gt; Tensor 
<a name="l5617"><span class="ln">5617 </span></a> 
<a name="l5618"><span class="ln">5618 </span></a>    Tests if all elements in :attr:`input` evaluate to `True`. 
<a name="l5619"><span class="ln">5619 </span></a> 
<a name="l5620"><span class="ln">5620 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l5621"><span class="ln">5621 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l5622"><span class="ln">5622 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l5623"><span class="ln">5623 </span></a> 
<a name="l5624"><span class="ln">5624 </span></a>    Args: 
<a name="l5625"><span class="ln">5625 </span></a>        input (Tensor): the input tensor. 
<a name="l5626"><span class="ln">5626 </span></a> 
<a name="l5627"><span class="ln">5627 </span></a>    Keyword args: 
<a name="l5628"><span class="ln">5628 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5629"><span class="ln">5629 </span></a> 
<a name="l5630"><span class="ln">5630 </span></a>    Example:: 
<a name="l5631"><span class="ln">5631 </span></a> 
<a name="l5632"><span class="ln">5632 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l5633"><span class="ln">5633 </span></a>        &gt;&gt;&gt; a 
<a name="l5634"><span class="ln">5634 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l5635"><span class="ln">5635 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5636"><span class="ln">5636 </span></a>        tensor(False, dtype=torch.bool) 
<a name="l5637"><span class="ln">5637 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l5638"><span class="ln">5638 </span></a>        &gt;&gt;&gt; a 
<a name="l5639"><span class="ln">5639 </span></a>        tensor([0, 1, 2]) 
<a name="l5640"><span class="ln">5640 </span></a>        &gt;&gt;&gt; torch.all(a) 
<a name="l5641"><span class="ln">5641 </span></a>        tensor(False) 
<a name="l5642"><span class="ln">5642 </span></a> 
<a name="l5643"><span class="ln">5643 </span></a>    .. function:: all(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l5644"><span class="ln">5644 </span></a>       :noindex: 
<a name="l5645"><span class="ln">5645 </span></a> 
<a name="l5646"><span class="ln">5646 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l5647"><span class="ln">5647 </span></a>    returns `True` if all elements in the row evaluate to `True` and `False` otherwise. 
<a name="l5648"><span class="ln">5648 </span></a> 
<a name="l5649"><span class="ln">5649 </span></a> 
<a name="l5650"><span class="ln">5650 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l5651"><span class="ln">5651 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l5652"><span class="ln">5652 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l5653"><span class="ln">5653 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l5654"><span class="ln">5654 </span></a> 
<a name="l5655"><span class="ln">5655 </span></a> 
<a name="l5656"><span class="ln">5656 </span></a>    Args: 
<a name="l5657"><span class="ln">5657 </span></a>        input (Tensor): the input tensor. 
<a name="l5658"><span class="ln">5658 </span></a> 
<a name="l5659"><span class="ln">5659 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l5660"><span class="ln">5660 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l5661"><span class="ln">5661 </span></a> 
<a name="l5662"><span class="ln">5662 </span></a> 
<a name="l5663"><span class="ln">5663 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l5664"><span class="ln">5664 </span></a> 
<a name="l5665"><span class="ln">5665 </span></a> 
<a name="l5666"><span class="ln">5666 </span></a>    Keyword args: 
<a name="l5667"><span class="ln">5667 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5668"><span class="ln">5668 </span></a> 
<a name="l5669"><span class="ln">5669 </span></a>    Example:: 
<a name="l5670"><span class="ln">5670 </span></a> 
<a name="l5671"><span class="ln">5671 </span></a>        &gt;&gt;&gt; a = torch.rand(4, 2).bool() 
<a name="l5672"><span class="ln">5672 </span></a>        &gt;&gt;&gt; a 
<a name="l5673"><span class="ln">5673 </span></a>        tensor([[True, True], 
<a name="l5674"><span class="ln">5674 </span></a>                [True, False], 
<a name="l5675"><span class="ln">5675 </span></a>                [True, True], 
<a name="l5676"><span class="ln">5676 </span></a>                [True, True]], dtype=torch.bool) 
<a name="l5677"><span class="ln">5677 </span></a>        &gt;&gt;&gt; torch.all(a, dim=1) 
<a name="l5678"><span class="ln">5678 </span></a>        tensor([ True, False,  True,  True], dtype=torch.bool) 
<a name="l5679"><span class="ln">5679 </span></a>        &gt;&gt;&gt; torch.all(a, dim=0) 
<a name="l5680"><span class="ln">5680 </span></a>        tensor([ True, False], dtype=torch.bool) 
<a name="l5681"><span class="ln">5681 </span></a>    &quot;&quot;&quot;</span>
<a name="l5682"><span class="ln">5682 </span></a>
<a name="l5683"><span class="ln">5683 </span></a><span class="s2">def </span><span class="s1">allclose</span><span class="s3">(</span>
<a name="l5684"><span class="ln">5684 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5685"><span class="ln">5685 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5686"><span class="ln">5686 </span></a>    <span class="s1">rtol</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-05</span><span class="s3">,</span>
<a name="l5687"><span class="ln">5687 </span></a>    <span class="s1">atol</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-08</span><span class="s3">,</span>
<a name="l5688"><span class="ln">5688 </span></a>    <span class="s1">equal_nan</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5689"><span class="ln">5689 </span></a><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l5690"><span class="ln">5690 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5691"><span class="ln">5691 </span></a>    allclose(input: Tensor, other: Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -&gt; bool 
<a name="l5692"><span class="ln">5692 </span></a> 
<a name="l5693"><span class="ln">5693 </span></a>    This function checks if :attr:`input` and :attr:`other` satisfy the condition: 
<a name="l5694"><span class="ln">5694 </span></a> 
<a name="l5695"><span class="ln">5695 </span></a>    .. math:: 
<a name="l5696"><span class="ln">5696 </span></a>        \lvert \text{input}_i - \text{other}_i \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other}_i \rvert 
<a name="l5697"><span class="ln">5697 </span></a> 
<a name="l5698"><span class="ln">5698 </span></a>    elementwise, for all elements of :attr:`input` and :attr:`other`. The behaviour of this function is analogous to 
<a name="l5699"><span class="ln">5699 </span></a>    `numpy.allclose &lt;https://numpy.org/doc/stable/reference/generated/numpy.allclose.html&gt;`_ 
<a name="l5700"><span class="ln">5700 </span></a> 
<a name="l5701"><span class="ln">5701 </span></a>    Args: 
<a name="l5702"><span class="ln">5702 </span></a>        input (Tensor): first tensor to compare 
<a name="l5703"><span class="ln">5703 </span></a>        other (Tensor): second tensor to compare 
<a name="l5704"><span class="ln">5704 </span></a>        atol (float, optional): absolute tolerance. Default: 1e-08 
<a name="l5705"><span class="ln">5705 </span></a>        rtol (float, optional): relative tolerance. Default: 1e-05 
<a name="l5706"><span class="ln">5706 </span></a>        equal_nan (bool, optional): if ``True``, then two ``NaN`` s will be considered equal. Default: ``False`` 
<a name="l5707"><span class="ln">5707 </span></a> 
<a name="l5708"><span class="ln">5708 </span></a>    Example:: 
<a name="l5709"><span class="ln">5709 </span></a> 
<a name="l5710"><span class="ln">5710 </span></a>        &gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08])) 
<a name="l5711"><span class="ln">5711 </span></a>        False 
<a name="l5712"><span class="ln">5712 </span></a>        &gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09])) 
<a name="l5713"><span class="ln">5713 </span></a>        True 
<a name="l5714"><span class="ln">5714 </span></a>        &gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')])) 
<a name="l5715"><span class="ln">5715 </span></a>        False 
<a name="l5716"><span class="ln">5716 </span></a>        &gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True) 
<a name="l5717"><span class="ln">5717 </span></a>        True 
<a name="l5718"><span class="ln">5718 </span></a>    &quot;&quot;&quot;</span>
<a name="l5719"><span class="ln">5719 </span></a>
<a name="l5720"><span class="ln">5720 </span></a><span class="s2">def </span><span class="s1">alpha_dropout</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l5721"><span class="ln">5721 </span></a><span class="s2">def </span><span class="s1">alpha_dropout_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l5722"><span class="ln">5722 </span></a><span class="s2">def </span><span class="s1">amax</span><span class="s3">(</span>
<a name="l5723"><span class="ln">5723 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5724"><span class="ln">5724 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l5725"><span class="ln">5725 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5726"><span class="ln">5726 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5727"><span class="ln">5727 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5728"><span class="ln">5728 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5729"><span class="ln">5729 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5730"><span class="ln">5730 </span></a>    amax(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l5731"><span class="ln">5731 </span></a> 
<a name="l5732"><span class="ln">5732 </span></a>    Returns the maximum value of each slice of the :attr:`input` tensor in the given 
<a name="l5733"><span class="ln">5733 </span></a>    dimension(s) :attr:`dim`. 
<a name="l5734"><span class="ln">5734 </span></a> 
<a name="l5735"><span class="ln">5735 </span></a>    .. note:: 
<a name="l5736"><span class="ln">5736 </span></a>        The difference between ``max``/``min`` and ``amax``/``amin`` is: 
<a name="l5737"><span class="ln">5737 </span></a>            - ``amax``/``amin`` supports reducing on multiple dimensions, 
<a name="l5738"><span class="ln">5738 </span></a>            - ``amax``/``amin`` does not return indices. 
<a name="l5739"><span class="ln">5739 </span></a> 
<a name="l5740"><span class="ln">5740 </span></a>        Both ``max``/``min`` and ``amax``/``amin`` evenly distribute gradients between equal values 
<a name="l5741"><span class="ln">5741 </span></a>        when there are multiple input elements with the same minimum or maximum value. 
<a name="l5742"><span class="ln">5742 </span></a> 
<a name="l5743"><span class="ln">5743 </span></a> 
<a name="l5744"><span class="ln">5744 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l5745"><span class="ln">5745 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l5746"><span class="ln">5746 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l5747"><span class="ln">5747 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l5748"><span class="ln">5748 </span></a> 
<a name="l5749"><span class="ln">5749 </span></a> 
<a name="l5750"><span class="ln">5750 </span></a>    Args: 
<a name="l5751"><span class="ln">5751 </span></a>        input (Tensor): the input tensor. 
<a name="l5752"><span class="ln">5752 </span></a> 
<a name="l5753"><span class="ln">5753 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l5754"><span class="ln">5754 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l5755"><span class="ln">5755 </span></a> 
<a name="l5756"><span class="ln">5756 </span></a> 
<a name="l5757"><span class="ln">5757 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l5758"><span class="ln">5758 </span></a> 
<a name="l5759"><span class="ln">5759 </span></a> 
<a name="l5760"><span class="ln">5760 </span></a>    Keyword args: 
<a name="l5761"><span class="ln">5761 </span></a>      out (Tensor, optional): the output tensor. 
<a name="l5762"><span class="ln">5762 </span></a> 
<a name="l5763"><span class="ln">5763 </span></a>    Example:: 
<a name="l5764"><span class="ln">5764 </span></a> 
<a name="l5765"><span class="ln">5765 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l5766"><span class="ln">5766 </span></a>        &gt;&gt;&gt; a 
<a name="l5767"><span class="ln">5767 </span></a>        tensor([[ 0.8177,  1.4878, -0.2491,  0.9130], 
<a name="l5768"><span class="ln">5768 </span></a>                [-0.7158,  1.1775,  2.0992,  0.4817], 
<a name="l5769"><span class="ln">5769 </span></a>                [-0.0053,  0.0164, -1.3738, -0.0507], 
<a name="l5770"><span class="ln">5770 </span></a>                [ 1.9700,  1.1106, -1.0318, -1.0816]]) 
<a name="l5771"><span class="ln">5771 </span></a>        &gt;&gt;&gt; torch.amax(a, 1) 
<a name="l5772"><span class="ln">5772 </span></a>        tensor([1.4878, 2.0992, 0.0164, 1.9700]) 
<a name="l5773"><span class="ln">5773 </span></a>    &quot;&quot;&quot;</span>
<a name="l5774"><span class="ln">5774 </span></a>
<a name="l5775"><span class="ln">5775 </span></a><span class="s2">def </span><span class="s1">amin</span><span class="s3">(</span>
<a name="l5776"><span class="ln">5776 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5777"><span class="ln">5777 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l5778"><span class="ln">5778 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5779"><span class="ln">5779 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5780"><span class="ln">5780 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5781"><span class="ln">5781 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5782"><span class="ln">5782 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5783"><span class="ln">5783 </span></a>    amin(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l5784"><span class="ln">5784 </span></a> 
<a name="l5785"><span class="ln">5785 </span></a>    Returns the minimum value of each slice of the :attr:`input` tensor in the given 
<a name="l5786"><span class="ln">5786 </span></a>    dimension(s) :attr:`dim`. 
<a name="l5787"><span class="ln">5787 </span></a> 
<a name="l5788"><span class="ln">5788 </span></a>    .. note:: 
<a name="l5789"><span class="ln">5789 </span></a>        The difference between ``max``/``min`` and ``amax``/``amin`` is: 
<a name="l5790"><span class="ln">5790 </span></a>            - ``amax``/``amin`` supports reducing on multiple dimensions, 
<a name="l5791"><span class="ln">5791 </span></a>            - ``amax``/``amin`` does not return indices. 
<a name="l5792"><span class="ln">5792 </span></a> 
<a name="l5793"><span class="ln">5793 </span></a>        Both ``max``/``min`` and ``amax``/``amin`` evenly distribute gradients between equal values 
<a name="l5794"><span class="ln">5794 </span></a>        when there are multiple input elements with the same minimum or maximum value. 
<a name="l5795"><span class="ln">5795 </span></a> 
<a name="l5796"><span class="ln">5796 </span></a> 
<a name="l5797"><span class="ln">5797 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l5798"><span class="ln">5798 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l5799"><span class="ln">5799 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l5800"><span class="ln">5800 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l5801"><span class="ln">5801 </span></a> 
<a name="l5802"><span class="ln">5802 </span></a> 
<a name="l5803"><span class="ln">5803 </span></a>    Args: 
<a name="l5804"><span class="ln">5804 </span></a>        input (Tensor): the input tensor. 
<a name="l5805"><span class="ln">5805 </span></a> 
<a name="l5806"><span class="ln">5806 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l5807"><span class="ln">5807 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l5808"><span class="ln">5808 </span></a> 
<a name="l5809"><span class="ln">5809 </span></a> 
<a name="l5810"><span class="ln">5810 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l5811"><span class="ln">5811 </span></a> 
<a name="l5812"><span class="ln">5812 </span></a> 
<a name="l5813"><span class="ln">5813 </span></a>    Keyword args: 
<a name="l5814"><span class="ln">5814 </span></a>      out (Tensor, optional): the output tensor. 
<a name="l5815"><span class="ln">5815 </span></a> 
<a name="l5816"><span class="ln">5816 </span></a>    Example:: 
<a name="l5817"><span class="ln">5817 </span></a> 
<a name="l5818"><span class="ln">5818 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l5819"><span class="ln">5819 </span></a>        &gt;&gt;&gt; a 
<a name="l5820"><span class="ln">5820 </span></a>        tensor([[ 0.6451, -0.4866,  0.2987, -1.3312], 
<a name="l5821"><span class="ln">5821 </span></a>                [-0.5744,  1.2980,  1.8397, -0.2713], 
<a name="l5822"><span class="ln">5822 </span></a>                [ 0.9128,  0.9214, -1.7268, -0.2995], 
<a name="l5823"><span class="ln">5823 </span></a>                [ 0.9023,  0.4853,  0.9075, -1.6165]]) 
<a name="l5824"><span class="ln">5824 </span></a>        &gt;&gt;&gt; torch.amin(a, 1) 
<a name="l5825"><span class="ln">5825 </span></a>        tensor([-1.3312, -0.5744, -1.7268, -1.6165]) 
<a name="l5826"><span class="ln">5826 </span></a>    &quot;&quot;&quot;</span>
<a name="l5827"><span class="ln">5827 </span></a>
<a name="l5828"><span class="ln">5828 </span></a><span class="s2">def </span><span class="s1">aminmax</span><span class="s3">(</span>
<a name="l5829"><span class="ln">5829 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5830"><span class="ln">5830 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5831"><span class="ln">5831 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5832"><span class="ln">5832 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5833"><span class="ln">5833 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5834"><span class="ln">5834 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">aminmax</span><span class="s2">:</span>
<a name="l5835"><span class="ln">5835 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5836"><span class="ln">5836 </span></a>    aminmax(input, *, dim=None, keepdim=False, out=None) -&gt; (Tensor min, Tensor max) 
<a name="l5837"><span class="ln">5837 </span></a> 
<a name="l5838"><span class="ln">5838 </span></a>    Computes the minimum and maximum values of the :attr:`input` tensor. 
<a name="l5839"><span class="ln">5839 </span></a> 
<a name="l5840"><span class="ln">5840 </span></a>    Args: 
<a name="l5841"><span class="ln">5841 </span></a>        input (Tensor): 
<a name="l5842"><span class="ln">5842 </span></a>            The input tensor 
<a name="l5843"><span class="ln">5843 </span></a> 
<a name="l5844"><span class="ln">5844 </span></a>    Keyword Args: 
<a name="l5845"><span class="ln">5845 </span></a>        dim (Optional[int]): 
<a name="l5846"><span class="ln">5846 </span></a>            The dimension along which to compute the values. If `None`, 
<a name="l5847"><span class="ln">5847 </span></a>            computes the values over the entire :attr:`input` tensor. 
<a name="l5848"><span class="ln">5848 </span></a>            Default is `None`. 
<a name="l5849"><span class="ln">5849 </span></a>        keepdim (bool): 
<a name="l5850"><span class="ln">5850 </span></a>            If `True`, the reduced dimensions will be kept in the output 
<a name="l5851"><span class="ln">5851 </span></a>            tensor as dimensions with size 1 for broadcasting, otherwise 
<a name="l5852"><span class="ln">5852 </span></a>            they will be removed, as if calling (:func:`torch.squeeze`). 
<a name="l5853"><span class="ln">5853 </span></a>            Default is `False`. 
<a name="l5854"><span class="ln">5854 </span></a>        out (Optional[Tuple[Tensor, Tensor]]): 
<a name="l5855"><span class="ln">5855 </span></a>            Optional tensors on which to write the result. Must have the same 
<a name="l5856"><span class="ln">5856 </span></a>            shape and dtype as the expected output. 
<a name="l5857"><span class="ln">5857 </span></a>            Default is `None`. 
<a name="l5858"><span class="ln">5858 </span></a> 
<a name="l5859"><span class="ln">5859 </span></a>    Returns: 
<a name="l5860"><span class="ln">5860 </span></a>        A named tuple `(min, max)` containing the minimum and maximum values. 
<a name="l5861"><span class="ln">5861 </span></a> 
<a name="l5862"><span class="ln">5862 </span></a>    Raises: 
<a name="l5863"><span class="ln">5863 </span></a>        RuntimeError 
<a name="l5864"><span class="ln">5864 </span></a>            If any of the dimensions to compute the values over has size 0. 
<a name="l5865"><span class="ln">5865 </span></a> 
<a name="l5866"><span class="ln">5866 </span></a>    .. note:: 
<a name="l5867"><span class="ln">5867 </span></a>        NaN values are propagated to the output if at least one value is NaN. 
<a name="l5868"><span class="ln">5868 </span></a> 
<a name="l5869"><span class="ln">5869 </span></a>    .. seealso:: 
<a name="l5870"><span class="ln">5870 </span></a>        :func:`torch.amin` computes just the minimum value 
<a name="l5871"><span class="ln">5871 </span></a>        :func:`torch.amax` computes just the maximum value 
<a name="l5872"><span class="ln">5872 </span></a> 
<a name="l5873"><span class="ln">5873 </span></a>    Example:: 
<a name="l5874"><span class="ln">5874 </span></a> 
<a name="l5875"><span class="ln">5875 </span></a>        &gt;&gt;&gt; torch.aminmax(torch.tensor([1, -3, 5])) 
<a name="l5876"><span class="ln">5876 </span></a>        torch.return_types.aminmax( 
<a name="l5877"><span class="ln">5877 </span></a>        min=tensor(-3), 
<a name="l5878"><span class="ln">5878 </span></a>        max=tensor(5)) 
<a name="l5879"><span class="ln">5879 </span></a> 
<a name="l5880"><span class="ln">5880 </span></a>        &gt;&gt;&gt; # aminmax propagates NaNs 
<a name="l5881"><span class="ln">5881 </span></a>        &gt;&gt;&gt; torch.aminmax(torch.tensor([1, -3, 5, torch.nan])) 
<a name="l5882"><span class="ln">5882 </span></a>        torch.return_types.aminmax( 
<a name="l5883"><span class="ln">5883 </span></a>        min=tensor(nan), 
<a name="l5884"><span class="ln">5884 </span></a>        max=tensor(nan)) 
<a name="l5885"><span class="ln">5885 </span></a> 
<a name="l5886"><span class="ln">5886 </span></a>        &gt;&gt;&gt; t = torch.arange(10).view(2, 5) 
<a name="l5887"><span class="ln">5887 </span></a>        &gt;&gt;&gt; t 
<a name="l5888"><span class="ln">5888 </span></a>        tensor([[0, 1, 2, 3, 4], 
<a name="l5889"><span class="ln">5889 </span></a>                [5, 6, 7, 8, 9]]) 
<a name="l5890"><span class="ln">5890 </span></a>        &gt;&gt;&gt; t.aminmax(dim=0, keepdim=True) 
<a name="l5891"><span class="ln">5891 </span></a>        torch.return_types.aminmax( 
<a name="l5892"><span class="ln">5892 </span></a>        min=tensor([[0, 1, 2, 3, 4]]), 
<a name="l5893"><span class="ln">5893 </span></a>        max=tensor([[5, 6, 7, 8, 9]])) 
<a name="l5894"><span class="ln">5894 </span></a>    &quot;&quot;&quot;</span>
<a name="l5895"><span class="ln">5895 </span></a>
<a name="l5896"><span class="ln">5896 </span></a><span class="s2">def </span><span class="s1">angle</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5897"><span class="ln">5897 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5898"><span class="ln">5898 </span></a>    angle(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l5899"><span class="ln">5899 </span></a> 
<a name="l5900"><span class="ln">5900 </span></a>    Computes the element-wise angle (in radians) of the given :attr:`input` tensor. 
<a name="l5901"><span class="ln">5901 </span></a> 
<a name="l5902"><span class="ln">5902 </span></a>    .. math:: 
<a name="l5903"><span class="ln">5903 </span></a>        \text{out}_{i} = angle(\text{input}_{i}) 
<a name="l5904"><span class="ln">5904 </span></a> 
<a name="l5905"><span class="ln">5905 </span></a>    Args: 
<a name="l5906"><span class="ln">5906 </span></a>        input (Tensor): the input tensor. 
<a name="l5907"><span class="ln">5907 </span></a> 
<a name="l5908"><span class="ln">5908 </span></a>    Keyword args: 
<a name="l5909"><span class="ln">5909 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5910"><span class="ln">5910 </span></a> 
<a name="l5911"><span class="ln">5911 </span></a>    .. note:: Starting in PyTorch 1.8, angle returns pi for negative real numbers, 
<a name="l5912"><span class="ln">5912 </span></a>              zero for non-negative real numbers, and propagates NaNs. Previously 
<a name="l5913"><span class="ln">5913 </span></a>              the function would return zero for all real numbers and not propagate 
<a name="l5914"><span class="ln">5914 </span></a>              floating-point NaNs. 
<a name="l5915"><span class="ln">5915 </span></a> 
<a name="l5916"><span class="ln">5916 </span></a>    Example:: 
<a name="l5917"><span class="ln">5917 </span></a> 
<a name="l5918"><span class="ln">5918 </span></a>        &gt;&gt;&gt; torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159 
<a name="l5919"><span class="ln">5919 </span></a>        tensor([ 135.,  135,  -45]) 
<a name="l5920"><span class="ln">5920 </span></a>    &quot;&quot;&quot;</span>
<a name="l5921"><span class="ln">5921 </span></a>
<a name="l5922"><span class="ln">5922 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5923"><span class="ln">5923 </span></a><span class="s2">def </span><span class="s1">any</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l5924"><span class="ln">5924 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l5925"><span class="ln">5925 </span></a>    any(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l5926"><span class="ln">5926 </span></a> 
<a name="l5927"><span class="ln">5927 </span></a>    Tests if any element in :attr:`input` evaluates to `True`. 
<a name="l5928"><span class="ln">5928 </span></a> 
<a name="l5929"><span class="ln">5929 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l5930"><span class="ln">5930 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l5931"><span class="ln">5931 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l5932"><span class="ln">5932 </span></a> 
<a name="l5933"><span class="ln">5933 </span></a>    Args: 
<a name="l5934"><span class="ln">5934 </span></a>        input (Tensor): the input tensor. 
<a name="l5935"><span class="ln">5935 </span></a> 
<a name="l5936"><span class="ln">5936 </span></a>    Keyword args: 
<a name="l5937"><span class="ln">5937 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5938"><span class="ln">5938 </span></a> 
<a name="l5939"><span class="ln">5939 </span></a>    Example:: 
<a name="l5940"><span class="ln">5940 </span></a> 
<a name="l5941"><span class="ln">5941 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l5942"><span class="ln">5942 </span></a>        &gt;&gt;&gt; a 
<a name="l5943"><span class="ln">5943 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l5944"><span class="ln">5944 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l5945"><span class="ln">5945 </span></a>        tensor(True, dtype=torch.bool) 
<a name="l5946"><span class="ln">5946 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l5947"><span class="ln">5947 </span></a>        &gt;&gt;&gt; a 
<a name="l5948"><span class="ln">5948 </span></a>        tensor([0, 1, 2]) 
<a name="l5949"><span class="ln">5949 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l5950"><span class="ln">5950 </span></a>        tensor(True) 
<a name="l5951"><span class="ln">5951 </span></a> 
<a name="l5952"><span class="ln">5952 </span></a>    .. function:: any(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l5953"><span class="ln">5953 </span></a>       :noindex: 
<a name="l5954"><span class="ln">5954 </span></a> 
<a name="l5955"><span class="ln">5955 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l5956"><span class="ln">5956 </span></a>    returns `True` if any element in the row evaluate to `True` and `False` otherwise. 
<a name="l5957"><span class="ln">5957 </span></a> 
<a name="l5958"><span class="ln">5958 </span></a> 
<a name="l5959"><span class="ln">5959 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l5960"><span class="ln">5960 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l5961"><span class="ln">5961 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l5962"><span class="ln">5962 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l5963"><span class="ln">5963 </span></a> 
<a name="l5964"><span class="ln">5964 </span></a> 
<a name="l5965"><span class="ln">5965 </span></a>    Args: 
<a name="l5966"><span class="ln">5966 </span></a>        input (Tensor): the input tensor. 
<a name="l5967"><span class="ln">5967 </span></a> 
<a name="l5968"><span class="ln">5968 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l5969"><span class="ln">5969 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l5970"><span class="ln">5970 </span></a> 
<a name="l5971"><span class="ln">5971 </span></a> 
<a name="l5972"><span class="ln">5972 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l5973"><span class="ln">5973 </span></a> 
<a name="l5974"><span class="ln">5974 </span></a> 
<a name="l5975"><span class="ln">5975 </span></a>    Keyword args: 
<a name="l5976"><span class="ln">5976 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l5977"><span class="ln">5977 </span></a> 
<a name="l5978"><span class="ln">5978 </span></a>    Example:: 
<a name="l5979"><span class="ln">5979 </span></a> 
<a name="l5980"><span class="ln">5980 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0 
<a name="l5981"><span class="ln">5981 </span></a>        &gt;&gt;&gt; a 
<a name="l5982"><span class="ln">5982 </span></a>        tensor([[ True,  True], 
<a name="l5983"><span class="ln">5983 </span></a>                [False,  True], 
<a name="l5984"><span class="ln">5984 </span></a>                [ True,  True], 
<a name="l5985"><span class="ln">5985 </span></a>                [False, False]]) 
<a name="l5986"><span class="ln">5986 </span></a>        &gt;&gt;&gt; torch.any(a, 1) 
<a name="l5987"><span class="ln">5987 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l5988"><span class="ln">5988 </span></a>        &gt;&gt;&gt; torch.any(a, 0) 
<a name="l5989"><span class="ln">5989 </span></a>        tensor([True, True]) 
<a name="l5990"><span class="ln">5990 </span></a>    &quot;&quot;&quot;</span>
<a name="l5991"><span class="ln">5991 </span></a>
<a name="l5992"><span class="ln">5992 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l5993"><span class="ln">5993 </span></a><span class="s2">def </span><span class="s1">any</span><span class="s3">(</span>
<a name="l5994"><span class="ln">5994 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l5995"><span class="ln">5995 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5996"><span class="ln">5996 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l5997"><span class="ln">5997 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l5998"><span class="ln">5998 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l5999"><span class="ln">5999 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6000"><span class="ln">6000 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6001"><span class="ln">6001 </span></a>    any(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6002"><span class="ln">6002 </span></a> 
<a name="l6003"><span class="ln">6003 </span></a>    Tests if any element in :attr:`input` evaluates to `True`. 
<a name="l6004"><span class="ln">6004 </span></a> 
<a name="l6005"><span class="ln">6005 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l6006"><span class="ln">6006 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l6007"><span class="ln">6007 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l6008"><span class="ln">6008 </span></a> 
<a name="l6009"><span class="ln">6009 </span></a>    Args: 
<a name="l6010"><span class="ln">6010 </span></a>        input (Tensor): the input tensor. 
<a name="l6011"><span class="ln">6011 </span></a> 
<a name="l6012"><span class="ln">6012 </span></a>    Keyword args: 
<a name="l6013"><span class="ln">6013 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6014"><span class="ln">6014 </span></a> 
<a name="l6015"><span class="ln">6015 </span></a>    Example:: 
<a name="l6016"><span class="ln">6016 </span></a> 
<a name="l6017"><span class="ln">6017 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l6018"><span class="ln">6018 </span></a>        &gt;&gt;&gt; a 
<a name="l6019"><span class="ln">6019 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l6020"><span class="ln">6020 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l6021"><span class="ln">6021 </span></a>        tensor(True, dtype=torch.bool) 
<a name="l6022"><span class="ln">6022 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l6023"><span class="ln">6023 </span></a>        &gt;&gt;&gt; a 
<a name="l6024"><span class="ln">6024 </span></a>        tensor([0, 1, 2]) 
<a name="l6025"><span class="ln">6025 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l6026"><span class="ln">6026 </span></a>        tensor(True) 
<a name="l6027"><span class="ln">6027 </span></a> 
<a name="l6028"><span class="ln">6028 </span></a>    .. function:: any(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l6029"><span class="ln">6029 </span></a>       :noindex: 
<a name="l6030"><span class="ln">6030 </span></a> 
<a name="l6031"><span class="ln">6031 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l6032"><span class="ln">6032 </span></a>    returns `True` if any element in the row evaluate to `True` and `False` otherwise. 
<a name="l6033"><span class="ln">6033 </span></a> 
<a name="l6034"><span class="ln">6034 </span></a> 
<a name="l6035"><span class="ln">6035 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l6036"><span class="ln">6036 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l6037"><span class="ln">6037 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l6038"><span class="ln">6038 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l6039"><span class="ln">6039 </span></a> 
<a name="l6040"><span class="ln">6040 </span></a> 
<a name="l6041"><span class="ln">6041 </span></a>    Args: 
<a name="l6042"><span class="ln">6042 </span></a>        input (Tensor): the input tensor. 
<a name="l6043"><span class="ln">6043 </span></a> 
<a name="l6044"><span class="ln">6044 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l6045"><span class="ln">6045 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l6046"><span class="ln">6046 </span></a> 
<a name="l6047"><span class="ln">6047 </span></a> 
<a name="l6048"><span class="ln">6048 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l6049"><span class="ln">6049 </span></a> 
<a name="l6050"><span class="ln">6050 </span></a> 
<a name="l6051"><span class="ln">6051 </span></a>    Keyword args: 
<a name="l6052"><span class="ln">6052 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6053"><span class="ln">6053 </span></a> 
<a name="l6054"><span class="ln">6054 </span></a>    Example:: 
<a name="l6055"><span class="ln">6055 </span></a> 
<a name="l6056"><span class="ln">6056 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0 
<a name="l6057"><span class="ln">6057 </span></a>        &gt;&gt;&gt; a 
<a name="l6058"><span class="ln">6058 </span></a>        tensor([[ True,  True], 
<a name="l6059"><span class="ln">6059 </span></a>                [False,  True], 
<a name="l6060"><span class="ln">6060 </span></a>                [ True,  True], 
<a name="l6061"><span class="ln">6061 </span></a>                [False, False]]) 
<a name="l6062"><span class="ln">6062 </span></a>        &gt;&gt;&gt; torch.any(a, 1) 
<a name="l6063"><span class="ln">6063 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l6064"><span class="ln">6064 </span></a>        &gt;&gt;&gt; torch.any(a, 0) 
<a name="l6065"><span class="ln">6065 </span></a>        tensor([True, True]) 
<a name="l6066"><span class="ln">6066 </span></a>    &quot;&quot;&quot;</span>
<a name="l6067"><span class="ln">6067 </span></a>
<a name="l6068"><span class="ln">6068 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6069"><span class="ln">6069 </span></a><span class="s2">def </span><span class="s1">any</span><span class="s3">(</span>
<a name="l6070"><span class="ln">6070 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6071"><span class="ln">6071 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l6072"><span class="ln">6072 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6073"><span class="ln">6073 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6074"><span class="ln">6074 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6075"><span class="ln">6075 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6076"><span class="ln">6076 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6077"><span class="ln">6077 </span></a>    any(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6078"><span class="ln">6078 </span></a> 
<a name="l6079"><span class="ln">6079 </span></a>    Tests if any element in :attr:`input` evaluates to `True`. 
<a name="l6080"><span class="ln">6080 </span></a> 
<a name="l6081"><span class="ln">6081 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l6082"><span class="ln">6082 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l6083"><span class="ln">6083 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l6084"><span class="ln">6084 </span></a> 
<a name="l6085"><span class="ln">6085 </span></a>    Args: 
<a name="l6086"><span class="ln">6086 </span></a>        input (Tensor): the input tensor. 
<a name="l6087"><span class="ln">6087 </span></a> 
<a name="l6088"><span class="ln">6088 </span></a>    Keyword args: 
<a name="l6089"><span class="ln">6089 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6090"><span class="ln">6090 </span></a> 
<a name="l6091"><span class="ln">6091 </span></a>    Example:: 
<a name="l6092"><span class="ln">6092 </span></a> 
<a name="l6093"><span class="ln">6093 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l6094"><span class="ln">6094 </span></a>        &gt;&gt;&gt; a 
<a name="l6095"><span class="ln">6095 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l6096"><span class="ln">6096 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l6097"><span class="ln">6097 </span></a>        tensor(True, dtype=torch.bool) 
<a name="l6098"><span class="ln">6098 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l6099"><span class="ln">6099 </span></a>        &gt;&gt;&gt; a 
<a name="l6100"><span class="ln">6100 </span></a>        tensor([0, 1, 2]) 
<a name="l6101"><span class="ln">6101 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l6102"><span class="ln">6102 </span></a>        tensor(True) 
<a name="l6103"><span class="ln">6103 </span></a> 
<a name="l6104"><span class="ln">6104 </span></a>    .. function:: any(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l6105"><span class="ln">6105 </span></a>       :noindex: 
<a name="l6106"><span class="ln">6106 </span></a> 
<a name="l6107"><span class="ln">6107 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l6108"><span class="ln">6108 </span></a>    returns `True` if any element in the row evaluate to `True` and `False` otherwise. 
<a name="l6109"><span class="ln">6109 </span></a> 
<a name="l6110"><span class="ln">6110 </span></a> 
<a name="l6111"><span class="ln">6111 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l6112"><span class="ln">6112 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l6113"><span class="ln">6113 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l6114"><span class="ln">6114 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l6115"><span class="ln">6115 </span></a> 
<a name="l6116"><span class="ln">6116 </span></a> 
<a name="l6117"><span class="ln">6117 </span></a>    Args: 
<a name="l6118"><span class="ln">6118 </span></a>        input (Tensor): the input tensor. 
<a name="l6119"><span class="ln">6119 </span></a> 
<a name="l6120"><span class="ln">6120 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l6121"><span class="ln">6121 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l6122"><span class="ln">6122 </span></a> 
<a name="l6123"><span class="ln">6123 </span></a> 
<a name="l6124"><span class="ln">6124 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l6125"><span class="ln">6125 </span></a> 
<a name="l6126"><span class="ln">6126 </span></a> 
<a name="l6127"><span class="ln">6127 </span></a>    Keyword args: 
<a name="l6128"><span class="ln">6128 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6129"><span class="ln">6129 </span></a> 
<a name="l6130"><span class="ln">6130 </span></a>    Example:: 
<a name="l6131"><span class="ln">6131 </span></a> 
<a name="l6132"><span class="ln">6132 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0 
<a name="l6133"><span class="ln">6133 </span></a>        &gt;&gt;&gt; a 
<a name="l6134"><span class="ln">6134 </span></a>        tensor([[ True,  True], 
<a name="l6135"><span class="ln">6135 </span></a>                [False,  True], 
<a name="l6136"><span class="ln">6136 </span></a>                [ True,  True], 
<a name="l6137"><span class="ln">6137 </span></a>                [False, False]]) 
<a name="l6138"><span class="ln">6138 </span></a>        &gt;&gt;&gt; torch.any(a, 1) 
<a name="l6139"><span class="ln">6139 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l6140"><span class="ln">6140 </span></a>        &gt;&gt;&gt; torch.any(a, 0) 
<a name="l6141"><span class="ln">6141 </span></a>        tensor([True, True]) 
<a name="l6142"><span class="ln">6142 </span></a>    &quot;&quot;&quot;</span>
<a name="l6143"><span class="ln">6143 </span></a>
<a name="l6144"><span class="ln">6144 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6145"><span class="ln">6145 </span></a><span class="s2">def </span><span class="s1">any</span><span class="s3">(</span>
<a name="l6146"><span class="ln">6146 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6147"><span class="ln">6147 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l6148"><span class="ln">6148 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6149"><span class="ln">6149 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6150"><span class="ln">6150 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6151"><span class="ln">6151 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6152"><span class="ln">6152 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6153"><span class="ln">6153 </span></a>    any(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6154"><span class="ln">6154 </span></a> 
<a name="l6155"><span class="ln">6155 </span></a>    Tests if any element in :attr:`input` evaluates to `True`. 
<a name="l6156"><span class="ln">6156 </span></a> 
<a name="l6157"><span class="ln">6157 </span></a>    .. note:: This function matches the behaviour of NumPy in returning 
<a name="l6158"><span class="ln">6158 </span></a>              output of dtype `bool` for all supported dtypes except `uint8`. 
<a name="l6159"><span class="ln">6159 </span></a>              For `uint8` the dtype of output is `uint8` itself. 
<a name="l6160"><span class="ln">6160 </span></a> 
<a name="l6161"><span class="ln">6161 </span></a>    Args: 
<a name="l6162"><span class="ln">6162 </span></a>        input (Tensor): the input tensor. 
<a name="l6163"><span class="ln">6163 </span></a> 
<a name="l6164"><span class="ln">6164 </span></a>    Keyword args: 
<a name="l6165"><span class="ln">6165 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6166"><span class="ln">6166 </span></a> 
<a name="l6167"><span class="ln">6167 </span></a>    Example:: 
<a name="l6168"><span class="ln">6168 </span></a> 
<a name="l6169"><span class="ln">6169 </span></a>        &gt;&gt;&gt; a = torch.rand(1, 2).bool() 
<a name="l6170"><span class="ln">6170 </span></a>        &gt;&gt;&gt; a 
<a name="l6171"><span class="ln">6171 </span></a>        tensor([[False, True]], dtype=torch.bool) 
<a name="l6172"><span class="ln">6172 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l6173"><span class="ln">6173 </span></a>        tensor(True, dtype=torch.bool) 
<a name="l6174"><span class="ln">6174 </span></a>        &gt;&gt;&gt; a = torch.arange(0, 3) 
<a name="l6175"><span class="ln">6175 </span></a>        &gt;&gt;&gt; a 
<a name="l6176"><span class="ln">6176 </span></a>        tensor([0, 1, 2]) 
<a name="l6177"><span class="ln">6177 </span></a>        &gt;&gt;&gt; torch.any(a) 
<a name="l6178"><span class="ln">6178 </span></a>        tensor(True) 
<a name="l6179"><span class="ln">6179 </span></a> 
<a name="l6180"><span class="ln">6180 </span></a>    .. function:: any(input, dim, keepdim=False, *, out=None) -&gt; Tensor 
<a name="l6181"><span class="ln">6181 </span></a>       :noindex: 
<a name="l6182"><span class="ln">6182 </span></a> 
<a name="l6183"><span class="ln">6183 </span></a>    For each row of :attr:`input` in the given dimension :attr:`dim`, 
<a name="l6184"><span class="ln">6184 </span></a>    returns `True` if any element in the row evaluate to `True` and `False` otherwise. 
<a name="l6185"><span class="ln">6185 </span></a> 
<a name="l6186"><span class="ln">6186 </span></a> 
<a name="l6187"><span class="ln">6187 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l6188"><span class="ln">6188 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l6189"><span class="ln">6189 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l6190"><span class="ln">6190 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l6191"><span class="ln">6191 </span></a> 
<a name="l6192"><span class="ln">6192 </span></a> 
<a name="l6193"><span class="ln">6193 </span></a>    Args: 
<a name="l6194"><span class="ln">6194 </span></a>        input (Tensor): the input tensor. 
<a name="l6195"><span class="ln">6195 </span></a> 
<a name="l6196"><span class="ln">6196 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l6197"><span class="ln">6197 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l6198"><span class="ln">6198 </span></a> 
<a name="l6199"><span class="ln">6199 </span></a> 
<a name="l6200"><span class="ln">6200 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l6201"><span class="ln">6201 </span></a> 
<a name="l6202"><span class="ln">6202 </span></a> 
<a name="l6203"><span class="ln">6203 </span></a>    Keyword args: 
<a name="l6204"><span class="ln">6204 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6205"><span class="ln">6205 </span></a> 
<a name="l6206"><span class="ln">6206 </span></a>    Example:: 
<a name="l6207"><span class="ln">6207 </span></a> 
<a name="l6208"><span class="ln">6208 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0 
<a name="l6209"><span class="ln">6209 </span></a>        &gt;&gt;&gt; a 
<a name="l6210"><span class="ln">6210 </span></a>        tensor([[ True,  True], 
<a name="l6211"><span class="ln">6211 </span></a>                [False,  True], 
<a name="l6212"><span class="ln">6212 </span></a>                [ True,  True], 
<a name="l6213"><span class="ln">6213 </span></a>                [False, False]]) 
<a name="l6214"><span class="ln">6214 </span></a>        &gt;&gt;&gt; torch.any(a, 1) 
<a name="l6215"><span class="ln">6215 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l6216"><span class="ln">6216 </span></a>        &gt;&gt;&gt; torch.any(a, 0) 
<a name="l6217"><span class="ln">6217 </span></a>        tensor([True, True]) 
<a name="l6218"><span class="ln">6218 </span></a>    &quot;&quot;&quot;</span>
<a name="l6219"><span class="ln">6219 </span></a>
<a name="l6220"><span class="ln">6220 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6221"><span class="ln">6221 </span></a><span class="s2">def </span><span class="s1">arange</span><span class="s3">(</span>
<a name="l6222"><span class="ln">6222 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l6223"><span class="ln">6223 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l6224"><span class="ln">6224 </span></a>    <span class="s1">step</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l6225"><span class="ln">6225 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6226"><span class="ln">6226 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6227"><span class="ln">6227 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6228"><span class="ln">6228 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6229"><span class="ln">6229 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6230"><span class="ln">6230 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6231"><span class="ln">6231 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6232"><span class="ln">6232 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6233"><span class="ln">6233 </span></a>    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l6234"><span class="ln">6234 </span></a> 
<a name="l6235"><span class="ln">6235 </span></a>    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil` 
<a name="l6236"><span class="ln">6236 </span></a>    with values from the interval ``[start, end)`` taken with common difference 
<a name="l6237"><span class="ln">6237 </span></a>    :attr:`step` beginning from `start`. 
<a name="l6238"><span class="ln">6238 </span></a> 
<a name="l6239"><span class="ln">6239 </span></a>    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``), 
<a name="l6240"><span class="ln">6240 </span></a>    the results may be affected by floating-point rounding behavior. Some values in the sequence 
<a name="l6241"><span class="ln">6241 </span></a>    might not be exactly representable in certain floating-point formats, which can lead to 
<a name="l6242"><span class="ln">6242 </span></a>    repeated values or unexpected rounding. For precise sequences, it is recommended to use 
<a name="l6243"><span class="ln">6243 </span></a>    integer dtypes instead of floating-point dtypes. 
<a name="l6244"><span class="ln">6244 </span></a> 
<a name="l6245"><span class="ln">6245 </span></a>    Note that non-integer :attr:`step` is subject to floating point rounding errors when 
<a name="l6246"><span class="ln">6246 </span></a>    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end` 
<a name="l6247"><span class="ln">6247 </span></a>    in such cases. 
<a name="l6248"><span class="ln">6248 </span></a> 
<a name="l6249"><span class="ln">6249 </span></a>    .. math:: 
<a name="l6250"><span class="ln">6250 </span></a>        \text{out}_{{i+1}} = \text{out}_{i} + \text{step} 
<a name="l6251"><span class="ln">6251 </span></a> 
<a name="l6252"><span class="ln">6252 </span></a>    Args: 
<a name="l6253"><span class="ln">6253 </span></a>        start (Number, optional): the starting value for the set of points. Default: ``0``. 
<a name="l6254"><span class="ln">6254 </span></a>        end (Number): the ending value for the set of points 
<a name="l6255"><span class="ln">6255 </span></a>        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l6256"><span class="ln">6256 </span></a> 
<a name="l6257"><span class="ln">6257 </span></a>    Keyword args: 
<a name="l6258"><span class="ln">6258 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6259"><span class="ln">6259 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l6260"><span class="ln">6260 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input 
<a name="l6261"><span class="ln">6261 </span></a>            arguments. If any of `start`, `end`, or `stop` are floating-point, the 
<a name="l6262"><span class="ln">6262 </span></a>            `dtype` is inferred to be the default dtype, see 
<a name="l6263"><span class="ln">6263 </span></a>            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l6264"><span class="ln">6264 </span></a>            be `torch.int64`. 
<a name="l6265"><span class="ln">6265 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l6266"><span class="ln">6266 </span></a>            Default: ``torch.strided``. 
<a name="l6267"><span class="ln">6267 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l6268"><span class="ln">6268 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l6269"><span class="ln">6269 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l6270"><span class="ln">6270 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l6271"><span class="ln">6271 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l6272"><span class="ln">6272 </span></a>            returned tensor. Default: ``False``. 
<a name="l6273"><span class="ln">6273 </span></a> 
<a name="l6274"><span class="ln">6274 </span></a>    Example:: 
<a name="l6275"><span class="ln">6275 </span></a> 
<a name="l6276"><span class="ln">6276 </span></a>        &gt;&gt;&gt; torch.arange(5) 
<a name="l6277"><span class="ln">6277 </span></a>        tensor([ 0,  1,  2,  3,  4]) 
<a name="l6278"><span class="ln">6278 </span></a>        &gt;&gt;&gt; torch.arange(1, 4) 
<a name="l6279"><span class="ln">6279 </span></a>        tensor([ 1,  2,  3]) 
<a name="l6280"><span class="ln">6280 </span></a>        &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 
<a name="l6281"><span class="ln">6281 </span></a>        tensor([ 1.0000,  1.5000,  2.0000]) 
<a name="l6282"><span class="ln">6282 </span></a>    &quot;&quot;&quot;</span>
<a name="l6283"><span class="ln">6283 </span></a>
<a name="l6284"><span class="ln">6284 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6285"><span class="ln">6285 </span></a><span class="s2">def </span><span class="s1">arange</span><span class="s3">(</span>
<a name="l6286"><span class="ln">6286 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l6287"><span class="ln">6287 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l6288"><span class="ln">6288 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6289"><span class="ln">6289 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6290"><span class="ln">6290 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6291"><span class="ln">6291 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6292"><span class="ln">6292 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6293"><span class="ln">6293 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6294"><span class="ln">6294 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6295"><span class="ln">6295 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6296"><span class="ln">6296 </span></a>    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l6297"><span class="ln">6297 </span></a> 
<a name="l6298"><span class="ln">6298 </span></a>    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil` 
<a name="l6299"><span class="ln">6299 </span></a>    with values from the interval ``[start, end)`` taken with common difference 
<a name="l6300"><span class="ln">6300 </span></a>    :attr:`step` beginning from `start`. 
<a name="l6301"><span class="ln">6301 </span></a> 
<a name="l6302"><span class="ln">6302 </span></a>    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``), 
<a name="l6303"><span class="ln">6303 </span></a>    the results may be affected by floating-point rounding behavior. Some values in the sequence 
<a name="l6304"><span class="ln">6304 </span></a>    might not be exactly representable in certain floating-point formats, which can lead to 
<a name="l6305"><span class="ln">6305 </span></a>    repeated values or unexpected rounding. For precise sequences, it is recommended to use 
<a name="l6306"><span class="ln">6306 </span></a>    integer dtypes instead of floating-point dtypes. 
<a name="l6307"><span class="ln">6307 </span></a> 
<a name="l6308"><span class="ln">6308 </span></a>    Note that non-integer :attr:`step` is subject to floating point rounding errors when 
<a name="l6309"><span class="ln">6309 </span></a>    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end` 
<a name="l6310"><span class="ln">6310 </span></a>    in such cases. 
<a name="l6311"><span class="ln">6311 </span></a> 
<a name="l6312"><span class="ln">6312 </span></a>    .. math:: 
<a name="l6313"><span class="ln">6313 </span></a>        \text{out}_{{i+1}} = \text{out}_{i} + \text{step} 
<a name="l6314"><span class="ln">6314 </span></a> 
<a name="l6315"><span class="ln">6315 </span></a>    Args: 
<a name="l6316"><span class="ln">6316 </span></a>        start (Number, optional): the starting value for the set of points. Default: ``0``. 
<a name="l6317"><span class="ln">6317 </span></a>        end (Number): the ending value for the set of points 
<a name="l6318"><span class="ln">6318 </span></a>        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l6319"><span class="ln">6319 </span></a> 
<a name="l6320"><span class="ln">6320 </span></a>    Keyword args: 
<a name="l6321"><span class="ln">6321 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6322"><span class="ln">6322 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l6323"><span class="ln">6323 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input 
<a name="l6324"><span class="ln">6324 </span></a>            arguments. If any of `start`, `end`, or `stop` are floating-point, the 
<a name="l6325"><span class="ln">6325 </span></a>            `dtype` is inferred to be the default dtype, see 
<a name="l6326"><span class="ln">6326 </span></a>            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l6327"><span class="ln">6327 </span></a>            be `torch.int64`. 
<a name="l6328"><span class="ln">6328 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l6329"><span class="ln">6329 </span></a>            Default: ``torch.strided``. 
<a name="l6330"><span class="ln">6330 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l6331"><span class="ln">6331 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l6332"><span class="ln">6332 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l6333"><span class="ln">6333 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l6334"><span class="ln">6334 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l6335"><span class="ln">6335 </span></a>            returned tensor. Default: ``False``. 
<a name="l6336"><span class="ln">6336 </span></a> 
<a name="l6337"><span class="ln">6337 </span></a>    Example:: 
<a name="l6338"><span class="ln">6338 </span></a> 
<a name="l6339"><span class="ln">6339 </span></a>        &gt;&gt;&gt; torch.arange(5) 
<a name="l6340"><span class="ln">6340 </span></a>        tensor([ 0,  1,  2,  3,  4]) 
<a name="l6341"><span class="ln">6341 </span></a>        &gt;&gt;&gt; torch.arange(1, 4) 
<a name="l6342"><span class="ln">6342 </span></a>        tensor([ 1,  2,  3]) 
<a name="l6343"><span class="ln">6343 </span></a>        &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 
<a name="l6344"><span class="ln">6344 </span></a>        tensor([ 1.0000,  1.5000,  2.0000]) 
<a name="l6345"><span class="ln">6345 </span></a>    &quot;&quot;&quot;</span>
<a name="l6346"><span class="ln">6346 </span></a>
<a name="l6347"><span class="ln">6347 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6348"><span class="ln">6348 </span></a><span class="s2">def </span><span class="s1">arange</span><span class="s3">(</span>
<a name="l6349"><span class="ln">6349 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l6350"><span class="ln">6350 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6351"><span class="ln">6351 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6352"><span class="ln">6352 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6353"><span class="ln">6353 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6354"><span class="ln">6354 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6355"><span class="ln">6355 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6356"><span class="ln">6356 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6357"><span class="ln">6357 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6358"><span class="ln">6358 </span></a>    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l6359"><span class="ln">6359 </span></a> 
<a name="l6360"><span class="ln">6360 </span></a>    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil` 
<a name="l6361"><span class="ln">6361 </span></a>    with values from the interval ``[start, end)`` taken with common difference 
<a name="l6362"><span class="ln">6362 </span></a>    :attr:`step` beginning from `start`. 
<a name="l6363"><span class="ln">6363 </span></a> 
<a name="l6364"><span class="ln">6364 </span></a>    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``), 
<a name="l6365"><span class="ln">6365 </span></a>    the results may be affected by floating-point rounding behavior. Some values in the sequence 
<a name="l6366"><span class="ln">6366 </span></a>    might not be exactly representable in certain floating-point formats, which can lead to 
<a name="l6367"><span class="ln">6367 </span></a>    repeated values or unexpected rounding. For precise sequences, it is recommended to use 
<a name="l6368"><span class="ln">6368 </span></a>    integer dtypes instead of floating-point dtypes. 
<a name="l6369"><span class="ln">6369 </span></a> 
<a name="l6370"><span class="ln">6370 </span></a>    Note that non-integer :attr:`step` is subject to floating point rounding errors when 
<a name="l6371"><span class="ln">6371 </span></a>    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end` 
<a name="l6372"><span class="ln">6372 </span></a>    in such cases. 
<a name="l6373"><span class="ln">6373 </span></a> 
<a name="l6374"><span class="ln">6374 </span></a>    .. math:: 
<a name="l6375"><span class="ln">6375 </span></a>        \text{out}_{{i+1}} = \text{out}_{i} + \text{step} 
<a name="l6376"><span class="ln">6376 </span></a> 
<a name="l6377"><span class="ln">6377 </span></a>    Args: 
<a name="l6378"><span class="ln">6378 </span></a>        start (Number, optional): the starting value for the set of points. Default: ``0``. 
<a name="l6379"><span class="ln">6379 </span></a>        end (Number): the ending value for the set of points 
<a name="l6380"><span class="ln">6380 </span></a>        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l6381"><span class="ln">6381 </span></a> 
<a name="l6382"><span class="ln">6382 </span></a>    Keyword args: 
<a name="l6383"><span class="ln">6383 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6384"><span class="ln">6384 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l6385"><span class="ln">6385 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input 
<a name="l6386"><span class="ln">6386 </span></a>            arguments. If any of `start`, `end`, or `stop` are floating-point, the 
<a name="l6387"><span class="ln">6387 </span></a>            `dtype` is inferred to be the default dtype, see 
<a name="l6388"><span class="ln">6388 </span></a>            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l6389"><span class="ln">6389 </span></a>            be `torch.int64`. 
<a name="l6390"><span class="ln">6390 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l6391"><span class="ln">6391 </span></a>            Default: ``torch.strided``. 
<a name="l6392"><span class="ln">6392 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l6393"><span class="ln">6393 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l6394"><span class="ln">6394 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l6395"><span class="ln">6395 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l6396"><span class="ln">6396 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l6397"><span class="ln">6397 </span></a>            returned tensor. Default: ``False``. 
<a name="l6398"><span class="ln">6398 </span></a> 
<a name="l6399"><span class="ln">6399 </span></a>    Example:: 
<a name="l6400"><span class="ln">6400 </span></a> 
<a name="l6401"><span class="ln">6401 </span></a>        &gt;&gt;&gt; torch.arange(5) 
<a name="l6402"><span class="ln">6402 </span></a>        tensor([ 0,  1,  2,  3,  4]) 
<a name="l6403"><span class="ln">6403 </span></a>        &gt;&gt;&gt; torch.arange(1, 4) 
<a name="l6404"><span class="ln">6404 </span></a>        tensor([ 1,  2,  3]) 
<a name="l6405"><span class="ln">6405 </span></a>        &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 
<a name="l6406"><span class="ln">6406 </span></a>        tensor([ 1.0000,  1.5000,  2.0000]) 
<a name="l6407"><span class="ln">6407 </span></a>    &quot;&quot;&quot;</span>
<a name="l6408"><span class="ln">6408 </span></a>
<a name="l6409"><span class="ln">6409 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6410"><span class="ln">6410 </span></a><span class="s2">def </span><span class="s1">arange</span><span class="s3">(</span>
<a name="l6411"><span class="ln">6411 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l6412"><span class="ln">6412 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6413"><span class="ln">6413 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6414"><span class="ln">6414 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6415"><span class="ln">6415 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6416"><span class="ln">6416 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6417"><span class="ln">6417 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l6418"><span class="ln">6418 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l6419"><span class="ln">6419 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6420"><span class="ln">6420 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6421"><span class="ln">6421 </span></a>    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l6422"><span class="ln">6422 </span></a> 
<a name="l6423"><span class="ln">6423 </span></a>    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil` 
<a name="l6424"><span class="ln">6424 </span></a>    with values from the interval ``[start, end)`` taken with common difference 
<a name="l6425"><span class="ln">6425 </span></a>    :attr:`step` beginning from `start`. 
<a name="l6426"><span class="ln">6426 </span></a> 
<a name="l6427"><span class="ln">6427 </span></a>    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``), 
<a name="l6428"><span class="ln">6428 </span></a>    the results may be affected by floating-point rounding behavior. Some values in the sequence 
<a name="l6429"><span class="ln">6429 </span></a>    might not be exactly representable in certain floating-point formats, which can lead to 
<a name="l6430"><span class="ln">6430 </span></a>    repeated values or unexpected rounding. For precise sequences, it is recommended to use 
<a name="l6431"><span class="ln">6431 </span></a>    integer dtypes instead of floating-point dtypes. 
<a name="l6432"><span class="ln">6432 </span></a> 
<a name="l6433"><span class="ln">6433 </span></a>    Note that non-integer :attr:`step` is subject to floating point rounding errors when 
<a name="l6434"><span class="ln">6434 </span></a>    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end` 
<a name="l6435"><span class="ln">6435 </span></a>    in such cases. 
<a name="l6436"><span class="ln">6436 </span></a> 
<a name="l6437"><span class="ln">6437 </span></a>    .. math:: 
<a name="l6438"><span class="ln">6438 </span></a>        \text{out}_{{i+1}} = \text{out}_{i} + \text{step} 
<a name="l6439"><span class="ln">6439 </span></a> 
<a name="l6440"><span class="ln">6440 </span></a>    Args: 
<a name="l6441"><span class="ln">6441 </span></a>        start (Number, optional): the starting value for the set of points. Default: ``0``. 
<a name="l6442"><span class="ln">6442 </span></a>        end (Number): the ending value for the set of points 
<a name="l6443"><span class="ln">6443 </span></a>        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l6444"><span class="ln">6444 </span></a> 
<a name="l6445"><span class="ln">6445 </span></a>    Keyword args: 
<a name="l6446"><span class="ln">6446 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6447"><span class="ln">6447 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l6448"><span class="ln">6448 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input 
<a name="l6449"><span class="ln">6449 </span></a>            arguments. If any of `start`, `end`, or `stop` are floating-point, the 
<a name="l6450"><span class="ln">6450 </span></a>            `dtype` is inferred to be the default dtype, see 
<a name="l6451"><span class="ln">6451 </span></a>            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l6452"><span class="ln">6452 </span></a>            be `torch.int64`. 
<a name="l6453"><span class="ln">6453 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l6454"><span class="ln">6454 </span></a>            Default: ``torch.strided``. 
<a name="l6455"><span class="ln">6455 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l6456"><span class="ln">6456 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l6457"><span class="ln">6457 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l6458"><span class="ln">6458 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l6459"><span class="ln">6459 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l6460"><span class="ln">6460 </span></a>            returned tensor. Default: ``False``. 
<a name="l6461"><span class="ln">6461 </span></a> 
<a name="l6462"><span class="ln">6462 </span></a>    Example:: 
<a name="l6463"><span class="ln">6463 </span></a> 
<a name="l6464"><span class="ln">6464 </span></a>        &gt;&gt;&gt; torch.arange(5) 
<a name="l6465"><span class="ln">6465 </span></a>        tensor([ 0,  1,  2,  3,  4]) 
<a name="l6466"><span class="ln">6466 </span></a>        &gt;&gt;&gt; torch.arange(1, 4) 
<a name="l6467"><span class="ln">6467 </span></a>        tensor([ 1,  2,  3]) 
<a name="l6468"><span class="ln">6468 </span></a>        &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 
<a name="l6469"><span class="ln">6469 </span></a>        tensor([ 1.0000,  1.5000,  2.0000]) 
<a name="l6470"><span class="ln">6470 </span></a>    &quot;&quot;&quot;</span>
<a name="l6471"><span class="ln">6471 </span></a>
<a name="l6472"><span class="ln">6472 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6473"><span class="ln">6473 </span></a><span class="s2">def </span><span class="s1">arange</span><span class="s3">(</span>
<a name="l6474"><span class="ln">6474 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l6475"><span class="ln">6475 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l6476"><span class="ln">6476 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6477"><span class="ln">6477 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6478"><span class="ln">6478 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6479"><span class="ln">6479 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6480"><span class="ln">6480 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l6481"><span class="ln">6481 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l6482"><span class="ln">6482 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6483"><span class="ln">6483 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6484"><span class="ln">6484 </span></a>    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l6485"><span class="ln">6485 </span></a> 
<a name="l6486"><span class="ln">6486 </span></a>    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil` 
<a name="l6487"><span class="ln">6487 </span></a>    with values from the interval ``[start, end)`` taken with common difference 
<a name="l6488"><span class="ln">6488 </span></a>    :attr:`step` beginning from `start`. 
<a name="l6489"><span class="ln">6489 </span></a> 
<a name="l6490"><span class="ln">6490 </span></a>    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``), 
<a name="l6491"><span class="ln">6491 </span></a>    the results may be affected by floating-point rounding behavior. Some values in the sequence 
<a name="l6492"><span class="ln">6492 </span></a>    might not be exactly representable in certain floating-point formats, which can lead to 
<a name="l6493"><span class="ln">6493 </span></a>    repeated values or unexpected rounding. For precise sequences, it is recommended to use 
<a name="l6494"><span class="ln">6494 </span></a>    integer dtypes instead of floating-point dtypes. 
<a name="l6495"><span class="ln">6495 </span></a> 
<a name="l6496"><span class="ln">6496 </span></a>    Note that non-integer :attr:`step` is subject to floating point rounding errors when 
<a name="l6497"><span class="ln">6497 </span></a>    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end` 
<a name="l6498"><span class="ln">6498 </span></a>    in such cases. 
<a name="l6499"><span class="ln">6499 </span></a> 
<a name="l6500"><span class="ln">6500 </span></a>    .. math:: 
<a name="l6501"><span class="ln">6501 </span></a>        \text{out}_{{i+1}} = \text{out}_{i} + \text{step} 
<a name="l6502"><span class="ln">6502 </span></a> 
<a name="l6503"><span class="ln">6503 </span></a>    Args: 
<a name="l6504"><span class="ln">6504 </span></a>        start (Number, optional): the starting value for the set of points. Default: ``0``. 
<a name="l6505"><span class="ln">6505 </span></a>        end (Number): the ending value for the set of points 
<a name="l6506"><span class="ln">6506 </span></a>        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l6507"><span class="ln">6507 </span></a> 
<a name="l6508"><span class="ln">6508 </span></a>    Keyword args: 
<a name="l6509"><span class="ln">6509 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6510"><span class="ln">6510 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l6511"><span class="ln">6511 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input 
<a name="l6512"><span class="ln">6512 </span></a>            arguments. If any of `start`, `end`, or `stop` are floating-point, the 
<a name="l6513"><span class="ln">6513 </span></a>            `dtype` is inferred to be the default dtype, see 
<a name="l6514"><span class="ln">6514 </span></a>            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l6515"><span class="ln">6515 </span></a>            be `torch.int64`. 
<a name="l6516"><span class="ln">6516 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l6517"><span class="ln">6517 </span></a>            Default: ``torch.strided``. 
<a name="l6518"><span class="ln">6518 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l6519"><span class="ln">6519 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l6520"><span class="ln">6520 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l6521"><span class="ln">6521 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l6522"><span class="ln">6522 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l6523"><span class="ln">6523 </span></a>            returned tensor. Default: ``False``. 
<a name="l6524"><span class="ln">6524 </span></a> 
<a name="l6525"><span class="ln">6525 </span></a>    Example:: 
<a name="l6526"><span class="ln">6526 </span></a> 
<a name="l6527"><span class="ln">6527 </span></a>        &gt;&gt;&gt; torch.arange(5) 
<a name="l6528"><span class="ln">6528 </span></a>        tensor([ 0,  1,  2,  3,  4]) 
<a name="l6529"><span class="ln">6529 </span></a>        &gt;&gt;&gt; torch.arange(1, 4) 
<a name="l6530"><span class="ln">6530 </span></a>        tensor([ 1,  2,  3]) 
<a name="l6531"><span class="ln">6531 </span></a>        &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 
<a name="l6532"><span class="ln">6532 </span></a>        tensor([ 1.0000,  1.5000,  2.0000]) 
<a name="l6533"><span class="ln">6533 </span></a>    &quot;&quot;&quot;</span>
<a name="l6534"><span class="ln">6534 </span></a>
<a name="l6535"><span class="ln">6535 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6536"><span class="ln">6536 </span></a><span class="s2">def </span><span class="s1">arange</span><span class="s3">(</span>
<a name="l6537"><span class="ln">6537 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l6538"><span class="ln">6538 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l6539"><span class="ln">6539 </span></a>    <span class="s1">step</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l6540"><span class="ln">6540 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6541"><span class="ln">6541 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6542"><span class="ln">6542 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6543"><span class="ln">6543 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6544"><span class="ln">6544 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6545"><span class="ln">6545 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l6546"><span class="ln">6546 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l6547"><span class="ln">6547 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6548"><span class="ln">6548 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6549"><span class="ln">6549 </span></a>    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l6550"><span class="ln">6550 </span></a> 
<a name="l6551"><span class="ln">6551 </span></a>    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil` 
<a name="l6552"><span class="ln">6552 </span></a>    with values from the interval ``[start, end)`` taken with common difference 
<a name="l6553"><span class="ln">6553 </span></a>    :attr:`step` beginning from `start`. 
<a name="l6554"><span class="ln">6554 </span></a> 
<a name="l6555"><span class="ln">6555 </span></a>    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``), 
<a name="l6556"><span class="ln">6556 </span></a>    the results may be affected by floating-point rounding behavior. Some values in the sequence 
<a name="l6557"><span class="ln">6557 </span></a>    might not be exactly representable in certain floating-point formats, which can lead to 
<a name="l6558"><span class="ln">6558 </span></a>    repeated values or unexpected rounding. For precise sequences, it is recommended to use 
<a name="l6559"><span class="ln">6559 </span></a>    integer dtypes instead of floating-point dtypes. 
<a name="l6560"><span class="ln">6560 </span></a> 
<a name="l6561"><span class="ln">6561 </span></a>    Note that non-integer :attr:`step` is subject to floating point rounding errors when 
<a name="l6562"><span class="ln">6562 </span></a>    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end` 
<a name="l6563"><span class="ln">6563 </span></a>    in such cases. 
<a name="l6564"><span class="ln">6564 </span></a> 
<a name="l6565"><span class="ln">6565 </span></a>    .. math:: 
<a name="l6566"><span class="ln">6566 </span></a>        \text{out}_{{i+1}} = \text{out}_{i} + \text{step} 
<a name="l6567"><span class="ln">6567 </span></a> 
<a name="l6568"><span class="ln">6568 </span></a>    Args: 
<a name="l6569"><span class="ln">6569 </span></a>        start (Number, optional): the starting value for the set of points. Default: ``0``. 
<a name="l6570"><span class="ln">6570 </span></a>        end (Number): the ending value for the set of points 
<a name="l6571"><span class="ln">6571 </span></a>        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l6572"><span class="ln">6572 </span></a> 
<a name="l6573"><span class="ln">6573 </span></a>    Keyword args: 
<a name="l6574"><span class="ln">6574 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l6575"><span class="ln">6575 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l6576"><span class="ln">6576 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input 
<a name="l6577"><span class="ln">6577 </span></a>            arguments. If any of `start`, `end`, or `stop` are floating-point, the 
<a name="l6578"><span class="ln">6578 </span></a>            `dtype` is inferred to be the default dtype, see 
<a name="l6579"><span class="ln">6579 </span></a>            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l6580"><span class="ln">6580 </span></a>            be `torch.int64`. 
<a name="l6581"><span class="ln">6581 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l6582"><span class="ln">6582 </span></a>            Default: ``torch.strided``. 
<a name="l6583"><span class="ln">6583 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l6584"><span class="ln">6584 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l6585"><span class="ln">6585 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l6586"><span class="ln">6586 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l6587"><span class="ln">6587 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l6588"><span class="ln">6588 </span></a>            returned tensor. Default: ``False``. 
<a name="l6589"><span class="ln">6589 </span></a> 
<a name="l6590"><span class="ln">6590 </span></a>    Example:: 
<a name="l6591"><span class="ln">6591 </span></a> 
<a name="l6592"><span class="ln">6592 </span></a>        &gt;&gt;&gt; torch.arange(5) 
<a name="l6593"><span class="ln">6593 </span></a>        tensor([ 0,  1,  2,  3,  4]) 
<a name="l6594"><span class="ln">6594 </span></a>        &gt;&gt;&gt; torch.arange(1, 4) 
<a name="l6595"><span class="ln">6595 </span></a>        tensor([ 1,  2,  3]) 
<a name="l6596"><span class="ln">6596 </span></a>        &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 
<a name="l6597"><span class="ln">6597 </span></a>        tensor([ 1.0000,  1.5000,  2.0000]) 
<a name="l6598"><span class="ln">6598 </span></a>    &quot;&quot;&quot;</span>
<a name="l6599"><span class="ln">6599 </span></a>
<a name="l6600"><span class="ln">6600 </span></a><span class="s2">def </span><span class="s1">arccos</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6601"><span class="ln">6601 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6602"><span class="ln">6602 </span></a>    arccos(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6603"><span class="ln">6603 </span></a> 
<a name="l6604"><span class="ln">6604 </span></a>    Alias for :func:`torch.acos`. 
<a name="l6605"><span class="ln">6605 </span></a>    &quot;&quot;&quot;</span>
<a name="l6606"><span class="ln">6606 </span></a>
<a name="l6607"><span class="ln">6607 </span></a><span class="s2">def </span><span class="s1">arccos_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l6608"><span class="ln">6608 </span></a><span class="s2">def </span><span class="s1">arccosh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6609"><span class="ln">6609 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6610"><span class="ln">6610 </span></a>    arccosh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6611"><span class="ln">6611 </span></a> 
<a name="l6612"><span class="ln">6612 </span></a>    Alias for :func:`torch.acosh`. 
<a name="l6613"><span class="ln">6613 </span></a>    &quot;&quot;&quot;</span>
<a name="l6614"><span class="ln">6614 </span></a>
<a name="l6615"><span class="ln">6615 </span></a><span class="s2">def </span><span class="s1">arccosh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l6616"><span class="ln">6616 </span></a><span class="s2">def </span><span class="s1">arcsin</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6617"><span class="ln">6617 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6618"><span class="ln">6618 </span></a>    arcsin(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6619"><span class="ln">6619 </span></a> 
<a name="l6620"><span class="ln">6620 </span></a>    Alias for :func:`torch.asin`. 
<a name="l6621"><span class="ln">6621 </span></a>    &quot;&quot;&quot;</span>
<a name="l6622"><span class="ln">6622 </span></a>
<a name="l6623"><span class="ln">6623 </span></a><span class="s2">def </span><span class="s1">arcsin_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l6624"><span class="ln">6624 </span></a><span class="s2">def </span><span class="s1">arcsinh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6625"><span class="ln">6625 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6626"><span class="ln">6626 </span></a>    arcsinh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6627"><span class="ln">6627 </span></a> 
<a name="l6628"><span class="ln">6628 </span></a>    Alias for :func:`torch.asinh`. 
<a name="l6629"><span class="ln">6629 </span></a>    &quot;&quot;&quot;</span>
<a name="l6630"><span class="ln">6630 </span></a>
<a name="l6631"><span class="ln">6631 </span></a><span class="s2">def </span><span class="s1">arcsinh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l6632"><span class="ln">6632 </span></a><span class="s2">def </span><span class="s1">arctan</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6633"><span class="ln">6633 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6634"><span class="ln">6634 </span></a>    arctan(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6635"><span class="ln">6635 </span></a> 
<a name="l6636"><span class="ln">6636 </span></a>    Alias for :func:`torch.atan`. 
<a name="l6637"><span class="ln">6637 </span></a>    &quot;&quot;&quot;</span>
<a name="l6638"><span class="ln">6638 </span></a>
<a name="l6639"><span class="ln">6639 </span></a><span class="s2">def </span><span class="s1">arctan2</span><span class="s3">(</span>
<a name="l6640"><span class="ln">6640 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6641"><span class="ln">6641 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6642"><span class="ln">6642 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6643"><span class="ln">6643 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6644"><span class="ln">6644 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6645"><span class="ln">6645 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6646"><span class="ln">6646 </span></a>    arctan2(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6647"><span class="ln">6647 </span></a>    Alias for :func:`torch.atan2`. 
<a name="l6648"><span class="ln">6648 </span></a>    &quot;&quot;&quot;</span>
<a name="l6649"><span class="ln">6649 </span></a>
<a name="l6650"><span class="ln">6650 </span></a><span class="s2">def </span><span class="s1">arctan_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l6651"><span class="ln">6651 </span></a><span class="s2">def </span><span class="s1">arctanh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6652"><span class="ln">6652 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6653"><span class="ln">6653 </span></a>    arctanh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l6654"><span class="ln">6654 </span></a> 
<a name="l6655"><span class="ln">6655 </span></a>    Alias for :func:`torch.atanh`. 
<a name="l6656"><span class="ln">6656 </span></a>    &quot;&quot;&quot;</span>
<a name="l6657"><span class="ln">6657 </span></a>
<a name="l6658"><span class="ln">6658 </span></a><span class="s2">def </span><span class="s1">arctanh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l6659"><span class="ln">6659 </span></a><span class="s2">def </span><span class="s1">argmax</span><span class="s3">(</span>
<a name="l6660"><span class="ln">6660 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6661"><span class="ln">6661 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6662"><span class="ln">6662 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6663"><span class="ln">6663 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6664"><span class="ln">6664 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6665"><span class="ln">6665 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6666"><span class="ln">6666 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6667"><span class="ln">6667 </span></a>    argmax(input) -&gt; LongTensor 
<a name="l6668"><span class="ln">6668 </span></a> 
<a name="l6669"><span class="ln">6669 </span></a>    Returns the indices of the maximum value of all elements in the :attr:`input` tensor. 
<a name="l6670"><span class="ln">6670 </span></a> 
<a name="l6671"><span class="ln">6671 </span></a>    This is the second value returned by :meth:`torch.max`. See its 
<a name="l6672"><span class="ln">6672 </span></a>    documentation for the exact semantics of this method. 
<a name="l6673"><span class="ln">6673 </span></a> 
<a name="l6674"><span class="ln">6674 </span></a>    .. note:: If there are multiple maximal values then the indices of the first maximal value are returned. 
<a name="l6675"><span class="ln">6675 </span></a> 
<a name="l6676"><span class="ln">6676 </span></a>    Args: 
<a name="l6677"><span class="ln">6677 </span></a>        input (Tensor): the input tensor. 
<a name="l6678"><span class="ln">6678 </span></a> 
<a name="l6679"><span class="ln">6679 </span></a>    Example:: 
<a name="l6680"><span class="ln">6680 </span></a> 
<a name="l6681"><span class="ln">6681 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6682"><span class="ln">6682 </span></a>        &gt;&gt;&gt; a 
<a name="l6683"><span class="ln">6683 </span></a>        tensor([[ 1.3398,  0.2663, -0.2686,  0.2450], 
<a name="l6684"><span class="ln">6684 </span></a>                [-0.7401, -0.8805, -0.3402, -1.1936], 
<a name="l6685"><span class="ln">6685 </span></a>                [ 0.4907, -1.3948, -1.0691, -0.3132], 
<a name="l6686"><span class="ln">6686 </span></a>                [-1.6092,  0.5419, -0.2993,  0.3195]]) 
<a name="l6687"><span class="ln">6687 </span></a>        &gt;&gt;&gt; torch.argmax(a) 
<a name="l6688"><span class="ln">6688 </span></a>        tensor(0) 
<a name="l6689"><span class="ln">6689 </span></a> 
<a name="l6690"><span class="ln">6690 </span></a>    .. function:: argmax(input, dim, keepdim=False) -&gt; LongTensor 
<a name="l6691"><span class="ln">6691 </span></a>       :noindex: 
<a name="l6692"><span class="ln">6692 </span></a> 
<a name="l6693"><span class="ln">6693 </span></a>    Returns the indices of the maximum values of a tensor across a dimension. 
<a name="l6694"><span class="ln">6694 </span></a> 
<a name="l6695"><span class="ln">6695 </span></a>    This is the second value returned by :meth:`torch.max`. See its 
<a name="l6696"><span class="ln">6696 </span></a>    documentation for the exact semantics of this method. 
<a name="l6697"><span class="ln">6697 </span></a> 
<a name="l6698"><span class="ln">6698 </span></a>    Args: 
<a name="l6699"><span class="ln">6699 </span></a>        input (Tensor): the input tensor. 
<a name="l6700"><span class="ln">6700 </span></a> 
<a name="l6701"><span class="ln">6701 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l6702"><span class="ln">6702 </span></a>     If ``None``, the argmax of the flattened input is returned. 
<a name="l6703"><span class="ln">6703 </span></a> 
<a name="l6704"><span class="ln">6704 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l6705"><span class="ln">6705 </span></a> 
<a name="l6706"><span class="ln">6706 </span></a> 
<a name="l6707"><span class="ln">6707 </span></a>    Example:: 
<a name="l6708"><span class="ln">6708 </span></a> 
<a name="l6709"><span class="ln">6709 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6710"><span class="ln">6710 </span></a>        &gt;&gt;&gt; a 
<a name="l6711"><span class="ln">6711 </span></a>        tensor([[ 1.3398,  0.2663, -0.2686,  0.2450], 
<a name="l6712"><span class="ln">6712 </span></a>                [-0.7401, -0.8805, -0.3402, -1.1936], 
<a name="l6713"><span class="ln">6713 </span></a>                [ 0.4907, -1.3948, -1.0691, -0.3132], 
<a name="l6714"><span class="ln">6714 </span></a>                [-1.6092,  0.5419, -0.2993,  0.3195]]) 
<a name="l6715"><span class="ln">6715 </span></a>        &gt;&gt;&gt; torch.argmax(a, dim=1) 
<a name="l6716"><span class="ln">6716 </span></a>        tensor([ 0,  2,  0,  1]) 
<a name="l6717"><span class="ln">6717 </span></a>    &quot;&quot;&quot;</span>
<a name="l6718"><span class="ln">6718 </span></a>
<a name="l6719"><span class="ln">6719 </span></a><span class="s2">def </span><span class="s1">argmin</span><span class="s3">(</span>
<a name="l6720"><span class="ln">6720 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6721"><span class="ln">6721 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6722"><span class="ln">6722 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6723"><span class="ln">6723 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6724"><span class="ln">6724 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6725"><span class="ln">6725 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6726"><span class="ln">6726 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6727"><span class="ln">6727 </span></a>    argmin(input, dim=None, keepdim=False) -&gt; LongTensor 
<a name="l6728"><span class="ln">6728 </span></a> 
<a name="l6729"><span class="ln">6729 </span></a>    Returns the indices of the minimum value(s) of the flattened tensor or along a dimension 
<a name="l6730"><span class="ln">6730 </span></a> 
<a name="l6731"><span class="ln">6731 </span></a>    This is the second value returned by :meth:`torch.min`. See its 
<a name="l6732"><span class="ln">6732 </span></a>    documentation for the exact semantics of this method. 
<a name="l6733"><span class="ln">6733 </span></a> 
<a name="l6734"><span class="ln">6734 </span></a>    .. note:: If there are multiple minimal values then the indices of the first minimal value are returned. 
<a name="l6735"><span class="ln">6735 </span></a> 
<a name="l6736"><span class="ln">6736 </span></a>    Args: 
<a name="l6737"><span class="ln">6737 </span></a>        input (Tensor): the input tensor. 
<a name="l6738"><span class="ln">6738 </span></a> 
<a name="l6739"><span class="ln">6739 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l6740"><span class="ln">6740 </span></a>     If ``None``, the argmin of the flattened input is returned. 
<a name="l6741"><span class="ln">6741 </span></a> 
<a name="l6742"><span class="ln">6742 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l6743"><span class="ln">6743 </span></a> 
<a name="l6744"><span class="ln">6744 </span></a> 
<a name="l6745"><span class="ln">6745 </span></a>    Example:: 
<a name="l6746"><span class="ln">6746 </span></a> 
<a name="l6747"><span class="ln">6747 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6748"><span class="ln">6748 </span></a>        &gt;&gt;&gt; a 
<a name="l6749"><span class="ln">6749 </span></a>        tensor([[ 0.1139,  0.2254, -0.1381,  0.3687], 
<a name="l6750"><span class="ln">6750 </span></a>                [ 1.0100, -1.1975, -0.0102, -0.4732], 
<a name="l6751"><span class="ln">6751 </span></a>                [-0.9240,  0.1207, -0.7506, -1.0213], 
<a name="l6752"><span class="ln">6752 </span></a>                [ 1.7809, -1.2960,  0.9384,  0.1438]]) 
<a name="l6753"><span class="ln">6753 </span></a>        &gt;&gt;&gt; torch.argmin(a) 
<a name="l6754"><span class="ln">6754 </span></a>        tensor(13) 
<a name="l6755"><span class="ln">6755 </span></a>        &gt;&gt;&gt; torch.argmin(a, dim=1) 
<a name="l6756"><span class="ln">6756 </span></a>        tensor([ 2,  1,  3,  1]) 
<a name="l6757"><span class="ln">6757 </span></a>        &gt;&gt;&gt; torch.argmin(a, dim=1, keepdim=True) 
<a name="l6758"><span class="ln">6758 </span></a>        tensor([[2], 
<a name="l6759"><span class="ln">6759 </span></a>                [1], 
<a name="l6760"><span class="ln">6760 </span></a>                [3], 
<a name="l6761"><span class="ln">6761 </span></a>                [1]]) 
<a name="l6762"><span class="ln">6762 </span></a>    &quot;&quot;&quot;</span>
<a name="l6763"><span class="ln">6763 </span></a>
<a name="l6764"><span class="ln">6764 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6765"><span class="ln">6765 </span></a><span class="s2">def </span><span class="s1">argsort</span><span class="s3">(</span>
<a name="l6766"><span class="ln">6766 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6767"><span class="ln">6767 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6768"><span class="ln">6768 </span></a>    <span class="s1">stable</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l6769"><span class="ln">6769 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l6770"><span class="ln">6770 </span></a>    <span class="s1">descending</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6771"><span class="ln">6771 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6772"><span class="ln">6772 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6773"><span class="ln">6773 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6774"><span class="ln">6774 </span></a>    argsort(input, dim=-1, descending=False, stable=False) -&gt; Tensor 
<a name="l6775"><span class="ln">6775 </span></a> 
<a name="l6776"><span class="ln">6776 </span></a>    Returns the indices that sort a tensor along a given dimension in ascending 
<a name="l6777"><span class="ln">6777 </span></a>    order by value. 
<a name="l6778"><span class="ln">6778 </span></a> 
<a name="l6779"><span class="ln">6779 </span></a>    This is the second value returned by :meth:`torch.sort`.  See its documentation 
<a name="l6780"><span class="ln">6780 </span></a>    for the exact semantics of this method. 
<a name="l6781"><span class="ln">6781 </span></a> 
<a name="l6782"><span class="ln">6782 </span></a>    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l6783"><span class="ln">6783 </span></a>    the order of equivalent elements. If ``False``, the relative order of values 
<a name="l6784"><span class="ln">6784 </span></a>    which compare equal is not guaranteed. ``True`` is slower. 
<a name="l6785"><span class="ln">6785 </span></a> 
<a name="l6786"><span class="ln">6786 </span></a>    Args: 
<a name="l6787"><span class="ln">6787 </span></a>        input (Tensor): the input tensor. 
<a name="l6788"><span class="ln">6788 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l6789"><span class="ln">6789 </span></a>        descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l6790"><span class="ln">6790 </span></a>        stable (bool, optional): controls the relative order of equivalent elements 
<a name="l6791"><span class="ln">6791 </span></a> 
<a name="l6792"><span class="ln">6792 </span></a>    Example:: 
<a name="l6793"><span class="ln">6793 </span></a> 
<a name="l6794"><span class="ln">6794 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6795"><span class="ln">6795 </span></a>        &gt;&gt;&gt; a 
<a name="l6796"><span class="ln">6796 </span></a>        tensor([[ 0.0785,  1.5267, -0.8521,  0.4065], 
<a name="l6797"><span class="ln">6797 </span></a>                [ 0.1598,  0.0788, -0.0745, -1.2700], 
<a name="l6798"><span class="ln">6798 </span></a>                [ 1.2208,  1.0722, -0.7064,  1.2564], 
<a name="l6799"><span class="ln">6799 </span></a>                [ 0.0669, -0.2318, -0.8229, -0.9280]]) 
<a name="l6800"><span class="ln">6800 </span></a> 
<a name="l6801"><span class="ln">6801 </span></a> 
<a name="l6802"><span class="ln">6802 </span></a>        &gt;&gt;&gt; torch.argsort(a, dim=1) 
<a name="l6803"><span class="ln">6803 </span></a>        tensor([[2, 0, 3, 1], 
<a name="l6804"><span class="ln">6804 </span></a>                [3, 2, 1, 0], 
<a name="l6805"><span class="ln">6805 </span></a>                [2, 1, 0, 3], 
<a name="l6806"><span class="ln">6806 </span></a>                [3, 2, 1, 0]]) 
<a name="l6807"><span class="ln">6807 </span></a>    &quot;&quot;&quot;</span>
<a name="l6808"><span class="ln">6808 </span></a>
<a name="l6809"><span class="ln">6809 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6810"><span class="ln">6810 </span></a><span class="s2">def </span><span class="s1">argsort</span><span class="s3">(</span>
<a name="l6811"><span class="ln">6811 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6812"><span class="ln">6812 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l6813"><span class="ln">6813 </span></a>    <span class="s1">descending</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6814"><span class="ln">6814 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6815"><span class="ln">6815 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6816"><span class="ln">6816 </span></a>    argsort(input, dim=-1, descending=False, stable=False) -&gt; Tensor 
<a name="l6817"><span class="ln">6817 </span></a> 
<a name="l6818"><span class="ln">6818 </span></a>    Returns the indices that sort a tensor along a given dimension in ascending 
<a name="l6819"><span class="ln">6819 </span></a>    order by value. 
<a name="l6820"><span class="ln">6820 </span></a> 
<a name="l6821"><span class="ln">6821 </span></a>    This is the second value returned by :meth:`torch.sort`.  See its documentation 
<a name="l6822"><span class="ln">6822 </span></a>    for the exact semantics of this method. 
<a name="l6823"><span class="ln">6823 </span></a> 
<a name="l6824"><span class="ln">6824 </span></a>    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l6825"><span class="ln">6825 </span></a>    the order of equivalent elements. If ``False``, the relative order of values 
<a name="l6826"><span class="ln">6826 </span></a>    which compare equal is not guaranteed. ``True`` is slower. 
<a name="l6827"><span class="ln">6827 </span></a> 
<a name="l6828"><span class="ln">6828 </span></a>    Args: 
<a name="l6829"><span class="ln">6829 </span></a>        input (Tensor): the input tensor. 
<a name="l6830"><span class="ln">6830 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l6831"><span class="ln">6831 </span></a>        descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l6832"><span class="ln">6832 </span></a>        stable (bool, optional): controls the relative order of equivalent elements 
<a name="l6833"><span class="ln">6833 </span></a> 
<a name="l6834"><span class="ln">6834 </span></a>    Example:: 
<a name="l6835"><span class="ln">6835 </span></a> 
<a name="l6836"><span class="ln">6836 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6837"><span class="ln">6837 </span></a>        &gt;&gt;&gt; a 
<a name="l6838"><span class="ln">6838 </span></a>        tensor([[ 0.0785,  1.5267, -0.8521,  0.4065], 
<a name="l6839"><span class="ln">6839 </span></a>                [ 0.1598,  0.0788, -0.0745, -1.2700], 
<a name="l6840"><span class="ln">6840 </span></a>                [ 1.2208,  1.0722, -0.7064,  1.2564], 
<a name="l6841"><span class="ln">6841 </span></a>                [ 0.0669, -0.2318, -0.8229, -0.9280]]) 
<a name="l6842"><span class="ln">6842 </span></a> 
<a name="l6843"><span class="ln">6843 </span></a> 
<a name="l6844"><span class="ln">6844 </span></a>        &gt;&gt;&gt; torch.argsort(a, dim=1) 
<a name="l6845"><span class="ln">6845 </span></a>        tensor([[2, 0, 3, 1], 
<a name="l6846"><span class="ln">6846 </span></a>                [3, 2, 1, 0], 
<a name="l6847"><span class="ln">6847 </span></a>                [2, 1, 0, 3], 
<a name="l6848"><span class="ln">6848 </span></a>                [3, 2, 1, 0]]) 
<a name="l6849"><span class="ln">6849 </span></a>    &quot;&quot;&quot;</span>
<a name="l6850"><span class="ln">6850 </span></a>
<a name="l6851"><span class="ln">6851 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l6852"><span class="ln">6852 </span></a><span class="s2">def </span><span class="s1">argsort</span><span class="s3">(</span>
<a name="l6853"><span class="ln">6853 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6854"><span class="ln">6854 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l6855"><span class="ln">6855 </span></a>    <span class="s1">descending</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l6856"><span class="ln">6856 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6857"><span class="ln">6857 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6858"><span class="ln">6858 </span></a>    argsort(input, dim=-1, descending=False, stable=False) -&gt; Tensor 
<a name="l6859"><span class="ln">6859 </span></a> 
<a name="l6860"><span class="ln">6860 </span></a>    Returns the indices that sort a tensor along a given dimension in ascending 
<a name="l6861"><span class="ln">6861 </span></a>    order by value. 
<a name="l6862"><span class="ln">6862 </span></a> 
<a name="l6863"><span class="ln">6863 </span></a>    This is the second value returned by :meth:`torch.sort`.  See its documentation 
<a name="l6864"><span class="ln">6864 </span></a>    for the exact semantics of this method. 
<a name="l6865"><span class="ln">6865 </span></a> 
<a name="l6866"><span class="ln">6866 </span></a>    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l6867"><span class="ln">6867 </span></a>    the order of equivalent elements. If ``False``, the relative order of values 
<a name="l6868"><span class="ln">6868 </span></a>    which compare equal is not guaranteed. ``True`` is slower. 
<a name="l6869"><span class="ln">6869 </span></a> 
<a name="l6870"><span class="ln">6870 </span></a>    Args: 
<a name="l6871"><span class="ln">6871 </span></a>        input (Tensor): the input tensor. 
<a name="l6872"><span class="ln">6872 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l6873"><span class="ln">6873 </span></a>        descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l6874"><span class="ln">6874 </span></a>        stable (bool, optional): controls the relative order of equivalent elements 
<a name="l6875"><span class="ln">6875 </span></a> 
<a name="l6876"><span class="ln">6876 </span></a>    Example:: 
<a name="l6877"><span class="ln">6877 </span></a> 
<a name="l6878"><span class="ln">6878 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l6879"><span class="ln">6879 </span></a>        &gt;&gt;&gt; a 
<a name="l6880"><span class="ln">6880 </span></a>        tensor([[ 0.0785,  1.5267, -0.8521,  0.4065], 
<a name="l6881"><span class="ln">6881 </span></a>                [ 0.1598,  0.0788, -0.0745, -1.2700], 
<a name="l6882"><span class="ln">6882 </span></a>                [ 1.2208,  1.0722, -0.7064,  1.2564], 
<a name="l6883"><span class="ln">6883 </span></a>                [ 0.0669, -0.2318, -0.8229, -0.9280]]) 
<a name="l6884"><span class="ln">6884 </span></a> 
<a name="l6885"><span class="ln">6885 </span></a> 
<a name="l6886"><span class="ln">6886 </span></a>        &gt;&gt;&gt; torch.argsort(a, dim=1) 
<a name="l6887"><span class="ln">6887 </span></a>        tensor([[2, 0, 3, 1], 
<a name="l6888"><span class="ln">6888 </span></a>                [3, 2, 1, 0], 
<a name="l6889"><span class="ln">6889 </span></a>                [2, 1, 0, 3], 
<a name="l6890"><span class="ln">6890 </span></a>                [3, 2, 1, 0]]) 
<a name="l6891"><span class="ln">6891 </span></a>    &quot;&quot;&quot;</span>
<a name="l6892"><span class="ln">6892 </span></a>
<a name="l6893"><span class="ln">6893 </span></a><span class="s2">def </span><span class="s1">argwhere</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6894"><span class="ln">6894 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6895"><span class="ln">6895 </span></a>    argwhere(input) -&gt; Tensor 
<a name="l6896"><span class="ln">6896 </span></a> 
<a name="l6897"><span class="ln">6897 </span></a>    Returns a tensor containing the indices of all non-zero elements of 
<a name="l6898"><span class="ln">6898 </span></a>    :attr:`input`.  Each row in the result contains the indices of a non-zero 
<a name="l6899"><span class="ln">6899 </span></a>    element in :attr:`input`. The result is sorted lexicographically, with 
<a name="l6900"><span class="ln">6900 </span></a>    the last index changing the fastest (C-style). 
<a name="l6901"><span class="ln">6901 </span></a> 
<a name="l6902"><span class="ln">6902 </span></a>    If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor 
<a name="l6903"><span class="ln">6903 </span></a>    :attr:`out` is of size :math:`(z \times n)`, where :math:`z` is the total number of 
<a name="l6904"><span class="ln">6904 </span></a>    non-zero elements in the :attr:`input` tensor. 
<a name="l6905"><span class="ln">6905 </span></a> 
<a name="l6906"><span class="ln">6906 </span></a>    .. note:: 
<a name="l6907"><span class="ln">6907 </span></a>        This function is similar to NumPy's `argwhere`. 
<a name="l6908"><span class="ln">6908 </span></a> 
<a name="l6909"><span class="ln">6909 </span></a>        When :attr:`input` is on CUDA, this function causes host-device synchronization. 
<a name="l6910"><span class="ln">6910 </span></a> 
<a name="l6911"><span class="ln">6911 </span></a>    Args: 
<a name="l6912"><span class="ln">6912 </span></a>        {input} 
<a name="l6913"><span class="ln">6913 </span></a> 
<a name="l6914"><span class="ln">6914 </span></a>    Example:: 
<a name="l6915"><span class="ln">6915 </span></a> 
<a name="l6916"><span class="ln">6916 </span></a>        &gt;&gt;&gt; t = torch.tensor([1, 0, 1]) 
<a name="l6917"><span class="ln">6917 </span></a>        &gt;&gt;&gt; torch.argwhere(t) 
<a name="l6918"><span class="ln">6918 </span></a>        tensor([[0], 
<a name="l6919"><span class="ln">6919 </span></a>                [2]]) 
<a name="l6920"><span class="ln">6920 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 0, 1], [0, 1, 1]]) 
<a name="l6921"><span class="ln">6921 </span></a>        &gt;&gt;&gt; torch.argwhere(t) 
<a name="l6922"><span class="ln">6922 </span></a>        tensor([[0, 0], 
<a name="l6923"><span class="ln">6923 </span></a>                [0, 2], 
<a name="l6924"><span class="ln">6924 </span></a>                [1, 1], 
<a name="l6925"><span class="ln">6925 </span></a>                [1, 2]]) 
<a name="l6926"><span class="ln">6926 </span></a>    &quot;&quot;&quot;</span>
<a name="l6927"><span class="ln">6927 </span></a>
<a name="l6928"><span class="ln">6928 </span></a><span class="s2">def </span><span class="s1">as_strided</span><span class="s3">(</span>
<a name="l6929"><span class="ln">6929 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6930"><span class="ln">6930 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6931"><span class="ln">6931 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6932"><span class="ln">6932 </span></a>    <span class="s1">storage_offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6933"><span class="ln">6933 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6934"><span class="ln">6934 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6935"><span class="ln">6935 </span></a>    as_strided(input, size, stride, storage_offset=None) -&gt; Tensor 
<a name="l6936"><span class="ln">6936 </span></a> 
<a name="l6937"><span class="ln">6937 </span></a>    Create a view of an existing `torch.Tensor` :attr:`input` with specified 
<a name="l6938"><span class="ln">6938 </span></a>    :attr:`size`, :attr:`stride` and :attr:`storage_offset`. 
<a name="l6939"><span class="ln">6939 </span></a> 
<a name="l6940"><span class="ln">6940 </span></a>    .. warning:: 
<a name="l6941"><span class="ln">6941 </span></a>        Prefer using other view functions, like :meth:`torch.Tensor.view` or 
<a name="l6942"><span class="ln">6942 </span></a>        :meth:`torch.Tensor.expand`, to setting a view's strides manually with 
<a name="l6943"><span class="ln">6943 </span></a>        `as_strided`, as this function will throw an error on non-standard Pytorch 
<a name="l6944"><span class="ln">6944 </span></a>        backends (that do not have a concept of stride) and the result will depend 
<a name="l6945"><span class="ln">6945 </span></a>        on the current layout in memory. The constructed view must only refer to 
<a name="l6946"><span class="ln">6946 </span></a>        elements within the Tensor's storage or a runtime error will be thrown. 
<a name="l6947"><span class="ln">6947 </span></a>        If the generated view is &quot;overlapped&quot; (with multiple indices referring to 
<a name="l6948"><span class="ln">6948 </span></a>        the same element in memory), the behavior of inplace operations on this view 
<a name="l6949"><span class="ln">6949 </span></a>        is undefined (and might not throw runtime errors). 
<a name="l6950"><span class="ln">6950 </span></a> 
<a name="l6951"><span class="ln">6951 </span></a>    Args: 
<a name="l6952"><span class="ln">6952 </span></a>        input (Tensor): the input tensor. 
<a name="l6953"><span class="ln">6953 </span></a>        size (tuple or ints): the shape of the output tensor 
<a name="l6954"><span class="ln">6954 </span></a>        stride (tuple or ints): the stride of the output tensor 
<a name="l6955"><span class="ln">6955 </span></a>        storage_offset (int, optional): the offset in the underlying storage of the output tensor. 
<a name="l6956"><span class="ln">6956 </span></a>            If ``None``, the storage_offset of the output tensor will match the input tensor. 
<a name="l6957"><span class="ln">6957 </span></a> 
<a name="l6958"><span class="ln">6958 </span></a>    Example:: 
<a name="l6959"><span class="ln">6959 </span></a> 
<a name="l6960"><span class="ln">6960 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 3) 
<a name="l6961"><span class="ln">6961 </span></a>        &gt;&gt;&gt; x 
<a name="l6962"><span class="ln">6962 </span></a>        tensor([[ 0.9039,  0.6291,  1.0795], 
<a name="l6963"><span class="ln">6963 </span></a>                [ 0.1586,  2.1939, -0.4900], 
<a name="l6964"><span class="ln">6964 </span></a>                [-0.1909, -0.7503,  1.9355]]) 
<a name="l6965"><span class="ln">6965 </span></a>        &gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2)) 
<a name="l6966"><span class="ln">6966 </span></a>        &gt;&gt;&gt; t 
<a name="l6967"><span class="ln">6967 </span></a>        tensor([[0.9039, 1.0795], 
<a name="l6968"><span class="ln">6968 </span></a>                [0.6291, 0.1586]]) 
<a name="l6969"><span class="ln">6969 </span></a>        &gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2), 1) 
<a name="l6970"><span class="ln">6970 </span></a>        tensor([[0.6291, 0.1586], 
<a name="l6971"><span class="ln">6971 </span></a>                [1.0795, 2.1939]]) 
<a name="l6972"><span class="ln">6972 </span></a>    &quot;&quot;&quot;</span>
<a name="l6973"><span class="ln">6973 </span></a>
<a name="l6974"><span class="ln">6974 </span></a><span class="s2">def </span><span class="s1">as_strided_</span><span class="s3">(</span>
<a name="l6975"><span class="ln">6975 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6976"><span class="ln">6976 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6977"><span class="ln">6977 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6978"><span class="ln">6978 </span></a>    <span class="s1">storage_offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6979"><span class="ln">6979 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l6980"><span class="ln">6980 </span></a><span class="s2">def </span><span class="s1">as_strided_copy</span><span class="s3">(</span>
<a name="l6981"><span class="ln">6981 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6982"><span class="ln">6982 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6983"><span class="ln">6983 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6984"><span class="ln">6984 </span></a>    <span class="s1">storage_offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6985"><span class="ln">6985 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l6986"><span class="ln">6986 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6987"><span class="ln">6987 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l6988"><span class="ln">6988 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l6989"><span class="ln">6989 </span></a>    Performs the same operation as :func:`torch.as_strided`, but all output tensors 
<a name="l6990"><span class="ln">6990 </span></a>    are freshly created instead of aliasing the input. 
<a name="l6991"><span class="ln">6991 </span></a>    &quot;&quot;&quot;</span>
<a name="l6992"><span class="ln">6992 </span></a>
<a name="l6993"><span class="ln">6993 </span></a><span class="s2">def </span><span class="s1">as_strided_scatter</span><span class="s3">(</span>
<a name="l6994"><span class="ln">6994 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6995"><span class="ln">6995 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l6996"><span class="ln">6996 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6997"><span class="ln">6997 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l6998"><span class="ln">6998 </span></a>    <span class="s1">storage_offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l6999"><span class="ln">6999 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7000"><span class="ln">7000 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7001"><span class="ln">7001 </span></a>    as_strided_scatter(input, src, size, stride, storage_offset=None) -&gt; Tensor 
<a name="l7002"><span class="ln">7002 </span></a> 
<a name="l7003"><span class="ln">7003 </span></a>    Embeds the values of the :attr:`src` tensor into :attr:`input` along 
<a name="l7004"><span class="ln">7004 </span></a>    the elements corresponding to the result of calling 
<a name="l7005"><span class="ln">7005 </span></a>    input.as_strided(size, stride, storage_offset). 
<a name="l7006"><span class="ln">7006 </span></a> 
<a name="l7007"><span class="ln">7007 </span></a>    This function returns a tensor with fresh storage; it does not 
<a name="l7008"><span class="ln">7008 </span></a>    return a view. 
<a name="l7009"><span class="ln">7009 </span></a> 
<a name="l7010"><span class="ln">7010 </span></a>    Args: 
<a name="l7011"><span class="ln">7011 </span></a>        input (Tensor): the input tensor. 
<a name="l7012"><span class="ln">7012 </span></a>        size (tuple or ints): the shape of the output tensor 
<a name="l7013"><span class="ln">7013 </span></a>        stride (tuple or ints): the stride of the output tensor 
<a name="l7014"><span class="ln">7014 </span></a>        storage_offset (int, optional): the offset in the underlying storage of the output tensor 
<a name="l7015"><span class="ln">7015 </span></a> 
<a name="l7016"><span class="ln">7016 </span></a>    .. note:: 
<a name="l7017"><span class="ln">7017 </span></a> 
<a name="l7018"><span class="ln">7018 </span></a>        :attr:`src` must be of the proper size in order to be embedded 
<a name="l7019"><span class="ln">7019 </span></a>        into :attr:`input`. Specifically, it should have the same shape as 
<a name="l7020"><span class="ln">7020 </span></a>        `torch.as_strided(input, size, stride, storage_offset)` 
<a name="l7021"><span class="ln">7021 </span></a> 
<a name="l7022"><span class="ln">7022 </span></a>    Example:: 
<a name="l7023"><span class="ln">7023 </span></a> 
<a name="l7024"><span class="ln">7024 </span></a>        &gt;&gt;&gt; a = torch.arange(4).reshape(2, 2) + 1 
<a name="l7025"><span class="ln">7025 </span></a>        &gt;&gt;&gt; a 
<a name="l7026"><span class="ln">7026 </span></a>        tensor([[1, 2], 
<a name="l7027"><span class="ln">7027 </span></a>                [3, 4]]) 
<a name="l7028"><span class="ln">7028 </span></a>        &gt;&gt;&gt; b = torch.zeros(3, 3) 
<a name="l7029"><span class="ln">7029 </span></a>        &gt;&gt;&gt; b 
<a name="l7030"><span class="ln">7030 </span></a>        tensor([[0., 0., 0.], 
<a name="l7031"><span class="ln">7031 </span></a>                [0., 0., 0.], 
<a name="l7032"><span class="ln">7032 </span></a>                [0., 0., 0.]]) 
<a name="l7033"><span class="ln">7033 </span></a>        &gt;&gt;&gt; torch.as_strided_scatter(b, a, (2, 2), (1, 2)) 
<a name="l7034"><span class="ln">7034 </span></a>        tensor([[1., 3., 2.], 
<a name="l7035"><span class="ln">7035 </span></a>                [4., 0., 0.], 
<a name="l7036"><span class="ln">7036 </span></a>                [0., 0., 0.]]) 
<a name="l7037"><span class="ln">7037 </span></a>    &quot;&quot;&quot;</span>
<a name="l7038"><span class="ln">7038 </span></a>
<a name="l7039"><span class="ln">7039 </span></a><span class="s2">def </span><span class="s1">as_tensor</span><span class="s3">(</span>
<a name="l7040"><span class="ln">7040 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Any</span><span class="s3">,</span>
<a name="l7041"><span class="ln">7041 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7042"><span class="ln">7042 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7043"><span class="ln">7043 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7044"><span class="ln">7044 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7045"><span class="ln">7045 </span></a>    as_tensor(data: Any, dtype: Optional[dtype] = None, device: Optional[DeviceLikeType]) -&gt; Tensor 
<a name="l7046"><span class="ln">7046 </span></a> 
<a name="l7047"><span class="ln">7047 </span></a>    Converts :attr:`data` into a tensor, sharing data and preserving autograd 
<a name="l7048"><span class="ln">7048 </span></a>    history if possible. 
<a name="l7049"><span class="ln">7049 </span></a> 
<a name="l7050"><span class="ln">7050 </span></a>    If :attr:`data` is already a tensor with the requested dtype and device 
<a name="l7051"><span class="ln">7051 </span></a>    then :attr:`data` itself is returned, but if :attr:`data` is a 
<a name="l7052"><span class="ln">7052 </span></a>    tensor with a different dtype or device then it's copied as if using 
<a name="l7053"><span class="ln">7053 </span></a>    `data.to(dtype=dtype, device=device)`. 
<a name="l7054"><span class="ln">7054 </span></a> 
<a name="l7055"><span class="ln">7055 </span></a>    If :attr:`data` is a NumPy array (an ndarray) with the same dtype and device then a 
<a name="l7056"><span class="ln">7056 </span></a>    tensor is constructed using :func:`torch.from_numpy`. 
<a name="l7057"><span class="ln">7057 </span></a> 
<a name="l7058"><span class="ln">7058 </span></a>    If :attr:`data` is a CuPy array, the returned tensor will be located on the same device as the CuPy array unless 
<a name="l7059"><span class="ln">7059 </span></a>    specifically overwritten by :attr:`device` or a default device. 
<a name="l7060"><span class="ln">7060 </span></a> 
<a name="l7061"><span class="ln">7061 </span></a>    .. seealso:: 
<a name="l7062"><span class="ln">7062 </span></a> 
<a name="l7063"><span class="ln">7063 </span></a>        :func:`torch.tensor` never shares its data and creates a new &quot;leaf tensor&quot; (see :doc:`/notes/autograd`). 
<a name="l7064"><span class="ln">7064 </span></a> 
<a name="l7065"><span class="ln">7065 </span></a> 
<a name="l7066"><span class="ln">7066 </span></a>    Args: 
<a name="l7067"><span class="ln">7067 </span></a>        data (array_like): Initial data for the tensor. Can be a list, tuple, 
<a name="l7068"><span class="ln">7068 </span></a>            NumPy ``ndarray``, scalar, and other types. 
<a name="l7069"><span class="ln">7069 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l7070"><span class="ln">7070 </span></a>            Default: if ``None``, infers data type from :attr:`data`. 
<a name="l7071"><span class="ln">7071 </span></a>        device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor 
<a name="l7072"><span class="ln">7072 </span></a>            then the device of data is used. If None and data is not a tensor then 
<a name="l7073"><span class="ln">7073 </span></a>            the result tensor is constructed on the current device. 
<a name="l7074"><span class="ln">7074 </span></a> 
<a name="l7075"><span class="ln">7075 </span></a> 
<a name="l7076"><span class="ln">7076 </span></a>    Example:: 
<a name="l7077"><span class="ln">7077 </span></a> 
<a name="l7078"><span class="ln">7078 </span></a>        &gt;&gt;&gt; a = numpy.array([1, 2, 3]) 
<a name="l7079"><span class="ln">7079 </span></a>        &gt;&gt;&gt; t = torch.as_tensor(a) 
<a name="l7080"><span class="ln">7080 </span></a>        &gt;&gt;&gt; t 
<a name="l7081"><span class="ln">7081 </span></a>        tensor([ 1,  2,  3]) 
<a name="l7082"><span class="ln">7082 </span></a>        &gt;&gt;&gt; t[0] = -1 
<a name="l7083"><span class="ln">7083 </span></a>        &gt;&gt;&gt; a 
<a name="l7084"><span class="ln">7084 </span></a>        array([-1,  2,  3]) 
<a name="l7085"><span class="ln">7085 </span></a> 
<a name="l7086"><span class="ln">7086 </span></a>        &gt;&gt;&gt; a = numpy.array([1, 2, 3]) 
<a name="l7087"><span class="ln">7087 </span></a>        &gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda')) 
<a name="l7088"><span class="ln">7088 </span></a>        &gt;&gt;&gt; t 
<a name="l7089"><span class="ln">7089 </span></a>        tensor([ 1,  2,  3]) 
<a name="l7090"><span class="ln">7090 </span></a>        &gt;&gt;&gt; t[0] = -1 
<a name="l7091"><span class="ln">7091 </span></a>        &gt;&gt;&gt; a 
<a name="l7092"><span class="ln">7092 </span></a>        array([1,  2,  3]) 
<a name="l7093"><span class="ln">7093 </span></a>    &quot;&quot;&quot;</span>
<a name="l7094"><span class="ln">7094 </span></a>
<a name="l7095"><span class="ln">7095 </span></a><span class="s2">def </span><span class="s1">asarray</span><span class="s3">(</span>
<a name="l7096"><span class="ln">7096 </span></a>    <span class="s1">obj</span><span class="s2">: </span><span class="s1">Any</span><span class="s3">,</span>
<a name="l7097"><span class="ln">7097 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7098"><span class="ln">7098 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7099"><span class="ln">7099 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7100"><span class="ln">7100 </span></a>    <span class="s1">copy</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7101"><span class="ln">7101 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l7102"><span class="ln">7102 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7103"><span class="ln">7103 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7104"><span class="ln">7104 </span></a>    asarray(obj: Any, *, dtype: Optional[dtype], device: Optional[DeviceLikeType], copy: Optional[bool] = None, requires_grad: bool = False) -&gt; Tensor # noqa: B950 
<a name="l7105"><span class="ln">7105 </span></a> 
<a name="l7106"><span class="ln">7106 </span></a>    Converts :attr:`obj` to a tensor. 
<a name="l7107"><span class="ln">7107 </span></a> 
<a name="l7108"><span class="ln">7108 </span></a>    :attr:`obj` can be one of: 
<a name="l7109"><span class="ln">7109 </span></a> 
<a name="l7110"><span class="ln">7110 </span></a>    1. a tensor 
<a name="l7111"><span class="ln">7111 </span></a>    2. a NumPy array or a NumPy scalar 
<a name="l7112"><span class="ln">7112 </span></a>    3. a DLPack capsule 
<a name="l7113"><span class="ln">7113 </span></a>    4. an object that implements Python's buffer protocol 
<a name="l7114"><span class="ln">7114 </span></a>    5. a scalar 
<a name="l7115"><span class="ln">7115 </span></a>    6. a sequence of scalars 
<a name="l7116"><span class="ln">7116 </span></a> 
<a name="l7117"><span class="ln">7117 </span></a>    When :attr:`obj` is a tensor, NumPy array, or DLPack capsule the returned tensor will, 
<a name="l7118"><span class="ln">7118 </span></a>    by default, not require a gradient, have the same datatype as :attr:`obj`, be on the 
<a name="l7119"><span class="ln">7119 </span></a>    same device, and share memory with it. These properties can be controlled with the 
<a name="l7120"><span class="ln">7120 </span></a>    :attr:`dtype`, :attr:`device`, :attr:`copy`, and :attr:`requires_grad` keyword arguments. 
<a name="l7121"><span class="ln">7121 </span></a>    If the returned tensor is of a different datatype, on a different device, or a copy is 
<a name="l7122"><span class="ln">7122 </span></a>    requested then it will not share its memory with :attr:`obj`. If :attr:`requires_grad` 
<a name="l7123"><span class="ln">7123 </span></a>    is ``True`` then the returned tensor will require a gradient, and if :attr:`obj` is 
<a name="l7124"><span class="ln">7124 </span></a>    also a tensor with an autograd history then the returned tensor will have the same history. 
<a name="l7125"><span class="ln">7125 </span></a> 
<a name="l7126"><span class="ln">7126 </span></a>    When :attr:`obj` is not a tensor, NumPy array, or DLPack capsule but implements Python's 
<a name="l7127"><span class="ln">7127 </span></a>    buffer protocol then the buffer is interpreted as an array of bytes grouped according to 
<a name="l7128"><span class="ln">7128 </span></a>    the size of the datatype passed to the :attr:`dtype` keyword argument. (If no datatype is 
<a name="l7129"><span class="ln">7129 </span></a>    passed then the default floating point datatype is used, instead.) The returned tensor 
<a name="l7130"><span class="ln">7130 </span></a>    will have the specified datatype (or default floating point datatype if none is specified) 
<a name="l7131"><span class="ln">7131 </span></a>    and, by default, be on the CPU device and share memory with the buffer. 
<a name="l7132"><span class="ln">7132 </span></a> 
<a name="l7133"><span class="ln">7133 </span></a>    When :attr:`obj` is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on 
<a name="l7134"><span class="ln">7134 </span></a>    the CPU and that doesn't share its memory (i.e. ``copy=True``). By default datatype will 
<a name="l7135"><span class="ln">7135 </span></a>    be the PyTorch datatype corresponding to the NumPy's scalar's datatype. 
<a name="l7136"><span class="ln">7136 </span></a> 
<a name="l7137"><span class="ln">7137 </span></a>    When :attr:`obj` is none of the above but a scalar, or a sequence of scalars then the 
<a name="l7138"><span class="ln">7138 </span></a>    returned tensor will, by default, infer its datatype from the scalar values, be on the 
<a name="l7139"><span class="ln">7139 </span></a>    current default device, and not share its memory. 
<a name="l7140"><span class="ln">7140 </span></a> 
<a name="l7141"><span class="ln">7141 </span></a>    .. seealso:: 
<a name="l7142"><span class="ln">7142 </span></a> 
<a name="l7143"><span class="ln">7143 </span></a>        :func:`torch.tensor` creates a tensor that always copies the data from the input object. 
<a name="l7144"><span class="ln">7144 </span></a>        :func:`torch.from_numpy` creates a tensor that always shares memory from NumPy arrays. 
<a name="l7145"><span class="ln">7145 </span></a>        :func:`torch.frombuffer` creates a tensor that always shares memory from objects that 
<a name="l7146"><span class="ln">7146 </span></a>        implement the buffer protocol. 
<a name="l7147"><span class="ln">7147 </span></a>        :func:`torch.from_dlpack` creates a tensor that always shares memory from 
<a name="l7148"><span class="ln">7148 </span></a>        DLPack capsules. 
<a name="l7149"><span class="ln">7149 </span></a> 
<a name="l7150"><span class="ln">7150 </span></a>    Args: 
<a name="l7151"><span class="ln">7151 </span></a>        obj (object): a tensor, NumPy array, DLPack Capsule, object that implements Python's 
<a name="l7152"><span class="ln">7152 </span></a>               buffer protocol, scalar, or sequence of scalars. 
<a name="l7153"><span class="ln">7153 </span></a> 
<a name="l7154"><span class="ln">7154 </span></a>    Keyword args: 
<a name="l7155"><span class="ln">7155 </span></a>        dtype (:class:`torch.dtype`, optional): the datatype of the returned tensor. 
<a name="l7156"><span class="ln">7156 </span></a>               Default: ``None``, which causes the datatype of the returned tensor to be 
<a name="l7157"><span class="ln">7157 </span></a>               inferred from :attr:`obj`. 
<a name="l7158"><span class="ln">7158 </span></a>        copy (bool, optional): controls whether the returned tensor shares memory with :attr:`obj`. 
<a name="l7159"><span class="ln">7159 </span></a>               Default: ``None``, which causes the returned tensor to share memory with :attr:`obj` 
<a name="l7160"><span class="ln">7160 </span></a>               whenever possible. If ``True`` then the returned tensor does not share its memory. 
<a name="l7161"><span class="ln">7161 </span></a>               If ``False`` then the returned tensor shares its memory with :attr:`obj` and an 
<a name="l7162"><span class="ln">7162 </span></a>               error is thrown if it cannot. 
<a name="l7163"><span class="ln">7163 </span></a>        device (:class:`torch.device`, optional): the device of the returned tensor. 
<a name="l7164"><span class="ln">7164 </span></a>               Default: ``None``, which causes the device of :attr:`obj` to be used. Or, if 
<a name="l7165"><span class="ln">7165 </span></a>               :attr:`obj` is a Python sequence, the current default device will be used. 
<a name="l7166"><span class="ln">7166 </span></a>        requires_grad (bool, optional): whether the returned tensor requires grad. 
<a name="l7167"><span class="ln">7167 </span></a>               Default: ``False``, which causes the returned tensor not to require a gradient. 
<a name="l7168"><span class="ln">7168 </span></a>               If ``True``, then the returned tensor will require a gradient, and if :attr:`obj` 
<a name="l7169"><span class="ln">7169 </span></a>               is also a tensor with an autograd history then the returned tensor will have 
<a name="l7170"><span class="ln">7170 </span></a>               the same history. 
<a name="l7171"><span class="ln">7171 </span></a> 
<a name="l7172"><span class="ln">7172 </span></a>    Example:: 
<a name="l7173"><span class="ln">7173 </span></a> 
<a name="l7174"><span class="ln">7174 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l7175"><span class="ln">7175 </span></a>        &gt;&gt;&gt; # Shares memory with tensor 'a' 
<a name="l7176"><span class="ln">7176 </span></a>        &gt;&gt;&gt; b = torch.asarray(a) 
<a name="l7177"><span class="ln">7177 </span></a>        &gt;&gt;&gt; a.data_ptr() == b.data_ptr() 
<a name="l7178"><span class="ln">7178 </span></a>        True 
<a name="l7179"><span class="ln">7179 </span></a>        &gt;&gt;&gt; # Forces memory copy 
<a name="l7180"><span class="ln">7180 </span></a>        &gt;&gt;&gt; c = torch.asarray(a, copy=True) 
<a name="l7181"><span class="ln">7181 </span></a>        &gt;&gt;&gt; a.data_ptr() == c.data_ptr() 
<a name="l7182"><span class="ln">7182 </span></a>        False 
<a name="l7183"><span class="ln">7183 </span></a> 
<a name="l7184"><span class="ln">7184 </span></a>        &gt;&gt;&gt; a = torch.tensor([1., 2., 3.], requires_grad=True) 
<a name="l7185"><span class="ln">7185 </span></a>        &gt;&gt;&gt; b = a + 2 
<a name="l7186"><span class="ln">7186 </span></a>        &gt;&gt;&gt; b 
<a name="l7187"><span class="ln">7187 </span></a>        tensor([3., 4., 5.], grad_fn=&lt;AddBackward0&gt;) 
<a name="l7188"><span class="ln">7188 </span></a>        &gt;&gt;&gt; # Shares memory with tensor 'b', with no grad 
<a name="l7189"><span class="ln">7189 </span></a>        &gt;&gt;&gt; c = torch.asarray(b) 
<a name="l7190"><span class="ln">7190 </span></a>        &gt;&gt;&gt; c 
<a name="l7191"><span class="ln">7191 </span></a>        tensor([3., 4., 5.]) 
<a name="l7192"><span class="ln">7192 </span></a>        &gt;&gt;&gt; # Shares memory with tensor 'b', retaining autograd history 
<a name="l7193"><span class="ln">7193 </span></a>        &gt;&gt;&gt; d = torch.asarray(b, requires_grad=True) 
<a name="l7194"><span class="ln">7194 </span></a>        &gt;&gt;&gt; d 
<a name="l7195"><span class="ln">7195 </span></a>        tensor([3., 4., 5.], grad_fn=&lt;AddBackward0&gt;) 
<a name="l7196"><span class="ln">7196 </span></a> 
<a name="l7197"><span class="ln">7197 </span></a>        &gt;&gt;&gt; array = numpy.array([1, 2, 3]) 
<a name="l7198"><span class="ln">7198 </span></a>        &gt;&gt;&gt; # Shares memory with array 'array' 
<a name="l7199"><span class="ln">7199 </span></a>        &gt;&gt;&gt; t1 = torch.asarray(array) 
<a name="l7200"><span class="ln">7200 </span></a>        &gt;&gt;&gt; array.__array_interface__['data'][0] == t1.data_ptr() 
<a name="l7201"><span class="ln">7201 </span></a>        True 
<a name="l7202"><span class="ln">7202 </span></a>        &gt;&gt;&gt; # Copies memory due to dtype mismatch 
<a name="l7203"><span class="ln">7203 </span></a>        &gt;&gt;&gt; t2 = torch.asarray(array, dtype=torch.float32) 
<a name="l7204"><span class="ln">7204 </span></a>        &gt;&gt;&gt; array.__array_interface__['data'][0] == t2.data_ptr() 
<a name="l7205"><span class="ln">7205 </span></a>        False 
<a name="l7206"><span class="ln">7206 </span></a> 
<a name="l7207"><span class="ln">7207 </span></a>        &gt;&gt;&gt; scalar = numpy.float64(0.5) 
<a name="l7208"><span class="ln">7208 </span></a>        &gt;&gt;&gt; torch.asarray(scalar) 
<a name="l7209"><span class="ln">7209 </span></a>        tensor(0.5000, dtype=torch.float64) 
<a name="l7210"><span class="ln">7210 </span></a>    &quot;&quot;&quot;</span>
<a name="l7211"><span class="ln">7211 </span></a>
<a name="l7212"><span class="ln">7212 </span></a><span class="s2">def </span><span class="s1">asin</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7213"><span class="ln">7213 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7214"><span class="ln">7214 </span></a>    asin(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l7215"><span class="ln">7215 </span></a> 
<a name="l7216"><span class="ln">7216 </span></a>    Returns a new tensor with the arcsine of the elements of :attr:`input`. 
<a name="l7217"><span class="ln">7217 </span></a> 
<a name="l7218"><span class="ln">7218 </span></a>    .. math:: 
<a name="l7219"><span class="ln">7219 </span></a>        \text{out}_{i} = \sin^{-1}(\text{input}_{i}) 
<a name="l7220"><span class="ln">7220 </span></a> 
<a name="l7221"><span class="ln">7221 </span></a>    Args: 
<a name="l7222"><span class="ln">7222 </span></a>        input (Tensor): the input tensor. 
<a name="l7223"><span class="ln">7223 </span></a> 
<a name="l7224"><span class="ln">7224 </span></a>    Keyword args: 
<a name="l7225"><span class="ln">7225 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7226"><span class="ln">7226 </span></a> 
<a name="l7227"><span class="ln">7227 </span></a>    Example:: 
<a name="l7228"><span class="ln">7228 </span></a> 
<a name="l7229"><span class="ln">7229 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l7230"><span class="ln">7230 </span></a>        &gt;&gt;&gt; a 
<a name="l7231"><span class="ln">7231 </span></a>        tensor([-0.5962,  1.4985, -0.4396,  1.4525]) 
<a name="l7232"><span class="ln">7232 </span></a>        &gt;&gt;&gt; torch.asin(a) 
<a name="l7233"><span class="ln">7233 </span></a>        tensor([-0.6387,     nan, -0.4552,     nan]) 
<a name="l7234"><span class="ln">7234 </span></a>    &quot;&quot;&quot;</span>
<a name="l7235"><span class="ln">7235 </span></a>
<a name="l7236"><span class="ln">7236 </span></a><span class="s2">def </span><span class="s1">asin_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7237"><span class="ln">7237 </span></a><span class="s2">def </span><span class="s1">asinh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7238"><span class="ln">7238 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7239"><span class="ln">7239 </span></a>    asinh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l7240"><span class="ln">7240 </span></a> 
<a name="l7241"><span class="ln">7241 </span></a>    Returns a new tensor with the inverse hyperbolic sine of the elements of :attr:`input`. 
<a name="l7242"><span class="ln">7242 </span></a> 
<a name="l7243"><span class="ln">7243 </span></a>    .. math:: 
<a name="l7244"><span class="ln">7244 </span></a>        \text{out}_{i} = \sinh^{-1}(\text{input}_{i}) 
<a name="l7245"><span class="ln">7245 </span></a> 
<a name="l7246"><span class="ln">7246 </span></a>    Args: 
<a name="l7247"><span class="ln">7247 </span></a>        input (Tensor): the input tensor. 
<a name="l7248"><span class="ln">7248 </span></a> 
<a name="l7249"><span class="ln">7249 </span></a>    Keyword arguments: 
<a name="l7250"><span class="ln">7250 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7251"><span class="ln">7251 </span></a> 
<a name="l7252"><span class="ln">7252 </span></a>    Example:: 
<a name="l7253"><span class="ln">7253 </span></a> 
<a name="l7254"><span class="ln">7254 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l7255"><span class="ln">7255 </span></a>        &gt;&gt;&gt; a 
<a name="l7256"><span class="ln">7256 </span></a>        tensor([ 0.1606, -1.4267, -1.0899, -1.0250 ]) 
<a name="l7257"><span class="ln">7257 </span></a>        &gt;&gt;&gt; torch.asinh(a) 
<a name="l7258"><span class="ln">7258 </span></a>        tensor([ 0.1599, -1.1534, -0.9435, -0.8990 ]) 
<a name="l7259"><span class="ln">7259 </span></a>    &quot;&quot;&quot;</span>
<a name="l7260"><span class="ln">7260 </span></a>
<a name="l7261"><span class="ln">7261 </span></a><span class="s2">def </span><span class="s1">asinh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7262"><span class="ln">7262 </span></a><span class="s2">def </span><span class="s1">atan</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7263"><span class="ln">7263 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7264"><span class="ln">7264 </span></a>    atan(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l7265"><span class="ln">7265 </span></a> 
<a name="l7266"><span class="ln">7266 </span></a>    Returns a new tensor with the arctangent of the elements of :attr:`input`. 
<a name="l7267"><span class="ln">7267 </span></a> 
<a name="l7268"><span class="ln">7268 </span></a>    .. math:: 
<a name="l7269"><span class="ln">7269 </span></a>        \text{out}_{i} = \tan^{-1}(\text{input}_{i}) 
<a name="l7270"><span class="ln">7270 </span></a> 
<a name="l7271"><span class="ln">7271 </span></a>    Args: 
<a name="l7272"><span class="ln">7272 </span></a>        input (Tensor): the input tensor. 
<a name="l7273"><span class="ln">7273 </span></a> 
<a name="l7274"><span class="ln">7274 </span></a>    Keyword args: 
<a name="l7275"><span class="ln">7275 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7276"><span class="ln">7276 </span></a> 
<a name="l7277"><span class="ln">7277 </span></a>    Example:: 
<a name="l7278"><span class="ln">7278 </span></a> 
<a name="l7279"><span class="ln">7279 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l7280"><span class="ln">7280 </span></a>        &gt;&gt;&gt; a 
<a name="l7281"><span class="ln">7281 </span></a>        tensor([ 0.2341,  0.2539, -0.6256, -0.6448]) 
<a name="l7282"><span class="ln">7282 </span></a>        &gt;&gt;&gt; torch.atan(a) 
<a name="l7283"><span class="ln">7283 </span></a>        tensor([ 0.2299,  0.2487, -0.5591, -0.5727]) 
<a name="l7284"><span class="ln">7284 </span></a>    &quot;&quot;&quot;</span>
<a name="l7285"><span class="ln">7285 </span></a>
<a name="l7286"><span class="ln">7286 </span></a><span class="s2">def </span><span class="s1">atan2</span><span class="s3">(</span>
<a name="l7287"><span class="ln">7287 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7288"><span class="ln">7288 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7289"><span class="ln">7289 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7290"><span class="ln">7290 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7291"><span class="ln">7291 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7292"><span class="ln">7292 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7293"><span class="ln">7293 </span></a>    atan2(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l7294"><span class="ln">7294 </span></a> 
<a name="l7295"><span class="ln">7295 </span></a>    Element-wise arctangent of :math:`\text{input}_{i} / \text{other}_{i}` 
<a name="l7296"><span class="ln">7296 </span></a>    with consideration of the quadrant. Returns a new tensor with the signed angles 
<a name="l7297"><span class="ln">7297 </span></a>    in radians between vector :math:`(\text{other}_{i}, \text{input}_{i})` 
<a name="l7298"><span class="ln">7298 </span></a>    and vector :math:`(1, 0)`. (Note that :math:`\text{other}_{i}`, the second 
<a name="l7299"><span class="ln">7299 </span></a>    parameter, is the x-coordinate, while :math:`\text{input}_{i}`, the first 
<a name="l7300"><span class="ln">7300 </span></a>    parameter, is the y-coordinate.) 
<a name="l7301"><span class="ln">7301 </span></a> 
<a name="l7302"><span class="ln">7302 </span></a>    The shapes of ``input`` and ``other`` must be 
<a name="l7303"><span class="ln">7303 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l7304"><span class="ln">7304 </span></a> 
<a name="l7305"><span class="ln">7305 </span></a>    Args: 
<a name="l7306"><span class="ln">7306 </span></a>        input (Tensor): the first input tensor 
<a name="l7307"><span class="ln">7307 </span></a>        other (Tensor): the second input tensor 
<a name="l7308"><span class="ln">7308 </span></a> 
<a name="l7309"><span class="ln">7309 </span></a>    Keyword args: 
<a name="l7310"><span class="ln">7310 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7311"><span class="ln">7311 </span></a> 
<a name="l7312"><span class="ln">7312 </span></a>    Example:: 
<a name="l7313"><span class="ln">7313 </span></a> 
<a name="l7314"><span class="ln">7314 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l7315"><span class="ln">7315 </span></a>        &gt;&gt;&gt; a 
<a name="l7316"><span class="ln">7316 </span></a>        tensor([ 0.9041,  0.0196, -0.3108, -2.4423]) 
<a name="l7317"><span class="ln">7317 </span></a>        &gt;&gt;&gt; torch.atan2(a, torch.randn(4)) 
<a name="l7318"><span class="ln">7318 </span></a>        tensor([ 0.9833,  0.0811, -1.9743, -1.4151]) 
<a name="l7319"><span class="ln">7319 </span></a>    &quot;&quot;&quot;</span>
<a name="l7320"><span class="ln">7320 </span></a>
<a name="l7321"><span class="ln">7321 </span></a><span class="s2">def </span><span class="s1">atan_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7322"><span class="ln">7322 </span></a><span class="s2">def </span><span class="s1">atanh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7323"><span class="ln">7323 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7324"><span class="ln">7324 </span></a>    atanh(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l7325"><span class="ln">7325 </span></a> 
<a name="l7326"><span class="ln">7326 </span></a>    Returns a new tensor with the inverse hyperbolic tangent of the elements of :attr:`input`. 
<a name="l7327"><span class="ln">7327 </span></a> 
<a name="l7328"><span class="ln">7328 </span></a>    Note: 
<a name="l7329"><span class="ln">7329 </span></a>        The domain of the inverse hyperbolic tangent is `(-1, 1)` and values outside this range 
<a name="l7330"><span class="ln">7330 </span></a>        will be mapped to ``NaN``, except for the values `1` and `-1` for which the output is 
<a name="l7331"><span class="ln">7331 </span></a>        mapped to `+/-INF` respectively. 
<a name="l7332"><span class="ln">7332 </span></a> 
<a name="l7333"><span class="ln">7333 </span></a>    .. math:: 
<a name="l7334"><span class="ln">7334 </span></a>        \text{out}_{i} = \tanh^{-1}(\text{input}_{i}) 
<a name="l7335"><span class="ln">7335 </span></a> 
<a name="l7336"><span class="ln">7336 </span></a>    Args: 
<a name="l7337"><span class="ln">7337 </span></a>        input (Tensor): the input tensor. 
<a name="l7338"><span class="ln">7338 </span></a> 
<a name="l7339"><span class="ln">7339 </span></a>    Keyword arguments: 
<a name="l7340"><span class="ln">7340 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7341"><span class="ln">7341 </span></a> 
<a name="l7342"><span class="ln">7342 </span></a>    Example:: 
<a name="l7343"><span class="ln">7343 </span></a> 
<a name="l7344"><span class="ln">7344 </span></a>        &gt;&gt;&gt; a = torch.randn(4).uniform_(-1, 1) 
<a name="l7345"><span class="ln">7345 </span></a>        &gt;&gt;&gt; a 
<a name="l7346"><span class="ln">7346 </span></a>        tensor([ -0.9385, 0.2968, -0.8591, -0.1871 ]) 
<a name="l7347"><span class="ln">7347 </span></a>        &gt;&gt;&gt; torch.atanh(a) 
<a name="l7348"><span class="ln">7348 </span></a>        tensor([ -1.7253, 0.3060, -1.2899, -0.1893 ]) 
<a name="l7349"><span class="ln">7349 </span></a>    &quot;&quot;&quot;</span>
<a name="l7350"><span class="ln">7350 </span></a>
<a name="l7351"><span class="ln">7351 </span></a><span class="s2">def </span><span class="s1">atanh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7352"><span class="ln">7352 </span></a><span class="s2">def </span><span class="s1">avg_pool1d</span><span class="s3">(</span>
<a name="l7353"><span class="ln">7353 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7354"><span class="ln">7354 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l7355"><span class="ln">7355 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l7356"><span class="ln">7356 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l7357"><span class="ln">7357 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l7358"><span class="ln">7358 </span></a>    <span class="s1">count_include_pad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l7359"><span class="ln">7359 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7360"><span class="ln">7360 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7361"><span class="ln">7361 </span></a><span class="s2">def </span><span class="s1">baddbmm</span><span class="s3">(</span>
<a name="l7362"><span class="ln">7362 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l7363"><span class="ln">7363 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7364"><span class="ln">7364 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l7365"><span class="ln">7365 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7366"><span class="ln">7366 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7367"><span class="ln">7367 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7368"><span class="ln">7368 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7369"><span class="ln">7369 </span></a>    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l7370"><span class="ln">7370 </span></a> 
<a name="l7371"><span class="ln">7371 </span></a>    Performs a batch matrix-matrix product of matrices in :attr:`batch1` 
<a name="l7372"><span class="ln">7372 </span></a>    and :attr:`batch2`. 
<a name="l7373"><span class="ln">7373 </span></a>    :attr:`input` is added to the final result. 
<a name="l7374"><span class="ln">7374 </span></a> 
<a name="l7375"><span class="ln">7375 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same 
<a name="l7376"><span class="ln">7376 </span></a>    number of matrices. 
<a name="l7377"><span class="ln">7377 </span></a> 
<a name="l7378"><span class="ln">7378 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l7379"><span class="ln">7379 </span></a>    :math:`(b \times m \times p)` tensor, then :attr:`input` must be 
<a name="l7380"><span class="ln">7380 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 
<a name="l7381"><span class="ln">7381 </span></a>    :math:`(b \times n \times p)` tensor and :attr:`out` will be a 
<a name="l7382"><span class="ln">7382 </span></a>    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the 
<a name="l7383"><span class="ln">7383 </span></a>    same as the scaling factors used in :meth:`torch.addbmm`. 
<a name="l7384"><span class="ln">7384 </span></a> 
<a name="l7385"><span class="ln">7385 </span></a>    .. math:: 
<a name="l7386"><span class="ln">7386 </span></a>        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l7387"><span class="ln">7387 </span></a> 
<a name="l7388"><span class="ln">7388 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l7389"><span class="ln">7389 </span></a>    it will not be propagated. 
<a name="l7390"><span class="ln">7390 </span></a> 
<a name="l7391"><span class="ln">7391 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l7392"><span class="ln">7392 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l7393"><span class="ln">7393 </span></a> 
<a name="l7394"><span class="ln">7394 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l7395"><span class="ln">7395 </span></a> 
<a name="l7396"><span class="ln">7396 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l7397"><span class="ln">7397 </span></a> 
<a name="l7398"><span class="ln">7398 </span></a>    Args: 
<a name="l7399"><span class="ln">7399 </span></a>        input (Tensor): the tensor to be added 
<a name="l7400"><span class="ln">7400 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l7401"><span class="ln">7401 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l7402"><span class="ln">7402 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l7403"><span class="ln">7403 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l7404"><span class="ln">7404 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l7405"><span class="ln">7405 </span></a> 
<a name="l7406"><span class="ln">7406 </span></a>    Keyword args: 
<a name="l7407"><span class="ln">7407 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l7408"><span class="ln">7408 </span></a>        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`) 
<a name="l7409"><span class="ln">7409 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7410"><span class="ln">7410 </span></a> 
<a name="l7411"><span class="ln">7411 </span></a>    Example:: 
<a name="l7412"><span class="ln">7412 </span></a> 
<a name="l7413"><span class="ln">7413 </span></a>        &gt;&gt;&gt; M = torch.randn(10, 3, 5) 
<a name="l7414"><span class="ln">7414 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l7415"><span class="ln">7415 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l7416"><span class="ln">7416 </span></a>        &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() 
<a name="l7417"><span class="ln">7417 </span></a>        torch.Size([10, 3, 5]) 
<a name="l7418"><span class="ln">7418 </span></a>    &quot;&quot;&quot;</span>
<a name="l7419"><span class="ln">7419 </span></a>
<a name="l7420"><span class="ln">7420 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7421"><span class="ln">7421 </span></a><span class="s2">def </span><span class="s1">baddbmm</span><span class="s3">(</span>
<a name="l7422"><span class="ln">7422 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l7423"><span class="ln">7423 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7424"><span class="ln">7424 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l7425"><span class="ln">7425 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7426"><span class="ln">7426 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7427"><span class="ln">7427 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7428"><span class="ln">7428 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7429"><span class="ln">7429 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7430"><span class="ln">7430 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7431"><span class="ln">7431 </span></a>    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l7432"><span class="ln">7432 </span></a> 
<a name="l7433"><span class="ln">7433 </span></a>    Performs a batch matrix-matrix product of matrices in :attr:`batch1` 
<a name="l7434"><span class="ln">7434 </span></a>    and :attr:`batch2`. 
<a name="l7435"><span class="ln">7435 </span></a>    :attr:`input` is added to the final result. 
<a name="l7436"><span class="ln">7436 </span></a> 
<a name="l7437"><span class="ln">7437 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same 
<a name="l7438"><span class="ln">7438 </span></a>    number of matrices. 
<a name="l7439"><span class="ln">7439 </span></a> 
<a name="l7440"><span class="ln">7440 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l7441"><span class="ln">7441 </span></a>    :math:`(b \times m \times p)` tensor, then :attr:`input` must be 
<a name="l7442"><span class="ln">7442 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 
<a name="l7443"><span class="ln">7443 </span></a>    :math:`(b \times n \times p)` tensor and :attr:`out` will be a 
<a name="l7444"><span class="ln">7444 </span></a>    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the 
<a name="l7445"><span class="ln">7445 </span></a>    same as the scaling factors used in :meth:`torch.addbmm`. 
<a name="l7446"><span class="ln">7446 </span></a> 
<a name="l7447"><span class="ln">7447 </span></a>    .. math:: 
<a name="l7448"><span class="ln">7448 </span></a>        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l7449"><span class="ln">7449 </span></a> 
<a name="l7450"><span class="ln">7450 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l7451"><span class="ln">7451 </span></a>    it will not be propagated. 
<a name="l7452"><span class="ln">7452 </span></a> 
<a name="l7453"><span class="ln">7453 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l7454"><span class="ln">7454 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l7455"><span class="ln">7455 </span></a> 
<a name="l7456"><span class="ln">7456 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l7457"><span class="ln">7457 </span></a> 
<a name="l7458"><span class="ln">7458 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l7459"><span class="ln">7459 </span></a> 
<a name="l7460"><span class="ln">7460 </span></a>    Args: 
<a name="l7461"><span class="ln">7461 </span></a>        input (Tensor): the tensor to be added 
<a name="l7462"><span class="ln">7462 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l7463"><span class="ln">7463 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l7464"><span class="ln">7464 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l7465"><span class="ln">7465 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l7466"><span class="ln">7466 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l7467"><span class="ln">7467 </span></a> 
<a name="l7468"><span class="ln">7468 </span></a>    Keyword args: 
<a name="l7469"><span class="ln">7469 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l7470"><span class="ln">7470 </span></a>        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`) 
<a name="l7471"><span class="ln">7471 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7472"><span class="ln">7472 </span></a> 
<a name="l7473"><span class="ln">7473 </span></a>    Example:: 
<a name="l7474"><span class="ln">7474 </span></a> 
<a name="l7475"><span class="ln">7475 </span></a>        &gt;&gt;&gt; M = torch.randn(10, 3, 5) 
<a name="l7476"><span class="ln">7476 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l7477"><span class="ln">7477 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l7478"><span class="ln">7478 </span></a>        &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() 
<a name="l7479"><span class="ln">7479 </span></a>        torch.Size([10, 3, 5]) 
<a name="l7480"><span class="ln">7480 </span></a>    &quot;&quot;&quot;</span>
<a name="l7481"><span class="ln">7481 </span></a>
<a name="l7482"><span class="ln">7482 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7483"><span class="ln">7483 </span></a><span class="s2">def </span><span class="s1">baddbmm</span><span class="s3">(</span>
<a name="l7484"><span class="ln">7484 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7485"><span class="ln">7485 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7486"><span class="ln">7486 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7487"><span class="ln">7487 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7488"><span class="ln">7488 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l7489"><span class="ln">7489 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l7490"><span class="ln">7490 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7491"><span class="ln">7491 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7492"><span class="ln">7492 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7493"><span class="ln">7493 </span></a>    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l7494"><span class="ln">7494 </span></a> 
<a name="l7495"><span class="ln">7495 </span></a>    Performs a batch matrix-matrix product of matrices in :attr:`batch1` 
<a name="l7496"><span class="ln">7496 </span></a>    and :attr:`batch2`. 
<a name="l7497"><span class="ln">7497 </span></a>    :attr:`input` is added to the final result. 
<a name="l7498"><span class="ln">7498 </span></a> 
<a name="l7499"><span class="ln">7499 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same 
<a name="l7500"><span class="ln">7500 </span></a>    number of matrices. 
<a name="l7501"><span class="ln">7501 </span></a> 
<a name="l7502"><span class="ln">7502 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l7503"><span class="ln">7503 </span></a>    :math:`(b \times m \times p)` tensor, then :attr:`input` must be 
<a name="l7504"><span class="ln">7504 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 
<a name="l7505"><span class="ln">7505 </span></a>    :math:`(b \times n \times p)` tensor and :attr:`out` will be a 
<a name="l7506"><span class="ln">7506 </span></a>    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the 
<a name="l7507"><span class="ln">7507 </span></a>    same as the scaling factors used in :meth:`torch.addbmm`. 
<a name="l7508"><span class="ln">7508 </span></a> 
<a name="l7509"><span class="ln">7509 </span></a>    .. math:: 
<a name="l7510"><span class="ln">7510 </span></a>        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l7511"><span class="ln">7511 </span></a> 
<a name="l7512"><span class="ln">7512 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l7513"><span class="ln">7513 </span></a>    it will not be propagated. 
<a name="l7514"><span class="ln">7514 </span></a> 
<a name="l7515"><span class="ln">7515 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l7516"><span class="ln">7516 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l7517"><span class="ln">7517 </span></a> 
<a name="l7518"><span class="ln">7518 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l7519"><span class="ln">7519 </span></a> 
<a name="l7520"><span class="ln">7520 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l7521"><span class="ln">7521 </span></a> 
<a name="l7522"><span class="ln">7522 </span></a>    Args: 
<a name="l7523"><span class="ln">7523 </span></a>        input (Tensor): the tensor to be added 
<a name="l7524"><span class="ln">7524 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l7525"><span class="ln">7525 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l7526"><span class="ln">7526 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l7527"><span class="ln">7527 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l7528"><span class="ln">7528 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l7529"><span class="ln">7529 </span></a> 
<a name="l7530"><span class="ln">7530 </span></a>    Keyword args: 
<a name="l7531"><span class="ln">7531 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l7532"><span class="ln">7532 </span></a>        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`) 
<a name="l7533"><span class="ln">7533 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7534"><span class="ln">7534 </span></a> 
<a name="l7535"><span class="ln">7535 </span></a>    Example:: 
<a name="l7536"><span class="ln">7536 </span></a> 
<a name="l7537"><span class="ln">7537 </span></a>        &gt;&gt;&gt; M = torch.randn(10, 3, 5) 
<a name="l7538"><span class="ln">7538 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l7539"><span class="ln">7539 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l7540"><span class="ln">7540 </span></a>        &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() 
<a name="l7541"><span class="ln">7541 </span></a>        torch.Size([10, 3, 5]) 
<a name="l7542"><span class="ln">7542 </span></a>    &quot;&quot;&quot;</span>
<a name="l7543"><span class="ln">7543 </span></a>
<a name="l7544"><span class="ln">7544 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7545"><span class="ln">7545 </span></a><span class="s2">def </span><span class="s1">baddbmm</span><span class="s3">(</span>
<a name="l7546"><span class="ln">7546 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7547"><span class="ln">7547 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7548"><span class="ln">7548 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7549"><span class="ln">7549 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l7550"><span class="ln">7550 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7551"><span class="ln">7551 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l7552"><span class="ln">7552 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l7553"><span class="ln">7553 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7554"><span class="ln">7554 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7555"><span class="ln">7555 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7556"><span class="ln">7556 </span></a>    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l7557"><span class="ln">7557 </span></a> 
<a name="l7558"><span class="ln">7558 </span></a>    Performs a batch matrix-matrix product of matrices in :attr:`batch1` 
<a name="l7559"><span class="ln">7559 </span></a>    and :attr:`batch2`. 
<a name="l7560"><span class="ln">7560 </span></a>    :attr:`input` is added to the final result. 
<a name="l7561"><span class="ln">7561 </span></a> 
<a name="l7562"><span class="ln">7562 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same 
<a name="l7563"><span class="ln">7563 </span></a>    number of matrices. 
<a name="l7564"><span class="ln">7564 </span></a> 
<a name="l7565"><span class="ln">7565 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l7566"><span class="ln">7566 </span></a>    :math:`(b \times m \times p)` tensor, then :attr:`input` must be 
<a name="l7567"><span class="ln">7567 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 
<a name="l7568"><span class="ln">7568 </span></a>    :math:`(b \times n \times p)` tensor and :attr:`out` will be a 
<a name="l7569"><span class="ln">7569 </span></a>    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the 
<a name="l7570"><span class="ln">7570 </span></a>    same as the scaling factors used in :meth:`torch.addbmm`. 
<a name="l7571"><span class="ln">7571 </span></a> 
<a name="l7572"><span class="ln">7572 </span></a>    .. math:: 
<a name="l7573"><span class="ln">7573 </span></a>        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l7574"><span class="ln">7574 </span></a> 
<a name="l7575"><span class="ln">7575 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l7576"><span class="ln">7576 </span></a>    it will not be propagated. 
<a name="l7577"><span class="ln">7577 </span></a> 
<a name="l7578"><span class="ln">7578 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l7579"><span class="ln">7579 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l7580"><span class="ln">7580 </span></a> 
<a name="l7581"><span class="ln">7581 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l7582"><span class="ln">7582 </span></a> 
<a name="l7583"><span class="ln">7583 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l7584"><span class="ln">7584 </span></a> 
<a name="l7585"><span class="ln">7585 </span></a>    Args: 
<a name="l7586"><span class="ln">7586 </span></a>        input (Tensor): the tensor to be added 
<a name="l7587"><span class="ln">7587 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l7588"><span class="ln">7588 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l7589"><span class="ln">7589 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l7590"><span class="ln">7590 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l7591"><span class="ln">7591 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l7592"><span class="ln">7592 </span></a> 
<a name="l7593"><span class="ln">7593 </span></a>    Keyword args: 
<a name="l7594"><span class="ln">7594 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l7595"><span class="ln">7595 </span></a>        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`) 
<a name="l7596"><span class="ln">7596 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7597"><span class="ln">7597 </span></a> 
<a name="l7598"><span class="ln">7598 </span></a>    Example:: 
<a name="l7599"><span class="ln">7599 </span></a> 
<a name="l7600"><span class="ln">7600 </span></a>        &gt;&gt;&gt; M = torch.randn(10, 3, 5) 
<a name="l7601"><span class="ln">7601 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l7602"><span class="ln">7602 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l7603"><span class="ln">7603 </span></a>        &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() 
<a name="l7604"><span class="ln">7604 </span></a>        torch.Size([10, 3, 5]) 
<a name="l7605"><span class="ln">7605 </span></a>    &quot;&quot;&quot;</span>
<a name="l7606"><span class="ln">7606 </span></a>
<a name="l7607"><span class="ln">7607 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7608"><span class="ln">7608 </span></a><span class="s2">def </span><span class="s1">baddbmm</span><span class="s3">(</span>
<a name="l7609"><span class="ln">7609 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l7610"><span class="ln">7610 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7611"><span class="ln">7611 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7612"><span class="ln">7612 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7613"><span class="ln">7613 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7614"><span class="ln">7614 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7615"><span class="ln">7615 </span></a>    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l7616"><span class="ln">7616 </span></a> 
<a name="l7617"><span class="ln">7617 </span></a>    Performs a batch matrix-matrix product of matrices in :attr:`batch1` 
<a name="l7618"><span class="ln">7618 </span></a>    and :attr:`batch2`. 
<a name="l7619"><span class="ln">7619 </span></a>    :attr:`input` is added to the final result. 
<a name="l7620"><span class="ln">7620 </span></a> 
<a name="l7621"><span class="ln">7621 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same 
<a name="l7622"><span class="ln">7622 </span></a>    number of matrices. 
<a name="l7623"><span class="ln">7623 </span></a> 
<a name="l7624"><span class="ln">7624 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l7625"><span class="ln">7625 </span></a>    :math:`(b \times m \times p)` tensor, then :attr:`input` must be 
<a name="l7626"><span class="ln">7626 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 
<a name="l7627"><span class="ln">7627 </span></a>    :math:`(b \times n \times p)` tensor and :attr:`out` will be a 
<a name="l7628"><span class="ln">7628 </span></a>    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the 
<a name="l7629"><span class="ln">7629 </span></a>    same as the scaling factors used in :meth:`torch.addbmm`. 
<a name="l7630"><span class="ln">7630 </span></a> 
<a name="l7631"><span class="ln">7631 </span></a>    .. math:: 
<a name="l7632"><span class="ln">7632 </span></a>        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l7633"><span class="ln">7633 </span></a> 
<a name="l7634"><span class="ln">7634 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l7635"><span class="ln">7635 </span></a>    it will not be propagated. 
<a name="l7636"><span class="ln">7636 </span></a> 
<a name="l7637"><span class="ln">7637 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l7638"><span class="ln">7638 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l7639"><span class="ln">7639 </span></a> 
<a name="l7640"><span class="ln">7640 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l7641"><span class="ln">7641 </span></a> 
<a name="l7642"><span class="ln">7642 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l7643"><span class="ln">7643 </span></a> 
<a name="l7644"><span class="ln">7644 </span></a>    Args: 
<a name="l7645"><span class="ln">7645 </span></a>        input (Tensor): the tensor to be added 
<a name="l7646"><span class="ln">7646 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l7647"><span class="ln">7647 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l7648"><span class="ln">7648 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l7649"><span class="ln">7649 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l7650"><span class="ln">7650 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l7651"><span class="ln">7651 </span></a> 
<a name="l7652"><span class="ln">7652 </span></a>    Keyword args: 
<a name="l7653"><span class="ln">7653 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l7654"><span class="ln">7654 </span></a>        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`) 
<a name="l7655"><span class="ln">7655 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7656"><span class="ln">7656 </span></a> 
<a name="l7657"><span class="ln">7657 </span></a>    Example:: 
<a name="l7658"><span class="ln">7658 </span></a> 
<a name="l7659"><span class="ln">7659 </span></a>        &gt;&gt;&gt; M = torch.randn(10, 3, 5) 
<a name="l7660"><span class="ln">7660 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l7661"><span class="ln">7661 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l7662"><span class="ln">7662 </span></a>        &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() 
<a name="l7663"><span class="ln">7663 </span></a>        torch.Size([10, 3, 5]) 
<a name="l7664"><span class="ln">7664 </span></a>    &quot;&quot;&quot;</span>
<a name="l7665"><span class="ln">7665 </span></a>
<a name="l7666"><span class="ln">7666 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7667"><span class="ln">7667 </span></a><span class="s2">def </span><span class="s1">baddbmm</span><span class="s3">(</span>
<a name="l7668"><span class="ln">7668 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l7669"><span class="ln">7669 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7670"><span class="ln">7670 </span></a>    <span class="s1">batch1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7671"><span class="ln">7671 </span></a>    <span class="s1">batch2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7672"><span class="ln">7672 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7673"><span class="ln">7673 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7674"><span class="ln">7674 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7675"><span class="ln">7675 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7676"><span class="ln">7676 </span></a>    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l7677"><span class="ln">7677 </span></a> 
<a name="l7678"><span class="ln">7678 </span></a>    Performs a batch matrix-matrix product of matrices in :attr:`batch1` 
<a name="l7679"><span class="ln">7679 </span></a>    and :attr:`batch2`. 
<a name="l7680"><span class="ln">7680 </span></a>    :attr:`input` is added to the final result. 
<a name="l7681"><span class="ln">7681 </span></a> 
<a name="l7682"><span class="ln">7682 </span></a>    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same 
<a name="l7683"><span class="ln">7683 </span></a>    number of matrices. 
<a name="l7684"><span class="ln">7684 </span></a> 
<a name="l7685"><span class="ln">7685 </span></a>    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a 
<a name="l7686"><span class="ln">7686 </span></a>    :math:`(b \times m \times p)` tensor, then :attr:`input` must be 
<a name="l7687"><span class="ln">7687 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with a 
<a name="l7688"><span class="ln">7688 </span></a>    :math:`(b \times n \times p)` tensor and :attr:`out` will be a 
<a name="l7689"><span class="ln">7689 </span></a>    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the 
<a name="l7690"><span class="ln">7690 </span></a>    same as the scaling factors used in :meth:`torch.addbmm`. 
<a name="l7691"><span class="ln">7691 </span></a> 
<a name="l7692"><span class="ln">7692 </span></a>    .. math:: 
<a name="l7693"><span class="ln">7693 </span></a>        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i) 
<a name="l7694"><span class="ln">7694 </span></a> 
<a name="l7695"><span class="ln">7695 </span></a>    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in 
<a name="l7696"><span class="ln">7696 </span></a>    it will not be propagated. 
<a name="l7697"><span class="ln">7697 </span></a> 
<a name="l7698"><span class="ln">7698 </span></a>    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and 
<a name="l7699"><span class="ln">7699 </span></a>    :attr:`alpha` must be real numbers, otherwise they should be integers. 
<a name="l7700"><span class="ln">7700 </span></a> 
<a name="l7701"><span class="ln">7701 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l7702"><span class="ln">7702 </span></a> 
<a name="l7703"><span class="ln">7703 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l7704"><span class="ln">7704 </span></a> 
<a name="l7705"><span class="ln">7705 </span></a>    Args: 
<a name="l7706"><span class="ln">7706 </span></a>        input (Tensor): the tensor to be added 
<a name="l7707"><span class="ln">7707 </span></a>        batch1 (Tensor): the first batch of matrices to be multiplied 
<a name="l7708"><span class="ln">7708 </span></a>        batch2 (Tensor): the second batch of matrices to be multiplied 
<a name="l7709"><span class="ln">7709 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l7710"><span class="ln">7710 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l7711"><span class="ln">7711 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l7712"><span class="ln">7712 </span></a> 
<a name="l7713"><span class="ln">7713 </span></a>    Keyword args: 
<a name="l7714"><span class="ln">7714 </span></a>        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`) 
<a name="l7715"><span class="ln">7715 </span></a>        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`) 
<a name="l7716"><span class="ln">7716 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7717"><span class="ln">7717 </span></a> 
<a name="l7718"><span class="ln">7718 </span></a>    Example:: 
<a name="l7719"><span class="ln">7719 </span></a> 
<a name="l7720"><span class="ln">7720 </span></a>        &gt;&gt;&gt; M = torch.randn(10, 3, 5) 
<a name="l7721"><span class="ln">7721 </span></a>        &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) 
<a name="l7722"><span class="ln">7722 </span></a>        &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) 
<a name="l7723"><span class="ln">7723 </span></a>        &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() 
<a name="l7724"><span class="ln">7724 </span></a>        torch.Size([10, 3, 5]) 
<a name="l7725"><span class="ln">7725 </span></a>    &quot;&quot;&quot;</span>
<a name="l7726"><span class="ln">7726 </span></a>
<a name="l7727"><span class="ln">7727 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7728"><span class="ln">7728 </span></a><span class="s2">def </span><span class="s1">bartlett_window</span><span class="s3">(</span>
<a name="l7729"><span class="ln">7729 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l7730"><span class="ln">7730 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7731"><span class="ln">7731 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7732"><span class="ln">7732 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7733"><span class="ln">7733 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7734"><span class="ln">7734 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l7735"><span class="ln">7735 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l7736"><span class="ln">7736 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7737"><span class="ln">7737 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7738"><span class="ln">7738 </span></a>    bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l7739"><span class="ln">7739 </span></a> 
<a name="l7740"><span class="ln">7740 </span></a>    Bartlett window function. 
<a name="l7741"><span class="ln">7741 </span></a> 
<a name="l7742"><span class="ln">7742 </span></a>    .. math:: 
<a name="l7743"><span class="ln">7743 </span></a>        w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases} 
<a name="l7744"><span class="ln">7744 </span></a>            \frac{2n}{N - 1} &amp; \text{if } 0 \leq n \leq \frac{N - 1}{2} \\ 
<a name="l7745"><span class="ln">7745 </span></a>            2 - \frac{2n}{N - 1} &amp; \text{if } \frac{N - 1}{2} &lt; n &lt; N \\ 
<a name="l7746"><span class="ln">7746 </span></a>        \end{cases}, 
<a name="l7747"><span class="ln">7747 </span></a> 
<a name="l7748"><span class="ln">7748 </span></a>    where :math:`N` is the full window size. 
<a name="l7749"><span class="ln">7749 </span></a> 
<a name="l7750"><span class="ln">7750 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l7751"><span class="ln">7751 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l7752"><span class="ln">7752 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l7753"><span class="ln">7753 </span></a>    ready to be used as a periodic window with functions like 
<a name="l7754"><span class="ln">7754 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l7755"><span class="ln">7755 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l7756"><span class="ln">7756 </span></a>    ``torch.bartlett_window(L, periodic=True)`` equal to 
<a name="l7757"><span class="ln">7757 </span></a>    ``torch.bartlett_window(L + 1, periodic=False)[:-1])``. 
<a name="l7758"><span class="ln">7758 </span></a> 
<a name="l7759"><span class="ln">7759 </span></a>    .. note:: 
<a name="l7760"><span class="ln">7760 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l7761"><span class="ln">7761 </span></a> 
<a name="l7762"><span class="ln">7762 </span></a>    Arguments: 
<a name="l7763"><span class="ln">7763 </span></a>        window_length (int): the size of returned window 
<a name="l7764"><span class="ln">7764 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l7765"><span class="ln">7765 </span></a>            function. If False, return a symmetric window. 
<a name="l7766"><span class="ln">7766 </span></a> 
<a name="l7767"><span class="ln">7767 </span></a>    Keyword args: 
<a name="l7768"><span class="ln">7768 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l7769"><span class="ln">7769 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l7770"><span class="ln">7770 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l7771"><span class="ln">7771 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l7772"><span class="ln">7772 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l7773"><span class="ln">7773 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l7774"><span class="ln">7774 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l7775"><span class="ln">7775 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l7776"><span class="ln">7776 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l7777"><span class="ln">7777 </span></a>            returned tensor. Default: ``False``. 
<a name="l7778"><span class="ln">7778 </span></a> 
<a name="l7779"><span class="ln">7779 </span></a>    Returns: 
<a name="l7780"><span class="ln">7780 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window 
<a name="l7781"><span class="ln">7781 </span></a>    &quot;&quot;&quot;</span>
<a name="l7782"><span class="ln">7782 </span></a>
<a name="l7783"><span class="ln">7783 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7784"><span class="ln">7784 </span></a><span class="s2">def </span><span class="s1">bartlett_window</span><span class="s3">(</span>
<a name="l7785"><span class="ln">7785 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l7786"><span class="ln">7786 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l7787"><span class="ln">7787 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7788"><span class="ln">7788 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7789"><span class="ln">7789 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7790"><span class="ln">7790 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7791"><span class="ln">7791 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l7792"><span class="ln">7792 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l7793"><span class="ln">7793 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7794"><span class="ln">7794 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7795"><span class="ln">7795 </span></a>    bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l7796"><span class="ln">7796 </span></a> 
<a name="l7797"><span class="ln">7797 </span></a>    Bartlett window function. 
<a name="l7798"><span class="ln">7798 </span></a> 
<a name="l7799"><span class="ln">7799 </span></a>    .. math:: 
<a name="l7800"><span class="ln">7800 </span></a>        w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases} 
<a name="l7801"><span class="ln">7801 </span></a>            \frac{2n}{N - 1} &amp; \text{if } 0 \leq n \leq \frac{N - 1}{2} \\ 
<a name="l7802"><span class="ln">7802 </span></a>            2 - \frac{2n}{N - 1} &amp; \text{if } \frac{N - 1}{2} &lt; n &lt; N \\ 
<a name="l7803"><span class="ln">7803 </span></a>        \end{cases}, 
<a name="l7804"><span class="ln">7804 </span></a> 
<a name="l7805"><span class="ln">7805 </span></a>    where :math:`N` is the full window size. 
<a name="l7806"><span class="ln">7806 </span></a> 
<a name="l7807"><span class="ln">7807 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l7808"><span class="ln">7808 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l7809"><span class="ln">7809 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l7810"><span class="ln">7810 </span></a>    ready to be used as a periodic window with functions like 
<a name="l7811"><span class="ln">7811 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l7812"><span class="ln">7812 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l7813"><span class="ln">7813 </span></a>    ``torch.bartlett_window(L, periodic=True)`` equal to 
<a name="l7814"><span class="ln">7814 </span></a>    ``torch.bartlett_window(L + 1, periodic=False)[:-1])``. 
<a name="l7815"><span class="ln">7815 </span></a> 
<a name="l7816"><span class="ln">7816 </span></a>    .. note:: 
<a name="l7817"><span class="ln">7817 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l7818"><span class="ln">7818 </span></a> 
<a name="l7819"><span class="ln">7819 </span></a>    Arguments: 
<a name="l7820"><span class="ln">7820 </span></a>        window_length (int): the size of returned window 
<a name="l7821"><span class="ln">7821 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l7822"><span class="ln">7822 </span></a>            function. If False, return a symmetric window. 
<a name="l7823"><span class="ln">7823 </span></a> 
<a name="l7824"><span class="ln">7824 </span></a>    Keyword args: 
<a name="l7825"><span class="ln">7825 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l7826"><span class="ln">7826 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l7827"><span class="ln">7827 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l7828"><span class="ln">7828 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l7829"><span class="ln">7829 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l7830"><span class="ln">7830 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l7831"><span class="ln">7831 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l7832"><span class="ln">7832 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l7833"><span class="ln">7833 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l7834"><span class="ln">7834 </span></a>            returned tensor. Default: ``False``. 
<a name="l7835"><span class="ln">7835 </span></a> 
<a name="l7836"><span class="ln">7836 </span></a>    Returns: 
<a name="l7837"><span class="ln">7837 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window 
<a name="l7838"><span class="ln">7838 </span></a>    &quot;&quot;&quot;</span>
<a name="l7839"><span class="ln">7839 </span></a>
<a name="l7840"><span class="ln">7840 </span></a><span class="s2">def </span><span class="s1">batch_norm</span><span class="s3">(</span>
<a name="l7841"><span class="ln">7841 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7842"><span class="ln">7842 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7843"><span class="ln">7843 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7844"><span class="ln">7844 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7845"><span class="ln">7845 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7846"><span class="ln">7846 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l7847"><span class="ln">7847 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7848"><span class="ln">7848 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7849"><span class="ln">7849 </span></a>    <span class="s1">cudnn_enabled</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l7850"><span class="ln">7850 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7851"><span class="ln">7851 </span></a><span class="s2">def </span><span class="s1">batch_norm_backward_elemt</span><span class="s3">(</span>
<a name="l7852"><span class="ln">7852 </span></a>    <span class="s1">grad_out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7853"><span class="ln">7853 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7854"><span class="ln">7854 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7855"><span class="ln">7855 </span></a>    <span class="s1">invstd</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7856"><span class="ln">7856 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7857"><span class="ln">7857 </span></a>    <span class="s1">sum_dy</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7858"><span class="ln">7858 </span></a>    <span class="s1">sum_dy_xmu</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7859"><span class="ln">7859 </span></a>    <span class="s1">count</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7860"><span class="ln">7860 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7861"><span class="ln">7861 </span></a><span class="s2">def </span><span class="s1">batch_norm_backward_reduce</span><span class="s3">(</span>
<a name="l7862"><span class="ln">7862 </span></a>    <span class="s1">grad_out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7863"><span class="ln">7863 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7864"><span class="ln">7864 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7865"><span class="ln">7865 </span></a>    <span class="s1">invstd</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7866"><span class="ln">7866 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7867"><span class="ln">7867 </span></a>    <span class="s1">input_g</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l7868"><span class="ln">7868 </span></a>    <span class="s1">weight_g</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l7869"><span class="ln">7869 </span></a>    <span class="s1">bias_g</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l7870"><span class="ln">7870 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7871"><span class="ln">7871 </span></a><span class="s2">def </span><span class="s1">batch_norm_elemt</span><span class="s3">(</span>
<a name="l7872"><span class="ln">7872 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7873"><span class="ln">7873 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7874"><span class="ln">7874 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7875"><span class="ln">7875 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7876"><span class="ln">7876 </span></a>    <span class="s1">invstd</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7877"><span class="ln">7877 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7878"><span class="ln">7878 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7879"><span class="ln">7879 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7880"><span class="ln">7880 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7881"><span class="ln">7881 </span></a><span class="s2">def </span><span class="s1">batch_norm_gather_stats</span><span class="s3">(</span>
<a name="l7882"><span class="ln">7882 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7883"><span class="ln">7883 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7884"><span class="ln">7884 </span></a>    <span class="s1">invstd</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7885"><span class="ln">7885 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7886"><span class="ln">7886 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7887"><span class="ln">7887 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7888"><span class="ln">7888 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7889"><span class="ln">7889 </span></a>    <span class="s1">count</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l7890"><span class="ln">7890 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7891"><span class="ln">7891 </span></a><span class="s2">def </span><span class="s1">batch_norm_gather_stats_with_counts</span><span class="s3">(</span>
<a name="l7892"><span class="ln">7892 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7893"><span class="ln">7893 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7894"><span class="ln">7894 </span></a>    <span class="s1">invstd</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7895"><span class="ln">7895 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7896"><span class="ln">7896 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7897"><span class="ln">7897 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7898"><span class="ln">7898 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7899"><span class="ln">7899 </span></a>    <span class="s1">counts</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7900"><span class="ln">7900 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7901"><span class="ln">7901 </span></a><span class="s2">def </span><span class="s1">batch_norm_stats</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7902"><span class="ln">7902 </span></a><span class="s2">def </span><span class="s1">batch_norm_update_stats</span><span class="s3">(</span>
<a name="l7903"><span class="ln">7903 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7904"><span class="ln">7904 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7905"><span class="ln">7905 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l7906"><span class="ln">7906 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7907"><span class="ln">7907 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l7908"><span class="ln">7908 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7909"><span class="ln">7909 </span></a><span class="s2">def </span><span class="s1">bernoulli</span><span class="s3">(</span>
<a name="l7910"><span class="ln">7910 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7911"><span class="ln">7911 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7912"><span class="ln">7912 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7913"><span class="ln">7913 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7914"><span class="ln">7914 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7915"><span class="ln">7915 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7916"><span class="ln">7916 </span></a>    bernoulli(input: Tensor, *, generator: Optional[Generator], out: Optional[Tensor]) -&gt; Tensor 
<a name="l7917"><span class="ln">7917 </span></a> 
<a name="l7918"><span class="ln">7918 </span></a>    Draws binary random numbers (0 or 1) from a Bernoulli distribution. 
<a name="l7919"><span class="ln">7919 </span></a> 
<a name="l7920"><span class="ln">7920 </span></a>    The :attr:`input` tensor should be a tensor containing probabilities 
<a name="l7921"><span class="ln">7921 </span></a>    to be used for drawing the binary random number. 
<a name="l7922"><span class="ln">7922 </span></a>    Hence, all values in :attr:`input` have to be in the range: 
<a name="l7923"><span class="ln">7923 </span></a>    :math:`0 \leq \text{input}_i \leq 1`. 
<a name="l7924"><span class="ln">7924 </span></a> 
<a name="l7925"><span class="ln">7925 </span></a>    The :math:`\text{i}^{th}` element of the output tensor will draw a 
<a name="l7926"><span class="ln">7926 </span></a>    value :math:`1` according to the :math:`\text{i}^{th}` probability value given 
<a name="l7927"><span class="ln">7927 </span></a>    in :attr:`input`. 
<a name="l7928"><span class="ln">7928 </span></a> 
<a name="l7929"><span class="ln">7929 </span></a>    .. math:: 
<a name="l7930"><span class="ln">7930 </span></a>        \text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i}) 
<a name="l7931"><span class="ln">7931 </span></a> 
<a name="l7932"><span class="ln">7932 </span></a>    The returned :attr:`out` tensor only has values 0 or 1 and is of the same 
<a name="l7933"><span class="ln">7933 </span></a>    shape as :attr:`input`. 
<a name="l7934"><span class="ln">7934 </span></a> 
<a name="l7935"><span class="ln">7935 </span></a>    :attr:`out` can have integral ``dtype``, but :attr:`input` must have floating 
<a name="l7936"><span class="ln">7936 </span></a>    point ``dtype``. 
<a name="l7937"><span class="ln">7937 </span></a> 
<a name="l7938"><span class="ln">7938 </span></a>    Args: 
<a name="l7939"><span class="ln">7939 </span></a>        input (Tensor): the input tensor of probability values for the Bernoulli distribution 
<a name="l7940"><span class="ln">7940 </span></a> 
<a name="l7941"><span class="ln">7941 </span></a>    Keyword args: 
<a name="l7942"><span class="ln">7942 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l7943"><span class="ln">7943 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l7944"><span class="ln">7944 </span></a> 
<a name="l7945"><span class="ln">7945 </span></a>    Example:: 
<a name="l7946"><span class="ln">7946 </span></a> 
<a name="l7947"><span class="ln">7947 </span></a>        &gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1] 
<a name="l7948"><span class="ln">7948 </span></a>        &gt;&gt;&gt; a 
<a name="l7949"><span class="ln">7949 </span></a>        tensor([[ 0.1737,  0.0950,  0.3609], 
<a name="l7950"><span class="ln">7950 </span></a>                [ 0.7148,  0.0289,  0.2676], 
<a name="l7951"><span class="ln">7951 </span></a>                [ 0.9456,  0.8937,  0.7202]]) 
<a name="l7952"><span class="ln">7952 </span></a>        &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l7953"><span class="ln">7953 </span></a>        tensor([[ 1.,  0.,  0.], 
<a name="l7954"><span class="ln">7954 </span></a>                [ 0.,  0.,  0.], 
<a name="l7955"><span class="ln">7955 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l7956"><span class="ln">7956 </span></a> 
<a name="l7957"><span class="ln">7957 </span></a>        &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1 
<a name="l7958"><span class="ln">7958 </span></a>        &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l7959"><span class="ln">7959 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l7960"><span class="ln">7960 </span></a>                [ 1.,  1.,  1.], 
<a name="l7961"><span class="ln">7961 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l7962"><span class="ln">7962 </span></a>        &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0 
<a name="l7963"><span class="ln">7963 </span></a>        &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l7964"><span class="ln">7964 </span></a>        tensor([[ 0.,  0.,  0.], 
<a name="l7965"><span class="ln">7965 </span></a>                [ 0.,  0.,  0.], 
<a name="l7966"><span class="ln">7966 </span></a>                [ 0.,  0.,  0.]]) 
<a name="l7967"><span class="ln">7967 </span></a>    &quot;&quot;&quot;</span>
<a name="l7968"><span class="ln">7968 </span></a>
<a name="l7969"><span class="ln">7969 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l7970"><span class="ln">7970 </span></a><span class="s2">def </span><span class="s1">bernoulli</span><span class="s3">(</span>
<a name="l7971"><span class="ln">7971 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l7972"><span class="ln">7972 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l7973"><span class="ln">7973 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l7974"><span class="ln">7974 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l7975"><span class="ln">7975 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l7976"><span class="ln">7976 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l7977"><span class="ln">7977 </span></a>    bernoulli(input: Tensor, *, generator: Optional[Generator], out: Optional[Tensor]) -&gt; Tensor 
<a name="l7978"><span class="ln">7978 </span></a> 
<a name="l7979"><span class="ln">7979 </span></a>    Draws binary random numbers (0 or 1) from a Bernoulli distribution. 
<a name="l7980"><span class="ln">7980 </span></a> 
<a name="l7981"><span class="ln">7981 </span></a>    The :attr:`input` tensor should be a tensor containing probabilities 
<a name="l7982"><span class="ln">7982 </span></a>    to be used for drawing the binary random number. 
<a name="l7983"><span class="ln">7983 </span></a>    Hence, all values in :attr:`input` have to be in the range: 
<a name="l7984"><span class="ln">7984 </span></a>    :math:`0 \leq \text{input}_i \leq 1`. 
<a name="l7985"><span class="ln">7985 </span></a> 
<a name="l7986"><span class="ln">7986 </span></a>    The :math:`\text{i}^{th}` element of the output tensor will draw a 
<a name="l7987"><span class="ln">7987 </span></a>    value :math:`1` according to the :math:`\text{i}^{th}` probability value given 
<a name="l7988"><span class="ln">7988 </span></a>    in :attr:`input`. 
<a name="l7989"><span class="ln">7989 </span></a> 
<a name="l7990"><span class="ln">7990 </span></a>    .. math:: 
<a name="l7991"><span class="ln">7991 </span></a>        \text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i}) 
<a name="l7992"><span class="ln">7992 </span></a> 
<a name="l7993"><span class="ln">7993 </span></a>    The returned :attr:`out` tensor only has values 0 or 1 and is of the same 
<a name="l7994"><span class="ln">7994 </span></a>    shape as :attr:`input`. 
<a name="l7995"><span class="ln">7995 </span></a> 
<a name="l7996"><span class="ln">7996 </span></a>    :attr:`out` can have integral ``dtype``, but :attr:`input` must have floating 
<a name="l7997"><span class="ln">7997 </span></a>    point ``dtype``. 
<a name="l7998"><span class="ln">7998 </span></a> 
<a name="l7999"><span class="ln">7999 </span></a>    Args: 
<a name="l8000"><span class="ln">8000 </span></a>        input (Tensor): the input tensor of probability values for the Bernoulli distribution 
<a name="l8001"><span class="ln">8001 </span></a> 
<a name="l8002"><span class="ln">8002 </span></a>    Keyword args: 
<a name="l8003"><span class="ln">8003 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l8004"><span class="ln">8004 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8005"><span class="ln">8005 </span></a> 
<a name="l8006"><span class="ln">8006 </span></a>    Example:: 
<a name="l8007"><span class="ln">8007 </span></a> 
<a name="l8008"><span class="ln">8008 </span></a>        &gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1] 
<a name="l8009"><span class="ln">8009 </span></a>        &gt;&gt;&gt; a 
<a name="l8010"><span class="ln">8010 </span></a>        tensor([[ 0.1737,  0.0950,  0.3609], 
<a name="l8011"><span class="ln">8011 </span></a>                [ 0.7148,  0.0289,  0.2676], 
<a name="l8012"><span class="ln">8012 </span></a>                [ 0.9456,  0.8937,  0.7202]]) 
<a name="l8013"><span class="ln">8013 </span></a>        &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l8014"><span class="ln">8014 </span></a>        tensor([[ 1.,  0.,  0.], 
<a name="l8015"><span class="ln">8015 </span></a>                [ 0.,  0.,  0.], 
<a name="l8016"><span class="ln">8016 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l8017"><span class="ln">8017 </span></a> 
<a name="l8018"><span class="ln">8018 </span></a>        &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1 
<a name="l8019"><span class="ln">8019 </span></a>        &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l8020"><span class="ln">8020 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l8021"><span class="ln">8021 </span></a>                [ 1.,  1.,  1.], 
<a name="l8022"><span class="ln">8022 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l8023"><span class="ln">8023 </span></a>        &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0 
<a name="l8024"><span class="ln">8024 </span></a>        &gt;&gt;&gt; torch.bernoulli(a) 
<a name="l8025"><span class="ln">8025 </span></a>        tensor([[ 0.,  0.,  0.], 
<a name="l8026"><span class="ln">8026 </span></a>                [ 0.,  0.,  0.], 
<a name="l8027"><span class="ln">8027 </span></a>                [ 0.,  0.,  0.]]) 
<a name="l8028"><span class="ln">8028 </span></a>    &quot;&quot;&quot;</span>
<a name="l8029"><span class="ln">8029 </span></a>
<a name="l8030"><span class="ln">8030 </span></a><span class="s2">def </span><span class="s1">bilinear</span><span class="s3">(</span>
<a name="l8031"><span class="ln">8031 </span></a>    <span class="s1">input1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8032"><span class="ln">8032 </span></a>    <span class="s1">input2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8033"><span class="ln">8033 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8034"><span class="ln">8034 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8035"><span class="ln">8035 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l8036"><span class="ln">8036 </span></a><span class="s2">def </span><span class="s1">binary_cross_entropy_with_logits</span><span class="s3">(</span>
<a name="l8037"><span class="ln">8037 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8038"><span class="ln">8038 </span></a>    <span class="s1">target</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8039"><span class="ln">8039 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8040"><span class="ln">8040 </span></a>    <span class="s1">pos_weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8041"><span class="ln">8041 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l8042"><span class="ln">8042 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l8043"><span class="ln">8043 </span></a><span class="s2">def </span><span class="s1">bincount</span><span class="s3">(</span>
<a name="l8044"><span class="ln">8044 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8045"><span class="ln">8045 </span></a>    <span class="s1">weights</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8046"><span class="ln">8046 </span></a>    <span class="s1">minlength</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l8047"><span class="ln">8047 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8048"><span class="ln">8048 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8049"><span class="ln">8049 </span></a>    bincount(input, weights=None, minlength=0) -&gt; Tensor 
<a name="l8050"><span class="ln">8050 </span></a> 
<a name="l8051"><span class="ln">8051 </span></a>    Count the frequency of each value in an array of non-negative ints. 
<a name="l8052"><span class="ln">8052 </span></a> 
<a name="l8053"><span class="ln">8053 </span></a>    The number of bins (size 1) is one larger than the largest value in 
<a name="l8054"><span class="ln">8054 </span></a>    :attr:`input` unless :attr:`input` is empty, in which case the result is a 
<a name="l8055"><span class="ln">8055 </span></a>    tensor of size 0. If :attr:`minlength` is specified, the number of bins is at least 
<a name="l8056"><span class="ln">8056 </span></a>    :attr:`minlength` and if :attr:`input` is empty, then the result is tensor of size 
<a name="l8057"><span class="ln">8057 </span></a>    :attr:`minlength` filled with zeros. If ``n`` is the value at position ``i``, 
<a name="l8058"><span class="ln">8058 </span></a>    ``out[n] += weights[i]`` if :attr:`weights` is specified else 
<a name="l8059"><span class="ln">8059 </span></a>    ``out[n] += 1``. 
<a name="l8060"><span class="ln">8060 </span></a> 
<a name="l8061"><span class="ln">8061 </span></a>    Note: 
<a name="l8062"><span class="ln">8062 </span></a>        This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information. 
<a name="l8063"><span class="ln">8063 </span></a> 
<a name="l8064"><span class="ln">8064 </span></a>    Arguments: 
<a name="l8065"><span class="ln">8065 </span></a>        input (Tensor): 1-d int tensor 
<a name="l8066"><span class="ln">8066 </span></a>        weights (Tensor): optional, weight for each value in the input tensor. 
<a name="l8067"><span class="ln">8067 </span></a>            Should be of same size as input tensor. 
<a name="l8068"><span class="ln">8068 </span></a>        minlength (int): optional, minimum number of bins. Should be non-negative. 
<a name="l8069"><span class="ln">8069 </span></a> 
<a name="l8070"><span class="ln">8070 </span></a>    Returns: 
<a name="l8071"><span class="ln">8071 </span></a>        output (Tensor): a tensor of shape ``Size([max(input) + 1])`` if 
<a name="l8072"><span class="ln">8072 </span></a>        :attr:`input` is non-empty, else ``Size(0)`` 
<a name="l8073"><span class="ln">8073 </span></a> 
<a name="l8074"><span class="ln">8074 </span></a>    Example:: 
<a name="l8075"><span class="ln">8075 </span></a> 
<a name="l8076"><span class="ln">8076 </span></a>        &gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64) 
<a name="l8077"><span class="ln">8077 </span></a>        &gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5) 
<a name="l8078"><span class="ln">8078 </span></a>        &gt;&gt;&gt; input, weights 
<a name="l8079"><span class="ln">8079 </span></a>        (tensor([4, 3, 6, 3, 4]), 
<a name="l8080"><span class="ln">8080 </span></a>         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000]) 
<a name="l8081"><span class="ln">8081 </span></a> 
<a name="l8082"><span class="ln">8082 </span></a>        &gt;&gt;&gt; torch.bincount(input) 
<a name="l8083"><span class="ln">8083 </span></a>        tensor([0, 0, 0, 2, 2, 0, 1]) 
<a name="l8084"><span class="ln">8084 </span></a> 
<a name="l8085"><span class="ln">8085 </span></a>        &gt;&gt;&gt; input.bincount(weights) 
<a name="l8086"><span class="ln">8086 </span></a>        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000]) 
<a name="l8087"><span class="ln">8087 </span></a>    &quot;&quot;&quot;</span>
<a name="l8088"><span class="ln">8088 </span></a>
<a name="l8089"><span class="ln">8089 </span></a><span class="s2">def </span><span class="s1">binomial</span><span class="s3">(</span>
<a name="l8090"><span class="ln">8090 </span></a>    <span class="s1">count</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8091"><span class="ln">8091 </span></a>    <span class="s1">prob</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8092"><span class="ln">8092 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8093"><span class="ln">8093 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l8094"><span class="ln">8094 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8095"><span class="ln">8095 </span></a><span class="s2">def </span><span class="s1">bitwise_and</span><span class="s3">(</span>
<a name="l8096"><span class="ln">8096 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8097"><span class="ln">8097 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8098"><span class="ln">8098 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8099"><span class="ln">8099 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8100"><span class="ln">8100 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8101"><span class="ln">8101 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8102"><span class="ln">8102 </span></a>    bitwise_and(input, other, *, out=None) -&gt; Tensor 
<a name="l8103"><span class="ln">8103 </span></a> 
<a name="l8104"><span class="ln">8104 </span></a>    Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8105"><span class="ln">8105 </span></a>    integral or Boolean types. For bool tensors, it computes the logical AND. 
<a name="l8106"><span class="ln">8106 </span></a> 
<a name="l8107"><span class="ln">8107 </span></a>    Args: 
<a name="l8108"><span class="ln">8108 </span></a>        input: the first input tensor 
<a name="l8109"><span class="ln">8109 </span></a>        other: the second input tensor 
<a name="l8110"><span class="ln">8110 </span></a> 
<a name="l8111"><span class="ln">8111 </span></a>    Keyword args: 
<a name="l8112"><span class="ln">8112 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8113"><span class="ln">8113 </span></a> 
<a name="l8114"><span class="ln">8114 </span></a>    Example:: 
<a name="l8115"><span class="ln">8115 </span></a> 
<a name="l8116"><span class="ln">8116 </span></a>        &gt;&gt;&gt; torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8117"><span class="ln">8117 </span></a>        tensor([1, 0,  3], dtype=torch.int8) 
<a name="l8118"><span class="ln">8118 </span></a>        &gt;&gt;&gt; torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8119"><span class="ln">8119 </span></a>        tensor([ False, True, False]) 
<a name="l8120"><span class="ln">8120 </span></a>    &quot;&quot;&quot;</span>
<a name="l8121"><span class="ln">8121 </span></a>
<a name="l8122"><span class="ln">8122 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8123"><span class="ln">8123 </span></a><span class="s2">def </span><span class="s1">bitwise_and</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8124"><span class="ln">8124 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8125"><span class="ln">8125 </span></a>    bitwise_and(input, other, *, out=None) -&gt; Tensor 
<a name="l8126"><span class="ln">8126 </span></a> 
<a name="l8127"><span class="ln">8127 </span></a>    Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8128"><span class="ln">8128 </span></a>    integral or Boolean types. For bool tensors, it computes the logical AND. 
<a name="l8129"><span class="ln">8129 </span></a> 
<a name="l8130"><span class="ln">8130 </span></a>    Args: 
<a name="l8131"><span class="ln">8131 </span></a>        input: the first input tensor 
<a name="l8132"><span class="ln">8132 </span></a>        other: the second input tensor 
<a name="l8133"><span class="ln">8133 </span></a> 
<a name="l8134"><span class="ln">8134 </span></a>    Keyword args: 
<a name="l8135"><span class="ln">8135 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8136"><span class="ln">8136 </span></a> 
<a name="l8137"><span class="ln">8137 </span></a>    Example:: 
<a name="l8138"><span class="ln">8138 </span></a> 
<a name="l8139"><span class="ln">8139 </span></a>        &gt;&gt;&gt; torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8140"><span class="ln">8140 </span></a>        tensor([1, 0,  3], dtype=torch.int8) 
<a name="l8141"><span class="ln">8141 </span></a>        &gt;&gt;&gt; torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8142"><span class="ln">8142 </span></a>        tensor([ False, True, False]) 
<a name="l8143"><span class="ln">8143 </span></a>    &quot;&quot;&quot;</span>
<a name="l8144"><span class="ln">8144 </span></a>
<a name="l8145"><span class="ln">8145 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8146"><span class="ln">8146 </span></a><span class="s2">def </span><span class="s1">bitwise_and</span><span class="s3">(</span>
<a name="l8147"><span class="ln">8147 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8148"><span class="ln">8148 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l8149"><span class="ln">8149 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8150"><span class="ln">8150 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8151"><span class="ln">8151 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8152"><span class="ln">8152 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8153"><span class="ln">8153 </span></a>    bitwise_and(input, other, *, out=None) -&gt; Tensor 
<a name="l8154"><span class="ln">8154 </span></a> 
<a name="l8155"><span class="ln">8155 </span></a>    Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8156"><span class="ln">8156 </span></a>    integral or Boolean types. For bool tensors, it computes the logical AND. 
<a name="l8157"><span class="ln">8157 </span></a> 
<a name="l8158"><span class="ln">8158 </span></a>    Args: 
<a name="l8159"><span class="ln">8159 </span></a>        input: the first input tensor 
<a name="l8160"><span class="ln">8160 </span></a>        other: the second input tensor 
<a name="l8161"><span class="ln">8161 </span></a> 
<a name="l8162"><span class="ln">8162 </span></a>    Keyword args: 
<a name="l8163"><span class="ln">8163 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8164"><span class="ln">8164 </span></a> 
<a name="l8165"><span class="ln">8165 </span></a>    Example:: 
<a name="l8166"><span class="ln">8166 </span></a> 
<a name="l8167"><span class="ln">8167 </span></a>        &gt;&gt;&gt; torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8168"><span class="ln">8168 </span></a>        tensor([1, 0,  3], dtype=torch.int8) 
<a name="l8169"><span class="ln">8169 </span></a>        &gt;&gt;&gt; torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8170"><span class="ln">8170 </span></a>        tensor([ False, True, False]) 
<a name="l8171"><span class="ln">8171 </span></a>    &quot;&quot;&quot;</span>
<a name="l8172"><span class="ln">8172 </span></a>
<a name="l8173"><span class="ln">8173 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8174"><span class="ln">8174 </span></a><span class="s2">def </span><span class="s1">bitwise_left_shift</span><span class="s3">(</span>
<a name="l8175"><span class="ln">8175 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8176"><span class="ln">8176 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8177"><span class="ln">8177 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8178"><span class="ln">8178 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8179"><span class="ln">8179 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8180"><span class="ln">8180 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8181"><span class="ln">8181 </span></a>    bitwise_left_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l8182"><span class="ln">8182 </span></a> 
<a name="l8183"><span class="ln">8183 </span></a>    Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l8184"><span class="ln">8184 </span></a>    The input tensor must be of integral type. This operator supports 
<a name="l8185"><span class="ln">8185 </span></a>    :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l8186"><span class="ln">8186 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l8187"><span class="ln">8187 </span></a> 
<a name="l8188"><span class="ln">8188 </span></a>    The operation applied is: 
<a name="l8189"><span class="ln">8189 </span></a> 
<a name="l8190"><span class="ln">8190 </span></a>    .. math:: 
<a name="l8191"><span class="ln">8191 </span></a>        \text{out}_i = \text{input}_i &lt;&lt; \text{other}_i 
<a name="l8192"><span class="ln">8192 </span></a> 
<a name="l8193"><span class="ln">8193 </span></a>    Args: 
<a name="l8194"><span class="ln">8194 </span></a>        input (Tensor or Scalar): the first input tensor 
<a name="l8195"><span class="ln">8195 </span></a>        other (Tensor or Scalar): the second input tensor 
<a name="l8196"><span class="ln">8196 </span></a> 
<a name="l8197"><span class="ln">8197 </span></a>    Keyword args: 
<a name="l8198"><span class="ln">8198 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8199"><span class="ln">8199 </span></a> 
<a name="l8200"><span class="ln">8200 </span></a>    Example:: 
<a name="l8201"><span class="ln">8201 </span></a> 
<a name="l8202"><span class="ln">8202 </span></a>        &gt;&gt;&gt; torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8203"><span class="ln">8203 </span></a>        tensor([-2, -2, 24], dtype=torch.int8) 
<a name="l8204"><span class="ln">8204 </span></a>    &quot;&quot;&quot;</span>
<a name="l8205"><span class="ln">8205 </span></a>
<a name="l8206"><span class="ln">8206 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8207"><span class="ln">8207 </span></a><span class="s2">def </span><span class="s1">bitwise_left_shift</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8208"><span class="ln">8208 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8209"><span class="ln">8209 </span></a>    bitwise_left_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l8210"><span class="ln">8210 </span></a> 
<a name="l8211"><span class="ln">8211 </span></a>    Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l8212"><span class="ln">8212 </span></a>    The input tensor must be of integral type. This operator supports 
<a name="l8213"><span class="ln">8213 </span></a>    :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l8214"><span class="ln">8214 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l8215"><span class="ln">8215 </span></a> 
<a name="l8216"><span class="ln">8216 </span></a>    The operation applied is: 
<a name="l8217"><span class="ln">8217 </span></a> 
<a name="l8218"><span class="ln">8218 </span></a>    .. math:: 
<a name="l8219"><span class="ln">8219 </span></a>        \text{out}_i = \text{input}_i &lt;&lt; \text{other}_i 
<a name="l8220"><span class="ln">8220 </span></a> 
<a name="l8221"><span class="ln">8221 </span></a>    Args: 
<a name="l8222"><span class="ln">8222 </span></a>        input (Tensor or Scalar): the first input tensor 
<a name="l8223"><span class="ln">8223 </span></a>        other (Tensor or Scalar): the second input tensor 
<a name="l8224"><span class="ln">8224 </span></a> 
<a name="l8225"><span class="ln">8225 </span></a>    Keyword args: 
<a name="l8226"><span class="ln">8226 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8227"><span class="ln">8227 </span></a> 
<a name="l8228"><span class="ln">8228 </span></a>    Example:: 
<a name="l8229"><span class="ln">8229 </span></a> 
<a name="l8230"><span class="ln">8230 </span></a>        &gt;&gt;&gt; torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8231"><span class="ln">8231 </span></a>        tensor([-2, -2, 24], dtype=torch.int8) 
<a name="l8232"><span class="ln">8232 </span></a>    &quot;&quot;&quot;</span>
<a name="l8233"><span class="ln">8233 </span></a>
<a name="l8234"><span class="ln">8234 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8235"><span class="ln">8235 </span></a><span class="s2">def </span><span class="s1">bitwise_left_shift</span><span class="s3">(</span>
<a name="l8236"><span class="ln">8236 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8237"><span class="ln">8237 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l8238"><span class="ln">8238 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8239"><span class="ln">8239 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8240"><span class="ln">8240 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8241"><span class="ln">8241 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8242"><span class="ln">8242 </span></a>    bitwise_left_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l8243"><span class="ln">8243 </span></a> 
<a name="l8244"><span class="ln">8244 </span></a>    Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l8245"><span class="ln">8245 </span></a>    The input tensor must be of integral type. This operator supports 
<a name="l8246"><span class="ln">8246 </span></a>    :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l8247"><span class="ln">8247 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l8248"><span class="ln">8248 </span></a> 
<a name="l8249"><span class="ln">8249 </span></a>    The operation applied is: 
<a name="l8250"><span class="ln">8250 </span></a> 
<a name="l8251"><span class="ln">8251 </span></a>    .. math:: 
<a name="l8252"><span class="ln">8252 </span></a>        \text{out}_i = \text{input}_i &lt;&lt; \text{other}_i 
<a name="l8253"><span class="ln">8253 </span></a> 
<a name="l8254"><span class="ln">8254 </span></a>    Args: 
<a name="l8255"><span class="ln">8255 </span></a>        input (Tensor or Scalar): the first input tensor 
<a name="l8256"><span class="ln">8256 </span></a>        other (Tensor or Scalar): the second input tensor 
<a name="l8257"><span class="ln">8257 </span></a> 
<a name="l8258"><span class="ln">8258 </span></a>    Keyword args: 
<a name="l8259"><span class="ln">8259 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8260"><span class="ln">8260 </span></a> 
<a name="l8261"><span class="ln">8261 </span></a>    Example:: 
<a name="l8262"><span class="ln">8262 </span></a> 
<a name="l8263"><span class="ln">8263 </span></a>        &gt;&gt;&gt; torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8264"><span class="ln">8264 </span></a>        tensor([-2, -2, 24], dtype=torch.int8) 
<a name="l8265"><span class="ln">8265 </span></a>    &quot;&quot;&quot;</span>
<a name="l8266"><span class="ln">8266 </span></a>
<a name="l8267"><span class="ln">8267 </span></a><span class="s2">def </span><span class="s1">bitwise_not</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8268"><span class="ln">8268 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8269"><span class="ln">8269 </span></a>    bitwise_not(input, *, out=None) -&gt; Tensor 
<a name="l8270"><span class="ln">8270 </span></a> 
<a name="l8271"><span class="ln">8271 </span></a>    Computes the bitwise NOT of the given input tensor. The input tensor must be of 
<a name="l8272"><span class="ln">8272 </span></a>    integral or Boolean types. For bool tensors, it computes the logical NOT. 
<a name="l8273"><span class="ln">8273 </span></a> 
<a name="l8274"><span class="ln">8274 </span></a>    Args: 
<a name="l8275"><span class="ln">8275 </span></a>        input (Tensor): the input tensor. 
<a name="l8276"><span class="ln">8276 </span></a> 
<a name="l8277"><span class="ln">8277 </span></a>    Keyword args: 
<a name="l8278"><span class="ln">8278 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8279"><span class="ln">8279 </span></a> 
<a name="l8280"><span class="ln">8280 </span></a>    Example:: 
<a name="l8281"><span class="ln">8281 </span></a> 
<a name="l8282"><span class="ln">8282 </span></a>        &gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8)) 
<a name="l8283"><span class="ln">8283 </span></a>        tensor([ 0,  1, -4], dtype=torch.int8) 
<a name="l8284"><span class="ln">8284 </span></a>    &quot;&quot;&quot;</span>
<a name="l8285"><span class="ln">8285 </span></a>
<a name="l8286"><span class="ln">8286 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8287"><span class="ln">8287 </span></a><span class="s2">def </span><span class="s1">bitwise_or</span><span class="s3">(</span>
<a name="l8288"><span class="ln">8288 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8289"><span class="ln">8289 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8290"><span class="ln">8290 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8291"><span class="ln">8291 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8292"><span class="ln">8292 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8293"><span class="ln">8293 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8294"><span class="ln">8294 </span></a>    bitwise_or(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l8295"><span class="ln">8295 </span></a> 
<a name="l8296"><span class="ln">8296 </span></a>    Computes the bitwise OR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8297"><span class="ln">8297 </span></a>    integral or Boolean types. For bool tensors, it computes the logical OR. 
<a name="l8298"><span class="ln">8298 </span></a> 
<a name="l8299"><span class="ln">8299 </span></a>    Args: 
<a name="l8300"><span class="ln">8300 </span></a>        input: the first input tensor 
<a name="l8301"><span class="ln">8301 </span></a>        other: the second input tensor 
<a name="l8302"><span class="ln">8302 </span></a> 
<a name="l8303"><span class="ln">8303 </span></a>    Keyword args: 
<a name="l8304"><span class="ln">8304 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8305"><span class="ln">8305 </span></a> 
<a name="l8306"><span class="ln">8306 </span></a>    Example:: 
<a name="l8307"><span class="ln">8307 </span></a> 
<a name="l8308"><span class="ln">8308 </span></a>        &gt;&gt;&gt; torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8309"><span class="ln">8309 </span></a>        tensor([-1, -2,  3], dtype=torch.int8) 
<a name="l8310"><span class="ln">8310 </span></a>        &gt;&gt;&gt; torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8311"><span class="ln">8311 </span></a>        tensor([ True, True, False]) 
<a name="l8312"><span class="ln">8312 </span></a>    &quot;&quot;&quot;</span>
<a name="l8313"><span class="ln">8313 </span></a>
<a name="l8314"><span class="ln">8314 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8315"><span class="ln">8315 </span></a><span class="s2">def </span><span class="s1">bitwise_or</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8316"><span class="ln">8316 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8317"><span class="ln">8317 </span></a>    bitwise_or(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l8318"><span class="ln">8318 </span></a> 
<a name="l8319"><span class="ln">8319 </span></a>    Computes the bitwise OR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8320"><span class="ln">8320 </span></a>    integral or Boolean types. For bool tensors, it computes the logical OR. 
<a name="l8321"><span class="ln">8321 </span></a> 
<a name="l8322"><span class="ln">8322 </span></a>    Args: 
<a name="l8323"><span class="ln">8323 </span></a>        input: the first input tensor 
<a name="l8324"><span class="ln">8324 </span></a>        other: the second input tensor 
<a name="l8325"><span class="ln">8325 </span></a> 
<a name="l8326"><span class="ln">8326 </span></a>    Keyword args: 
<a name="l8327"><span class="ln">8327 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8328"><span class="ln">8328 </span></a> 
<a name="l8329"><span class="ln">8329 </span></a>    Example:: 
<a name="l8330"><span class="ln">8330 </span></a> 
<a name="l8331"><span class="ln">8331 </span></a>        &gt;&gt;&gt; torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8332"><span class="ln">8332 </span></a>        tensor([-1, -2,  3], dtype=torch.int8) 
<a name="l8333"><span class="ln">8333 </span></a>        &gt;&gt;&gt; torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8334"><span class="ln">8334 </span></a>        tensor([ True, True, False]) 
<a name="l8335"><span class="ln">8335 </span></a>    &quot;&quot;&quot;</span>
<a name="l8336"><span class="ln">8336 </span></a>
<a name="l8337"><span class="ln">8337 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8338"><span class="ln">8338 </span></a><span class="s2">def </span><span class="s1">bitwise_or</span><span class="s3">(</span>
<a name="l8339"><span class="ln">8339 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8340"><span class="ln">8340 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l8341"><span class="ln">8341 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8342"><span class="ln">8342 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8343"><span class="ln">8343 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8344"><span class="ln">8344 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8345"><span class="ln">8345 </span></a>    bitwise_or(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l8346"><span class="ln">8346 </span></a> 
<a name="l8347"><span class="ln">8347 </span></a>    Computes the bitwise OR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8348"><span class="ln">8348 </span></a>    integral or Boolean types. For bool tensors, it computes the logical OR. 
<a name="l8349"><span class="ln">8349 </span></a> 
<a name="l8350"><span class="ln">8350 </span></a>    Args: 
<a name="l8351"><span class="ln">8351 </span></a>        input: the first input tensor 
<a name="l8352"><span class="ln">8352 </span></a>        other: the second input tensor 
<a name="l8353"><span class="ln">8353 </span></a> 
<a name="l8354"><span class="ln">8354 </span></a>    Keyword args: 
<a name="l8355"><span class="ln">8355 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8356"><span class="ln">8356 </span></a> 
<a name="l8357"><span class="ln">8357 </span></a>    Example:: 
<a name="l8358"><span class="ln">8358 </span></a> 
<a name="l8359"><span class="ln">8359 </span></a>        &gt;&gt;&gt; torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8360"><span class="ln">8360 </span></a>        tensor([-1, -2,  3], dtype=torch.int8) 
<a name="l8361"><span class="ln">8361 </span></a>        &gt;&gt;&gt; torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8362"><span class="ln">8362 </span></a>        tensor([ True, True, False]) 
<a name="l8363"><span class="ln">8363 </span></a>    &quot;&quot;&quot;</span>
<a name="l8364"><span class="ln">8364 </span></a>
<a name="l8365"><span class="ln">8365 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8366"><span class="ln">8366 </span></a><span class="s2">def </span><span class="s1">bitwise_right_shift</span><span class="s3">(</span>
<a name="l8367"><span class="ln">8367 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8368"><span class="ln">8368 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8369"><span class="ln">8369 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8370"><span class="ln">8370 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8371"><span class="ln">8371 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8372"><span class="ln">8372 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8373"><span class="ln">8373 </span></a>    bitwise_right_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l8374"><span class="ln">8374 </span></a> 
<a name="l8375"><span class="ln">8375 </span></a>    Computes the right arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l8376"><span class="ln">8376 </span></a>    The input tensor must be of integral type. This operator supports 
<a name="l8377"><span class="ln">8377 </span></a>    :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l8378"><span class="ln">8378 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l8379"><span class="ln">8379 </span></a>    In any case, if the value of the right operand is negative or is greater 
<a name="l8380"><span class="ln">8380 </span></a>    or equal to the number of bits in the promoted left operand, the behavior is undefined. 
<a name="l8381"><span class="ln">8381 </span></a> 
<a name="l8382"><span class="ln">8382 </span></a>    The operation applied is: 
<a name="l8383"><span class="ln">8383 </span></a> 
<a name="l8384"><span class="ln">8384 </span></a>    .. math:: 
<a name="l8385"><span class="ln">8385 </span></a>        \text{out}_i = \text{input}_i &gt;&gt; \text{other}_i 
<a name="l8386"><span class="ln">8386 </span></a> 
<a name="l8387"><span class="ln">8387 </span></a>    Args: 
<a name="l8388"><span class="ln">8388 </span></a>        input (Tensor or Scalar): the first input tensor 
<a name="l8389"><span class="ln">8389 </span></a>        other (Tensor or Scalar): the second input tensor 
<a name="l8390"><span class="ln">8390 </span></a> 
<a name="l8391"><span class="ln">8391 </span></a>    Keyword args: 
<a name="l8392"><span class="ln">8392 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8393"><span class="ln">8393 </span></a> 
<a name="l8394"><span class="ln">8394 </span></a>    Example:: 
<a name="l8395"><span class="ln">8395 </span></a> 
<a name="l8396"><span class="ln">8396 </span></a>        &gt;&gt;&gt; torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8397"><span class="ln">8397 </span></a>        tensor([-1, -7,  3], dtype=torch.int8) 
<a name="l8398"><span class="ln">8398 </span></a>    &quot;&quot;&quot;</span>
<a name="l8399"><span class="ln">8399 </span></a>
<a name="l8400"><span class="ln">8400 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8401"><span class="ln">8401 </span></a><span class="s2">def </span><span class="s1">bitwise_right_shift</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8402"><span class="ln">8402 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8403"><span class="ln">8403 </span></a>    bitwise_right_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l8404"><span class="ln">8404 </span></a> 
<a name="l8405"><span class="ln">8405 </span></a>    Computes the right arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l8406"><span class="ln">8406 </span></a>    The input tensor must be of integral type. This operator supports 
<a name="l8407"><span class="ln">8407 </span></a>    :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l8408"><span class="ln">8408 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l8409"><span class="ln">8409 </span></a>    In any case, if the value of the right operand is negative or is greater 
<a name="l8410"><span class="ln">8410 </span></a>    or equal to the number of bits in the promoted left operand, the behavior is undefined. 
<a name="l8411"><span class="ln">8411 </span></a> 
<a name="l8412"><span class="ln">8412 </span></a>    The operation applied is: 
<a name="l8413"><span class="ln">8413 </span></a> 
<a name="l8414"><span class="ln">8414 </span></a>    .. math:: 
<a name="l8415"><span class="ln">8415 </span></a>        \text{out}_i = \text{input}_i &gt;&gt; \text{other}_i 
<a name="l8416"><span class="ln">8416 </span></a> 
<a name="l8417"><span class="ln">8417 </span></a>    Args: 
<a name="l8418"><span class="ln">8418 </span></a>        input (Tensor or Scalar): the first input tensor 
<a name="l8419"><span class="ln">8419 </span></a>        other (Tensor or Scalar): the second input tensor 
<a name="l8420"><span class="ln">8420 </span></a> 
<a name="l8421"><span class="ln">8421 </span></a>    Keyword args: 
<a name="l8422"><span class="ln">8422 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8423"><span class="ln">8423 </span></a> 
<a name="l8424"><span class="ln">8424 </span></a>    Example:: 
<a name="l8425"><span class="ln">8425 </span></a> 
<a name="l8426"><span class="ln">8426 </span></a>        &gt;&gt;&gt; torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8427"><span class="ln">8427 </span></a>        tensor([-1, -7,  3], dtype=torch.int8) 
<a name="l8428"><span class="ln">8428 </span></a>    &quot;&quot;&quot;</span>
<a name="l8429"><span class="ln">8429 </span></a>
<a name="l8430"><span class="ln">8430 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8431"><span class="ln">8431 </span></a><span class="s2">def </span><span class="s1">bitwise_right_shift</span><span class="s3">(</span>
<a name="l8432"><span class="ln">8432 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8433"><span class="ln">8433 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l8434"><span class="ln">8434 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8435"><span class="ln">8435 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8436"><span class="ln">8436 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8437"><span class="ln">8437 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8438"><span class="ln">8438 </span></a>    bitwise_right_shift(input, other, *, out=None) -&gt; Tensor 
<a name="l8439"><span class="ln">8439 </span></a> 
<a name="l8440"><span class="ln">8440 </span></a>    Computes the right arithmetic shift of :attr:`input` by :attr:`other` bits. 
<a name="l8441"><span class="ln">8441 </span></a>    The input tensor must be of integral type. This operator supports 
<a name="l8442"><span class="ln">8442 </span></a>    :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;` and 
<a name="l8443"><span class="ln">8443 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l8444"><span class="ln">8444 </span></a>    In any case, if the value of the right operand is negative or is greater 
<a name="l8445"><span class="ln">8445 </span></a>    or equal to the number of bits in the promoted left operand, the behavior is undefined. 
<a name="l8446"><span class="ln">8446 </span></a> 
<a name="l8447"><span class="ln">8447 </span></a>    The operation applied is: 
<a name="l8448"><span class="ln">8448 </span></a> 
<a name="l8449"><span class="ln">8449 </span></a>    .. math:: 
<a name="l8450"><span class="ln">8450 </span></a>        \text{out}_i = \text{input}_i &gt;&gt; \text{other}_i 
<a name="l8451"><span class="ln">8451 </span></a> 
<a name="l8452"><span class="ln">8452 </span></a>    Args: 
<a name="l8453"><span class="ln">8453 </span></a>        input (Tensor or Scalar): the first input tensor 
<a name="l8454"><span class="ln">8454 </span></a>        other (Tensor or Scalar): the second input tensor 
<a name="l8455"><span class="ln">8455 </span></a> 
<a name="l8456"><span class="ln">8456 </span></a>    Keyword args: 
<a name="l8457"><span class="ln">8457 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8458"><span class="ln">8458 </span></a> 
<a name="l8459"><span class="ln">8459 </span></a>    Example:: 
<a name="l8460"><span class="ln">8460 </span></a> 
<a name="l8461"><span class="ln">8461 </span></a>        &gt;&gt;&gt; torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8462"><span class="ln">8462 </span></a>        tensor([-1, -7,  3], dtype=torch.int8) 
<a name="l8463"><span class="ln">8463 </span></a>    &quot;&quot;&quot;</span>
<a name="l8464"><span class="ln">8464 </span></a>
<a name="l8465"><span class="ln">8465 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8466"><span class="ln">8466 </span></a><span class="s2">def </span><span class="s1">bitwise_xor</span><span class="s3">(</span>
<a name="l8467"><span class="ln">8467 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8468"><span class="ln">8468 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8469"><span class="ln">8469 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8470"><span class="ln">8470 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8471"><span class="ln">8471 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8472"><span class="ln">8472 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8473"><span class="ln">8473 </span></a>    bitwise_xor(input, other, *, out=None) -&gt; Tensor 
<a name="l8474"><span class="ln">8474 </span></a> 
<a name="l8475"><span class="ln">8475 </span></a>    Computes the bitwise XOR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8476"><span class="ln">8476 </span></a>    integral or Boolean types. For bool tensors, it computes the logical XOR. 
<a name="l8477"><span class="ln">8477 </span></a> 
<a name="l8478"><span class="ln">8478 </span></a>    Args: 
<a name="l8479"><span class="ln">8479 </span></a>        input: the first input tensor 
<a name="l8480"><span class="ln">8480 </span></a>        other: the second input tensor 
<a name="l8481"><span class="ln">8481 </span></a> 
<a name="l8482"><span class="ln">8482 </span></a>    Keyword args: 
<a name="l8483"><span class="ln">8483 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8484"><span class="ln">8484 </span></a> 
<a name="l8485"><span class="ln">8485 </span></a>    Example:: 
<a name="l8486"><span class="ln">8486 </span></a> 
<a name="l8487"><span class="ln">8487 </span></a>        &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8488"><span class="ln">8488 </span></a>        tensor([-2, -2,  0], dtype=torch.int8) 
<a name="l8489"><span class="ln">8489 </span></a>        &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8490"><span class="ln">8490 </span></a>        tensor([ True, False, False]) 
<a name="l8491"><span class="ln">8491 </span></a>    &quot;&quot;&quot;</span>
<a name="l8492"><span class="ln">8492 </span></a>
<a name="l8493"><span class="ln">8493 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8494"><span class="ln">8494 </span></a><span class="s2">def </span><span class="s1">bitwise_xor</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8495"><span class="ln">8495 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8496"><span class="ln">8496 </span></a>    bitwise_xor(input, other, *, out=None) -&gt; Tensor 
<a name="l8497"><span class="ln">8497 </span></a> 
<a name="l8498"><span class="ln">8498 </span></a>    Computes the bitwise XOR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8499"><span class="ln">8499 </span></a>    integral or Boolean types. For bool tensors, it computes the logical XOR. 
<a name="l8500"><span class="ln">8500 </span></a> 
<a name="l8501"><span class="ln">8501 </span></a>    Args: 
<a name="l8502"><span class="ln">8502 </span></a>        input: the first input tensor 
<a name="l8503"><span class="ln">8503 </span></a>        other: the second input tensor 
<a name="l8504"><span class="ln">8504 </span></a> 
<a name="l8505"><span class="ln">8505 </span></a>    Keyword args: 
<a name="l8506"><span class="ln">8506 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8507"><span class="ln">8507 </span></a> 
<a name="l8508"><span class="ln">8508 </span></a>    Example:: 
<a name="l8509"><span class="ln">8509 </span></a> 
<a name="l8510"><span class="ln">8510 </span></a>        &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8511"><span class="ln">8511 </span></a>        tensor([-2, -2,  0], dtype=torch.int8) 
<a name="l8512"><span class="ln">8512 </span></a>        &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8513"><span class="ln">8513 </span></a>        tensor([ True, False, False]) 
<a name="l8514"><span class="ln">8514 </span></a>    &quot;&quot;&quot;</span>
<a name="l8515"><span class="ln">8515 </span></a>
<a name="l8516"><span class="ln">8516 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8517"><span class="ln">8517 </span></a><span class="s2">def </span><span class="s1">bitwise_xor</span><span class="s3">(</span>
<a name="l8518"><span class="ln">8518 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8519"><span class="ln">8519 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l8520"><span class="ln">8520 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8521"><span class="ln">8521 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8522"><span class="ln">8522 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8523"><span class="ln">8523 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8524"><span class="ln">8524 </span></a>    bitwise_xor(input, other, *, out=None) -&gt; Tensor 
<a name="l8525"><span class="ln">8525 </span></a> 
<a name="l8526"><span class="ln">8526 </span></a>    Computes the bitwise XOR of :attr:`input` and :attr:`other`. The input tensor must be of 
<a name="l8527"><span class="ln">8527 </span></a>    integral or Boolean types. For bool tensors, it computes the logical XOR. 
<a name="l8528"><span class="ln">8528 </span></a> 
<a name="l8529"><span class="ln">8529 </span></a>    Args: 
<a name="l8530"><span class="ln">8530 </span></a>        input: the first input tensor 
<a name="l8531"><span class="ln">8531 </span></a>        other: the second input tensor 
<a name="l8532"><span class="ln">8532 </span></a> 
<a name="l8533"><span class="ln">8533 </span></a>    Keyword args: 
<a name="l8534"><span class="ln">8534 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8535"><span class="ln">8535 </span></a> 
<a name="l8536"><span class="ln">8536 </span></a>    Example:: 
<a name="l8537"><span class="ln">8537 </span></a> 
<a name="l8538"><span class="ln">8538 </span></a>        &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) 
<a name="l8539"><span class="ln">8539 </span></a>        tensor([-2, -2,  0], dtype=torch.int8) 
<a name="l8540"><span class="ln">8540 </span></a>        &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False])) 
<a name="l8541"><span class="ln">8541 </span></a>        tensor([ True, False, False]) 
<a name="l8542"><span class="ln">8542 </span></a>    &quot;&quot;&quot;</span>
<a name="l8543"><span class="ln">8543 </span></a>
<a name="l8544"><span class="ln">8544 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8545"><span class="ln">8545 </span></a><span class="s2">def </span><span class="s1">blackman_window</span><span class="s3">(</span>
<a name="l8546"><span class="ln">8546 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l8547"><span class="ln">8547 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8548"><span class="ln">8548 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8549"><span class="ln">8549 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8550"><span class="ln">8550 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8551"><span class="ln">8551 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l8552"><span class="ln">8552 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l8553"><span class="ln">8553 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8554"><span class="ln">8554 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8555"><span class="ln">8555 </span></a>    blackman_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l8556"><span class="ln">8556 </span></a> 
<a name="l8557"><span class="ln">8557 </span></a>    Blackman window function. 
<a name="l8558"><span class="ln">8558 </span></a> 
<a name="l8559"><span class="ln">8559 </span></a>    .. math:: 
<a name="l8560"><span class="ln">8560 </span></a>        w[n] = 0.42 - 0.5 \cos \left( \frac{2 \pi n}{N - 1} \right) + 0.08 \cos \left( \frac{4 \pi n}{N - 1} \right) 
<a name="l8561"><span class="ln">8561 </span></a> 
<a name="l8562"><span class="ln">8562 </span></a>    where :math:`N` is the full window size. 
<a name="l8563"><span class="ln">8563 </span></a> 
<a name="l8564"><span class="ln">8564 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l8565"><span class="ln">8565 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l8566"><span class="ln">8566 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l8567"><span class="ln">8567 </span></a>    ready to be used as a periodic window with functions like 
<a name="l8568"><span class="ln">8568 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l8569"><span class="ln">8569 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l8570"><span class="ln">8570 </span></a>    ``torch.blackman_window(L, periodic=True)`` equal to 
<a name="l8571"><span class="ln">8571 </span></a>    ``torch.blackman_window(L + 1, periodic=False)[:-1]``. 
<a name="l8572"><span class="ln">8572 </span></a> 
<a name="l8573"><span class="ln">8573 </span></a>    .. note:: 
<a name="l8574"><span class="ln">8574 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l8575"><span class="ln">8575 </span></a> 
<a name="l8576"><span class="ln">8576 </span></a>    Arguments: 
<a name="l8577"><span class="ln">8577 </span></a>        window_length (int): the size of returned window 
<a name="l8578"><span class="ln">8578 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l8579"><span class="ln">8579 </span></a>            function. If False, return a symmetric window. 
<a name="l8580"><span class="ln">8580 </span></a> 
<a name="l8581"><span class="ln">8581 </span></a>    Keyword args: 
<a name="l8582"><span class="ln">8582 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l8583"><span class="ln">8583 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l8584"><span class="ln">8584 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l8585"><span class="ln">8585 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l8586"><span class="ln">8586 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l8587"><span class="ln">8587 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l8588"><span class="ln">8588 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l8589"><span class="ln">8589 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l8590"><span class="ln">8590 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l8591"><span class="ln">8591 </span></a>            returned tensor. Default: ``False``. 
<a name="l8592"><span class="ln">8592 </span></a> 
<a name="l8593"><span class="ln">8593 </span></a>    Returns: 
<a name="l8594"><span class="ln">8594 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window 
<a name="l8595"><span class="ln">8595 </span></a>    &quot;&quot;&quot;</span>
<a name="l8596"><span class="ln">8596 </span></a>
<a name="l8597"><span class="ln">8597 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8598"><span class="ln">8598 </span></a><span class="s2">def </span><span class="s1">blackman_window</span><span class="s3">(</span>
<a name="l8599"><span class="ln">8599 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l8600"><span class="ln">8600 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l8601"><span class="ln">8601 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8602"><span class="ln">8602 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8603"><span class="ln">8603 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8604"><span class="ln">8604 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8605"><span class="ln">8605 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l8606"><span class="ln">8606 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l8607"><span class="ln">8607 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8608"><span class="ln">8608 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8609"><span class="ln">8609 </span></a>    blackman_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l8610"><span class="ln">8610 </span></a> 
<a name="l8611"><span class="ln">8611 </span></a>    Blackman window function. 
<a name="l8612"><span class="ln">8612 </span></a> 
<a name="l8613"><span class="ln">8613 </span></a>    .. math:: 
<a name="l8614"><span class="ln">8614 </span></a>        w[n] = 0.42 - 0.5 \cos \left( \frac{2 \pi n}{N - 1} \right) + 0.08 \cos \left( \frac{4 \pi n}{N - 1} \right) 
<a name="l8615"><span class="ln">8615 </span></a> 
<a name="l8616"><span class="ln">8616 </span></a>    where :math:`N` is the full window size. 
<a name="l8617"><span class="ln">8617 </span></a> 
<a name="l8618"><span class="ln">8618 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l8619"><span class="ln">8619 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l8620"><span class="ln">8620 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l8621"><span class="ln">8621 </span></a>    ready to be used as a periodic window with functions like 
<a name="l8622"><span class="ln">8622 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l8623"><span class="ln">8623 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l8624"><span class="ln">8624 </span></a>    ``torch.blackman_window(L, periodic=True)`` equal to 
<a name="l8625"><span class="ln">8625 </span></a>    ``torch.blackman_window(L + 1, periodic=False)[:-1]``. 
<a name="l8626"><span class="ln">8626 </span></a> 
<a name="l8627"><span class="ln">8627 </span></a>    .. note:: 
<a name="l8628"><span class="ln">8628 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l8629"><span class="ln">8629 </span></a> 
<a name="l8630"><span class="ln">8630 </span></a>    Arguments: 
<a name="l8631"><span class="ln">8631 </span></a>        window_length (int): the size of returned window 
<a name="l8632"><span class="ln">8632 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l8633"><span class="ln">8633 </span></a>            function. If False, return a symmetric window. 
<a name="l8634"><span class="ln">8634 </span></a> 
<a name="l8635"><span class="ln">8635 </span></a>    Keyword args: 
<a name="l8636"><span class="ln">8636 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l8637"><span class="ln">8637 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l8638"><span class="ln">8638 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l8639"><span class="ln">8639 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l8640"><span class="ln">8640 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l8641"><span class="ln">8641 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l8642"><span class="ln">8642 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l8643"><span class="ln">8643 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l8644"><span class="ln">8644 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l8645"><span class="ln">8645 </span></a>            returned tensor. Default: ``False``. 
<a name="l8646"><span class="ln">8646 </span></a> 
<a name="l8647"><span class="ln">8647 </span></a>    Returns: 
<a name="l8648"><span class="ln">8648 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window 
<a name="l8649"><span class="ln">8649 </span></a>    &quot;&quot;&quot;</span>
<a name="l8650"><span class="ln">8650 </span></a>
<a name="l8651"><span class="ln">8651 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8652"><span class="ln">8652 </span></a><span class="s2">def </span><span class="s1">bmm</span><span class="s3">(</span>
<a name="l8653"><span class="ln">8653 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8654"><span class="ln">8654 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8655"><span class="ln">8655 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8656"><span class="ln">8656 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8657"><span class="ln">8657 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8658"><span class="ln">8658 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8659"><span class="ln">8659 </span></a>    bmm(input, mat2, out_dtype=None, *, out=None) -&gt; Tensor 
<a name="l8660"><span class="ln">8660 </span></a> 
<a name="l8661"><span class="ln">8661 </span></a>    Performs a batch matrix-matrix product of matrices stored in :attr:`input` 
<a name="l8662"><span class="ln">8662 </span></a>    and :attr:`mat2`. 
<a name="l8663"><span class="ln">8663 </span></a> 
<a name="l8664"><span class="ln">8664 </span></a>    :attr:`input` and :attr:`mat2` must be 3-D tensors each containing 
<a name="l8665"><span class="ln">8665 </span></a>    the same number of matrices. 
<a name="l8666"><span class="ln">8666 </span></a> 
<a name="l8667"><span class="ln">8667 </span></a>    If :attr:`input` is a :math:`(b \times n \times m)` tensor, :attr:`mat2` is a 
<a name="l8668"><span class="ln">8668 </span></a>    :math:`(b \times m \times p)` tensor, :attr:`out` will be a 
<a name="l8669"><span class="ln">8669 </span></a>    :math:`(b \times n \times p)` tensor. 
<a name="l8670"><span class="ln">8670 </span></a> 
<a name="l8671"><span class="ln">8671 </span></a>    .. math:: 
<a name="l8672"><span class="ln">8672 </span></a>        \text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i 
<a name="l8673"><span class="ln">8673 </span></a> 
<a name="l8674"><span class="ln">8674 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l8675"><span class="ln">8675 </span></a> 
<a name="l8676"><span class="ln">8676 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l8677"><span class="ln">8677 </span></a> 
<a name="l8678"><span class="ln">8678 </span></a>    .. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l8679"><span class="ln">8679 </span></a>              For broadcasting matrix products, see :func:`torch.matmul`. 
<a name="l8680"><span class="ln">8680 </span></a> 
<a name="l8681"><span class="ln">8681 </span></a>    Args: 
<a name="l8682"><span class="ln">8682 </span></a>        input (Tensor): the first batch of matrices to be multiplied 
<a name="l8683"><span class="ln">8683 </span></a>        mat2 (Tensor): the second batch of matrices to be multiplied 
<a name="l8684"><span class="ln">8684 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l8685"><span class="ln">8685 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l8686"><span class="ln">8686 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l8687"><span class="ln">8687 </span></a> 
<a name="l8688"><span class="ln">8688 </span></a>    Keyword Args: 
<a name="l8689"><span class="ln">8689 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8690"><span class="ln">8690 </span></a> 
<a name="l8691"><span class="ln">8691 </span></a>    Example:: 
<a name="l8692"><span class="ln">8692 </span></a> 
<a name="l8693"><span class="ln">8693 </span></a>        &gt;&gt;&gt; input = torch.randn(10, 3, 4) 
<a name="l8694"><span class="ln">8694 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(10, 4, 5) 
<a name="l8695"><span class="ln">8695 </span></a>        &gt;&gt;&gt; res = torch.bmm(input, mat2) 
<a name="l8696"><span class="ln">8696 </span></a>        &gt;&gt;&gt; res.size() 
<a name="l8697"><span class="ln">8697 </span></a>        torch.Size([10, 3, 5]) 
<a name="l8698"><span class="ln">8698 </span></a>    &quot;&quot;&quot;</span>
<a name="l8699"><span class="ln">8699 </span></a>
<a name="l8700"><span class="ln">8700 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8701"><span class="ln">8701 </span></a><span class="s2">def </span><span class="s1">bmm</span><span class="s3">(</span>
<a name="l8702"><span class="ln">8702 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8703"><span class="ln">8703 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8704"><span class="ln">8704 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l8705"><span class="ln">8705 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8706"><span class="ln">8706 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8707"><span class="ln">8707 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8708"><span class="ln">8708 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8709"><span class="ln">8709 </span></a>    bmm(input, mat2, out_dtype=None, *, out=None) -&gt; Tensor 
<a name="l8710"><span class="ln">8710 </span></a> 
<a name="l8711"><span class="ln">8711 </span></a>    Performs a batch matrix-matrix product of matrices stored in :attr:`input` 
<a name="l8712"><span class="ln">8712 </span></a>    and :attr:`mat2`. 
<a name="l8713"><span class="ln">8713 </span></a> 
<a name="l8714"><span class="ln">8714 </span></a>    :attr:`input` and :attr:`mat2` must be 3-D tensors each containing 
<a name="l8715"><span class="ln">8715 </span></a>    the same number of matrices. 
<a name="l8716"><span class="ln">8716 </span></a> 
<a name="l8717"><span class="ln">8717 </span></a>    If :attr:`input` is a :math:`(b \times n \times m)` tensor, :attr:`mat2` is a 
<a name="l8718"><span class="ln">8718 </span></a>    :math:`(b \times m \times p)` tensor, :attr:`out` will be a 
<a name="l8719"><span class="ln">8719 </span></a>    :math:`(b \times n \times p)` tensor. 
<a name="l8720"><span class="ln">8720 </span></a> 
<a name="l8721"><span class="ln">8721 </span></a>    .. math:: 
<a name="l8722"><span class="ln">8722 </span></a>        \text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i 
<a name="l8723"><span class="ln">8723 </span></a> 
<a name="l8724"><span class="ln">8724 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l8725"><span class="ln">8725 </span></a> 
<a name="l8726"><span class="ln">8726 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l8727"><span class="ln">8727 </span></a> 
<a name="l8728"><span class="ln">8728 </span></a>    .. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l8729"><span class="ln">8729 </span></a>              For broadcasting matrix products, see :func:`torch.matmul`. 
<a name="l8730"><span class="ln">8730 </span></a> 
<a name="l8731"><span class="ln">8731 </span></a>    Args: 
<a name="l8732"><span class="ln">8732 </span></a>        input (Tensor): the first batch of matrices to be multiplied 
<a name="l8733"><span class="ln">8733 </span></a>        mat2 (Tensor): the second batch of matrices to be multiplied 
<a name="l8734"><span class="ln">8734 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l8735"><span class="ln">8735 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l8736"><span class="ln">8736 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l8737"><span class="ln">8737 </span></a> 
<a name="l8738"><span class="ln">8738 </span></a>    Keyword Args: 
<a name="l8739"><span class="ln">8739 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8740"><span class="ln">8740 </span></a> 
<a name="l8741"><span class="ln">8741 </span></a>    Example:: 
<a name="l8742"><span class="ln">8742 </span></a> 
<a name="l8743"><span class="ln">8743 </span></a>        &gt;&gt;&gt; input = torch.randn(10, 3, 4) 
<a name="l8744"><span class="ln">8744 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(10, 4, 5) 
<a name="l8745"><span class="ln">8745 </span></a>        &gt;&gt;&gt; res = torch.bmm(input, mat2) 
<a name="l8746"><span class="ln">8746 </span></a>        &gt;&gt;&gt; res.size() 
<a name="l8747"><span class="ln">8747 </span></a>        torch.Size([10, 3, 5]) 
<a name="l8748"><span class="ln">8748 </span></a>    &quot;&quot;&quot;</span>
<a name="l8749"><span class="ln">8749 </span></a>
<a name="l8750"><span class="ln">8750 </span></a><span class="s2">def </span><span class="s1">broadcast_to</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">]) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8751"><span class="ln">8751 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8752"><span class="ln">8752 </span></a>    broadcast_to(input, shape) -&gt; Tensor 
<a name="l8753"><span class="ln">8753 </span></a> 
<a name="l8754"><span class="ln">8754 </span></a>    Broadcasts :attr:`input` to the shape :attr:`\shape`. 
<a name="l8755"><span class="ln">8755 </span></a>    Equivalent to calling ``input.expand(shape)``. See :meth:`~Tensor.expand` for details. 
<a name="l8756"><span class="ln">8756 </span></a> 
<a name="l8757"><span class="ln">8757 </span></a>    Args: 
<a name="l8758"><span class="ln">8758 </span></a>        input (Tensor): the input tensor. 
<a name="l8759"><span class="ln">8759 </span></a>        shape (list, tuple, or :class:`torch.Size`): the new shape. 
<a name="l8760"><span class="ln">8760 </span></a> 
<a name="l8761"><span class="ln">8761 </span></a>    Example:: 
<a name="l8762"><span class="ln">8762 </span></a> 
<a name="l8763"><span class="ln">8763 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l8764"><span class="ln">8764 </span></a>        &gt;&gt;&gt; torch.broadcast_to(x, (3, 3)) 
<a name="l8765"><span class="ln">8765 </span></a>        tensor([[1, 2, 3], 
<a name="l8766"><span class="ln">8766 </span></a>                [1, 2, 3], 
<a name="l8767"><span class="ln">8767 </span></a>                [1, 2, 3]]) 
<a name="l8768"><span class="ln">8768 </span></a>    &quot;&quot;&quot;</span>
<a name="l8769"><span class="ln">8769 </span></a>
<a name="l8770"><span class="ln">8770 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8771"><span class="ln">8771 </span></a><span class="s2">def </span><span class="s1">bucketize</span><span class="s3">(</span>
<a name="l8772"><span class="ln">8772 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8773"><span class="ln">8773 </span></a>    <span class="s1">boundaries</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8774"><span class="ln">8774 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8775"><span class="ln">8775 </span></a>    <span class="s1">out_int32</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l8776"><span class="ln">8776 </span></a>    <span class="s1">right</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l8777"><span class="ln">8777 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8778"><span class="ln">8778 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8779"><span class="ln">8779 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8780"><span class="ln">8780 </span></a>    bucketize(input, boundaries, *, out_int32=False, right=False, out=None) -&gt; Tensor 
<a name="l8781"><span class="ln">8781 </span></a> 
<a name="l8782"><span class="ln">8782 </span></a>    Returns the indices of the buckets to which each value in the :attr:`input` belongs, where the 
<a name="l8783"><span class="ln">8783 </span></a>    boundaries of the buckets are set by :attr:`boundaries`. Return a new tensor with the same size 
<a name="l8784"><span class="ln">8784 </span></a>    as :attr:`input`. If :attr:`right` is False (default), then the left boundary is open. Note that 
<a name="l8785"><span class="ln">8785 </span></a>    this behavior is opposite the behavior of 
<a name="l8786"><span class="ln">8786 </span></a>    `numpy.digitize &lt;https://numpy.org/doc/stable/reference/generated/numpy.digitize.html&gt;`_. 
<a name="l8787"><span class="ln">8787 </span></a>    More formally, the returned index satisfies the following rules: 
<a name="l8788"><span class="ln">8788 </span></a> 
<a name="l8789"><span class="ln">8789 </span></a>    .. list-table:: 
<a name="l8790"><span class="ln">8790 </span></a>       :widths: 15 85 
<a name="l8791"><span class="ln">8791 </span></a>       :header-rows: 1 
<a name="l8792"><span class="ln">8792 </span></a> 
<a name="l8793"><span class="ln">8793 </span></a>       * - :attr:`right` 
<a name="l8794"><span class="ln">8794 </span></a>         - *returned index satisfies* 
<a name="l8795"><span class="ln">8795 </span></a>       * - False 
<a name="l8796"><span class="ln">8796 </span></a>         - ``boundaries[i-1] &lt; input[m][n]...[l][x] &lt;= boundaries[i]`` 
<a name="l8797"><span class="ln">8797 </span></a>       * - True 
<a name="l8798"><span class="ln">8798 </span></a>         - ``boundaries[i-1] &lt;= input[m][n]...[l][x] &lt; boundaries[i]`` 
<a name="l8799"><span class="ln">8799 </span></a> 
<a name="l8800"><span class="ln">8800 </span></a>    Args: 
<a name="l8801"><span class="ln">8801 </span></a>        input (Tensor or Scalar): N-D tensor or a Scalar containing the search value(s). 
<a name="l8802"><span class="ln">8802 </span></a>        boundaries (Tensor): 1-D tensor, must contain a strictly increasing sequence, or the return value is undefined. 
<a name="l8803"><span class="ln">8803 </span></a> 
<a name="l8804"><span class="ln">8804 </span></a>    Keyword args: 
<a name="l8805"><span class="ln">8805 </span></a>        out_int32 (bool, optional): indicate the output data type. torch.int32 if True, torch.int64 otherwise. 
<a name="l8806"><span class="ln">8806 </span></a>                                    Default value is False, i.e. default output data type is torch.int64. 
<a name="l8807"><span class="ln">8807 </span></a>        right (bool, optional): determines the behavior for values in :attr:`boundaries`. See the table above. 
<a name="l8808"><span class="ln">8808 </span></a>        out (Tensor, optional): the output tensor, must be the same size as :attr:`input` if provided. 
<a name="l8809"><span class="ln">8809 </span></a> 
<a name="l8810"><span class="ln">8810 </span></a> 
<a name="l8811"><span class="ln">8811 </span></a>    Example:: 
<a name="l8812"><span class="ln">8812 </span></a> 
<a name="l8813"><span class="ln">8813 </span></a>        &gt;&gt;&gt; boundaries = torch.tensor([1, 3, 5, 7, 9]) 
<a name="l8814"><span class="ln">8814 </span></a>        &gt;&gt;&gt; boundaries 
<a name="l8815"><span class="ln">8815 </span></a>        tensor([1, 3, 5, 7, 9]) 
<a name="l8816"><span class="ln">8816 </span></a>        &gt;&gt;&gt; v = torch.tensor([[3, 6, 9], [3, 6, 9]]) 
<a name="l8817"><span class="ln">8817 </span></a>        &gt;&gt;&gt; v 
<a name="l8818"><span class="ln">8818 </span></a>        tensor([[3, 6, 9], 
<a name="l8819"><span class="ln">8819 </span></a>                [3, 6, 9]]) 
<a name="l8820"><span class="ln">8820 </span></a>        &gt;&gt;&gt; torch.bucketize(v, boundaries) 
<a name="l8821"><span class="ln">8821 </span></a>        tensor([[1, 3, 4], 
<a name="l8822"><span class="ln">8822 </span></a>                [1, 3, 4]]) 
<a name="l8823"><span class="ln">8823 </span></a>        &gt;&gt;&gt; torch.bucketize(v, boundaries, right=True) 
<a name="l8824"><span class="ln">8824 </span></a>        tensor([[2, 3, 5], 
<a name="l8825"><span class="ln">8825 </span></a>                [2, 3, 5]]) 
<a name="l8826"><span class="ln">8826 </span></a>    &quot;&quot;&quot;</span>
<a name="l8827"><span class="ln">8827 </span></a>
<a name="l8828"><span class="ln">8828 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8829"><span class="ln">8829 </span></a><span class="s2">def </span><span class="s1">bucketize</span><span class="s3">(</span>
<a name="l8830"><span class="ln">8830 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l8831"><span class="ln">8831 </span></a>    <span class="s1">boundaries</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l8832"><span class="ln">8832 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8833"><span class="ln">8833 </span></a>    <span class="s1">out_int32</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l8834"><span class="ln">8834 </span></a>    <span class="s1">right</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l8835"><span class="ln">8835 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8836"><span class="ln">8836 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8837"><span class="ln">8837 </span></a>    bucketize(input, boundaries, *, out_int32=False, right=False, out=None) -&gt; Tensor 
<a name="l8838"><span class="ln">8838 </span></a> 
<a name="l8839"><span class="ln">8839 </span></a>    Returns the indices of the buckets to which each value in the :attr:`input` belongs, where the 
<a name="l8840"><span class="ln">8840 </span></a>    boundaries of the buckets are set by :attr:`boundaries`. Return a new tensor with the same size 
<a name="l8841"><span class="ln">8841 </span></a>    as :attr:`input`. If :attr:`right` is False (default), then the left boundary is open. Note that 
<a name="l8842"><span class="ln">8842 </span></a>    this behavior is opposite the behavior of 
<a name="l8843"><span class="ln">8843 </span></a>    `numpy.digitize &lt;https://numpy.org/doc/stable/reference/generated/numpy.digitize.html&gt;`_. 
<a name="l8844"><span class="ln">8844 </span></a>    More formally, the returned index satisfies the following rules: 
<a name="l8845"><span class="ln">8845 </span></a> 
<a name="l8846"><span class="ln">8846 </span></a>    .. list-table:: 
<a name="l8847"><span class="ln">8847 </span></a>       :widths: 15 85 
<a name="l8848"><span class="ln">8848 </span></a>       :header-rows: 1 
<a name="l8849"><span class="ln">8849 </span></a> 
<a name="l8850"><span class="ln">8850 </span></a>       * - :attr:`right` 
<a name="l8851"><span class="ln">8851 </span></a>         - *returned index satisfies* 
<a name="l8852"><span class="ln">8852 </span></a>       * - False 
<a name="l8853"><span class="ln">8853 </span></a>         - ``boundaries[i-1] &lt; input[m][n]...[l][x] &lt;= boundaries[i]`` 
<a name="l8854"><span class="ln">8854 </span></a>       * - True 
<a name="l8855"><span class="ln">8855 </span></a>         - ``boundaries[i-1] &lt;= input[m][n]...[l][x] &lt; boundaries[i]`` 
<a name="l8856"><span class="ln">8856 </span></a> 
<a name="l8857"><span class="ln">8857 </span></a>    Args: 
<a name="l8858"><span class="ln">8858 </span></a>        input (Tensor or Scalar): N-D tensor or a Scalar containing the search value(s). 
<a name="l8859"><span class="ln">8859 </span></a>        boundaries (Tensor): 1-D tensor, must contain a strictly increasing sequence, or the return value is undefined. 
<a name="l8860"><span class="ln">8860 </span></a> 
<a name="l8861"><span class="ln">8861 </span></a>    Keyword args: 
<a name="l8862"><span class="ln">8862 </span></a>        out_int32 (bool, optional): indicate the output data type. torch.int32 if True, torch.int64 otherwise. 
<a name="l8863"><span class="ln">8863 </span></a>                                    Default value is False, i.e. default output data type is torch.int64. 
<a name="l8864"><span class="ln">8864 </span></a>        right (bool, optional): determines the behavior for values in :attr:`boundaries`. See the table above. 
<a name="l8865"><span class="ln">8865 </span></a>        out (Tensor, optional): the output tensor, must be the same size as :attr:`input` if provided. 
<a name="l8866"><span class="ln">8866 </span></a> 
<a name="l8867"><span class="ln">8867 </span></a> 
<a name="l8868"><span class="ln">8868 </span></a>    Example:: 
<a name="l8869"><span class="ln">8869 </span></a> 
<a name="l8870"><span class="ln">8870 </span></a>        &gt;&gt;&gt; boundaries = torch.tensor([1, 3, 5, 7, 9]) 
<a name="l8871"><span class="ln">8871 </span></a>        &gt;&gt;&gt; boundaries 
<a name="l8872"><span class="ln">8872 </span></a>        tensor([1, 3, 5, 7, 9]) 
<a name="l8873"><span class="ln">8873 </span></a>        &gt;&gt;&gt; v = torch.tensor([[3, 6, 9], [3, 6, 9]]) 
<a name="l8874"><span class="ln">8874 </span></a>        &gt;&gt;&gt; v 
<a name="l8875"><span class="ln">8875 </span></a>        tensor([[3, 6, 9], 
<a name="l8876"><span class="ln">8876 </span></a>                [3, 6, 9]]) 
<a name="l8877"><span class="ln">8877 </span></a>        &gt;&gt;&gt; torch.bucketize(v, boundaries) 
<a name="l8878"><span class="ln">8878 </span></a>        tensor([[1, 3, 4], 
<a name="l8879"><span class="ln">8879 </span></a>                [1, 3, 4]]) 
<a name="l8880"><span class="ln">8880 </span></a>        &gt;&gt;&gt; torch.bucketize(v, boundaries, right=True) 
<a name="l8881"><span class="ln">8881 </span></a>        tensor([[2, 3, 5], 
<a name="l8882"><span class="ln">8882 </span></a>                [2, 3, 5]]) 
<a name="l8883"><span class="ln">8883 </span></a>    &quot;&quot;&quot;</span>
<a name="l8884"><span class="ln">8884 </span></a>
<a name="l8885"><span class="ln">8885 </span></a><span class="s2">def </span><span class="s1">can_cast</span><span class="s3">(</span><span class="s1">from_</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">, </span><span class="s1">to</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l8886"><span class="ln">8886 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8887"><span class="ln">8887 </span></a>    can_cast(from_, to) -&gt; bool 
<a name="l8888"><span class="ln">8888 </span></a> 
<a name="l8889"><span class="ln">8889 </span></a>    Determines if a type conversion is allowed under PyTorch casting rules 
<a name="l8890"><span class="ln">8890 </span></a>    described in the type promotion :ref:`documentation &lt;type-promotion-doc&gt;`. 
<a name="l8891"><span class="ln">8891 </span></a> 
<a name="l8892"><span class="ln">8892 </span></a>    Args: 
<a name="l8893"><span class="ln">8893 </span></a>        from\_ (dtype): The original :class:`torch.dtype`. 
<a name="l8894"><span class="ln">8894 </span></a>        to (dtype): The target :class:`torch.dtype`. 
<a name="l8895"><span class="ln">8895 </span></a> 
<a name="l8896"><span class="ln">8896 </span></a>    Example:: 
<a name="l8897"><span class="ln">8897 </span></a> 
<a name="l8898"><span class="ln">8898 </span></a>        &gt;&gt;&gt; torch.can_cast(torch.double, torch.float) 
<a name="l8899"><span class="ln">8899 </span></a>        True 
<a name="l8900"><span class="ln">8900 </span></a>        &gt;&gt;&gt; torch.can_cast(torch.float, torch.int) 
<a name="l8901"><span class="ln">8901 </span></a>        False 
<a name="l8902"><span class="ln">8902 </span></a>    &quot;&quot;&quot;</span>
<a name="l8903"><span class="ln">8903 </span></a>
<a name="l8904"><span class="ln">8904 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8905"><span class="ln">8905 </span></a><span class="s2">def </span><span class="s1">cat</span><span class="s3">(</span>
<a name="l8906"><span class="ln">8906 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l8907"><span class="ln">8907 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l8908"><span class="ln">8908 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8909"><span class="ln">8909 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8910"><span class="ln">8910 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8911"><span class="ln">8911 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8912"><span class="ln">8912 </span></a>    cat(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l8913"><span class="ln">8913 </span></a> 
<a name="l8914"><span class="ln">8914 </span></a>    Concatenates the given sequence of tensors in :attr:`tensors` in the given dimension. 
<a name="l8915"><span class="ln">8915 </span></a>    All tensors must either have the same shape (except in the concatenating 
<a name="l8916"><span class="ln">8916 </span></a>    dimension) or be a 1-D empty tensor with size ``(0,)``. 
<a name="l8917"><span class="ln">8917 </span></a> 
<a name="l8918"><span class="ln">8918 </span></a>    :func:`torch.cat` can be seen as an inverse operation for :func:`torch.split` 
<a name="l8919"><span class="ln">8919 </span></a>    and :func:`torch.chunk`. 
<a name="l8920"><span class="ln">8920 </span></a> 
<a name="l8921"><span class="ln">8921 </span></a>    :func:`torch.cat` can be best understood via examples. 
<a name="l8922"><span class="ln">8922 </span></a> 
<a name="l8923"><span class="ln">8923 </span></a>    .. seealso:: 
<a name="l8924"><span class="ln">8924 </span></a> 
<a name="l8925"><span class="ln">8925 </span></a>        :func:`torch.stack` concatenates the given sequence along a new dimension. 
<a name="l8926"><span class="ln">8926 </span></a> 
<a name="l8927"><span class="ln">8927 </span></a>    Args: 
<a name="l8928"><span class="ln">8928 </span></a>        tensors (sequence of Tensors): Non-empty tensors provided must have the same shape, 
<a name="l8929"><span class="ln">8929 </span></a>            except in the cat dimension. 
<a name="l8930"><span class="ln">8930 </span></a> 
<a name="l8931"><span class="ln">8931 </span></a>        dim (int, optional): the dimension over which the tensors are concatenated 
<a name="l8932"><span class="ln">8932 </span></a> 
<a name="l8933"><span class="ln">8933 </span></a>    Keyword args: 
<a name="l8934"><span class="ln">8934 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8935"><span class="ln">8935 </span></a> 
<a name="l8936"><span class="ln">8936 </span></a>    Example:: 
<a name="l8937"><span class="ln">8937 </span></a> 
<a name="l8938"><span class="ln">8938 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l8939"><span class="ln">8939 </span></a>        &gt;&gt;&gt; x 
<a name="l8940"><span class="ln">8940 </span></a>        tensor([[ 0.6580, -1.0969, -0.4614], 
<a name="l8941"><span class="ln">8941 </span></a>                [-0.1034, -0.5790,  0.1497]]) 
<a name="l8942"><span class="ln">8942 </span></a>        &gt;&gt;&gt; torch.cat((x, x, x), 0) 
<a name="l8943"><span class="ln">8943 </span></a>        tensor([[ 0.6580, -1.0969, -0.4614], 
<a name="l8944"><span class="ln">8944 </span></a>                [-0.1034, -0.5790,  0.1497], 
<a name="l8945"><span class="ln">8945 </span></a>                [ 0.6580, -1.0969, -0.4614], 
<a name="l8946"><span class="ln">8946 </span></a>                [-0.1034, -0.5790,  0.1497], 
<a name="l8947"><span class="ln">8947 </span></a>                [ 0.6580, -1.0969, -0.4614], 
<a name="l8948"><span class="ln">8948 </span></a>                [-0.1034, -0.5790,  0.1497]]) 
<a name="l8949"><span class="ln">8949 </span></a>        &gt;&gt;&gt; torch.cat((x, x, x), 1) 
<a name="l8950"><span class="ln">8950 </span></a>        tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580, 
<a name="l8951"><span class="ln">8951 </span></a>                 -1.0969, -0.4614], 
<a name="l8952"><span class="ln">8952 </span></a>                [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034, 
<a name="l8953"><span class="ln">8953 </span></a>                 -0.5790,  0.1497]]) 
<a name="l8954"><span class="ln">8954 </span></a>    &quot;&quot;&quot;</span>
<a name="l8955"><span class="ln">8955 </span></a>
<a name="l8956"><span class="ln">8956 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l8957"><span class="ln">8957 </span></a><span class="s2">def </span><span class="s1">cat</span><span class="s3">(</span>
<a name="l8958"><span class="ln">8958 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l8959"><span class="ln">8959 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l8960"><span class="ln">8960 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l8961"><span class="ln">8961 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l8962"><span class="ln">8962 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l8963"><span class="ln">8963 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l8964"><span class="ln">8964 </span></a>    cat(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l8965"><span class="ln">8965 </span></a> 
<a name="l8966"><span class="ln">8966 </span></a>    Concatenates the given sequence of tensors in :attr:`tensors` in the given dimension. 
<a name="l8967"><span class="ln">8967 </span></a>    All tensors must either have the same shape (except in the concatenating 
<a name="l8968"><span class="ln">8968 </span></a>    dimension) or be a 1-D empty tensor with size ``(0,)``. 
<a name="l8969"><span class="ln">8969 </span></a> 
<a name="l8970"><span class="ln">8970 </span></a>    :func:`torch.cat` can be seen as an inverse operation for :func:`torch.split` 
<a name="l8971"><span class="ln">8971 </span></a>    and :func:`torch.chunk`. 
<a name="l8972"><span class="ln">8972 </span></a> 
<a name="l8973"><span class="ln">8973 </span></a>    :func:`torch.cat` can be best understood via examples. 
<a name="l8974"><span class="ln">8974 </span></a> 
<a name="l8975"><span class="ln">8975 </span></a>    .. seealso:: 
<a name="l8976"><span class="ln">8976 </span></a> 
<a name="l8977"><span class="ln">8977 </span></a>        :func:`torch.stack` concatenates the given sequence along a new dimension. 
<a name="l8978"><span class="ln">8978 </span></a> 
<a name="l8979"><span class="ln">8979 </span></a>    Args: 
<a name="l8980"><span class="ln">8980 </span></a>        tensors (sequence of Tensors): Non-empty tensors provided must have the same shape, 
<a name="l8981"><span class="ln">8981 </span></a>            except in the cat dimension. 
<a name="l8982"><span class="ln">8982 </span></a> 
<a name="l8983"><span class="ln">8983 </span></a>        dim (int, optional): the dimension over which the tensors are concatenated 
<a name="l8984"><span class="ln">8984 </span></a> 
<a name="l8985"><span class="ln">8985 </span></a>    Keyword args: 
<a name="l8986"><span class="ln">8986 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l8987"><span class="ln">8987 </span></a> 
<a name="l8988"><span class="ln">8988 </span></a>    Example:: 
<a name="l8989"><span class="ln">8989 </span></a> 
<a name="l8990"><span class="ln">8990 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l8991"><span class="ln">8991 </span></a>        &gt;&gt;&gt; x 
<a name="l8992"><span class="ln">8992 </span></a>        tensor([[ 0.6580, -1.0969, -0.4614], 
<a name="l8993"><span class="ln">8993 </span></a>                [-0.1034, -0.5790,  0.1497]]) 
<a name="l8994"><span class="ln">8994 </span></a>        &gt;&gt;&gt; torch.cat((x, x, x), 0) 
<a name="l8995"><span class="ln">8995 </span></a>        tensor([[ 0.6580, -1.0969, -0.4614], 
<a name="l8996"><span class="ln">8996 </span></a>                [-0.1034, -0.5790,  0.1497], 
<a name="l8997"><span class="ln">8997 </span></a>                [ 0.6580, -1.0969, -0.4614], 
<a name="l8998"><span class="ln">8998 </span></a>                [-0.1034, -0.5790,  0.1497], 
<a name="l8999"><span class="ln">8999 </span></a>                [ 0.6580, -1.0969, -0.4614], 
<a name="l9000"><span class="ln">9000 </span></a>                [-0.1034, -0.5790,  0.1497]]) 
<a name="l9001"><span class="ln">9001 </span></a>        &gt;&gt;&gt; torch.cat((x, x, x), 1) 
<a name="l9002"><span class="ln">9002 </span></a>        tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580, 
<a name="l9003"><span class="ln">9003 </span></a>                 -1.0969, -0.4614], 
<a name="l9004"><span class="ln">9004 </span></a>                [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034, 
<a name="l9005"><span class="ln">9005 </span></a>                 -0.5790,  0.1497]]) 
<a name="l9006"><span class="ln">9006 </span></a>    &quot;&quot;&quot;</span>
<a name="l9007"><span class="ln">9007 </span></a>
<a name="l9008"><span class="ln">9008 </span></a><span class="s2">def </span><span class="s1">ccol_indices_copy</span><span class="s3">(</span>
<a name="l9009"><span class="ln">9009 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9010"><span class="ln">9010 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9011"><span class="ln">9011 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9012"><span class="ln">9012 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9013"><span class="ln">9013 </span></a><span class="s2">def </span><span class="s1">ceil</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9014"><span class="ln">9014 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9015"><span class="ln">9015 </span></a>    ceil(input, *, out=None) -&gt; Tensor 
<a name="l9016"><span class="ln">9016 </span></a> 
<a name="l9017"><span class="ln">9017 </span></a>    Returns a new tensor with the ceil of the elements of :attr:`input`, 
<a name="l9018"><span class="ln">9018 </span></a>    the smallest integer greater than or equal to each element. 
<a name="l9019"><span class="ln">9019 </span></a> 
<a name="l9020"><span class="ln">9020 </span></a>    For integer inputs, follows the array-api convention of returning a 
<a name="l9021"><span class="ln">9021 </span></a>    copy of the input tensor. 
<a name="l9022"><span class="ln">9022 </span></a> 
<a name="l9023"><span class="ln">9023 </span></a>    .. math:: 
<a name="l9024"><span class="ln">9024 </span></a>        \text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil 
<a name="l9025"><span class="ln">9025 </span></a> 
<a name="l9026"><span class="ln">9026 </span></a>    Args: 
<a name="l9027"><span class="ln">9027 </span></a>        input (Tensor): the input tensor. 
<a name="l9028"><span class="ln">9028 </span></a> 
<a name="l9029"><span class="ln">9029 </span></a>    Keyword args: 
<a name="l9030"><span class="ln">9030 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l9031"><span class="ln">9031 </span></a> 
<a name="l9032"><span class="ln">9032 </span></a>    Example:: 
<a name="l9033"><span class="ln">9033 </span></a> 
<a name="l9034"><span class="ln">9034 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l9035"><span class="ln">9035 </span></a>        &gt;&gt;&gt; a 
<a name="l9036"><span class="ln">9036 </span></a>        tensor([-0.6341, -1.4208, -1.0900,  0.5826]) 
<a name="l9037"><span class="ln">9037 </span></a>        &gt;&gt;&gt; torch.ceil(a) 
<a name="l9038"><span class="ln">9038 </span></a>        tensor([-0., -1., -1.,  1.]) 
<a name="l9039"><span class="ln">9039 </span></a>    &quot;&quot;&quot;</span>
<a name="l9040"><span class="ln">9040 </span></a>
<a name="l9041"><span class="ln">9041 </span></a><span class="s2">def </span><span class="s1">ceil_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9042"><span class="ln">9042 </span></a><span class="s2">def </span><span class="s1">celu</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1.0</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9043"><span class="ln">9043 </span></a><span class="s2">def </span><span class="s1">celu_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1.0</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9044"><span class="ln">9044 </span></a><span class="s2">def </span><span class="s1">channel_shuffle</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9045"><span class="ln">9045 </span></a><span class="s2">def </span><span class="s1">cholesky</span><span class="s3">(</span>
<a name="l9046"><span class="ln">9046 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9047"><span class="ln">9047 </span></a>    <span class="s1">upper</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l9048"><span class="ln">9048 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9049"><span class="ln">9049 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9050"><span class="ln">9050 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9051"><span class="ln">9051 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9052"><span class="ln">9052 </span></a>    cholesky(input, upper=False, *, out=None) -&gt; Tensor 
<a name="l9053"><span class="ln">9053 </span></a> 
<a name="l9054"><span class="ln">9054 </span></a>    Computes the Cholesky decomposition of a symmetric positive-definite 
<a name="l9055"><span class="ln">9055 </span></a>    matrix :math:`A` or for batches of symmetric positive-definite matrices. 
<a name="l9056"><span class="ln">9056 </span></a> 
<a name="l9057"><span class="ln">9057 </span></a>    If :attr:`upper` is ``True``, the returned matrix ``U`` is upper-triangular, and 
<a name="l9058"><span class="ln">9058 </span></a>    the decomposition has the form: 
<a name="l9059"><span class="ln">9059 </span></a> 
<a name="l9060"><span class="ln">9060 </span></a>    .. math:: 
<a name="l9061"><span class="ln">9061 </span></a> 
<a name="l9062"><span class="ln">9062 </span></a>      A = U^TU 
<a name="l9063"><span class="ln">9063 </span></a> 
<a name="l9064"><span class="ln">9064 </span></a>    If :attr:`upper` is ``False``, the returned matrix ``L`` is lower-triangular, and 
<a name="l9065"><span class="ln">9065 </span></a>    the decomposition has the form: 
<a name="l9066"><span class="ln">9066 </span></a> 
<a name="l9067"><span class="ln">9067 </span></a>    .. math:: 
<a name="l9068"><span class="ln">9068 </span></a> 
<a name="l9069"><span class="ln">9069 </span></a>        A = LL^T 
<a name="l9070"><span class="ln">9070 </span></a> 
<a name="l9071"><span class="ln">9071 </span></a>    If :attr:`upper` is ``True``, and :math:`A` is a batch of symmetric positive-definite 
<a name="l9072"><span class="ln">9072 </span></a>    matrices, then the returned tensor will be composed of upper-triangular Cholesky factors 
<a name="l9073"><span class="ln">9073 </span></a>    of each of the individual matrices. Similarly, when :attr:`upper` is ``False``, the returned 
<a name="l9074"><span class="ln">9074 </span></a>    tensor will be composed of lower-triangular Cholesky factors of each of the individual 
<a name="l9075"><span class="ln">9075 </span></a>    matrices. 
<a name="l9076"><span class="ln">9076 </span></a> 
<a name="l9077"><span class="ln">9077 </span></a>    .. warning:: 
<a name="l9078"><span class="ln">9078 </span></a> 
<a name="l9079"><span class="ln">9079 </span></a>        :func:`torch.cholesky` is deprecated in favor of :func:`torch.linalg.cholesky` 
<a name="l9080"><span class="ln">9080 </span></a>        and will be removed in a future PyTorch release. 
<a name="l9081"><span class="ln">9081 </span></a> 
<a name="l9082"><span class="ln">9082 </span></a>        ``L = torch.cholesky(A)`` should be replaced with 
<a name="l9083"><span class="ln">9083 </span></a> 
<a name="l9084"><span class="ln">9084 </span></a>        .. code:: python 
<a name="l9085"><span class="ln">9085 </span></a> 
<a name="l9086"><span class="ln">9086 </span></a>            L = torch.linalg.cholesky(A) 
<a name="l9087"><span class="ln">9087 </span></a> 
<a name="l9088"><span class="ln">9088 </span></a>        ``U = torch.cholesky(A, upper=True)`` should be replaced with 
<a name="l9089"><span class="ln">9089 </span></a> 
<a name="l9090"><span class="ln">9090 </span></a>        .. code:: python 
<a name="l9091"><span class="ln">9091 </span></a> 
<a name="l9092"><span class="ln">9092 </span></a>            U = torch.linalg.cholesky(A).mH 
<a name="l9093"><span class="ln">9093 </span></a> 
<a name="l9094"><span class="ln">9094 </span></a>        This transform will produce equivalent results for all valid (symmetric positive definite) inputs. 
<a name="l9095"><span class="ln">9095 </span></a> 
<a name="l9096"><span class="ln">9096 </span></a>    Args: 
<a name="l9097"><span class="ln">9097 </span></a>        input (Tensor): the input tensor :math:`A` of size :math:`(*, n, n)` where `*` is zero or more 
<a name="l9098"><span class="ln">9098 </span></a>                    batch dimensions consisting of symmetric positive-definite matrices. 
<a name="l9099"><span class="ln">9099 </span></a>        upper (bool, optional): flag that indicates whether to return a 
<a name="l9100"><span class="ln">9100 </span></a>                                upper or lower triangular matrix. Default: ``False`` 
<a name="l9101"><span class="ln">9101 </span></a> 
<a name="l9102"><span class="ln">9102 </span></a>    Keyword args: 
<a name="l9103"><span class="ln">9103 </span></a>        out (Tensor, optional): the output matrix 
<a name="l9104"><span class="ln">9104 </span></a> 
<a name="l9105"><span class="ln">9105 </span></a>    Example:: 
<a name="l9106"><span class="ln">9106 </span></a> 
<a name="l9107"><span class="ln">9107 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l9108"><span class="ln">9108 </span></a>        &gt;&gt;&gt; a = a @ a.mT + 1e-3 # make symmetric positive-definite 
<a name="l9109"><span class="ln">9109 </span></a>        &gt;&gt;&gt; l = torch.cholesky(a) 
<a name="l9110"><span class="ln">9110 </span></a>        &gt;&gt;&gt; a 
<a name="l9111"><span class="ln">9111 </span></a>        tensor([[ 2.4112, -0.7486,  1.4551], 
<a name="l9112"><span class="ln">9112 </span></a>                [-0.7486,  1.3544,  0.1294], 
<a name="l9113"><span class="ln">9113 </span></a>                [ 1.4551,  0.1294,  1.6724]]) 
<a name="l9114"><span class="ln">9114 </span></a>        &gt;&gt;&gt; l 
<a name="l9115"><span class="ln">9115 </span></a>        tensor([[ 1.5528,  0.0000,  0.0000], 
<a name="l9116"><span class="ln">9116 </span></a>                [-0.4821,  1.0592,  0.0000], 
<a name="l9117"><span class="ln">9117 </span></a>                [ 0.9371,  0.5487,  0.7023]]) 
<a name="l9118"><span class="ln">9118 </span></a>        &gt;&gt;&gt; l @ l.mT 
<a name="l9119"><span class="ln">9119 </span></a>        tensor([[ 2.4112, -0.7486,  1.4551], 
<a name="l9120"><span class="ln">9120 </span></a>                [-0.7486,  1.3544,  0.1294], 
<a name="l9121"><span class="ln">9121 </span></a>                [ 1.4551,  0.1294,  1.6724]]) 
<a name="l9122"><span class="ln">9122 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 2, 2) # Example for batched input 
<a name="l9123"><span class="ln">9123 </span></a>        &gt;&gt;&gt; a = a @ a.mT + 1e-03 # make symmetric positive-definite 
<a name="l9124"><span class="ln">9124 </span></a>        &gt;&gt;&gt; l = torch.cholesky(a) 
<a name="l9125"><span class="ln">9125 </span></a>        &gt;&gt;&gt; z = l @ l.mT 
<a name="l9126"><span class="ln">9126 </span></a>        &gt;&gt;&gt; torch.dist(z, a) 
<a name="l9127"><span class="ln">9127 </span></a>        tensor(2.3842e-07) 
<a name="l9128"><span class="ln">9128 </span></a>    &quot;&quot;&quot;</span>
<a name="l9129"><span class="ln">9129 </span></a>
<a name="l9130"><span class="ln">9130 </span></a><span class="s2">def </span><span class="s1">cholesky_inverse</span><span class="s3">(</span>
<a name="l9131"><span class="ln">9131 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9132"><span class="ln">9132 </span></a>    <span class="s1">upper</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l9133"><span class="ln">9133 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9134"><span class="ln">9134 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9135"><span class="ln">9135 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9136"><span class="ln">9136 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9137"><span class="ln">9137 </span></a>    cholesky_inverse(L, upper=False, *, out=None) -&gt; Tensor 
<a name="l9138"><span class="ln">9138 </span></a> 
<a name="l9139"><span class="ln">9139 </span></a>    Computes the inverse of a complex Hermitian or real symmetric 
<a name="l9140"><span class="ln">9140 </span></a>    positive-definite matrix given its Cholesky decomposition. 
<a name="l9141"><span class="ln">9141 </span></a> 
<a name="l9142"><span class="ln">9142 </span></a>    Let :math:`A` be a complex Hermitian or real symmetric positive-definite matrix, 
<a name="l9143"><span class="ln">9143 </span></a>    and :math:`L` its Cholesky decomposition such that: 
<a name="l9144"><span class="ln">9144 </span></a> 
<a name="l9145"><span class="ln">9145 </span></a>    .. math:: 
<a name="l9146"><span class="ln">9146 </span></a> 
<a name="l9147"><span class="ln">9147 </span></a>        A = LL^{\text{H}} 
<a name="l9148"><span class="ln">9148 </span></a> 
<a name="l9149"><span class="ln">9149 </span></a>    where :math:`L^{\text{H}}` is the conjugate transpose when :math:`L` is complex, 
<a name="l9150"><span class="ln">9150 </span></a>    and the transpose when :math:`L` is real-valued. 
<a name="l9151"><span class="ln">9151 </span></a> 
<a name="l9152"><span class="ln">9152 </span></a>    Computes the inverse matrix :math:`A^{-1}`. 
<a name="l9153"><span class="ln">9153 </span></a> 
<a name="l9154"><span class="ln">9154 </span></a>    Supports input of float, double, cfloat and cdouble dtypes. 
<a name="l9155"><span class="ln">9155 </span></a>    Also supports batches of matrices, and if :math:`A` is a batch of matrices 
<a name="l9156"><span class="ln">9156 </span></a>    then the output has the same batch dimensions. 
<a name="l9157"><span class="ln">9157 </span></a> 
<a name="l9158"><span class="ln">9158 </span></a>    Args: 
<a name="l9159"><span class="ln">9159 </span></a>        L (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions 
<a name="l9160"><span class="ln">9160 </span></a>            consisting of lower or upper triangular Cholesky decompositions of 
<a name="l9161"><span class="ln">9161 </span></a>            symmetric or Hermitian positive-definite matrices. 
<a name="l9162"><span class="ln">9162 </span></a>        upper (bool, optional): flag that indicates whether :math:`L` is lower triangular 
<a name="l9163"><span class="ln">9163 </span></a>            or upper triangular. Default: ``False`` 
<a name="l9164"><span class="ln">9164 </span></a> 
<a name="l9165"><span class="ln">9165 </span></a>    Keyword args: 
<a name="l9166"><span class="ln">9166 </span></a>        out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`. 
<a name="l9167"><span class="ln">9167 </span></a> 
<a name="l9168"><span class="ln">9168 </span></a>    Example:: 
<a name="l9169"><span class="ln">9169 </span></a> 
<a name="l9170"><span class="ln">9170 </span></a>        &gt;&gt;&gt; A = torch.randn(3, 3) 
<a name="l9171"><span class="ln">9171 </span></a>        &gt;&gt;&gt; A = A @ A.T + torch.eye(3) * 1e-3 # Creates a symmetric positive-definite matrix 
<a name="l9172"><span class="ln">9172 </span></a>        &gt;&gt;&gt; L = torch.linalg.cholesky(A) # Extract Cholesky decomposition 
<a name="l9173"><span class="ln">9173 </span></a>        &gt;&gt;&gt; torch.cholesky_inverse(L) 
<a name="l9174"><span class="ln">9174 </span></a>        tensor([[ 1.9314,  1.2251, -0.0889], 
<a name="l9175"><span class="ln">9175 </span></a>                [ 1.2251,  2.4439,  0.2122], 
<a name="l9176"><span class="ln">9176 </span></a>                [-0.0889,  0.2122,  0.1412]]) 
<a name="l9177"><span class="ln">9177 </span></a>        &gt;&gt;&gt; A.inverse() 
<a name="l9178"><span class="ln">9178 </span></a>        tensor([[ 1.9314,  1.2251, -0.0889], 
<a name="l9179"><span class="ln">9179 </span></a>                [ 1.2251,  2.4439,  0.2122], 
<a name="l9180"><span class="ln">9180 </span></a>                [-0.0889,  0.2122,  0.1412]]) 
<a name="l9181"><span class="ln">9181 </span></a> 
<a name="l9182"><span class="ln">9182 </span></a>        &gt;&gt;&gt; A = torch.randn(3, 2, 2, dtype=torch.complex64) 
<a name="l9183"><span class="ln">9183 </span></a>        &gt;&gt;&gt; A = A @ A.mH + torch.eye(2) * 1e-3 # Batch of Hermitian positive-definite matrices 
<a name="l9184"><span class="ln">9184 </span></a>        &gt;&gt;&gt; L = torch.linalg.cholesky(A) 
<a name="l9185"><span class="ln">9185 </span></a>        &gt;&gt;&gt; torch.dist(torch.inverse(A), torch.cholesky_inverse(L)) 
<a name="l9186"><span class="ln">9186 </span></a>        tensor(5.6358e-7) 
<a name="l9187"><span class="ln">9187 </span></a>    &quot;&quot;&quot;</span>
<a name="l9188"><span class="ln">9188 </span></a>
<a name="l9189"><span class="ln">9189 </span></a><span class="s2">def </span><span class="s1">cholesky_solve</span><span class="s3">(</span>
<a name="l9190"><span class="ln">9190 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9191"><span class="ln">9191 </span></a>    <span class="s1">input2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9192"><span class="ln">9192 </span></a>    <span class="s1">upper</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l9193"><span class="ln">9193 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9194"><span class="ln">9194 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9195"><span class="ln">9195 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9196"><span class="ln">9196 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9197"><span class="ln">9197 </span></a>    cholesky_solve(B, L, upper=False, *, out=None) -&gt; Tensor 
<a name="l9198"><span class="ln">9198 </span></a> 
<a name="l9199"><span class="ln">9199 </span></a>    Computes the solution of a system of linear equations with complex Hermitian 
<a name="l9200"><span class="ln">9200 </span></a>    or real symmetric positive-definite lhs given its Cholesky decomposition. 
<a name="l9201"><span class="ln">9201 </span></a> 
<a name="l9202"><span class="ln">9202 </span></a>    Let :math:`A` be a complex Hermitian or real symmetric positive-definite matrix, 
<a name="l9203"><span class="ln">9203 </span></a>    and :math:`L` its Cholesky decomposition such that: 
<a name="l9204"><span class="ln">9204 </span></a> 
<a name="l9205"><span class="ln">9205 </span></a>    .. math:: 
<a name="l9206"><span class="ln">9206 </span></a> 
<a name="l9207"><span class="ln">9207 </span></a>        A = LL^{\text{H}} 
<a name="l9208"><span class="ln">9208 </span></a> 
<a name="l9209"><span class="ln">9209 </span></a>    where :math:`L^{\text{H}}` is the conjugate transpose when :math:`L` is complex, 
<a name="l9210"><span class="ln">9210 </span></a>    and the transpose when :math:`L` is real-valued. 
<a name="l9211"><span class="ln">9211 </span></a> 
<a name="l9212"><span class="ln">9212 </span></a>    Returns the solution :math:`X` of the following linear system: 
<a name="l9213"><span class="ln">9213 </span></a> 
<a name="l9214"><span class="ln">9214 </span></a>    .. math:: 
<a name="l9215"><span class="ln">9215 </span></a> 
<a name="l9216"><span class="ln">9216 </span></a>        AX = B 
<a name="l9217"><span class="ln">9217 </span></a> 
<a name="l9218"><span class="ln">9218 </span></a>    Supports inputs of float, double, cfloat and cdouble dtypes. 
<a name="l9219"><span class="ln">9219 </span></a>    Also supports batches of matrices, and if :math:`A` or :math:`B` is a batch of matrices 
<a name="l9220"><span class="ln">9220 </span></a>    then the output has the same batch dimensions. 
<a name="l9221"><span class="ln">9221 </span></a> 
<a name="l9222"><span class="ln">9222 </span></a>    Args: 
<a name="l9223"><span class="ln">9223 </span></a>        B (Tensor): right-hand side tensor of shape `(*, n, k)` 
<a name="l9224"><span class="ln">9224 </span></a>            where :math:`*` is zero or more batch dimensions 
<a name="l9225"><span class="ln">9225 </span></a>        L (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions 
<a name="l9226"><span class="ln">9226 </span></a>            consisting of lower or upper triangular Cholesky decompositions of 
<a name="l9227"><span class="ln">9227 </span></a>            symmetric or Hermitian positive-definite matrices. 
<a name="l9228"><span class="ln">9228 </span></a>        upper (bool, optional): flag that indicates whether :math:`L` is lower triangular 
<a name="l9229"><span class="ln">9229 </span></a>            or upper triangular. Default: ``False``. 
<a name="l9230"><span class="ln">9230 </span></a> 
<a name="l9231"><span class="ln">9231 </span></a>    Keyword args: 
<a name="l9232"><span class="ln">9232 </span></a>        out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`. 
<a name="l9233"><span class="ln">9233 </span></a> 
<a name="l9234"><span class="ln">9234 </span></a>    Example:: 
<a name="l9235"><span class="ln">9235 </span></a> 
<a name="l9236"><span class="ln">9236 </span></a>        &gt;&gt;&gt; A = torch.randn(3, 3) 
<a name="l9237"><span class="ln">9237 </span></a>        &gt;&gt;&gt; A = A @ A.T + torch.eye(3) * 1e-3 # Creates a symmetric positive-definite matrix 
<a name="l9238"><span class="ln">9238 </span></a>        &gt;&gt;&gt; L = torch.linalg.cholesky(A) # Extract Cholesky decomposition 
<a name="l9239"><span class="ln">9239 </span></a>        &gt;&gt;&gt; B = torch.randn(3, 2) 
<a name="l9240"><span class="ln">9240 </span></a>        &gt;&gt;&gt; torch.cholesky_solve(B, L) 
<a name="l9241"><span class="ln">9241 </span></a>        tensor([[ -8.1625,  19.6097], 
<a name="l9242"><span class="ln">9242 </span></a>                [ -5.8398,  14.2387], 
<a name="l9243"><span class="ln">9243 </span></a>                [ -4.3771,  10.4173]]) 
<a name="l9244"><span class="ln">9244 </span></a>        &gt;&gt;&gt; A.inverse() @  B 
<a name="l9245"><span class="ln">9245 </span></a>        tensor([[ -8.1626,  19.6097], 
<a name="l9246"><span class="ln">9246 </span></a>                [ -5.8398,  14.2387], 
<a name="l9247"><span class="ln">9247 </span></a>                [ -4.3771,  10.4173]]) 
<a name="l9248"><span class="ln">9248 </span></a> 
<a name="l9249"><span class="ln">9249 </span></a>        &gt;&gt;&gt; A = torch.randn(3, 2, 2, dtype=torch.complex64) 
<a name="l9250"><span class="ln">9250 </span></a>        &gt;&gt;&gt; A = A @ A.mH + torch.eye(2) * 1e-3 # Batch of Hermitian positive-definite matrices 
<a name="l9251"><span class="ln">9251 </span></a>        &gt;&gt;&gt; L = torch.linalg.cholesky(A) 
<a name="l9252"><span class="ln">9252 </span></a>        &gt;&gt;&gt; B = torch.randn(2, 1, dtype=torch.complex64) 
<a name="l9253"><span class="ln">9253 </span></a>        &gt;&gt;&gt; X = torch.cholesky_solve(B, L) 
<a name="l9254"><span class="ln">9254 </span></a>        &gt;&gt;&gt; torch.dist(X, A.inverse() @ B) 
<a name="l9255"><span class="ln">9255 </span></a>        tensor(1.6881e-5) 
<a name="l9256"><span class="ln">9256 </span></a>    &quot;&quot;&quot;</span>
<a name="l9257"><span class="ln">9257 </span></a>
<a name="l9258"><span class="ln">9258 </span></a><span class="s2">def </span><span class="s1">choose_qparams_optimized</span><span class="s3">(</span>
<a name="l9259"><span class="ln">9259 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9260"><span class="ln">9260 </span></a>    <span class="s1">numel</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l9261"><span class="ln">9261 </span></a>    <span class="s1">n_bins</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l9262"><span class="ln">9262 </span></a>    <span class="s1">ratio</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l9263"><span class="ln">9263 </span></a>    <span class="s1">bit_width</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l9264"><span class="ln">9264 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9265"><span class="ln">9265 </span></a><span class="s2">def </span><span class="s1">chunk</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">chunks</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l9266"><span class="ln">9266 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9267"><span class="ln">9267 </span></a>    chunk(input: Tensor, chunks: int, dim: int = 0) -&gt; Tuple[Tensor, ...] 
<a name="l9268"><span class="ln">9268 </span></a> 
<a name="l9269"><span class="ln">9269 </span></a>    Attempts to split a tensor into the specified number of chunks. Each chunk is a view of 
<a name="l9270"><span class="ln">9270 </span></a>    the input tensor. 
<a name="l9271"><span class="ln">9271 </span></a> 
<a name="l9272"><span class="ln">9272 </span></a> 
<a name="l9273"><span class="ln">9273 </span></a>    .. note:: 
<a name="l9274"><span class="ln">9274 </span></a> 
<a name="l9275"><span class="ln">9275 </span></a>        This function may return fewer than the specified number of chunks! 
<a name="l9276"><span class="ln">9276 </span></a> 
<a name="l9277"><span class="ln">9277 </span></a>    .. seealso:: 
<a name="l9278"><span class="ln">9278 </span></a> 
<a name="l9279"><span class="ln">9279 </span></a>        :func:`torch.tensor_split` a function that always returns exactly the specified number of chunks 
<a name="l9280"><span class="ln">9280 </span></a> 
<a name="l9281"><span class="ln">9281 </span></a>    If the tensor size along the given dimension :attr:`dim` is divisible by :attr:`chunks`, 
<a name="l9282"><span class="ln">9282 </span></a>    all returned chunks will be the same size. 
<a name="l9283"><span class="ln">9283 </span></a>    If the tensor size along the given dimension :attr:`dim` is not divisible by :attr:`chunks`, 
<a name="l9284"><span class="ln">9284 </span></a>    all returned chunks will be the same size, except the last one. 
<a name="l9285"><span class="ln">9285 </span></a>    If such division is not possible, this function may return fewer 
<a name="l9286"><span class="ln">9286 </span></a>    than the specified number of chunks. 
<a name="l9287"><span class="ln">9287 </span></a> 
<a name="l9288"><span class="ln">9288 </span></a>    Arguments: 
<a name="l9289"><span class="ln">9289 </span></a>        input (Tensor): the tensor to split 
<a name="l9290"><span class="ln">9290 </span></a>        chunks (int): number of chunks to return 
<a name="l9291"><span class="ln">9291 </span></a>        dim (int): dimension along which to split the tensor 
<a name="l9292"><span class="ln">9292 </span></a> 
<a name="l9293"><span class="ln">9293 </span></a>    Example: 
<a name="l9294"><span class="ln">9294 </span></a>        &gt;&gt;&gt; torch.arange(11).chunk(6) 
<a name="l9295"><span class="ln">9295 </span></a>        (tensor([0, 1]), 
<a name="l9296"><span class="ln">9296 </span></a>         tensor([2, 3]), 
<a name="l9297"><span class="ln">9297 </span></a>         tensor([4, 5]), 
<a name="l9298"><span class="ln">9298 </span></a>         tensor([6, 7]), 
<a name="l9299"><span class="ln">9299 </span></a>         tensor([8, 9]), 
<a name="l9300"><span class="ln">9300 </span></a>         tensor([10])) 
<a name="l9301"><span class="ln">9301 </span></a>        &gt;&gt;&gt; torch.arange(12).chunk(6) 
<a name="l9302"><span class="ln">9302 </span></a>        (tensor([0, 1]), 
<a name="l9303"><span class="ln">9303 </span></a>         tensor([2, 3]), 
<a name="l9304"><span class="ln">9304 </span></a>         tensor([4, 5]), 
<a name="l9305"><span class="ln">9305 </span></a>         tensor([6, 7]), 
<a name="l9306"><span class="ln">9306 </span></a>         tensor([8, 9]), 
<a name="l9307"><span class="ln">9307 </span></a>         tensor([10, 11])) 
<a name="l9308"><span class="ln">9308 </span></a>        &gt;&gt;&gt; torch.arange(13).chunk(6) 
<a name="l9309"><span class="ln">9309 </span></a>        (tensor([0, 1, 2]), 
<a name="l9310"><span class="ln">9310 </span></a>         tensor([3, 4, 5]), 
<a name="l9311"><span class="ln">9311 </span></a>         tensor([6, 7, 8]), 
<a name="l9312"><span class="ln">9312 </span></a>         tensor([ 9, 10, 11]), 
<a name="l9313"><span class="ln">9313 </span></a>         tensor([12])) 
<a name="l9314"><span class="ln">9314 </span></a>    &quot;&quot;&quot;</span>
<a name="l9315"><span class="ln">9315 </span></a>
<a name="l9316"><span class="ln">9316 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9317"><span class="ln">9317 </span></a><span class="s2">def </span><span class="s1">clamp</span><span class="s3">(</span>
<a name="l9318"><span class="ln">9318 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9319"><span class="ln">9319 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9320"><span class="ln">9320 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9321"><span class="ln">9321 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9322"><span class="ln">9322 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9323"><span class="ln">9323 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9324"><span class="ln">9324 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9325"><span class="ln">9325 </span></a>    clamp(input, min=None, max=None, *, out=None) -&gt; Tensor 
<a name="l9326"><span class="ln">9326 </span></a> 
<a name="l9327"><span class="ln">9327 </span></a>    Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`. 
<a name="l9328"><span class="ln">9328 </span></a>    Letting min_value and max_value be :attr:`min` and :attr:`max`, respectively, this returns: 
<a name="l9329"><span class="ln">9329 </span></a> 
<a name="l9330"><span class="ln">9330 </span></a>    .. math:: 
<a name="l9331"><span class="ln">9331 </span></a>        y_i = \min(\max(x_i, \text{min\_value}_i), \text{max\_value}_i) 
<a name="l9332"><span class="ln">9332 </span></a> 
<a name="l9333"><span class="ln">9333 </span></a>    If :attr:`min` is ``None``, there is no lower bound. 
<a name="l9334"><span class="ln">9334 </span></a>    Or, if :attr:`max` is ``None`` there is no upper bound. 
<a name="l9335"><span class="ln">9335 </span></a> 
<a name="l9336"><span class="ln">9336 </span></a> 
<a name="l9337"><span class="ln">9337 </span></a>    .. note:: 
<a name="l9338"><span class="ln">9338 </span></a>        If :attr:`min` is greater than :attr:`max` :func:`torch.clamp(..., min, max) &lt;torch.clamp&gt;` 
<a name="l9339"><span class="ln">9339 </span></a>        sets all elements in :attr:`input` to the value of :attr:`max`. 
<a name="l9340"><span class="ln">9340 </span></a> 
<a name="l9341"><span class="ln">9341 </span></a>    Args: 
<a name="l9342"><span class="ln">9342 </span></a>        input (Tensor): the input tensor. 
<a name="l9343"><span class="ln">9343 </span></a>        min (Number or Tensor, optional): lower-bound of the range to be clamped to 
<a name="l9344"><span class="ln">9344 </span></a>        max (Number or Tensor, optional): upper-bound of the range to be clamped to 
<a name="l9345"><span class="ln">9345 </span></a> 
<a name="l9346"><span class="ln">9346 </span></a>    Keyword args: 
<a name="l9347"><span class="ln">9347 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l9348"><span class="ln">9348 </span></a> 
<a name="l9349"><span class="ln">9349 </span></a>    Example:: 
<a name="l9350"><span class="ln">9350 </span></a> 
<a name="l9351"><span class="ln">9351 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l9352"><span class="ln">9352 </span></a>        &gt;&gt;&gt; a 
<a name="l9353"><span class="ln">9353 </span></a>        tensor([-1.7120,  0.1734, -0.0478, -0.0922]) 
<a name="l9354"><span class="ln">9354 </span></a>        &gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5) 
<a name="l9355"><span class="ln">9355 </span></a>        tensor([-0.5000,  0.1734, -0.0478, -0.0922]) 
<a name="l9356"><span class="ln">9356 </span></a> 
<a name="l9357"><span class="ln">9357 </span></a>        &gt;&gt;&gt; min = torch.linspace(-1, 1, steps=4) 
<a name="l9358"><span class="ln">9358 </span></a>        &gt;&gt;&gt; torch.clamp(a, min=min) 
<a name="l9359"><span class="ln">9359 </span></a>        tensor([-1.0000,  0.1734,  0.3333,  1.0000]) 
<a name="l9360"><span class="ln">9360 </span></a>    &quot;&quot;&quot;</span>
<a name="l9361"><span class="ln">9361 </span></a>
<a name="l9362"><span class="ln">9362 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9363"><span class="ln">9363 </span></a><span class="s2">def </span><span class="s1">clamp</span><span class="s3">(</span>
<a name="l9364"><span class="ln">9364 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9365"><span class="ln">9365 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9366"><span class="ln">9366 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9367"><span class="ln">9367 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9368"><span class="ln">9368 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9369"><span class="ln">9369 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9370"><span class="ln">9370 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9371"><span class="ln">9371 </span></a>    clamp(input, min=None, max=None, *, out=None) -&gt; Tensor 
<a name="l9372"><span class="ln">9372 </span></a> 
<a name="l9373"><span class="ln">9373 </span></a>    Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`. 
<a name="l9374"><span class="ln">9374 </span></a>    Letting min_value and max_value be :attr:`min` and :attr:`max`, respectively, this returns: 
<a name="l9375"><span class="ln">9375 </span></a> 
<a name="l9376"><span class="ln">9376 </span></a>    .. math:: 
<a name="l9377"><span class="ln">9377 </span></a>        y_i = \min(\max(x_i, \text{min\_value}_i), \text{max\_value}_i) 
<a name="l9378"><span class="ln">9378 </span></a> 
<a name="l9379"><span class="ln">9379 </span></a>    If :attr:`min` is ``None``, there is no lower bound. 
<a name="l9380"><span class="ln">9380 </span></a>    Or, if :attr:`max` is ``None`` there is no upper bound. 
<a name="l9381"><span class="ln">9381 </span></a> 
<a name="l9382"><span class="ln">9382 </span></a> 
<a name="l9383"><span class="ln">9383 </span></a>    .. note:: 
<a name="l9384"><span class="ln">9384 </span></a>        If :attr:`min` is greater than :attr:`max` :func:`torch.clamp(..., min, max) &lt;torch.clamp&gt;` 
<a name="l9385"><span class="ln">9385 </span></a>        sets all elements in :attr:`input` to the value of :attr:`max`. 
<a name="l9386"><span class="ln">9386 </span></a> 
<a name="l9387"><span class="ln">9387 </span></a>    Args: 
<a name="l9388"><span class="ln">9388 </span></a>        input (Tensor): the input tensor. 
<a name="l9389"><span class="ln">9389 </span></a>        min (Number or Tensor, optional): lower-bound of the range to be clamped to 
<a name="l9390"><span class="ln">9390 </span></a>        max (Number or Tensor, optional): upper-bound of the range to be clamped to 
<a name="l9391"><span class="ln">9391 </span></a> 
<a name="l9392"><span class="ln">9392 </span></a>    Keyword args: 
<a name="l9393"><span class="ln">9393 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l9394"><span class="ln">9394 </span></a> 
<a name="l9395"><span class="ln">9395 </span></a>    Example:: 
<a name="l9396"><span class="ln">9396 </span></a> 
<a name="l9397"><span class="ln">9397 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l9398"><span class="ln">9398 </span></a>        &gt;&gt;&gt; a 
<a name="l9399"><span class="ln">9399 </span></a>        tensor([-1.7120,  0.1734, -0.0478, -0.0922]) 
<a name="l9400"><span class="ln">9400 </span></a>        &gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5) 
<a name="l9401"><span class="ln">9401 </span></a>        tensor([-0.5000,  0.1734, -0.0478, -0.0922]) 
<a name="l9402"><span class="ln">9402 </span></a> 
<a name="l9403"><span class="ln">9403 </span></a>        &gt;&gt;&gt; min = torch.linspace(-1, 1, steps=4) 
<a name="l9404"><span class="ln">9404 </span></a>        &gt;&gt;&gt; torch.clamp(a, min=min) 
<a name="l9405"><span class="ln">9405 </span></a>        tensor([-1.0000,  0.1734,  0.3333,  1.0000]) 
<a name="l9406"><span class="ln">9406 </span></a>    &quot;&quot;&quot;</span>
<a name="l9407"><span class="ln">9407 </span></a>
<a name="l9408"><span class="ln">9408 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9409"><span class="ln">9409 </span></a><span class="s2">def </span><span class="s1">clamp_</span><span class="s3">(</span>
<a name="l9410"><span class="ln">9410 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9411"><span class="ln">9411 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9412"><span class="ln">9412 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9413"><span class="ln">9413 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9414"><span class="ln">9414 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9415"><span class="ln">9415 </span></a><span class="s2">def </span><span class="s1">clamp_</span><span class="s3">(</span>
<a name="l9416"><span class="ln">9416 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9417"><span class="ln">9417 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9418"><span class="ln">9418 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9419"><span class="ln">9419 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9420"><span class="ln">9420 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9421"><span class="ln">9421 </span></a><span class="s2">def </span><span class="s1">clamp_max</span><span class="s3">(</span>
<a name="l9422"><span class="ln">9422 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9423"><span class="ln">9423 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9424"><span class="ln">9424 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9425"><span class="ln">9425 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9426"><span class="ln">9426 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9427"><span class="ln">9427 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9428"><span class="ln">9428 </span></a><span class="s2">def </span><span class="s1">clamp_max</span><span class="s3">(</span>
<a name="l9429"><span class="ln">9429 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9430"><span class="ln">9430 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l9431"><span class="ln">9431 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9432"><span class="ln">9432 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9433"><span class="ln">9433 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9434"><span class="ln">9434 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9435"><span class="ln">9435 </span></a><span class="s2">def </span><span class="s1">clamp_max_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">max</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9436"><span class="ln">9436 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9437"><span class="ln">9437 </span></a><span class="s2">def </span><span class="s1">clamp_max_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">max</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9438"><span class="ln">9438 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9439"><span class="ln">9439 </span></a><span class="s2">def </span><span class="s1">clamp_min</span><span class="s3">(</span>
<a name="l9440"><span class="ln">9440 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9441"><span class="ln">9441 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9442"><span class="ln">9442 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9443"><span class="ln">9443 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9444"><span class="ln">9444 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9445"><span class="ln">9445 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9446"><span class="ln">9446 </span></a><span class="s2">def </span><span class="s1">clamp_min</span><span class="s3">(</span>
<a name="l9447"><span class="ln">9447 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9448"><span class="ln">9448 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l9449"><span class="ln">9449 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9450"><span class="ln">9450 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9451"><span class="ln">9451 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9452"><span class="ln">9452 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9453"><span class="ln">9453 </span></a><span class="s2">def </span><span class="s1">clamp_min_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">min</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9454"><span class="ln">9454 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9455"><span class="ln">9455 </span></a><span class="s2">def </span><span class="s1">clamp_min_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">min</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9456"><span class="ln">9456 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9457"><span class="ln">9457 </span></a><span class="s2">def </span><span class="s1">clip</span><span class="s3">(</span>
<a name="l9458"><span class="ln">9458 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9459"><span class="ln">9459 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9460"><span class="ln">9460 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9461"><span class="ln">9461 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9462"><span class="ln">9462 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9463"><span class="ln">9463 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9464"><span class="ln">9464 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9465"><span class="ln">9465 </span></a>    clip(input, min=None, max=None, *, out=None) -&gt; Tensor 
<a name="l9466"><span class="ln">9466 </span></a> 
<a name="l9467"><span class="ln">9467 </span></a>    Alias for :func:`torch.clamp`. 
<a name="l9468"><span class="ln">9468 </span></a>    &quot;&quot;&quot;</span>
<a name="l9469"><span class="ln">9469 </span></a>
<a name="l9470"><span class="ln">9470 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9471"><span class="ln">9471 </span></a><span class="s2">def </span><span class="s1">clip</span><span class="s3">(</span>
<a name="l9472"><span class="ln">9472 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9473"><span class="ln">9473 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9474"><span class="ln">9474 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9475"><span class="ln">9475 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9476"><span class="ln">9476 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9477"><span class="ln">9477 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9478"><span class="ln">9478 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9479"><span class="ln">9479 </span></a>    clip(input, min=None, max=None, *, out=None) -&gt; Tensor 
<a name="l9480"><span class="ln">9480 </span></a> 
<a name="l9481"><span class="ln">9481 </span></a>    Alias for :func:`torch.clamp`. 
<a name="l9482"><span class="ln">9482 </span></a>    &quot;&quot;&quot;</span>
<a name="l9483"><span class="ln">9483 </span></a>
<a name="l9484"><span class="ln">9484 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9485"><span class="ln">9485 </span></a><span class="s2">def </span><span class="s1">clip_</span><span class="s3">(</span>
<a name="l9486"><span class="ln">9486 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9487"><span class="ln">9487 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9488"><span class="ln">9488 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9489"><span class="ln">9489 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9490"><span class="ln">9490 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9491"><span class="ln">9491 </span></a><span class="s2">def </span><span class="s1">clip_</span><span class="s3">(</span>
<a name="l9492"><span class="ln">9492 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9493"><span class="ln">9493 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9494"><span class="ln">9494 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9495"><span class="ln">9495 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9496"><span class="ln">9496 </span></a><span class="s2">def </span><span class="s1">clone</span><span class="s3">(</span>
<a name="l9497"><span class="ln">9497 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9498"><span class="ln">9498 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9499"><span class="ln">9499 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9500"><span class="ln">9500 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9501"><span class="ln">9501 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9502"><span class="ln">9502 </span></a>    clone(input, *, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l9503"><span class="ln">9503 </span></a> 
<a name="l9504"><span class="ln">9504 </span></a>    Returns a copy of :attr:`input`. 
<a name="l9505"><span class="ln">9505 </span></a> 
<a name="l9506"><span class="ln">9506 </span></a>    .. note:: 
<a name="l9507"><span class="ln">9507 </span></a> 
<a name="l9508"><span class="ln">9508 </span></a>        This function is differentiable, so gradients will flow back from the 
<a name="l9509"><span class="ln">9509 </span></a>        result of this operation to :attr:`input`. To create a tensor without an 
<a name="l9510"><span class="ln">9510 </span></a>        autograd relationship to :attr:`input` see :meth:`~Tensor.detach`. 
<a name="l9511"><span class="ln">9511 </span></a> 
<a name="l9512"><span class="ln">9512 </span></a>    Args: 
<a name="l9513"><span class="ln">9513 </span></a>        input (Tensor): the input tensor. 
<a name="l9514"><span class="ln">9514 </span></a> 
<a name="l9515"><span class="ln">9515 </span></a>    Keyword args: 
<a name="l9516"><span class="ln">9516 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l9517"><span class="ln">9517 </span></a>            returned tensor. Default: ``torch.preserve_format``. 
<a name="l9518"><span class="ln">9518 </span></a>    &quot;&quot;&quot;</span>
<a name="l9519"><span class="ln">9519 </span></a>
<a name="l9520"><span class="ln">9520 </span></a><span class="s2">def </span><span class="s1">col_indices_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9521"><span class="ln">9521 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9522"><span class="ln">9522 </span></a>    Performs the same operation as :func:`torch.col_indices`, but all output tensors 
<a name="l9523"><span class="ln">9523 </span></a>    are freshly created instead of aliasing the input. 
<a name="l9524"><span class="ln">9524 </span></a>    &quot;&quot;&quot;</span>
<a name="l9525"><span class="ln">9525 </span></a>
<a name="l9526"><span class="ln">9526 </span></a><span class="s2">def </span><span class="s1">column_stack</span><span class="s3">(</span>
<a name="l9527"><span class="ln">9527 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9528"><span class="ln">9528 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9529"><span class="ln">9529 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9530"><span class="ln">9530 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9531"><span class="ln">9531 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9532"><span class="ln">9532 </span></a>    column_stack(tensors, *, out=None) -&gt; Tensor 
<a name="l9533"><span class="ln">9533 </span></a> 
<a name="l9534"><span class="ln">9534 </span></a>    Creates a new tensor by horizontally stacking the tensors in :attr:`tensors`. 
<a name="l9535"><span class="ln">9535 </span></a> 
<a name="l9536"><span class="ln">9536 </span></a>    Equivalent to ``torch.hstack(tensors)``, except each zero or one dimensional tensor ``t`` 
<a name="l9537"><span class="ln">9537 </span></a>    in :attr:`tensors` is first reshaped into a ``(t.numel(), 1)`` column before being stacked horizontally. 
<a name="l9538"><span class="ln">9538 </span></a> 
<a name="l9539"><span class="ln">9539 </span></a>    Args: 
<a name="l9540"><span class="ln">9540 </span></a>        tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l9541"><span class="ln">9541 </span></a> 
<a name="l9542"><span class="ln">9542 </span></a>    Keyword args: 
<a name="l9543"><span class="ln">9543 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l9544"><span class="ln">9544 </span></a> 
<a name="l9545"><span class="ln">9545 </span></a>    Example:: 
<a name="l9546"><span class="ln">9546 </span></a> 
<a name="l9547"><span class="ln">9547 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l9548"><span class="ln">9548 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l9549"><span class="ln">9549 </span></a>        &gt;&gt;&gt; torch.column_stack((a, b)) 
<a name="l9550"><span class="ln">9550 </span></a>        tensor([[1, 4], 
<a name="l9551"><span class="ln">9551 </span></a>            [2, 5], 
<a name="l9552"><span class="ln">9552 </span></a>            [3, 6]]) 
<a name="l9553"><span class="ln">9553 </span></a>        &gt;&gt;&gt; a = torch.arange(5) 
<a name="l9554"><span class="ln">9554 </span></a>        &gt;&gt;&gt; b = torch.arange(10).reshape(5, 2) 
<a name="l9555"><span class="ln">9555 </span></a>        &gt;&gt;&gt; torch.column_stack((a, b, b)) 
<a name="l9556"><span class="ln">9556 </span></a>        tensor([[0, 0, 1, 0, 1], 
<a name="l9557"><span class="ln">9557 </span></a>                [1, 2, 3, 2, 3], 
<a name="l9558"><span class="ln">9558 </span></a>                [2, 4, 5, 4, 5], 
<a name="l9559"><span class="ln">9559 </span></a>                [3, 6, 7, 6, 7], 
<a name="l9560"><span class="ln">9560 </span></a>                [4, 8, 9, 8, 9]]) 
<a name="l9561"><span class="ln">9561 </span></a>    &quot;&quot;&quot;</span>
<a name="l9562"><span class="ln">9562 </span></a>
<a name="l9563"><span class="ln">9563 </span></a><span class="s2">def </span><span class="s1">combinations</span><span class="s3">(</span>
<a name="l9564"><span class="ln">9564 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9565"><span class="ln">9565 </span></a>    <span class="s1">r</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">2</span><span class="s3">,</span>
<a name="l9566"><span class="ln">9566 </span></a>    <span class="s1">with_replacement</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l9567"><span class="ln">9567 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9568"><span class="ln">9568 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9569"><span class="ln">9569 </span></a>    combinations(input: Tensor, r: int = 2, with_replacement: bool = False) -&gt; seq 
<a name="l9570"><span class="ln">9570 </span></a> 
<a name="l9571"><span class="ln">9571 </span></a>    Compute combinations of length :math:`r` of the given tensor. The behavior is similar to 
<a name="l9572"><span class="ln">9572 </span></a>    python's `itertools.combinations` when `with_replacement` is set to `False`, and 
<a name="l9573"><span class="ln">9573 </span></a>    `itertools.combinations_with_replacement` when `with_replacement` is set to `True`. 
<a name="l9574"><span class="ln">9574 </span></a> 
<a name="l9575"><span class="ln">9575 </span></a>    Arguments: 
<a name="l9576"><span class="ln">9576 </span></a>        input (Tensor): 1D vector. 
<a name="l9577"><span class="ln">9577 </span></a>        r (int, optional): number of elements to combine 
<a name="l9578"><span class="ln">9578 </span></a>        with_replacement (bool, optional): whether to allow duplication in combination 
<a name="l9579"><span class="ln">9579 </span></a> 
<a name="l9580"><span class="ln">9580 </span></a>    Returns: 
<a name="l9581"><span class="ln">9581 </span></a>        Tensor: A tensor equivalent to converting all the input tensors into lists, do 
<a name="l9582"><span class="ln">9582 </span></a>        `itertools.combinations` or `itertools.combinations_with_replacement` on these 
<a name="l9583"><span class="ln">9583 </span></a>        lists, and finally convert the resulting list into tensor. 
<a name="l9584"><span class="ln">9584 </span></a> 
<a name="l9585"><span class="ln">9585 </span></a>    Example:: 
<a name="l9586"><span class="ln">9586 </span></a> 
<a name="l9587"><span class="ln">9587 </span></a>        &gt;&gt;&gt; a = [1, 2, 3] 
<a name="l9588"><span class="ln">9588 </span></a>        &gt;&gt;&gt; list(itertools.combinations(a, r=2)) 
<a name="l9589"><span class="ln">9589 </span></a>        [(1, 2), (1, 3), (2, 3)] 
<a name="l9590"><span class="ln">9590 </span></a>        &gt;&gt;&gt; list(itertools.combinations(a, r=3)) 
<a name="l9591"><span class="ln">9591 </span></a>        [(1, 2, 3)] 
<a name="l9592"><span class="ln">9592 </span></a>        &gt;&gt;&gt; list(itertools.combinations_with_replacement(a, r=2)) 
<a name="l9593"><span class="ln">9593 </span></a>        [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)] 
<a name="l9594"><span class="ln">9594 </span></a>        &gt;&gt;&gt; tensor_a = torch.tensor(a) 
<a name="l9595"><span class="ln">9595 </span></a>        &gt;&gt;&gt; torch.combinations(tensor_a) 
<a name="l9596"><span class="ln">9596 </span></a>        tensor([[1, 2], 
<a name="l9597"><span class="ln">9597 </span></a>                [1, 3], 
<a name="l9598"><span class="ln">9598 </span></a>                [2, 3]]) 
<a name="l9599"><span class="ln">9599 </span></a>        &gt;&gt;&gt; torch.combinations(tensor_a, r=3) 
<a name="l9600"><span class="ln">9600 </span></a>        tensor([[1, 2, 3]]) 
<a name="l9601"><span class="ln">9601 </span></a>        &gt;&gt;&gt; torch.combinations(tensor_a, with_replacement=True) 
<a name="l9602"><span class="ln">9602 </span></a>        tensor([[1, 1], 
<a name="l9603"><span class="ln">9603 </span></a>                [1, 2], 
<a name="l9604"><span class="ln">9604 </span></a>                [1, 3], 
<a name="l9605"><span class="ln">9605 </span></a>                [2, 2], 
<a name="l9606"><span class="ln">9606 </span></a>                [2, 3], 
<a name="l9607"><span class="ln">9607 </span></a>                [3, 3]]) 
<a name="l9608"><span class="ln">9608 </span></a>    &quot;&quot;&quot;</span>
<a name="l9609"><span class="ln">9609 </span></a>
<a name="l9610"><span class="ln">9610 </span></a><span class="s2">def </span><span class="s1">complex</span><span class="s3">(</span>
<a name="l9611"><span class="ln">9611 </span></a>    <span class="s1">real</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9612"><span class="ln">9612 </span></a>    <span class="s1">imag</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9613"><span class="ln">9613 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9614"><span class="ln">9614 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9615"><span class="ln">9615 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9616"><span class="ln">9616 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9617"><span class="ln">9617 </span></a>    complex(real, imag, *, out=None) -&gt; Tensor 
<a name="l9618"><span class="ln">9618 </span></a> 
<a name="l9619"><span class="ln">9619 </span></a>    Constructs a complex tensor with its real part equal to :attr:`real` and its 
<a name="l9620"><span class="ln">9620 </span></a>    imaginary part equal to :attr:`imag`. 
<a name="l9621"><span class="ln">9621 </span></a> 
<a name="l9622"><span class="ln">9622 </span></a>    Args: 
<a name="l9623"><span class="ln">9623 </span></a>        real (Tensor): The real part of the complex tensor. Must be half, float or double. 
<a name="l9624"><span class="ln">9624 </span></a>        imag (Tensor): The imaginary part of the complex tensor. Must be same dtype 
<a name="l9625"><span class="ln">9625 </span></a>            as :attr:`real`. 
<a name="l9626"><span class="ln">9626 </span></a> 
<a name="l9627"><span class="ln">9627 </span></a>    Keyword args: 
<a name="l9628"><span class="ln">9628 </span></a>        out (Tensor): If the inputs are ``torch.float32``, must be 
<a name="l9629"><span class="ln">9629 </span></a>            ``torch.complex64``. If the inputs are ``torch.float64``, must be 
<a name="l9630"><span class="ln">9630 </span></a>            ``torch.complex128``. 
<a name="l9631"><span class="ln">9631 </span></a> 
<a name="l9632"><span class="ln">9632 </span></a>    Example:: 
<a name="l9633"><span class="ln">9633 </span></a> 
<a name="l9634"><span class="ln">9634 </span></a>        &gt;&gt;&gt; real = torch.tensor([1, 2], dtype=torch.float32) 
<a name="l9635"><span class="ln">9635 </span></a>        &gt;&gt;&gt; imag = torch.tensor([3, 4], dtype=torch.float32) 
<a name="l9636"><span class="ln">9636 </span></a>        &gt;&gt;&gt; z = torch.complex(real, imag) 
<a name="l9637"><span class="ln">9637 </span></a>        &gt;&gt;&gt; z 
<a name="l9638"><span class="ln">9638 </span></a>        tensor([(1.+3.j), (2.+4.j)]) 
<a name="l9639"><span class="ln">9639 </span></a>        &gt;&gt;&gt; z.dtype 
<a name="l9640"><span class="ln">9640 </span></a>        torch.complex64 
<a name="l9641"><span class="ln">9641 </span></a>    &quot;&quot;&quot;</span>
<a name="l9642"><span class="ln">9642 </span></a>
<a name="l9643"><span class="ln">9643 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9644"><span class="ln">9644 </span></a><span class="s2">def </span><span class="s1">concat</span><span class="s3">(</span>
<a name="l9645"><span class="ln">9645 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9646"><span class="ln">9646 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9647"><span class="ln">9647 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9648"><span class="ln">9648 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9649"><span class="ln">9649 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9650"><span class="ln">9650 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9651"><span class="ln">9651 </span></a>    concat(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l9652"><span class="ln">9652 </span></a> 
<a name="l9653"><span class="ln">9653 </span></a>    Alias of :func:`torch.cat`. 
<a name="l9654"><span class="ln">9654 </span></a>    &quot;&quot;&quot;</span>
<a name="l9655"><span class="ln">9655 </span></a>
<a name="l9656"><span class="ln">9656 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9657"><span class="ln">9657 </span></a><span class="s2">def </span><span class="s1">concat</span><span class="s3">(</span>
<a name="l9658"><span class="ln">9658 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9659"><span class="ln">9659 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9660"><span class="ln">9660 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9661"><span class="ln">9661 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9662"><span class="ln">9662 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9663"><span class="ln">9663 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9664"><span class="ln">9664 </span></a>    concat(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l9665"><span class="ln">9665 </span></a> 
<a name="l9666"><span class="ln">9666 </span></a>    Alias of :func:`torch.cat`. 
<a name="l9667"><span class="ln">9667 </span></a>    &quot;&quot;&quot;</span>
<a name="l9668"><span class="ln">9668 </span></a>
<a name="l9669"><span class="ln">9669 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9670"><span class="ln">9670 </span></a><span class="s2">def </span><span class="s1">concatenate</span><span class="s3">(</span>
<a name="l9671"><span class="ln">9671 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9672"><span class="ln">9672 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9673"><span class="ln">9673 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9674"><span class="ln">9674 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9675"><span class="ln">9675 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9676"><span class="ln">9676 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9677"><span class="ln">9677 </span></a>    concatenate(tensors, axis=0, out=None) -&gt; Tensor 
<a name="l9678"><span class="ln">9678 </span></a> 
<a name="l9679"><span class="ln">9679 </span></a>    Alias of :func:`torch.cat`. 
<a name="l9680"><span class="ln">9680 </span></a>    &quot;&quot;&quot;</span>
<a name="l9681"><span class="ln">9681 </span></a>
<a name="l9682"><span class="ln">9682 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9683"><span class="ln">9683 </span></a><span class="s2">def </span><span class="s1">concatenate</span><span class="s3">(</span>
<a name="l9684"><span class="ln">9684 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9685"><span class="ln">9685 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9686"><span class="ln">9686 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9687"><span class="ln">9687 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9688"><span class="ln">9688 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9689"><span class="ln">9689 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9690"><span class="ln">9690 </span></a>    concatenate(tensors, axis=0, out=None) -&gt; Tensor 
<a name="l9691"><span class="ln">9691 </span></a> 
<a name="l9692"><span class="ln">9692 </span></a>    Alias of :func:`torch.cat`. 
<a name="l9693"><span class="ln">9693 </span></a>    &quot;&quot;&quot;</span>
<a name="l9694"><span class="ln">9694 </span></a>
<a name="l9695"><span class="ln">9695 </span></a><span class="s2">def </span><span class="s1">conj</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9696"><span class="ln">9696 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9697"><span class="ln">9697 </span></a>    conj(input) -&gt; Tensor 
<a name="l9698"><span class="ln">9698 </span></a> 
<a name="l9699"><span class="ln">9699 </span></a>    Returns a view of :attr:`input` with a flipped conjugate bit. If :attr:`input` has a non-complex dtype, 
<a name="l9700"><span class="ln">9700 </span></a>    this function just returns :attr:`input`. 
<a name="l9701"><span class="ln">9701 </span></a> 
<a name="l9702"><span class="ln">9702 </span></a>    .. note:: 
<a name="l9703"><span class="ln">9703 </span></a>        :func:`torch.conj` performs a lazy conjugation, but the actual conjugated tensor can be materialized 
<a name="l9704"><span class="ln">9704 </span></a>        at any time using :func:`torch.resolve_conj`. 
<a name="l9705"><span class="ln">9705 </span></a> 
<a name="l9706"><span class="ln">9706 </span></a>    .. warning:: In the future, :func:`torch.conj` may return a non-writeable view for an :attr:`input` of 
<a name="l9707"><span class="ln">9707 </span></a>                 non-complex dtype. It's recommended that programs not modify the tensor returned by :func:`torch.conj_physical` 
<a name="l9708"><span class="ln">9708 </span></a>                 when :attr:`input` is of non-complex dtype to be compatible with this change. 
<a name="l9709"><span class="ln">9709 </span></a> 
<a name="l9710"><span class="ln">9710 </span></a>    Args: 
<a name="l9711"><span class="ln">9711 </span></a>        input (Tensor): the input tensor. 
<a name="l9712"><span class="ln">9712 </span></a> 
<a name="l9713"><span class="ln">9713 </span></a>    Example:: 
<a name="l9714"><span class="ln">9714 </span></a> 
<a name="l9715"><span class="ln">9715 </span></a>        &gt;&gt;&gt; x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]) 
<a name="l9716"><span class="ln">9716 </span></a>        &gt;&gt;&gt; x.is_conj() 
<a name="l9717"><span class="ln">9717 </span></a>        False 
<a name="l9718"><span class="ln">9718 </span></a>        &gt;&gt;&gt; y = torch.conj(x) 
<a name="l9719"><span class="ln">9719 </span></a>        &gt;&gt;&gt; y.is_conj() 
<a name="l9720"><span class="ln">9720 </span></a>        True 
<a name="l9721"><span class="ln">9721 </span></a>    &quot;&quot;&quot;</span>
<a name="l9722"><span class="ln">9722 </span></a>
<a name="l9723"><span class="ln">9723 </span></a><span class="s2">def </span><span class="s1">conj_physical</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9724"><span class="ln">9724 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9725"><span class="ln">9725 </span></a>    conj_physical(input, *, out=None) -&gt; Tensor 
<a name="l9726"><span class="ln">9726 </span></a> 
<a name="l9727"><span class="ln">9727 </span></a>    Computes the element-wise conjugate of the given :attr:`input` tensor. 
<a name="l9728"><span class="ln">9728 </span></a>    If :attr:`input` has a non-complex dtype, this function just returns :attr:`input`. 
<a name="l9729"><span class="ln">9729 </span></a> 
<a name="l9730"><span class="ln">9730 </span></a>    .. note:: 
<a name="l9731"><span class="ln">9731 </span></a>       This performs the conjugate operation regardless of the fact conjugate bit is set or not. 
<a name="l9732"><span class="ln">9732 </span></a> 
<a name="l9733"><span class="ln">9733 </span></a>    .. warning:: In the future, :func:`torch.conj_physical` may return a non-writeable view for an :attr:`input` of 
<a name="l9734"><span class="ln">9734 </span></a>                 non-complex dtype. It's recommended that programs not modify the tensor returned by :func:`torch.conj_physical` 
<a name="l9735"><span class="ln">9735 </span></a>                 when :attr:`input` is of non-complex dtype to be compatible with this change. 
<a name="l9736"><span class="ln">9736 </span></a> 
<a name="l9737"><span class="ln">9737 </span></a>    .. math:: 
<a name="l9738"><span class="ln">9738 </span></a>        \text{out}_{i} = conj(\text{input}_{i}) 
<a name="l9739"><span class="ln">9739 </span></a> 
<a name="l9740"><span class="ln">9740 </span></a>    Args: 
<a name="l9741"><span class="ln">9741 </span></a>        input (Tensor): the input tensor. 
<a name="l9742"><span class="ln">9742 </span></a> 
<a name="l9743"><span class="ln">9743 </span></a>    Keyword args: 
<a name="l9744"><span class="ln">9744 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l9745"><span class="ln">9745 </span></a> 
<a name="l9746"><span class="ln">9746 </span></a>    Example:: 
<a name="l9747"><span class="ln">9747 </span></a> 
<a name="l9748"><span class="ln">9748 </span></a>        &gt;&gt;&gt; torch.conj_physical(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])) 
<a name="l9749"><span class="ln">9749 </span></a>        tensor([-1 - 1j, -2 - 2j, 3 + 3j]) 
<a name="l9750"><span class="ln">9750 </span></a>    &quot;&quot;&quot;</span>
<a name="l9751"><span class="ln">9751 </span></a>
<a name="l9752"><span class="ln">9752 </span></a><span class="s2">def </span><span class="s1">conj_physical_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9753"><span class="ln">9753 </span></a><span class="s2">def </span><span class="s1">constant_pad_nd</span><span class="s3">(</span>
<a name="l9754"><span class="ln">9754 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9755"><span class="ln">9755 </span></a>    <span class="s1">pad</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l9756"><span class="ln">9756 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9757"><span class="ln">9757 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9758"><span class="ln">9758 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9759"><span class="ln">9759 </span></a><span class="s2">def </span><span class="s1">conv1d</span><span class="s3">(</span>
<a name="l9760"><span class="ln">9760 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9761"><span class="ln">9761 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9762"><span class="ln">9762 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9763"><span class="ln">9763 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9764"><span class="ln">9764 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9765"><span class="ln">9765 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9766"><span class="ln">9766 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9767"><span class="ln">9767 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9768"><span class="ln">9768 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9769"><span class="ln">9769 </span></a><span class="s2">def </span><span class="s1">conv1d</span><span class="s3">(</span>
<a name="l9770"><span class="ln">9770 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9771"><span class="ln">9771 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9772"><span class="ln">9772 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9773"><span class="ln">9773 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9774"><span class="ln">9774 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;valid&quot;</span><span class="s3">,</span>
<a name="l9775"><span class="ln">9775 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9776"><span class="ln">9776 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9777"><span class="ln">9777 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9778"><span class="ln">9778 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9779"><span class="ln">9779 </span></a><span class="s2">def </span><span class="s1">conv2d</span><span class="s3">(</span>
<a name="l9780"><span class="ln">9780 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9781"><span class="ln">9781 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9782"><span class="ln">9782 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9783"><span class="ln">9783 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9784"><span class="ln">9784 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9785"><span class="ln">9785 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9786"><span class="ln">9786 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9787"><span class="ln">9787 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9788"><span class="ln">9788 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9789"><span class="ln">9789 </span></a><span class="s2">def </span><span class="s1">conv2d</span><span class="s3">(</span>
<a name="l9790"><span class="ln">9790 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9791"><span class="ln">9791 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9792"><span class="ln">9792 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9793"><span class="ln">9793 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9794"><span class="ln">9794 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;valid&quot;</span><span class="s3">,</span>
<a name="l9795"><span class="ln">9795 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9796"><span class="ln">9796 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9797"><span class="ln">9797 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9798"><span class="ln">9798 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9799"><span class="ln">9799 </span></a><span class="s2">def </span><span class="s1">conv3d</span><span class="s3">(</span>
<a name="l9800"><span class="ln">9800 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9801"><span class="ln">9801 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9802"><span class="ln">9802 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9803"><span class="ln">9803 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9804"><span class="ln">9804 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9805"><span class="ln">9805 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9806"><span class="ln">9806 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9807"><span class="ln">9807 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9808"><span class="ln">9808 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9809"><span class="ln">9809 </span></a><span class="s2">def </span><span class="s1">conv3d</span><span class="s3">(</span>
<a name="l9810"><span class="ln">9810 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9811"><span class="ln">9811 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9812"><span class="ln">9812 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9813"><span class="ln">9813 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9814"><span class="ln">9814 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;valid&quot;</span><span class="s3">,</span>
<a name="l9815"><span class="ln">9815 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9816"><span class="ln">9816 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9817"><span class="ln">9817 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9818"><span class="ln">9818 </span></a><span class="s2">def </span><span class="s1">conv_tbc</span><span class="s3">(</span>
<a name="l9819"><span class="ln">9819 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9820"><span class="ln">9820 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9821"><span class="ln">9821 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9822"><span class="ln">9822 </span></a>    <span class="s1">pad</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9823"><span class="ln">9823 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9824"><span class="ln">9824 </span></a><span class="s2">def </span><span class="s1">conv_transpose1d</span><span class="s3">(</span>
<a name="l9825"><span class="ln">9825 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9826"><span class="ln">9826 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9827"><span class="ln">9827 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9828"><span class="ln">9828 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9829"><span class="ln">9829 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9830"><span class="ln">9830 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9831"><span class="ln">9831 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9832"><span class="ln">9832 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9833"><span class="ln">9833 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9834"><span class="ln">9834 </span></a><span class="s2">def </span><span class="s1">conv_transpose2d</span><span class="s3">(</span>
<a name="l9835"><span class="ln">9835 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9836"><span class="ln">9836 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9837"><span class="ln">9837 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9838"><span class="ln">9838 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9839"><span class="ln">9839 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9840"><span class="ln">9840 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9841"><span class="ln">9841 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9842"><span class="ln">9842 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9843"><span class="ln">9843 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9844"><span class="ln">9844 </span></a><span class="s2">def </span><span class="s1">conv_transpose3d</span><span class="s3">(</span>
<a name="l9845"><span class="ln">9845 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9846"><span class="ln">9846 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9847"><span class="ln">9847 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9848"><span class="ln">9848 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9849"><span class="ln">9849 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9850"><span class="ln">9850 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l9851"><span class="ln">9851 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9852"><span class="ln">9852 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">] </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l9853"><span class="ln">9853 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9854"><span class="ln">9854 </span></a><span class="s2">def </span><span class="s1">convolution</span><span class="s3">(</span>
<a name="l9855"><span class="ln">9855 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9856"><span class="ln">9856 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9857"><span class="ln">9857 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l9858"><span class="ln">9858 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l9859"><span class="ln">9859 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l9860"><span class="ln">9860 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l9861"><span class="ln">9861 </span></a>    <span class="s1">transposed</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l9862"><span class="ln">9862 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l9863"><span class="ln">9863 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l9864"><span class="ln">9864 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l9865"><span class="ln">9865 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9866"><span class="ln">9866 </span></a><span class="s2">def </span><span class="s1">copysign</span><span class="s3">(</span>
<a name="l9867"><span class="ln">9867 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9868"><span class="ln">9868 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9869"><span class="ln">9869 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9870"><span class="ln">9870 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9871"><span class="ln">9871 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9872"><span class="ln">9872 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9873"><span class="ln">9873 </span></a>    copysign(input, other, *, out=None) -&gt; Tensor 
<a name="l9874"><span class="ln">9874 </span></a> 
<a name="l9875"><span class="ln">9875 </span></a>    Create a new floating-point tensor with the magnitude of :attr:`input` and the sign of :attr:`other`, elementwise. 
<a name="l9876"><span class="ln">9876 </span></a> 
<a name="l9877"><span class="ln">9877 </span></a>    .. math:: 
<a name="l9878"><span class="ln">9878 </span></a>        \text{out}_{i} = \begin{cases} 
<a name="l9879"><span class="ln">9879 </span></a>            -|\text{input}_{i}| &amp; \text{if } \text{other}_{i} \leq -0.0 \\ 
<a name="l9880"><span class="ln">9880 </span></a>             |\text{input}_{i}| &amp; \text{if } \text{other}_{i} \geq 0.0 \\ 
<a name="l9881"><span class="ln">9881 </span></a>        \end{cases} 
<a name="l9882"><span class="ln">9882 </span></a> 
<a name="l9883"><span class="ln">9883 </span></a> 
<a name="l9884"><span class="ln">9884 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l9885"><span class="ln">9885 </span></a>    and integer and float inputs. 
<a name="l9886"><span class="ln">9886 </span></a> 
<a name="l9887"><span class="ln">9887 </span></a>    Args: 
<a name="l9888"><span class="ln">9888 </span></a>        input (Tensor): magnitudes. 
<a name="l9889"><span class="ln">9889 </span></a>        other (Tensor or Number): contains value(s) whose signbit(s) are 
<a name="l9890"><span class="ln">9890 </span></a>            applied to the magnitudes in :attr:`input`. 
<a name="l9891"><span class="ln">9891 </span></a> 
<a name="l9892"><span class="ln">9892 </span></a>    Keyword args: 
<a name="l9893"><span class="ln">9893 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l9894"><span class="ln">9894 </span></a> 
<a name="l9895"><span class="ln">9895 </span></a>    Example:: 
<a name="l9896"><span class="ln">9896 </span></a> 
<a name="l9897"><span class="ln">9897 </span></a>        &gt;&gt;&gt; a = torch.randn(5) 
<a name="l9898"><span class="ln">9898 </span></a>        &gt;&gt;&gt; a 
<a name="l9899"><span class="ln">9899 </span></a>        tensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244]) 
<a name="l9900"><span class="ln">9900 </span></a>        &gt;&gt;&gt; torch.copysign(a, 1) 
<a name="l9901"><span class="ln">9901 </span></a>        tensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244]) 
<a name="l9902"><span class="ln">9902 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l9903"><span class="ln">9903 </span></a>        &gt;&gt;&gt; a 
<a name="l9904"><span class="ln">9904 </span></a>        tensor([[ 0.7079,  0.2778, -1.0249,  0.5719], 
<a name="l9905"><span class="ln">9905 </span></a>                [-0.0059, -0.2600, -0.4475, -1.3948], 
<a name="l9906"><span class="ln">9906 </span></a>                [ 0.3667, -0.9567, -2.5757, -0.1751], 
<a name="l9907"><span class="ln">9907 </span></a>                [ 0.2046, -0.0742,  0.2998, -0.1054]]) 
<a name="l9908"><span class="ln">9908 </span></a>        &gt;&gt;&gt; b = torch.randn(4) 
<a name="l9909"><span class="ln">9909 </span></a>        tensor([ 0.2373,  0.3120,  0.3190, -1.1128]) 
<a name="l9910"><span class="ln">9910 </span></a>        &gt;&gt;&gt; torch.copysign(a, b) 
<a name="l9911"><span class="ln">9911 </span></a>        tensor([[ 0.7079,  0.2778,  1.0249, -0.5719], 
<a name="l9912"><span class="ln">9912 </span></a>                [ 0.0059,  0.2600,  0.4475, -1.3948], 
<a name="l9913"><span class="ln">9913 </span></a>                [ 0.3667,  0.9567,  2.5757, -0.1751], 
<a name="l9914"><span class="ln">9914 </span></a>                [ 0.2046,  0.0742,  0.2998, -0.1054]]) 
<a name="l9915"><span class="ln">9915 </span></a>        &gt;&gt;&gt; a = torch.tensor([1.]) 
<a name="l9916"><span class="ln">9916 </span></a>        &gt;&gt;&gt; b = torch.tensor([-0.]) 
<a name="l9917"><span class="ln">9917 </span></a>        &gt;&gt;&gt; torch.copysign(a, b) 
<a name="l9918"><span class="ln">9918 </span></a>        tensor([-1.]) 
<a name="l9919"><span class="ln">9919 </span></a> 
<a name="l9920"><span class="ln">9920 </span></a>    .. note:: 
<a name="l9921"><span class="ln">9921 </span></a>        copysign handles signed zeros. If the other argument has a negative zero (-0), 
<a name="l9922"><span class="ln">9922 </span></a>        the corresponding output value will be negative. 
<a name="l9923"><span class="ln">9923 </span></a>    &quot;&quot;&quot;</span>
<a name="l9924"><span class="ln">9924 </span></a>
<a name="l9925"><span class="ln">9925 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l9926"><span class="ln">9926 </span></a><span class="s2">def </span><span class="s1">copysign</span><span class="s3">(</span>
<a name="l9927"><span class="ln">9927 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l9928"><span class="ln">9928 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l9929"><span class="ln">9929 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l9930"><span class="ln">9930 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l9931"><span class="ln">9931 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9932"><span class="ln">9932 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9933"><span class="ln">9933 </span></a>    copysign(input, other, *, out=None) -&gt; Tensor 
<a name="l9934"><span class="ln">9934 </span></a> 
<a name="l9935"><span class="ln">9935 </span></a>    Create a new floating-point tensor with the magnitude of :attr:`input` and the sign of :attr:`other`, elementwise. 
<a name="l9936"><span class="ln">9936 </span></a> 
<a name="l9937"><span class="ln">9937 </span></a>    .. math:: 
<a name="l9938"><span class="ln">9938 </span></a>        \text{out}_{i} = \begin{cases} 
<a name="l9939"><span class="ln">9939 </span></a>            -|\text{input}_{i}| &amp; \text{if } \text{other}_{i} \leq -0.0 \\ 
<a name="l9940"><span class="ln">9940 </span></a>             |\text{input}_{i}| &amp; \text{if } \text{other}_{i} \geq 0.0 \\ 
<a name="l9941"><span class="ln">9941 </span></a>        \end{cases} 
<a name="l9942"><span class="ln">9942 </span></a> 
<a name="l9943"><span class="ln">9943 </span></a> 
<a name="l9944"><span class="ln">9944 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l9945"><span class="ln">9945 </span></a>    and integer and float inputs. 
<a name="l9946"><span class="ln">9946 </span></a> 
<a name="l9947"><span class="ln">9947 </span></a>    Args: 
<a name="l9948"><span class="ln">9948 </span></a>        input (Tensor): magnitudes. 
<a name="l9949"><span class="ln">9949 </span></a>        other (Tensor or Number): contains value(s) whose signbit(s) are 
<a name="l9950"><span class="ln">9950 </span></a>            applied to the magnitudes in :attr:`input`. 
<a name="l9951"><span class="ln">9951 </span></a> 
<a name="l9952"><span class="ln">9952 </span></a>    Keyword args: 
<a name="l9953"><span class="ln">9953 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l9954"><span class="ln">9954 </span></a> 
<a name="l9955"><span class="ln">9955 </span></a>    Example:: 
<a name="l9956"><span class="ln">9956 </span></a> 
<a name="l9957"><span class="ln">9957 </span></a>        &gt;&gt;&gt; a = torch.randn(5) 
<a name="l9958"><span class="ln">9958 </span></a>        &gt;&gt;&gt; a 
<a name="l9959"><span class="ln">9959 </span></a>        tensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244]) 
<a name="l9960"><span class="ln">9960 </span></a>        &gt;&gt;&gt; torch.copysign(a, 1) 
<a name="l9961"><span class="ln">9961 </span></a>        tensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244]) 
<a name="l9962"><span class="ln">9962 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l9963"><span class="ln">9963 </span></a>        &gt;&gt;&gt; a 
<a name="l9964"><span class="ln">9964 </span></a>        tensor([[ 0.7079,  0.2778, -1.0249,  0.5719], 
<a name="l9965"><span class="ln">9965 </span></a>                [-0.0059, -0.2600, -0.4475, -1.3948], 
<a name="l9966"><span class="ln">9966 </span></a>                [ 0.3667, -0.9567, -2.5757, -0.1751], 
<a name="l9967"><span class="ln">9967 </span></a>                [ 0.2046, -0.0742,  0.2998, -0.1054]]) 
<a name="l9968"><span class="ln">9968 </span></a>        &gt;&gt;&gt; b = torch.randn(4) 
<a name="l9969"><span class="ln">9969 </span></a>        tensor([ 0.2373,  0.3120,  0.3190, -1.1128]) 
<a name="l9970"><span class="ln">9970 </span></a>        &gt;&gt;&gt; torch.copysign(a, b) 
<a name="l9971"><span class="ln">9971 </span></a>        tensor([[ 0.7079,  0.2778,  1.0249, -0.5719], 
<a name="l9972"><span class="ln">9972 </span></a>                [ 0.0059,  0.2600,  0.4475, -1.3948], 
<a name="l9973"><span class="ln">9973 </span></a>                [ 0.3667,  0.9567,  2.5757, -0.1751], 
<a name="l9974"><span class="ln">9974 </span></a>                [ 0.2046,  0.0742,  0.2998, -0.1054]]) 
<a name="l9975"><span class="ln">9975 </span></a>        &gt;&gt;&gt; a = torch.tensor([1.]) 
<a name="l9976"><span class="ln">9976 </span></a>        &gt;&gt;&gt; b = torch.tensor([-0.]) 
<a name="l9977"><span class="ln">9977 </span></a>        &gt;&gt;&gt; torch.copysign(a, b) 
<a name="l9978"><span class="ln">9978 </span></a>        tensor([-1.]) 
<a name="l9979"><span class="ln">9979 </span></a> 
<a name="l9980"><span class="ln">9980 </span></a>    .. note:: 
<a name="l9981"><span class="ln">9981 </span></a>        copysign handles signed zeros. If the other argument has a negative zero (-0), 
<a name="l9982"><span class="ln">9982 </span></a>        the corresponding output value will be negative. 
<a name="l9983"><span class="ln">9983 </span></a>    &quot;&quot;&quot;</span>
<a name="l9984"><span class="ln">9984 </span></a>
<a name="l9985"><span class="ln">9985 </span></a><span class="s2">def </span><span class="s1">corrcoef</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l9986"><span class="ln">9986 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l9987"><span class="ln">9987 </span></a>    corrcoef(input) -&gt; Tensor 
<a name="l9988"><span class="ln">9988 </span></a> 
<a name="l9989"><span class="ln">9989 </span></a>    Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the :attr:`input` matrix, 
<a name="l9990"><span class="ln">9990 </span></a>    where rows are the variables and columns are the observations. 
<a name="l9991"><span class="ln">9991 </span></a> 
<a name="l9992"><span class="ln">9992 </span></a>    .. note:: 
<a name="l9993"><span class="ln">9993 </span></a> 
<a name="l9994"><span class="ln">9994 </span></a>        The correlation coefficient matrix R is computed using the covariance matrix C as given by 
<a name="l9995"><span class="ln">9995 </span></a>        :math:`R_{ij} = \frac{ C_{ij} } { \sqrt{ C_{ii} * C_{jj} } }` 
<a name="l9996"><span class="ln">9996 </span></a> 
<a name="l9997"><span class="ln">9997 </span></a>    .. note:: 
<a name="l9998"><span class="ln">9998 </span></a> 
<a name="l9999"><span class="ln">9999 </span></a>        Due to floating point rounding, the resulting array may not be Hermitian and its diagonal elements may not be 1. 
<a name="l10000"><span class="ln">10000 </span></a>        The real and imaginary values are clipped to the interval [-1, 1] in an attempt to improve this situation. 
<a name="l10001"><span class="ln">10001 </span></a> 
<a name="l10002"><span class="ln">10002 </span></a>    Args: 
<a name="l10003"><span class="ln">10003 </span></a>        input (Tensor): A 2D matrix containing multiple variables and observations, or a 
<a name="l10004"><span class="ln">10004 </span></a>            Scalar or 1D vector representing a single variable. 
<a name="l10005"><span class="ln">10005 </span></a> 
<a name="l10006"><span class="ln">10006 </span></a>    Returns: 
<a name="l10007"><span class="ln">10007 </span></a>        (Tensor) The correlation coefficient matrix of the variables. 
<a name="l10008"><span class="ln">10008 </span></a> 
<a name="l10009"><span class="ln">10009 </span></a>    .. seealso:: 
<a name="l10010"><span class="ln">10010 </span></a> 
<a name="l10011"><span class="ln">10011 </span></a>            :func:`torch.cov` covariance matrix. 
<a name="l10012"><span class="ln">10012 </span></a> 
<a name="l10013"><span class="ln">10013 </span></a>    Example:: 
<a name="l10014"><span class="ln">10014 </span></a> 
<a name="l10015"><span class="ln">10015 </span></a>        &gt;&gt;&gt; x = torch.tensor([[0, 1, 2], [2, 1, 0]]) 
<a name="l10016"><span class="ln">10016 </span></a>        &gt;&gt;&gt; torch.corrcoef(x) 
<a name="l10017"><span class="ln">10017 </span></a>        tensor([[ 1., -1.], 
<a name="l10018"><span class="ln">10018 </span></a>                [-1.,  1.]]) 
<a name="l10019"><span class="ln">10019 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 4) 
<a name="l10020"><span class="ln">10020 </span></a>        &gt;&gt;&gt; x 
<a name="l10021"><span class="ln">10021 </span></a>        tensor([[-0.2678, -0.0908, -0.3766,  0.2780], 
<a name="l10022"><span class="ln">10022 </span></a>                [-0.5812,  0.1535,  0.2387,  0.2350]]) 
<a name="l10023"><span class="ln">10023 </span></a>        &gt;&gt;&gt; torch.corrcoef(x) 
<a name="l10024"><span class="ln">10024 </span></a>        tensor([[1.0000, 0.3582], 
<a name="l10025"><span class="ln">10025 </span></a>                [0.3582, 1.0000]]) 
<a name="l10026"><span class="ln">10026 </span></a>        &gt;&gt;&gt; torch.corrcoef(x[0]) 
<a name="l10027"><span class="ln">10027 </span></a>        tensor(1.) 
<a name="l10028"><span class="ln">10028 </span></a>    &quot;&quot;&quot;</span>
<a name="l10029"><span class="ln">10029 </span></a>
<a name="l10030"><span class="ln">10030 </span></a><span class="s2">def </span><span class="s1">cos</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10031"><span class="ln">10031 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10032"><span class="ln">10032 </span></a>    cos(input, *, out=None) -&gt; Tensor 
<a name="l10033"><span class="ln">10033 </span></a> 
<a name="l10034"><span class="ln">10034 </span></a>    Returns a new tensor with the cosine  of the elements of :attr:`input`. 
<a name="l10035"><span class="ln">10035 </span></a> 
<a name="l10036"><span class="ln">10036 </span></a>    .. math:: 
<a name="l10037"><span class="ln">10037 </span></a>        \text{out}_{i} = \cos(\text{input}_{i}) 
<a name="l10038"><span class="ln">10038 </span></a> 
<a name="l10039"><span class="ln">10039 </span></a>    Args: 
<a name="l10040"><span class="ln">10040 </span></a>        input (Tensor): the input tensor. 
<a name="l10041"><span class="ln">10041 </span></a> 
<a name="l10042"><span class="ln">10042 </span></a>    Keyword args: 
<a name="l10043"><span class="ln">10043 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10044"><span class="ln">10044 </span></a> 
<a name="l10045"><span class="ln">10045 </span></a>    Example:: 
<a name="l10046"><span class="ln">10046 </span></a> 
<a name="l10047"><span class="ln">10047 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l10048"><span class="ln">10048 </span></a>        &gt;&gt;&gt; a 
<a name="l10049"><span class="ln">10049 </span></a>        tensor([ 1.4309,  1.2706, -0.8562,  0.9796]) 
<a name="l10050"><span class="ln">10050 </span></a>        &gt;&gt;&gt; torch.cos(a) 
<a name="l10051"><span class="ln">10051 </span></a>        tensor([ 0.1395,  0.2957,  0.6553,  0.5574]) 
<a name="l10052"><span class="ln">10052 </span></a>    &quot;&quot;&quot;</span>
<a name="l10053"><span class="ln">10053 </span></a>
<a name="l10054"><span class="ln">10054 </span></a><span class="s2">def </span><span class="s1">cos_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10055"><span class="ln">10055 </span></a><span class="s2">def </span><span class="s1">cosh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10056"><span class="ln">10056 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10057"><span class="ln">10057 </span></a>    cosh(input, *, out=None) -&gt; Tensor 
<a name="l10058"><span class="ln">10058 </span></a> 
<a name="l10059"><span class="ln">10059 </span></a>    Returns a new tensor with the hyperbolic cosine  of the elements of 
<a name="l10060"><span class="ln">10060 </span></a>    :attr:`input`. 
<a name="l10061"><span class="ln">10061 </span></a> 
<a name="l10062"><span class="ln">10062 </span></a>    .. math:: 
<a name="l10063"><span class="ln">10063 </span></a>        \text{out}_{i} = \cosh(\text{input}_{i}) 
<a name="l10064"><span class="ln">10064 </span></a> 
<a name="l10065"><span class="ln">10065 </span></a>    Args: 
<a name="l10066"><span class="ln">10066 </span></a>        input (Tensor): the input tensor. 
<a name="l10067"><span class="ln">10067 </span></a> 
<a name="l10068"><span class="ln">10068 </span></a>    Keyword args: 
<a name="l10069"><span class="ln">10069 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10070"><span class="ln">10070 </span></a> 
<a name="l10071"><span class="ln">10071 </span></a>    Example:: 
<a name="l10072"><span class="ln">10072 </span></a> 
<a name="l10073"><span class="ln">10073 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l10074"><span class="ln">10074 </span></a>        &gt;&gt;&gt; a 
<a name="l10075"><span class="ln">10075 </span></a>        tensor([ 0.1632,  1.1835, -0.6979, -0.7325]) 
<a name="l10076"><span class="ln">10076 </span></a>        &gt;&gt;&gt; torch.cosh(a) 
<a name="l10077"><span class="ln">10077 </span></a>        tensor([ 1.0133,  1.7860,  1.2536,  1.2805]) 
<a name="l10078"><span class="ln">10078 </span></a> 
<a name="l10079"><span class="ln">10079 </span></a>    .. note:: 
<a name="l10080"><span class="ln">10080 </span></a>       When :attr:`input` is on the CPU, the implementation of torch.cosh may use 
<a name="l10081"><span class="ln">10081 </span></a>       the Sleef library, which rounds very large results to infinity or negative 
<a name="l10082"><span class="ln">10082 </span></a>       infinity. See `here &lt;https://sleef.org/purec.xhtml&gt;`_ for details. 
<a name="l10083"><span class="ln">10083 </span></a>    &quot;&quot;&quot;</span>
<a name="l10084"><span class="ln">10084 </span></a>
<a name="l10085"><span class="ln">10085 </span></a><span class="s2">def </span><span class="s1">cosh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10086"><span class="ln">10086 </span></a><span class="s2">def </span><span class="s1">cosine_embedding_loss</span><span class="s3">(</span>
<a name="l10087"><span class="ln">10087 </span></a>    <span class="s1">input1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10088"><span class="ln">10088 </span></a>    <span class="s1">input2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10089"><span class="ln">10089 </span></a>    <span class="s1">target</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10090"><span class="ln">10090 </span></a>    <span class="s1">margin</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l10091"><span class="ln">10091 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l10092"><span class="ln">10092 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10093"><span class="ln">10093 </span></a><span class="s2">def </span><span class="s1">cosine_similarity</span><span class="s3">(</span>
<a name="l10094"><span class="ln">10094 </span></a>    <span class="s1">x1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10095"><span class="ln">10095 </span></a>    <span class="s1">x2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10096"><span class="ln">10096 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l10097"><span class="ln">10097 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-08</span><span class="s3">,</span>
<a name="l10098"><span class="ln">10098 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10099"><span class="ln">10099 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10100"><span class="ln">10100 </span></a><span class="s2">def </span><span class="s1">count_nonzero</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10101"><span class="ln">10101 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10102"><span class="ln">10102 </span></a>    count_nonzero(input, dim=None) -&gt; Tensor 
<a name="l10103"><span class="ln">10103 </span></a> 
<a name="l10104"><span class="ln">10104 </span></a>    Counts the number of non-zero values in the tensor :attr:`input` along the given :attr:`dim`. 
<a name="l10105"><span class="ln">10105 </span></a>    If no dim is specified then all non-zeros in the tensor are counted. 
<a name="l10106"><span class="ln">10106 </span></a> 
<a name="l10107"><span class="ln">10107 </span></a>    Args: 
<a name="l10108"><span class="ln">10108 </span></a>        input (Tensor): the input tensor. 
<a name="l10109"><span class="ln">10109 </span></a>        dim (int or tuple of ints, optional): Dim or tuple of dims along which to count non-zeros. 
<a name="l10110"><span class="ln">10110 </span></a> 
<a name="l10111"><span class="ln">10111 </span></a>    Example:: 
<a name="l10112"><span class="ln">10112 </span></a> 
<a name="l10113"><span class="ln">10113 </span></a>        &gt;&gt;&gt; x = torch.zeros(3,3) 
<a name="l10114"><span class="ln">10114 </span></a>        &gt;&gt;&gt; x[torch.randn(3,3) &gt; 0.5] = 1 
<a name="l10115"><span class="ln">10115 </span></a>        &gt;&gt;&gt; x 
<a name="l10116"><span class="ln">10116 </span></a>        tensor([[0., 1., 1.], 
<a name="l10117"><span class="ln">10117 </span></a>                [0., 0., 0.], 
<a name="l10118"><span class="ln">10118 </span></a>                [0., 0., 1.]]) 
<a name="l10119"><span class="ln">10119 </span></a>        &gt;&gt;&gt; torch.count_nonzero(x) 
<a name="l10120"><span class="ln">10120 </span></a>        tensor(3) 
<a name="l10121"><span class="ln">10121 </span></a>        &gt;&gt;&gt; torch.count_nonzero(x, dim=0) 
<a name="l10122"><span class="ln">10122 </span></a>        tensor([0, 1, 2]) 
<a name="l10123"><span class="ln">10123 </span></a>    &quot;&quot;&quot;</span>
<a name="l10124"><span class="ln">10124 </span></a>
<a name="l10125"><span class="ln">10125 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10126"><span class="ln">10126 </span></a><span class="s2">def </span><span class="s1">count_nonzero</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10127"><span class="ln">10127 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10128"><span class="ln">10128 </span></a>    count_nonzero(input, dim=None) -&gt; Tensor 
<a name="l10129"><span class="ln">10129 </span></a> 
<a name="l10130"><span class="ln">10130 </span></a>    Counts the number of non-zero values in the tensor :attr:`input` along the given :attr:`dim`. 
<a name="l10131"><span class="ln">10131 </span></a>    If no dim is specified then all non-zeros in the tensor are counted. 
<a name="l10132"><span class="ln">10132 </span></a> 
<a name="l10133"><span class="ln">10133 </span></a>    Args: 
<a name="l10134"><span class="ln">10134 </span></a>        input (Tensor): the input tensor. 
<a name="l10135"><span class="ln">10135 </span></a>        dim (int or tuple of ints, optional): Dim or tuple of dims along which to count non-zeros. 
<a name="l10136"><span class="ln">10136 </span></a> 
<a name="l10137"><span class="ln">10137 </span></a>    Example:: 
<a name="l10138"><span class="ln">10138 </span></a> 
<a name="l10139"><span class="ln">10139 </span></a>        &gt;&gt;&gt; x = torch.zeros(3,3) 
<a name="l10140"><span class="ln">10140 </span></a>        &gt;&gt;&gt; x[torch.randn(3,3) &gt; 0.5] = 1 
<a name="l10141"><span class="ln">10141 </span></a>        &gt;&gt;&gt; x 
<a name="l10142"><span class="ln">10142 </span></a>        tensor([[0., 1., 1.], 
<a name="l10143"><span class="ln">10143 </span></a>                [0., 0., 0.], 
<a name="l10144"><span class="ln">10144 </span></a>                [0., 0., 1.]]) 
<a name="l10145"><span class="ln">10145 </span></a>        &gt;&gt;&gt; torch.count_nonzero(x) 
<a name="l10146"><span class="ln">10146 </span></a>        tensor(3) 
<a name="l10147"><span class="ln">10147 </span></a>        &gt;&gt;&gt; torch.count_nonzero(x, dim=0) 
<a name="l10148"><span class="ln">10148 </span></a>        tensor([0, 1, 2]) 
<a name="l10149"><span class="ln">10149 </span></a>    &quot;&quot;&quot;</span>
<a name="l10150"><span class="ln">10150 </span></a>
<a name="l10151"><span class="ln">10151 </span></a><span class="s2">def </span><span class="s1">cov</span><span class="s3">(</span>
<a name="l10152"><span class="ln">10152 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10153"><span class="ln">10153 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10154"><span class="ln">10154 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l10155"><span class="ln">10155 </span></a>    <span class="s1">fweights</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10156"><span class="ln">10156 </span></a>    <span class="s1">aweights</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10157"><span class="ln">10157 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10158"><span class="ln">10158 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10159"><span class="ln">10159 </span></a>    cov(input, *, correction=1, fweights=None, aweights=None) -&gt; Tensor 
<a name="l10160"><span class="ln">10160 </span></a> 
<a name="l10161"><span class="ln">10161 </span></a>    Estimates the covariance matrix of the variables given by the :attr:`input` matrix, where rows are 
<a name="l10162"><span class="ln">10162 </span></a>    the variables and columns are the observations. 
<a name="l10163"><span class="ln">10163 </span></a> 
<a name="l10164"><span class="ln">10164 </span></a>    A covariance matrix is a square matrix giving the covariance of each pair of variables. The diagonal contains 
<a name="l10165"><span class="ln">10165 </span></a>    the variance of each variable (covariance of a variable with itself). By definition, if :attr:`input` represents 
<a name="l10166"><span class="ln">10166 </span></a>    a single variable (Scalar or 1D) then its variance is returned. 
<a name="l10167"><span class="ln">10167 </span></a> 
<a name="l10168"><span class="ln">10168 </span></a>    The sample covariance of the variables :math:`x` and :math:`y` is given by: 
<a name="l10169"><span class="ln">10169 </span></a> 
<a name="l10170"><span class="ln">10170 </span></a>    .. math:: 
<a name="l10171"><span class="ln">10171 </span></a>        \text{cov}(x,y) = \frac{\sum^{N}_{i = 1}(x_{i} - \bar{x})(y_{i} - \bar{y})}{\max(0,~N~-~\delta N)} 
<a name="l10172"><span class="ln">10172 </span></a> 
<a name="l10173"><span class="ln">10173 </span></a>    where :math:`\bar{x}` and :math:`\bar{y}` are the simple means of the :math:`x` and :math:`y` respectively, and 
<a name="l10174"><span class="ln">10174 </span></a>    :math:`\delta N` is the :attr:`correction`. 
<a name="l10175"><span class="ln">10175 </span></a> 
<a name="l10176"><span class="ln">10176 </span></a>    If :attr:`fweights` and/or :attr:`aweights` are provided, the weighted covariance 
<a name="l10177"><span class="ln">10177 </span></a>    is calculated, which is given by: 
<a name="l10178"><span class="ln">10178 </span></a> 
<a name="l10179"><span class="ln">10179 </span></a>    .. math:: 
<a name="l10180"><span class="ln">10180 </span></a>        \text{cov}_w(x,y) = \frac{\sum^{N}_{i = 1}w_i(x_{i} - \mu_x^*)(y_{i} - \mu_y^*)} 
<a name="l10181"><span class="ln">10181 </span></a>        {\max(0,~\sum^{N}_{i = 1}w_i~-~\frac{\sum^{N}_{i = 1}w_ia_i}{\sum^{N}_{i = 1}w_i}~\delta N)} 
<a name="l10182"><span class="ln">10182 </span></a> 
<a name="l10183"><span class="ln">10183 </span></a>    where :math:`w` denotes :attr:`fweights` or :attr:`aweights` (``f`` and ``a`` for brevity) based on whichever is 
<a name="l10184"><span class="ln">10184 </span></a>    provided, or :math:`w = f \times a` if both are provided, and 
<a name="l10185"><span class="ln">10185 </span></a>    :math:`\mu_x^* = \frac{\sum^{N}_{i = 1}w_ix_{i} }{\sum^{N}_{i = 1}w_i}` is the weighted mean of the variable. If not 
<a name="l10186"><span class="ln">10186 </span></a>    provided, ``f`` and/or ``a`` can be seen as a :math:`\mathbb{1}` vector of appropriate size. 
<a name="l10187"><span class="ln">10187 </span></a> 
<a name="l10188"><span class="ln">10188 </span></a>    Args: 
<a name="l10189"><span class="ln">10189 </span></a>        input (Tensor): A 2D matrix containing multiple variables and observations, or a 
<a name="l10190"><span class="ln">10190 </span></a>            Scalar or 1D vector representing a single variable. 
<a name="l10191"><span class="ln">10191 </span></a> 
<a name="l10192"><span class="ln">10192 </span></a>    Keyword Args: 
<a name="l10193"><span class="ln">10193 </span></a>        correction (int, optional): difference between the sample size and sample degrees of freedom. 
<a name="l10194"><span class="ln">10194 </span></a>            Defaults to Bessel's correction, ``correction = 1`` which returns the unbiased estimate, 
<a name="l10195"><span class="ln">10195 </span></a>            even if both :attr:`fweights` and :attr:`aweights` are specified. ``correction = 0`` 
<a name="l10196"><span class="ln">10196 </span></a>            will return the simple average. Defaults to ``1``. 
<a name="l10197"><span class="ln">10197 </span></a>        fweights (tensor, optional): A Scalar or 1D tensor of observation vector frequencies representing the number of 
<a name="l10198"><span class="ln">10198 </span></a>            times each observation should be repeated. Its numel must equal the number of columns of :attr:`input`. 
<a name="l10199"><span class="ln">10199 </span></a>            Must have integral dtype. Ignored if ``None``. Defaults to ``None``. 
<a name="l10200"><span class="ln">10200 </span></a>        aweights (tensor, optional): A Scalar or 1D array of observation vector weights. 
<a name="l10201"><span class="ln">10201 </span></a>            These relative weights are typically large for observations considered &quot;important&quot; and smaller for 
<a name="l10202"><span class="ln">10202 </span></a>            observations considered less &quot;important&quot;. Its numel must equal the number of columns of :attr:`input`. 
<a name="l10203"><span class="ln">10203 </span></a>            Must have floating point dtype. Ignored if ``None``. Defaults to ``None``. 
<a name="l10204"><span class="ln">10204 </span></a> 
<a name="l10205"><span class="ln">10205 </span></a>    Returns: 
<a name="l10206"><span class="ln">10206 </span></a>        (Tensor) The covariance matrix of the variables. 
<a name="l10207"><span class="ln">10207 </span></a> 
<a name="l10208"><span class="ln">10208 </span></a>    .. seealso:: 
<a name="l10209"><span class="ln">10209 </span></a> 
<a name="l10210"><span class="ln">10210 </span></a>            :func:`torch.corrcoef` normalized covariance matrix. 
<a name="l10211"><span class="ln">10211 </span></a> 
<a name="l10212"><span class="ln">10212 </span></a>    Example:: 
<a name="l10213"><span class="ln">10213 </span></a> 
<a name="l10214"><span class="ln">10214 </span></a>        &gt;&gt;&gt; x = torch.tensor([[0, 2], [1, 1], [2, 0]]).T 
<a name="l10215"><span class="ln">10215 </span></a>        &gt;&gt;&gt; x 
<a name="l10216"><span class="ln">10216 </span></a>        tensor([[0, 1, 2], 
<a name="l10217"><span class="ln">10217 </span></a>                [2, 1, 0]]) 
<a name="l10218"><span class="ln">10218 </span></a>        &gt;&gt;&gt; torch.cov(x) 
<a name="l10219"><span class="ln">10219 </span></a>        tensor([[ 1., -1.], 
<a name="l10220"><span class="ln">10220 </span></a>                [-1.,  1.]]) 
<a name="l10221"><span class="ln">10221 </span></a>        &gt;&gt;&gt; torch.cov(x, correction=0) 
<a name="l10222"><span class="ln">10222 </span></a>        tensor([[ 0.6667, -0.6667], 
<a name="l10223"><span class="ln">10223 </span></a>                [-0.6667,  0.6667]]) 
<a name="l10224"><span class="ln">10224 </span></a>        &gt;&gt;&gt; fw = torch.randint(1, 10, (3,)) 
<a name="l10225"><span class="ln">10225 </span></a>        &gt;&gt;&gt; fw 
<a name="l10226"><span class="ln">10226 </span></a>        tensor([1, 6, 9]) 
<a name="l10227"><span class="ln">10227 </span></a>        &gt;&gt;&gt; aw = torch.rand(3) 
<a name="l10228"><span class="ln">10228 </span></a>        &gt;&gt;&gt; aw 
<a name="l10229"><span class="ln">10229 </span></a>        tensor([0.4282, 0.0255, 0.4144]) 
<a name="l10230"><span class="ln">10230 </span></a>        &gt;&gt;&gt; torch.cov(x, fweights=fw, aweights=aw) 
<a name="l10231"><span class="ln">10231 </span></a>        tensor([[ 0.4169, -0.4169], 
<a name="l10232"><span class="ln">10232 </span></a>                [-0.4169,  0.4169]]) 
<a name="l10233"><span class="ln">10233 </span></a>    &quot;&quot;&quot;</span>
<a name="l10234"><span class="ln">10234 </span></a>
<a name="l10235"><span class="ln">10235 </span></a><span class="s2">def </span><span class="s1">cross</span><span class="s3">(</span>
<a name="l10236"><span class="ln">10236 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10237"><span class="ln">10237 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10238"><span class="ln">10238 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10239"><span class="ln">10239 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10240"><span class="ln">10240 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10241"><span class="ln">10241 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10242"><span class="ln">10242 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10243"><span class="ln">10243 </span></a>    cross(input, other, dim=None, *, out=None) -&gt; Tensor 
<a name="l10244"><span class="ln">10244 </span></a> 
<a name="l10245"><span class="ln">10245 </span></a> 
<a name="l10246"><span class="ln">10246 </span></a>    Returns the cross product of vectors in dimension :attr:`dim` of :attr:`input` 
<a name="l10247"><span class="ln">10247 </span></a>    and :attr:`other`. 
<a name="l10248"><span class="ln">10248 </span></a> 
<a name="l10249"><span class="ln">10249 </span></a>    Supports input of float, double, cfloat and cdouble dtypes. Also supports batches 
<a name="l10250"><span class="ln">10250 </span></a>    of vectors, for which it computes the product along the dimension :attr:`dim`. 
<a name="l10251"><span class="ln">10251 </span></a>    In this case, the output has the same batch dimensions as the inputs. 
<a name="l10252"><span class="ln">10252 </span></a> 
<a name="l10253"><span class="ln">10253 </span></a>    .. warning:: 
<a name="l10254"><span class="ln">10254 </span></a>        If :attr:`dim` is not given, it defaults to the first dimension found 
<a name="l10255"><span class="ln">10255 </span></a>        with the size 3. Note that this might be unexpected. 
<a name="l10256"><span class="ln">10256 </span></a> 
<a name="l10257"><span class="ln">10257 </span></a>        This behavior is deprecated and will be changed to match that of :func:`torch.linalg.cross` 
<a name="l10258"><span class="ln">10258 </span></a>        in a future release. 
<a name="l10259"><span class="ln">10259 </span></a> 
<a name="l10260"><span class="ln">10260 </span></a>    .. seealso:: 
<a name="l10261"><span class="ln">10261 </span></a>            :func:`torch.linalg.cross` which has dim=-1 as default. 
<a name="l10262"><span class="ln">10262 </span></a> 
<a name="l10263"><span class="ln">10263 </span></a> 
<a name="l10264"><span class="ln">10264 </span></a>    Args: 
<a name="l10265"><span class="ln">10265 </span></a>        input (Tensor): the input tensor. 
<a name="l10266"><span class="ln">10266 </span></a>        other (Tensor): the second input tensor 
<a name="l10267"><span class="ln">10267 </span></a>        dim  (int, optional): the dimension to take the cross-product in. 
<a name="l10268"><span class="ln">10268 </span></a> 
<a name="l10269"><span class="ln">10269 </span></a>    Keyword args: 
<a name="l10270"><span class="ln">10270 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10271"><span class="ln">10271 </span></a> 
<a name="l10272"><span class="ln">10272 </span></a>    Example:: 
<a name="l10273"><span class="ln">10273 </span></a> 
<a name="l10274"><span class="ln">10274 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 3) 
<a name="l10275"><span class="ln">10275 </span></a>        &gt;&gt;&gt; a 
<a name="l10276"><span class="ln">10276 </span></a>        tensor([[-0.3956,  1.1455,  1.6895], 
<a name="l10277"><span class="ln">10277 </span></a>                [-0.5849,  1.3672,  0.3599], 
<a name="l10278"><span class="ln">10278 </span></a>                [-1.1626,  0.7180, -0.0521], 
<a name="l10279"><span class="ln">10279 </span></a>                [-0.1339,  0.9902, -2.0225]]) 
<a name="l10280"><span class="ln">10280 </span></a>        &gt;&gt;&gt; b = torch.randn(4, 3) 
<a name="l10281"><span class="ln">10281 </span></a>        &gt;&gt;&gt; b 
<a name="l10282"><span class="ln">10282 </span></a>        tensor([[-0.0257, -1.4725, -1.2251], 
<a name="l10283"><span class="ln">10283 </span></a>                [-1.1479, -0.7005, -1.9757], 
<a name="l10284"><span class="ln">10284 </span></a>                [-1.3904,  0.3726, -1.1836], 
<a name="l10285"><span class="ln">10285 </span></a>                [-0.9688, -0.7153,  0.2159]]) 
<a name="l10286"><span class="ln">10286 </span></a>        &gt;&gt;&gt; torch.cross(a, b, dim=1) 
<a name="l10287"><span class="ln">10287 </span></a>        tensor([[ 1.0844, -0.5281,  0.6120], 
<a name="l10288"><span class="ln">10288 </span></a>                [-2.4490, -1.5687,  1.9792], 
<a name="l10289"><span class="ln">10289 </span></a>                [-0.8304, -1.3037,  0.5650], 
<a name="l10290"><span class="ln">10290 </span></a>                [-1.2329,  1.9883,  1.0551]]) 
<a name="l10291"><span class="ln">10291 </span></a>        &gt;&gt;&gt; torch.cross(a, b) 
<a name="l10292"><span class="ln">10292 </span></a>        tensor([[ 1.0844, -0.5281,  0.6120], 
<a name="l10293"><span class="ln">10293 </span></a>                [-2.4490, -1.5687,  1.9792], 
<a name="l10294"><span class="ln">10294 </span></a>                [-0.8304, -1.3037,  0.5650], 
<a name="l10295"><span class="ln">10295 </span></a>                [-1.2329,  1.9883,  1.0551]]) 
<a name="l10296"><span class="ln">10296 </span></a>    &quot;&quot;&quot;</span>
<a name="l10297"><span class="ln">10297 </span></a>
<a name="l10298"><span class="ln">10298 </span></a><span class="s2">def </span><span class="s1">crow_indices_copy</span><span class="s3">(</span>
<a name="l10299"><span class="ln">10299 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10300"><span class="ln">10300 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10301"><span class="ln">10301 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10302"><span class="ln">10302 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10303"><span class="ln">10303 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10304"><span class="ln">10304 </span></a>    Performs the same operation as :func:`torch.crow_indices`, but all output tensors 
<a name="l10305"><span class="ln">10305 </span></a>    are freshly created instead of aliasing the input. 
<a name="l10306"><span class="ln">10306 </span></a>    &quot;&quot;&quot;</span>
<a name="l10307"><span class="ln">10307 </span></a>
<a name="l10308"><span class="ln">10308 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10309"><span class="ln">10309 </span></a><span class="s2">def </span><span class="s1">ctc_loss</span><span class="s3">(</span>
<a name="l10310"><span class="ln">10310 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10311"><span class="ln">10311 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10312"><span class="ln">10312 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l10313"><span class="ln">10313 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l10314"><span class="ln">10314 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l10315"><span class="ln">10315 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l10316"><span class="ln">10316 </span></a>    <span class="s1">zero_infinity</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l10317"><span class="ln">10317 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10318"><span class="ln">10318 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10319"><span class="ln">10319 </span></a><span class="s2">def </span><span class="s1">ctc_loss</span><span class="s3">(</span>
<a name="l10320"><span class="ln">10320 </span></a>    <span class="s1">log_probs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10321"><span class="ln">10321 </span></a>    <span class="s1">targets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10322"><span class="ln">10322 </span></a>    <span class="s1">input_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10323"><span class="ln">10323 </span></a>    <span class="s1">target_lengths</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10324"><span class="ln">10324 </span></a>    <span class="s1">blank</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l10325"><span class="ln">10325 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l10326"><span class="ln">10326 </span></a>    <span class="s1">zero_infinity</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l10327"><span class="ln">10327 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10328"><span class="ln">10328 </span></a><span class="s2">def </span><span class="s1">cudnn_affine_grid_generator</span><span class="s3">(</span>
<a name="l10329"><span class="ln">10329 </span></a>    <span class="s1">theta</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10330"><span class="ln">10330 </span></a>    <span class="s1">N</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10331"><span class="ln">10331 </span></a>    <span class="s1">C</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10332"><span class="ln">10332 </span></a>    <span class="s1">H</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10333"><span class="ln">10333 </span></a>    <span class="s1">W</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10334"><span class="ln">10334 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10335"><span class="ln">10335 </span></a><span class="s2">def </span><span class="s1">cudnn_batch_norm</span><span class="s3">(</span>
<a name="l10336"><span class="ln">10336 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10337"><span class="ln">10337 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10338"><span class="ln">10338 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10339"><span class="ln">10339 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10340"><span class="ln">10340 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10341"><span class="ln">10341 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l10342"><span class="ln">10342 </span></a>    <span class="s1">exponential_average_factor</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l10343"><span class="ln">10343 </span></a>    <span class="s1">epsilon</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l10344"><span class="ln">10344 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10345"><span class="ln">10345 </span></a><span class="s2">def </span><span class="s1">cudnn_convolution</span><span class="s3">(</span>
<a name="l10346"><span class="ln">10346 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10347"><span class="ln">10347 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10348"><span class="ln">10348 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10349"><span class="ln">10349 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10350"><span class="ln">10350 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10351"><span class="ln">10351 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l10352"><span class="ln">10352 </span></a>    <span class="s1">benchmark</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l10353"><span class="ln">10353 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l10354"><span class="ln">10354 </span></a>    <span class="s1">allow_tf32</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l10355"><span class="ln">10355 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10356"><span class="ln">10356 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10357"><span class="ln">10357 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10358"><span class="ln">10358 </span></a><span class="s2">def </span><span class="s1">cudnn_convolution_add_relu</span><span class="s3">(</span>
<a name="l10359"><span class="ln">10359 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10360"><span class="ln">10360 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10361"><span class="ln">10361 </span></a>    <span class="s1">z</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10362"><span class="ln">10362 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10363"><span class="ln">10363 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10364"><span class="ln">10364 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10365"><span class="ln">10365 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10366"><span class="ln">10366 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10367"><span class="ln">10367 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l10368"><span class="ln">10368 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10369"><span class="ln">10369 </span></a><span class="s2">def </span><span class="s1">cudnn_convolution_relu</span><span class="s3">(</span>
<a name="l10370"><span class="ln">10370 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10371"><span class="ln">10371 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10372"><span class="ln">10372 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10373"><span class="ln">10373 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10374"><span class="ln">10374 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10375"><span class="ln">10375 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10376"><span class="ln">10376 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l10377"><span class="ln">10377 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10378"><span class="ln">10378 </span></a><span class="s2">def </span><span class="s1">cudnn_convolution_transpose</span><span class="s3">(</span>
<a name="l10379"><span class="ln">10379 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10380"><span class="ln">10380 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10381"><span class="ln">10381 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10382"><span class="ln">10382 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10383"><span class="ln">10383 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10384"><span class="ln">10384 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l10385"><span class="ln">10385 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l10386"><span class="ln">10386 </span></a>    <span class="s1">benchmark</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l10387"><span class="ln">10387 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l10388"><span class="ln">10388 </span></a>    <span class="s1">allow_tf32</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l10389"><span class="ln">10389 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10390"><span class="ln">10390 </span></a><span class="s2">def </span><span class="s1">cudnn_grid_sampler</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">grid</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10391"><span class="ln">10391 </span></a><span class="s2">def </span><span class="s1">cudnn_is_acceptable</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10392"><span class="ln">10392 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10393"><span class="ln">10393 </span></a><span class="s2">def </span><span class="s1">cummax</span><span class="s3">(</span>
<a name="l10394"><span class="ln">10394 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10395"><span class="ln">10395 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10396"><span class="ln">10396 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10397"><span class="ln">10397 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10398"><span class="ln">10398 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">cummax</span><span class="s2">:</span>
<a name="l10399"><span class="ln">10399 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10400"><span class="ln">10400 </span></a>    cummax(input, dim, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l10401"><span class="ln">10401 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative maximum of 
<a name="l10402"><span class="ln">10402 </span></a>    elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index 
<a name="l10403"><span class="ln">10403 </span></a>    location of each maximum value found in the dimension :attr:`dim`. 
<a name="l10404"><span class="ln">10404 </span></a> 
<a name="l10405"><span class="ln">10405 </span></a>    .. math:: 
<a name="l10406"><span class="ln">10406 </span></a>        y_i = max(x_1, x_2, x_3, \dots, x_i) 
<a name="l10407"><span class="ln">10407 </span></a> 
<a name="l10408"><span class="ln">10408 </span></a>    Args: 
<a name="l10409"><span class="ln">10409 </span></a>        input (Tensor): the input tensor. 
<a name="l10410"><span class="ln">10410 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10411"><span class="ln">10411 </span></a> 
<a name="l10412"><span class="ln">10412 </span></a>    Keyword args: 
<a name="l10413"><span class="ln">10413 </span></a>        out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l10414"><span class="ln">10414 </span></a> 
<a name="l10415"><span class="ln">10415 </span></a>    Example:: 
<a name="l10416"><span class="ln">10416 </span></a> 
<a name="l10417"><span class="ln">10417 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l10418"><span class="ln">10418 </span></a>        &gt;&gt;&gt; a 
<a name="l10419"><span class="ln">10419 </span></a>        tensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284, 
<a name="l10420"><span class="ln">10420 </span></a>             1.9946, -0.8209]) 
<a name="l10421"><span class="ln">10421 </span></a>        &gt;&gt;&gt; torch.cummax(a, dim=0) 
<a name="l10422"><span class="ln">10422 </span></a>        torch.return_types.cummax( 
<a name="l10423"><span class="ln">10423 </span></a>            values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696, 
<a name="l10424"><span class="ln">10424 </span></a>             1.9946,  1.9946]), 
<a name="l10425"><span class="ln">10425 </span></a>            indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8])) 
<a name="l10426"><span class="ln">10426 </span></a>    &quot;&quot;&quot;</span>
<a name="l10427"><span class="ln">10427 </span></a>
<a name="l10428"><span class="ln">10428 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10429"><span class="ln">10429 </span></a><span class="s2">def </span><span class="s1">cummax</span><span class="s3">(</span>
<a name="l10430"><span class="ln">10430 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10431"><span class="ln">10431 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10432"><span class="ln">10432 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10433"><span class="ln">10433 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10434"><span class="ln">10434 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">cummax</span><span class="s2">:</span>
<a name="l10435"><span class="ln">10435 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10436"><span class="ln">10436 </span></a>    cummax(input, dim, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l10437"><span class="ln">10437 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative maximum of 
<a name="l10438"><span class="ln">10438 </span></a>    elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index 
<a name="l10439"><span class="ln">10439 </span></a>    location of each maximum value found in the dimension :attr:`dim`. 
<a name="l10440"><span class="ln">10440 </span></a> 
<a name="l10441"><span class="ln">10441 </span></a>    .. math:: 
<a name="l10442"><span class="ln">10442 </span></a>        y_i = max(x_1, x_2, x_3, \dots, x_i) 
<a name="l10443"><span class="ln">10443 </span></a> 
<a name="l10444"><span class="ln">10444 </span></a>    Args: 
<a name="l10445"><span class="ln">10445 </span></a>        input (Tensor): the input tensor. 
<a name="l10446"><span class="ln">10446 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10447"><span class="ln">10447 </span></a> 
<a name="l10448"><span class="ln">10448 </span></a>    Keyword args: 
<a name="l10449"><span class="ln">10449 </span></a>        out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l10450"><span class="ln">10450 </span></a> 
<a name="l10451"><span class="ln">10451 </span></a>    Example:: 
<a name="l10452"><span class="ln">10452 </span></a> 
<a name="l10453"><span class="ln">10453 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l10454"><span class="ln">10454 </span></a>        &gt;&gt;&gt; a 
<a name="l10455"><span class="ln">10455 </span></a>        tensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284, 
<a name="l10456"><span class="ln">10456 </span></a>             1.9946, -0.8209]) 
<a name="l10457"><span class="ln">10457 </span></a>        &gt;&gt;&gt; torch.cummax(a, dim=0) 
<a name="l10458"><span class="ln">10458 </span></a>        torch.return_types.cummax( 
<a name="l10459"><span class="ln">10459 </span></a>            values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696, 
<a name="l10460"><span class="ln">10460 </span></a>             1.9946,  1.9946]), 
<a name="l10461"><span class="ln">10461 </span></a>            indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8])) 
<a name="l10462"><span class="ln">10462 </span></a>    &quot;&quot;&quot;</span>
<a name="l10463"><span class="ln">10463 </span></a>
<a name="l10464"><span class="ln">10464 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10465"><span class="ln">10465 </span></a><span class="s2">def </span><span class="s1">cummin</span><span class="s3">(</span>
<a name="l10466"><span class="ln">10466 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10467"><span class="ln">10467 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10468"><span class="ln">10468 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10469"><span class="ln">10469 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10470"><span class="ln">10470 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">cummin</span><span class="s2">:</span>
<a name="l10471"><span class="ln">10471 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10472"><span class="ln">10472 </span></a>    cummin(input, dim, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l10473"><span class="ln">10473 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative minimum of 
<a name="l10474"><span class="ln">10474 </span></a>    elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index 
<a name="l10475"><span class="ln">10475 </span></a>    location of each maximum value found in the dimension :attr:`dim`. 
<a name="l10476"><span class="ln">10476 </span></a> 
<a name="l10477"><span class="ln">10477 </span></a>    .. math:: 
<a name="l10478"><span class="ln">10478 </span></a>        y_i = min(x_1, x_2, x_3, \dots, x_i) 
<a name="l10479"><span class="ln">10479 </span></a> 
<a name="l10480"><span class="ln">10480 </span></a>    Args: 
<a name="l10481"><span class="ln">10481 </span></a>        input (Tensor): the input tensor. 
<a name="l10482"><span class="ln">10482 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10483"><span class="ln">10483 </span></a> 
<a name="l10484"><span class="ln">10484 </span></a>    Keyword args: 
<a name="l10485"><span class="ln">10485 </span></a>        out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l10486"><span class="ln">10486 </span></a> 
<a name="l10487"><span class="ln">10487 </span></a>    Example:: 
<a name="l10488"><span class="ln">10488 </span></a> 
<a name="l10489"><span class="ln">10489 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l10490"><span class="ln">10490 </span></a>        &gt;&gt;&gt; a 
<a name="l10491"><span class="ln">10491 </span></a>        tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762, 
<a name="l10492"><span class="ln">10492 </span></a>             0.9165,  1.6684]) 
<a name="l10493"><span class="ln">10493 </span></a>        &gt;&gt;&gt; torch.cummin(a, dim=0) 
<a name="l10494"><span class="ln">10494 </span></a>        torch.return_types.cummin( 
<a name="l10495"><span class="ln">10495 </span></a>            values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298, 
<a name="l10496"><span class="ln">10496 </span></a>            -1.3298, -1.3298]), 
<a name="l10497"><span class="ln">10497 </span></a>            indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4])) 
<a name="l10498"><span class="ln">10498 </span></a>    &quot;&quot;&quot;</span>
<a name="l10499"><span class="ln">10499 </span></a>
<a name="l10500"><span class="ln">10500 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10501"><span class="ln">10501 </span></a><span class="s2">def </span><span class="s1">cummin</span><span class="s3">(</span>
<a name="l10502"><span class="ln">10502 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10503"><span class="ln">10503 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10504"><span class="ln">10504 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10505"><span class="ln">10505 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10506"><span class="ln">10506 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">cummin</span><span class="s2">:</span>
<a name="l10507"><span class="ln">10507 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10508"><span class="ln">10508 </span></a>    cummin(input, dim, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l10509"><span class="ln">10509 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative minimum of 
<a name="l10510"><span class="ln">10510 </span></a>    elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index 
<a name="l10511"><span class="ln">10511 </span></a>    location of each maximum value found in the dimension :attr:`dim`. 
<a name="l10512"><span class="ln">10512 </span></a> 
<a name="l10513"><span class="ln">10513 </span></a>    .. math:: 
<a name="l10514"><span class="ln">10514 </span></a>        y_i = min(x_1, x_2, x_3, \dots, x_i) 
<a name="l10515"><span class="ln">10515 </span></a> 
<a name="l10516"><span class="ln">10516 </span></a>    Args: 
<a name="l10517"><span class="ln">10517 </span></a>        input (Tensor): the input tensor. 
<a name="l10518"><span class="ln">10518 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10519"><span class="ln">10519 </span></a> 
<a name="l10520"><span class="ln">10520 </span></a>    Keyword args: 
<a name="l10521"><span class="ln">10521 </span></a>        out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l10522"><span class="ln">10522 </span></a> 
<a name="l10523"><span class="ln">10523 </span></a>    Example:: 
<a name="l10524"><span class="ln">10524 </span></a> 
<a name="l10525"><span class="ln">10525 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l10526"><span class="ln">10526 </span></a>        &gt;&gt;&gt; a 
<a name="l10527"><span class="ln">10527 </span></a>        tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762, 
<a name="l10528"><span class="ln">10528 </span></a>             0.9165,  1.6684]) 
<a name="l10529"><span class="ln">10529 </span></a>        &gt;&gt;&gt; torch.cummin(a, dim=0) 
<a name="l10530"><span class="ln">10530 </span></a>        torch.return_types.cummin( 
<a name="l10531"><span class="ln">10531 </span></a>            values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298, 
<a name="l10532"><span class="ln">10532 </span></a>            -1.3298, -1.3298]), 
<a name="l10533"><span class="ln">10533 </span></a>            indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4])) 
<a name="l10534"><span class="ln">10534 </span></a>    &quot;&quot;&quot;</span>
<a name="l10535"><span class="ln">10535 </span></a>
<a name="l10536"><span class="ln">10536 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10537"><span class="ln">10537 </span></a><span class="s2">def </span><span class="s1">cumprod</span><span class="s3">(</span>
<a name="l10538"><span class="ln">10538 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10539"><span class="ln">10539 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10540"><span class="ln">10540 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10541"><span class="ln">10541 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10542"><span class="ln">10542 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10543"><span class="ln">10543 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10544"><span class="ln">10544 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10545"><span class="ln">10545 </span></a>    cumprod(input, dim, *, dtype=None, out=None) -&gt; Tensor 
<a name="l10546"><span class="ln">10546 </span></a> 
<a name="l10547"><span class="ln">10547 </span></a>    Returns the cumulative product of elements of :attr:`input` in the dimension 
<a name="l10548"><span class="ln">10548 </span></a>    :attr:`dim`. 
<a name="l10549"><span class="ln">10549 </span></a> 
<a name="l10550"><span class="ln">10550 </span></a>    For example, if :attr:`input` is a vector of size N, the result will also be 
<a name="l10551"><span class="ln">10551 </span></a>    a vector of size N, with elements. 
<a name="l10552"><span class="ln">10552 </span></a> 
<a name="l10553"><span class="ln">10553 </span></a>    .. math:: 
<a name="l10554"><span class="ln">10554 </span></a>        y_i = x_1 \times x_2\times x_3\times \dots \times x_i 
<a name="l10555"><span class="ln">10555 </span></a> 
<a name="l10556"><span class="ln">10556 </span></a>    Args: 
<a name="l10557"><span class="ln">10557 </span></a>        input (Tensor): the input tensor. 
<a name="l10558"><span class="ln">10558 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10559"><span class="ln">10559 </span></a> 
<a name="l10560"><span class="ln">10560 </span></a>    Keyword args: 
<a name="l10561"><span class="ln">10561 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l10562"><span class="ln">10562 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l10563"><span class="ln">10563 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l10564"><span class="ln">10564 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10565"><span class="ln">10565 </span></a> 
<a name="l10566"><span class="ln">10566 </span></a>    Example:: 
<a name="l10567"><span class="ln">10567 </span></a> 
<a name="l10568"><span class="ln">10568 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l10569"><span class="ln">10569 </span></a>        &gt;&gt;&gt; a 
<a name="l10570"><span class="ln">10570 </span></a>        tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126, 
<a name="l10571"><span class="ln">10571 </span></a>                -0.2129, -0.4206,  0.1968]) 
<a name="l10572"><span class="ln">10572 </span></a>        &gt;&gt;&gt; torch.cumprod(a, dim=0) 
<a name="l10573"><span class="ln">10573 </span></a>        tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065, 
<a name="l10574"><span class="ln">10574 </span></a>                 0.0014, -0.0006, -0.0001]) 
<a name="l10575"><span class="ln">10575 </span></a> 
<a name="l10576"><span class="ln">10576 </span></a>        &gt;&gt;&gt; a[5] = 0.0 
<a name="l10577"><span class="ln">10577 </span></a>        &gt;&gt;&gt; torch.cumprod(a, dim=0) 
<a name="l10578"><span class="ln">10578 </span></a>        tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000, 
<a name="l10579"><span class="ln">10579 </span></a>                 0.0000, -0.0000, -0.0000]) 
<a name="l10580"><span class="ln">10580 </span></a>    &quot;&quot;&quot;</span>
<a name="l10581"><span class="ln">10581 </span></a>
<a name="l10582"><span class="ln">10582 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10583"><span class="ln">10583 </span></a><span class="s2">def </span><span class="s1">cumprod</span><span class="s3">(</span>
<a name="l10584"><span class="ln">10584 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10585"><span class="ln">10585 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10586"><span class="ln">10586 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10587"><span class="ln">10587 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10588"><span class="ln">10588 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10589"><span class="ln">10589 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10590"><span class="ln">10590 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10591"><span class="ln">10591 </span></a>    cumprod(input, dim, *, dtype=None, out=None) -&gt; Tensor 
<a name="l10592"><span class="ln">10592 </span></a> 
<a name="l10593"><span class="ln">10593 </span></a>    Returns the cumulative product of elements of :attr:`input` in the dimension 
<a name="l10594"><span class="ln">10594 </span></a>    :attr:`dim`. 
<a name="l10595"><span class="ln">10595 </span></a> 
<a name="l10596"><span class="ln">10596 </span></a>    For example, if :attr:`input` is a vector of size N, the result will also be 
<a name="l10597"><span class="ln">10597 </span></a>    a vector of size N, with elements. 
<a name="l10598"><span class="ln">10598 </span></a> 
<a name="l10599"><span class="ln">10599 </span></a>    .. math:: 
<a name="l10600"><span class="ln">10600 </span></a>        y_i = x_1 \times x_2\times x_3\times \dots \times x_i 
<a name="l10601"><span class="ln">10601 </span></a> 
<a name="l10602"><span class="ln">10602 </span></a>    Args: 
<a name="l10603"><span class="ln">10603 </span></a>        input (Tensor): the input tensor. 
<a name="l10604"><span class="ln">10604 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10605"><span class="ln">10605 </span></a> 
<a name="l10606"><span class="ln">10606 </span></a>    Keyword args: 
<a name="l10607"><span class="ln">10607 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l10608"><span class="ln">10608 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l10609"><span class="ln">10609 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l10610"><span class="ln">10610 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10611"><span class="ln">10611 </span></a> 
<a name="l10612"><span class="ln">10612 </span></a>    Example:: 
<a name="l10613"><span class="ln">10613 </span></a> 
<a name="l10614"><span class="ln">10614 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l10615"><span class="ln">10615 </span></a>        &gt;&gt;&gt; a 
<a name="l10616"><span class="ln">10616 </span></a>        tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126, 
<a name="l10617"><span class="ln">10617 </span></a>                -0.2129, -0.4206,  0.1968]) 
<a name="l10618"><span class="ln">10618 </span></a>        &gt;&gt;&gt; torch.cumprod(a, dim=0) 
<a name="l10619"><span class="ln">10619 </span></a>        tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065, 
<a name="l10620"><span class="ln">10620 </span></a>                 0.0014, -0.0006, -0.0001]) 
<a name="l10621"><span class="ln">10621 </span></a> 
<a name="l10622"><span class="ln">10622 </span></a>        &gt;&gt;&gt; a[5] = 0.0 
<a name="l10623"><span class="ln">10623 </span></a>        &gt;&gt;&gt; torch.cumprod(a, dim=0) 
<a name="l10624"><span class="ln">10624 </span></a>        tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000, 
<a name="l10625"><span class="ln">10625 </span></a>                 0.0000, -0.0000, -0.0000]) 
<a name="l10626"><span class="ln">10626 </span></a>    &quot;&quot;&quot;</span>
<a name="l10627"><span class="ln">10627 </span></a>
<a name="l10628"><span class="ln">10628 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10629"><span class="ln">10629 </span></a><span class="s2">def </span><span class="s1">cumsum</span><span class="s3">(</span>
<a name="l10630"><span class="ln">10630 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10631"><span class="ln">10631 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l10632"><span class="ln">10632 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10633"><span class="ln">10633 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10634"><span class="ln">10634 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10635"><span class="ln">10635 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10636"><span class="ln">10636 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10637"><span class="ln">10637 </span></a>    cumsum(input, dim, *, dtype=None, out=None) -&gt; Tensor 
<a name="l10638"><span class="ln">10638 </span></a> 
<a name="l10639"><span class="ln">10639 </span></a>    Returns the cumulative sum of elements of :attr:`input` in the dimension 
<a name="l10640"><span class="ln">10640 </span></a>    :attr:`dim`. 
<a name="l10641"><span class="ln">10641 </span></a> 
<a name="l10642"><span class="ln">10642 </span></a>    For example, if :attr:`input` is a vector of size N, the result will also be 
<a name="l10643"><span class="ln">10643 </span></a>    a vector of size N, with elements. 
<a name="l10644"><span class="ln">10644 </span></a> 
<a name="l10645"><span class="ln">10645 </span></a>    .. math:: 
<a name="l10646"><span class="ln">10646 </span></a>        y_i = x_1 + x_2 + x_3 + \dots + x_i 
<a name="l10647"><span class="ln">10647 </span></a> 
<a name="l10648"><span class="ln">10648 </span></a>    Args: 
<a name="l10649"><span class="ln">10649 </span></a>        input (Tensor): the input tensor. 
<a name="l10650"><span class="ln">10650 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10651"><span class="ln">10651 </span></a> 
<a name="l10652"><span class="ln">10652 </span></a>    Keyword args: 
<a name="l10653"><span class="ln">10653 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l10654"><span class="ln">10654 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l10655"><span class="ln">10655 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l10656"><span class="ln">10656 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10657"><span class="ln">10657 </span></a> 
<a name="l10658"><span class="ln">10658 </span></a>    Example:: 
<a name="l10659"><span class="ln">10659 </span></a> 
<a name="l10660"><span class="ln">10660 </span></a>        &gt;&gt;&gt; a = torch.randint(1, 20, (10,)) 
<a name="l10661"><span class="ln">10661 </span></a>        &gt;&gt;&gt; a 
<a name="l10662"><span class="ln">10662 </span></a>        tensor([13,  7,  3, 10, 13,  3, 15, 10,  9, 10]) 
<a name="l10663"><span class="ln">10663 </span></a>        &gt;&gt;&gt; torch.cumsum(a, dim=0) 
<a name="l10664"><span class="ln">10664 </span></a>        tensor([13, 20, 23, 33, 46, 49, 64, 74, 83, 93]) 
<a name="l10665"><span class="ln">10665 </span></a>    &quot;&quot;&quot;</span>
<a name="l10666"><span class="ln">10666 </span></a>
<a name="l10667"><span class="ln">10667 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10668"><span class="ln">10668 </span></a><span class="s2">def </span><span class="s1">cumsum</span><span class="s3">(</span>
<a name="l10669"><span class="ln">10669 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10670"><span class="ln">10670 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10671"><span class="ln">10671 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10672"><span class="ln">10672 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10673"><span class="ln">10673 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10674"><span class="ln">10674 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10675"><span class="ln">10675 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10676"><span class="ln">10676 </span></a>    cumsum(input, dim, *, dtype=None, out=None) -&gt; Tensor 
<a name="l10677"><span class="ln">10677 </span></a> 
<a name="l10678"><span class="ln">10678 </span></a>    Returns the cumulative sum of elements of :attr:`input` in the dimension 
<a name="l10679"><span class="ln">10679 </span></a>    :attr:`dim`. 
<a name="l10680"><span class="ln">10680 </span></a> 
<a name="l10681"><span class="ln">10681 </span></a>    For example, if :attr:`input` is a vector of size N, the result will also be 
<a name="l10682"><span class="ln">10682 </span></a>    a vector of size N, with elements. 
<a name="l10683"><span class="ln">10683 </span></a> 
<a name="l10684"><span class="ln">10684 </span></a>    .. math:: 
<a name="l10685"><span class="ln">10685 </span></a>        y_i = x_1 + x_2 + x_3 + \dots + x_i 
<a name="l10686"><span class="ln">10686 </span></a> 
<a name="l10687"><span class="ln">10687 </span></a>    Args: 
<a name="l10688"><span class="ln">10688 </span></a>        input (Tensor): the input tensor. 
<a name="l10689"><span class="ln">10689 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l10690"><span class="ln">10690 </span></a> 
<a name="l10691"><span class="ln">10691 </span></a>    Keyword args: 
<a name="l10692"><span class="ln">10692 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l10693"><span class="ln">10693 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l10694"><span class="ln">10694 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l10695"><span class="ln">10695 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10696"><span class="ln">10696 </span></a> 
<a name="l10697"><span class="ln">10697 </span></a>    Example:: 
<a name="l10698"><span class="ln">10698 </span></a> 
<a name="l10699"><span class="ln">10699 </span></a>        &gt;&gt;&gt; a = torch.randint(1, 20, (10,)) 
<a name="l10700"><span class="ln">10700 </span></a>        &gt;&gt;&gt; a 
<a name="l10701"><span class="ln">10701 </span></a>        tensor([13,  7,  3, 10, 13,  3, 15, 10,  9, 10]) 
<a name="l10702"><span class="ln">10702 </span></a>        &gt;&gt;&gt; torch.cumsum(a, dim=0) 
<a name="l10703"><span class="ln">10703 </span></a>        tensor([13, 20, 23, 33, 46, 49, 64, 74, 83, 93]) 
<a name="l10704"><span class="ln">10704 </span></a>    &quot;&quot;&quot;</span>
<a name="l10705"><span class="ln">10705 </span></a>
<a name="l10706"><span class="ln">10706 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10707"><span class="ln">10707 </span></a><span class="s2">def </span><span class="s1">cumulative_trapezoid</span><span class="s3">(</span><span class="s1">y</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">x</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10708"><span class="ln">10708 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10709"><span class="ln">10709 </span></a>    cumulative_trapezoid(y, x=None, *, dx=None, dim=-1) -&gt; Tensor 
<a name="l10710"><span class="ln">10710 </span></a> 
<a name="l10711"><span class="ln">10711 </span></a>    Cumulatively computes the `trapezoidal rule &lt;https://en.wikipedia.org/wiki/Trapezoidal_rule&gt;`_ 
<a name="l10712"><span class="ln">10712 </span></a>    along :attr:`dim`. By default the spacing between elements is assumed to be 1, but 
<a name="l10713"><span class="ln">10713 </span></a>    :attr:`dx` can be used to specify a different constant spacing, and :attr:`x` can be 
<a name="l10714"><span class="ln">10714 </span></a>    used to specify arbitrary spacing along :attr:`dim`. 
<a name="l10715"><span class="ln">10715 </span></a> 
<a name="l10716"><span class="ln">10716 </span></a>    For more details, please read :func:`torch.trapezoid`. The difference between :func:`torch.trapezoid` 
<a name="l10717"><span class="ln">10717 </span></a>    and this function is that, :func:`torch.trapezoid` returns a value for each integration, 
<a name="l10718"><span class="ln">10718 </span></a>    where as this function returns a cumulative value for every spacing within the integration. This 
<a name="l10719"><span class="ln">10719 </span></a>    is analogous to how `.sum` returns a value and `.cumsum` returns a cumulative sum. 
<a name="l10720"><span class="ln">10720 </span></a> 
<a name="l10721"><span class="ln">10721 </span></a>    Arguments: 
<a name="l10722"><span class="ln">10722 </span></a>        y (Tensor): Values to use when computing the trapezoidal rule. 
<a name="l10723"><span class="ln">10723 </span></a>        x (Tensor): If specified, defines spacing between values as specified above. 
<a name="l10724"><span class="ln">10724 </span></a> 
<a name="l10725"><span class="ln">10725 </span></a>    Keyword arguments: 
<a name="l10726"><span class="ln">10726 </span></a>        dx (float): constant spacing between values. If neither :attr:`x` or :attr:`dx` 
<a name="l10727"><span class="ln">10727 </span></a>            are specified then this defaults to 1. Effectively multiplies the result by its value. 
<a name="l10728"><span class="ln">10728 </span></a>        dim (int): The dimension along which to compute the trapezoidal rule. 
<a name="l10729"><span class="ln">10729 </span></a>            The last (inner-most) dimension by default. 
<a name="l10730"><span class="ln">10730 </span></a> 
<a name="l10731"><span class="ln">10731 </span></a>    Examples:: 
<a name="l10732"><span class="ln">10732 </span></a> 
<a name="l10733"><span class="ln">10733 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D, spacing is implicitly 1. 
<a name="l10734"><span class="ln">10734 </span></a>        &gt;&gt;&gt; y = torch.tensor([1, 5, 10]) 
<a name="l10735"><span class="ln">10735 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y) 
<a name="l10736"><span class="ln">10736 </span></a>        tensor([3., 10.5]) 
<a name="l10737"><span class="ln">10737 </span></a> 
<a name="l10738"><span class="ln">10738 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly up to each element to verify 
<a name="l10739"><span class="ln">10739 </span></a>        &gt;&gt;&gt; (1 + 5) / 2 
<a name="l10740"><span class="ln">10740 </span></a>        3.0 
<a name="l10741"><span class="ln">10741 </span></a>        &gt;&gt;&gt; (1 + 10 + 10) / 2 
<a name="l10742"><span class="ln">10742 </span></a>        10.5 
<a name="l10743"><span class="ln">10743 </span></a> 
<a name="l10744"><span class="ln">10744 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D with constant spacing of 2 
<a name="l10745"><span class="ln">10745 </span></a>        &gt;&gt;&gt; # NOTE: the result is the same as before, but multiplied by 2 
<a name="l10746"><span class="ln">10746 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, dx=2) 
<a name="l10747"><span class="ln">10747 </span></a>        tensor([6., 21.]) 
<a name="l10748"><span class="ln">10748 </span></a> 
<a name="l10749"><span class="ln">10749 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D with arbitrary spacing 
<a name="l10750"><span class="ln">10750 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l10751"><span class="ln">10751 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l10752"><span class="ln">10752 </span></a>        tensor([6., 28.5]) 
<a name="l10753"><span class="ln">10753 </span></a> 
<a name="l10754"><span class="ln">10754 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly up to each element to verify 
<a name="l10755"><span class="ln">10755 </span></a>        &gt;&gt;&gt; ((3 - 1) * (1 + 5)) / 2 
<a name="l10756"><span class="ln">10756 </span></a>        6.0 
<a name="l10757"><span class="ln">10757 </span></a>        &gt;&gt;&gt; ((3 - 1) * (1 + 5) + (6 - 3) * (5 + 10)) / 2 
<a name="l10758"><span class="ln">10758 </span></a>        28.5 
<a name="l10759"><span class="ln">10759 </span></a> 
<a name="l10760"><span class="ln">10760 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 matrix 
<a name="l10761"><span class="ln">10761 </span></a>        &gt;&gt;&gt; y = torch.arange(9).reshape(3, 3) 
<a name="l10762"><span class="ln">10762 </span></a>        tensor([[0, 1, 2], 
<a name="l10763"><span class="ln">10763 </span></a>                [3, 4, 5], 
<a name="l10764"><span class="ln">10764 </span></a>                [6, 7, 8]]) 
<a name="l10765"><span class="ln">10765 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y) 
<a name="l10766"><span class="ln">10766 </span></a>        tensor([[ 0.5,  2.], 
<a name="l10767"><span class="ln">10767 </span></a>                [ 3.5,  8.], 
<a name="l10768"><span class="ln">10768 </span></a>                [ 6.5, 14.]]) 
<a name="l10769"><span class="ln">10769 </span></a> 
<a name="l10770"><span class="ln">10770 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each column of the matrix 
<a name="l10771"><span class="ln">10771 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, dim=0) 
<a name="l10772"><span class="ln">10772 </span></a>        tensor([[ 1.5,  2.5,  3.5], 
<a name="l10773"><span class="ln">10773 </span></a>                [ 6.0,  8.0, 10.0]]) 
<a name="l10774"><span class="ln">10774 </span></a> 
<a name="l10775"><span class="ln">10775 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l10776"><span class="ln">10776 </span></a>        &gt;&gt;&gt; #   with the same arbitrary spacing 
<a name="l10777"><span class="ln">10777 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l10778"><span class="ln">10778 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l10779"><span class="ln">10779 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l10780"><span class="ln">10780 </span></a>        tensor([[2., 5.], 
<a name="l10781"><span class="ln">10781 </span></a>                [2., 5.], 
<a name="l10782"><span class="ln">10782 </span></a>                [2., 5.]]) 
<a name="l10783"><span class="ln">10783 </span></a> 
<a name="l10784"><span class="ln">10784 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l10785"><span class="ln">10785 </span></a>        &gt;&gt;&gt; #   with different arbitrary spacing per row 
<a name="l10786"><span class="ln">10786 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l10787"><span class="ln">10787 </span></a>        &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [1, 3, 5], [1, 4, 7]]) 
<a name="l10788"><span class="ln">10788 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l10789"><span class="ln">10789 </span></a>        tensor([[1., 2.], 
<a name="l10790"><span class="ln">10790 </span></a>                [2., 4.], 
<a name="l10791"><span class="ln">10791 </span></a>                [3., 6.]]) 
<a name="l10792"><span class="ln">10792 </span></a>    &quot;&quot;&quot;</span>
<a name="l10793"><span class="ln">10793 </span></a>
<a name="l10794"><span class="ln">10794 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10795"><span class="ln">10795 </span></a><span class="s2">def </span><span class="s1">cumulative_trapezoid</span><span class="s3">(</span>
<a name="l10796"><span class="ln">10796 </span></a>    <span class="s1">y</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10797"><span class="ln">10797 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10798"><span class="ln">10798 </span></a>    <span class="s1">dx</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l10799"><span class="ln">10799 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l10800"><span class="ln">10800 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10801"><span class="ln">10801 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10802"><span class="ln">10802 </span></a>    cumulative_trapezoid(y, x=None, *, dx=None, dim=-1) -&gt; Tensor 
<a name="l10803"><span class="ln">10803 </span></a> 
<a name="l10804"><span class="ln">10804 </span></a>    Cumulatively computes the `trapezoidal rule &lt;https://en.wikipedia.org/wiki/Trapezoidal_rule&gt;`_ 
<a name="l10805"><span class="ln">10805 </span></a>    along :attr:`dim`. By default the spacing between elements is assumed to be 1, but 
<a name="l10806"><span class="ln">10806 </span></a>    :attr:`dx` can be used to specify a different constant spacing, and :attr:`x` can be 
<a name="l10807"><span class="ln">10807 </span></a>    used to specify arbitrary spacing along :attr:`dim`. 
<a name="l10808"><span class="ln">10808 </span></a> 
<a name="l10809"><span class="ln">10809 </span></a>    For more details, please read :func:`torch.trapezoid`. The difference between :func:`torch.trapezoid` 
<a name="l10810"><span class="ln">10810 </span></a>    and this function is that, :func:`torch.trapezoid` returns a value for each integration, 
<a name="l10811"><span class="ln">10811 </span></a>    where as this function returns a cumulative value for every spacing within the integration. This 
<a name="l10812"><span class="ln">10812 </span></a>    is analogous to how `.sum` returns a value and `.cumsum` returns a cumulative sum. 
<a name="l10813"><span class="ln">10813 </span></a> 
<a name="l10814"><span class="ln">10814 </span></a>    Arguments: 
<a name="l10815"><span class="ln">10815 </span></a>        y (Tensor): Values to use when computing the trapezoidal rule. 
<a name="l10816"><span class="ln">10816 </span></a>        x (Tensor): If specified, defines spacing between values as specified above. 
<a name="l10817"><span class="ln">10817 </span></a> 
<a name="l10818"><span class="ln">10818 </span></a>    Keyword arguments: 
<a name="l10819"><span class="ln">10819 </span></a>        dx (float): constant spacing between values. If neither :attr:`x` or :attr:`dx` 
<a name="l10820"><span class="ln">10820 </span></a>            are specified then this defaults to 1. Effectively multiplies the result by its value. 
<a name="l10821"><span class="ln">10821 </span></a>        dim (int): The dimension along which to compute the trapezoidal rule. 
<a name="l10822"><span class="ln">10822 </span></a>            The last (inner-most) dimension by default. 
<a name="l10823"><span class="ln">10823 </span></a> 
<a name="l10824"><span class="ln">10824 </span></a>    Examples:: 
<a name="l10825"><span class="ln">10825 </span></a> 
<a name="l10826"><span class="ln">10826 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D, spacing is implicitly 1. 
<a name="l10827"><span class="ln">10827 </span></a>        &gt;&gt;&gt; y = torch.tensor([1, 5, 10]) 
<a name="l10828"><span class="ln">10828 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y) 
<a name="l10829"><span class="ln">10829 </span></a>        tensor([3., 10.5]) 
<a name="l10830"><span class="ln">10830 </span></a> 
<a name="l10831"><span class="ln">10831 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly up to each element to verify 
<a name="l10832"><span class="ln">10832 </span></a>        &gt;&gt;&gt; (1 + 5) / 2 
<a name="l10833"><span class="ln">10833 </span></a>        3.0 
<a name="l10834"><span class="ln">10834 </span></a>        &gt;&gt;&gt; (1 + 10 + 10) / 2 
<a name="l10835"><span class="ln">10835 </span></a>        10.5 
<a name="l10836"><span class="ln">10836 </span></a> 
<a name="l10837"><span class="ln">10837 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D with constant spacing of 2 
<a name="l10838"><span class="ln">10838 </span></a>        &gt;&gt;&gt; # NOTE: the result is the same as before, but multiplied by 2 
<a name="l10839"><span class="ln">10839 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, dx=2) 
<a name="l10840"><span class="ln">10840 </span></a>        tensor([6., 21.]) 
<a name="l10841"><span class="ln">10841 </span></a> 
<a name="l10842"><span class="ln">10842 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule in 1D with arbitrary spacing 
<a name="l10843"><span class="ln">10843 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l10844"><span class="ln">10844 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l10845"><span class="ln">10845 </span></a>        tensor([6., 28.5]) 
<a name="l10846"><span class="ln">10846 </span></a> 
<a name="l10847"><span class="ln">10847 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly up to each element to verify 
<a name="l10848"><span class="ln">10848 </span></a>        &gt;&gt;&gt; ((3 - 1) * (1 + 5)) / 2 
<a name="l10849"><span class="ln">10849 </span></a>        6.0 
<a name="l10850"><span class="ln">10850 </span></a>        &gt;&gt;&gt; ((3 - 1) * (1 + 5) + (6 - 3) * (5 + 10)) / 2 
<a name="l10851"><span class="ln">10851 </span></a>        28.5 
<a name="l10852"><span class="ln">10852 </span></a> 
<a name="l10853"><span class="ln">10853 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 matrix 
<a name="l10854"><span class="ln">10854 </span></a>        &gt;&gt;&gt; y = torch.arange(9).reshape(3, 3) 
<a name="l10855"><span class="ln">10855 </span></a>        tensor([[0, 1, 2], 
<a name="l10856"><span class="ln">10856 </span></a>                [3, 4, 5], 
<a name="l10857"><span class="ln">10857 </span></a>                [6, 7, 8]]) 
<a name="l10858"><span class="ln">10858 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y) 
<a name="l10859"><span class="ln">10859 </span></a>        tensor([[ 0.5,  2.], 
<a name="l10860"><span class="ln">10860 </span></a>                [ 3.5,  8.], 
<a name="l10861"><span class="ln">10861 </span></a>                [ 6.5, 14.]]) 
<a name="l10862"><span class="ln">10862 </span></a> 
<a name="l10863"><span class="ln">10863 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each column of the matrix 
<a name="l10864"><span class="ln">10864 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, dim=0) 
<a name="l10865"><span class="ln">10865 </span></a>        tensor([[ 1.5,  2.5,  3.5], 
<a name="l10866"><span class="ln">10866 </span></a>                [ 6.0,  8.0, 10.0]]) 
<a name="l10867"><span class="ln">10867 </span></a> 
<a name="l10868"><span class="ln">10868 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l10869"><span class="ln">10869 </span></a>        &gt;&gt;&gt; #   with the same arbitrary spacing 
<a name="l10870"><span class="ln">10870 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l10871"><span class="ln">10871 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l10872"><span class="ln">10872 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l10873"><span class="ln">10873 </span></a>        tensor([[2., 5.], 
<a name="l10874"><span class="ln">10874 </span></a>                [2., 5.], 
<a name="l10875"><span class="ln">10875 </span></a>                [2., 5.]]) 
<a name="l10876"><span class="ln">10876 </span></a> 
<a name="l10877"><span class="ln">10877 </span></a>        &gt;&gt;&gt; # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l10878"><span class="ln">10878 </span></a>        &gt;&gt;&gt; #   with different arbitrary spacing per row 
<a name="l10879"><span class="ln">10879 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l10880"><span class="ln">10880 </span></a>        &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [1, 3, 5], [1, 4, 7]]) 
<a name="l10881"><span class="ln">10881 </span></a>        &gt;&gt;&gt; torch.cumulative_trapezoid(y, x) 
<a name="l10882"><span class="ln">10882 </span></a>        tensor([[1., 2.], 
<a name="l10883"><span class="ln">10883 </span></a>                [2., 4.], 
<a name="l10884"><span class="ln">10884 </span></a>                [3., 6.]]) 
<a name="l10885"><span class="ln">10885 </span></a>    &quot;&quot;&quot;</span>
<a name="l10886"><span class="ln">10886 </span></a>
<a name="l10887"><span class="ln">10887 </span></a><span class="s2">def </span><span class="s1">deg2rad</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10888"><span class="ln">10888 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10889"><span class="ln">10889 </span></a>    deg2rad(input, *, out=None) -&gt; Tensor 
<a name="l10890"><span class="ln">10890 </span></a> 
<a name="l10891"><span class="ln">10891 </span></a>    Returns a new tensor with each of the elements of :attr:`input` 
<a name="l10892"><span class="ln">10892 </span></a>    converted from angles in degrees to radians. 
<a name="l10893"><span class="ln">10893 </span></a> 
<a name="l10894"><span class="ln">10894 </span></a>    Args: 
<a name="l10895"><span class="ln">10895 </span></a>        input (Tensor): the input tensor. 
<a name="l10896"><span class="ln">10896 </span></a> 
<a name="l10897"><span class="ln">10897 </span></a>    Keyword arguments: 
<a name="l10898"><span class="ln">10898 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10899"><span class="ln">10899 </span></a> 
<a name="l10900"><span class="ln">10900 </span></a>    Example:: 
<a name="l10901"><span class="ln">10901 </span></a> 
<a name="l10902"><span class="ln">10902 </span></a>        &gt;&gt;&gt; a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]]) 
<a name="l10903"><span class="ln">10903 </span></a>        &gt;&gt;&gt; torch.deg2rad(a) 
<a name="l10904"><span class="ln">10904 </span></a>        tensor([[ 3.1416, -3.1416], 
<a name="l10905"><span class="ln">10905 </span></a>                [ 6.2832, -6.2832], 
<a name="l10906"><span class="ln">10906 </span></a>                [ 1.5708, -1.5708]]) 
<a name="l10907"><span class="ln">10907 </span></a>    &quot;&quot;&quot;</span>
<a name="l10908"><span class="ln">10908 </span></a>
<a name="l10909"><span class="ln">10909 </span></a><span class="s2">def </span><span class="s1">deg2rad_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10910"><span class="ln">10910 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10911"><span class="ln">10911 </span></a><span class="s2">def </span><span class="s1">dequantize</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10912"><span class="ln">10912 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10913"><span class="ln">10913 </span></a>    dequantize(tensor) -&gt; Tensor 
<a name="l10914"><span class="ln">10914 </span></a> 
<a name="l10915"><span class="ln">10915 </span></a>    Returns an fp32 Tensor by dequantizing a quantized Tensor 
<a name="l10916"><span class="ln">10916 </span></a> 
<a name="l10917"><span class="ln">10917 </span></a>    Args: 
<a name="l10918"><span class="ln">10918 </span></a>        tensor (Tensor): A quantized Tensor 
<a name="l10919"><span class="ln">10919 </span></a> 
<a name="l10920"><span class="ln">10920 </span></a>    .. function:: dequantize(tensors) -&gt; sequence of Tensors 
<a name="l10921"><span class="ln">10921 </span></a>       :noindex: 
<a name="l10922"><span class="ln">10922 </span></a> 
<a name="l10923"><span class="ln">10923 </span></a>    Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors 
<a name="l10924"><span class="ln">10924 </span></a> 
<a name="l10925"><span class="ln">10925 </span></a>    Args: 
<a name="l10926"><span class="ln">10926 </span></a>         tensors (sequence of Tensors): A list of quantized Tensors 
<a name="l10927"><span class="ln">10927 </span></a>    &quot;&quot;&quot;</span>
<a name="l10928"><span class="ln">10928 </span></a>
<a name="l10929"><span class="ln">10929 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l10930"><span class="ln">10930 </span></a><span class="s2">def </span><span class="s1">dequantize</span><span class="s3">(</span>
<a name="l10931"><span class="ln">10931 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l10932"><span class="ln">10932 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l10933"><span class="ln">10933 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10934"><span class="ln">10934 </span></a>    dequantize(tensor) -&gt; Tensor 
<a name="l10935"><span class="ln">10935 </span></a> 
<a name="l10936"><span class="ln">10936 </span></a>    Returns an fp32 Tensor by dequantizing a quantized Tensor 
<a name="l10937"><span class="ln">10937 </span></a> 
<a name="l10938"><span class="ln">10938 </span></a>    Args: 
<a name="l10939"><span class="ln">10939 </span></a>        tensor (Tensor): A quantized Tensor 
<a name="l10940"><span class="ln">10940 </span></a> 
<a name="l10941"><span class="ln">10941 </span></a>    .. function:: dequantize(tensors) -&gt; sequence of Tensors 
<a name="l10942"><span class="ln">10942 </span></a>       :noindex: 
<a name="l10943"><span class="ln">10943 </span></a> 
<a name="l10944"><span class="ln">10944 </span></a>    Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors 
<a name="l10945"><span class="ln">10945 </span></a> 
<a name="l10946"><span class="ln">10946 </span></a>    Args: 
<a name="l10947"><span class="ln">10947 </span></a>         tensors (sequence of Tensors): A list of quantized Tensors 
<a name="l10948"><span class="ln">10948 </span></a>    &quot;&quot;&quot;</span>
<a name="l10949"><span class="ln">10949 </span></a>
<a name="l10950"><span class="ln">10950 </span></a><span class="s2">def </span><span class="s1">det</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10951"><span class="ln">10951 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10952"><span class="ln">10952 </span></a>    det(input) -&gt; Tensor 
<a name="l10953"><span class="ln">10953 </span></a> 
<a name="l10954"><span class="ln">10954 </span></a>    Alias for :func:`torch.linalg.det` 
<a name="l10955"><span class="ln">10955 </span></a>    &quot;&quot;&quot;</span>
<a name="l10956"><span class="ln">10956 </span></a>
<a name="l10957"><span class="ln">10957 </span></a><span class="s2">def </span><span class="s1">detach</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10958"><span class="ln">10958 </span></a><span class="s2">def </span><span class="s1">detach_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l10959"><span class="ln">10959 </span></a><span class="s2">def </span><span class="s1">detach_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10960"><span class="ln">10960 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10961"><span class="ln">10961 </span></a>    Performs the same operation as :func:`torch.detach`, but all output tensors 
<a name="l10962"><span class="ln">10962 </span></a>    are freshly created instead of aliasing the input. 
<a name="l10963"><span class="ln">10963 </span></a>    &quot;&quot;&quot;</span>
<a name="l10964"><span class="ln">10964 </span></a>
<a name="l10965"><span class="ln">10965 </span></a><span class="s2">def </span><span class="s1">diag</span><span class="s3">(</span>
<a name="l10966"><span class="ln">10966 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l10967"><span class="ln">10967 </span></a>    <span class="s1">diagonal</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l10968"><span class="ln">10968 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l10969"><span class="ln">10969 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l10970"><span class="ln">10970 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l10971"><span class="ln">10971 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l10972"><span class="ln">10972 </span></a>    diag(input, diagonal=0, *, out=None) -&gt; Tensor 
<a name="l10973"><span class="ln">10973 </span></a> 
<a name="l10974"><span class="ln">10974 </span></a>    - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor 
<a name="l10975"><span class="ln">10975 </span></a>      with the elements of :attr:`input` as the diagonal. 
<a name="l10976"><span class="ln">10976 </span></a>    - If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with 
<a name="l10977"><span class="ln">10977 </span></a>      the diagonal elements of :attr:`input`. 
<a name="l10978"><span class="ln">10978 </span></a> 
<a name="l10979"><span class="ln">10979 </span></a>    The argument :attr:`diagonal` controls which diagonal to consider: 
<a name="l10980"><span class="ln">10980 </span></a> 
<a name="l10981"><span class="ln">10981 </span></a>    - If :attr:`diagonal` = 0, it is the main diagonal. 
<a name="l10982"><span class="ln">10982 </span></a>    - If :attr:`diagonal` &gt; 0, it is above the main diagonal. 
<a name="l10983"><span class="ln">10983 </span></a>    - If :attr:`diagonal` &lt; 0, it is below the main diagonal. 
<a name="l10984"><span class="ln">10984 </span></a> 
<a name="l10985"><span class="ln">10985 </span></a>    Args: 
<a name="l10986"><span class="ln">10986 </span></a>        input (Tensor): the input tensor. 
<a name="l10987"><span class="ln">10987 </span></a>        diagonal (int, optional): the diagonal to consider 
<a name="l10988"><span class="ln">10988 </span></a> 
<a name="l10989"><span class="ln">10989 </span></a>    Keyword args: 
<a name="l10990"><span class="ln">10990 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l10991"><span class="ln">10991 </span></a> 
<a name="l10992"><span class="ln">10992 </span></a>    .. seealso:: 
<a name="l10993"><span class="ln">10993 </span></a> 
<a name="l10994"><span class="ln">10994 </span></a>            :func:`torch.diagonal` always returns the diagonal of its input. 
<a name="l10995"><span class="ln">10995 </span></a> 
<a name="l10996"><span class="ln">10996 </span></a>            :func:`torch.diagflat` always constructs a tensor with diagonal elements 
<a name="l10997"><span class="ln">10997 </span></a>            specified by the input. 
<a name="l10998"><span class="ln">10998 </span></a> 
<a name="l10999"><span class="ln">10999 </span></a>    Examples: 
<a name="l11000"><span class="ln">11000 </span></a> 
<a name="l11001"><span class="ln">11001 </span></a>    Get the square matrix where the input vector is the diagonal:: 
<a name="l11002"><span class="ln">11002 </span></a> 
<a name="l11003"><span class="ln">11003 </span></a>        &gt;&gt;&gt; a = torch.randn(3) 
<a name="l11004"><span class="ln">11004 </span></a>        &gt;&gt;&gt; a 
<a name="l11005"><span class="ln">11005 </span></a>        tensor([ 0.5950,-0.0872, 2.3298]) 
<a name="l11006"><span class="ln">11006 </span></a>        &gt;&gt;&gt; torch.diag(a) 
<a name="l11007"><span class="ln">11007 </span></a>        tensor([[ 0.5950, 0.0000, 0.0000], 
<a name="l11008"><span class="ln">11008 </span></a>                [ 0.0000,-0.0872, 0.0000], 
<a name="l11009"><span class="ln">11009 </span></a>                [ 0.0000, 0.0000, 2.3298]]) 
<a name="l11010"><span class="ln">11010 </span></a>        &gt;&gt;&gt; torch.diag(a, 1) 
<a name="l11011"><span class="ln">11011 </span></a>        tensor([[ 0.0000, 0.5950, 0.0000, 0.0000], 
<a name="l11012"><span class="ln">11012 </span></a>                [ 0.0000, 0.0000,-0.0872, 0.0000], 
<a name="l11013"><span class="ln">11013 </span></a>                [ 0.0000, 0.0000, 0.0000, 2.3298], 
<a name="l11014"><span class="ln">11014 </span></a>                [ 0.0000, 0.0000, 0.0000, 0.0000]]) 
<a name="l11015"><span class="ln">11015 </span></a> 
<a name="l11016"><span class="ln">11016 </span></a>    Get the k-th diagonal of a given matrix:: 
<a name="l11017"><span class="ln">11017 </span></a> 
<a name="l11018"><span class="ln">11018 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l11019"><span class="ln">11019 </span></a>        &gt;&gt;&gt; a 
<a name="l11020"><span class="ln">11020 </span></a>        tensor([[-0.4264, 0.0255,-0.1064], 
<a name="l11021"><span class="ln">11021 </span></a>                [ 0.8795,-0.2429, 0.1374], 
<a name="l11022"><span class="ln">11022 </span></a>                [ 0.1029,-0.6482,-1.6300]]) 
<a name="l11023"><span class="ln">11023 </span></a>        &gt;&gt;&gt; torch.diag(a, 0) 
<a name="l11024"><span class="ln">11024 </span></a>        tensor([-0.4264,-0.2429,-1.6300]) 
<a name="l11025"><span class="ln">11025 </span></a>        &gt;&gt;&gt; torch.diag(a, 1) 
<a name="l11026"><span class="ln">11026 </span></a>        tensor([ 0.0255, 0.1374]) 
<a name="l11027"><span class="ln">11027 </span></a>    &quot;&quot;&quot;</span>
<a name="l11028"><span class="ln">11028 </span></a>
<a name="l11029"><span class="ln">11029 </span></a><span class="s2">def </span><span class="s1">diag_embed</span><span class="s3">(</span>
<a name="l11030"><span class="ln">11030 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11031"><span class="ln">11031 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11032"><span class="ln">11032 </span></a>    <span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">2</span><span class="s3">,</span>
<a name="l11033"><span class="ln">11033 </span></a>    <span class="s1">dim2</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l11034"><span class="ln">11034 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11035"><span class="ln">11035 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11036"><span class="ln">11036 </span></a>    diag_embed(input, offset=0, dim1=-2, dim2=-1) -&gt; Tensor 
<a name="l11037"><span class="ln">11037 </span></a> 
<a name="l11038"><span class="ln">11038 </span></a>    Creates a tensor whose diagonals of certain 2D planes (specified by 
<a name="l11039"><span class="ln">11039 </span></a>    :attr:`dim1` and :attr:`dim2`) are filled by :attr:`input`. 
<a name="l11040"><span class="ln">11040 </span></a>    To facilitate creating batched diagonal matrices, the 2D planes formed by 
<a name="l11041"><span class="ln">11041 </span></a>    the last two dimensions of the returned tensor are chosen by default. 
<a name="l11042"><span class="ln">11042 </span></a> 
<a name="l11043"><span class="ln">11043 </span></a>    The argument :attr:`offset` controls which diagonal to consider: 
<a name="l11044"><span class="ln">11044 </span></a> 
<a name="l11045"><span class="ln">11045 </span></a>    - If :attr:`offset` = 0, it is the main diagonal. 
<a name="l11046"><span class="ln">11046 </span></a>    - If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l11047"><span class="ln">11047 </span></a>    - If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l11048"><span class="ln">11048 </span></a> 
<a name="l11049"><span class="ln">11049 </span></a>    The size of the new matrix will be calculated to make the specified diagonal 
<a name="l11050"><span class="ln">11050 </span></a>    of the size of the last input dimension. 
<a name="l11051"><span class="ln">11051 </span></a>    Note that for :attr:`offset` other than :math:`0`, the order of :attr:`dim1` 
<a name="l11052"><span class="ln">11052 </span></a>    and :attr:`dim2` matters. Exchanging them is equivalent to changing the 
<a name="l11053"><span class="ln">11053 </span></a>    sign of :attr:`offset`. 
<a name="l11054"><span class="ln">11054 </span></a> 
<a name="l11055"><span class="ln">11055 </span></a>    Applying :meth:`torch.diagonal` to the output of this function with 
<a name="l11056"><span class="ln">11056 </span></a>    the same arguments yields a matrix identical to input. However, 
<a name="l11057"><span class="ln">11057 </span></a>    :meth:`torch.diagonal` has different default dimensions, so those 
<a name="l11058"><span class="ln">11058 </span></a>    need to be explicitly specified. 
<a name="l11059"><span class="ln">11059 </span></a> 
<a name="l11060"><span class="ln">11060 </span></a>    Args: 
<a name="l11061"><span class="ln">11061 </span></a>        input (Tensor): the input tensor. Must be at least 1-dimensional. 
<a name="l11062"><span class="ln">11062 </span></a>        offset (int, optional): which diagonal to consider. Default: 0 
<a name="l11063"><span class="ln">11063 </span></a>            (main diagonal). 
<a name="l11064"><span class="ln">11064 </span></a>        dim1 (int, optional): first dimension with respect to which to 
<a name="l11065"><span class="ln">11065 </span></a>            take diagonal. Default: -2. 
<a name="l11066"><span class="ln">11066 </span></a>        dim2 (int, optional): second dimension with respect to which to 
<a name="l11067"><span class="ln">11067 </span></a>            take diagonal. Default: -1. 
<a name="l11068"><span class="ln">11068 </span></a> 
<a name="l11069"><span class="ln">11069 </span></a>    Example:: 
<a name="l11070"><span class="ln">11070 </span></a> 
<a name="l11071"><span class="ln">11071 </span></a>        &gt;&gt;&gt; a = torch.randn(2, 3) 
<a name="l11072"><span class="ln">11072 </span></a>        &gt;&gt;&gt; torch.diag_embed(a) 
<a name="l11073"><span class="ln">11073 </span></a>        tensor([[[ 1.5410,  0.0000,  0.0000], 
<a name="l11074"><span class="ln">11074 </span></a>                 [ 0.0000, -0.2934,  0.0000], 
<a name="l11075"><span class="ln">11075 </span></a>                 [ 0.0000,  0.0000, -2.1788]], 
<a name="l11076"><span class="ln">11076 </span></a> 
<a name="l11077"><span class="ln">11077 </span></a>                [[ 0.5684,  0.0000,  0.0000], 
<a name="l11078"><span class="ln">11078 </span></a>                 [ 0.0000, -1.0845,  0.0000], 
<a name="l11079"><span class="ln">11079 </span></a>                 [ 0.0000,  0.0000, -1.3986]]]) 
<a name="l11080"><span class="ln">11080 </span></a> 
<a name="l11081"><span class="ln">11081 </span></a>        &gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2) 
<a name="l11082"><span class="ln">11082 </span></a>        tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000], 
<a name="l11083"><span class="ln">11083 </span></a>                 [ 0.0000,  0.5684,  0.0000,  0.0000]], 
<a name="l11084"><span class="ln">11084 </span></a> 
<a name="l11085"><span class="ln">11085 </span></a>                [[ 0.0000,  0.0000, -0.2934,  0.0000], 
<a name="l11086"><span class="ln">11086 </span></a>                 [ 0.0000,  0.0000, -1.0845,  0.0000]], 
<a name="l11087"><span class="ln">11087 </span></a> 
<a name="l11088"><span class="ln">11088 </span></a>                [[ 0.0000,  0.0000,  0.0000, -2.1788], 
<a name="l11089"><span class="ln">11089 </span></a>                 [ 0.0000,  0.0000,  0.0000, -1.3986]], 
<a name="l11090"><span class="ln">11090 </span></a> 
<a name="l11091"><span class="ln">11091 </span></a>                [[ 0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l11092"><span class="ln">11092 </span></a>                 [ 0.0000,  0.0000,  0.0000,  0.0000]]]) 
<a name="l11093"><span class="ln">11093 </span></a>    &quot;&quot;&quot;</span>
<a name="l11094"><span class="ln">11094 </span></a>
<a name="l11095"><span class="ln">11095 </span></a><span class="s2">def </span><span class="s1">diagflat</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11096"><span class="ln">11096 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11097"><span class="ln">11097 </span></a>    diagflat(input, offset=0) -&gt; Tensor 
<a name="l11098"><span class="ln">11098 </span></a> 
<a name="l11099"><span class="ln">11099 </span></a>    - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor 
<a name="l11100"><span class="ln">11100 </span></a>      with the elements of :attr:`input` as the diagonal. 
<a name="l11101"><span class="ln">11101 </span></a>    - If :attr:`input` is a tensor with more than one dimension, then returns a 
<a name="l11102"><span class="ln">11102 </span></a>      2-D tensor with diagonal elements equal to a flattened :attr:`input`. 
<a name="l11103"><span class="ln">11103 </span></a> 
<a name="l11104"><span class="ln">11104 </span></a>    The argument :attr:`offset` controls which diagonal to consider: 
<a name="l11105"><span class="ln">11105 </span></a> 
<a name="l11106"><span class="ln">11106 </span></a>    - If :attr:`offset` = 0, it is the main diagonal. 
<a name="l11107"><span class="ln">11107 </span></a>    - If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l11108"><span class="ln">11108 </span></a>    - If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l11109"><span class="ln">11109 </span></a> 
<a name="l11110"><span class="ln">11110 </span></a>    Args: 
<a name="l11111"><span class="ln">11111 </span></a>        input (Tensor): the input tensor. 
<a name="l11112"><span class="ln">11112 </span></a>        offset (int, optional): the diagonal to consider. Default: 0 (main 
<a name="l11113"><span class="ln">11113 </span></a>            diagonal). 
<a name="l11114"><span class="ln">11114 </span></a> 
<a name="l11115"><span class="ln">11115 </span></a>    Examples:: 
<a name="l11116"><span class="ln">11116 </span></a> 
<a name="l11117"><span class="ln">11117 </span></a>        &gt;&gt;&gt; a = torch.randn(3) 
<a name="l11118"><span class="ln">11118 </span></a>        &gt;&gt;&gt; a 
<a name="l11119"><span class="ln">11119 </span></a>        tensor([-0.2956, -0.9068,  0.1695]) 
<a name="l11120"><span class="ln">11120 </span></a>        &gt;&gt;&gt; torch.diagflat(a) 
<a name="l11121"><span class="ln">11121 </span></a>        tensor([[-0.2956,  0.0000,  0.0000], 
<a name="l11122"><span class="ln">11122 </span></a>                [ 0.0000, -0.9068,  0.0000], 
<a name="l11123"><span class="ln">11123 </span></a>                [ 0.0000,  0.0000,  0.1695]]) 
<a name="l11124"><span class="ln">11124 </span></a>        &gt;&gt;&gt; torch.diagflat(a, 1) 
<a name="l11125"><span class="ln">11125 </span></a>        tensor([[ 0.0000, -0.2956,  0.0000,  0.0000], 
<a name="l11126"><span class="ln">11126 </span></a>                [ 0.0000,  0.0000, -0.9068,  0.0000], 
<a name="l11127"><span class="ln">11127 </span></a>                [ 0.0000,  0.0000,  0.0000,  0.1695], 
<a name="l11128"><span class="ln">11128 </span></a>                [ 0.0000,  0.0000,  0.0000,  0.0000]]) 
<a name="l11129"><span class="ln">11129 </span></a> 
<a name="l11130"><span class="ln">11130 </span></a>        &gt;&gt;&gt; a = torch.randn(2, 2) 
<a name="l11131"><span class="ln">11131 </span></a>        &gt;&gt;&gt; a 
<a name="l11132"><span class="ln">11132 </span></a>        tensor([[ 0.2094, -0.3018], 
<a name="l11133"><span class="ln">11133 </span></a>                [-0.1516,  1.9342]]) 
<a name="l11134"><span class="ln">11134 </span></a>        &gt;&gt;&gt; torch.diagflat(a) 
<a name="l11135"><span class="ln">11135 </span></a>        tensor([[ 0.2094,  0.0000,  0.0000,  0.0000], 
<a name="l11136"><span class="ln">11136 </span></a>                [ 0.0000, -0.3018,  0.0000,  0.0000], 
<a name="l11137"><span class="ln">11137 </span></a>                [ 0.0000,  0.0000, -0.1516,  0.0000], 
<a name="l11138"><span class="ln">11138 </span></a>                [ 0.0000,  0.0000,  0.0000,  1.9342]]) 
<a name="l11139"><span class="ln">11139 </span></a>    &quot;&quot;&quot;</span>
<a name="l11140"><span class="ln">11140 </span></a>
<a name="l11141"><span class="ln">11141 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11142"><span class="ln">11142 </span></a><span class="s2">def </span><span class="s1">diagonal</span><span class="s3">(</span>
<a name="l11143"><span class="ln">11143 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11144"><span class="ln">11144 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11145"><span class="ln">11145 </span></a>    <span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11146"><span class="ln">11146 </span></a>    <span class="s1">dim2</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l11147"><span class="ln">11147 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11148"><span class="ln">11148 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11149"><span class="ln">11149 </span></a>    diagonal(input, offset=0, dim1=0, dim2=1) -&gt; Tensor 
<a name="l11150"><span class="ln">11150 </span></a> 
<a name="l11151"><span class="ln">11151 </span></a>    Returns a partial view of :attr:`input` with the its diagonal elements 
<a name="l11152"><span class="ln">11152 </span></a>    with respect to :attr:`dim1` and :attr:`dim2` appended as a dimension 
<a name="l11153"><span class="ln">11153 </span></a>    at the end of the shape. 
<a name="l11154"><span class="ln">11154 </span></a> 
<a name="l11155"><span class="ln">11155 </span></a>    The argument :attr:`offset` controls which diagonal to consider: 
<a name="l11156"><span class="ln">11156 </span></a> 
<a name="l11157"><span class="ln">11157 </span></a>    - If :attr:`offset` = 0, it is the main diagonal. 
<a name="l11158"><span class="ln">11158 </span></a>    - If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l11159"><span class="ln">11159 </span></a>    - If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l11160"><span class="ln">11160 </span></a> 
<a name="l11161"><span class="ln">11161 </span></a>    Applying :meth:`torch.diag_embed` to the output of this function with 
<a name="l11162"><span class="ln">11162 </span></a>    the same arguments yields a diagonal matrix with the diagonal entries 
<a name="l11163"><span class="ln">11163 </span></a>    of the input. However, :meth:`torch.diag_embed` has different default 
<a name="l11164"><span class="ln">11164 </span></a>    dimensions, so those need to be explicitly specified. 
<a name="l11165"><span class="ln">11165 </span></a> 
<a name="l11166"><span class="ln">11166 </span></a>    Args: 
<a name="l11167"><span class="ln">11167 </span></a>        input (Tensor): the input tensor. Must be at least 2-dimensional. 
<a name="l11168"><span class="ln">11168 </span></a>        offset (int, optional): which diagonal to consider. Default: 0 
<a name="l11169"><span class="ln">11169 </span></a>            (main diagonal). 
<a name="l11170"><span class="ln">11170 </span></a>        dim1 (int, optional): first dimension with respect to which to 
<a name="l11171"><span class="ln">11171 </span></a>            take diagonal. Default: 0. 
<a name="l11172"><span class="ln">11172 </span></a>        dim2 (int, optional): second dimension with respect to which to 
<a name="l11173"><span class="ln">11173 </span></a>            take diagonal. Default: 1. 
<a name="l11174"><span class="ln">11174 </span></a> 
<a name="l11175"><span class="ln">11175 </span></a>    .. note::  To take a batch diagonal, pass in dim1=-2, dim2=-1. 
<a name="l11176"><span class="ln">11176 </span></a> 
<a name="l11177"><span class="ln">11177 </span></a>    Examples:: 
<a name="l11178"><span class="ln">11178 </span></a> 
<a name="l11179"><span class="ln">11179 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l11180"><span class="ln">11180 </span></a>        &gt;&gt;&gt; a 
<a name="l11181"><span class="ln">11181 </span></a>        tensor([[-1.0854,  1.1431, -0.1752], 
<a name="l11182"><span class="ln">11182 </span></a>                [ 0.8536, -0.0905,  0.0360], 
<a name="l11183"><span class="ln">11183 </span></a>                [ 0.6927, -0.3735, -0.4945]]) 
<a name="l11184"><span class="ln">11184 </span></a> 
<a name="l11185"><span class="ln">11185 </span></a> 
<a name="l11186"><span class="ln">11186 </span></a>        &gt;&gt;&gt; torch.diagonal(a) 
<a name="l11187"><span class="ln">11187 </span></a>        tensor([-1.0854, -0.0905, -0.4945]) 
<a name="l11188"><span class="ln">11188 </span></a> 
<a name="l11189"><span class="ln">11189 </span></a> 
<a name="l11190"><span class="ln">11190 </span></a>        &gt;&gt;&gt; torch.diagonal(a, 1) 
<a name="l11191"><span class="ln">11191 </span></a>        tensor([ 1.1431,  0.0360]) 
<a name="l11192"><span class="ln">11192 </span></a> 
<a name="l11193"><span class="ln">11193 </span></a>        &gt;&gt;&gt; b = torch.randn(2, 5) 
<a name="l11194"><span class="ln">11194 </span></a>        &gt;&gt;&gt; b 
<a name="l11195"><span class="ln">11195 </span></a>        tensor([[-1.7948, -1.2731, -0.3181,  2.0200, -1.6745], 
<a name="l11196"><span class="ln">11196 </span></a>                [ 1.8262, -1.5049,  0.4114,  1.0704, -1.2607]]) 
<a name="l11197"><span class="ln">11197 </span></a> 
<a name="l11198"><span class="ln">11198 </span></a>        &gt;&gt;&gt; torch.diagonal(b, 1, 1, 0) 
<a name="l11199"><span class="ln">11199 </span></a>        tensor([1.8262]) 
<a name="l11200"><span class="ln">11200 </span></a> 
<a name="l11201"><span class="ln">11201 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 5, 4, 2) 
<a name="l11202"><span class="ln">11202 </span></a>        &gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2) 
<a name="l11203"><span class="ln">11203 </span></a>        tensor([[[-1.2631,  0.3755, -1.5977, -1.8172], 
<a name="l11204"><span class="ln">11204 </span></a>                 [-1.1065,  1.0401, -0.2235, -0.7938]], 
<a name="l11205"><span class="ln">11205 </span></a> 
<a name="l11206"><span class="ln">11206 </span></a>                [[-1.7325, -0.3081,  0.6166,  0.2335], 
<a name="l11207"><span class="ln">11207 </span></a>                 [ 1.0500,  0.7336, -0.3836, -1.1015]]]) 
<a name="l11208"><span class="ln">11208 </span></a>    &quot;&quot;&quot;</span>
<a name="l11209"><span class="ln">11209 </span></a>
<a name="l11210"><span class="ln">11210 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11211"><span class="ln">11211 </span></a><span class="s2">def </span><span class="s1">diagonal</span><span class="s3">(</span>
<a name="l11212"><span class="ln">11212 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11213"><span class="ln">11213 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11214"><span class="ln">11214 </span></a>    <span class="s1">outdim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11215"><span class="ln">11215 </span></a>    <span class="s1">dim1</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11216"><span class="ln">11216 </span></a>    <span class="s1">dim2</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11217"><span class="ln">11217 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11218"><span class="ln">11218 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11219"><span class="ln">11219 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11220"><span class="ln">11220 </span></a>    diagonal(input, offset=0, dim1=0, dim2=1) -&gt; Tensor 
<a name="l11221"><span class="ln">11221 </span></a> 
<a name="l11222"><span class="ln">11222 </span></a>    Returns a partial view of :attr:`input` with the its diagonal elements 
<a name="l11223"><span class="ln">11223 </span></a>    with respect to :attr:`dim1` and :attr:`dim2` appended as a dimension 
<a name="l11224"><span class="ln">11224 </span></a>    at the end of the shape. 
<a name="l11225"><span class="ln">11225 </span></a> 
<a name="l11226"><span class="ln">11226 </span></a>    The argument :attr:`offset` controls which diagonal to consider: 
<a name="l11227"><span class="ln">11227 </span></a> 
<a name="l11228"><span class="ln">11228 </span></a>    - If :attr:`offset` = 0, it is the main diagonal. 
<a name="l11229"><span class="ln">11229 </span></a>    - If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l11230"><span class="ln">11230 </span></a>    - If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l11231"><span class="ln">11231 </span></a> 
<a name="l11232"><span class="ln">11232 </span></a>    Applying :meth:`torch.diag_embed` to the output of this function with 
<a name="l11233"><span class="ln">11233 </span></a>    the same arguments yields a diagonal matrix with the diagonal entries 
<a name="l11234"><span class="ln">11234 </span></a>    of the input. However, :meth:`torch.diag_embed` has different default 
<a name="l11235"><span class="ln">11235 </span></a>    dimensions, so those need to be explicitly specified. 
<a name="l11236"><span class="ln">11236 </span></a> 
<a name="l11237"><span class="ln">11237 </span></a>    Args: 
<a name="l11238"><span class="ln">11238 </span></a>        input (Tensor): the input tensor. Must be at least 2-dimensional. 
<a name="l11239"><span class="ln">11239 </span></a>        offset (int, optional): which diagonal to consider. Default: 0 
<a name="l11240"><span class="ln">11240 </span></a>            (main diagonal). 
<a name="l11241"><span class="ln">11241 </span></a>        dim1 (int, optional): first dimension with respect to which to 
<a name="l11242"><span class="ln">11242 </span></a>            take diagonal. Default: 0. 
<a name="l11243"><span class="ln">11243 </span></a>        dim2 (int, optional): second dimension with respect to which to 
<a name="l11244"><span class="ln">11244 </span></a>            take diagonal. Default: 1. 
<a name="l11245"><span class="ln">11245 </span></a> 
<a name="l11246"><span class="ln">11246 </span></a>    .. note::  To take a batch diagonal, pass in dim1=-2, dim2=-1. 
<a name="l11247"><span class="ln">11247 </span></a> 
<a name="l11248"><span class="ln">11248 </span></a>    Examples:: 
<a name="l11249"><span class="ln">11249 </span></a> 
<a name="l11250"><span class="ln">11250 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l11251"><span class="ln">11251 </span></a>        &gt;&gt;&gt; a 
<a name="l11252"><span class="ln">11252 </span></a>        tensor([[-1.0854,  1.1431, -0.1752], 
<a name="l11253"><span class="ln">11253 </span></a>                [ 0.8536, -0.0905,  0.0360], 
<a name="l11254"><span class="ln">11254 </span></a>                [ 0.6927, -0.3735, -0.4945]]) 
<a name="l11255"><span class="ln">11255 </span></a> 
<a name="l11256"><span class="ln">11256 </span></a> 
<a name="l11257"><span class="ln">11257 </span></a>        &gt;&gt;&gt; torch.diagonal(a) 
<a name="l11258"><span class="ln">11258 </span></a>        tensor([-1.0854, -0.0905, -0.4945]) 
<a name="l11259"><span class="ln">11259 </span></a> 
<a name="l11260"><span class="ln">11260 </span></a> 
<a name="l11261"><span class="ln">11261 </span></a>        &gt;&gt;&gt; torch.diagonal(a, 1) 
<a name="l11262"><span class="ln">11262 </span></a>        tensor([ 1.1431,  0.0360]) 
<a name="l11263"><span class="ln">11263 </span></a> 
<a name="l11264"><span class="ln">11264 </span></a>        &gt;&gt;&gt; b = torch.randn(2, 5) 
<a name="l11265"><span class="ln">11265 </span></a>        &gt;&gt;&gt; b 
<a name="l11266"><span class="ln">11266 </span></a>        tensor([[-1.7948, -1.2731, -0.3181,  2.0200, -1.6745], 
<a name="l11267"><span class="ln">11267 </span></a>                [ 1.8262, -1.5049,  0.4114,  1.0704, -1.2607]]) 
<a name="l11268"><span class="ln">11268 </span></a> 
<a name="l11269"><span class="ln">11269 </span></a>        &gt;&gt;&gt; torch.diagonal(b, 1, 1, 0) 
<a name="l11270"><span class="ln">11270 </span></a>        tensor([1.8262]) 
<a name="l11271"><span class="ln">11271 </span></a> 
<a name="l11272"><span class="ln">11272 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 5, 4, 2) 
<a name="l11273"><span class="ln">11273 </span></a>        &gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2) 
<a name="l11274"><span class="ln">11274 </span></a>        tensor([[[-1.2631,  0.3755, -1.5977, -1.8172], 
<a name="l11275"><span class="ln">11275 </span></a>                 [-1.1065,  1.0401, -0.2235, -0.7938]], 
<a name="l11276"><span class="ln">11276 </span></a> 
<a name="l11277"><span class="ln">11277 </span></a>                [[-1.7325, -0.3081,  0.6166,  0.2335], 
<a name="l11278"><span class="ln">11278 </span></a>                 [ 1.0500,  0.7336, -0.3836, -1.1015]]]) 
<a name="l11279"><span class="ln">11279 </span></a>    &quot;&quot;&quot;</span>
<a name="l11280"><span class="ln">11280 </span></a>
<a name="l11281"><span class="ln">11281 </span></a><span class="s2">def </span><span class="s1">diagonal_copy</span><span class="s3">(</span>
<a name="l11282"><span class="ln">11282 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11283"><span class="ln">11283 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11284"><span class="ln">11284 </span></a>    <span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11285"><span class="ln">11285 </span></a>    <span class="s1">dim2</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l11286"><span class="ln">11286 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11287"><span class="ln">11287 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11288"><span class="ln">11288 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11289"><span class="ln">11289 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11290"><span class="ln">11290 </span></a>    Performs the same operation as :func:`torch.diagonal`, but all output tensors 
<a name="l11291"><span class="ln">11291 </span></a>    are freshly created instead of aliasing the input. 
<a name="l11292"><span class="ln">11292 </span></a>    &quot;&quot;&quot;</span>
<a name="l11293"><span class="ln">11293 </span></a>
<a name="l11294"><span class="ln">11294 </span></a><span class="s2">def </span><span class="s1">diagonal_scatter</span><span class="s3">(</span>
<a name="l11295"><span class="ln">11295 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11296"><span class="ln">11296 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11297"><span class="ln">11297 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11298"><span class="ln">11298 </span></a>    <span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11299"><span class="ln">11299 </span></a>    <span class="s1">dim2</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l11300"><span class="ln">11300 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11301"><span class="ln">11301 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11302"><span class="ln">11302 </span></a>    diagonal_scatter(input, src, offset=0, dim1=0, dim2=1) -&gt; Tensor 
<a name="l11303"><span class="ln">11303 </span></a> 
<a name="l11304"><span class="ln">11304 </span></a>    Embeds the values of the :attr:`src` tensor into :attr:`input` along 
<a name="l11305"><span class="ln">11305 </span></a>    the diagonal elements of :attr:`input`, with respect to :attr:`dim1` 
<a name="l11306"><span class="ln">11306 </span></a>    and :attr:`dim2`. 
<a name="l11307"><span class="ln">11307 </span></a> 
<a name="l11308"><span class="ln">11308 </span></a>    This function returns a tensor with fresh storage; it does not 
<a name="l11309"><span class="ln">11309 </span></a>    return a view. 
<a name="l11310"><span class="ln">11310 </span></a> 
<a name="l11311"><span class="ln">11311 </span></a>    The argument :attr:`offset` controls which diagonal to consider: 
<a name="l11312"><span class="ln">11312 </span></a> 
<a name="l11313"><span class="ln">11313 </span></a>    - If :attr:`offset` = 0, it is the main diagonal. 
<a name="l11314"><span class="ln">11314 </span></a>    - If :attr:`offset` &gt; 0, it is above the main diagonal. 
<a name="l11315"><span class="ln">11315 </span></a>    - If :attr:`offset` &lt; 0, it is below the main diagonal. 
<a name="l11316"><span class="ln">11316 </span></a> 
<a name="l11317"><span class="ln">11317 </span></a>    Args: 
<a name="l11318"><span class="ln">11318 </span></a>        input (Tensor): the input tensor. Must be at least 2-dimensional. 
<a name="l11319"><span class="ln">11319 </span></a>        src (Tensor): the tensor to embed into :attr:`input`. 
<a name="l11320"><span class="ln">11320 </span></a>        offset (int, optional): which diagonal to consider. Default: 0 
<a name="l11321"><span class="ln">11321 </span></a>            (main diagonal). 
<a name="l11322"><span class="ln">11322 </span></a>        dim1 (int, optional): first dimension with respect to which to 
<a name="l11323"><span class="ln">11323 </span></a>            take diagonal. Default: 0. 
<a name="l11324"><span class="ln">11324 </span></a>        dim2 (int, optional): second dimension with respect to which to 
<a name="l11325"><span class="ln">11325 </span></a>            take diagonal. Default: 1. 
<a name="l11326"><span class="ln">11326 </span></a> 
<a name="l11327"><span class="ln">11327 </span></a>    .. note:: 
<a name="l11328"><span class="ln">11328 </span></a> 
<a name="l11329"><span class="ln">11329 </span></a>        :attr:`src` must be of the proper size in order to be embedded 
<a name="l11330"><span class="ln">11330 </span></a>        into :attr:`input`. Specifically, it should have the same shape as 
<a name="l11331"><span class="ln">11331 </span></a>        ``torch.diagonal(input, offset, dim1, dim2)`` 
<a name="l11332"><span class="ln">11332 </span></a> 
<a name="l11333"><span class="ln">11333 </span></a>    Examples:: 
<a name="l11334"><span class="ln">11334 </span></a> 
<a name="l11335"><span class="ln">11335 </span></a>        &gt;&gt;&gt; a = torch.zeros(3, 3) 
<a name="l11336"><span class="ln">11336 </span></a>        &gt;&gt;&gt; a 
<a name="l11337"><span class="ln">11337 </span></a>        tensor([[0., 0., 0.], 
<a name="l11338"><span class="ln">11338 </span></a>                [0., 0., 0.], 
<a name="l11339"><span class="ln">11339 </span></a>                [0., 0., 0.]]) 
<a name="l11340"><span class="ln">11340 </span></a> 
<a name="l11341"><span class="ln">11341 </span></a>        &gt;&gt;&gt; torch.diagonal_scatter(a, torch.ones(3), 0) 
<a name="l11342"><span class="ln">11342 </span></a>        tensor([[1., 0., 0.], 
<a name="l11343"><span class="ln">11343 </span></a>                [0., 1., 0.], 
<a name="l11344"><span class="ln">11344 </span></a>                [0., 0., 1.]]) 
<a name="l11345"><span class="ln">11345 </span></a> 
<a name="l11346"><span class="ln">11346 </span></a>        &gt;&gt;&gt; torch.diagonal_scatter(a, torch.ones(2), 1) 
<a name="l11347"><span class="ln">11347 </span></a>        tensor([[0., 1., 0.], 
<a name="l11348"><span class="ln">11348 </span></a>                [0., 0., 1.], 
<a name="l11349"><span class="ln">11349 </span></a>                [0., 0., 0.]]) 
<a name="l11350"><span class="ln">11350 </span></a>    &quot;&quot;&quot;</span>
<a name="l11351"><span class="ln">11351 </span></a>
<a name="l11352"><span class="ln">11352 </span></a><span class="s2">def </span><span class="s1">diff</span><span class="s3">(</span>
<a name="l11353"><span class="ln">11353 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11354"><span class="ln">11354 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l11355"><span class="ln">11355 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l11356"><span class="ln">11356 </span></a>    <span class="s1">prepend</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11357"><span class="ln">11357 </span></a>    <span class="s1">append</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11358"><span class="ln">11358 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11359"><span class="ln">11359 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11360"><span class="ln">11360 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11361"><span class="ln">11361 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11362"><span class="ln">11362 </span></a>    diff(input, n=1, dim=-1, prepend=None, append=None) -&gt; Tensor 
<a name="l11363"><span class="ln">11363 </span></a> 
<a name="l11364"><span class="ln">11364 </span></a>    Computes the n-th forward difference along the given dimension. 
<a name="l11365"><span class="ln">11365 </span></a> 
<a name="l11366"><span class="ln">11366 </span></a>    The first-order differences are given by `out[i] = input[i + 1] - input[i]`. Higher-order 
<a name="l11367"><span class="ln">11367 </span></a>    differences are calculated by using :func:`torch.diff` recursively. 
<a name="l11368"><span class="ln">11368 </span></a> 
<a name="l11369"><span class="ln">11369 </span></a>    Args: 
<a name="l11370"><span class="ln">11370 </span></a>        input (Tensor): the tensor to compute the differences on 
<a name="l11371"><span class="ln">11371 </span></a>        n (int, optional): the number of times to recursively compute the difference 
<a name="l11372"><span class="ln">11372 </span></a>        dim (int, optional): the dimension to compute the difference along. 
<a name="l11373"><span class="ln">11373 </span></a>            Default is the last dimension. 
<a name="l11374"><span class="ln">11374 </span></a>        prepend, append (Tensor, optional): values to prepend or append to 
<a name="l11375"><span class="ln">11375 </span></a>            :attr:`input` along :attr:`dim` before computing the difference. 
<a name="l11376"><span class="ln">11376 </span></a>            Their dimensions must be equivalent to that of input, and their shapes 
<a name="l11377"><span class="ln">11377 </span></a>            must match input's shape except on :attr:`dim`. 
<a name="l11378"><span class="ln">11378 </span></a> 
<a name="l11379"><span class="ln">11379 </span></a>    Keyword args: 
<a name="l11380"><span class="ln">11380 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11381"><span class="ln">11381 </span></a> 
<a name="l11382"><span class="ln">11382 </span></a>    Example:: 
<a name="l11383"><span class="ln">11383 </span></a> 
<a name="l11384"><span class="ln">11384 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, 3, 2]) 
<a name="l11385"><span class="ln">11385 </span></a>        &gt;&gt;&gt; torch.diff(a) 
<a name="l11386"><span class="ln">11386 </span></a>        tensor([ 2, -1]) 
<a name="l11387"><span class="ln">11387 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 5]) 
<a name="l11388"><span class="ln">11388 </span></a>        &gt;&gt;&gt; torch.diff(a, append=b) 
<a name="l11389"><span class="ln">11389 </span></a>        tensor([ 2, -1,  2,  1]) 
<a name="l11390"><span class="ln">11390 </span></a>        &gt;&gt;&gt; c = torch.tensor([[1, 2, 3], [3, 4, 5]]) 
<a name="l11391"><span class="ln">11391 </span></a>        &gt;&gt;&gt; torch.diff(c, dim=0) 
<a name="l11392"><span class="ln">11392 </span></a>        tensor([[2, 2, 2]]) 
<a name="l11393"><span class="ln">11393 </span></a>        &gt;&gt;&gt; torch.diff(c, dim=1) 
<a name="l11394"><span class="ln">11394 </span></a>        tensor([[1, 1], 
<a name="l11395"><span class="ln">11395 </span></a>                [1, 1]]) 
<a name="l11396"><span class="ln">11396 </span></a>    &quot;&quot;&quot;</span>
<a name="l11397"><span class="ln">11397 </span></a>
<a name="l11398"><span class="ln">11398 </span></a><span class="s2">def </span><span class="s1">digamma</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11399"><span class="ln">11399 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11400"><span class="ln">11400 </span></a>    digamma(input, *, out=None) -&gt; Tensor 
<a name="l11401"><span class="ln">11401 </span></a> 
<a name="l11402"><span class="ln">11402 </span></a>    Alias for :func:`torch.special.digamma`. 
<a name="l11403"><span class="ln">11403 </span></a>    &quot;&quot;&quot;</span>
<a name="l11404"><span class="ln">11404 </span></a>
<a name="l11405"><span class="ln">11405 </span></a><span class="s2">def </span><span class="s1">dist</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">2</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11406"><span class="ln">11406 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11407"><span class="ln">11407 </span></a>    dist(input, other, p=2) -&gt; Tensor 
<a name="l11408"><span class="ln">11408 </span></a> 
<a name="l11409"><span class="ln">11409 </span></a>    Returns the p-norm of (:attr:`input` - :attr:`other`) 
<a name="l11410"><span class="ln">11410 </span></a> 
<a name="l11411"><span class="ln">11411 </span></a>    The shapes of :attr:`input` and :attr:`other` must be 
<a name="l11412"><span class="ln">11412 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l11413"><span class="ln">11413 </span></a> 
<a name="l11414"><span class="ln">11414 </span></a>    Args: 
<a name="l11415"><span class="ln">11415 </span></a>        input (Tensor): the input tensor. 
<a name="l11416"><span class="ln">11416 </span></a>        other (Tensor): the Right-hand-side input tensor 
<a name="l11417"><span class="ln">11417 </span></a>        p (float, optional): the norm to be computed 
<a name="l11418"><span class="ln">11418 </span></a> 
<a name="l11419"><span class="ln">11419 </span></a>    Example:: 
<a name="l11420"><span class="ln">11420 </span></a> 
<a name="l11421"><span class="ln">11421 </span></a>        &gt;&gt;&gt; x = torch.randn(4) 
<a name="l11422"><span class="ln">11422 </span></a>        &gt;&gt;&gt; x 
<a name="l11423"><span class="ln">11423 </span></a>        tensor([-1.5393, -0.8675,  0.5916,  1.6321]) 
<a name="l11424"><span class="ln">11424 </span></a>        &gt;&gt;&gt; y = torch.randn(4) 
<a name="l11425"><span class="ln">11425 </span></a>        &gt;&gt;&gt; y 
<a name="l11426"><span class="ln">11426 </span></a>        tensor([ 0.0967, -1.0511,  0.6295,  0.8360]) 
<a name="l11427"><span class="ln">11427 </span></a>        &gt;&gt;&gt; torch.dist(x, y, 3.5) 
<a name="l11428"><span class="ln">11428 </span></a>        tensor(1.6727) 
<a name="l11429"><span class="ln">11429 </span></a>        &gt;&gt;&gt; torch.dist(x, y, 3) 
<a name="l11430"><span class="ln">11430 </span></a>        tensor(1.6973) 
<a name="l11431"><span class="ln">11431 </span></a>        &gt;&gt;&gt; torch.dist(x, y, 0) 
<a name="l11432"><span class="ln">11432 </span></a>        tensor(4.) 
<a name="l11433"><span class="ln">11433 </span></a>        &gt;&gt;&gt; torch.dist(x, y, 1) 
<a name="l11434"><span class="ln">11434 </span></a>        tensor(2.6537) 
<a name="l11435"><span class="ln">11435 </span></a>    &quot;&quot;&quot;</span>
<a name="l11436"><span class="ln">11436 </span></a>
<a name="l11437"><span class="ln">11437 </span></a><span class="s2">def </span><span class="s1">div</span><span class="s3">(</span>
<a name="l11438"><span class="ln">11438 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l11439"><span class="ln">11439 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l11440"><span class="ln">11440 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11441"><span class="ln">11441 </span></a>    <span class="s1">rounding_mode</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11442"><span class="ln">11442 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11443"><span class="ln">11443 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11444"><span class="ln">11444 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11445"><span class="ln">11445 </span></a>    div(input, other, *, rounding_mode=None, out=None) -&gt; Tensor 
<a name="l11446"><span class="ln">11446 </span></a> 
<a name="l11447"><span class="ln">11447 </span></a>    Divides each element of the input ``input`` by the corresponding element of 
<a name="l11448"><span class="ln">11448 </span></a>    :attr:`other`. 
<a name="l11449"><span class="ln">11449 </span></a> 
<a name="l11450"><span class="ln">11450 </span></a>    .. math:: 
<a name="l11451"><span class="ln">11451 </span></a>        \text{out}_i = \frac{\text{input}_i}{\text{other}_i} 
<a name="l11452"><span class="ln">11452 </span></a> 
<a name="l11453"><span class="ln">11453 </span></a>    .. note:: 
<a name="l11454"><span class="ln">11454 </span></a>        By default, this performs a &quot;true&quot; division like Python 3. 
<a name="l11455"><span class="ln">11455 </span></a>        See the :attr:`rounding_mode` argument for floor division. 
<a name="l11456"><span class="ln">11456 </span></a> 
<a name="l11457"><span class="ln">11457 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l11458"><span class="ln">11458 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l11459"><span class="ln">11459 </span></a>    Always promotes integer types to the default scalar type. 
<a name="l11460"><span class="ln">11460 </span></a> 
<a name="l11461"><span class="ln">11461 </span></a>    Args: 
<a name="l11462"><span class="ln">11462 </span></a>        input (Tensor): the dividend 
<a name="l11463"><span class="ln">11463 </span></a>        other (Tensor or Number): the divisor 
<a name="l11464"><span class="ln">11464 </span></a> 
<a name="l11465"><span class="ln">11465 </span></a>    Keyword args: 
<a name="l11466"><span class="ln">11466 </span></a>        rounding_mode (str, optional): Type of rounding applied to the result: 
<a name="l11467"><span class="ln">11467 </span></a> 
<a name="l11468"><span class="ln">11468 </span></a>            * None - default behavior. Performs no rounding and, if both :attr:`input` and 
<a name="l11469"><span class="ln">11469 </span></a>              :attr:`other` are integer types, promotes the inputs to the default scalar type. 
<a name="l11470"><span class="ln">11470 </span></a>              Equivalent to true division in Python (the ``/`` operator) and NumPy's ``np.true_divide``. 
<a name="l11471"><span class="ln">11471 </span></a>            * ``&quot;trunc&quot;`` - rounds the results of the division towards zero. 
<a name="l11472"><span class="ln">11472 </span></a>              Equivalent to C-style integer division. 
<a name="l11473"><span class="ln">11473 </span></a>            * ``&quot;floor&quot;`` - rounds the results of the division down. 
<a name="l11474"><span class="ln">11474 </span></a>              Equivalent to floor division in Python (the ``//`` operator) and NumPy's ``np.floor_divide``. 
<a name="l11475"><span class="ln">11475 </span></a> 
<a name="l11476"><span class="ln">11476 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11477"><span class="ln">11477 </span></a> 
<a name="l11478"><span class="ln">11478 </span></a>    Examples:: 
<a name="l11479"><span class="ln">11479 </span></a> 
<a name="l11480"><span class="ln">11480 </span></a>        &gt;&gt;&gt; x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637]) 
<a name="l11481"><span class="ln">11481 </span></a>        &gt;&gt;&gt; torch.div(x, 0.5) 
<a name="l11482"><span class="ln">11482 </span></a>        tensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274]) 
<a name="l11483"><span class="ln">11483 </span></a> 
<a name="l11484"><span class="ln">11484 </span></a>        &gt;&gt;&gt; a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917], 
<a name="l11485"><span class="ln">11485 </span></a>        ...                   [ 0.1815, -1.0111,  0.9805, -1.5923], 
<a name="l11486"><span class="ln">11486 </span></a>        ...                   [ 0.1062,  1.4581,  0.7759, -1.2344], 
<a name="l11487"><span class="ln">11487 </span></a>        ...                   [-0.1830, -0.0313,  1.1908, -1.4757]]) 
<a name="l11488"><span class="ln">11488 </span></a>        &gt;&gt;&gt; b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308]) 
<a name="l11489"><span class="ln">11489 </span></a>        &gt;&gt;&gt; torch.div(a, b) 
<a name="l11490"><span class="ln">11490 </span></a>        tensor([[-0.4620, -6.6051,  0.5676,  1.2639], 
<a name="l11491"><span class="ln">11491 </span></a>                [ 0.2260, -3.4509, -1.2086,  6.8990], 
<a name="l11492"><span class="ln">11492 </span></a>                [ 0.1322,  4.9764, -0.9564,  5.3484], 
<a name="l11493"><span class="ln">11493 </span></a>                [-0.2278, -0.1068, -1.4678,  6.3938]]) 
<a name="l11494"><span class="ln">11494 </span></a> 
<a name="l11495"><span class="ln">11495 </span></a>        &gt;&gt;&gt; torch.div(a, b, rounding_mode='trunc') 
<a name="l11496"><span class="ln">11496 </span></a>        tensor([[-0., -6.,  0.,  1.], 
<a name="l11497"><span class="ln">11497 </span></a>                [ 0., -3., -1.,  6.], 
<a name="l11498"><span class="ln">11498 </span></a>                [ 0.,  4., -0.,  5.], 
<a name="l11499"><span class="ln">11499 </span></a>                [-0., -0., -1.,  6.]]) 
<a name="l11500"><span class="ln">11500 </span></a> 
<a name="l11501"><span class="ln">11501 </span></a>        &gt;&gt;&gt; torch.div(a, b, rounding_mode='floor') 
<a name="l11502"><span class="ln">11502 </span></a>        tensor([[-1., -7.,  0.,  1.], 
<a name="l11503"><span class="ln">11503 </span></a>                [ 0., -4., -2.,  6.], 
<a name="l11504"><span class="ln">11504 </span></a>                [ 0.,  4., -1.,  5.], 
<a name="l11505"><span class="ln">11505 </span></a>                [-1., -1., -2.,  6.]]) 
<a name="l11506"><span class="ln">11506 </span></a>    &quot;&quot;&quot;</span>
<a name="l11507"><span class="ln">11507 </span></a>
<a name="l11508"><span class="ln">11508 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11509"><span class="ln">11509 </span></a><span class="s2">def </span><span class="s1">divide</span><span class="s3">(</span>
<a name="l11510"><span class="ln">11510 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11511"><span class="ln">11511 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11512"><span class="ln">11512 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11513"><span class="ln">11513 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11514"><span class="ln">11514 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11515"><span class="ln">11515 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11516"><span class="ln">11516 </span></a>    divide(input, other, *, rounding_mode=None, out=None) -&gt; Tensor 
<a name="l11517"><span class="ln">11517 </span></a> 
<a name="l11518"><span class="ln">11518 </span></a>    Alias for :func:`torch.div`. 
<a name="l11519"><span class="ln">11519 </span></a>    &quot;&quot;&quot;</span>
<a name="l11520"><span class="ln">11520 </span></a>
<a name="l11521"><span class="ln">11521 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11522"><span class="ln">11522 </span></a><span class="s2">def </span><span class="s1">divide</span><span class="s3">(</span>
<a name="l11523"><span class="ln">11523 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11524"><span class="ln">11524 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11525"><span class="ln">11525 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11526"><span class="ln">11526 </span></a>    <span class="s1">rounding_mode</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11527"><span class="ln">11527 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11528"><span class="ln">11528 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11529"><span class="ln">11529 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11530"><span class="ln">11530 </span></a>    divide(input, other, *, rounding_mode=None, out=None) -&gt; Tensor 
<a name="l11531"><span class="ln">11531 </span></a> 
<a name="l11532"><span class="ln">11532 </span></a>    Alias for :func:`torch.div`. 
<a name="l11533"><span class="ln">11533 </span></a>    &quot;&quot;&quot;</span>
<a name="l11534"><span class="ln">11534 </span></a>
<a name="l11535"><span class="ln">11535 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11536"><span class="ln">11536 </span></a><span class="s2">def </span><span class="s1">divide</span><span class="s3">(</span>
<a name="l11537"><span class="ln">11537 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11538"><span class="ln">11538 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l11539"><span class="ln">11539 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11540"><span class="ln">11540 </span></a>    <span class="s1">rounding_mode</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11541"><span class="ln">11541 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11542"><span class="ln">11542 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11543"><span class="ln">11543 </span></a>    divide(input, other, *, rounding_mode=None, out=None) -&gt; Tensor 
<a name="l11544"><span class="ln">11544 </span></a> 
<a name="l11545"><span class="ln">11545 </span></a>    Alias for :func:`torch.div`. 
<a name="l11546"><span class="ln">11546 </span></a>    &quot;&quot;&quot;</span>
<a name="l11547"><span class="ln">11547 </span></a>
<a name="l11548"><span class="ln">11548 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11549"><span class="ln">11549 </span></a><span class="s2">def </span><span class="s1">divide</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11550"><span class="ln">11550 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11551"><span class="ln">11551 </span></a>    divide(input, other, *, rounding_mode=None, out=None) -&gt; Tensor 
<a name="l11552"><span class="ln">11552 </span></a> 
<a name="l11553"><span class="ln">11553 </span></a>    Alias for :func:`torch.div`. 
<a name="l11554"><span class="ln">11554 </span></a>    &quot;&quot;&quot;</span>
<a name="l11555"><span class="ln">11555 </span></a>
<a name="l11556"><span class="ln">11556 </span></a><span class="s2">def </span><span class="s1">dot</span><span class="s3">(</span>
<a name="l11557"><span class="ln">11557 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11558"><span class="ln">11558 </span></a>    <span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11559"><span class="ln">11559 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11560"><span class="ln">11560 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11561"><span class="ln">11561 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11562"><span class="ln">11562 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11563"><span class="ln">11563 </span></a>    dot(input, tensor, *, out=None) -&gt; Tensor 
<a name="l11564"><span class="ln">11564 </span></a> 
<a name="l11565"><span class="ln">11565 </span></a>    Computes the dot product of two 1D tensors. 
<a name="l11566"><span class="ln">11566 </span></a> 
<a name="l11567"><span class="ln">11567 </span></a>    .. note:: 
<a name="l11568"><span class="ln">11568 </span></a> 
<a name="l11569"><span class="ln">11569 </span></a>        Unlike NumPy's dot, torch.dot intentionally only supports computing the dot product 
<a name="l11570"><span class="ln">11570 </span></a>        of two 1D tensors with the same number of elements. 
<a name="l11571"><span class="ln">11571 </span></a> 
<a name="l11572"><span class="ln">11572 </span></a>    Args: 
<a name="l11573"><span class="ln">11573 </span></a>        input (Tensor): first tensor in the dot product, must be 1D. 
<a name="l11574"><span class="ln">11574 </span></a>        tensor (Tensor): second tensor in the dot product, must be 1D. 
<a name="l11575"><span class="ln">11575 </span></a> 
<a name="l11576"><span class="ln">11576 </span></a>    Keyword args: 
<a name="l11577"><span class="ln">11577 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11578"><span class="ln">11578 </span></a> 
<a name="l11579"><span class="ln">11579 </span></a>    Example:: 
<a name="l11580"><span class="ln">11580 </span></a> 
<a name="l11581"><span class="ln">11581 </span></a>        &gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])) 
<a name="l11582"><span class="ln">11582 </span></a>        tensor(7) 
<a name="l11583"><span class="ln">11583 </span></a> 
<a name="l11584"><span class="ln">11584 </span></a>        &gt;&gt;&gt; t1, t2 = torch.tensor([0, 1]), torch.tensor([2, 3]) 
<a name="l11585"><span class="ln">11585 </span></a>        &gt;&gt;&gt; torch.dot(t1, t2) 
<a name="l11586"><span class="ln">11586 </span></a>        tensor(3) 
<a name="l11587"><span class="ln">11587 </span></a>    &quot;&quot;&quot;</span>
<a name="l11588"><span class="ln">11588 </span></a>
<a name="l11589"><span class="ln">11589 </span></a><span class="s2">def </span><span class="s1">dropout</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l11590"><span class="ln">11590 </span></a><span class="s2">def </span><span class="s1">dropout_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l11591"><span class="ln">11591 </span></a><span class="s2">def </span><span class="s1">dsmm</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l11592"><span class="ln">11592 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11593"><span class="ln">11593 </span></a><span class="s2">def </span><span class="s1">dsplit</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">sections</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l11594"><span class="ln">11594 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11595"><span class="ln">11595 </span></a>    dsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l11596"><span class="ln">11596 </span></a> 
<a name="l11597"><span class="ln">11597 </span></a>    Splits :attr:`input`, a tensor with three or more dimensions, into multiple tensors 
<a name="l11598"><span class="ln">11598 </span></a>    depthwise according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l11599"><span class="ln">11599 </span></a>    :attr:`input`. 
<a name="l11600"><span class="ln">11600 </span></a> 
<a name="l11601"><span class="ln">11601 </span></a>    This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=2) 
<a name="l11602"><span class="ln">11602 </span></a>    (the split dimension is 2), except that if :attr:`indices_or_sections` is an integer 
<a name="l11603"><span class="ln">11603 </span></a>    it must evenly divide the split dimension or a runtime error will be thrown. 
<a name="l11604"><span class="ln">11604 </span></a> 
<a name="l11605"><span class="ln">11605 </span></a>    This function is based on NumPy's :func:`numpy.dsplit`. 
<a name="l11606"><span class="ln">11606 </span></a> 
<a name="l11607"><span class="ln">11607 </span></a>    Args: 
<a name="l11608"><span class="ln">11608 </span></a>        input (Tensor): tensor to split. 
<a name="l11609"><span class="ln">11609 </span></a>        indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l11610"><span class="ln">11610 </span></a> 
<a name="l11611"><span class="ln">11611 </span></a>    Example:: 
<a name="l11612"><span class="ln">11612 </span></a> 
<a name="l11613"><span class="ln">11613 </span></a>        &gt;&gt;&gt; t = torch.arange(16.0).reshape(2, 2, 4) 
<a name="l11614"><span class="ln">11614 </span></a>        &gt;&gt;&gt; t 
<a name="l11615"><span class="ln">11615 </span></a>        tensor([[[ 0.,  1.,  2.,  3.], 
<a name="l11616"><span class="ln">11616 </span></a>                 [ 4.,  5.,  6.,  7.]], 
<a name="l11617"><span class="ln">11617 </span></a>                [[ 8.,  9., 10., 11.], 
<a name="l11618"><span class="ln">11618 </span></a>                 [12., 13., 14., 15.]]]) 
<a name="l11619"><span class="ln">11619 </span></a>        &gt;&gt;&gt; torch.dsplit(t, 2) 
<a name="l11620"><span class="ln">11620 </span></a>        (tensor([[[ 0.,  1.], 
<a name="l11621"><span class="ln">11621 </span></a>                [ 4.,  5.]], 
<a name="l11622"><span class="ln">11622 </span></a>               [[ 8.,  9.], 
<a name="l11623"><span class="ln">11623 </span></a>                [12., 13.]]]), 
<a name="l11624"><span class="ln">11624 </span></a>         tensor([[[ 2.,  3.], 
<a name="l11625"><span class="ln">11625 </span></a>                  [ 6.,  7.]], 
<a name="l11626"><span class="ln">11626 </span></a>                 [[10., 11.], 
<a name="l11627"><span class="ln">11627 </span></a>                  [14., 15.]]])) 
<a name="l11628"><span class="ln">11628 </span></a> 
<a name="l11629"><span class="ln">11629 </span></a>        &gt;&gt;&gt; torch.dsplit(t, [3, 6]) 
<a name="l11630"><span class="ln">11630 </span></a>        (tensor([[[ 0.,  1.,  2.], 
<a name="l11631"><span class="ln">11631 </span></a>                  [ 4.,  5.,  6.]], 
<a name="l11632"><span class="ln">11632 </span></a>                 [[ 8.,  9., 10.], 
<a name="l11633"><span class="ln">11633 </span></a>                  [12., 13., 14.]]]), 
<a name="l11634"><span class="ln">11634 </span></a>         tensor([[[ 3.], 
<a name="l11635"><span class="ln">11635 </span></a>                  [ 7.]], 
<a name="l11636"><span class="ln">11636 </span></a>                 [[11.], 
<a name="l11637"><span class="ln">11637 </span></a>                  [15.]]]), 
<a name="l11638"><span class="ln">11638 </span></a>         tensor([], size=(2, 2, 0))) 
<a name="l11639"><span class="ln">11639 </span></a>    &quot;&quot;&quot;</span>
<a name="l11640"><span class="ln">11640 </span></a>
<a name="l11641"><span class="ln">11641 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11642"><span class="ln">11642 </span></a><span class="s2">def </span><span class="s1">dsplit</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">indices</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l11643"><span class="ln">11643 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11644"><span class="ln">11644 </span></a>    dsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l11645"><span class="ln">11645 </span></a> 
<a name="l11646"><span class="ln">11646 </span></a>    Splits :attr:`input`, a tensor with three or more dimensions, into multiple tensors 
<a name="l11647"><span class="ln">11647 </span></a>    depthwise according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l11648"><span class="ln">11648 </span></a>    :attr:`input`. 
<a name="l11649"><span class="ln">11649 </span></a> 
<a name="l11650"><span class="ln">11650 </span></a>    This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=2) 
<a name="l11651"><span class="ln">11651 </span></a>    (the split dimension is 2), except that if :attr:`indices_or_sections` is an integer 
<a name="l11652"><span class="ln">11652 </span></a>    it must evenly divide the split dimension or a runtime error will be thrown. 
<a name="l11653"><span class="ln">11653 </span></a> 
<a name="l11654"><span class="ln">11654 </span></a>    This function is based on NumPy's :func:`numpy.dsplit`. 
<a name="l11655"><span class="ln">11655 </span></a> 
<a name="l11656"><span class="ln">11656 </span></a>    Args: 
<a name="l11657"><span class="ln">11657 </span></a>        input (Tensor): tensor to split. 
<a name="l11658"><span class="ln">11658 </span></a>        indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l11659"><span class="ln">11659 </span></a> 
<a name="l11660"><span class="ln">11660 </span></a>    Example:: 
<a name="l11661"><span class="ln">11661 </span></a> 
<a name="l11662"><span class="ln">11662 </span></a>        &gt;&gt;&gt; t = torch.arange(16.0).reshape(2, 2, 4) 
<a name="l11663"><span class="ln">11663 </span></a>        &gt;&gt;&gt; t 
<a name="l11664"><span class="ln">11664 </span></a>        tensor([[[ 0.,  1.,  2.,  3.], 
<a name="l11665"><span class="ln">11665 </span></a>                 [ 4.,  5.,  6.,  7.]], 
<a name="l11666"><span class="ln">11666 </span></a>                [[ 8.,  9., 10., 11.], 
<a name="l11667"><span class="ln">11667 </span></a>                 [12., 13., 14., 15.]]]) 
<a name="l11668"><span class="ln">11668 </span></a>        &gt;&gt;&gt; torch.dsplit(t, 2) 
<a name="l11669"><span class="ln">11669 </span></a>        (tensor([[[ 0.,  1.], 
<a name="l11670"><span class="ln">11670 </span></a>                [ 4.,  5.]], 
<a name="l11671"><span class="ln">11671 </span></a>               [[ 8.,  9.], 
<a name="l11672"><span class="ln">11672 </span></a>                [12., 13.]]]), 
<a name="l11673"><span class="ln">11673 </span></a>         tensor([[[ 2.,  3.], 
<a name="l11674"><span class="ln">11674 </span></a>                  [ 6.,  7.]], 
<a name="l11675"><span class="ln">11675 </span></a>                 [[10., 11.], 
<a name="l11676"><span class="ln">11676 </span></a>                  [14., 15.]]])) 
<a name="l11677"><span class="ln">11677 </span></a> 
<a name="l11678"><span class="ln">11678 </span></a>        &gt;&gt;&gt; torch.dsplit(t, [3, 6]) 
<a name="l11679"><span class="ln">11679 </span></a>        (tensor([[[ 0.,  1.,  2.], 
<a name="l11680"><span class="ln">11680 </span></a>                  [ 4.,  5.,  6.]], 
<a name="l11681"><span class="ln">11681 </span></a>                 [[ 8.,  9., 10.], 
<a name="l11682"><span class="ln">11682 </span></a>                  [12., 13., 14.]]]), 
<a name="l11683"><span class="ln">11683 </span></a>         tensor([[[ 3.], 
<a name="l11684"><span class="ln">11684 </span></a>                  [ 7.]], 
<a name="l11685"><span class="ln">11685 </span></a>                 [[11.], 
<a name="l11686"><span class="ln">11686 </span></a>                  [15.]]]), 
<a name="l11687"><span class="ln">11687 </span></a>         tensor([], size=(2, 2, 0))) 
<a name="l11688"><span class="ln">11688 </span></a>    &quot;&quot;&quot;</span>
<a name="l11689"><span class="ln">11689 </span></a>
<a name="l11690"><span class="ln">11690 </span></a><span class="s2">def </span><span class="s1">dstack</span><span class="s3">(</span>
<a name="l11691"><span class="ln">11691 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11692"><span class="ln">11692 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11693"><span class="ln">11693 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11694"><span class="ln">11694 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11695"><span class="ln">11695 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11696"><span class="ln">11696 </span></a>    dstack(tensors, *, out=None) -&gt; Tensor 
<a name="l11697"><span class="ln">11697 </span></a> 
<a name="l11698"><span class="ln">11698 </span></a>    Stack tensors in sequence depthwise (along third axis). 
<a name="l11699"><span class="ln">11699 </span></a> 
<a name="l11700"><span class="ln">11700 </span></a>    This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by :func:`torch.atleast_3d`. 
<a name="l11701"><span class="ln">11701 </span></a> 
<a name="l11702"><span class="ln">11702 </span></a>    Args: 
<a name="l11703"><span class="ln">11703 </span></a>        tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l11704"><span class="ln">11704 </span></a> 
<a name="l11705"><span class="ln">11705 </span></a>    Keyword args: 
<a name="l11706"><span class="ln">11706 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11707"><span class="ln">11707 </span></a> 
<a name="l11708"><span class="ln">11708 </span></a>    Example:: 
<a name="l11709"><span class="ln">11709 </span></a> 
<a name="l11710"><span class="ln">11710 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l11711"><span class="ln">11711 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l11712"><span class="ln">11712 </span></a>        &gt;&gt;&gt; torch.dstack((a,b)) 
<a name="l11713"><span class="ln">11713 </span></a>        tensor([[[1, 4], 
<a name="l11714"><span class="ln">11714 </span></a>                 [2, 5], 
<a name="l11715"><span class="ln">11715 </span></a>                 [3, 6]]]) 
<a name="l11716"><span class="ln">11716 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1],[2],[3]]) 
<a name="l11717"><span class="ln">11717 </span></a>        &gt;&gt;&gt; b = torch.tensor([[4],[5],[6]]) 
<a name="l11718"><span class="ln">11718 </span></a>        &gt;&gt;&gt; torch.dstack((a,b)) 
<a name="l11719"><span class="ln">11719 </span></a>        tensor([[[1, 4]], 
<a name="l11720"><span class="ln">11720 </span></a>                [[2, 5]], 
<a name="l11721"><span class="ln">11721 </span></a>                [[3, 6]]]) 
<a name="l11722"><span class="ln">11722 </span></a>    &quot;&quot;&quot;</span>
<a name="l11723"><span class="ln">11723 </span></a>
<a name="l11724"><span class="ln">11724 </span></a><span class="s2">def </span><span class="s1">embedding</span><span class="s3">(</span>
<a name="l11725"><span class="ln">11725 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11726"><span class="ln">11726 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11727"><span class="ln">11727 </span></a>    <span class="s1">padding_idx</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l11728"><span class="ln">11728 </span></a>    <span class="s1">scale_grad_by_freq</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l11729"><span class="ln">11729 </span></a>    <span class="s1">sparse</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l11730"><span class="ln">11730 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l11731"><span class="ln">11731 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11732"><span class="ln">11732 </span></a><span class="s2">def </span><span class="s1">embedding_bag</span><span class="s3">(</span>
<a name="l11733"><span class="ln">11733 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11734"><span class="ln">11734 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11735"><span class="ln">11735 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11736"><span class="ln">11736 </span></a>    <span class="s1">scale_grad_by_freq</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l11737"><span class="ln">11737 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l11738"><span class="ln">11738 </span></a>    <span class="s1">sparse</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l11739"><span class="ln">11739 </span></a>    <span class="s1">per_sample_weights</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11740"><span class="ln">11740 </span></a>    <span class="s1">include_last_offset</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l11741"><span class="ln">11741 </span></a>    <span class="s1">padding_idx</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11742"><span class="ln">11742 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l11743"><span class="ln">11743 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11744"><span class="ln">11744 </span></a><span class="s2">def </span><span class="s1">embedding_bag</span><span class="s3">(</span>
<a name="l11745"><span class="ln">11745 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11746"><span class="ln">11746 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11747"><span class="ln">11747 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11748"><span class="ln">11748 </span></a>    <span class="s1">scale_grad_by_freq</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l11749"><span class="ln">11749 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l11750"><span class="ln">11750 </span></a>    <span class="s1">sparse</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l11751"><span class="ln">11751 </span></a>    <span class="s1">per_sample_weights</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11752"><span class="ln">11752 </span></a>    <span class="s1">include_last_offset</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l11753"><span class="ln">11753 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l11754"><span class="ln">11754 </span></a><span class="s2">def </span><span class="s1">embedding_renorm_</span><span class="s3">(</span>
<a name="l11755"><span class="ln">11755 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11756"><span class="ln">11756 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11757"><span class="ln">11757 </span></a>    <span class="s1">max_norm</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l11758"><span class="ln">11758 </span></a>    <span class="s1">norm_type</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l11759"><span class="ln">11759 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l11760"><span class="ln">11760 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11761"><span class="ln">11761 </span></a><span class="s2">def </span><span class="s1">empty</span><span class="s3">(</span>
<a name="l11762"><span class="ln">11762 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l11763"><span class="ln">11763 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11764"><span class="ln">11764 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11765"><span class="ln">11765 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11766"><span class="ln">11766 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11767"><span class="ln">11767 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11768"><span class="ln">11768 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11769"><span class="ln">11769 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11770"><span class="ln">11770 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11771"><span class="ln">11771 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11772"><span class="ln">11772 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11773"><span class="ln">11773 </span></a>    empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l11774"><span class="ln">11774 </span></a> 
<a name="l11775"><span class="ln">11775 </span></a>    Returns a tensor filled with uninitialized data. The shape of the tensor is 
<a name="l11776"><span class="ln">11776 </span></a>    defined by the variable argument :attr:`size`. 
<a name="l11777"><span class="ln">11777 </span></a> 
<a name="l11778"><span class="ln">11778 </span></a>    .. note:: 
<a name="l11779"><span class="ln">11779 </span></a>        If :func:`torch.use_deterministic_algorithms()` and 
<a name="l11780"><span class="ln">11780 </span></a>        :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l11781"><span class="ln">11781 </span></a>        ``True``, the output tensor is initialized to prevent any possible 
<a name="l11782"><span class="ln">11782 </span></a>        nondeterministic behavior from using the data as an input to an operation. 
<a name="l11783"><span class="ln">11783 </span></a>        Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l11784"><span class="ln">11784 </span></a>        are filled with the maximum value. 
<a name="l11785"><span class="ln">11785 </span></a> 
<a name="l11786"><span class="ln">11786 </span></a>    Args: 
<a name="l11787"><span class="ln">11787 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l11788"><span class="ln">11788 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l11789"><span class="ln">11789 </span></a> 
<a name="l11790"><span class="ln">11790 </span></a>    Keyword args: 
<a name="l11791"><span class="ln">11791 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11792"><span class="ln">11792 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l11793"><span class="ln">11793 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l11794"><span class="ln">11794 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l11795"><span class="ln">11795 </span></a>            Default: ``torch.strided``. 
<a name="l11796"><span class="ln">11796 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l11797"><span class="ln">11797 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l11798"><span class="ln">11798 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l11799"><span class="ln">11799 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l11800"><span class="ln">11800 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l11801"><span class="ln">11801 </span></a>            returned tensor. Default: ``False``. 
<a name="l11802"><span class="ln">11802 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l11803"><span class="ln">11803 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l11804"><span class="ln">11804 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l11805"><span class="ln">11805 </span></a>            returned Tensor. Default: ``torch.contiguous_format``. 
<a name="l11806"><span class="ln">11806 </span></a> 
<a name="l11807"><span class="ln">11807 </span></a>    Example:: 
<a name="l11808"><span class="ln">11808 </span></a> 
<a name="l11809"><span class="ln">11809 </span></a>        &gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64) 
<a name="l11810"><span class="ln">11810 </span></a>        tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13], 
<a name="l11811"><span class="ln">11811 </span></a>                [ 7.5751e+18,  7.1428e+18,  7.5955e+18]]) 
<a name="l11812"><span class="ln">11812 </span></a>    &quot;&quot;&quot;</span>
<a name="l11813"><span class="ln">11813 </span></a>
<a name="l11814"><span class="ln">11814 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11815"><span class="ln">11815 </span></a><span class="s2">def </span><span class="s1">empty</span><span class="s3">(</span>
<a name="l11816"><span class="ln">11816 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l11817"><span class="ln">11817 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11818"><span class="ln">11818 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11819"><span class="ln">11819 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11820"><span class="ln">11820 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11821"><span class="ln">11821 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11822"><span class="ln">11822 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11823"><span class="ln">11823 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11824"><span class="ln">11824 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11825"><span class="ln">11825 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11826"><span class="ln">11826 </span></a>    empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l11827"><span class="ln">11827 </span></a> 
<a name="l11828"><span class="ln">11828 </span></a>    Returns a tensor filled with uninitialized data. The shape of the tensor is 
<a name="l11829"><span class="ln">11829 </span></a>    defined by the variable argument :attr:`size`. 
<a name="l11830"><span class="ln">11830 </span></a> 
<a name="l11831"><span class="ln">11831 </span></a>    .. note:: 
<a name="l11832"><span class="ln">11832 </span></a>        If :func:`torch.use_deterministic_algorithms()` and 
<a name="l11833"><span class="ln">11833 </span></a>        :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l11834"><span class="ln">11834 </span></a>        ``True``, the output tensor is initialized to prevent any possible 
<a name="l11835"><span class="ln">11835 </span></a>        nondeterministic behavior from using the data as an input to an operation. 
<a name="l11836"><span class="ln">11836 </span></a>        Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l11837"><span class="ln">11837 </span></a>        are filled with the maximum value. 
<a name="l11838"><span class="ln">11838 </span></a> 
<a name="l11839"><span class="ln">11839 </span></a>    Args: 
<a name="l11840"><span class="ln">11840 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l11841"><span class="ln">11841 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l11842"><span class="ln">11842 </span></a> 
<a name="l11843"><span class="ln">11843 </span></a>    Keyword args: 
<a name="l11844"><span class="ln">11844 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11845"><span class="ln">11845 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l11846"><span class="ln">11846 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l11847"><span class="ln">11847 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l11848"><span class="ln">11848 </span></a>            Default: ``torch.strided``. 
<a name="l11849"><span class="ln">11849 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l11850"><span class="ln">11850 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l11851"><span class="ln">11851 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l11852"><span class="ln">11852 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l11853"><span class="ln">11853 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l11854"><span class="ln">11854 </span></a>            returned tensor. Default: ``False``. 
<a name="l11855"><span class="ln">11855 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l11856"><span class="ln">11856 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l11857"><span class="ln">11857 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l11858"><span class="ln">11858 </span></a>            returned Tensor. Default: ``torch.contiguous_format``. 
<a name="l11859"><span class="ln">11859 </span></a> 
<a name="l11860"><span class="ln">11860 </span></a>    Example:: 
<a name="l11861"><span class="ln">11861 </span></a> 
<a name="l11862"><span class="ln">11862 </span></a>        &gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64) 
<a name="l11863"><span class="ln">11863 </span></a>        tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13], 
<a name="l11864"><span class="ln">11864 </span></a>                [ 7.5751e+18,  7.1428e+18,  7.5955e+18]]) 
<a name="l11865"><span class="ln">11865 </span></a>    &quot;&quot;&quot;</span>
<a name="l11866"><span class="ln">11866 </span></a>
<a name="l11867"><span class="ln">11867 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11868"><span class="ln">11868 </span></a><span class="s2">def </span><span class="s1">empty</span><span class="s3">(</span>
<a name="l11869"><span class="ln">11869 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l11870"><span class="ln">11870 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11871"><span class="ln">11871 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11872"><span class="ln">11872 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11873"><span class="ln">11873 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11874"><span class="ln">11874 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11875"><span class="ln">11875 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11876"><span class="ln">11876 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11877"><span class="ln">11877 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11878"><span class="ln">11878 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11879"><span class="ln">11879 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11880"><span class="ln">11880 </span></a>    empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l11881"><span class="ln">11881 </span></a> 
<a name="l11882"><span class="ln">11882 </span></a>    Returns a tensor filled with uninitialized data. The shape of the tensor is 
<a name="l11883"><span class="ln">11883 </span></a>    defined by the variable argument :attr:`size`. 
<a name="l11884"><span class="ln">11884 </span></a> 
<a name="l11885"><span class="ln">11885 </span></a>    .. note:: 
<a name="l11886"><span class="ln">11886 </span></a>        If :func:`torch.use_deterministic_algorithms()` and 
<a name="l11887"><span class="ln">11887 </span></a>        :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l11888"><span class="ln">11888 </span></a>        ``True``, the output tensor is initialized to prevent any possible 
<a name="l11889"><span class="ln">11889 </span></a>        nondeterministic behavior from using the data as an input to an operation. 
<a name="l11890"><span class="ln">11890 </span></a>        Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l11891"><span class="ln">11891 </span></a>        are filled with the maximum value. 
<a name="l11892"><span class="ln">11892 </span></a> 
<a name="l11893"><span class="ln">11893 </span></a>    Args: 
<a name="l11894"><span class="ln">11894 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l11895"><span class="ln">11895 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l11896"><span class="ln">11896 </span></a> 
<a name="l11897"><span class="ln">11897 </span></a>    Keyword args: 
<a name="l11898"><span class="ln">11898 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11899"><span class="ln">11899 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l11900"><span class="ln">11900 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l11901"><span class="ln">11901 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l11902"><span class="ln">11902 </span></a>            Default: ``torch.strided``. 
<a name="l11903"><span class="ln">11903 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l11904"><span class="ln">11904 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l11905"><span class="ln">11905 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l11906"><span class="ln">11906 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l11907"><span class="ln">11907 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l11908"><span class="ln">11908 </span></a>            returned tensor. Default: ``False``. 
<a name="l11909"><span class="ln">11909 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l11910"><span class="ln">11910 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l11911"><span class="ln">11911 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l11912"><span class="ln">11912 </span></a>            returned Tensor. Default: ``torch.contiguous_format``. 
<a name="l11913"><span class="ln">11913 </span></a> 
<a name="l11914"><span class="ln">11914 </span></a>    Example:: 
<a name="l11915"><span class="ln">11915 </span></a> 
<a name="l11916"><span class="ln">11916 </span></a>        &gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64) 
<a name="l11917"><span class="ln">11917 </span></a>        tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13], 
<a name="l11918"><span class="ln">11918 </span></a>                [ 7.5751e+18,  7.1428e+18,  7.5955e+18]]) 
<a name="l11919"><span class="ln">11919 </span></a>    &quot;&quot;&quot;</span>
<a name="l11920"><span class="ln">11920 </span></a>
<a name="l11921"><span class="ln">11921 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l11922"><span class="ln">11922 </span></a><span class="s2">def </span><span class="s1">empty</span><span class="s3">(</span>
<a name="l11923"><span class="ln">11923 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l11924"><span class="ln">11924 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l11925"><span class="ln">11925 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11926"><span class="ln">11926 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11927"><span class="ln">11927 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11928"><span class="ln">11928 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11929"><span class="ln">11929 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11930"><span class="ln">11930 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11931"><span class="ln">11931 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11932"><span class="ln">11932 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11933"><span class="ln">11933 </span></a>    empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) -&gt; Tensor 
<a name="l11934"><span class="ln">11934 </span></a> 
<a name="l11935"><span class="ln">11935 </span></a>    Returns a tensor filled with uninitialized data. The shape of the tensor is 
<a name="l11936"><span class="ln">11936 </span></a>    defined by the variable argument :attr:`size`. 
<a name="l11937"><span class="ln">11937 </span></a> 
<a name="l11938"><span class="ln">11938 </span></a>    .. note:: 
<a name="l11939"><span class="ln">11939 </span></a>        If :func:`torch.use_deterministic_algorithms()` and 
<a name="l11940"><span class="ln">11940 </span></a>        :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l11941"><span class="ln">11941 </span></a>        ``True``, the output tensor is initialized to prevent any possible 
<a name="l11942"><span class="ln">11942 </span></a>        nondeterministic behavior from using the data as an input to an operation. 
<a name="l11943"><span class="ln">11943 </span></a>        Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l11944"><span class="ln">11944 </span></a>        are filled with the maximum value. 
<a name="l11945"><span class="ln">11945 </span></a> 
<a name="l11946"><span class="ln">11946 </span></a>    Args: 
<a name="l11947"><span class="ln">11947 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l11948"><span class="ln">11948 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l11949"><span class="ln">11949 </span></a> 
<a name="l11950"><span class="ln">11950 </span></a>    Keyword args: 
<a name="l11951"><span class="ln">11951 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l11952"><span class="ln">11952 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l11953"><span class="ln">11953 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l11954"><span class="ln">11954 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l11955"><span class="ln">11955 </span></a>            Default: ``torch.strided``. 
<a name="l11956"><span class="ln">11956 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l11957"><span class="ln">11957 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l11958"><span class="ln">11958 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l11959"><span class="ln">11959 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l11960"><span class="ln">11960 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l11961"><span class="ln">11961 </span></a>            returned tensor. Default: ``False``. 
<a name="l11962"><span class="ln">11962 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l11963"><span class="ln">11963 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l11964"><span class="ln">11964 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l11965"><span class="ln">11965 </span></a>            returned Tensor. Default: ``torch.contiguous_format``. 
<a name="l11966"><span class="ln">11966 </span></a> 
<a name="l11967"><span class="ln">11967 </span></a>    Example:: 
<a name="l11968"><span class="ln">11968 </span></a> 
<a name="l11969"><span class="ln">11969 </span></a>        &gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64) 
<a name="l11970"><span class="ln">11970 </span></a>        tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13], 
<a name="l11971"><span class="ln">11971 </span></a>                [ 7.5751e+18,  7.1428e+18,  7.5955e+18]]) 
<a name="l11972"><span class="ln">11972 </span></a>    &quot;&quot;&quot;</span>
<a name="l11973"><span class="ln">11973 </span></a>
<a name="l11974"><span class="ln">11974 </span></a><span class="s2">def </span><span class="s1">empty_like</span><span class="s3">(</span>
<a name="l11975"><span class="ln">11975 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l11976"><span class="ln">11976 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l11977"><span class="ln">11977 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11978"><span class="ln">11978 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11979"><span class="ln">11979 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11980"><span class="ln">11980 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l11981"><span class="ln">11981 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11982"><span class="ln">11982 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l11983"><span class="ln">11983 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l11984"><span class="ln">11984 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l11985"><span class="ln">11985 </span></a>    empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l11986"><span class="ln">11986 </span></a> 
<a name="l11987"><span class="ln">11987 </span></a>    Returns an uninitialized tensor with the same size as :attr:`input`. 
<a name="l11988"><span class="ln">11988 </span></a>    ``torch.empty_like(input)`` is equivalent to 
<a name="l11989"><span class="ln">11989 </span></a>    ``torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l11990"><span class="ln">11990 </span></a> 
<a name="l11991"><span class="ln">11991 </span></a>    .. note:: 
<a name="l11992"><span class="ln">11992 </span></a>        If :func:`torch.use_deterministic_algorithms()` and 
<a name="l11993"><span class="ln">11993 </span></a>        :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l11994"><span class="ln">11994 </span></a>        ``True``, the output tensor is initialized to prevent any possible 
<a name="l11995"><span class="ln">11995 </span></a>        nondeterministic behavior from using the data as an input to an operation. 
<a name="l11996"><span class="ln">11996 </span></a>        Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l11997"><span class="ln">11997 </span></a>        are filled with the maximum value. 
<a name="l11998"><span class="ln">11998 </span></a> 
<a name="l11999"><span class="ln">11999 </span></a>    Args: 
<a name="l12000"><span class="ln">12000 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l12001"><span class="ln">12001 </span></a> 
<a name="l12002"><span class="ln">12002 </span></a>    Keyword args: 
<a name="l12003"><span class="ln">12003 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l12004"><span class="ln">12004 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l12005"><span class="ln">12005 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l12006"><span class="ln">12006 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l12007"><span class="ln">12007 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l12008"><span class="ln">12008 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l12009"><span class="ln">12009 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l12010"><span class="ln">12010 </span></a>            returned tensor. Default: ``False``. 
<a name="l12011"><span class="ln">12011 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l12012"><span class="ln">12012 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l12013"><span class="ln">12013 </span></a> 
<a name="l12014"><span class="ln">12014 </span></a>    Example:: 
<a name="l12015"><span class="ln">12015 </span></a> 
<a name="l12016"><span class="ln">12016 </span></a>        &gt;&gt;&gt; a=torch.empty((2,3), dtype=torch.int32, device = 'cuda') 
<a name="l12017"><span class="ln">12017 </span></a>        &gt;&gt;&gt; torch.empty_like(a) 
<a name="l12018"><span class="ln">12018 </span></a>        tensor([[0, 0, 0], 
<a name="l12019"><span class="ln">12019 </span></a>                [0, 0, 0]], device='cuda:0', dtype=torch.int32) 
<a name="l12020"><span class="ln">12020 </span></a>    &quot;&quot;&quot;</span>
<a name="l12021"><span class="ln">12021 </span></a>
<a name="l12022"><span class="ln">12022 </span></a><span class="s2">def </span><span class="s1">empty_permuted</span><span class="s3">(</span>
<a name="l12023"><span class="ln">12023 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l12024"><span class="ln">12024 </span></a>    <span class="s1">physical_layout</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l12025"><span class="ln">12025 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12026"><span class="ln">12026 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12027"><span class="ln">12027 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12028"><span class="ln">12028 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12029"><span class="ln">12029 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12030"><span class="ln">12030 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12031"><span class="ln">12031 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12032"><span class="ln">12032 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12033"><span class="ln">12033 </span></a>    empty_permuted(size, physical_layout, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l12034"><span class="ln">12034 </span></a> 
<a name="l12035"><span class="ln">12035 </span></a>    Creates an uninitialized, non-overlapping and dense tensor with the 
<a name="l12036"><span class="ln">12036 </span></a>    specified :attr:`size`, with :attr:`physical_layout` specifying how the 
<a name="l12037"><span class="ln">12037 </span></a>    dimensions are physically laid out in memory (each logical dimension is listed 
<a name="l12038"><span class="ln">12038 </span></a>    from outermost to innermost).  :attr:`physical_layout` is a generalization 
<a name="l12039"><span class="ln">12039 </span></a>    of NCHW/NHWC notation: if each dimension is assigned a number according to 
<a name="l12040"><span class="ln">12040 </span></a>    what order they occur in size (N=0, C=1, H=2, W=3), then NCHW is ``(0, 1, 2, 3)`` 
<a name="l12041"><span class="ln">12041 </span></a>    while NHWC is ``(0, 2, 3, 1)``.  Equivalently, the strides of the output 
<a name="l12042"><span class="ln">12042 </span></a>    tensor ``t`` are such that ``t.stride(physical_layout[i]) == contiguous_strides[i]`` 
<a name="l12043"><span class="ln">12043 </span></a>    (notably, this function is *not* equivalent to ``torch.empty(size).permute(physical_layout)``). 
<a name="l12044"><span class="ln">12044 </span></a> 
<a name="l12045"><span class="ln">12045 </span></a>    Unlike :func:`torch.empty_strided`, this is guaranteed to produce a dense 
<a name="l12046"><span class="ln">12046 </span></a>    tensor with no overlaps.  If possible, prefer using this function over 
<a name="l12047"><span class="ln">12047 </span></a>    :func:`torch.empty_strided` or manual use of :func:`torch.as_strided`. 
<a name="l12048"><span class="ln">12048 </span></a> 
<a name="l12049"><span class="ln">12049 </span></a>    .. note:: 
<a name="l12050"><span class="ln">12050 </span></a>        If :func:`torch.use_deterministic_algorithms()` and 
<a name="l12051"><span class="ln">12051 </span></a>        :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l12052"><span class="ln">12052 </span></a>        ``True``, the output tensor is initialized to prevent any possible 
<a name="l12053"><span class="ln">12053 </span></a>        nondeterministic behavior from using the data as an input to an operation. 
<a name="l12054"><span class="ln">12054 </span></a>        Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l12055"><span class="ln">12055 </span></a>        are filled with the maximum value. 
<a name="l12056"><span class="ln">12056 </span></a> 
<a name="l12057"><span class="ln">12057 </span></a>    Args: 
<a name="l12058"><span class="ln">12058 </span></a>        size (tuple of int): the shape of the output tensor 
<a name="l12059"><span class="ln">12059 </span></a>        physical_layout (tuple of int): the ordering of dimensions physically in memory 
<a name="l12060"><span class="ln">12060 </span></a> 
<a name="l12061"><span class="ln">12061 </span></a>    Keyword args: 
<a name="l12062"><span class="ln">12062 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l12063"><span class="ln">12063 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l12064"><span class="ln">12064 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l12065"><span class="ln">12065 </span></a>            Default: ``torch.strided``. 
<a name="l12066"><span class="ln">12066 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l12067"><span class="ln">12067 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l12068"><span class="ln">12068 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l12069"><span class="ln">12069 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l12070"><span class="ln">12070 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l12071"><span class="ln">12071 </span></a>            returned tensor. Default: ``False``. 
<a name="l12072"><span class="ln">12072 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l12073"><span class="ln">12073 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l12074"><span class="ln">12074 </span></a> 
<a name="l12075"><span class="ln">12075 </span></a>    Examples: 
<a name="l12076"><span class="ln">12076 </span></a> 
<a name="l12077"><span class="ln">12077 </span></a>        &gt;&gt;&gt; torch.empty((2, 3, 5, 7)).stride() 
<a name="l12078"><span class="ln">12078 </span></a>        (105, 35, 7, 1) 
<a name="l12079"><span class="ln">12079 </span></a>        &gt;&gt;&gt; torch.empty_permuted((2, 3, 5, 7), (0, 1, 2, 3)).stride() 
<a name="l12080"><span class="ln">12080 </span></a>        (105, 35, 7, 1) 
<a name="l12081"><span class="ln">12081 </span></a>        &gt;&gt;&gt; torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).stride() 
<a name="l12082"><span class="ln">12082 </span></a>        (105, 1, 21, 3) 
<a name="l12083"><span class="ln">12083 </span></a>        &gt;&gt;&gt; torch.empty_permuted((2, 3, 5, 7), (0, 2, 3, 1)).stride() 
<a name="l12084"><span class="ln">12084 </span></a>        (105, 1, 21, 3) 
<a name="l12085"><span class="ln">12085 </span></a>        &gt;&gt;&gt; torch.empty_permuted((2, 3, 5, 7), (0, 2, 3, 1)).dim_order() 
<a name="l12086"><span class="ln">12086 </span></a>        (0, 2, 3, 1) 
<a name="l12087"><span class="ln">12087 </span></a>    &quot;&quot;&quot;</span>
<a name="l12088"><span class="ln">12088 </span></a>
<a name="l12089"><span class="ln">12089 </span></a><span class="s2">def </span><span class="s1">empty_quantized</span><span class="s3">(</span>
<a name="l12090"><span class="ln">12090 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l12091"><span class="ln">12091 </span></a>    <span class="s1">qtensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12092"><span class="ln">12092 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12093"><span class="ln">12093 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12094"><span class="ln">12094 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12095"><span class="ln">12095 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12096"><span class="ln">12096 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12097"><span class="ln">12097 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12098"><span class="ln">12098 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12099"><span class="ln">12099 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12100"><span class="ln">12100 </span></a><span class="s2">def </span><span class="s1">empty_strided</span><span class="s3">(</span>
<a name="l12101"><span class="ln">12101 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l12102"><span class="ln">12102 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l12103"><span class="ln">12103 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12104"><span class="ln">12104 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12105"><span class="ln">12105 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12106"><span class="ln">12106 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12107"><span class="ln">12107 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12108"><span class="ln">12108 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12109"><span class="ln">12109 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12110"><span class="ln">12110 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12111"><span class="ln">12111 </span></a>    empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l12112"><span class="ln">12112 </span></a> 
<a name="l12113"><span class="ln">12113 </span></a>    Creates a tensor with the specified :attr:`size` and :attr:`stride` and filled with undefined data. 
<a name="l12114"><span class="ln">12114 </span></a> 
<a name="l12115"><span class="ln">12115 </span></a>    .. warning:: 
<a name="l12116"><span class="ln">12116 </span></a>        If the constructed tensor is &quot;overlapped&quot; (with multiple indices referring to the same element 
<a name="l12117"><span class="ln">12117 </span></a>        in memory) its behavior is undefined. 
<a name="l12118"><span class="ln">12118 </span></a> 
<a name="l12119"><span class="ln">12119 </span></a>    .. note:: 
<a name="l12120"><span class="ln">12120 </span></a>        If :func:`torch.use_deterministic_algorithms()` and 
<a name="l12121"><span class="ln">12121 </span></a>        :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to 
<a name="l12122"><span class="ln">12122 </span></a>        ``True``, the output tensor is initialized to prevent any possible 
<a name="l12123"><span class="ln">12123 </span></a>        nondeterministic behavior from using the data as an input to an operation. 
<a name="l12124"><span class="ln">12124 </span></a>        Floating point and complex tensors are filled with NaN, and integer tensors 
<a name="l12125"><span class="ln">12125 </span></a>        are filled with the maximum value. 
<a name="l12126"><span class="ln">12126 </span></a> 
<a name="l12127"><span class="ln">12127 </span></a>    Args: 
<a name="l12128"><span class="ln">12128 </span></a>        size (tuple of int): the shape of the output tensor 
<a name="l12129"><span class="ln">12129 </span></a>        stride (tuple of int): the strides of the output tensor 
<a name="l12130"><span class="ln">12130 </span></a> 
<a name="l12131"><span class="ln">12131 </span></a>    Keyword args: 
<a name="l12132"><span class="ln">12132 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l12133"><span class="ln">12133 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l12134"><span class="ln">12134 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l12135"><span class="ln">12135 </span></a>            Default: ``torch.strided``. 
<a name="l12136"><span class="ln">12136 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l12137"><span class="ln">12137 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l12138"><span class="ln">12138 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l12139"><span class="ln">12139 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l12140"><span class="ln">12140 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l12141"><span class="ln">12141 </span></a>            returned tensor. Default: ``False``. 
<a name="l12142"><span class="ln">12142 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l12143"><span class="ln">12143 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l12144"><span class="ln">12144 </span></a> 
<a name="l12145"><span class="ln">12145 </span></a>    Example:: 
<a name="l12146"><span class="ln">12146 </span></a> 
<a name="l12147"><span class="ln">12147 </span></a>        &gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2)) 
<a name="l12148"><span class="ln">12148 </span></a>        &gt;&gt;&gt; a 
<a name="l12149"><span class="ln">12149 </span></a>        tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07], 
<a name="l12150"><span class="ln">12150 </span></a>                [0.0000e+00, 0.0000e+00, 3.0705e-41]]) 
<a name="l12151"><span class="ln">12151 </span></a>        &gt;&gt;&gt; a.stride() 
<a name="l12152"><span class="ln">12152 </span></a>        (1, 2) 
<a name="l12153"><span class="ln">12153 </span></a>        &gt;&gt;&gt; a.size() 
<a name="l12154"><span class="ln">12154 </span></a>        torch.Size([2, 3]) 
<a name="l12155"><span class="ln">12155 </span></a>    &quot;&quot;&quot;</span>
<a name="l12156"><span class="ln">12156 </span></a>
<a name="l12157"><span class="ln">12157 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12158"><span class="ln">12158 </span></a><span class="s2">def </span><span class="s1">eq</span><span class="s3">(</span>
<a name="l12159"><span class="ln">12159 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12160"><span class="ln">12160 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12161"><span class="ln">12161 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12162"><span class="ln">12162 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12163"><span class="ln">12163 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12164"><span class="ln">12164 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12165"><span class="ln">12165 </span></a>    eq(input, other, *, out=None) -&gt; Tensor 
<a name="l12166"><span class="ln">12166 </span></a> 
<a name="l12167"><span class="ln">12167 </span></a>    Computes element-wise equality 
<a name="l12168"><span class="ln">12168 </span></a> 
<a name="l12169"><span class="ln">12169 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l12170"><span class="ln">12170 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l12171"><span class="ln">12171 </span></a> 
<a name="l12172"><span class="ln">12172 </span></a>    Args: 
<a name="l12173"><span class="ln">12173 </span></a>        input (Tensor): the tensor to compare 
<a name="l12174"><span class="ln">12174 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l12175"><span class="ln">12175 </span></a> 
<a name="l12176"><span class="ln">12176 </span></a>    Keyword args: 
<a name="l12177"><span class="ln">12177 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12178"><span class="ln">12178 </span></a> 
<a name="l12179"><span class="ln">12179 </span></a>    Returns: 
<a name="l12180"><span class="ln">12180 </span></a>        A boolean tensor that is True where :attr:`input` is equal to :attr:`other` and False elsewhere 
<a name="l12181"><span class="ln">12181 </span></a> 
<a name="l12182"><span class="ln">12182 </span></a>    Example:: 
<a name="l12183"><span class="ln">12183 </span></a> 
<a name="l12184"><span class="ln">12184 </span></a>        &gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l12185"><span class="ln">12185 </span></a>        tensor([[ True, False], 
<a name="l12186"><span class="ln">12186 </span></a>                [False, True]]) 
<a name="l12187"><span class="ln">12187 </span></a>    &quot;&quot;&quot;</span>
<a name="l12188"><span class="ln">12188 </span></a>
<a name="l12189"><span class="ln">12189 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12190"><span class="ln">12190 </span></a><span class="s2">def </span><span class="s1">eq</span><span class="s3">(</span>
<a name="l12191"><span class="ln">12191 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12192"><span class="ln">12192 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l12193"><span class="ln">12193 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12194"><span class="ln">12194 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12195"><span class="ln">12195 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12196"><span class="ln">12196 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12197"><span class="ln">12197 </span></a>    eq(input, other, *, out=None) -&gt; Tensor 
<a name="l12198"><span class="ln">12198 </span></a> 
<a name="l12199"><span class="ln">12199 </span></a>    Computes element-wise equality 
<a name="l12200"><span class="ln">12200 </span></a> 
<a name="l12201"><span class="ln">12201 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l12202"><span class="ln">12202 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l12203"><span class="ln">12203 </span></a> 
<a name="l12204"><span class="ln">12204 </span></a>    Args: 
<a name="l12205"><span class="ln">12205 </span></a>        input (Tensor): the tensor to compare 
<a name="l12206"><span class="ln">12206 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l12207"><span class="ln">12207 </span></a> 
<a name="l12208"><span class="ln">12208 </span></a>    Keyword args: 
<a name="l12209"><span class="ln">12209 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12210"><span class="ln">12210 </span></a> 
<a name="l12211"><span class="ln">12211 </span></a>    Returns: 
<a name="l12212"><span class="ln">12212 </span></a>        A boolean tensor that is True where :attr:`input` is equal to :attr:`other` and False elsewhere 
<a name="l12213"><span class="ln">12213 </span></a> 
<a name="l12214"><span class="ln">12214 </span></a>    Example:: 
<a name="l12215"><span class="ln">12215 </span></a> 
<a name="l12216"><span class="ln">12216 </span></a>        &gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l12217"><span class="ln">12217 </span></a>        tensor([[ True, False], 
<a name="l12218"><span class="ln">12218 </span></a>                [False, True]]) 
<a name="l12219"><span class="ln">12219 </span></a>    &quot;&quot;&quot;</span>
<a name="l12220"><span class="ln">12220 </span></a>
<a name="l12221"><span class="ln">12221 </span></a><span class="s2">def </span><span class="s1">equal</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l12222"><span class="ln">12222 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12223"><span class="ln">12223 </span></a>    equal(input, other) -&gt; bool 
<a name="l12224"><span class="ln">12224 </span></a> 
<a name="l12225"><span class="ln">12225 </span></a>    ``True`` if two tensors have the same size and elements, ``False`` otherwise. 
<a name="l12226"><span class="ln">12226 </span></a> 
<a name="l12227"><span class="ln">12227 </span></a>    .. note:: 
<a name="l12228"><span class="ln">12228 </span></a> 
<a name="l12229"><span class="ln">12229 </span></a>        Tensors containing NaNs are never equal to each other. Additionally, this function does not 
<a name="l12230"><span class="ln">12230 </span></a>        differentiate between the data types of the tensors during comparison. For more thorough tensor checks, 
<a name="l12231"><span class="ln">12231 </span></a>        use :meth:`torch.testing.assert_close`. 
<a name="l12232"><span class="ln">12232 </span></a> 
<a name="l12233"><span class="ln">12233 </span></a>    Example:: 
<a name="l12234"><span class="ln">12234 </span></a> 
<a name="l12235"><span class="ln">12235 </span></a>        &gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2])) 
<a name="l12236"><span class="ln">12236 </span></a>        True 
<a name="l12237"><span class="ln">12237 </span></a>        &gt;&gt;&gt; torch.equal(torch.tensor([3, torch.nan]), torch.tensor([3, torch.nan])) 
<a name="l12238"><span class="ln">12238 </span></a>        False 
<a name="l12239"><span class="ln">12239 </span></a>        &gt;&gt;&gt; torch.equal(torch.tensor([1, 2, 3], dtype=torch.int32), torch.tensor([1, 2, 3], dtype=torch.float32)) 
<a name="l12240"><span class="ln">12240 </span></a>        True 
<a name="l12241"><span class="ln">12241 </span></a>    &quot;&quot;&quot;</span>
<a name="l12242"><span class="ln">12242 </span></a>
<a name="l12243"><span class="ln">12243 </span></a><span class="s2">def </span><span class="s1">erf</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12244"><span class="ln">12244 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12245"><span class="ln">12245 </span></a>    erf(input, *, out=None) -&gt; Tensor 
<a name="l12246"><span class="ln">12246 </span></a> 
<a name="l12247"><span class="ln">12247 </span></a>    Alias for :func:`torch.special.erf`. 
<a name="l12248"><span class="ln">12248 </span></a>    &quot;&quot;&quot;</span>
<a name="l12249"><span class="ln">12249 </span></a>
<a name="l12250"><span class="ln">12250 </span></a><span class="s2">def </span><span class="s1">erf_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12251"><span class="ln">12251 </span></a><span class="s2">def </span><span class="s1">erfc</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12252"><span class="ln">12252 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12253"><span class="ln">12253 </span></a>    erfc(input, *, out=None) -&gt; Tensor 
<a name="l12254"><span class="ln">12254 </span></a> 
<a name="l12255"><span class="ln">12255 </span></a>    Alias for :func:`torch.special.erfc`. 
<a name="l12256"><span class="ln">12256 </span></a>    &quot;&quot;&quot;</span>
<a name="l12257"><span class="ln">12257 </span></a>
<a name="l12258"><span class="ln">12258 </span></a><span class="s2">def </span><span class="s1">erfc_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12259"><span class="ln">12259 </span></a><span class="s2">def </span><span class="s1">erfinv</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12260"><span class="ln">12260 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12261"><span class="ln">12261 </span></a>    erfinv(input, *, out=None) -&gt; Tensor 
<a name="l12262"><span class="ln">12262 </span></a> 
<a name="l12263"><span class="ln">12263 </span></a>    Alias for :func:`torch.special.erfinv`. 
<a name="l12264"><span class="ln">12264 </span></a>    &quot;&quot;&quot;</span>
<a name="l12265"><span class="ln">12265 </span></a>
<a name="l12266"><span class="ln">12266 </span></a><span class="s2">def </span><span class="s1">exp</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12267"><span class="ln">12267 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12268"><span class="ln">12268 </span></a>    exp(input, *, out=None) -&gt; Tensor 
<a name="l12269"><span class="ln">12269 </span></a> 
<a name="l12270"><span class="ln">12270 </span></a>    Returns a new tensor with the exponential of the elements 
<a name="l12271"><span class="ln">12271 </span></a>    of the input tensor :attr:`input`. 
<a name="l12272"><span class="ln">12272 </span></a> 
<a name="l12273"><span class="ln">12273 </span></a>    .. math:: 
<a name="l12274"><span class="ln">12274 </span></a>        y_{i} = e^{x_{i}} 
<a name="l12275"><span class="ln">12275 </span></a> 
<a name="l12276"><span class="ln">12276 </span></a>    Args: 
<a name="l12277"><span class="ln">12277 </span></a>        input (Tensor): the input tensor. 
<a name="l12278"><span class="ln">12278 </span></a> 
<a name="l12279"><span class="ln">12279 </span></a>    Keyword args: 
<a name="l12280"><span class="ln">12280 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12281"><span class="ln">12281 </span></a> 
<a name="l12282"><span class="ln">12282 </span></a>    Example:: 
<a name="l12283"><span class="ln">12283 </span></a> 
<a name="l12284"><span class="ln">12284 </span></a>        &gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)])) 
<a name="l12285"><span class="ln">12285 </span></a>        tensor([ 1.,  2.]) 
<a name="l12286"><span class="ln">12286 </span></a>    &quot;&quot;&quot;</span>
<a name="l12287"><span class="ln">12287 </span></a>
<a name="l12288"><span class="ln">12288 </span></a><span class="s2">def </span><span class="s1">exp2</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12289"><span class="ln">12289 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12290"><span class="ln">12290 </span></a>    exp2(input, *, out=None) -&gt; Tensor 
<a name="l12291"><span class="ln">12291 </span></a> 
<a name="l12292"><span class="ln">12292 </span></a>    Alias for :func:`torch.special.exp2`. 
<a name="l12293"><span class="ln">12293 </span></a>    &quot;&quot;&quot;</span>
<a name="l12294"><span class="ln">12294 </span></a>
<a name="l12295"><span class="ln">12295 </span></a><span class="s2">def </span><span class="s1">exp2_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12296"><span class="ln">12296 </span></a><span class="s2">def </span><span class="s1">exp_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12297"><span class="ln">12297 </span></a><span class="s2">def </span><span class="s1">expand_copy</span><span class="s3">(</span>
<a name="l12298"><span class="ln">12298 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12299"><span class="ln">12299 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l12300"><span class="ln">12300 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12301"><span class="ln">12301 </span></a>    <span class="s1">implicit</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l12302"><span class="ln">12302 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12303"><span class="ln">12303 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12304"><span class="ln">12304 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12305"><span class="ln">12305 </span></a>    Performs the same operation as :func:`torch.Tensor.expand`, but all output tensors 
<a name="l12306"><span class="ln">12306 </span></a>    are freshly created instead of aliasing the input. 
<a name="l12307"><span class="ln">12307 </span></a>    &quot;&quot;&quot;</span>
<a name="l12308"><span class="ln">12308 </span></a>
<a name="l12309"><span class="ln">12309 </span></a><span class="s2">def </span><span class="s1">expm1</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12310"><span class="ln">12310 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12311"><span class="ln">12311 </span></a>    expm1(input, *, out=None) -&gt; Tensor 
<a name="l12312"><span class="ln">12312 </span></a> 
<a name="l12313"><span class="ln">12313 </span></a>    Alias for :func:`torch.special.expm1`. 
<a name="l12314"><span class="ln">12314 </span></a>    &quot;&quot;&quot;</span>
<a name="l12315"><span class="ln">12315 </span></a>
<a name="l12316"><span class="ln">12316 </span></a><span class="s2">def </span><span class="s1">expm1_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12317"><span class="ln">12317 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12318"><span class="ln">12318 </span></a><span class="s2">def </span><span class="s1">eye</span><span class="s3">(</span>
<a name="l12319"><span class="ln">12319 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l12320"><span class="ln">12320 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12321"><span class="ln">12321 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12322"><span class="ln">12322 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12323"><span class="ln">12323 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12324"><span class="ln">12324 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12325"><span class="ln">12325 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12326"><span class="ln">12326 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12327"><span class="ln">12327 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12328"><span class="ln">12328 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12329"><span class="ln">12329 </span></a>    eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12330"><span class="ln">12330 </span></a> 
<a name="l12331"><span class="ln">12331 </span></a>    Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. 
<a name="l12332"><span class="ln">12332 </span></a> 
<a name="l12333"><span class="ln">12333 </span></a>    Args: 
<a name="l12334"><span class="ln">12334 </span></a>        n (int): the number of rows 
<a name="l12335"><span class="ln">12335 </span></a>        m (int, optional): the number of columns with default being :attr:`n` 
<a name="l12336"><span class="ln">12336 </span></a> 
<a name="l12337"><span class="ln">12337 </span></a>    Keyword arguments: 
<a name="l12338"><span class="ln">12338 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12339"><span class="ln">12339 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l12340"><span class="ln">12340 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l12341"><span class="ln">12341 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l12342"><span class="ln">12342 </span></a>            Default: ``torch.strided``. 
<a name="l12343"><span class="ln">12343 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l12344"><span class="ln">12344 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l12345"><span class="ln">12345 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l12346"><span class="ln">12346 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l12347"><span class="ln">12347 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l12348"><span class="ln">12348 </span></a>            returned tensor. Default: ``False``. 
<a name="l12349"><span class="ln">12349 </span></a> 
<a name="l12350"><span class="ln">12350 </span></a>    Returns: 
<a name="l12351"><span class="ln">12351 </span></a>        Tensor: A 2-D tensor with ones on the diagonal and zeros elsewhere 
<a name="l12352"><span class="ln">12352 </span></a> 
<a name="l12353"><span class="ln">12353 </span></a>    Example:: 
<a name="l12354"><span class="ln">12354 </span></a> 
<a name="l12355"><span class="ln">12355 </span></a>        &gt;&gt;&gt; torch.eye(3) 
<a name="l12356"><span class="ln">12356 </span></a>        tensor([[ 1.,  0.,  0.], 
<a name="l12357"><span class="ln">12357 </span></a>                [ 0.,  1.,  0.], 
<a name="l12358"><span class="ln">12358 </span></a>                [ 0.,  0.,  1.]]) 
<a name="l12359"><span class="ln">12359 </span></a>    &quot;&quot;&quot;</span>
<a name="l12360"><span class="ln">12360 </span></a>
<a name="l12361"><span class="ln">12361 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12362"><span class="ln">12362 </span></a><span class="s2">def </span><span class="s1">eye</span><span class="s3">(</span>
<a name="l12363"><span class="ln">12363 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l12364"><span class="ln">12364 </span></a>    <span class="s1">m</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l12365"><span class="ln">12365 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12366"><span class="ln">12366 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12367"><span class="ln">12367 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12368"><span class="ln">12368 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12369"><span class="ln">12369 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12370"><span class="ln">12370 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12371"><span class="ln">12371 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l12372"><span class="ln">12372 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12373"><span class="ln">12373 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12374"><span class="ln">12374 </span></a>    eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l12375"><span class="ln">12375 </span></a> 
<a name="l12376"><span class="ln">12376 </span></a>    Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. 
<a name="l12377"><span class="ln">12377 </span></a> 
<a name="l12378"><span class="ln">12378 </span></a>    Args: 
<a name="l12379"><span class="ln">12379 </span></a>        n (int): the number of rows 
<a name="l12380"><span class="ln">12380 </span></a>        m (int, optional): the number of columns with default being :attr:`n` 
<a name="l12381"><span class="ln">12381 </span></a> 
<a name="l12382"><span class="ln">12382 </span></a>    Keyword arguments: 
<a name="l12383"><span class="ln">12383 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12384"><span class="ln">12384 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l12385"><span class="ln">12385 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l12386"><span class="ln">12386 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l12387"><span class="ln">12387 </span></a>            Default: ``torch.strided``. 
<a name="l12388"><span class="ln">12388 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l12389"><span class="ln">12389 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l12390"><span class="ln">12390 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l12391"><span class="ln">12391 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l12392"><span class="ln">12392 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l12393"><span class="ln">12393 </span></a>            returned tensor. Default: ``False``. 
<a name="l12394"><span class="ln">12394 </span></a> 
<a name="l12395"><span class="ln">12395 </span></a>    Returns: 
<a name="l12396"><span class="ln">12396 </span></a>        Tensor: A 2-D tensor with ones on the diagonal and zeros elsewhere 
<a name="l12397"><span class="ln">12397 </span></a> 
<a name="l12398"><span class="ln">12398 </span></a>    Example:: 
<a name="l12399"><span class="ln">12399 </span></a> 
<a name="l12400"><span class="ln">12400 </span></a>        &gt;&gt;&gt; torch.eye(3) 
<a name="l12401"><span class="ln">12401 </span></a>        tensor([[ 1.,  0.,  0.], 
<a name="l12402"><span class="ln">12402 </span></a>                [ 0.,  1.,  0.], 
<a name="l12403"><span class="ln">12403 </span></a>                [ 0.,  0.,  1.]]) 
<a name="l12404"><span class="ln">12404 </span></a>    &quot;&quot;&quot;</span>
<a name="l12405"><span class="ln">12405 </span></a>
<a name="l12406"><span class="ln">12406 </span></a><span class="s2">def </span><span class="s1">fake_quantize_per_channel_affine</span><span class="s3">(</span>
<a name="l12407"><span class="ln">12407 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12408"><span class="ln">12408 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12409"><span class="ln">12409 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12410"><span class="ln">12410 </span></a>    <span class="s1">axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12411"><span class="ln">12411 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12412"><span class="ln">12412 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12413"><span class="ln">12413 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12414"><span class="ln">12414 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12415"><span class="ln">12415 </span></a>    fake_quantize_per_channel_affine(input, scale, zero_point, axis, quant_min, quant_max) -&gt; Tensor 
<a name="l12416"><span class="ln">12416 </span></a> 
<a name="l12417"><span class="ln">12417 </span></a>    Returns a new tensor with the data in :attr:`input` fake quantized per channel using :attr:`scale`, 
<a name="l12418"><span class="ln">12418 </span></a>    :attr:`zero_point`, :attr:`quant_min` and :attr:`quant_max`, across the channel specified by :attr:`axis`. 
<a name="l12419"><span class="ln">12419 </span></a> 
<a name="l12420"><span class="ln">12420 </span></a>    .. math:: 
<a name="l12421"><span class="ln">12421 </span></a>        \text{output} = ( 
<a name="l12422"><span class="ln">12422 </span></a>            min( 
<a name="l12423"><span class="ln">12423 </span></a>                \text{quant\_max}, 
<a name="l12424"><span class="ln">12424 </span></a>                max( 
<a name="l12425"><span class="ln">12425 </span></a>                    \text{quant\_min}, 
<a name="l12426"><span class="ln">12426 </span></a>                    \text{std::nearby\_int}(\text{input} / \text{scale}) + \text{zero\_point} 
<a name="l12427"><span class="ln">12427 </span></a>                ) 
<a name="l12428"><span class="ln">12428 </span></a>            ) - \text{zero\_point} 
<a name="l12429"><span class="ln">12429 </span></a>        ) \times \text{scale} 
<a name="l12430"><span class="ln">12430 </span></a> 
<a name="l12431"><span class="ln">12431 </span></a>    Args: 
<a name="l12432"><span class="ln">12432 </span></a>        input (Tensor): the input value(s), in ``torch.float32`` 
<a name="l12433"><span class="ln">12433 </span></a>        scale (Tensor): quantization scale, per channel in ``torch.float32`` 
<a name="l12434"><span class="ln">12434 </span></a>        zero_point (Tensor): quantization zero_point, per channel in ``torch.int32`` or ``torch.half`` or ``torch.float32`` 
<a name="l12435"><span class="ln">12435 </span></a>        axis (int32): channel axis 
<a name="l12436"><span class="ln">12436 </span></a>        quant_min (int64): lower bound of the quantized domain 
<a name="l12437"><span class="ln">12437 </span></a>        quant_max (int64): upper bound of the quantized domain 
<a name="l12438"><span class="ln">12438 </span></a> 
<a name="l12439"><span class="ln">12439 </span></a>    Returns: 
<a name="l12440"><span class="ln">12440 </span></a>        Tensor: A newly fake_quantized per channel ``torch.float32`` tensor 
<a name="l12441"><span class="ln">12441 </span></a> 
<a name="l12442"><span class="ln">12442 </span></a>    Example:: 
<a name="l12443"><span class="ln">12443 </span></a> 
<a name="l12444"><span class="ln">12444 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 2, 2) 
<a name="l12445"><span class="ln">12445 </span></a>        &gt;&gt;&gt; x 
<a name="l12446"><span class="ln">12446 </span></a>        tensor([[[-0.2525, -0.0466], 
<a name="l12447"><span class="ln">12447 </span></a>                 [ 0.3491, -0.2168]], 
<a name="l12448"><span class="ln">12448 </span></a> 
<a name="l12449"><span class="ln">12449 </span></a>                [[-0.5906,  1.6258], 
<a name="l12450"><span class="ln">12450 </span></a>                 [ 0.6444, -0.0542]]]) 
<a name="l12451"><span class="ln">12451 </span></a>        &gt;&gt;&gt; scales = (torch.randn(2) + 1) * 0.05 
<a name="l12452"><span class="ln">12452 </span></a>        &gt;&gt;&gt; scales 
<a name="l12453"><span class="ln">12453 </span></a>        tensor([0.0475, 0.0486]) 
<a name="l12454"><span class="ln">12454 </span></a>        &gt;&gt;&gt; zero_points = torch.zeros(2).to(torch.int32) 
<a name="l12455"><span class="ln">12455 </span></a>        &gt;&gt;&gt; zero_points 
<a name="l12456"><span class="ln">12456 </span></a>        tensor([0, 0]) 
<a name="l12457"><span class="ln">12457 </span></a>        &gt;&gt;&gt; torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255) 
<a name="l12458"><span class="ln">12458 </span></a>        tensor([[[0.0000, 0.0000], 
<a name="l12459"><span class="ln">12459 </span></a>                 [0.3405, 0.0000]], 
<a name="l12460"><span class="ln">12460 </span></a> 
<a name="l12461"><span class="ln">12461 </span></a>                [[0.0000, 1.6134], 
<a name="l12462"><span class="ln">12462 </span></a>                [0.6323, 0.0000]]]) 
<a name="l12463"><span class="ln">12463 </span></a>    &quot;&quot;&quot;</span>
<a name="l12464"><span class="ln">12464 </span></a>
<a name="l12465"><span class="ln">12465 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12466"><span class="ln">12466 </span></a><span class="s2">def </span><span class="s1">fake_quantize_per_tensor_affine</span><span class="s3">(</span>
<a name="l12467"><span class="ln">12467 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12468"><span class="ln">12468 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l12469"><span class="ln">12469 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12470"><span class="ln">12470 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12471"><span class="ln">12471 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12472"><span class="ln">12472 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12473"><span class="ln">12473 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12474"><span class="ln">12474 </span></a>    fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max) -&gt; Tensor 
<a name="l12475"><span class="ln">12475 </span></a> 
<a name="l12476"><span class="ln">12476 </span></a>    Returns a new tensor with the data in :attr:`input` fake quantized using :attr:`scale`, 
<a name="l12477"><span class="ln">12477 </span></a>    :attr:`zero_point`, :attr:`quant_min` and :attr:`quant_max`. 
<a name="l12478"><span class="ln">12478 </span></a> 
<a name="l12479"><span class="ln">12479 </span></a>    .. math:: 
<a name="l12480"><span class="ln">12480 </span></a>        \text{output} = ( 
<a name="l12481"><span class="ln">12481 </span></a>            min( 
<a name="l12482"><span class="ln">12482 </span></a>                \text{quant\_max}, 
<a name="l12483"><span class="ln">12483 </span></a>                max( 
<a name="l12484"><span class="ln">12484 </span></a>                    \text{quant\_min}, 
<a name="l12485"><span class="ln">12485 </span></a>                    \text{std::nearby\_int}(\text{input} / \text{scale}) + \text{zero\_point} 
<a name="l12486"><span class="ln">12486 </span></a>                ) 
<a name="l12487"><span class="ln">12487 </span></a>            ) - \text{zero\_point} 
<a name="l12488"><span class="ln">12488 </span></a>        ) \times \text{scale} 
<a name="l12489"><span class="ln">12489 </span></a> 
<a name="l12490"><span class="ln">12490 </span></a>    Args: 
<a name="l12491"><span class="ln">12491 </span></a>        input (Tensor): the input value(s), ``torch.float32`` tensor 
<a name="l12492"><span class="ln">12492 </span></a>        scale (double scalar or ``float32`` Tensor): quantization scale 
<a name="l12493"><span class="ln">12493 </span></a>        zero_point (int64 scalar or ``int32`` Tensor): quantization zero_point 
<a name="l12494"><span class="ln">12494 </span></a>        quant_min (int64): lower bound of the quantized domain 
<a name="l12495"><span class="ln">12495 </span></a>        quant_max (int64): upper bound of the quantized domain 
<a name="l12496"><span class="ln">12496 </span></a> 
<a name="l12497"><span class="ln">12497 </span></a>    Returns: 
<a name="l12498"><span class="ln">12498 </span></a>        Tensor: A newly fake_quantized ``torch.float32`` tensor 
<a name="l12499"><span class="ln">12499 </span></a> 
<a name="l12500"><span class="ln">12500 </span></a>    Example:: 
<a name="l12501"><span class="ln">12501 </span></a> 
<a name="l12502"><span class="ln">12502 </span></a>        &gt;&gt;&gt; x = torch.randn(4) 
<a name="l12503"><span class="ln">12503 </span></a>        &gt;&gt;&gt; x 
<a name="l12504"><span class="ln">12504 </span></a>        tensor([ 0.0552,  0.9730,  0.3973, -1.0780]) 
<a name="l12505"><span class="ln">12505 </span></a>        &gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255) 
<a name="l12506"><span class="ln">12506 </span></a>        tensor([0.1000, 1.0000, 0.4000, 0.0000]) 
<a name="l12507"><span class="ln">12507 </span></a>        &gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, torch.tensor(0.1), torch.tensor(0), 0, 255) 
<a name="l12508"><span class="ln">12508 </span></a>        tensor([0.1000, 1.0000, 0.4000, 0.0000]) 
<a name="l12509"><span class="ln">12509 </span></a>    &quot;&quot;&quot;</span>
<a name="l12510"><span class="ln">12510 </span></a>
<a name="l12511"><span class="ln">12511 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12512"><span class="ln">12512 </span></a><span class="s2">def </span><span class="s1">fake_quantize_per_tensor_affine</span><span class="s3">(</span>
<a name="l12513"><span class="ln">12513 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12514"><span class="ln">12514 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12515"><span class="ln">12515 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12516"><span class="ln">12516 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12517"><span class="ln">12517 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12518"><span class="ln">12518 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12519"><span class="ln">12519 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12520"><span class="ln">12520 </span></a>    fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max) -&gt; Tensor 
<a name="l12521"><span class="ln">12521 </span></a> 
<a name="l12522"><span class="ln">12522 </span></a>    Returns a new tensor with the data in :attr:`input` fake quantized using :attr:`scale`, 
<a name="l12523"><span class="ln">12523 </span></a>    :attr:`zero_point`, :attr:`quant_min` and :attr:`quant_max`. 
<a name="l12524"><span class="ln">12524 </span></a> 
<a name="l12525"><span class="ln">12525 </span></a>    .. math:: 
<a name="l12526"><span class="ln">12526 </span></a>        \text{output} = ( 
<a name="l12527"><span class="ln">12527 </span></a>            min( 
<a name="l12528"><span class="ln">12528 </span></a>                \text{quant\_max}, 
<a name="l12529"><span class="ln">12529 </span></a>                max( 
<a name="l12530"><span class="ln">12530 </span></a>                    \text{quant\_min}, 
<a name="l12531"><span class="ln">12531 </span></a>                    \text{std::nearby\_int}(\text{input} / \text{scale}) + \text{zero\_point} 
<a name="l12532"><span class="ln">12532 </span></a>                ) 
<a name="l12533"><span class="ln">12533 </span></a>            ) - \text{zero\_point} 
<a name="l12534"><span class="ln">12534 </span></a>        ) \times \text{scale} 
<a name="l12535"><span class="ln">12535 </span></a> 
<a name="l12536"><span class="ln">12536 </span></a>    Args: 
<a name="l12537"><span class="ln">12537 </span></a>        input (Tensor): the input value(s), ``torch.float32`` tensor 
<a name="l12538"><span class="ln">12538 </span></a>        scale (double scalar or ``float32`` Tensor): quantization scale 
<a name="l12539"><span class="ln">12539 </span></a>        zero_point (int64 scalar or ``int32`` Tensor): quantization zero_point 
<a name="l12540"><span class="ln">12540 </span></a>        quant_min (int64): lower bound of the quantized domain 
<a name="l12541"><span class="ln">12541 </span></a>        quant_max (int64): upper bound of the quantized domain 
<a name="l12542"><span class="ln">12542 </span></a> 
<a name="l12543"><span class="ln">12543 </span></a>    Returns: 
<a name="l12544"><span class="ln">12544 </span></a>        Tensor: A newly fake_quantized ``torch.float32`` tensor 
<a name="l12545"><span class="ln">12545 </span></a> 
<a name="l12546"><span class="ln">12546 </span></a>    Example:: 
<a name="l12547"><span class="ln">12547 </span></a> 
<a name="l12548"><span class="ln">12548 </span></a>        &gt;&gt;&gt; x = torch.randn(4) 
<a name="l12549"><span class="ln">12549 </span></a>        &gt;&gt;&gt; x 
<a name="l12550"><span class="ln">12550 </span></a>        tensor([ 0.0552,  0.9730,  0.3973, -1.0780]) 
<a name="l12551"><span class="ln">12551 </span></a>        &gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255) 
<a name="l12552"><span class="ln">12552 </span></a>        tensor([0.1000, 1.0000, 0.4000, 0.0000]) 
<a name="l12553"><span class="ln">12553 </span></a>        &gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, torch.tensor(0.1), torch.tensor(0), 0, 255) 
<a name="l12554"><span class="ln">12554 </span></a>        tensor([0.1000, 1.0000, 0.4000, 0.0000]) 
<a name="l12555"><span class="ln">12555 </span></a>    &quot;&quot;&quot;</span>
<a name="l12556"><span class="ln">12556 </span></a>
<a name="l12557"><span class="ln">12557 </span></a><span class="s2">def </span><span class="s1">fbgemm_linear_fp16_weight</span><span class="s3">(</span>
<a name="l12558"><span class="ln">12558 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12559"><span class="ln">12559 </span></a>    <span class="s1">packed_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12560"><span class="ln">12560 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12561"><span class="ln">12561 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12562"><span class="ln">12562 </span></a><span class="s2">def </span><span class="s1">fbgemm_linear_fp16_weight_fp32_activation</span><span class="s3">(</span>
<a name="l12563"><span class="ln">12563 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12564"><span class="ln">12564 </span></a>    <span class="s1">packed_weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12565"><span class="ln">12565 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12566"><span class="ln">12566 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12567"><span class="ln">12567 </span></a><span class="s2">def </span><span class="s1">fbgemm_linear_int8_weight</span><span class="s3">(</span>
<a name="l12568"><span class="ln">12568 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12569"><span class="ln">12569 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12570"><span class="ln">12570 </span></a>    <span class="s1">packed</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12571"><span class="ln">12571 </span></a>    <span class="s1">col_offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12572"><span class="ln">12572 </span></a>    <span class="s1">weight_scale</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l12573"><span class="ln">12573 </span></a>    <span class="s1">weight_zero_point</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l12574"><span class="ln">12574 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12575"><span class="ln">12575 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12576"><span class="ln">12576 </span></a><span class="s2">def </span><span class="s1">fbgemm_linear_int8_weight_fp32_activation</span><span class="s3">(</span>
<a name="l12577"><span class="ln">12577 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12578"><span class="ln">12578 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12579"><span class="ln">12579 </span></a>    <span class="s1">packed</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12580"><span class="ln">12580 </span></a>    <span class="s1">col_offsets</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12581"><span class="ln">12581 </span></a>    <span class="s1">weight_scale</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l12582"><span class="ln">12582 </span></a>    <span class="s1">weight_zero_point</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l12583"><span class="ln">12583 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12584"><span class="ln">12584 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12585"><span class="ln">12585 </span></a><span class="s2">def </span><span class="s1">fbgemm_linear_quantize_weight</span><span class="s3">(</span>
<a name="l12586"><span class="ln">12586 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12587"><span class="ln">12587 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">_int</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12588"><span class="ln">12588 </span></a><span class="s2">def </span><span class="s1">fbgemm_pack_gemm_matrix_fp16</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12589"><span class="ln">12589 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12590"><span class="ln">12590 </span></a><span class="s2">def </span><span class="s1">fbgemm_pack_quantized_matrix</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12591"><span class="ln">12591 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12592"><span class="ln">12592 </span></a><span class="s2">def </span><span class="s1">fbgemm_pack_quantized_matrix</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">K</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">N</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12593"><span class="ln">12593 </span></a><span class="s2">def </span><span class="s1">feature_alpha_dropout</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12594"><span class="ln">12594 </span></a><span class="s2">def </span><span class="s1">feature_alpha_dropout_</span><span class="s3">(</span>
<a name="l12595"><span class="ln">12595 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12596"><span class="ln">12596 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l12597"><span class="ln">12597 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l12598"><span class="ln">12598 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12599"><span class="ln">12599 </span></a><span class="s2">def </span><span class="s1">feature_dropout</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12600"><span class="ln">12600 </span></a><span class="s2">def </span><span class="s1">feature_dropout_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">, </span><span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12601"><span class="ln">12601 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12602"><span class="ln">12602 </span></a><span class="s2">def </span><span class="s1">fill</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12603"><span class="ln">12603 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12604"><span class="ln">12604 </span></a><span class="s2">def </span><span class="s1">fill</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12605"><span class="ln">12605 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12606"><span class="ln">12606 </span></a><span class="s2">def </span><span class="s1">fill_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12607"><span class="ln">12607 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12608"><span class="ln">12608 </span></a><span class="s2">def </span><span class="s1">fill_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12609"><span class="ln">12609 </span></a><span class="s2">def </span><span class="s1">fix</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12610"><span class="ln">12610 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12611"><span class="ln">12611 </span></a>    fix(input, *, out=None) -&gt; Tensor 
<a name="l12612"><span class="ln">12612 </span></a> 
<a name="l12613"><span class="ln">12613 </span></a>    Alias for :func:`torch.trunc` 
<a name="l12614"><span class="ln">12614 </span></a>    &quot;&quot;&quot;</span>
<a name="l12615"><span class="ln">12615 </span></a>
<a name="l12616"><span class="ln">12616 </span></a><span class="s2">def </span><span class="s1">fix_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l12617"><span class="ln">12617 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12618"><span class="ln">12618 </span></a><span class="s2">def </span><span class="s1">flatten</span><span class="s3">(</span>
<a name="l12619"><span class="ln">12619 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12620"><span class="ln">12620 </span></a>    <span class="s1">start_dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l12621"><span class="ln">12621 </span></a>    <span class="s1">end_dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l12622"><span class="ln">12622 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12623"><span class="ln">12623 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12624"><span class="ln">12624 </span></a>    flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor 
<a name="l12625"><span class="ln">12625 </span></a> 
<a name="l12626"><span class="ln">12626 </span></a>    Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim` 
<a name="l12627"><span class="ln">12627 </span></a>    are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened. 
<a name="l12628"><span class="ln">12628 </span></a>    The order of elements in :attr:`input` is unchanged. 
<a name="l12629"><span class="ln">12629 </span></a> 
<a name="l12630"><span class="ln">12630 </span></a>    Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view, 
<a name="l12631"><span class="ln">12631 </span></a>    or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can 
<a name="l12632"><span class="ln">12632 </span></a>    be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the 
<a name="l12633"><span class="ln">12633 </span></a>    flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned. 
<a name="l12634"><span class="ln">12634 </span></a> 
<a name="l12635"><span class="ln">12635 </span></a>    .. note:: 
<a name="l12636"><span class="ln">12636 </span></a>        Flattening a zero-dimensional tensor will return a one-dimensional view. 
<a name="l12637"><span class="ln">12637 </span></a> 
<a name="l12638"><span class="ln">12638 </span></a>    Args: 
<a name="l12639"><span class="ln">12639 </span></a>        input (Tensor): the input tensor. 
<a name="l12640"><span class="ln">12640 </span></a>        start_dim (int): the first dim to flatten 
<a name="l12641"><span class="ln">12641 </span></a>        end_dim (int): the last dim to flatten 
<a name="l12642"><span class="ln">12642 </span></a> 
<a name="l12643"><span class="ln">12643 </span></a>    Example:: 
<a name="l12644"><span class="ln">12644 </span></a> 
<a name="l12645"><span class="ln">12645 </span></a>        &gt;&gt;&gt; t = torch.tensor([[[1, 2], 
<a name="l12646"><span class="ln">12646 </span></a>        ...                    [3, 4]], 
<a name="l12647"><span class="ln">12647 </span></a>        ...                   [[5, 6], 
<a name="l12648"><span class="ln">12648 </span></a>        ...                    [7, 8]]]) 
<a name="l12649"><span class="ln">12649 </span></a>        &gt;&gt;&gt; torch.flatten(t) 
<a name="l12650"><span class="ln">12650 </span></a>        tensor([1, 2, 3, 4, 5, 6, 7, 8]) 
<a name="l12651"><span class="ln">12651 </span></a>        &gt;&gt;&gt; torch.flatten(t, start_dim=1) 
<a name="l12652"><span class="ln">12652 </span></a>        tensor([[1, 2, 3, 4], 
<a name="l12653"><span class="ln">12653 </span></a>                [5, 6, 7, 8]]) 
<a name="l12654"><span class="ln">12654 </span></a>    &quot;&quot;&quot;</span>
<a name="l12655"><span class="ln">12655 </span></a>
<a name="l12656"><span class="ln">12656 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12657"><span class="ln">12657 </span></a><span class="s2">def </span><span class="s1">flatten</span><span class="s3">(</span>
<a name="l12658"><span class="ln">12658 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12659"><span class="ln">12659 </span></a>    <span class="s1">start_dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12660"><span class="ln">12660 </span></a>    <span class="s1">end_dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l12661"><span class="ln">12661 </span></a>    <span class="s1">out_dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l12662"><span class="ln">12662 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12663"><span class="ln">12663 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12664"><span class="ln">12664 </span></a>    flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor 
<a name="l12665"><span class="ln">12665 </span></a> 
<a name="l12666"><span class="ln">12666 </span></a>    Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim` 
<a name="l12667"><span class="ln">12667 </span></a>    are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened. 
<a name="l12668"><span class="ln">12668 </span></a>    The order of elements in :attr:`input` is unchanged. 
<a name="l12669"><span class="ln">12669 </span></a> 
<a name="l12670"><span class="ln">12670 </span></a>    Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view, 
<a name="l12671"><span class="ln">12671 </span></a>    or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can 
<a name="l12672"><span class="ln">12672 </span></a>    be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the 
<a name="l12673"><span class="ln">12673 </span></a>    flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned. 
<a name="l12674"><span class="ln">12674 </span></a> 
<a name="l12675"><span class="ln">12675 </span></a>    .. note:: 
<a name="l12676"><span class="ln">12676 </span></a>        Flattening a zero-dimensional tensor will return a one-dimensional view. 
<a name="l12677"><span class="ln">12677 </span></a> 
<a name="l12678"><span class="ln">12678 </span></a>    Args: 
<a name="l12679"><span class="ln">12679 </span></a>        input (Tensor): the input tensor. 
<a name="l12680"><span class="ln">12680 </span></a>        start_dim (int): the first dim to flatten 
<a name="l12681"><span class="ln">12681 </span></a>        end_dim (int): the last dim to flatten 
<a name="l12682"><span class="ln">12682 </span></a> 
<a name="l12683"><span class="ln">12683 </span></a>    Example:: 
<a name="l12684"><span class="ln">12684 </span></a> 
<a name="l12685"><span class="ln">12685 </span></a>        &gt;&gt;&gt; t = torch.tensor([[[1, 2], 
<a name="l12686"><span class="ln">12686 </span></a>        ...                    [3, 4]], 
<a name="l12687"><span class="ln">12687 </span></a>        ...                   [[5, 6], 
<a name="l12688"><span class="ln">12688 </span></a>        ...                    [7, 8]]]) 
<a name="l12689"><span class="ln">12689 </span></a>        &gt;&gt;&gt; torch.flatten(t) 
<a name="l12690"><span class="ln">12690 </span></a>        tensor([1, 2, 3, 4, 5, 6, 7, 8]) 
<a name="l12691"><span class="ln">12691 </span></a>        &gt;&gt;&gt; torch.flatten(t, start_dim=1) 
<a name="l12692"><span class="ln">12692 </span></a>        tensor([[1, 2, 3, 4], 
<a name="l12693"><span class="ln">12693 </span></a>                [5, 6, 7, 8]]) 
<a name="l12694"><span class="ln">12694 </span></a>    &quot;&quot;&quot;</span>
<a name="l12695"><span class="ln">12695 </span></a>
<a name="l12696"><span class="ln">12696 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12697"><span class="ln">12697 </span></a><span class="s2">def </span><span class="s1">flatten</span><span class="s3">(</span>
<a name="l12698"><span class="ln">12698 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12699"><span class="ln">12699 </span></a>    <span class="s1">start_dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l12700"><span class="ln">12700 </span></a>    <span class="s1">end_dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l12701"><span class="ln">12701 </span></a>    <span class="s1">out_dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l12702"><span class="ln">12702 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12703"><span class="ln">12703 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12704"><span class="ln">12704 </span></a>    flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor 
<a name="l12705"><span class="ln">12705 </span></a> 
<a name="l12706"><span class="ln">12706 </span></a>    Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim` 
<a name="l12707"><span class="ln">12707 </span></a>    are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened. 
<a name="l12708"><span class="ln">12708 </span></a>    The order of elements in :attr:`input` is unchanged. 
<a name="l12709"><span class="ln">12709 </span></a> 
<a name="l12710"><span class="ln">12710 </span></a>    Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view, 
<a name="l12711"><span class="ln">12711 </span></a>    or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can 
<a name="l12712"><span class="ln">12712 </span></a>    be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the 
<a name="l12713"><span class="ln">12713 </span></a>    flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned. 
<a name="l12714"><span class="ln">12714 </span></a> 
<a name="l12715"><span class="ln">12715 </span></a>    .. note:: 
<a name="l12716"><span class="ln">12716 </span></a>        Flattening a zero-dimensional tensor will return a one-dimensional view. 
<a name="l12717"><span class="ln">12717 </span></a> 
<a name="l12718"><span class="ln">12718 </span></a>    Args: 
<a name="l12719"><span class="ln">12719 </span></a>        input (Tensor): the input tensor. 
<a name="l12720"><span class="ln">12720 </span></a>        start_dim (int): the first dim to flatten 
<a name="l12721"><span class="ln">12721 </span></a>        end_dim (int): the last dim to flatten 
<a name="l12722"><span class="ln">12722 </span></a> 
<a name="l12723"><span class="ln">12723 </span></a>    Example:: 
<a name="l12724"><span class="ln">12724 </span></a> 
<a name="l12725"><span class="ln">12725 </span></a>        &gt;&gt;&gt; t = torch.tensor([[[1, 2], 
<a name="l12726"><span class="ln">12726 </span></a>        ...                    [3, 4]], 
<a name="l12727"><span class="ln">12727 </span></a>        ...                   [[5, 6], 
<a name="l12728"><span class="ln">12728 </span></a>        ...                    [7, 8]]]) 
<a name="l12729"><span class="ln">12729 </span></a>        &gt;&gt;&gt; torch.flatten(t) 
<a name="l12730"><span class="ln">12730 </span></a>        tensor([1, 2, 3, 4, 5, 6, 7, 8]) 
<a name="l12731"><span class="ln">12731 </span></a>        &gt;&gt;&gt; torch.flatten(t, start_dim=1) 
<a name="l12732"><span class="ln">12732 </span></a>        tensor([[1, 2, 3, 4], 
<a name="l12733"><span class="ln">12733 </span></a>                [5, 6, 7, 8]]) 
<a name="l12734"><span class="ln">12734 </span></a>    &quot;&quot;&quot;</span>
<a name="l12735"><span class="ln">12735 </span></a>
<a name="l12736"><span class="ln">12736 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12737"><span class="ln">12737 </span></a><span class="s2">def </span><span class="s1">flatten</span><span class="s3">(</span>
<a name="l12738"><span class="ln">12738 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12739"><span class="ln">12739 </span></a>    <span class="s1">dims</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l12740"><span class="ln">12740 </span></a>    <span class="s1">out_dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l12741"><span class="ln">12741 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12742"><span class="ln">12742 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12743"><span class="ln">12743 </span></a>    flatten(input, start_dim=0, end_dim=-1) -&gt; Tensor 
<a name="l12744"><span class="ln">12744 </span></a> 
<a name="l12745"><span class="ln">12745 </span></a>    Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim` 
<a name="l12746"><span class="ln">12746 </span></a>    are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened. 
<a name="l12747"><span class="ln">12747 </span></a>    The order of elements in :attr:`input` is unchanged. 
<a name="l12748"><span class="ln">12748 </span></a> 
<a name="l12749"><span class="ln">12749 </span></a>    Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view, 
<a name="l12750"><span class="ln">12750 </span></a>    or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can 
<a name="l12751"><span class="ln">12751 </span></a>    be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the 
<a name="l12752"><span class="ln">12752 </span></a>    flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned. 
<a name="l12753"><span class="ln">12753 </span></a> 
<a name="l12754"><span class="ln">12754 </span></a>    .. note:: 
<a name="l12755"><span class="ln">12755 </span></a>        Flattening a zero-dimensional tensor will return a one-dimensional view. 
<a name="l12756"><span class="ln">12756 </span></a> 
<a name="l12757"><span class="ln">12757 </span></a>    Args: 
<a name="l12758"><span class="ln">12758 </span></a>        input (Tensor): the input tensor. 
<a name="l12759"><span class="ln">12759 </span></a>        start_dim (int): the first dim to flatten 
<a name="l12760"><span class="ln">12760 </span></a>        end_dim (int): the last dim to flatten 
<a name="l12761"><span class="ln">12761 </span></a> 
<a name="l12762"><span class="ln">12762 </span></a>    Example:: 
<a name="l12763"><span class="ln">12763 </span></a> 
<a name="l12764"><span class="ln">12764 </span></a>        &gt;&gt;&gt; t = torch.tensor([[[1, 2], 
<a name="l12765"><span class="ln">12765 </span></a>        ...                    [3, 4]], 
<a name="l12766"><span class="ln">12766 </span></a>        ...                   [[5, 6], 
<a name="l12767"><span class="ln">12767 </span></a>        ...                    [7, 8]]]) 
<a name="l12768"><span class="ln">12768 </span></a>        &gt;&gt;&gt; torch.flatten(t) 
<a name="l12769"><span class="ln">12769 </span></a>        tensor([1, 2, 3, 4, 5, 6, 7, 8]) 
<a name="l12770"><span class="ln">12770 </span></a>        &gt;&gt;&gt; torch.flatten(t, start_dim=1) 
<a name="l12771"><span class="ln">12771 </span></a>        tensor([[1, 2, 3, 4], 
<a name="l12772"><span class="ln">12772 </span></a>                [5, 6, 7, 8]]) 
<a name="l12773"><span class="ln">12773 </span></a>    &quot;&quot;&quot;</span>
<a name="l12774"><span class="ln">12774 </span></a>
<a name="l12775"><span class="ln">12775 </span></a><span class="s2">def </span><span class="s1">flip</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dims</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12776"><span class="ln">12776 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12777"><span class="ln">12777 </span></a>    flip(input, dims) -&gt; Tensor 
<a name="l12778"><span class="ln">12778 </span></a> 
<a name="l12779"><span class="ln">12779 </span></a>    Reverse the order of an n-D tensor along given axis in dims. 
<a name="l12780"><span class="ln">12780 </span></a> 
<a name="l12781"><span class="ln">12781 </span></a>    .. note:: 
<a name="l12782"><span class="ln">12782 </span></a>        `torch.flip` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.flip`, 
<a name="l12783"><span class="ln">12783 </span></a>        which returns a view in constant time. Since copying a tensor's data is more work than viewing that data, 
<a name="l12784"><span class="ln">12784 </span></a>        `torch.flip` is expected to be slower than `np.flip`. 
<a name="l12785"><span class="ln">12785 </span></a> 
<a name="l12786"><span class="ln">12786 </span></a>    Args: 
<a name="l12787"><span class="ln">12787 </span></a>        input (Tensor): the input tensor. 
<a name="l12788"><span class="ln">12788 </span></a>        dims (a list or tuple): axis to flip on 
<a name="l12789"><span class="ln">12789 </span></a> 
<a name="l12790"><span class="ln">12790 </span></a>    Example:: 
<a name="l12791"><span class="ln">12791 </span></a> 
<a name="l12792"><span class="ln">12792 </span></a>        &gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2) 
<a name="l12793"><span class="ln">12793 </span></a>        &gt;&gt;&gt; x 
<a name="l12794"><span class="ln">12794 </span></a>        tensor([[[ 0,  1], 
<a name="l12795"><span class="ln">12795 </span></a>                 [ 2,  3]], 
<a name="l12796"><span class="ln">12796 </span></a> 
<a name="l12797"><span class="ln">12797 </span></a>                [[ 4,  5], 
<a name="l12798"><span class="ln">12798 </span></a>                 [ 6,  7]]]) 
<a name="l12799"><span class="ln">12799 </span></a>        &gt;&gt;&gt; torch.flip(x, [0, 1]) 
<a name="l12800"><span class="ln">12800 </span></a>        tensor([[[ 6,  7], 
<a name="l12801"><span class="ln">12801 </span></a>                 [ 4,  5]], 
<a name="l12802"><span class="ln">12802 </span></a> 
<a name="l12803"><span class="ln">12803 </span></a>                [[ 2,  3], 
<a name="l12804"><span class="ln">12804 </span></a>                 [ 0,  1]]]) 
<a name="l12805"><span class="ln">12805 </span></a>    &quot;&quot;&quot;</span>
<a name="l12806"><span class="ln">12806 </span></a>
<a name="l12807"><span class="ln">12807 </span></a><span class="s2">def </span><span class="s1">fliplr</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12808"><span class="ln">12808 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12809"><span class="ln">12809 </span></a>    fliplr(input) -&gt; Tensor 
<a name="l12810"><span class="ln">12810 </span></a> 
<a name="l12811"><span class="ln">12811 </span></a>    Flip tensor in the left/right direction, returning a new tensor. 
<a name="l12812"><span class="ln">12812 </span></a> 
<a name="l12813"><span class="ln">12813 </span></a>    Flip the entries in each row in the left/right direction. 
<a name="l12814"><span class="ln">12814 </span></a>    Columns are preserved, but appear in a different order than before. 
<a name="l12815"><span class="ln">12815 </span></a> 
<a name="l12816"><span class="ln">12816 </span></a>    Note: 
<a name="l12817"><span class="ln">12817 </span></a>        Requires the tensor to be at least 2-D. 
<a name="l12818"><span class="ln">12818 </span></a> 
<a name="l12819"><span class="ln">12819 </span></a>    .. note:: 
<a name="l12820"><span class="ln">12820 </span></a>        `torch.fliplr` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.fliplr`, 
<a name="l12821"><span class="ln">12821 </span></a>        which returns a view in constant time. Since copying a tensor's data is more work than viewing that data, 
<a name="l12822"><span class="ln">12822 </span></a>        `torch.fliplr` is expected to be slower than `np.fliplr`. 
<a name="l12823"><span class="ln">12823 </span></a> 
<a name="l12824"><span class="ln">12824 </span></a>    Args: 
<a name="l12825"><span class="ln">12825 </span></a>        input (Tensor): Must be at least 2-dimensional. 
<a name="l12826"><span class="ln">12826 </span></a> 
<a name="l12827"><span class="ln">12827 </span></a>    Example:: 
<a name="l12828"><span class="ln">12828 </span></a> 
<a name="l12829"><span class="ln">12829 </span></a>        &gt;&gt;&gt; x = torch.arange(4).view(2, 2) 
<a name="l12830"><span class="ln">12830 </span></a>        &gt;&gt;&gt; x 
<a name="l12831"><span class="ln">12831 </span></a>        tensor([[0, 1], 
<a name="l12832"><span class="ln">12832 </span></a>                [2, 3]]) 
<a name="l12833"><span class="ln">12833 </span></a>        &gt;&gt;&gt; torch.fliplr(x) 
<a name="l12834"><span class="ln">12834 </span></a>        tensor([[1, 0], 
<a name="l12835"><span class="ln">12835 </span></a>                [3, 2]]) 
<a name="l12836"><span class="ln">12836 </span></a>    &quot;&quot;&quot;</span>
<a name="l12837"><span class="ln">12837 </span></a>
<a name="l12838"><span class="ln">12838 </span></a><span class="s2">def </span><span class="s1">flipud</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12839"><span class="ln">12839 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12840"><span class="ln">12840 </span></a>    flipud(input) -&gt; Tensor 
<a name="l12841"><span class="ln">12841 </span></a> 
<a name="l12842"><span class="ln">12842 </span></a>    Flip tensor in the up/down direction, returning a new tensor. 
<a name="l12843"><span class="ln">12843 </span></a> 
<a name="l12844"><span class="ln">12844 </span></a>    Flip the entries in each column in the up/down direction. 
<a name="l12845"><span class="ln">12845 </span></a>    Rows are preserved, but appear in a different order than before. 
<a name="l12846"><span class="ln">12846 </span></a> 
<a name="l12847"><span class="ln">12847 </span></a>    Note: 
<a name="l12848"><span class="ln">12848 </span></a>        Requires the tensor to be at least 1-D. 
<a name="l12849"><span class="ln">12849 </span></a> 
<a name="l12850"><span class="ln">12850 </span></a>    .. note:: 
<a name="l12851"><span class="ln">12851 </span></a>        `torch.flipud` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.flipud`, 
<a name="l12852"><span class="ln">12852 </span></a>        which returns a view in constant time. Since copying a tensor's data is more work than viewing that data, 
<a name="l12853"><span class="ln">12853 </span></a>        `torch.flipud` is expected to be slower than `np.flipud`. 
<a name="l12854"><span class="ln">12854 </span></a> 
<a name="l12855"><span class="ln">12855 </span></a>    Args: 
<a name="l12856"><span class="ln">12856 </span></a>        input (Tensor): Must be at least 1-dimensional. 
<a name="l12857"><span class="ln">12857 </span></a> 
<a name="l12858"><span class="ln">12858 </span></a>    Example:: 
<a name="l12859"><span class="ln">12859 </span></a> 
<a name="l12860"><span class="ln">12860 </span></a>        &gt;&gt;&gt; x = torch.arange(4).view(2, 2) 
<a name="l12861"><span class="ln">12861 </span></a>        &gt;&gt;&gt; x 
<a name="l12862"><span class="ln">12862 </span></a>        tensor([[0, 1], 
<a name="l12863"><span class="ln">12863 </span></a>                [2, 3]]) 
<a name="l12864"><span class="ln">12864 </span></a>        &gt;&gt;&gt; torch.flipud(x) 
<a name="l12865"><span class="ln">12865 </span></a>        tensor([[2, 3], 
<a name="l12866"><span class="ln">12866 </span></a>                [0, 1]]) 
<a name="l12867"><span class="ln">12867 </span></a>    &quot;&quot;&quot;</span>
<a name="l12868"><span class="ln">12868 </span></a>
<a name="l12869"><span class="ln">12869 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12870"><span class="ln">12870 </span></a><span class="s2">def </span><span class="s1">float_power</span><span class="s3">(</span>
<a name="l12871"><span class="ln">12871 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12872"><span class="ln">12872 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12873"><span class="ln">12873 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12874"><span class="ln">12874 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12875"><span class="ln">12875 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12876"><span class="ln">12876 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12877"><span class="ln">12877 </span></a>    float_power(input, exponent, *, out=None) -&gt; Tensor 
<a name="l12878"><span class="ln">12878 </span></a> 
<a name="l12879"><span class="ln">12879 </span></a>    Raises :attr:`input` to the power of :attr:`exponent`, elementwise, in double precision. 
<a name="l12880"><span class="ln">12880 </span></a>    If neither input is complex returns a ``torch.float64`` tensor, 
<a name="l12881"><span class="ln">12881 </span></a>    and if one or more inputs is complex returns a ``torch.complex128`` tensor. 
<a name="l12882"><span class="ln">12882 </span></a> 
<a name="l12883"><span class="ln">12883 </span></a>    .. note:: 
<a name="l12884"><span class="ln">12884 </span></a>        This function always computes in double precision, unlike :func:`torch.pow`, 
<a name="l12885"><span class="ln">12885 </span></a>        which implements more typical :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l12886"><span class="ln">12886 </span></a>        This is useful when the computation needs to be performed in a wider or more precise dtype, 
<a name="l12887"><span class="ln">12887 </span></a>        or the results of the computation may contain fractional values not representable in the input dtypes, 
<a name="l12888"><span class="ln">12888 </span></a>        like when an integer base is raised to a negative integer exponent. 
<a name="l12889"><span class="ln">12889 </span></a> 
<a name="l12890"><span class="ln">12890 </span></a>    Args: 
<a name="l12891"><span class="ln">12891 </span></a>        input (Tensor or Number): the base value(s) 
<a name="l12892"><span class="ln">12892 </span></a>        exponent (Tensor or Number): the exponent value(s) 
<a name="l12893"><span class="ln">12893 </span></a> 
<a name="l12894"><span class="ln">12894 </span></a>    Keyword args: 
<a name="l12895"><span class="ln">12895 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12896"><span class="ln">12896 </span></a> 
<a name="l12897"><span class="ln">12897 </span></a>    Example:: 
<a name="l12898"><span class="ln">12898 </span></a> 
<a name="l12899"><span class="ln">12899 </span></a>        &gt;&gt;&gt; a = torch.randint(10, (4,)) 
<a name="l12900"><span class="ln">12900 </span></a>        &gt;&gt;&gt; a 
<a name="l12901"><span class="ln">12901 </span></a>        tensor([6, 4, 7, 1]) 
<a name="l12902"><span class="ln">12902 </span></a>        &gt;&gt;&gt; torch.float_power(a, 2) 
<a name="l12903"><span class="ln">12903 </span></a>        tensor([36., 16., 49.,  1.], dtype=torch.float64) 
<a name="l12904"><span class="ln">12904 </span></a> 
<a name="l12905"><span class="ln">12905 </span></a>        &gt;&gt;&gt; a = torch.arange(1, 5) 
<a name="l12906"><span class="ln">12906 </span></a>        &gt;&gt;&gt; a 
<a name="l12907"><span class="ln">12907 </span></a>        tensor([ 1,  2,  3,  4]) 
<a name="l12908"><span class="ln">12908 </span></a>        &gt;&gt;&gt; exp = torch.tensor([2, -3, 4, -5]) 
<a name="l12909"><span class="ln">12909 </span></a>        &gt;&gt;&gt; exp 
<a name="l12910"><span class="ln">12910 </span></a>        tensor([ 2, -3,  4, -5]) 
<a name="l12911"><span class="ln">12911 </span></a>        &gt;&gt;&gt; torch.float_power(a, exp) 
<a name="l12912"><span class="ln">12912 </span></a>        tensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64) 
<a name="l12913"><span class="ln">12913 </span></a>    &quot;&quot;&quot;</span>
<a name="l12914"><span class="ln">12914 </span></a>
<a name="l12915"><span class="ln">12915 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12916"><span class="ln">12916 </span></a><span class="s2">def </span><span class="s1">float_power</span><span class="s3">(</span>
<a name="l12917"><span class="ln">12917 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l12918"><span class="ln">12918 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12919"><span class="ln">12919 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12920"><span class="ln">12920 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12921"><span class="ln">12921 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12922"><span class="ln">12922 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12923"><span class="ln">12923 </span></a>    float_power(input, exponent, *, out=None) -&gt; Tensor 
<a name="l12924"><span class="ln">12924 </span></a> 
<a name="l12925"><span class="ln">12925 </span></a>    Raises :attr:`input` to the power of :attr:`exponent`, elementwise, in double precision. 
<a name="l12926"><span class="ln">12926 </span></a>    If neither input is complex returns a ``torch.float64`` tensor, 
<a name="l12927"><span class="ln">12927 </span></a>    and if one or more inputs is complex returns a ``torch.complex128`` tensor. 
<a name="l12928"><span class="ln">12928 </span></a> 
<a name="l12929"><span class="ln">12929 </span></a>    .. note:: 
<a name="l12930"><span class="ln">12930 </span></a>        This function always computes in double precision, unlike :func:`torch.pow`, 
<a name="l12931"><span class="ln">12931 </span></a>        which implements more typical :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l12932"><span class="ln">12932 </span></a>        This is useful when the computation needs to be performed in a wider or more precise dtype, 
<a name="l12933"><span class="ln">12933 </span></a>        or the results of the computation may contain fractional values not representable in the input dtypes, 
<a name="l12934"><span class="ln">12934 </span></a>        like when an integer base is raised to a negative integer exponent. 
<a name="l12935"><span class="ln">12935 </span></a> 
<a name="l12936"><span class="ln">12936 </span></a>    Args: 
<a name="l12937"><span class="ln">12937 </span></a>        input (Tensor or Number): the base value(s) 
<a name="l12938"><span class="ln">12938 </span></a>        exponent (Tensor or Number): the exponent value(s) 
<a name="l12939"><span class="ln">12939 </span></a> 
<a name="l12940"><span class="ln">12940 </span></a>    Keyword args: 
<a name="l12941"><span class="ln">12941 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12942"><span class="ln">12942 </span></a> 
<a name="l12943"><span class="ln">12943 </span></a>    Example:: 
<a name="l12944"><span class="ln">12944 </span></a> 
<a name="l12945"><span class="ln">12945 </span></a>        &gt;&gt;&gt; a = torch.randint(10, (4,)) 
<a name="l12946"><span class="ln">12946 </span></a>        &gt;&gt;&gt; a 
<a name="l12947"><span class="ln">12947 </span></a>        tensor([6, 4, 7, 1]) 
<a name="l12948"><span class="ln">12948 </span></a>        &gt;&gt;&gt; torch.float_power(a, 2) 
<a name="l12949"><span class="ln">12949 </span></a>        tensor([36., 16., 49.,  1.], dtype=torch.float64) 
<a name="l12950"><span class="ln">12950 </span></a> 
<a name="l12951"><span class="ln">12951 </span></a>        &gt;&gt;&gt; a = torch.arange(1, 5) 
<a name="l12952"><span class="ln">12952 </span></a>        &gt;&gt;&gt; a 
<a name="l12953"><span class="ln">12953 </span></a>        tensor([ 1,  2,  3,  4]) 
<a name="l12954"><span class="ln">12954 </span></a>        &gt;&gt;&gt; exp = torch.tensor([2, -3, 4, -5]) 
<a name="l12955"><span class="ln">12955 </span></a>        &gt;&gt;&gt; exp 
<a name="l12956"><span class="ln">12956 </span></a>        tensor([ 2, -3,  4, -5]) 
<a name="l12957"><span class="ln">12957 </span></a>        &gt;&gt;&gt; torch.float_power(a, exp) 
<a name="l12958"><span class="ln">12958 </span></a>        tensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64) 
<a name="l12959"><span class="ln">12959 </span></a>    &quot;&quot;&quot;</span>
<a name="l12960"><span class="ln">12960 </span></a>
<a name="l12961"><span class="ln">12961 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l12962"><span class="ln">12962 </span></a><span class="s2">def </span><span class="s1">float_power</span><span class="s3">(</span>
<a name="l12963"><span class="ln">12963 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l12964"><span class="ln">12964 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l12965"><span class="ln">12965 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l12966"><span class="ln">12966 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l12967"><span class="ln">12967 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l12968"><span class="ln">12968 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l12969"><span class="ln">12969 </span></a>    float_power(input, exponent, *, out=None) -&gt; Tensor 
<a name="l12970"><span class="ln">12970 </span></a> 
<a name="l12971"><span class="ln">12971 </span></a>    Raises :attr:`input` to the power of :attr:`exponent`, elementwise, in double precision. 
<a name="l12972"><span class="ln">12972 </span></a>    If neither input is complex returns a ``torch.float64`` tensor, 
<a name="l12973"><span class="ln">12973 </span></a>    and if one or more inputs is complex returns a ``torch.complex128`` tensor. 
<a name="l12974"><span class="ln">12974 </span></a> 
<a name="l12975"><span class="ln">12975 </span></a>    .. note:: 
<a name="l12976"><span class="ln">12976 </span></a>        This function always computes in double precision, unlike :func:`torch.pow`, 
<a name="l12977"><span class="ln">12977 </span></a>        which implements more typical :ref:`type promotion &lt;type-promotion-doc&gt;`. 
<a name="l12978"><span class="ln">12978 </span></a>        This is useful when the computation needs to be performed in a wider or more precise dtype, 
<a name="l12979"><span class="ln">12979 </span></a>        or the results of the computation may contain fractional values not representable in the input dtypes, 
<a name="l12980"><span class="ln">12980 </span></a>        like when an integer base is raised to a negative integer exponent. 
<a name="l12981"><span class="ln">12981 </span></a> 
<a name="l12982"><span class="ln">12982 </span></a>    Args: 
<a name="l12983"><span class="ln">12983 </span></a>        input (Tensor or Number): the base value(s) 
<a name="l12984"><span class="ln">12984 </span></a>        exponent (Tensor or Number): the exponent value(s) 
<a name="l12985"><span class="ln">12985 </span></a> 
<a name="l12986"><span class="ln">12986 </span></a>    Keyword args: 
<a name="l12987"><span class="ln">12987 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l12988"><span class="ln">12988 </span></a> 
<a name="l12989"><span class="ln">12989 </span></a>    Example:: 
<a name="l12990"><span class="ln">12990 </span></a> 
<a name="l12991"><span class="ln">12991 </span></a>        &gt;&gt;&gt; a = torch.randint(10, (4,)) 
<a name="l12992"><span class="ln">12992 </span></a>        &gt;&gt;&gt; a 
<a name="l12993"><span class="ln">12993 </span></a>        tensor([6, 4, 7, 1]) 
<a name="l12994"><span class="ln">12994 </span></a>        &gt;&gt;&gt; torch.float_power(a, 2) 
<a name="l12995"><span class="ln">12995 </span></a>        tensor([36., 16., 49.,  1.], dtype=torch.float64) 
<a name="l12996"><span class="ln">12996 </span></a> 
<a name="l12997"><span class="ln">12997 </span></a>        &gt;&gt;&gt; a = torch.arange(1, 5) 
<a name="l12998"><span class="ln">12998 </span></a>        &gt;&gt;&gt; a 
<a name="l12999"><span class="ln">12999 </span></a>        tensor([ 1,  2,  3,  4]) 
<a name="l13000"><span class="ln">13000 </span></a>        &gt;&gt;&gt; exp = torch.tensor([2, -3, 4, -5]) 
<a name="l13001"><span class="ln">13001 </span></a>        &gt;&gt;&gt; exp 
<a name="l13002"><span class="ln">13002 </span></a>        tensor([ 2, -3,  4, -5]) 
<a name="l13003"><span class="ln">13003 </span></a>        &gt;&gt;&gt; torch.float_power(a, exp) 
<a name="l13004"><span class="ln">13004 </span></a>        tensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64) 
<a name="l13005"><span class="ln">13005 </span></a>    &quot;&quot;&quot;</span>
<a name="l13006"><span class="ln">13006 </span></a>
<a name="l13007"><span class="ln">13007 </span></a><span class="s2">def </span><span class="s1">floor</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13008"><span class="ln">13008 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13009"><span class="ln">13009 </span></a>    floor(input, *, out=None) -&gt; Tensor 
<a name="l13010"><span class="ln">13010 </span></a> 
<a name="l13011"><span class="ln">13011 </span></a>    Returns a new tensor with the floor of the elements of :attr:`input`, 
<a name="l13012"><span class="ln">13012 </span></a>    the largest integer less than or equal to each element. 
<a name="l13013"><span class="ln">13013 </span></a> 
<a name="l13014"><span class="ln">13014 </span></a>    For integer inputs, follows the array-api convention of returning a 
<a name="l13015"><span class="ln">13015 </span></a>    copy of the input tensor. 
<a name="l13016"><span class="ln">13016 </span></a> 
<a name="l13017"><span class="ln">13017 </span></a>    .. math:: 
<a name="l13018"><span class="ln">13018 </span></a>        \text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor 
<a name="l13019"><span class="ln">13019 </span></a> 
<a name="l13020"><span class="ln">13020 </span></a>    Args: 
<a name="l13021"><span class="ln">13021 </span></a>        input (Tensor): the input tensor. 
<a name="l13022"><span class="ln">13022 </span></a> 
<a name="l13023"><span class="ln">13023 </span></a>    Keyword args: 
<a name="l13024"><span class="ln">13024 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13025"><span class="ln">13025 </span></a> 
<a name="l13026"><span class="ln">13026 </span></a>    Example:: 
<a name="l13027"><span class="ln">13027 </span></a> 
<a name="l13028"><span class="ln">13028 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l13029"><span class="ln">13029 </span></a>        &gt;&gt;&gt; a 
<a name="l13030"><span class="ln">13030 </span></a>        tensor([-0.8166,  1.5308, -0.2530, -0.2091]) 
<a name="l13031"><span class="ln">13031 </span></a>        &gt;&gt;&gt; torch.floor(a) 
<a name="l13032"><span class="ln">13032 </span></a>        tensor([-1.,  1., -1., -1.]) 
<a name="l13033"><span class="ln">13033 </span></a>    &quot;&quot;&quot;</span>
<a name="l13034"><span class="ln">13034 </span></a>
<a name="l13035"><span class="ln">13035 </span></a><span class="s2">def </span><span class="s1">floor_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l13036"><span class="ln">13036 </span></a><span class="s2">def </span><span class="s1">floor_divide</span><span class="s3">(</span>
<a name="l13037"><span class="ln">13037 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l13038"><span class="ln">13038 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l13039"><span class="ln">13039 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13040"><span class="ln">13040 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13041"><span class="ln">13041 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13042"><span class="ln">13042 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13043"><span class="ln">13043 </span></a>    floor_divide(input, other, *, out=None) -&gt; Tensor 
<a name="l13044"><span class="ln">13044 </span></a> 
<a name="l13045"><span class="ln">13045 </span></a>    .. note:: 
<a name="l13046"><span class="ln">13046 </span></a> 
<a name="l13047"><span class="ln">13047 </span></a>        Before PyTorch 1.13 :func:`torch.floor_divide` incorrectly performed 
<a name="l13048"><span class="ln">13048 </span></a>        truncation division. To restore the previous behavior use 
<a name="l13049"><span class="ln">13049 </span></a>        :func:`torch.div` with ``rounding_mode='trunc'``. 
<a name="l13050"><span class="ln">13050 </span></a> 
<a name="l13051"><span class="ln">13051 </span></a>    Computes :attr:`input` divided by :attr:`other`, elementwise, and floors 
<a name="l13052"><span class="ln">13052 </span></a>    the result. 
<a name="l13053"><span class="ln">13053 </span></a> 
<a name="l13054"><span class="ln">13054 </span></a>    .. math:: 
<a name="l13055"><span class="ln">13055 </span></a>        \text{{out}}_i = \text{floor} \left( \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right) 
<a name="l13056"><span class="ln">13056 </span></a> 
<a name="l13057"><span class="ln">13057 </span></a> 
<a name="l13058"><span class="ln">13058 </span></a> 
<a name="l13059"><span class="ln">13059 </span></a>    Supports broadcasting to a common shape, type promotion, and integer and float inputs. 
<a name="l13060"><span class="ln">13060 </span></a> 
<a name="l13061"><span class="ln">13061 </span></a>    Args: 
<a name="l13062"><span class="ln">13062 </span></a>        input (Tensor or Number): the dividend 
<a name="l13063"><span class="ln">13063 </span></a>        other (Tensor or Number): the divisor 
<a name="l13064"><span class="ln">13064 </span></a> 
<a name="l13065"><span class="ln">13065 </span></a>    Keyword args: 
<a name="l13066"><span class="ln">13066 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13067"><span class="ln">13067 </span></a> 
<a name="l13068"><span class="ln">13068 </span></a>    Example:: 
<a name="l13069"><span class="ln">13069 </span></a> 
<a name="l13070"><span class="ln">13070 </span></a>        &gt;&gt;&gt; a = torch.tensor([4.0, 3.0]) 
<a name="l13071"><span class="ln">13071 </span></a>        &gt;&gt;&gt; b = torch.tensor([2.0, 2.0]) 
<a name="l13072"><span class="ln">13072 </span></a>        &gt;&gt;&gt; torch.floor_divide(a, b) 
<a name="l13073"><span class="ln">13073 </span></a>        tensor([2.0, 1.0]) 
<a name="l13074"><span class="ln">13074 </span></a>        &gt;&gt;&gt; torch.floor_divide(a, 1.4) 
<a name="l13075"><span class="ln">13075 </span></a>        tensor([2.0, 2.0]) 
<a name="l13076"><span class="ln">13076 </span></a>    &quot;&quot;&quot;</span>
<a name="l13077"><span class="ln">13077 </span></a>
<a name="l13078"><span class="ln">13078 </span></a><span class="s2">def </span><span class="s1">fmax</span><span class="s3">(</span>
<a name="l13079"><span class="ln">13079 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13080"><span class="ln">13080 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13081"><span class="ln">13081 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13082"><span class="ln">13082 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13083"><span class="ln">13083 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13084"><span class="ln">13084 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13085"><span class="ln">13085 </span></a>    fmax(input, other, *, out=None) -&gt; Tensor 
<a name="l13086"><span class="ln">13086 </span></a> 
<a name="l13087"><span class="ln">13087 </span></a>    Computes the element-wise maximum of :attr:`input` and :attr:`other`. 
<a name="l13088"><span class="ln">13088 </span></a> 
<a name="l13089"><span class="ln">13089 </span></a>    This is like :func:`torch.maximum` except it handles NaNs differently: 
<a name="l13090"><span class="ln">13090 </span></a>    if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. 
<a name="l13091"><span class="ln">13091 </span></a>    Only if both elements are NaN is NaN propagated. 
<a name="l13092"><span class="ln">13092 </span></a> 
<a name="l13093"><span class="ln">13093 </span></a>    This function is a wrapper around C++'s ``std::fmax`` and is similar to NumPy's ``fmax`` function. 
<a name="l13094"><span class="ln">13094 </span></a> 
<a name="l13095"><span class="ln">13095 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l13096"><span class="ln">13096 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and floating-point inputs. 
<a name="l13097"><span class="ln">13097 </span></a> 
<a name="l13098"><span class="ln">13098 </span></a>    Args: 
<a name="l13099"><span class="ln">13099 </span></a>        input (Tensor): the input tensor. 
<a name="l13100"><span class="ln">13100 </span></a>        other (Tensor): the second input tensor 
<a name="l13101"><span class="ln">13101 </span></a> 
<a name="l13102"><span class="ln">13102 </span></a>    Keyword args: 
<a name="l13103"><span class="ln">13103 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13104"><span class="ln">13104 </span></a> 
<a name="l13105"><span class="ln">13105 </span></a>    Example:: 
<a name="l13106"><span class="ln">13106 </span></a> 
<a name="l13107"><span class="ln">13107 </span></a>        &gt;&gt;&gt; a = torch.tensor([9.7, float('nan'), 3.1, float('nan')]) 
<a name="l13108"><span class="ln">13108 </span></a>        &gt;&gt;&gt; b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')]) 
<a name="l13109"><span class="ln">13109 </span></a>        &gt;&gt;&gt; torch.fmax(a, b) 
<a name="l13110"><span class="ln">13110 </span></a>        tensor([9.7000, 0.5000, 3.1000,    nan]) 
<a name="l13111"><span class="ln">13111 </span></a>    &quot;&quot;&quot;</span>
<a name="l13112"><span class="ln">13112 </span></a>
<a name="l13113"><span class="ln">13113 </span></a><span class="s2">def </span><span class="s1">fmin</span><span class="s3">(</span>
<a name="l13114"><span class="ln">13114 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13115"><span class="ln">13115 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13116"><span class="ln">13116 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13117"><span class="ln">13117 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13118"><span class="ln">13118 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13119"><span class="ln">13119 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13120"><span class="ln">13120 </span></a>    fmin(input, other, *, out=None) -&gt; Tensor 
<a name="l13121"><span class="ln">13121 </span></a> 
<a name="l13122"><span class="ln">13122 </span></a>    Computes the element-wise minimum of :attr:`input` and :attr:`other`. 
<a name="l13123"><span class="ln">13123 </span></a> 
<a name="l13124"><span class="ln">13124 </span></a>    This is like :func:`torch.minimum` except it handles NaNs differently: 
<a name="l13125"><span class="ln">13125 </span></a>    if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum. 
<a name="l13126"><span class="ln">13126 </span></a>    Only if both elements are NaN is NaN propagated. 
<a name="l13127"><span class="ln">13127 </span></a> 
<a name="l13128"><span class="ln">13128 </span></a>    This function is a wrapper around C++'s ``std::fmin`` and is similar to NumPy's ``fmin`` function. 
<a name="l13129"><span class="ln">13129 </span></a> 
<a name="l13130"><span class="ln">13130 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l13131"><span class="ln">13131 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and floating-point inputs. 
<a name="l13132"><span class="ln">13132 </span></a> 
<a name="l13133"><span class="ln">13133 </span></a>    Args: 
<a name="l13134"><span class="ln">13134 </span></a>        input (Tensor): the input tensor. 
<a name="l13135"><span class="ln">13135 </span></a>        other (Tensor): the second input tensor 
<a name="l13136"><span class="ln">13136 </span></a> 
<a name="l13137"><span class="ln">13137 </span></a>    Keyword args: 
<a name="l13138"><span class="ln">13138 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13139"><span class="ln">13139 </span></a> 
<a name="l13140"><span class="ln">13140 </span></a>    Example:: 
<a name="l13141"><span class="ln">13141 </span></a> 
<a name="l13142"><span class="ln">13142 </span></a>        &gt;&gt;&gt; a = torch.tensor([2.2, float('nan'), 2.1, float('nan')]) 
<a name="l13143"><span class="ln">13143 </span></a>        &gt;&gt;&gt; b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')]) 
<a name="l13144"><span class="ln">13144 </span></a>        &gt;&gt;&gt; torch.fmin(a, b) 
<a name="l13145"><span class="ln">13145 </span></a>        tensor([-9.3000, 0.1000, 2.1000,    nan]) 
<a name="l13146"><span class="ln">13146 </span></a>    &quot;&quot;&quot;</span>
<a name="l13147"><span class="ln">13147 </span></a>
<a name="l13148"><span class="ln">13148 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13149"><span class="ln">13149 </span></a><span class="s2">def </span><span class="s1">fmod</span><span class="s3">(</span>
<a name="l13150"><span class="ln">13150 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13151"><span class="ln">13151 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13152"><span class="ln">13152 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13153"><span class="ln">13153 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13154"><span class="ln">13154 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13155"><span class="ln">13155 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13156"><span class="ln">13156 </span></a>    fmod(input, other, *, out=None) -&gt; Tensor 
<a name="l13157"><span class="ln">13157 </span></a> 
<a name="l13158"><span class="ln">13158 </span></a>    Applies C++'s `std::fmod &lt;https://en.cppreference.com/w/cpp/numeric/math/fmod&gt;`_ entrywise. 
<a name="l13159"><span class="ln">13159 </span></a>    The result has the same sign as the dividend :attr:`input` and its absolute value 
<a name="l13160"><span class="ln">13160 </span></a>    is less than that of :attr:`other`. 
<a name="l13161"><span class="ln">13161 </span></a> 
<a name="l13162"><span class="ln">13162 </span></a>    This function may be defined in terms of :func:`torch.div` as 
<a name="l13163"><span class="ln">13163 </span></a> 
<a name="l13164"><span class="ln">13164 </span></a>    .. code:: python 
<a name="l13165"><span class="ln">13165 </span></a> 
<a name="l13166"><span class="ln">13166 </span></a>        torch.fmod(a, b) == a - a.div(b, rounding_mode=&quot;trunc&quot;) * b 
<a name="l13167"><span class="ln">13167 </span></a> 
<a name="l13168"><span class="ln">13168 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l13169"><span class="ln">13169 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and float inputs. 
<a name="l13170"><span class="ln">13170 </span></a> 
<a name="l13171"><span class="ln">13171 </span></a>    .. note:: 
<a name="l13172"><span class="ln">13172 </span></a> 
<a name="l13173"><span class="ln">13173 </span></a>        When the divisor is zero, returns ``NaN`` for floating point dtypes 
<a name="l13174"><span class="ln">13174 </span></a>        on both CPU and GPU; raises ``RuntimeError`` for integer division by 
<a name="l13175"><span class="ln">13175 </span></a>        zero on CPU; Integer division by zero on GPU may return any value. 
<a name="l13176"><span class="ln">13176 </span></a> 
<a name="l13177"><span class="ln">13177 </span></a>    .. note:: 
<a name="l13178"><span class="ln">13178 </span></a> 
<a name="l13179"><span class="ln">13179 </span></a>       Complex inputs are not supported. In some cases, it is not mathematically 
<a name="l13180"><span class="ln">13180 </span></a>       possible to satisfy the definition of a modulo operation with complex numbers. 
<a name="l13181"><span class="ln">13181 </span></a> 
<a name="l13182"><span class="ln">13182 </span></a>    .. seealso:: 
<a name="l13183"><span class="ln">13183 </span></a> 
<a name="l13184"><span class="ln">13184 </span></a>        :func:`torch.remainder` which implements Python's modulus operator. 
<a name="l13185"><span class="ln">13185 </span></a>        This one is defined using division rounding down the result. 
<a name="l13186"><span class="ln">13186 </span></a> 
<a name="l13187"><span class="ln">13187 </span></a>    Args: 
<a name="l13188"><span class="ln">13188 </span></a>        input (Tensor): the dividend 
<a name="l13189"><span class="ln">13189 </span></a>        other (Tensor or Scalar): the divisor 
<a name="l13190"><span class="ln">13190 </span></a> 
<a name="l13191"><span class="ln">13191 </span></a>    Keyword args: 
<a name="l13192"><span class="ln">13192 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13193"><span class="ln">13193 </span></a> 
<a name="l13194"><span class="ln">13194 </span></a>    Example:: 
<a name="l13195"><span class="ln">13195 </span></a> 
<a name="l13196"><span class="ln">13196 </span></a>        &gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) 
<a name="l13197"><span class="ln">13197 </span></a>        tensor([-1., -0., -1.,  1.,  0.,  1.]) 
<a name="l13198"><span class="ln">13198 </span></a>        &gt;&gt;&gt; torch.fmod(torch.tensor([1, 2, 3, 4, 5]), -1.5) 
<a name="l13199"><span class="ln">13199 </span></a>        tensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000]) 
<a name="l13200"><span class="ln">13200 </span></a>    &quot;&quot;&quot;</span>
<a name="l13201"><span class="ln">13201 </span></a>
<a name="l13202"><span class="ln">13202 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13203"><span class="ln">13203 </span></a><span class="s2">def </span><span class="s1">fmod</span><span class="s3">(</span>
<a name="l13204"><span class="ln">13204 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13205"><span class="ln">13205 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l13206"><span class="ln">13206 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13207"><span class="ln">13207 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13208"><span class="ln">13208 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13209"><span class="ln">13209 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13210"><span class="ln">13210 </span></a>    fmod(input, other, *, out=None) -&gt; Tensor 
<a name="l13211"><span class="ln">13211 </span></a> 
<a name="l13212"><span class="ln">13212 </span></a>    Applies C++'s `std::fmod &lt;https://en.cppreference.com/w/cpp/numeric/math/fmod&gt;`_ entrywise. 
<a name="l13213"><span class="ln">13213 </span></a>    The result has the same sign as the dividend :attr:`input` and its absolute value 
<a name="l13214"><span class="ln">13214 </span></a>    is less than that of :attr:`other`. 
<a name="l13215"><span class="ln">13215 </span></a> 
<a name="l13216"><span class="ln">13216 </span></a>    This function may be defined in terms of :func:`torch.div` as 
<a name="l13217"><span class="ln">13217 </span></a> 
<a name="l13218"><span class="ln">13218 </span></a>    .. code:: python 
<a name="l13219"><span class="ln">13219 </span></a> 
<a name="l13220"><span class="ln">13220 </span></a>        torch.fmod(a, b) == a - a.div(b, rounding_mode=&quot;trunc&quot;) * b 
<a name="l13221"><span class="ln">13221 </span></a> 
<a name="l13222"><span class="ln">13222 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l13223"><span class="ln">13223 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and float inputs. 
<a name="l13224"><span class="ln">13224 </span></a> 
<a name="l13225"><span class="ln">13225 </span></a>    .. note:: 
<a name="l13226"><span class="ln">13226 </span></a> 
<a name="l13227"><span class="ln">13227 </span></a>        When the divisor is zero, returns ``NaN`` for floating point dtypes 
<a name="l13228"><span class="ln">13228 </span></a>        on both CPU and GPU; raises ``RuntimeError`` for integer division by 
<a name="l13229"><span class="ln">13229 </span></a>        zero on CPU; Integer division by zero on GPU may return any value. 
<a name="l13230"><span class="ln">13230 </span></a> 
<a name="l13231"><span class="ln">13231 </span></a>    .. note:: 
<a name="l13232"><span class="ln">13232 </span></a> 
<a name="l13233"><span class="ln">13233 </span></a>       Complex inputs are not supported. In some cases, it is not mathematically 
<a name="l13234"><span class="ln">13234 </span></a>       possible to satisfy the definition of a modulo operation with complex numbers. 
<a name="l13235"><span class="ln">13235 </span></a> 
<a name="l13236"><span class="ln">13236 </span></a>    .. seealso:: 
<a name="l13237"><span class="ln">13237 </span></a> 
<a name="l13238"><span class="ln">13238 </span></a>        :func:`torch.remainder` which implements Python's modulus operator. 
<a name="l13239"><span class="ln">13239 </span></a>        This one is defined using division rounding down the result. 
<a name="l13240"><span class="ln">13240 </span></a> 
<a name="l13241"><span class="ln">13241 </span></a>    Args: 
<a name="l13242"><span class="ln">13242 </span></a>        input (Tensor): the dividend 
<a name="l13243"><span class="ln">13243 </span></a>        other (Tensor or Scalar): the divisor 
<a name="l13244"><span class="ln">13244 </span></a> 
<a name="l13245"><span class="ln">13245 </span></a>    Keyword args: 
<a name="l13246"><span class="ln">13246 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13247"><span class="ln">13247 </span></a> 
<a name="l13248"><span class="ln">13248 </span></a>    Example:: 
<a name="l13249"><span class="ln">13249 </span></a> 
<a name="l13250"><span class="ln">13250 </span></a>        &gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) 
<a name="l13251"><span class="ln">13251 </span></a>        tensor([-1., -0., -1.,  1.,  0.,  1.]) 
<a name="l13252"><span class="ln">13252 </span></a>        &gt;&gt;&gt; torch.fmod(torch.tensor([1, 2, 3, 4, 5]), -1.5) 
<a name="l13253"><span class="ln">13253 </span></a>        tensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000]) 
<a name="l13254"><span class="ln">13254 </span></a>    &quot;&quot;&quot;</span>
<a name="l13255"><span class="ln">13255 </span></a>
<a name="l13256"><span class="ln">13256 </span></a><span class="s2">def </span><span class="s1">frac</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13257"><span class="ln">13257 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13258"><span class="ln">13258 </span></a>    frac(input, *, out=None) -&gt; Tensor 
<a name="l13259"><span class="ln">13259 </span></a> 
<a name="l13260"><span class="ln">13260 </span></a>    Computes the fractional portion of each element in :attr:`input`. 
<a name="l13261"><span class="ln">13261 </span></a> 
<a name="l13262"><span class="ln">13262 </span></a>    .. math:: 
<a name="l13263"><span class="ln">13263 </span></a>        \text{out}_{i} = \text{input}_{i} - \left\lfloor |\text{input}_{i}| \right\rfloor * \operatorname{sgn}(\text{input}_{i}) 
<a name="l13264"><span class="ln">13264 </span></a> 
<a name="l13265"><span class="ln">13265 </span></a>    Example:: 
<a name="l13266"><span class="ln">13266 </span></a> 
<a name="l13267"><span class="ln">13267 </span></a>        &gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2])) 
<a name="l13268"><span class="ln">13268 </span></a>        tensor([ 0.0000,  0.5000, -0.2000]) 
<a name="l13269"><span class="ln">13269 </span></a>    &quot;&quot;&quot;</span>
<a name="l13270"><span class="ln">13270 </span></a>
<a name="l13271"><span class="ln">13271 </span></a><span class="s2">def </span><span class="s1">frac_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l13272"><span class="ln">13272 </span></a><span class="s2">def </span><span class="s1">frexp</span><span class="s3">(</span>
<a name="l13273"><span class="ln">13273 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13274"><span class="ln">13274 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13275"><span class="ln">13275 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13276"><span class="ln">13276 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">frexp</span><span class="s2">:</span>
<a name="l13277"><span class="ln">13277 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13278"><span class="ln">13278 </span></a>    frexp(input, *, out=None) -&gt; (Tensor mantissa, Tensor exponent) 
<a name="l13279"><span class="ln">13279 </span></a> 
<a name="l13280"><span class="ln">13280 </span></a>    Decomposes :attr:`input` into mantissa and exponent tensors 
<a name="l13281"><span class="ln">13281 </span></a>    such that :math:`\text{input} = \text{mantissa} \times 2^{\text{exponent}}`. 
<a name="l13282"><span class="ln">13282 </span></a> 
<a name="l13283"><span class="ln">13283 </span></a>    The range of mantissa is the open interval (-1, 1). 
<a name="l13284"><span class="ln">13284 </span></a> 
<a name="l13285"><span class="ln">13285 </span></a>    Supports float inputs. 
<a name="l13286"><span class="ln">13286 </span></a> 
<a name="l13287"><span class="ln">13287 </span></a>    Args: 
<a name="l13288"><span class="ln">13288 </span></a>        input (Tensor): the input tensor 
<a name="l13289"><span class="ln">13289 </span></a> 
<a name="l13290"><span class="ln">13290 </span></a> 
<a name="l13291"><span class="ln">13291 </span></a>    Keyword args: 
<a name="l13292"><span class="ln">13292 </span></a>        out (tuple, optional): the output tensors 
<a name="l13293"><span class="ln">13293 </span></a> 
<a name="l13294"><span class="ln">13294 </span></a>    Example:: 
<a name="l13295"><span class="ln">13295 </span></a> 
<a name="l13296"><span class="ln">13296 </span></a>        &gt;&gt;&gt; x = torch.arange(9.) 
<a name="l13297"><span class="ln">13297 </span></a>        &gt;&gt;&gt; mantissa, exponent = torch.frexp(x) 
<a name="l13298"><span class="ln">13298 </span></a>        &gt;&gt;&gt; mantissa 
<a name="l13299"><span class="ln">13299 </span></a>        tensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000]) 
<a name="l13300"><span class="ln">13300 </span></a>        &gt;&gt;&gt; exponent 
<a name="l13301"><span class="ln">13301 </span></a>        tensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32) 
<a name="l13302"><span class="ln">13302 </span></a>        &gt;&gt;&gt; torch.ldexp(mantissa, exponent) 
<a name="l13303"><span class="ln">13303 </span></a>        tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.]) 
<a name="l13304"><span class="ln">13304 </span></a>    &quot;&quot;&quot;</span>
<a name="l13305"><span class="ln">13305 </span></a>
<a name="l13306"><span class="ln">13306 </span></a><span class="s2">def </span><span class="s1">frobenius_norm</span><span class="s3">(</span>
<a name="l13307"><span class="ln">13307 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13308"><span class="ln">13308 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l13309"><span class="ln">13309 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13310"><span class="ln">13310 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13311"><span class="ln">13311 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13312"><span class="ln">13312 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l13313"><span class="ln">13313 </span></a><span class="s2">def </span><span class="s1">from_file</span><span class="s3">(</span>
<a name="l13314"><span class="ln">13314 </span></a>    <span class="s1">filename</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l13315"><span class="ln">13315 </span></a>    <span class="s1">shared</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13316"><span class="ln">13316 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = </span><span class="s5">0</span><span class="s3">,</span>
<a name="l13317"><span class="ln">13317 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13318"><span class="ln">13318 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13319"><span class="ln">13319 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13320"><span class="ln">13320 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13321"><span class="ln">13321 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13322"><span class="ln">13322 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13323"><span class="ln">13323 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13324"><span class="ln">13324 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13325"><span class="ln">13325 </span></a>    from_file(filename, shared=None, size=0, *, dtype=None, layout=None, device=None, pin_memory=False) 
<a name="l13326"><span class="ln">13326 </span></a> 
<a name="l13327"><span class="ln">13327 </span></a>    Creates a CPU tensor with a storage backed by a memory-mapped file. 
<a name="l13328"><span class="ln">13328 </span></a> 
<a name="l13329"><span class="ln">13329 </span></a>    If ``shared`` is True, then memory is shared between processes. All changes are written to the file. 
<a name="l13330"><span class="ln">13330 </span></a>    If ``shared`` is False, then changes to the tensor do not affect the file. 
<a name="l13331"><span class="ln">13331 </span></a> 
<a name="l13332"><span class="ln">13332 </span></a>    ``size`` is the number of elements in the Tensor. If ``shared`` is ``False``, then the file must contain 
<a name="l13333"><span class="ln">13333 </span></a>    at least ``size * sizeof(dtype)`` bytes. If ``shared`` is ``True`` the file will be created if needed. 
<a name="l13334"><span class="ln">13334 </span></a> 
<a name="l13335"><span class="ln">13335 </span></a>    .. note:: 
<a name="l13336"><span class="ln">13336 </span></a>        Only CPU tensors can be mapped to files. 
<a name="l13337"><span class="ln">13337 </span></a> 
<a name="l13338"><span class="ln">13338 </span></a>    .. note:: 
<a name="l13339"><span class="ln">13339 </span></a>        For now, tensors with storages backed by a memory-mapped file cannot be created in pinned memory. 
<a name="l13340"><span class="ln">13340 </span></a> 
<a name="l13341"><span class="ln">13341 </span></a> 
<a name="l13342"><span class="ln">13342 </span></a>    Args: 
<a name="l13343"><span class="ln">13343 </span></a>        filename (str): file name to map 
<a name="l13344"><span class="ln">13344 </span></a>        shared (bool): whether to share memory (whether ``MAP_SHARED`` or ``MAP_PRIVATE`` is passed to the 
<a name="l13345"><span class="ln">13345 </span></a>                        underlying `mmap(2) call &lt;https://man7.org/linux/man-pages/man2/mmap.2.html&gt;`_) 
<a name="l13346"><span class="ln">13346 </span></a>        size (int): number of elements in the tensor 
<a name="l13347"><span class="ln">13347 </span></a> 
<a name="l13348"><span class="ln">13348 </span></a>    Keyword args: 
<a name="l13349"><span class="ln">13349 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l13350"><span class="ln">13350 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l13351"><span class="ln">13351 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l13352"><span class="ln">13352 </span></a>            Default: ``torch.strided``. 
<a name="l13353"><span class="ln">13353 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l13354"><span class="ln">13354 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l13355"><span class="ln">13355 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l13356"><span class="ln">13356 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l13357"><span class="ln">13357 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l13358"><span class="ln">13358 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l13359"><span class="ln">13359 </span></a> 
<a name="l13360"><span class="ln">13360 </span></a>    Example:: 
<a name="l13361"><span class="ln">13361 </span></a> 
<a name="l13362"><span class="ln">13362 </span></a>        &gt;&gt;&gt; t = torch.randn(2, 5, dtype=torch.float64) 
<a name="l13363"><span class="ln">13363 </span></a>        &gt;&gt;&gt; t.numpy().tofile('storage.pt') 
<a name="l13364"><span class="ln">13364 </span></a>        &gt;&gt;&gt; t_mapped = torch.from_file('storage.pt', shared=False, size=10, dtype=torch.float64) 
<a name="l13365"><span class="ln">13365 </span></a>    &quot;&quot;&quot;</span>
<a name="l13366"><span class="ln">13366 </span></a>
<a name="l13367"><span class="ln">13367 </span></a><span class="s2">def </span><span class="s1">from_numpy</span><span class="s3">(</span><span class="s1">ndarray</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13368"><span class="ln">13368 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13369"><span class="ln">13369 </span></a>    from_numpy(ndarray) -&gt; Tensor 
<a name="l13370"><span class="ln">13370 </span></a> 
<a name="l13371"><span class="ln">13371 </span></a>    Creates a :class:`Tensor` from a :class:`numpy.ndarray`. 
<a name="l13372"><span class="ln">13372 </span></a> 
<a name="l13373"><span class="ln">13373 </span></a>    The returned tensor and :attr:`ndarray` share the same memory. Modifications to 
<a name="l13374"><span class="ln">13374 </span></a>    the tensor will be reflected in the :attr:`ndarray` and vice versa. The returned 
<a name="l13375"><span class="ln">13375 </span></a>    tensor is not resizable. 
<a name="l13376"><span class="ln">13376 </span></a> 
<a name="l13377"><span class="ln">13377 </span></a>    It currently accepts :attr:`ndarray` with dtypes of ``numpy.float64``, 
<a name="l13378"><span class="ln">13378 </span></a>    ``numpy.float32``, ``numpy.float16``, ``numpy.complex64``, ``numpy.complex128``, 
<a name="l13379"><span class="ln">13379 </span></a>    ``numpy.int64``, ``numpy.int32``, ``numpy.int16``, ``numpy.int8``, ``numpy.uint8``, 
<a name="l13380"><span class="ln">13380 </span></a>    and ``bool``. 
<a name="l13381"><span class="ln">13381 </span></a> 
<a name="l13382"><span class="ln">13382 </span></a>    .. warning:: 
<a name="l13383"><span class="ln">13383 </span></a>        Writing to a tensor created from a read-only NumPy array is not supported and will result in undefined behavior. 
<a name="l13384"><span class="ln">13384 </span></a> 
<a name="l13385"><span class="ln">13385 </span></a>    Example:: 
<a name="l13386"><span class="ln">13386 </span></a> 
<a name="l13387"><span class="ln">13387 </span></a>        &gt;&gt;&gt; a = numpy.array([1, 2, 3]) 
<a name="l13388"><span class="ln">13388 </span></a>        &gt;&gt;&gt; t = torch.from_numpy(a) 
<a name="l13389"><span class="ln">13389 </span></a>        &gt;&gt;&gt; t 
<a name="l13390"><span class="ln">13390 </span></a>        tensor([ 1,  2,  3]) 
<a name="l13391"><span class="ln">13391 </span></a>        &gt;&gt;&gt; t[0] = -1 
<a name="l13392"><span class="ln">13392 </span></a>        &gt;&gt;&gt; a 
<a name="l13393"><span class="ln">13393 </span></a>        array([-1,  2,  3]) 
<a name="l13394"><span class="ln">13394 </span></a>    &quot;&quot;&quot;</span>
<a name="l13395"><span class="ln">13395 </span></a>
<a name="l13396"><span class="ln">13396 </span></a><span class="s2">def </span><span class="s1">frombuffer</span><span class="s3">(</span>
<a name="l13397"><span class="ln">13397 </span></a>    <span class="s1">buffer</span><span class="s2">: </span><span class="s1">Any</span><span class="s3">,</span>
<a name="l13398"><span class="ln">13398 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13399"><span class="ln">13399 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l13400"><span class="ln">13400 </span></a>    <span class="s1">count</span><span class="s2">: </span><span class="s1">int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l13401"><span class="ln">13401 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l13402"><span class="ln">13402 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13403"><span class="ln">13403 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13404"><span class="ln">13404 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13405"><span class="ln">13405 </span></a>    frombuffer(buffer, *, dtype, count=-1, offset=0, requires_grad=False) -&gt; Tensor 
<a name="l13406"><span class="ln">13406 </span></a> 
<a name="l13407"><span class="ln">13407 </span></a>    Creates a 1-dimensional :class:`Tensor` from an object that implements 
<a name="l13408"><span class="ln">13408 </span></a>    the Python buffer protocol. 
<a name="l13409"><span class="ln">13409 </span></a> 
<a name="l13410"><span class="ln">13410 </span></a>    Skips the first :attr:`offset` bytes in the buffer, and interprets the rest of 
<a name="l13411"><span class="ln">13411 </span></a>    the raw bytes as a 1-dimensional tensor of type :attr:`dtype` with :attr:`count` 
<a name="l13412"><span class="ln">13412 </span></a>    elements. 
<a name="l13413"><span class="ln">13413 </span></a> 
<a name="l13414"><span class="ln">13414 </span></a>    Note that either of the following must be true: 
<a name="l13415"><span class="ln">13415 </span></a> 
<a name="l13416"><span class="ln">13416 </span></a>    1. :attr:`count` is a positive non-zero number, and the total number of bytes 
<a name="l13417"><span class="ln">13417 </span></a>    in the buffer is more than :attr:`offset` plus :attr:`count` times the size 
<a name="l13418"><span class="ln">13418 </span></a>    (in bytes) of :attr:`dtype`. 
<a name="l13419"><span class="ln">13419 </span></a> 
<a name="l13420"><span class="ln">13420 </span></a>    2. :attr:`count` is negative, and the length (number of bytes) of the buffer 
<a name="l13421"><span class="ln">13421 </span></a>    subtracted by the :attr:`offset` is a multiple of the size (in bytes) of 
<a name="l13422"><span class="ln">13422 </span></a>    :attr:`dtype`. 
<a name="l13423"><span class="ln">13423 </span></a> 
<a name="l13424"><span class="ln">13424 </span></a>    The returned tensor and buffer share the same memory. Modifications to 
<a name="l13425"><span class="ln">13425 </span></a>    the tensor will be reflected in the buffer and vice versa. The returned 
<a name="l13426"><span class="ln">13426 </span></a>    tensor is not resizable. 
<a name="l13427"><span class="ln">13427 </span></a> 
<a name="l13428"><span class="ln">13428 </span></a>    .. note:: 
<a name="l13429"><span class="ln">13429 </span></a>        This function increments the reference count for the object that 
<a name="l13430"><span class="ln">13430 </span></a>        owns the shared memory. Therefore, such memory will not be deallocated 
<a name="l13431"><span class="ln">13431 </span></a>        before the returned tensor goes out of scope. 
<a name="l13432"><span class="ln">13432 </span></a> 
<a name="l13433"><span class="ln">13433 </span></a>    .. warning:: 
<a name="l13434"><span class="ln">13434 </span></a>        This function's behavior is undefined when passed an object implementing 
<a name="l13435"><span class="ln">13435 </span></a>        the buffer protocol whose data is not on the CPU. Doing so is likely to 
<a name="l13436"><span class="ln">13436 </span></a>        cause a segmentation fault. 
<a name="l13437"><span class="ln">13437 </span></a> 
<a name="l13438"><span class="ln">13438 </span></a>    .. warning:: 
<a name="l13439"><span class="ln">13439 </span></a>        This function does not try to infer the :attr:`dtype` (hence, it is not 
<a name="l13440"><span class="ln">13440 </span></a>        optional). Passing a different :attr:`dtype` than its source may result 
<a name="l13441"><span class="ln">13441 </span></a>        in unexpected behavior. 
<a name="l13442"><span class="ln">13442 </span></a> 
<a name="l13443"><span class="ln">13443 </span></a>    Args: 
<a name="l13444"><span class="ln">13444 </span></a>        buffer (object): a Python object that exposes the buffer interface. 
<a name="l13445"><span class="ln">13445 </span></a> 
<a name="l13446"><span class="ln">13446 </span></a>    Keyword args: 
<a name="l13447"><span class="ln">13447 </span></a>        dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l13448"><span class="ln">13448 </span></a>        count (int, optional): the number of desired elements to be read. 
<a name="l13449"><span class="ln">13449 </span></a>            If negative, all the elements (until the end of the buffer) will be 
<a name="l13450"><span class="ln">13450 </span></a>            read. Default: -1. 
<a name="l13451"><span class="ln">13451 </span></a>        offset (int, optional): the number of bytes to skip at the start of 
<a name="l13452"><span class="ln">13452 </span></a>            the buffer. Default: 0. 
<a name="l13453"><span class="ln">13453 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l13454"><span class="ln">13454 </span></a>            returned tensor. Default: ``False``. 
<a name="l13455"><span class="ln">13455 </span></a> 
<a name="l13456"><span class="ln">13456 </span></a>    Example:: 
<a name="l13457"><span class="ln">13457 </span></a> 
<a name="l13458"><span class="ln">13458 </span></a>        &gt;&gt;&gt; import array 
<a name="l13459"><span class="ln">13459 </span></a>        &gt;&gt;&gt; a = array.array('i', [1, 2, 3]) 
<a name="l13460"><span class="ln">13460 </span></a>        &gt;&gt;&gt; t = torch.frombuffer(a, dtype=torch.int32) 
<a name="l13461"><span class="ln">13461 </span></a>        &gt;&gt;&gt; t 
<a name="l13462"><span class="ln">13462 </span></a>        tensor([ 1,  2,  3]) 
<a name="l13463"><span class="ln">13463 </span></a>        &gt;&gt;&gt; t[0] = -1 
<a name="l13464"><span class="ln">13464 </span></a>        &gt;&gt;&gt; a 
<a name="l13465"><span class="ln">13465 </span></a>        array([-1,  2,  3]) 
<a name="l13466"><span class="ln">13466 </span></a> 
<a name="l13467"><span class="ln">13467 </span></a>        &gt;&gt;&gt; # Interprets the signed char bytes as 32-bit integers. 
<a name="l13468"><span class="ln">13468 </span></a>        &gt;&gt;&gt; # Each 4 signed char elements will be interpreted as 
<a name="l13469"><span class="ln">13469 </span></a>        &gt;&gt;&gt; # 1 signed 32-bit integer. 
<a name="l13470"><span class="ln">13470 </span></a>        &gt;&gt;&gt; import array 
<a name="l13471"><span class="ln">13471 </span></a>        &gt;&gt;&gt; a = array.array('b', [-1, 0, 0, 0]) 
<a name="l13472"><span class="ln">13472 </span></a>        &gt;&gt;&gt; torch.frombuffer(a, dtype=torch.int32) 
<a name="l13473"><span class="ln">13473 </span></a>        tensor([255], dtype=torch.int32) 
<a name="l13474"><span class="ln">13474 </span></a>    &quot;&quot;&quot;</span>
<a name="l13475"><span class="ln">13475 </span></a>
<a name="l13476"><span class="ln">13476 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13477"><span class="ln">13477 </span></a><span class="s2">def </span><span class="s1">full</span><span class="s3">(</span>
<a name="l13478"><span class="ln">13478 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l13479"><span class="ln">13479 </span></a>    <span class="s1">fill_value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l13480"><span class="ln">13480 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13481"><span class="ln">13481 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13482"><span class="ln">13482 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">= </span><span class="s1">strided</span><span class="s3">,</span>
<a name="l13483"><span class="ln">13483 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13484"><span class="ln">13484 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13485"><span class="ln">13485 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13486"><span class="ln">13486 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13487"><span class="ln">13487 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13488"><span class="ln">13488 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13489"><span class="ln">13489 </span></a>    full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l13490"><span class="ln">13490 </span></a> 
<a name="l13491"><span class="ln">13491 </span></a>    Creates a tensor of size :attr:`size` filled with :attr:`fill_value`. The 
<a name="l13492"><span class="ln">13492 </span></a>    tensor's dtype is inferred from :attr:`fill_value`. 
<a name="l13493"><span class="ln">13493 </span></a> 
<a name="l13494"><span class="ln">13494 </span></a>    Args: 
<a name="l13495"><span class="ln">13495 </span></a>        size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l13496"><span class="ln">13496 </span></a>            shape of the output tensor. 
<a name="l13497"><span class="ln">13497 </span></a>        fill_value (Scalar): the value to fill the output tensor with. 
<a name="l13498"><span class="ln">13498 </span></a> 
<a name="l13499"><span class="ln">13499 </span></a>    Keyword args: 
<a name="l13500"><span class="ln">13500 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13501"><span class="ln">13501 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l13502"><span class="ln">13502 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l13503"><span class="ln">13503 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l13504"><span class="ln">13504 </span></a>            Default: ``torch.strided``. 
<a name="l13505"><span class="ln">13505 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l13506"><span class="ln">13506 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l13507"><span class="ln">13507 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l13508"><span class="ln">13508 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l13509"><span class="ln">13509 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l13510"><span class="ln">13510 </span></a>            returned tensor. Default: ``False``. 
<a name="l13511"><span class="ln">13511 </span></a> 
<a name="l13512"><span class="ln">13512 </span></a>    Example:: 
<a name="l13513"><span class="ln">13513 </span></a> 
<a name="l13514"><span class="ln">13514 </span></a>        &gt;&gt;&gt; torch.full((2, 3), 3.141592) 
<a name="l13515"><span class="ln">13515 </span></a>        tensor([[ 3.1416,  3.1416,  3.1416], 
<a name="l13516"><span class="ln">13516 </span></a>                [ 3.1416,  3.1416,  3.1416]]) 
<a name="l13517"><span class="ln">13517 </span></a>    &quot;&quot;&quot;</span>
<a name="l13518"><span class="ln">13518 </span></a>
<a name="l13519"><span class="ln">13519 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13520"><span class="ln">13520 </span></a><span class="s2">def </span><span class="s1">full</span><span class="s3">(</span>
<a name="l13521"><span class="ln">13521 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l13522"><span class="ln">13522 </span></a>    <span class="s1">fill_value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l13523"><span class="ln">13523 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13524"><span class="ln">13524 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">list</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l13525"><span class="ln">13525 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">= </span><span class="s1">strided</span><span class="s3">,</span>
<a name="l13526"><span class="ln">13526 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13527"><span class="ln">13527 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13528"><span class="ln">13528 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13529"><span class="ln">13529 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13530"><span class="ln">13530 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13531"><span class="ln">13531 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13532"><span class="ln">13532 </span></a>    full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l13533"><span class="ln">13533 </span></a> 
<a name="l13534"><span class="ln">13534 </span></a>    Creates a tensor of size :attr:`size` filled with :attr:`fill_value`. The 
<a name="l13535"><span class="ln">13535 </span></a>    tensor's dtype is inferred from :attr:`fill_value`. 
<a name="l13536"><span class="ln">13536 </span></a> 
<a name="l13537"><span class="ln">13537 </span></a>    Args: 
<a name="l13538"><span class="ln">13538 </span></a>        size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l13539"><span class="ln">13539 </span></a>            shape of the output tensor. 
<a name="l13540"><span class="ln">13540 </span></a>        fill_value (Scalar): the value to fill the output tensor with. 
<a name="l13541"><span class="ln">13541 </span></a> 
<a name="l13542"><span class="ln">13542 </span></a>    Keyword args: 
<a name="l13543"><span class="ln">13543 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13544"><span class="ln">13544 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l13545"><span class="ln">13545 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l13546"><span class="ln">13546 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l13547"><span class="ln">13547 </span></a>            Default: ``torch.strided``. 
<a name="l13548"><span class="ln">13548 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l13549"><span class="ln">13549 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l13550"><span class="ln">13550 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l13551"><span class="ln">13551 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l13552"><span class="ln">13552 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l13553"><span class="ln">13553 </span></a>            returned tensor. Default: ``False``. 
<a name="l13554"><span class="ln">13554 </span></a> 
<a name="l13555"><span class="ln">13555 </span></a>    Example:: 
<a name="l13556"><span class="ln">13556 </span></a> 
<a name="l13557"><span class="ln">13557 </span></a>        &gt;&gt;&gt; torch.full((2, 3), 3.141592) 
<a name="l13558"><span class="ln">13558 </span></a>        tensor([[ 3.1416,  3.1416,  3.1416], 
<a name="l13559"><span class="ln">13559 </span></a>                [ 3.1416,  3.1416,  3.1416]]) 
<a name="l13560"><span class="ln">13560 </span></a>    &quot;&quot;&quot;</span>
<a name="l13561"><span class="ln">13561 </span></a>
<a name="l13562"><span class="ln">13562 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13563"><span class="ln">13563 </span></a><span class="s2">def </span><span class="s1">full</span><span class="s3">(</span>
<a name="l13564"><span class="ln">13564 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l13565"><span class="ln">13565 </span></a>    <span class="s1">fill_value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l13566"><span class="ln">13566 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13567"><span class="ln">13567 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13568"><span class="ln">13568 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13569"><span class="ln">13569 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13570"><span class="ln">13570 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13571"><span class="ln">13571 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13572"><span class="ln">13572 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13573"><span class="ln">13573 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13574"><span class="ln">13574 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13575"><span class="ln">13575 </span></a>    full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l13576"><span class="ln">13576 </span></a> 
<a name="l13577"><span class="ln">13577 </span></a>    Creates a tensor of size :attr:`size` filled with :attr:`fill_value`. The 
<a name="l13578"><span class="ln">13578 </span></a>    tensor's dtype is inferred from :attr:`fill_value`. 
<a name="l13579"><span class="ln">13579 </span></a> 
<a name="l13580"><span class="ln">13580 </span></a>    Args: 
<a name="l13581"><span class="ln">13581 </span></a>        size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l13582"><span class="ln">13582 </span></a>            shape of the output tensor. 
<a name="l13583"><span class="ln">13583 </span></a>        fill_value (Scalar): the value to fill the output tensor with. 
<a name="l13584"><span class="ln">13584 </span></a> 
<a name="l13585"><span class="ln">13585 </span></a>    Keyword args: 
<a name="l13586"><span class="ln">13586 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13587"><span class="ln">13587 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l13588"><span class="ln">13588 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l13589"><span class="ln">13589 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l13590"><span class="ln">13590 </span></a>            Default: ``torch.strided``. 
<a name="l13591"><span class="ln">13591 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l13592"><span class="ln">13592 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l13593"><span class="ln">13593 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l13594"><span class="ln">13594 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l13595"><span class="ln">13595 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l13596"><span class="ln">13596 </span></a>            returned tensor. Default: ``False``. 
<a name="l13597"><span class="ln">13597 </span></a> 
<a name="l13598"><span class="ln">13598 </span></a>    Example:: 
<a name="l13599"><span class="ln">13599 </span></a> 
<a name="l13600"><span class="ln">13600 </span></a>        &gt;&gt;&gt; torch.full((2, 3), 3.141592) 
<a name="l13601"><span class="ln">13601 </span></a>        tensor([[ 3.1416,  3.1416,  3.1416], 
<a name="l13602"><span class="ln">13602 </span></a>                [ 3.1416,  3.1416,  3.1416]]) 
<a name="l13603"><span class="ln">13603 </span></a>    &quot;&quot;&quot;</span>
<a name="l13604"><span class="ln">13604 </span></a>
<a name="l13605"><span class="ln">13605 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13606"><span class="ln">13606 </span></a><span class="s2">def </span><span class="s1">full</span><span class="s3">(</span>
<a name="l13607"><span class="ln">13607 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l13608"><span class="ln">13608 </span></a>    <span class="s1">fill_value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l13609"><span class="ln">13609 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13610"><span class="ln">13610 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l13611"><span class="ln">13611 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13612"><span class="ln">13612 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13613"><span class="ln">13613 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13614"><span class="ln">13614 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13615"><span class="ln">13615 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13616"><span class="ln">13616 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13617"><span class="ln">13617 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13618"><span class="ln">13618 </span></a>    full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l13619"><span class="ln">13619 </span></a> 
<a name="l13620"><span class="ln">13620 </span></a>    Creates a tensor of size :attr:`size` filled with :attr:`fill_value`. The 
<a name="l13621"><span class="ln">13621 </span></a>    tensor's dtype is inferred from :attr:`fill_value`. 
<a name="l13622"><span class="ln">13622 </span></a> 
<a name="l13623"><span class="ln">13623 </span></a>    Args: 
<a name="l13624"><span class="ln">13624 </span></a>        size (int...): a list, tuple, or :class:`torch.Size` of integers defining the 
<a name="l13625"><span class="ln">13625 </span></a>            shape of the output tensor. 
<a name="l13626"><span class="ln">13626 </span></a>        fill_value (Scalar): the value to fill the output tensor with. 
<a name="l13627"><span class="ln">13627 </span></a> 
<a name="l13628"><span class="ln">13628 </span></a>    Keyword args: 
<a name="l13629"><span class="ln">13629 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13630"><span class="ln">13630 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l13631"><span class="ln">13631 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l13632"><span class="ln">13632 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l13633"><span class="ln">13633 </span></a>            Default: ``torch.strided``. 
<a name="l13634"><span class="ln">13634 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l13635"><span class="ln">13635 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l13636"><span class="ln">13636 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l13637"><span class="ln">13637 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l13638"><span class="ln">13638 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l13639"><span class="ln">13639 </span></a>            returned tensor. Default: ``False``. 
<a name="l13640"><span class="ln">13640 </span></a> 
<a name="l13641"><span class="ln">13641 </span></a>    Example:: 
<a name="l13642"><span class="ln">13642 </span></a> 
<a name="l13643"><span class="ln">13643 </span></a>        &gt;&gt;&gt; torch.full((2, 3), 3.141592) 
<a name="l13644"><span class="ln">13644 </span></a>        tensor([[ 3.1416,  3.1416,  3.1416], 
<a name="l13645"><span class="ln">13645 </span></a>                [ 3.1416,  3.1416,  3.1416]]) 
<a name="l13646"><span class="ln">13646 </span></a>    &quot;&quot;&quot;</span>
<a name="l13647"><span class="ln">13647 </span></a>
<a name="l13648"><span class="ln">13648 </span></a><span class="s2">def </span><span class="s1">full_like</span><span class="s3">(</span>
<a name="l13649"><span class="ln">13649 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13650"><span class="ln">13650 </span></a>    <span class="s1">fill_value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l13651"><span class="ln">13651 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13652"><span class="ln">13652 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13653"><span class="ln">13653 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13654"><span class="ln">13654 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13655"><span class="ln">13655 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13656"><span class="ln">13656 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13657"><span class="ln">13657 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l13658"><span class="ln">13658 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13659"><span class="ln">13659 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13660"><span class="ln">13660 </span></a>    full_like(input, fill_value, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l13661"><span class="ln">13661 </span></a> 
<a name="l13662"><span class="ln">13662 </span></a>    Returns a tensor with the same size as :attr:`input` filled with :attr:`fill_value`. 
<a name="l13663"><span class="ln">13663 </span></a>    ``torch.full_like(input, fill_value)`` is equivalent to 
<a name="l13664"><span class="ln">13664 </span></a>    ``torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l13665"><span class="ln">13665 </span></a> 
<a name="l13666"><span class="ln">13666 </span></a>    Args: 
<a name="l13667"><span class="ln">13667 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l13668"><span class="ln">13668 </span></a>        fill_value: the number to fill the output tensor with. 
<a name="l13669"><span class="ln">13669 </span></a> 
<a name="l13670"><span class="ln">13670 </span></a>    Keyword args: 
<a name="l13671"><span class="ln">13671 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l13672"><span class="ln">13672 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l13673"><span class="ln">13673 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l13674"><span class="ln">13674 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l13675"><span class="ln">13675 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l13676"><span class="ln">13676 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l13677"><span class="ln">13677 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l13678"><span class="ln">13678 </span></a>            returned tensor. Default: ``False``. 
<a name="l13679"><span class="ln">13679 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l13680"><span class="ln">13680 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l13681"><span class="ln">13681 </span></a>    &quot;&quot;&quot;</span>
<a name="l13682"><span class="ln">13682 </span></a>
<a name="l13683"><span class="ln">13683 </span></a><span class="s2">def </span><span class="s1">fused_moving_avg_obs_fake_quant</span><span class="s3">(</span>
<a name="l13684"><span class="ln">13684 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13685"><span class="ln">13685 </span></a>    <span class="s1">observer_on</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13686"><span class="ln">13686 </span></a>    <span class="s1">fake_quant_on</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13687"><span class="ln">13687 </span></a>    <span class="s1">running_min</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13688"><span class="ln">13688 </span></a>    <span class="s1">running_max</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13689"><span class="ln">13689 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13690"><span class="ln">13690 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13691"><span class="ln">13691 </span></a>    <span class="s1">averaging_const</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l13692"><span class="ln">13692 </span></a>    <span class="s1">quant_min</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l13693"><span class="ln">13693 </span></a>    <span class="s1">quant_max</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l13694"><span class="ln">13694 </span></a>    <span class="s1">ch_axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l13695"><span class="ln">13695 </span></a>    <span class="s1">per_row_fake_quant</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13696"><span class="ln">13696 </span></a>    <span class="s1">symmetric_quant</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13697"><span class="ln">13697 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l13698"><span class="ln">13698 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13699"><span class="ln">13699 </span></a><span class="s2">def </span><span class="s1">gather</span><span class="s3">(</span>
<a name="l13700"><span class="ln">13700 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13701"><span class="ln">13701 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l13702"><span class="ln">13702 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13703"><span class="ln">13703 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13704"><span class="ln">13704 </span></a>    <span class="s1">sparse_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13705"><span class="ln">13705 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13706"><span class="ln">13706 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13707"><span class="ln">13707 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13708"><span class="ln">13708 </span></a>    gather(input, dim, index, *, sparse_grad=False, out=None) -&gt; Tensor 
<a name="l13709"><span class="ln">13709 </span></a> 
<a name="l13710"><span class="ln">13710 </span></a>    Gathers values along an axis specified by `dim`. 
<a name="l13711"><span class="ln">13711 </span></a> 
<a name="l13712"><span class="ln">13712 </span></a>    For a 3-D tensor the output is specified by:: 
<a name="l13713"><span class="ln">13713 </span></a> 
<a name="l13714"><span class="ln">13714 </span></a>        out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0 
<a name="l13715"><span class="ln">13715 </span></a>        out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1 
<a name="l13716"><span class="ln">13716 </span></a>        out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2 
<a name="l13717"><span class="ln">13717 </span></a> 
<a name="l13718"><span class="ln">13718 </span></a>    :attr:`input` and :attr:`index` must have the same number of dimensions. 
<a name="l13719"><span class="ln">13719 </span></a>    It is also required that ``index.size(d) &lt;= input.size(d)`` for all 
<a name="l13720"><span class="ln">13720 </span></a>    dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`. 
<a name="l13721"><span class="ln">13721 </span></a>    Note that ``input`` and ``index`` do not broadcast against each other. 
<a name="l13722"><span class="ln">13722 </span></a> 
<a name="l13723"><span class="ln">13723 </span></a>    Args: 
<a name="l13724"><span class="ln">13724 </span></a>        input (Tensor): the source tensor 
<a name="l13725"><span class="ln">13725 </span></a>        dim (int): the axis along which to index 
<a name="l13726"><span class="ln">13726 </span></a>        index (LongTensor): the indices of elements to gather 
<a name="l13727"><span class="ln">13727 </span></a> 
<a name="l13728"><span class="ln">13728 </span></a>    Keyword arguments: 
<a name="l13729"><span class="ln">13729 </span></a>        sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor. 
<a name="l13730"><span class="ln">13730 </span></a>        out (Tensor, optional): the destination tensor 
<a name="l13731"><span class="ln">13731 </span></a> 
<a name="l13732"><span class="ln">13732 </span></a>    Example:: 
<a name="l13733"><span class="ln">13733 </span></a> 
<a name="l13734"><span class="ln">13734 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2], [3, 4]]) 
<a name="l13735"><span class="ln">13735 </span></a>        &gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]])) 
<a name="l13736"><span class="ln">13736 </span></a>        tensor([[ 1,  1], 
<a name="l13737"><span class="ln">13737 </span></a>                [ 4,  3]]) 
<a name="l13738"><span class="ln">13738 </span></a>    &quot;&quot;&quot;</span>
<a name="l13739"><span class="ln">13739 </span></a>
<a name="l13740"><span class="ln">13740 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13741"><span class="ln">13741 </span></a><span class="s2">def </span><span class="s1">gather</span><span class="s3">(</span>
<a name="l13742"><span class="ln">13742 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13743"><span class="ln">13743 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l13744"><span class="ln">13744 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13745"><span class="ln">13745 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13746"><span class="ln">13746 </span></a>    <span class="s1">sparse_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l13747"><span class="ln">13747 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13748"><span class="ln">13748 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13749"><span class="ln">13749 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13750"><span class="ln">13750 </span></a>    gather(input, dim, index, *, sparse_grad=False, out=None) -&gt; Tensor 
<a name="l13751"><span class="ln">13751 </span></a> 
<a name="l13752"><span class="ln">13752 </span></a>    Gathers values along an axis specified by `dim`. 
<a name="l13753"><span class="ln">13753 </span></a> 
<a name="l13754"><span class="ln">13754 </span></a>    For a 3-D tensor the output is specified by:: 
<a name="l13755"><span class="ln">13755 </span></a> 
<a name="l13756"><span class="ln">13756 </span></a>        out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0 
<a name="l13757"><span class="ln">13757 </span></a>        out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1 
<a name="l13758"><span class="ln">13758 </span></a>        out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2 
<a name="l13759"><span class="ln">13759 </span></a> 
<a name="l13760"><span class="ln">13760 </span></a>    :attr:`input` and :attr:`index` must have the same number of dimensions. 
<a name="l13761"><span class="ln">13761 </span></a>    It is also required that ``index.size(d) &lt;= input.size(d)`` for all 
<a name="l13762"><span class="ln">13762 </span></a>    dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`. 
<a name="l13763"><span class="ln">13763 </span></a>    Note that ``input`` and ``index`` do not broadcast against each other. 
<a name="l13764"><span class="ln">13764 </span></a> 
<a name="l13765"><span class="ln">13765 </span></a>    Args: 
<a name="l13766"><span class="ln">13766 </span></a>        input (Tensor): the source tensor 
<a name="l13767"><span class="ln">13767 </span></a>        dim (int): the axis along which to index 
<a name="l13768"><span class="ln">13768 </span></a>        index (LongTensor): the indices of elements to gather 
<a name="l13769"><span class="ln">13769 </span></a> 
<a name="l13770"><span class="ln">13770 </span></a>    Keyword arguments: 
<a name="l13771"><span class="ln">13771 </span></a>        sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor. 
<a name="l13772"><span class="ln">13772 </span></a>        out (Tensor, optional): the destination tensor 
<a name="l13773"><span class="ln">13773 </span></a> 
<a name="l13774"><span class="ln">13774 </span></a>    Example:: 
<a name="l13775"><span class="ln">13775 </span></a> 
<a name="l13776"><span class="ln">13776 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2], [3, 4]]) 
<a name="l13777"><span class="ln">13777 </span></a>        &gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]])) 
<a name="l13778"><span class="ln">13778 </span></a>        tensor([[ 1,  1], 
<a name="l13779"><span class="ln">13779 </span></a>                [ 4,  3]]) 
<a name="l13780"><span class="ln">13780 </span></a>    &quot;&quot;&quot;</span>
<a name="l13781"><span class="ln">13781 </span></a>
<a name="l13782"><span class="ln">13782 </span></a><span class="s2">def </span><span class="s1">gcd</span><span class="s3">(</span>
<a name="l13783"><span class="ln">13783 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13784"><span class="ln">13784 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13785"><span class="ln">13785 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13786"><span class="ln">13786 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13787"><span class="ln">13787 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13788"><span class="ln">13788 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13789"><span class="ln">13789 </span></a>    gcd(input, other, *, out=None) -&gt; Tensor 
<a name="l13790"><span class="ln">13790 </span></a> 
<a name="l13791"><span class="ln">13791 </span></a>    Computes the element-wise greatest common divisor (GCD) of :attr:`input` and :attr:`other`. 
<a name="l13792"><span class="ln">13792 </span></a> 
<a name="l13793"><span class="ln">13793 </span></a>    Both :attr:`input` and :attr:`other` must have integer types. 
<a name="l13794"><span class="ln">13794 </span></a> 
<a name="l13795"><span class="ln">13795 </span></a>    .. note:: 
<a name="l13796"><span class="ln">13796 </span></a>        This defines :math:`gcd(0, 0) = 0`. 
<a name="l13797"><span class="ln">13797 </span></a> 
<a name="l13798"><span class="ln">13798 </span></a>    Args: 
<a name="l13799"><span class="ln">13799 </span></a>        input (Tensor): the input tensor. 
<a name="l13800"><span class="ln">13800 </span></a>        other (Tensor): the second input tensor 
<a name="l13801"><span class="ln">13801 </span></a> 
<a name="l13802"><span class="ln">13802 </span></a>    Keyword arguments: 
<a name="l13803"><span class="ln">13803 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13804"><span class="ln">13804 </span></a> 
<a name="l13805"><span class="ln">13805 </span></a>    Example:: 
<a name="l13806"><span class="ln">13806 </span></a> 
<a name="l13807"><span class="ln">13807 </span></a>        &gt;&gt;&gt; a = torch.tensor([5, 10, 15]) 
<a name="l13808"><span class="ln">13808 </span></a>        &gt;&gt;&gt; b = torch.tensor([3, 4, 5]) 
<a name="l13809"><span class="ln">13809 </span></a>        &gt;&gt;&gt; torch.gcd(a, b) 
<a name="l13810"><span class="ln">13810 </span></a>        tensor([1, 2, 5]) 
<a name="l13811"><span class="ln">13811 </span></a>        &gt;&gt;&gt; c = torch.tensor([3]) 
<a name="l13812"><span class="ln">13812 </span></a>        &gt;&gt;&gt; torch.gcd(a, c) 
<a name="l13813"><span class="ln">13813 </span></a>        tensor([1, 1, 3]) 
<a name="l13814"><span class="ln">13814 </span></a>    &quot;&quot;&quot;</span>
<a name="l13815"><span class="ln">13815 </span></a>
<a name="l13816"><span class="ln">13816 </span></a><span class="s2">def </span><span class="s1">gcd_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l13817"><span class="ln">13817 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13818"><span class="ln">13818 </span></a><span class="s2">def </span><span class="s1">ge</span><span class="s3">(</span>
<a name="l13819"><span class="ln">13819 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13820"><span class="ln">13820 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13821"><span class="ln">13821 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13822"><span class="ln">13822 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13823"><span class="ln">13823 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13824"><span class="ln">13824 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13825"><span class="ln">13825 </span></a>    ge(input, other, *, out=None) -&gt; Tensor 
<a name="l13826"><span class="ln">13826 </span></a> 
<a name="l13827"><span class="ln">13827 </span></a>    Computes :math:`\text{input} \geq \text{other}` element-wise. 
<a name="l13828"><span class="ln">13828 </span></a> 
<a name="l13829"><span class="ln">13829 </span></a> 
<a name="l13830"><span class="ln">13830 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l13831"><span class="ln">13831 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l13832"><span class="ln">13832 </span></a> 
<a name="l13833"><span class="ln">13833 </span></a>    Args: 
<a name="l13834"><span class="ln">13834 </span></a>        input (Tensor): the tensor to compare 
<a name="l13835"><span class="ln">13835 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l13836"><span class="ln">13836 </span></a> 
<a name="l13837"><span class="ln">13837 </span></a>    Keyword args: 
<a name="l13838"><span class="ln">13838 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13839"><span class="ln">13839 </span></a> 
<a name="l13840"><span class="ln">13840 </span></a>    Returns: 
<a name="l13841"><span class="ln">13841 </span></a>        A boolean tensor that is True where :attr:`input` is greater than or equal to :attr:`other` and False elsewhere 
<a name="l13842"><span class="ln">13842 </span></a> 
<a name="l13843"><span class="ln">13843 </span></a>    Example:: 
<a name="l13844"><span class="ln">13844 </span></a> 
<a name="l13845"><span class="ln">13845 </span></a>        &gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l13846"><span class="ln">13846 </span></a>        tensor([[True, True], [False, True]]) 
<a name="l13847"><span class="ln">13847 </span></a>    &quot;&quot;&quot;</span>
<a name="l13848"><span class="ln">13848 </span></a>
<a name="l13849"><span class="ln">13849 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13850"><span class="ln">13850 </span></a><span class="s2">def </span><span class="s1">ge</span><span class="s3">(</span>
<a name="l13851"><span class="ln">13851 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13852"><span class="ln">13852 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l13853"><span class="ln">13853 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13854"><span class="ln">13854 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13855"><span class="ln">13855 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13856"><span class="ln">13856 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13857"><span class="ln">13857 </span></a>    ge(input, other, *, out=None) -&gt; Tensor 
<a name="l13858"><span class="ln">13858 </span></a> 
<a name="l13859"><span class="ln">13859 </span></a>    Computes :math:`\text{input} \geq \text{other}` element-wise. 
<a name="l13860"><span class="ln">13860 </span></a> 
<a name="l13861"><span class="ln">13861 </span></a> 
<a name="l13862"><span class="ln">13862 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l13863"><span class="ln">13863 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l13864"><span class="ln">13864 </span></a> 
<a name="l13865"><span class="ln">13865 </span></a>    Args: 
<a name="l13866"><span class="ln">13866 </span></a>        input (Tensor): the tensor to compare 
<a name="l13867"><span class="ln">13867 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l13868"><span class="ln">13868 </span></a> 
<a name="l13869"><span class="ln">13869 </span></a>    Keyword args: 
<a name="l13870"><span class="ln">13870 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l13871"><span class="ln">13871 </span></a> 
<a name="l13872"><span class="ln">13872 </span></a>    Returns: 
<a name="l13873"><span class="ln">13873 </span></a>        A boolean tensor that is True where :attr:`input` is greater than or equal to :attr:`other` and False elsewhere 
<a name="l13874"><span class="ln">13874 </span></a> 
<a name="l13875"><span class="ln">13875 </span></a>    Example:: 
<a name="l13876"><span class="ln">13876 </span></a> 
<a name="l13877"><span class="ln">13877 </span></a>        &gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l13878"><span class="ln">13878 </span></a>        tensor([[True, True], [False, True]]) 
<a name="l13879"><span class="ln">13879 </span></a>    &quot;&quot;&quot;</span>
<a name="l13880"><span class="ln">13880 </span></a>
<a name="l13881"><span class="ln">13881 </span></a><span class="s2">def </span><span class="s1">geqrf</span><span class="s3">(</span>
<a name="l13882"><span class="ln">13882 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13883"><span class="ln">13883 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13884"><span class="ln">13884 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13885"><span class="ln">13885 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">geqrf</span><span class="s2">:</span>
<a name="l13886"><span class="ln">13886 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13887"><span class="ln">13887 </span></a>    geqrf(input, *, out=None) -&gt; (Tensor, Tensor) 
<a name="l13888"><span class="ln">13888 </span></a> 
<a name="l13889"><span class="ln">13889 </span></a>    This is a low-level function for calling LAPACK's geqrf directly. This function 
<a name="l13890"><span class="ln">13890 </span></a>    returns a namedtuple (a, tau) as defined in `LAPACK documentation for geqrf`_ . 
<a name="l13891"><span class="ln">13891 </span></a> 
<a name="l13892"><span class="ln">13892 </span></a>    Computes a QR decomposition of :attr:`input`. 
<a name="l13893"><span class="ln">13893 </span></a>    Both `Q` and `R` matrices are stored in the same output tensor `a`. 
<a name="l13894"><span class="ln">13894 </span></a>    The elements of `R` are stored on and above the diagonal. 
<a name="l13895"><span class="ln">13895 </span></a>    Elementary reflectors (or Householder vectors) implicitly defining matrix `Q` 
<a name="l13896"><span class="ln">13896 </span></a>    are stored below the diagonal. 
<a name="l13897"><span class="ln">13897 </span></a>    The results of this function can be used together with :func:`torch.linalg.householder_product` 
<a name="l13898"><span class="ln">13898 </span></a>    to obtain the `Q` matrix or 
<a name="l13899"><span class="ln">13899 </span></a>    with :func:`torch.ormqr`, which uses an implicit representation of the `Q` matrix, 
<a name="l13900"><span class="ln">13900 </span></a>    for an efficient matrix-matrix multiplication. 
<a name="l13901"><span class="ln">13901 </span></a> 
<a name="l13902"><span class="ln">13902 </span></a>    See `LAPACK documentation for geqrf`_ for further details. 
<a name="l13903"><span class="ln">13903 </span></a> 
<a name="l13904"><span class="ln">13904 </span></a>    .. note:: 
<a name="l13905"><span class="ln">13905 </span></a>        See also :func:`torch.linalg.qr`, which computes Q and R matrices, and :func:`torch.linalg.lstsq` 
<a name="l13906"><span class="ln">13906 </span></a>        with the ``driver=&quot;gels&quot;`` option for a function that can solve matrix equations using a QR decomposition. 
<a name="l13907"><span class="ln">13907 </span></a> 
<a name="l13908"><span class="ln">13908 </span></a>    Args: 
<a name="l13909"><span class="ln">13909 </span></a>        input (Tensor): the input matrix 
<a name="l13910"><span class="ln">13910 </span></a> 
<a name="l13911"><span class="ln">13911 </span></a>    Keyword args: 
<a name="l13912"><span class="ln">13912 </span></a>        out (tuple, optional): the output tuple of (Tensor, Tensor). Ignored if `None`. Default: `None`. 
<a name="l13913"><span class="ln">13913 </span></a> 
<a name="l13914"><span class="ln">13914 </span></a>    .. _LAPACK documentation for geqrf: 
<a name="l13915"><span class="ln">13915 </span></a>        http://www.netlib.org/lapack/explore-html/df/dc5/group__variants_g_ecomputational_ga3766ea903391b5cf9008132f7440ec7b.html 
<a name="l13916"><span class="ln">13916 </span></a>    &quot;&quot;&quot;</span>
<a name="l13917"><span class="ln">13917 </span></a>
<a name="l13918"><span class="ln">13918 </span></a><span class="s2">def </span><span class="s1">ger</span><span class="s3">(</span>
<a name="l13919"><span class="ln">13919 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13920"><span class="ln">13920 </span></a>    <span class="s1">vec2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13921"><span class="ln">13921 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13922"><span class="ln">13922 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13923"><span class="ln">13923 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l13924"><span class="ln">13924 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13925"><span class="ln">13925 </span></a>    ger(input, vec2, *, out=None) -&gt; Tensor 
<a name="l13926"><span class="ln">13926 </span></a> 
<a name="l13927"><span class="ln">13927 </span></a>    Alias of :func:`torch.outer`. 
<a name="l13928"><span class="ln">13928 </span></a> 
<a name="l13929"><span class="ln">13929 </span></a>    .. warning:: 
<a name="l13930"><span class="ln">13930 </span></a>        This function is deprecated and will be removed in a future PyTorch release. 
<a name="l13931"><span class="ln">13931 </span></a>        Use :func:`torch.outer` instead. 
<a name="l13932"><span class="ln">13932 </span></a>    &quot;&quot;&quot;</span>
<a name="l13933"><span class="ln">13933 </span></a>
<a name="l13934"><span class="ln">13934 </span></a><span class="s2">def </span><span class="s1">get_default_dtype</span><span class="s3">() </span><span class="s1">-&gt; _dtype</span><span class="s2">:</span>
<a name="l13935"><span class="ln">13935 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13936"><span class="ln">13936 </span></a>    get_default_dtype() -&gt; torch.dtype 
<a name="l13937"><span class="ln">13937 </span></a> 
<a name="l13938"><span class="ln">13938 </span></a>    Get the current default floating point :class:`torch.dtype`. 
<a name="l13939"><span class="ln">13939 </span></a> 
<a name="l13940"><span class="ln">13940 </span></a>    Example:: 
<a name="l13941"><span class="ln">13941 </span></a> 
<a name="l13942"><span class="ln">13942 </span></a>        &gt;&gt;&gt; torch.get_default_dtype()  # initial default for floating point is torch.float32 
<a name="l13943"><span class="ln">13943 </span></a>        torch.float32 
<a name="l13944"><span class="ln">13944 </span></a>        &gt;&gt;&gt; torch.set_default_dtype(torch.float64) 
<a name="l13945"><span class="ln">13945 </span></a>        &gt;&gt;&gt; torch.get_default_dtype()  # default is now changed to torch.float64 
<a name="l13946"><span class="ln">13946 </span></a>        torch.float64 
<a name="l13947"><span class="ln">13947 </span></a>    &quot;&quot;&quot;</span>
<a name="l13948"><span class="ln">13948 </span></a>
<a name="l13949"><span class="ln">13949 </span></a><span class="s2">def </span><span class="s1">get_num_interop_threads</span><span class="s3">() </span><span class="s1">-&gt; _int</span><span class="s2">:</span>
<a name="l13950"><span class="ln">13950 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13951"><span class="ln">13951 </span></a>    get_num_interop_threads() -&gt; int 
<a name="l13952"><span class="ln">13952 </span></a> 
<a name="l13953"><span class="ln">13953 </span></a>    Returns the number of threads used for inter-op parallelism on CPU 
<a name="l13954"><span class="ln">13954 </span></a>    (e.g. in JIT interpreter) 
<a name="l13955"><span class="ln">13955 </span></a>    &quot;&quot;&quot;</span>
<a name="l13956"><span class="ln">13956 </span></a>
<a name="l13957"><span class="ln">13957 </span></a><span class="s2">def </span><span class="s1">get_num_threads</span><span class="s3">() </span><span class="s1">-&gt; _int</span><span class="s2">:</span>
<a name="l13958"><span class="ln">13958 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13959"><span class="ln">13959 </span></a>    get_num_threads() -&gt; int 
<a name="l13960"><span class="ln">13960 </span></a> 
<a name="l13961"><span class="ln">13961 </span></a>    Returns the number of threads used for parallelizing CPU operations 
<a name="l13962"><span class="ln">13962 </span></a>    &quot;&quot;&quot;</span>
<a name="l13963"><span class="ln">13963 </span></a>
<a name="l13964"><span class="ln">13964 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l13965"><span class="ln">13965 </span></a><span class="s2">def </span><span class="s1">gradient</span><span class="s3">(</span>
<a name="l13966"><span class="ln">13966 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l13967"><span class="ln">13967 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l13968"><span class="ln">13968 </span></a>    <span class="s1">spacing</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13969"><span class="ln">13969 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l13970"><span class="ln">13970 </span></a>    <span class="s1">edge_order</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l13971"><span class="ln">13971 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l13972"><span class="ln">13972 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l13973"><span class="ln">13973 </span></a>    gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l13974"><span class="ln">13974 </span></a> 
<a name="l13975"><span class="ln">13975 </span></a>    Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l13976"><span class="ln">13976 </span></a>    one or more dimensions using the `second-order accurate central differences method 
<a name="l13977"><span class="ln">13977 </span></a>    &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l13978"><span class="ln">13978 </span></a>    either first or second order estimates at the boundaries. 
<a name="l13979"><span class="ln">13979 </span></a> 
<a name="l13980"><span class="ln">13980 </span></a>    The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l13981"><span class="ln">13981 </span></a>    specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l13982"><span class="ln">13982 </span></a>    to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l13983"><span class="ln">13983 </span></a>    :attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l13984"><span class="ln">13984 </span></a>    :math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l13985"><span class="ln">13985 </span></a> 
<a name="l13986"><span class="ln">13986 </span></a>    When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l13987"><span class="ln">13987 </span></a>    This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l13988"><span class="ln">13988 </span></a> 
<a name="l13989"><span class="ln">13989 </span></a>    The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l13990"><span class="ln">13990 </span></a>    accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l13991"><span class="ln">13991 </span></a>    improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l13992"><span class="ln">13992 </span></a>    is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l13993"><span class="ln">13993 </span></a>    Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l13994"><span class="ln">13994 </span></a>    it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l13995"><span class="ln">13995 </span></a> 
<a name="l13996"><span class="ln">13996 </span></a>    .. math:: 
<a name="l13997"><span class="ln">13997 </span></a>        \begin{aligned} 
<a name="l13998"><span class="ln">13998 </span></a>            f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l13999"><span class="ln">13999 </span></a>            f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l14000"><span class="ln">14000 </span></a>        \end{aligned} 
<a name="l14001"><span class="ln">14001 </span></a> 
<a name="l14002"><span class="ln">14002 </span></a>    Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l14003"><span class="ln">14003 </span></a> 
<a name="l14004"><span class="ln">14004 </span></a>    .. math:: 
<a name="l14005"><span class="ln">14005 </span></a>        f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l14006"><span class="ln">14006 </span></a>              + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l14007"><span class="ln">14007 </span></a> 
<a name="l14008"><span class="ln">14008 </span></a>    .. note:: 
<a name="l14009"><span class="ln">14009 </span></a>        We estimate the gradient of functions in complex domain 
<a name="l14010"><span class="ln">14010 </span></a>        :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l14011"><span class="ln">14011 </span></a> 
<a name="l14012"><span class="ln">14012 </span></a>    The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l14013"><span class="ln">14013 </span></a> 
<a name="l14014"><span class="ln">14014 </span></a>    Args: 
<a name="l14015"><span class="ln">14015 </span></a>        input (``Tensor``): the tensor that represents the values of the function 
<a name="l14016"><span class="ln">14016 </span></a> 
<a name="l14017"><span class="ln">14017 </span></a>    Keyword args: 
<a name="l14018"><span class="ln">14018 </span></a>        spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l14019"><span class="ln">14019 </span></a>            how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l14020"><span class="ln">14020 </span></a>            the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l14021"><span class="ln">14021 </span></a>            indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l14022"><span class="ln">14022 </span></a>            indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l14023"><span class="ln">14023 </span></a>            Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l14024"><span class="ln">14024 </span></a>            the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l14025"><span class="ln">14025 </span></a>            the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l14026"><span class="ln">14026 </span></a> 
<a name="l14027"><span class="ln">14027 </span></a>        dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l14028"><span class="ln">14028 </span></a>            the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l14029"><span class="ln">14029 </span></a>            the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l14030"><span class="ln">14030 </span></a> 
<a name="l14031"><span class="ln">14031 </span></a>        edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l14032"><span class="ln">14032 </span></a>            &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l14033"><span class="ln">14033 </span></a>            `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l14034"><span class="ln">14034 </span></a>            estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l14035"><span class="ln">14035 </span></a> 
<a name="l14036"><span class="ln">14036 </span></a>    Examples:: 
<a name="l14037"><span class="ln">14037 </span></a> 
<a name="l14038"><span class="ln">14038 </span></a>        &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l14039"><span class="ln">14039 </span></a>        &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l14040"><span class="ln">14040 </span></a>        &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l14041"><span class="ln">14041 </span></a>        &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l14042"><span class="ln">14042 </span></a>        (tensor([-3., -2., 2., 5.]),) 
<a name="l14043"><span class="ln">14043 </span></a> 
<a name="l14044"><span class="ln">14044 </span></a>        &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l14045"><span class="ln">14045 </span></a>        &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l14046"><span class="ln">14046 </span></a>        &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l14047"><span class="ln">14047 </span></a>        &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l14048"><span class="ln">14048 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l14049"><span class="ln">14049 </span></a>        &gt;&gt;&gt; torch.gradient(t) 
<a name="l14050"><span class="ln">14050 </span></a>        (tensor([[ 9., 18., 36., 72.], 
<a name="l14051"><span class="ln">14051 </span></a>                 [ 9., 18., 36., 72.]]), 
<a name="l14052"><span class="ln">14052 </span></a>         tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14053"><span class="ln">14053 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l14054"><span class="ln">14054 </span></a> 
<a name="l14055"><span class="ln">14055 </span></a>        &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l14056"><span class="ln">14056 </span></a>        &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l14057"><span class="ln">14057 </span></a>        &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l14058"><span class="ln">14058 </span></a>        &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l14059"><span class="ln">14059 </span></a>        &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l14060"><span class="ln">14060 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l14061"><span class="ln">14061 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14062"><span class="ln">14062 </span></a>                  [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14063"><span class="ln">14063 </span></a>         tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l14064"><span class="ln">14064 </span></a>                  [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l14065"><span class="ln">14065 </span></a>        &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l14066"><span class="ln">14066 </span></a> 
<a name="l14067"><span class="ln">14067 </span></a>        &gt;&gt;&gt; 
<a name="l14068"><span class="ln">14068 </span></a>        &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l14069"><span class="ln">14069 </span></a>        &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l14070"><span class="ln">14070 </span></a>        (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14071"><span class="ln">14071 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l14072"><span class="ln">14072 </span></a> 
<a name="l14073"><span class="ln">14073 </span></a>        &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l14074"><span class="ln">14074 </span></a>        &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l14075"><span class="ln">14075 </span></a>        &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l14076"><span class="ln">14076 </span></a>        &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l14077"><span class="ln">14077 </span></a>        &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l14078"><span class="ln">14078 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l14079"><span class="ln">14079 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14080"><span class="ln">14080 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14081"><span class="ln">14081 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14082"><span class="ln">14082 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14083"><span class="ln">14083 </span></a> 
<a name="l14084"><span class="ln">14084 </span></a>        &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l14085"><span class="ln">14085 </span></a>        &gt;&gt;&gt; # coordinates. 
<a name="l14086"><span class="ln">14086 </span></a>        &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l14087"><span class="ln">14087 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l14088"><span class="ln">14088 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14089"><span class="ln">14089 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14090"><span class="ln">14090 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14091"><span class="ln">14091 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14092"><span class="ln">14092 </span></a>    &quot;&quot;&quot;</span>
<a name="l14093"><span class="ln">14093 </span></a>
<a name="l14094"><span class="ln">14094 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14095"><span class="ln">14095 </span></a><span class="s2">def </span><span class="s1">gradient</span><span class="s3">(</span>
<a name="l14096"><span class="ln">14096 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14097"><span class="ln">14097 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14098"><span class="ln">14098 </span></a>    <span class="s1">spacing</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l14099"><span class="ln">14099 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14100"><span class="ln">14100 </span></a>    <span class="s1">edge_order</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l14101"><span class="ln">14101 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l14102"><span class="ln">14102 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14103"><span class="ln">14103 </span></a>    gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l14104"><span class="ln">14104 </span></a> 
<a name="l14105"><span class="ln">14105 </span></a>    Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l14106"><span class="ln">14106 </span></a>    one or more dimensions using the `second-order accurate central differences method 
<a name="l14107"><span class="ln">14107 </span></a>    &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l14108"><span class="ln">14108 </span></a>    either first or second order estimates at the boundaries. 
<a name="l14109"><span class="ln">14109 </span></a> 
<a name="l14110"><span class="ln">14110 </span></a>    The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l14111"><span class="ln">14111 </span></a>    specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l14112"><span class="ln">14112 </span></a>    to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l14113"><span class="ln">14113 </span></a>    :attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l14114"><span class="ln">14114 </span></a>    :math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l14115"><span class="ln">14115 </span></a> 
<a name="l14116"><span class="ln">14116 </span></a>    When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l14117"><span class="ln">14117 </span></a>    This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l14118"><span class="ln">14118 </span></a> 
<a name="l14119"><span class="ln">14119 </span></a>    The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l14120"><span class="ln">14120 </span></a>    accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l14121"><span class="ln">14121 </span></a>    improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l14122"><span class="ln">14122 </span></a>    is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l14123"><span class="ln">14123 </span></a>    Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l14124"><span class="ln">14124 </span></a>    it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l14125"><span class="ln">14125 </span></a> 
<a name="l14126"><span class="ln">14126 </span></a>    .. math:: 
<a name="l14127"><span class="ln">14127 </span></a>        \begin{aligned} 
<a name="l14128"><span class="ln">14128 </span></a>            f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l14129"><span class="ln">14129 </span></a>            f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l14130"><span class="ln">14130 </span></a>        \end{aligned} 
<a name="l14131"><span class="ln">14131 </span></a> 
<a name="l14132"><span class="ln">14132 </span></a>    Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l14133"><span class="ln">14133 </span></a> 
<a name="l14134"><span class="ln">14134 </span></a>    .. math:: 
<a name="l14135"><span class="ln">14135 </span></a>        f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l14136"><span class="ln">14136 </span></a>              + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l14137"><span class="ln">14137 </span></a> 
<a name="l14138"><span class="ln">14138 </span></a>    .. note:: 
<a name="l14139"><span class="ln">14139 </span></a>        We estimate the gradient of functions in complex domain 
<a name="l14140"><span class="ln">14140 </span></a>        :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l14141"><span class="ln">14141 </span></a> 
<a name="l14142"><span class="ln">14142 </span></a>    The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l14143"><span class="ln">14143 </span></a> 
<a name="l14144"><span class="ln">14144 </span></a>    Args: 
<a name="l14145"><span class="ln">14145 </span></a>        input (``Tensor``): the tensor that represents the values of the function 
<a name="l14146"><span class="ln">14146 </span></a> 
<a name="l14147"><span class="ln">14147 </span></a>    Keyword args: 
<a name="l14148"><span class="ln">14148 </span></a>        spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l14149"><span class="ln">14149 </span></a>            how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l14150"><span class="ln">14150 </span></a>            the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l14151"><span class="ln">14151 </span></a>            indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l14152"><span class="ln">14152 </span></a>            indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l14153"><span class="ln">14153 </span></a>            Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l14154"><span class="ln">14154 </span></a>            the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l14155"><span class="ln">14155 </span></a>            the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l14156"><span class="ln">14156 </span></a> 
<a name="l14157"><span class="ln">14157 </span></a>        dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l14158"><span class="ln">14158 </span></a>            the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l14159"><span class="ln">14159 </span></a>            the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l14160"><span class="ln">14160 </span></a> 
<a name="l14161"><span class="ln">14161 </span></a>        edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l14162"><span class="ln">14162 </span></a>            &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l14163"><span class="ln">14163 </span></a>            `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l14164"><span class="ln">14164 </span></a>            estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l14165"><span class="ln">14165 </span></a> 
<a name="l14166"><span class="ln">14166 </span></a>    Examples:: 
<a name="l14167"><span class="ln">14167 </span></a> 
<a name="l14168"><span class="ln">14168 </span></a>        &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l14169"><span class="ln">14169 </span></a>        &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l14170"><span class="ln">14170 </span></a>        &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l14171"><span class="ln">14171 </span></a>        &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l14172"><span class="ln">14172 </span></a>        (tensor([-3., -2., 2., 5.]),) 
<a name="l14173"><span class="ln">14173 </span></a> 
<a name="l14174"><span class="ln">14174 </span></a>        &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l14175"><span class="ln">14175 </span></a>        &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l14176"><span class="ln">14176 </span></a>        &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l14177"><span class="ln">14177 </span></a>        &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l14178"><span class="ln">14178 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l14179"><span class="ln">14179 </span></a>        &gt;&gt;&gt; torch.gradient(t) 
<a name="l14180"><span class="ln">14180 </span></a>        (tensor([[ 9., 18., 36., 72.], 
<a name="l14181"><span class="ln">14181 </span></a>                 [ 9., 18., 36., 72.]]), 
<a name="l14182"><span class="ln">14182 </span></a>         tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14183"><span class="ln">14183 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l14184"><span class="ln">14184 </span></a> 
<a name="l14185"><span class="ln">14185 </span></a>        &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l14186"><span class="ln">14186 </span></a>        &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l14187"><span class="ln">14187 </span></a>        &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l14188"><span class="ln">14188 </span></a>        &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l14189"><span class="ln">14189 </span></a>        &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l14190"><span class="ln">14190 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l14191"><span class="ln">14191 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14192"><span class="ln">14192 </span></a>                  [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14193"><span class="ln">14193 </span></a>         tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l14194"><span class="ln">14194 </span></a>                  [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l14195"><span class="ln">14195 </span></a>        &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l14196"><span class="ln">14196 </span></a> 
<a name="l14197"><span class="ln">14197 </span></a>        &gt;&gt;&gt; 
<a name="l14198"><span class="ln">14198 </span></a>        &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l14199"><span class="ln">14199 </span></a>        &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l14200"><span class="ln">14200 </span></a>        (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14201"><span class="ln">14201 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l14202"><span class="ln">14202 </span></a> 
<a name="l14203"><span class="ln">14203 </span></a>        &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l14204"><span class="ln">14204 </span></a>        &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l14205"><span class="ln">14205 </span></a>        &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l14206"><span class="ln">14206 </span></a>        &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l14207"><span class="ln">14207 </span></a>        &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l14208"><span class="ln">14208 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l14209"><span class="ln">14209 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14210"><span class="ln">14210 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14211"><span class="ln">14211 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14212"><span class="ln">14212 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14213"><span class="ln">14213 </span></a> 
<a name="l14214"><span class="ln">14214 </span></a>        &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l14215"><span class="ln">14215 </span></a>        &gt;&gt;&gt; # coordinates. 
<a name="l14216"><span class="ln">14216 </span></a>        &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l14217"><span class="ln">14217 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l14218"><span class="ln">14218 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14219"><span class="ln">14219 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14220"><span class="ln">14220 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14221"><span class="ln">14221 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14222"><span class="ln">14222 </span></a>    &quot;&quot;&quot;</span>
<a name="l14223"><span class="ln">14223 </span></a>
<a name="l14224"><span class="ln">14224 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14225"><span class="ln">14225 </span></a><span class="s2">def </span><span class="s1">gradient</span><span class="s3">(</span>
<a name="l14226"><span class="ln">14226 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14227"><span class="ln">14227 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14228"><span class="ln">14228 </span></a>    <span class="s1">spacing</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">],</span>
<a name="l14229"><span class="ln">14229 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l14230"><span class="ln">14230 </span></a>    <span class="s1">edge_order</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l14231"><span class="ln">14231 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l14232"><span class="ln">14232 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14233"><span class="ln">14233 </span></a>    gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l14234"><span class="ln">14234 </span></a> 
<a name="l14235"><span class="ln">14235 </span></a>    Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l14236"><span class="ln">14236 </span></a>    one or more dimensions using the `second-order accurate central differences method 
<a name="l14237"><span class="ln">14237 </span></a>    &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l14238"><span class="ln">14238 </span></a>    either first or second order estimates at the boundaries. 
<a name="l14239"><span class="ln">14239 </span></a> 
<a name="l14240"><span class="ln">14240 </span></a>    The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l14241"><span class="ln">14241 </span></a>    specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l14242"><span class="ln">14242 </span></a>    to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l14243"><span class="ln">14243 </span></a>    :attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l14244"><span class="ln">14244 </span></a>    :math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l14245"><span class="ln">14245 </span></a> 
<a name="l14246"><span class="ln">14246 </span></a>    When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l14247"><span class="ln">14247 </span></a>    This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l14248"><span class="ln">14248 </span></a> 
<a name="l14249"><span class="ln">14249 </span></a>    The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l14250"><span class="ln">14250 </span></a>    accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l14251"><span class="ln">14251 </span></a>    improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l14252"><span class="ln">14252 </span></a>    is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l14253"><span class="ln">14253 </span></a>    Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l14254"><span class="ln">14254 </span></a>    it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l14255"><span class="ln">14255 </span></a> 
<a name="l14256"><span class="ln">14256 </span></a>    .. math:: 
<a name="l14257"><span class="ln">14257 </span></a>        \begin{aligned} 
<a name="l14258"><span class="ln">14258 </span></a>            f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l14259"><span class="ln">14259 </span></a>            f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l14260"><span class="ln">14260 </span></a>        \end{aligned} 
<a name="l14261"><span class="ln">14261 </span></a> 
<a name="l14262"><span class="ln">14262 </span></a>    Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l14263"><span class="ln">14263 </span></a> 
<a name="l14264"><span class="ln">14264 </span></a>    .. math:: 
<a name="l14265"><span class="ln">14265 </span></a>        f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l14266"><span class="ln">14266 </span></a>              + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l14267"><span class="ln">14267 </span></a> 
<a name="l14268"><span class="ln">14268 </span></a>    .. note:: 
<a name="l14269"><span class="ln">14269 </span></a>        We estimate the gradient of functions in complex domain 
<a name="l14270"><span class="ln">14270 </span></a>        :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l14271"><span class="ln">14271 </span></a> 
<a name="l14272"><span class="ln">14272 </span></a>    The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l14273"><span class="ln">14273 </span></a> 
<a name="l14274"><span class="ln">14274 </span></a>    Args: 
<a name="l14275"><span class="ln">14275 </span></a>        input (``Tensor``): the tensor that represents the values of the function 
<a name="l14276"><span class="ln">14276 </span></a> 
<a name="l14277"><span class="ln">14277 </span></a>    Keyword args: 
<a name="l14278"><span class="ln">14278 </span></a>        spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l14279"><span class="ln">14279 </span></a>            how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l14280"><span class="ln">14280 </span></a>            the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l14281"><span class="ln">14281 </span></a>            indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l14282"><span class="ln">14282 </span></a>            indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l14283"><span class="ln">14283 </span></a>            Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l14284"><span class="ln">14284 </span></a>            the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l14285"><span class="ln">14285 </span></a>            the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l14286"><span class="ln">14286 </span></a> 
<a name="l14287"><span class="ln">14287 </span></a>        dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l14288"><span class="ln">14288 </span></a>            the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l14289"><span class="ln">14289 </span></a>            the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l14290"><span class="ln">14290 </span></a> 
<a name="l14291"><span class="ln">14291 </span></a>        edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l14292"><span class="ln">14292 </span></a>            &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l14293"><span class="ln">14293 </span></a>            `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l14294"><span class="ln">14294 </span></a>            estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l14295"><span class="ln">14295 </span></a> 
<a name="l14296"><span class="ln">14296 </span></a>    Examples:: 
<a name="l14297"><span class="ln">14297 </span></a> 
<a name="l14298"><span class="ln">14298 </span></a>        &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l14299"><span class="ln">14299 </span></a>        &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l14300"><span class="ln">14300 </span></a>        &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l14301"><span class="ln">14301 </span></a>        &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l14302"><span class="ln">14302 </span></a>        (tensor([-3., -2., 2., 5.]),) 
<a name="l14303"><span class="ln">14303 </span></a> 
<a name="l14304"><span class="ln">14304 </span></a>        &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l14305"><span class="ln">14305 </span></a>        &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l14306"><span class="ln">14306 </span></a>        &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l14307"><span class="ln">14307 </span></a>        &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l14308"><span class="ln">14308 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l14309"><span class="ln">14309 </span></a>        &gt;&gt;&gt; torch.gradient(t) 
<a name="l14310"><span class="ln">14310 </span></a>        (tensor([[ 9., 18., 36., 72.], 
<a name="l14311"><span class="ln">14311 </span></a>                 [ 9., 18., 36., 72.]]), 
<a name="l14312"><span class="ln">14312 </span></a>         tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14313"><span class="ln">14313 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l14314"><span class="ln">14314 </span></a> 
<a name="l14315"><span class="ln">14315 </span></a>        &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l14316"><span class="ln">14316 </span></a>        &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l14317"><span class="ln">14317 </span></a>        &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l14318"><span class="ln">14318 </span></a>        &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l14319"><span class="ln">14319 </span></a>        &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l14320"><span class="ln">14320 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l14321"><span class="ln">14321 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14322"><span class="ln">14322 </span></a>                  [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14323"><span class="ln">14323 </span></a>         tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l14324"><span class="ln">14324 </span></a>                  [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l14325"><span class="ln">14325 </span></a>        &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l14326"><span class="ln">14326 </span></a> 
<a name="l14327"><span class="ln">14327 </span></a>        &gt;&gt;&gt; 
<a name="l14328"><span class="ln">14328 </span></a>        &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l14329"><span class="ln">14329 </span></a>        &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l14330"><span class="ln">14330 </span></a>        (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14331"><span class="ln">14331 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l14332"><span class="ln">14332 </span></a> 
<a name="l14333"><span class="ln">14333 </span></a>        &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l14334"><span class="ln">14334 </span></a>        &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l14335"><span class="ln">14335 </span></a>        &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l14336"><span class="ln">14336 </span></a>        &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l14337"><span class="ln">14337 </span></a>        &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l14338"><span class="ln">14338 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l14339"><span class="ln">14339 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14340"><span class="ln">14340 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14341"><span class="ln">14341 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14342"><span class="ln">14342 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14343"><span class="ln">14343 </span></a> 
<a name="l14344"><span class="ln">14344 </span></a>        &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l14345"><span class="ln">14345 </span></a>        &gt;&gt;&gt; # coordinates. 
<a name="l14346"><span class="ln">14346 </span></a>        &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l14347"><span class="ln">14347 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l14348"><span class="ln">14348 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14349"><span class="ln">14349 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14350"><span class="ln">14350 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14351"><span class="ln">14351 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14352"><span class="ln">14352 </span></a>    &quot;&quot;&quot;</span>
<a name="l14353"><span class="ln">14353 </span></a>
<a name="l14354"><span class="ln">14354 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14355"><span class="ln">14355 </span></a><span class="s2">def </span><span class="s1">gradient</span><span class="s3">(</span>
<a name="l14356"><span class="ln">14356 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14357"><span class="ln">14357 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14358"><span class="ln">14358 </span></a>    <span class="s1">spacing</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l14359"><span class="ln">14359 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14360"><span class="ln">14360 </span></a>    <span class="s1">edge_order</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l14361"><span class="ln">14361 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l14362"><span class="ln">14362 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14363"><span class="ln">14363 </span></a>    gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l14364"><span class="ln">14364 </span></a> 
<a name="l14365"><span class="ln">14365 </span></a>    Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l14366"><span class="ln">14366 </span></a>    one or more dimensions using the `second-order accurate central differences method 
<a name="l14367"><span class="ln">14367 </span></a>    &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l14368"><span class="ln">14368 </span></a>    either first or second order estimates at the boundaries. 
<a name="l14369"><span class="ln">14369 </span></a> 
<a name="l14370"><span class="ln">14370 </span></a>    The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l14371"><span class="ln">14371 </span></a>    specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l14372"><span class="ln">14372 </span></a>    to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l14373"><span class="ln">14373 </span></a>    :attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l14374"><span class="ln">14374 </span></a>    :math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l14375"><span class="ln">14375 </span></a> 
<a name="l14376"><span class="ln">14376 </span></a>    When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l14377"><span class="ln">14377 </span></a>    This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l14378"><span class="ln">14378 </span></a> 
<a name="l14379"><span class="ln">14379 </span></a>    The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l14380"><span class="ln">14380 </span></a>    accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l14381"><span class="ln">14381 </span></a>    improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l14382"><span class="ln">14382 </span></a>    is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l14383"><span class="ln">14383 </span></a>    Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l14384"><span class="ln">14384 </span></a>    it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l14385"><span class="ln">14385 </span></a> 
<a name="l14386"><span class="ln">14386 </span></a>    .. math:: 
<a name="l14387"><span class="ln">14387 </span></a>        \begin{aligned} 
<a name="l14388"><span class="ln">14388 </span></a>            f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l14389"><span class="ln">14389 </span></a>            f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l14390"><span class="ln">14390 </span></a>        \end{aligned} 
<a name="l14391"><span class="ln">14391 </span></a> 
<a name="l14392"><span class="ln">14392 </span></a>    Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l14393"><span class="ln">14393 </span></a> 
<a name="l14394"><span class="ln">14394 </span></a>    .. math:: 
<a name="l14395"><span class="ln">14395 </span></a>        f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l14396"><span class="ln">14396 </span></a>              + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l14397"><span class="ln">14397 </span></a> 
<a name="l14398"><span class="ln">14398 </span></a>    .. note:: 
<a name="l14399"><span class="ln">14399 </span></a>        We estimate the gradient of functions in complex domain 
<a name="l14400"><span class="ln">14400 </span></a>        :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l14401"><span class="ln">14401 </span></a> 
<a name="l14402"><span class="ln">14402 </span></a>    The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l14403"><span class="ln">14403 </span></a> 
<a name="l14404"><span class="ln">14404 </span></a>    Args: 
<a name="l14405"><span class="ln">14405 </span></a>        input (``Tensor``): the tensor that represents the values of the function 
<a name="l14406"><span class="ln">14406 </span></a> 
<a name="l14407"><span class="ln">14407 </span></a>    Keyword args: 
<a name="l14408"><span class="ln">14408 </span></a>        spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l14409"><span class="ln">14409 </span></a>            how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l14410"><span class="ln">14410 </span></a>            the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l14411"><span class="ln">14411 </span></a>            indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l14412"><span class="ln">14412 </span></a>            indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l14413"><span class="ln">14413 </span></a>            Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l14414"><span class="ln">14414 </span></a>            the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l14415"><span class="ln">14415 </span></a>            the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l14416"><span class="ln">14416 </span></a> 
<a name="l14417"><span class="ln">14417 </span></a>        dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l14418"><span class="ln">14418 </span></a>            the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l14419"><span class="ln">14419 </span></a>            the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l14420"><span class="ln">14420 </span></a> 
<a name="l14421"><span class="ln">14421 </span></a>        edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l14422"><span class="ln">14422 </span></a>            &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l14423"><span class="ln">14423 </span></a>            `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l14424"><span class="ln">14424 </span></a>            estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l14425"><span class="ln">14425 </span></a> 
<a name="l14426"><span class="ln">14426 </span></a>    Examples:: 
<a name="l14427"><span class="ln">14427 </span></a> 
<a name="l14428"><span class="ln">14428 </span></a>        &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l14429"><span class="ln">14429 </span></a>        &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l14430"><span class="ln">14430 </span></a>        &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l14431"><span class="ln">14431 </span></a>        &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l14432"><span class="ln">14432 </span></a>        (tensor([-3., -2., 2., 5.]),) 
<a name="l14433"><span class="ln">14433 </span></a> 
<a name="l14434"><span class="ln">14434 </span></a>        &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l14435"><span class="ln">14435 </span></a>        &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l14436"><span class="ln">14436 </span></a>        &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l14437"><span class="ln">14437 </span></a>        &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l14438"><span class="ln">14438 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l14439"><span class="ln">14439 </span></a>        &gt;&gt;&gt; torch.gradient(t) 
<a name="l14440"><span class="ln">14440 </span></a>        (tensor([[ 9., 18., 36., 72.], 
<a name="l14441"><span class="ln">14441 </span></a>                 [ 9., 18., 36., 72.]]), 
<a name="l14442"><span class="ln">14442 </span></a>         tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14443"><span class="ln">14443 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l14444"><span class="ln">14444 </span></a> 
<a name="l14445"><span class="ln">14445 </span></a>        &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l14446"><span class="ln">14446 </span></a>        &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l14447"><span class="ln">14447 </span></a>        &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l14448"><span class="ln">14448 </span></a>        &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l14449"><span class="ln">14449 </span></a>        &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l14450"><span class="ln">14450 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l14451"><span class="ln">14451 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14452"><span class="ln">14452 </span></a>                  [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14453"><span class="ln">14453 </span></a>         tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l14454"><span class="ln">14454 </span></a>                  [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l14455"><span class="ln">14455 </span></a>        &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l14456"><span class="ln">14456 </span></a> 
<a name="l14457"><span class="ln">14457 </span></a>        &gt;&gt;&gt; 
<a name="l14458"><span class="ln">14458 </span></a>        &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l14459"><span class="ln">14459 </span></a>        &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l14460"><span class="ln">14460 </span></a>        (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14461"><span class="ln">14461 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l14462"><span class="ln">14462 </span></a> 
<a name="l14463"><span class="ln">14463 </span></a>        &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l14464"><span class="ln">14464 </span></a>        &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l14465"><span class="ln">14465 </span></a>        &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l14466"><span class="ln">14466 </span></a>        &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l14467"><span class="ln">14467 </span></a>        &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l14468"><span class="ln">14468 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l14469"><span class="ln">14469 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14470"><span class="ln">14470 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14471"><span class="ln">14471 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14472"><span class="ln">14472 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14473"><span class="ln">14473 </span></a> 
<a name="l14474"><span class="ln">14474 </span></a>        &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l14475"><span class="ln">14475 </span></a>        &gt;&gt;&gt; # coordinates. 
<a name="l14476"><span class="ln">14476 </span></a>        &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l14477"><span class="ln">14477 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l14478"><span class="ln">14478 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14479"><span class="ln">14479 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14480"><span class="ln">14480 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14481"><span class="ln">14481 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14482"><span class="ln">14482 </span></a>    &quot;&quot;&quot;</span>
<a name="l14483"><span class="ln">14483 </span></a>
<a name="l14484"><span class="ln">14484 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14485"><span class="ln">14485 </span></a><span class="s2">def </span><span class="s1">gradient</span><span class="s3">(</span>
<a name="l14486"><span class="ln">14486 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14487"><span class="ln">14487 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14488"><span class="ln">14488 </span></a>    <span class="s1">spacing</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l14489"><span class="ln">14489 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l14490"><span class="ln">14490 </span></a>    <span class="s1">edge_order</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l14491"><span class="ln">14491 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l14492"><span class="ln">14492 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14493"><span class="ln">14493 </span></a>    gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l14494"><span class="ln">14494 </span></a> 
<a name="l14495"><span class="ln">14495 </span></a>    Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l14496"><span class="ln">14496 </span></a>    one or more dimensions using the `second-order accurate central differences method 
<a name="l14497"><span class="ln">14497 </span></a>    &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l14498"><span class="ln">14498 </span></a>    either first or second order estimates at the boundaries. 
<a name="l14499"><span class="ln">14499 </span></a> 
<a name="l14500"><span class="ln">14500 </span></a>    The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l14501"><span class="ln">14501 </span></a>    specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l14502"><span class="ln">14502 </span></a>    to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l14503"><span class="ln">14503 </span></a>    :attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l14504"><span class="ln">14504 </span></a>    :math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l14505"><span class="ln">14505 </span></a> 
<a name="l14506"><span class="ln">14506 </span></a>    When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l14507"><span class="ln">14507 </span></a>    This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l14508"><span class="ln">14508 </span></a> 
<a name="l14509"><span class="ln">14509 </span></a>    The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l14510"><span class="ln">14510 </span></a>    accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l14511"><span class="ln">14511 </span></a>    improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l14512"><span class="ln">14512 </span></a>    is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l14513"><span class="ln">14513 </span></a>    Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l14514"><span class="ln">14514 </span></a>    it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l14515"><span class="ln">14515 </span></a> 
<a name="l14516"><span class="ln">14516 </span></a>    .. math:: 
<a name="l14517"><span class="ln">14517 </span></a>        \begin{aligned} 
<a name="l14518"><span class="ln">14518 </span></a>            f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l14519"><span class="ln">14519 </span></a>            f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l14520"><span class="ln">14520 </span></a>        \end{aligned} 
<a name="l14521"><span class="ln">14521 </span></a> 
<a name="l14522"><span class="ln">14522 </span></a>    Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l14523"><span class="ln">14523 </span></a> 
<a name="l14524"><span class="ln">14524 </span></a>    .. math:: 
<a name="l14525"><span class="ln">14525 </span></a>        f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l14526"><span class="ln">14526 </span></a>              + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l14527"><span class="ln">14527 </span></a> 
<a name="l14528"><span class="ln">14528 </span></a>    .. note:: 
<a name="l14529"><span class="ln">14529 </span></a>        We estimate the gradient of functions in complex domain 
<a name="l14530"><span class="ln">14530 </span></a>        :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l14531"><span class="ln">14531 </span></a> 
<a name="l14532"><span class="ln">14532 </span></a>    The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l14533"><span class="ln">14533 </span></a> 
<a name="l14534"><span class="ln">14534 </span></a>    Args: 
<a name="l14535"><span class="ln">14535 </span></a>        input (``Tensor``): the tensor that represents the values of the function 
<a name="l14536"><span class="ln">14536 </span></a> 
<a name="l14537"><span class="ln">14537 </span></a>    Keyword args: 
<a name="l14538"><span class="ln">14538 </span></a>        spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l14539"><span class="ln">14539 </span></a>            how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l14540"><span class="ln">14540 </span></a>            the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l14541"><span class="ln">14541 </span></a>            indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l14542"><span class="ln">14542 </span></a>            indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l14543"><span class="ln">14543 </span></a>            Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l14544"><span class="ln">14544 </span></a>            the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l14545"><span class="ln">14545 </span></a>            the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l14546"><span class="ln">14546 </span></a> 
<a name="l14547"><span class="ln">14547 </span></a>        dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l14548"><span class="ln">14548 </span></a>            the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l14549"><span class="ln">14549 </span></a>            the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l14550"><span class="ln">14550 </span></a> 
<a name="l14551"><span class="ln">14551 </span></a>        edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l14552"><span class="ln">14552 </span></a>            &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l14553"><span class="ln">14553 </span></a>            `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l14554"><span class="ln">14554 </span></a>            estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l14555"><span class="ln">14555 </span></a> 
<a name="l14556"><span class="ln">14556 </span></a>    Examples:: 
<a name="l14557"><span class="ln">14557 </span></a> 
<a name="l14558"><span class="ln">14558 </span></a>        &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l14559"><span class="ln">14559 </span></a>        &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l14560"><span class="ln">14560 </span></a>        &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l14561"><span class="ln">14561 </span></a>        &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l14562"><span class="ln">14562 </span></a>        (tensor([-3., -2., 2., 5.]),) 
<a name="l14563"><span class="ln">14563 </span></a> 
<a name="l14564"><span class="ln">14564 </span></a>        &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l14565"><span class="ln">14565 </span></a>        &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l14566"><span class="ln">14566 </span></a>        &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l14567"><span class="ln">14567 </span></a>        &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l14568"><span class="ln">14568 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l14569"><span class="ln">14569 </span></a>        &gt;&gt;&gt; torch.gradient(t) 
<a name="l14570"><span class="ln">14570 </span></a>        (tensor([[ 9., 18., 36., 72.], 
<a name="l14571"><span class="ln">14571 </span></a>                 [ 9., 18., 36., 72.]]), 
<a name="l14572"><span class="ln">14572 </span></a>         tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14573"><span class="ln">14573 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l14574"><span class="ln">14574 </span></a> 
<a name="l14575"><span class="ln">14575 </span></a>        &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l14576"><span class="ln">14576 </span></a>        &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l14577"><span class="ln">14577 </span></a>        &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l14578"><span class="ln">14578 </span></a>        &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l14579"><span class="ln">14579 </span></a>        &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l14580"><span class="ln">14580 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l14581"><span class="ln">14581 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14582"><span class="ln">14582 </span></a>                  [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14583"><span class="ln">14583 </span></a>         tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l14584"><span class="ln">14584 </span></a>                  [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l14585"><span class="ln">14585 </span></a>        &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l14586"><span class="ln">14586 </span></a> 
<a name="l14587"><span class="ln">14587 </span></a>        &gt;&gt;&gt; 
<a name="l14588"><span class="ln">14588 </span></a>        &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l14589"><span class="ln">14589 </span></a>        &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l14590"><span class="ln">14590 </span></a>        (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14591"><span class="ln">14591 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l14592"><span class="ln">14592 </span></a> 
<a name="l14593"><span class="ln">14593 </span></a>        &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l14594"><span class="ln">14594 </span></a>        &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l14595"><span class="ln">14595 </span></a>        &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l14596"><span class="ln">14596 </span></a>        &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l14597"><span class="ln">14597 </span></a>        &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l14598"><span class="ln">14598 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l14599"><span class="ln">14599 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14600"><span class="ln">14600 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14601"><span class="ln">14601 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14602"><span class="ln">14602 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14603"><span class="ln">14603 </span></a> 
<a name="l14604"><span class="ln">14604 </span></a>        &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l14605"><span class="ln">14605 </span></a>        &gt;&gt;&gt; # coordinates. 
<a name="l14606"><span class="ln">14606 </span></a>        &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l14607"><span class="ln">14607 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l14608"><span class="ln">14608 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14609"><span class="ln">14609 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14610"><span class="ln">14610 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14611"><span class="ln">14611 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14612"><span class="ln">14612 </span></a>    &quot;&quot;&quot;</span>
<a name="l14613"><span class="ln">14613 </span></a>
<a name="l14614"><span class="ln">14614 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14615"><span class="ln">14615 </span></a><span class="s2">def </span><span class="s1">gradient</span><span class="s3">(</span>
<a name="l14616"><span class="ln">14616 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14617"><span class="ln">14617 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14618"><span class="ln">14618 </span></a>    <span class="s1">spacing</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l14619"><span class="ln">14619 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l14620"><span class="ln">14620 </span></a>    <span class="s1">edge_order</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l14621"><span class="ln">14621 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l14622"><span class="ln">14622 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14623"><span class="ln">14623 </span></a>    gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l14624"><span class="ln">14624 </span></a> 
<a name="l14625"><span class="ln">14625 </span></a>    Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l14626"><span class="ln">14626 </span></a>    one or more dimensions using the `second-order accurate central differences method 
<a name="l14627"><span class="ln">14627 </span></a>    &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l14628"><span class="ln">14628 </span></a>    either first or second order estimates at the boundaries. 
<a name="l14629"><span class="ln">14629 </span></a> 
<a name="l14630"><span class="ln">14630 </span></a>    The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l14631"><span class="ln">14631 </span></a>    specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l14632"><span class="ln">14632 </span></a>    to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l14633"><span class="ln">14633 </span></a>    :attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l14634"><span class="ln">14634 </span></a>    :math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l14635"><span class="ln">14635 </span></a> 
<a name="l14636"><span class="ln">14636 </span></a>    When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l14637"><span class="ln">14637 </span></a>    This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l14638"><span class="ln">14638 </span></a> 
<a name="l14639"><span class="ln">14639 </span></a>    The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l14640"><span class="ln">14640 </span></a>    accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l14641"><span class="ln">14641 </span></a>    improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l14642"><span class="ln">14642 </span></a>    is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l14643"><span class="ln">14643 </span></a>    Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l14644"><span class="ln">14644 </span></a>    it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l14645"><span class="ln">14645 </span></a> 
<a name="l14646"><span class="ln">14646 </span></a>    .. math:: 
<a name="l14647"><span class="ln">14647 </span></a>        \begin{aligned} 
<a name="l14648"><span class="ln">14648 </span></a>            f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l14649"><span class="ln">14649 </span></a>            f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l14650"><span class="ln">14650 </span></a>        \end{aligned} 
<a name="l14651"><span class="ln">14651 </span></a> 
<a name="l14652"><span class="ln">14652 </span></a>    Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l14653"><span class="ln">14653 </span></a> 
<a name="l14654"><span class="ln">14654 </span></a>    .. math:: 
<a name="l14655"><span class="ln">14655 </span></a>        f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l14656"><span class="ln">14656 </span></a>              + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l14657"><span class="ln">14657 </span></a> 
<a name="l14658"><span class="ln">14658 </span></a>    .. note:: 
<a name="l14659"><span class="ln">14659 </span></a>        We estimate the gradient of functions in complex domain 
<a name="l14660"><span class="ln">14660 </span></a>        :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l14661"><span class="ln">14661 </span></a> 
<a name="l14662"><span class="ln">14662 </span></a>    The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l14663"><span class="ln">14663 </span></a> 
<a name="l14664"><span class="ln">14664 </span></a>    Args: 
<a name="l14665"><span class="ln">14665 </span></a>        input (``Tensor``): the tensor that represents the values of the function 
<a name="l14666"><span class="ln">14666 </span></a> 
<a name="l14667"><span class="ln">14667 </span></a>    Keyword args: 
<a name="l14668"><span class="ln">14668 </span></a>        spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l14669"><span class="ln">14669 </span></a>            how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l14670"><span class="ln">14670 </span></a>            the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l14671"><span class="ln">14671 </span></a>            indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l14672"><span class="ln">14672 </span></a>            indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l14673"><span class="ln">14673 </span></a>            Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l14674"><span class="ln">14674 </span></a>            the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l14675"><span class="ln">14675 </span></a>            the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l14676"><span class="ln">14676 </span></a> 
<a name="l14677"><span class="ln">14677 </span></a>        dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l14678"><span class="ln">14678 </span></a>            the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l14679"><span class="ln">14679 </span></a>            the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l14680"><span class="ln">14680 </span></a> 
<a name="l14681"><span class="ln">14681 </span></a>        edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l14682"><span class="ln">14682 </span></a>            &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l14683"><span class="ln">14683 </span></a>            `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l14684"><span class="ln">14684 </span></a>            estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l14685"><span class="ln">14685 </span></a> 
<a name="l14686"><span class="ln">14686 </span></a>    Examples:: 
<a name="l14687"><span class="ln">14687 </span></a> 
<a name="l14688"><span class="ln">14688 </span></a>        &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l14689"><span class="ln">14689 </span></a>        &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l14690"><span class="ln">14690 </span></a>        &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l14691"><span class="ln">14691 </span></a>        &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l14692"><span class="ln">14692 </span></a>        (tensor([-3., -2., 2., 5.]),) 
<a name="l14693"><span class="ln">14693 </span></a> 
<a name="l14694"><span class="ln">14694 </span></a>        &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l14695"><span class="ln">14695 </span></a>        &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l14696"><span class="ln">14696 </span></a>        &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l14697"><span class="ln">14697 </span></a>        &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l14698"><span class="ln">14698 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l14699"><span class="ln">14699 </span></a>        &gt;&gt;&gt; torch.gradient(t) 
<a name="l14700"><span class="ln">14700 </span></a>        (tensor([[ 9., 18., 36., 72.], 
<a name="l14701"><span class="ln">14701 </span></a>                 [ 9., 18., 36., 72.]]), 
<a name="l14702"><span class="ln">14702 </span></a>         tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14703"><span class="ln">14703 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l14704"><span class="ln">14704 </span></a> 
<a name="l14705"><span class="ln">14705 </span></a>        &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l14706"><span class="ln">14706 </span></a>        &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l14707"><span class="ln">14707 </span></a>        &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l14708"><span class="ln">14708 </span></a>        &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l14709"><span class="ln">14709 </span></a>        &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l14710"><span class="ln">14710 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l14711"><span class="ln">14711 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14712"><span class="ln">14712 </span></a>                  [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14713"><span class="ln">14713 </span></a>         tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l14714"><span class="ln">14714 </span></a>                  [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l14715"><span class="ln">14715 </span></a>        &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l14716"><span class="ln">14716 </span></a> 
<a name="l14717"><span class="ln">14717 </span></a>        &gt;&gt;&gt; 
<a name="l14718"><span class="ln">14718 </span></a>        &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l14719"><span class="ln">14719 </span></a>        &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l14720"><span class="ln">14720 </span></a>        (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14721"><span class="ln">14721 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l14722"><span class="ln">14722 </span></a> 
<a name="l14723"><span class="ln">14723 </span></a>        &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l14724"><span class="ln">14724 </span></a>        &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l14725"><span class="ln">14725 </span></a>        &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l14726"><span class="ln">14726 </span></a>        &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l14727"><span class="ln">14727 </span></a>        &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l14728"><span class="ln">14728 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l14729"><span class="ln">14729 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14730"><span class="ln">14730 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14731"><span class="ln">14731 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14732"><span class="ln">14732 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14733"><span class="ln">14733 </span></a> 
<a name="l14734"><span class="ln">14734 </span></a>        &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l14735"><span class="ln">14735 </span></a>        &gt;&gt;&gt; # coordinates. 
<a name="l14736"><span class="ln">14736 </span></a>        &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l14737"><span class="ln">14737 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l14738"><span class="ln">14738 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14739"><span class="ln">14739 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14740"><span class="ln">14740 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14741"><span class="ln">14741 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14742"><span class="ln">14742 </span></a>    &quot;&quot;&quot;</span>
<a name="l14743"><span class="ln">14743 </span></a>
<a name="l14744"><span class="ln">14744 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14745"><span class="ln">14745 </span></a><span class="s2">def </span><span class="s1">gradient</span><span class="s3">(</span>
<a name="l14746"><span class="ln">14746 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14747"><span class="ln">14747 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14748"><span class="ln">14748 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l14749"><span class="ln">14749 </span></a>    <span class="s1">edge_order</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l14750"><span class="ln">14750 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l14751"><span class="ln">14751 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14752"><span class="ln">14752 </span></a>    gradient(input, *, spacing=1, dim=None, edge_order=1) -&gt; List of Tensors 
<a name="l14753"><span class="ln">14753 </span></a> 
<a name="l14754"><span class="ln">14754 </span></a>    Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in 
<a name="l14755"><span class="ln">14755 </span></a>    one or more dimensions using the `second-order accurate central differences method 
<a name="l14756"><span class="ln">14756 </span></a>    &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ and 
<a name="l14757"><span class="ln">14757 </span></a>    either first or second order estimates at the boundaries. 
<a name="l14758"><span class="ln">14758 </span></a> 
<a name="l14759"><span class="ln">14759 </span></a>    The gradient of :math:`g` is estimated using samples. By default, when :attr:`spacing` is not 
<a name="l14760"><span class="ln">14760 </span></a>    specified, the samples are entirely described by :attr:`input`, and the mapping of input coordinates 
<a name="l14761"><span class="ln">14761 </span></a>    to an output is the same as the tensor's mapping of indices to values. For example, for a three-dimensional 
<a name="l14762"><span class="ln">14762 </span></a>    :attr:`input` the function described is :math:`g : \mathbb{R}^3 \rightarrow \mathbb{R}`, and 
<a name="l14763"><span class="ln">14763 </span></a>    :math:`g(1, 2, 3)\ == input[1, 2, 3]`. 
<a name="l14764"><span class="ln">14764 </span></a> 
<a name="l14765"><span class="ln">14765 </span></a>    When :attr:`spacing` is specified, it modifies the relationship between :attr:`input` and input coordinates. 
<a name="l14766"><span class="ln">14766 </span></a>    This is detailed in the &quot;Keyword Arguments&quot; section below. 
<a name="l14767"><span class="ln">14767 </span></a> 
<a name="l14768"><span class="ln">14768 </span></a>    The gradient is estimated by estimating each partial derivative of :math:`g` independently. This estimation is 
<a name="l14769"><span class="ln">14769 </span></a>    accurate if :math:`g` is in :math:`C^3` (it has at least 3 continuous derivatives), and the estimation can be 
<a name="l14770"><span class="ln">14770 </span></a>    improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative 
<a name="l14771"><span class="ln">14771 </span></a>    is estimated using `Taylor's theorem with remainder &lt;https://en.wikipedia.org/wiki/Taylor%27s_theorem&gt;`_. 
<a name="l14772"><span class="ln">14772 </span></a>    Letting :math:`x` be an interior point with :math:`x-h_l` and :math:`x+h_r` be points neighboring 
<a name="l14773"><span class="ln">14773 </span></a>    it to the left and right respectively, :math:`f(x+h_r)` and :math:`f(x-h_l)` can be estimated using: 
<a name="l14774"><span class="ln">14774 </span></a> 
<a name="l14775"><span class="ln">14775 </span></a>    .. math:: 
<a name="l14776"><span class="ln">14776 </span></a>        \begin{aligned} 
<a name="l14777"><span class="ln">14777 </span></a>            f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2  \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ 
<a name="l14778"><span class="ln">14778 </span></a>            f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2  \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ 
<a name="l14779"><span class="ln">14779 </span></a>        \end{aligned} 
<a name="l14780"><span class="ln">14780 </span></a> 
<a name="l14781"><span class="ln">14781 </span></a>    Using the fact that :math:`f \in C^3` and solving the linear system, we derive: 
<a name="l14782"><span class="ln">14782 </span></a> 
<a name="l14783"><span class="ln">14783 </span></a>    .. math:: 
<a name="l14784"><span class="ln">14784 </span></a>        f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) 
<a name="l14785"><span class="ln">14785 </span></a>              + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } 
<a name="l14786"><span class="ln">14786 </span></a> 
<a name="l14787"><span class="ln">14787 </span></a>    .. note:: 
<a name="l14788"><span class="ln">14788 </span></a>        We estimate the gradient of functions in complex domain 
<a name="l14789"><span class="ln">14789 </span></a>        :math:`g : \mathbb{C}^n \rightarrow \mathbb{C}` in the same way. 
<a name="l14790"><span class="ln">14790 </span></a> 
<a name="l14791"><span class="ln">14791 </span></a>    The value of each partial derivative at the boundary points is computed differently. See edge_order below. 
<a name="l14792"><span class="ln">14792 </span></a> 
<a name="l14793"><span class="ln">14793 </span></a>    Args: 
<a name="l14794"><span class="ln">14794 </span></a>        input (``Tensor``): the tensor that represents the values of the function 
<a name="l14795"><span class="ln">14795 </span></a> 
<a name="l14796"><span class="ln">14796 </span></a>    Keyword args: 
<a name="l14797"><span class="ln">14797 </span></a>        spacing (``scalar``, ``list of scalar``, ``list of Tensor``, optional): :attr:`spacing` can be used to modify 
<a name="l14798"><span class="ln">14798 </span></a>            how the :attr:`input` tensor's indices relate to sample coordinates. If :attr:`spacing` is a scalar then 
<a name="l14799"><span class="ln">14799 </span></a>            the indices are multiplied by the scalar to produce the coordinates. For example, if :attr:`spacing=2` the 
<a name="l14800"><span class="ln">14800 </span></a>            indices (1, 2, 3) become coordinates (2, 4, 6). If :attr:`spacing` is a list of scalars then the corresponding 
<a name="l14801"><span class="ln">14801 </span></a>            indices are multiplied. For example, if :attr:`spacing=(2, -1, 3)` the indices (1, 2, 3) become coordinates (2, -2, 9). 
<a name="l14802"><span class="ln">14802 </span></a>            Finally, if :attr:`spacing` is a list of one-dimensional tensors then each tensor specifies the coordinates for 
<a name="l14803"><span class="ln">14803 </span></a>            the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then 
<a name="l14804"><span class="ln">14804 </span></a>            the coordinates are (t0[1], t1[2], t2[3]) 
<a name="l14805"><span class="ln">14805 </span></a> 
<a name="l14806"><span class="ln">14806 </span></a>        dim (``int``, ``list of int``, optional): the dimension or dimensions to approximate the gradient over.  By default 
<a name="l14807"><span class="ln">14807 </span></a>            the partial  gradient in every dimension is computed. Note that when :attr:`dim` is  specified the elements of 
<a name="l14808"><span class="ln">14808 </span></a>            the :attr:`spacing` argument must correspond with the specified dims.&quot; 
<a name="l14809"><span class="ln">14809 </span></a> 
<a name="l14810"><span class="ln">14810 </span></a>        edge_order (``int``, optional): 1 or 2, for `first-order 
<a name="l14811"><span class="ln">14811 </span></a>            &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ or 
<a name="l14812"><span class="ln">14812 </span></a>            `second-order &lt;https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf&gt;`_ 
<a name="l14813"><span class="ln">14813 </span></a>            estimation of the boundary (&quot;edge&quot;) values, respectively. 
<a name="l14814"><span class="ln">14814 </span></a> 
<a name="l14815"><span class="ln">14815 </span></a>    Examples:: 
<a name="l14816"><span class="ln">14816 </span></a> 
<a name="l14817"><span class="ln">14817 </span></a>        &gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4] 
<a name="l14818"><span class="ln">14818 </span></a>        &gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),) 
<a name="l14819"><span class="ln">14819 </span></a>        &gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], ) 
<a name="l14820"><span class="ln">14820 </span></a>        &gt;&gt;&gt; torch.gradient(values, spacing = coordinates) 
<a name="l14821"><span class="ln">14821 </span></a>        (tensor([-3., -2., 2., 5.]),) 
<a name="l14822"><span class="ln">14822 </span></a> 
<a name="l14823"><span class="ln">14823 </span></a>        &gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are 
<a name="l14824"><span class="ln">14824 </span></a>        &gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost 
<a name="l14825"><span class="ln">14825 </span></a>        &gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates 
<a name="l14826"><span class="ln">14826 </span></a>        &gt;&gt;&gt; # partial derivative for both dimensions. 
<a name="l14827"><span class="ln">14827 </span></a>        &gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]]) 
<a name="l14828"><span class="ln">14828 </span></a>        &gt;&gt;&gt; torch.gradient(t) 
<a name="l14829"><span class="ln">14829 </span></a>        (tensor([[ 9., 18., 36., 72.], 
<a name="l14830"><span class="ln">14830 </span></a>                 [ 9., 18., 36., 72.]]), 
<a name="l14831"><span class="ln">14831 </span></a>         tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14832"><span class="ln">14832 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]])) 
<a name="l14833"><span class="ln">14833 </span></a> 
<a name="l14834"><span class="ln">14834 </span></a>        &gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices 
<a name="l14835"><span class="ln">14835 </span></a>        &gt;&gt;&gt; # and input coordinates by multiplying the indices to find the 
<a name="l14836"><span class="ln">14836 </span></a>        &gt;&gt;&gt; # coordinates. For example, below the indices of the innermost 
<a name="l14837"><span class="ln">14837 </span></a>        &gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of 
<a name="l14838"><span class="ln">14838 </span></a>        &gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2]. 
<a name="l14839"><span class="ln">14839 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1]) 
<a name="l14840"><span class="ln">14840 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14841"><span class="ln">14841 </span></a>                  [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14842"><span class="ln">14842 </span></a>         tensor([[ 0.5000, 0.7500, 1.5000, 2.0000], 
<a name="l14843"><span class="ln">14843 </span></a>                  [ 5.0000, 7.5000, 15.0000, 20.0000]])) 
<a name="l14844"><span class="ln">14844 </span></a>        &gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients. 
<a name="l14845"><span class="ln">14845 </span></a> 
<a name="l14846"><span class="ln">14846 </span></a>        &gt;&gt;&gt; 
<a name="l14847"><span class="ln">14847 </span></a>        &gt;&gt;&gt; # Estimates only the partial derivative for dimension 1 
<a name="l14848"><span class="ln">14848 </span></a>        &gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.) 
<a name="l14849"><span class="ln">14849 </span></a>        (tensor([[ 1.0000, 1.5000, 3.0000, 4.0000], 
<a name="l14850"><span class="ln">14850 </span></a>                 [10.0000, 15.0000, 30.0000, 40.0000]]),) 
<a name="l14851"><span class="ln">14851 </span></a> 
<a name="l14852"><span class="ln">14852 </span></a>        &gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor 
<a name="l14853"><span class="ln">14853 </span></a>        &gt;&gt;&gt; # indices and input coordinates changes based on dimension. 
<a name="l14854"><span class="ln">14854 </span></a>        &gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate 
<a name="l14855"><span class="ln">14855 </span></a>        &gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension 
<a name="l14856"><span class="ln">14856 </span></a>        &gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2]. 
<a name="l14857"><span class="ln">14857 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.]) 
<a name="l14858"><span class="ln">14858 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14859"><span class="ln">14859 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14860"><span class="ln">14860 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14861"><span class="ln">14861 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14862"><span class="ln">14862 </span></a> 
<a name="l14863"><span class="ln">14863 </span></a>        &gt;&gt;&gt; # The following example is a replication of the previous one with explicit 
<a name="l14864"><span class="ln">14864 </span></a>        &gt;&gt;&gt; # coordinates. 
<a name="l14865"><span class="ln">14865 </span></a>        &gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9])) 
<a name="l14866"><span class="ln">14866 </span></a>        &gt;&gt;&gt; torch.gradient(t, spacing = coords) 
<a name="l14867"><span class="ln">14867 </span></a>        (tensor([[ 4.5000, 9.0000, 18.0000, 36.0000], 
<a name="l14868"><span class="ln">14868 </span></a>                 [ 4.5000, 9.0000, 18.0000, 36.0000]]), 
<a name="l14869"><span class="ln">14869 </span></a>         tensor([[ 0.3333, 0.5000, 1.0000, 1.3333], 
<a name="l14870"><span class="ln">14870 </span></a>                 [ 3.3333, 5.0000, 10.0000, 13.3333]])) 
<a name="l14871"><span class="ln">14871 </span></a>    &quot;&quot;&quot;</span>
<a name="l14872"><span class="ln">14872 </span></a>
<a name="l14873"><span class="ln">14873 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14874"><span class="ln">14874 </span></a><span class="s2">def </span><span class="s1">greater</span><span class="s3">(</span>
<a name="l14875"><span class="ln">14875 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14876"><span class="ln">14876 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14877"><span class="ln">14877 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14878"><span class="ln">14878 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14879"><span class="ln">14879 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l14880"><span class="ln">14880 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14881"><span class="ln">14881 </span></a>    greater(input, other, *, out=None) -&gt; Tensor 
<a name="l14882"><span class="ln">14882 </span></a> 
<a name="l14883"><span class="ln">14883 </span></a>    Alias for :func:`torch.gt`. 
<a name="l14884"><span class="ln">14884 </span></a>    &quot;&quot;&quot;</span>
<a name="l14885"><span class="ln">14885 </span></a>
<a name="l14886"><span class="ln">14886 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14887"><span class="ln">14887 </span></a><span class="s2">def </span><span class="s1">greater</span><span class="s3">(</span>
<a name="l14888"><span class="ln">14888 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14889"><span class="ln">14889 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l14890"><span class="ln">14890 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14891"><span class="ln">14891 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14892"><span class="ln">14892 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l14893"><span class="ln">14893 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14894"><span class="ln">14894 </span></a>    greater(input, other, *, out=None) -&gt; Tensor 
<a name="l14895"><span class="ln">14895 </span></a> 
<a name="l14896"><span class="ln">14896 </span></a>    Alias for :func:`torch.gt`. 
<a name="l14897"><span class="ln">14897 </span></a>    &quot;&quot;&quot;</span>
<a name="l14898"><span class="ln">14898 </span></a>
<a name="l14899"><span class="ln">14899 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14900"><span class="ln">14900 </span></a><span class="s2">def </span><span class="s1">greater_equal</span><span class="s3">(</span>
<a name="l14901"><span class="ln">14901 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14902"><span class="ln">14902 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14903"><span class="ln">14903 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14904"><span class="ln">14904 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14905"><span class="ln">14905 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l14906"><span class="ln">14906 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14907"><span class="ln">14907 </span></a>    greater_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l14908"><span class="ln">14908 </span></a> 
<a name="l14909"><span class="ln">14909 </span></a>    Alias for :func:`torch.ge`. 
<a name="l14910"><span class="ln">14910 </span></a>    &quot;&quot;&quot;</span>
<a name="l14911"><span class="ln">14911 </span></a>
<a name="l14912"><span class="ln">14912 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14913"><span class="ln">14913 </span></a><span class="s2">def </span><span class="s1">greater_equal</span><span class="s3">(</span>
<a name="l14914"><span class="ln">14914 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14915"><span class="ln">14915 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l14916"><span class="ln">14916 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14917"><span class="ln">14917 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14918"><span class="ln">14918 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l14919"><span class="ln">14919 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14920"><span class="ln">14920 </span></a>    greater_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l14921"><span class="ln">14921 </span></a> 
<a name="l14922"><span class="ln">14922 </span></a>    Alias for :func:`torch.ge`. 
<a name="l14923"><span class="ln">14923 </span></a>    &quot;&quot;&quot;</span>
<a name="l14924"><span class="ln">14924 </span></a>
<a name="l14925"><span class="ln">14925 </span></a><span class="s2">def </span><span class="s1">grid_sampler</span><span class="s3">(</span>
<a name="l14926"><span class="ln">14926 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14927"><span class="ln">14927 </span></a>    <span class="s1">grid</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14928"><span class="ln">14928 </span></a>    <span class="s1">interpolation_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14929"><span class="ln">14929 </span></a>    <span class="s1">padding_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14930"><span class="ln">14930 </span></a>    <span class="s1">align_corners</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14931"><span class="ln">14931 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l14932"><span class="ln">14932 </span></a><span class="s2">def </span><span class="s1">grid_sampler_2d</span><span class="s3">(</span>
<a name="l14933"><span class="ln">14933 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14934"><span class="ln">14934 </span></a>    <span class="s1">grid</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14935"><span class="ln">14935 </span></a>    <span class="s1">interpolation_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14936"><span class="ln">14936 </span></a>    <span class="s1">padding_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14937"><span class="ln">14937 </span></a>    <span class="s1">align_corners</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14938"><span class="ln">14938 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l14939"><span class="ln">14939 </span></a><span class="s2">def </span><span class="s1">grid_sampler_3d</span><span class="s3">(</span>
<a name="l14940"><span class="ln">14940 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14941"><span class="ln">14941 </span></a>    <span class="s1">grid</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14942"><span class="ln">14942 </span></a>    <span class="s1">interpolation_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14943"><span class="ln">14943 </span></a>    <span class="s1">padding_mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14944"><span class="ln">14944 </span></a>    <span class="s1">align_corners</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14945"><span class="ln">14945 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l14946"><span class="ln">14946 </span></a><span class="s2">def </span><span class="s1">group_norm</span><span class="s3">(</span>
<a name="l14947"><span class="ln">14947 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14948"><span class="ln">14948 </span></a>    <span class="s1">num_groups</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14949"><span class="ln">14949 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14950"><span class="ln">14950 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14951"><span class="ln">14951 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-05</span><span class="s3">,</span>
<a name="l14952"><span class="ln">14952 </span></a>    <span class="s1">cudnn_enabled</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l14953"><span class="ln">14953 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l14954"><span class="ln">14954 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14955"><span class="ln">14955 </span></a><span class="s2">def </span><span class="s1">gru</span><span class="s3">(</span>
<a name="l14956"><span class="ln">14956 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14957"><span class="ln">14957 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14958"><span class="ln">14958 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14959"><span class="ln">14959 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l14960"><span class="ln">14960 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14961"><span class="ln">14961 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14962"><span class="ln">14962 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l14963"><span class="ln">14963 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14964"><span class="ln">14964 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14965"><span class="ln">14965 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l14966"><span class="ln">14966 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14967"><span class="ln">14967 </span></a><span class="s2">def </span><span class="s1">gru</span><span class="s3">(</span>
<a name="l14968"><span class="ln">14968 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14969"><span class="ln">14969 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14970"><span class="ln">14970 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l14971"><span class="ln">14971 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14972"><span class="ln">14972 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l14973"><span class="ln">14973 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l14974"><span class="ln">14974 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14975"><span class="ln">14975 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14976"><span class="ln">14976 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l14977"><span class="ln">14977 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l14978"><span class="ln">14978 </span></a><span class="s2">def </span><span class="s1">gru_cell</span><span class="s3">(</span>
<a name="l14979"><span class="ln">14979 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14980"><span class="ln">14980 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14981"><span class="ln">14981 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14982"><span class="ln">14982 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14983"><span class="ln">14983 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14984"><span class="ln">14984 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14985"><span class="ln">14985 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l14986"><span class="ln">14986 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l14987"><span class="ln">14987 </span></a><span class="s2">def </span><span class="s1">gt</span><span class="s3">(</span>
<a name="l14988"><span class="ln">14988 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14989"><span class="ln">14989 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l14990"><span class="ln">14990 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l14991"><span class="ln">14991 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l14992"><span class="ln">14992 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l14993"><span class="ln">14993 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l14994"><span class="ln">14994 </span></a>    gt(input, other, *, out=None) -&gt; Tensor 
<a name="l14995"><span class="ln">14995 </span></a> 
<a name="l14996"><span class="ln">14996 </span></a>    Computes :math:`\text{input} &gt; \text{other}` element-wise. 
<a name="l14997"><span class="ln">14997 </span></a> 
<a name="l14998"><span class="ln">14998 </span></a> 
<a name="l14999"><span class="ln">14999 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l15000"><span class="ln">15000 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l15001"><span class="ln">15001 </span></a> 
<a name="l15002"><span class="ln">15002 </span></a>    Args: 
<a name="l15003"><span class="ln">15003 </span></a>        input (Tensor): the tensor to compare 
<a name="l15004"><span class="ln">15004 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l15005"><span class="ln">15005 </span></a> 
<a name="l15006"><span class="ln">15006 </span></a>    Keyword args: 
<a name="l15007"><span class="ln">15007 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l15008"><span class="ln">15008 </span></a> 
<a name="l15009"><span class="ln">15009 </span></a>    Returns: 
<a name="l15010"><span class="ln">15010 </span></a>        A boolean tensor that is True where :attr:`input` is greater than :attr:`other` and False elsewhere 
<a name="l15011"><span class="ln">15011 </span></a> 
<a name="l15012"><span class="ln">15012 </span></a>    Example:: 
<a name="l15013"><span class="ln">15013 </span></a> 
<a name="l15014"><span class="ln">15014 </span></a>        &gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l15015"><span class="ln">15015 </span></a>        tensor([[False, True], [False, False]]) 
<a name="l15016"><span class="ln">15016 </span></a>    &quot;&quot;&quot;</span>
<a name="l15017"><span class="ln">15017 </span></a>
<a name="l15018"><span class="ln">15018 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15019"><span class="ln">15019 </span></a><span class="s2">def </span><span class="s1">gt</span><span class="s3">(</span>
<a name="l15020"><span class="ln">15020 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15021"><span class="ln">15021 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l15022"><span class="ln">15022 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15023"><span class="ln">15023 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15024"><span class="ln">15024 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15025"><span class="ln">15025 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15026"><span class="ln">15026 </span></a>    gt(input, other, *, out=None) -&gt; Tensor 
<a name="l15027"><span class="ln">15027 </span></a> 
<a name="l15028"><span class="ln">15028 </span></a>    Computes :math:`\text{input} &gt; \text{other}` element-wise. 
<a name="l15029"><span class="ln">15029 </span></a> 
<a name="l15030"><span class="ln">15030 </span></a> 
<a name="l15031"><span class="ln">15031 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l15032"><span class="ln">15032 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l15033"><span class="ln">15033 </span></a> 
<a name="l15034"><span class="ln">15034 </span></a>    Args: 
<a name="l15035"><span class="ln">15035 </span></a>        input (Tensor): the tensor to compare 
<a name="l15036"><span class="ln">15036 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l15037"><span class="ln">15037 </span></a> 
<a name="l15038"><span class="ln">15038 </span></a>    Keyword args: 
<a name="l15039"><span class="ln">15039 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l15040"><span class="ln">15040 </span></a> 
<a name="l15041"><span class="ln">15041 </span></a>    Returns: 
<a name="l15042"><span class="ln">15042 </span></a>        A boolean tensor that is True where :attr:`input` is greater than :attr:`other` and False elsewhere 
<a name="l15043"><span class="ln">15043 </span></a> 
<a name="l15044"><span class="ln">15044 </span></a>    Example:: 
<a name="l15045"><span class="ln">15045 </span></a> 
<a name="l15046"><span class="ln">15046 </span></a>        &gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l15047"><span class="ln">15047 </span></a>        tensor([[False, True], [False, False]]) 
<a name="l15048"><span class="ln">15048 </span></a>    &quot;&quot;&quot;</span>
<a name="l15049"><span class="ln">15049 </span></a>
<a name="l15050"><span class="ln">15050 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15051"><span class="ln">15051 </span></a><span class="s2">def </span><span class="s1">hamming_window</span><span class="s3">(</span>
<a name="l15052"><span class="ln">15052 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l15053"><span class="ln">15053 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15054"><span class="ln">15054 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15055"><span class="ln">15055 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15056"><span class="ln">15056 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15057"><span class="ln">15057 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15058"><span class="ln">15058 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15059"><span class="ln">15059 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15060"><span class="ln">15060 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15061"><span class="ln">15061 </span></a>    hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l15062"><span class="ln">15062 </span></a> 
<a name="l15063"><span class="ln">15063 </span></a>    Hamming window function. 
<a name="l15064"><span class="ln">15064 </span></a> 
<a name="l15065"><span class="ln">15065 </span></a>    .. math:: 
<a name="l15066"><span class="ln">15066 </span></a>        w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right), 
<a name="l15067"><span class="ln">15067 </span></a> 
<a name="l15068"><span class="ln">15068 </span></a>    where :math:`N` is the full window size. 
<a name="l15069"><span class="ln">15069 </span></a> 
<a name="l15070"><span class="ln">15070 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l15071"><span class="ln">15071 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l15072"><span class="ln">15072 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l15073"><span class="ln">15073 </span></a>    ready to be used as a periodic window with functions like 
<a name="l15074"><span class="ln">15074 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l15075"><span class="ln">15075 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l15076"><span class="ln">15076 </span></a>    ``torch.hamming_window(L, periodic=True)`` equal to 
<a name="l15077"><span class="ln">15077 </span></a>    ``torch.hamming_window(L + 1, periodic=False)[:-1])``. 
<a name="l15078"><span class="ln">15078 </span></a> 
<a name="l15079"><span class="ln">15079 </span></a>    .. note:: 
<a name="l15080"><span class="ln">15080 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l15081"><span class="ln">15081 </span></a> 
<a name="l15082"><span class="ln">15082 </span></a>    .. note:: 
<a name="l15083"><span class="ln">15083 </span></a>        This is a generalized version of :meth:`torch.hann_window`. 
<a name="l15084"><span class="ln">15084 </span></a> 
<a name="l15085"><span class="ln">15085 </span></a>    Arguments: 
<a name="l15086"><span class="ln">15086 </span></a>        window_length (int): the size of returned window 
<a name="l15087"><span class="ln">15087 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l15088"><span class="ln">15088 </span></a>            function. If False, return a symmetric window. 
<a name="l15089"><span class="ln">15089 </span></a>        alpha (float, optional): The coefficient :math:`\alpha` in the equation above 
<a name="l15090"><span class="ln">15090 </span></a>        beta (float, optional): The coefficient :math:`\beta` in the equation above 
<a name="l15091"><span class="ln">15091 </span></a> 
<a name="l15092"><span class="ln">15092 </span></a>    Keyword args: 
<a name="l15093"><span class="ln">15093 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l15094"><span class="ln">15094 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l15095"><span class="ln">15095 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l15096"><span class="ln">15096 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l15097"><span class="ln">15097 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l15098"><span class="ln">15098 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l15099"><span class="ln">15099 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l15100"><span class="ln">15100 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l15101"><span class="ln">15101 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l15102"><span class="ln">15102 </span></a>            returned tensor. Default: ``False``. 
<a name="l15103"><span class="ln">15103 </span></a> 
<a name="l15104"><span class="ln">15104 </span></a>    Returns: 
<a name="l15105"><span class="ln">15105 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window. 
<a name="l15106"><span class="ln">15106 </span></a>    &quot;&quot;&quot;</span>
<a name="l15107"><span class="ln">15107 </span></a>
<a name="l15108"><span class="ln">15108 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15109"><span class="ln">15109 </span></a><span class="s2">def </span><span class="s1">hamming_window</span><span class="s3">(</span>
<a name="l15110"><span class="ln">15110 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l15111"><span class="ln">15111 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l15112"><span class="ln">15112 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15113"><span class="ln">15113 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15114"><span class="ln">15114 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15115"><span class="ln">15115 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15116"><span class="ln">15116 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15117"><span class="ln">15117 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15118"><span class="ln">15118 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15119"><span class="ln">15119 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15120"><span class="ln">15120 </span></a>    hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l15121"><span class="ln">15121 </span></a> 
<a name="l15122"><span class="ln">15122 </span></a>    Hamming window function. 
<a name="l15123"><span class="ln">15123 </span></a> 
<a name="l15124"><span class="ln">15124 </span></a>    .. math:: 
<a name="l15125"><span class="ln">15125 </span></a>        w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right), 
<a name="l15126"><span class="ln">15126 </span></a> 
<a name="l15127"><span class="ln">15127 </span></a>    where :math:`N` is the full window size. 
<a name="l15128"><span class="ln">15128 </span></a> 
<a name="l15129"><span class="ln">15129 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l15130"><span class="ln">15130 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l15131"><span class="ln">15131 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l15132"><span class="ln">15132 </span></a>    ready to be used as a periodic window with functions like 
<a name="l15133"><span class="ln">15133 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l15134"><span class="ln">15134 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l15135"><span class="ln">15135 </span></a>    ``torch.hamming_window(L, periodic=True)`` equal to 
<a name="l15136"><span class="ln">15136 </span></a>    ``torch.hamming_window(L + 1, periodic=False)[:-1])``. 
<a name="l15137"><span class="ln">15137 </span></a> 
<a name="l15138"><span class="ln">15138 </span></a>    .. note:: 
<a name="l15139"><span class="ln">15139 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l15140"><span class="ln">15140 </span></a> 
<a name="l15141"><span class="ln">15141 </span></a>    .. note:: 
<a name="l15142"><span class="ln">15142 </span></a>        This is a generalized version of :meth:`torch.hann_window`. 
<a name="l15143"><span class="ln">15143 </span></a> 
<a name="l15144"><span class="ln">15144 </span></a>    Arguments: 
<a name="l15145"><span class="ln">15145 </span></a>        window_length (int): the size of returned window 
<a name="l15146"><span class="ln">15146 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l15147"><span class="ln">15147 </span></a>            function. If False, return a symmetric window. 
<a name="l15148"><span class="ln">15148 </span></a>        alpha (float, optional): The coefficient :math:`\alpha` in the equation above 
<a name="l15149"><span class="ln">15149 </span></a>        beta (float, optional): The coefficient :math:`\beta` in the equation above 
<a name="l15150"><span class="ln">15150 </span></a> 
<a name="l15151"><span class="ln">15151 </span></a>    Keyword args: 
<a name="l15152"><span class="ln">15152 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l15153"><span class="ln">15153 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l15154"><span class="ln">15154 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l15155"><span class="ln">15155 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l15156"><span class="ln">15156 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l15157"><span class="ln">15157 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l15158"><span class="ln">15158 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l15159"><span class="ln">15159 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l15160"><span class="ln">15160 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l15161"><span class="ln">15161 </span></a>            returned tensor. Default: ``False``. 
<a name="l15162"><span class="ln">15162 </span></a> 
<a name="l15163"><span class="ln">15163 </span></a>    Returns: 
<a name="l15164"><span class="ln">15164 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window. 
<a name="l15165"><span class="ln">15165 </span></a>    &quot;&quot;&quot;</span>
<a name="l15166"><span class="ln">15166 </span></a>
<a name="l15167"><span class="ln">15167 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15168"><span class="ln">15168 </span></a><span class="s2">def </span><span class="s1">hamming_window</span><span class="s3">(</span>
<a name="l15169"><span class="ln">15169 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l15170"><span class="ln">15170 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l15171"><span class="ln">15171 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l15172"><span class="ln">15172 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15173"><span class="ln">15173 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15174"><span class="ln">15174 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15175"><span class="ln">15175 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15176"><span class="ln">15176 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15177"><span class="ln">15177 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15178"><span class="ln">15178 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15179"><span class="ln">15179 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15180"><span class="ln">15180 </span></a>    hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l15181"><span class="ln">15181 </span></a> 
<a name="l15182"><span class="ln">15182 </span></a>    Hamming window function. 
<a name="l15183"><span class="ln">15183 </span></a> 
<a name="l15184"><span class="ln">15184 </span></a>    .. math:: 
<a name="l15185"><span class="ln">15185 </span></a>        w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right), 
<a name="l15186"><span class="ln">15186 </span></a> 
<a name="l15187"><span class="ln">15187 </span></a>    where :math:`N` is the full window size. 
<a name="l15188"><span class="ln">15188 </span></a> 
<a name="l15189"><span class="ln">15189 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l15190"><span class="ln">15190 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l15191"><span class="ln">15191 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l15192"><span class="ln">15192 </span></a>    ready to be used as a periodic window with functions like 
<a name="l15193"><span class="ln">15193 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l15194"><span class="ln">15194 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l15195"><span class="ln">15195 </span></a>    ``torch.hamming_window(L, periodic=True)`` equal to 
<a name="l15196"><span class="ln">15196 </span></a>    ``torch.hamming_window(L + 1, periodic=False)[:-1])``. 
<a name="l15197"><span class="ln">15197 </span></a> 
<a name="l15198"><span class="ln">15198 </span></a>    .. note:: 
<a name="l15199"><span class="ln">15199 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l15200"><span class="ln">15200 </span></a> 
<a name="l15201"><span class="ln">15201 </span></a>    .. note:: 
<a name="l15202"><span class="ln">15202 </span></a>        This is a generalized version of :meth:`torch.hann_window`. 
<a name="l15203"><span class="ln">15203 </span></a> 
<a name="l15204"><span class="ln">15204 </span></a>    Arguments: 
<a name="l15205"><span class="ln">15205 </span></a>        window_length (int): the size of returned window 
<a name="l15206"><span class="ln">15206 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l15207"><span class="ln">15207 </span></a>            function. If False, return a symmetric window. 
<a name="l15208"><span class="ln">15208 </span></a>        alpha (float, optional): The coefficient :math:`\alpha` in the equation above 
<a name="l15209"><span class="ln">15209 </span></a>        beta (float, optional): The coefficient :math:`\beta` in the equation above 
<a name="l15210"><span class="ln">15210 </span></a> 
<a name="l15211"><span class="ln">15211 </span></a>    Keyword args: 
<a name="l15212"><span class="ln">15212 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l15213"><span class="ln">15213 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l15214"><span class="ln">15214 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l15215"><span class="ln">15215 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l15216"><span class="ln">15216 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l15217"><span class="ln">15217 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l15218"><span class="ln">15218 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l15219"><span class="ln">15219 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l15220"><span class="ln">15220 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l15221"><span class="ln">15221 </span></a>            returned tensor. Default: ``False``. 
<a name="l15222"><span class="ln">15222 </span></a> 
<a name="l15223"><span class="ln">15223 </span></a>    Returns: 
<a name="l15224"><span class="ln">15224 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window. 
<a name="l15225"><span class="ln">15225 </span></a>    &quot;&quot;&quot;</span>
<a name="l15226"><span class="ln">15226 </span></a>
<a name="l15227"><span class="ln">15227 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15228"><span class="ln">15228 </span></a><span class="s2">def </span><span class="s1">hamming_window</span><span class="s3">(</span>
<a name="l15229"><span class="ln">15229 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l15230"><span class="ln">15230 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l15231"><span class="ln">15231 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l15232"><span class="ln">15232 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l15233"><span class="ln">15233 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15234"><span class="ln">15234 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15235"><span class="ln">15235 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15236"><span class="ln">15236 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15237"><span class="ln">15237 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15238"><span class="ln">15238 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15239"><span class="ln">15239 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15240"><span class="ln">15240 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15241"><span class="ln">15241 </span></a>    hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l15242"><span class="ln">15242 </span></a> 
<a name="l15243"><span class="ln">15243 </span></a>    Hamming window function. 
<a name="l15244"><span class="ln">15244 </span></a> 
<a name="l15245"><span class="ln">15245 </span></a>    .. math:: 
<a name="l15246"><span class="ln">15246 </span></a>        w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right), 
<a name="l15247"><span class="ln">15247 </span></a> 
<a name="l15248"><span class="ln">15248 </span></a>    where :math:`N` is the full window size. 
<a name="l15249"><span class="ln">15249 </span></a> 
<a name="l15250"><span class="ln">15250 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l15251"><span class="ln">15251 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l15252"><span class="ln">15252 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l15253"><span class="ln">15253 </span></a>    ready to be used as a periodic window with functions like 
<a name="l15254"><span class="ln">15254 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l15255"><span class="ln">15255 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l15256"><span class="ln">15256 </span></a>    ``torch.hamming_window(L, periodic=True)`` equal to 
<a name="l15257"><span class="ln">15257 </span></a>    ``torch.hamming_window(L + 1, periodic=False)[:-1])``. 
<a name="l15258"><span class="ln">15258 </span></a> 
<a name="l15259"><span class="ln">15259 </span></a>    .. note:: 
<a name="l15260"><span class="ln">15260 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l15261"><span class="ln">15261 </span></a> 
<a name="l15262"><span class="ln">15262 </span></a>    .. note:: 
<a name="l15263"><span class="ln">15263 </span></a>        This is a generalized version of :meth:`torch.hann_window`. 
<a name="l15264"><span class="ln">15264 </span></a> 
<a name="l15265"><span class="ln">15265 </span></a>    Arguments: 
<a name="l15266"><span class="ln">15266 </span></a>        window_length (int): the size of returned window 
<a name="l15267"><span class="ln">15267 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l15268"><span class="ln">15268 </span></a>            function. If False, return a symmetric window. 
<a name="l15269"><span class="ln">15269 </span></a>        alpha (float, optional): The coefficient :math:`\alpha` in the equation above 
<a name="l15270"><span class="ln">15270 </span></a>        beta (float, optional): The coefficient :math:`\beta` in the equation above 
<a name="l15271"><span class="ln">15271 </span></a> 
<a name="l15272"><span class="ln">15272 </span></a>    Keyword args: 
<a name="l15273"><span class="ln">15273 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l15274"><span class="ln">15274 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l15275"><span class="ln">15275 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l15276"><span class="ln">15276 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l15277"><span class="ln">15277 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l15278"><span class="ln">15278 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l15279"><span class="ln">15279 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l15280"><span class="ln">15280 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l15281"><span class="ln">15281 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l15282"><span class="ln">15282 </span></a>            returned tensor. Default: ``False``. 
<a name="l15283"><span class="ln">15283 </span></a> 
<a name="l15284"><span class="ln">15284 </span></a>    Returns: 
<a name="l15285"><span class="ln">15285 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window. 
<a name="l15286"><span class="ln">15286 </span></a>    &quot;&quot;&quot;</span>
<a name="l15287"><span class="ln">15287 </span></a>
<a name="l15288"><span class="ln">15288 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15289"><span class="ln">15289 </span></a><span class="s2">def </span><span class="s1">hann_window</span><span class="s3">(</span>
<a name="l15290"><span class="ln">15290 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l15291"><span class="ln">15291 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15292"><span class="ln">15292 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15293"><span class="ln">15293 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15294"><span class="ln">15294 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15295"><span class="ln">15295 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15296"><span class="ln">15296 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15297"><span class="ln">15297 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15298"><span class="ln">15298 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15299"><span class="ln">15299 </span></a>    hann_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l15300"><span class="ln">15300 </span></a> 
<a name="l15301"><span class="ln">15301 </span></a>    Hann window function. 
<a name="l15302"><span class="ln">15302 </span></a> 
<a name="l15303"><span class="ln">15303 </span></a>    .. math:: 
<a name="l15304"><span class="ln">15304 </span></a>        w[n] = \frac{1}{2}\ \left[1 - \cos \left( \frac{2 \pi n}{N - 1} \right)\right] = 
<a name="l15305"><span class="ln">15305 </span></a>                \sin^2 \left( \frac{\pi n}{N - 1} \right), 
<a name="l15306"><span class="ln">15306 </span></a> 
<a name="l15307"><span class="ln">15307 </span></a>    where :math:`N` is the full window size. 
<a name="l15308"><span class="ln">15308 </span></a> 
<a name="l15309"><span class="ln">15309 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l15310"><span class="ln">15310 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l15311"><span class="ln">15311 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l15312"><span class="ln">15312 </span></a>    ready to be used as a periodic window with functions like 
<a name="l15313"><span class="ln">15313 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l15314"><span class="ln">15314 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l15315"><span class="ln">15315 </span></a>    ``torch.hann_window(L, periodic=True)`` equal to 
<a name="l15316"><span class="ln">15316 </span></a>    ``torch.hann_window(L + 1, periodic=False)[:-1])``. 
<a name="l15317"><span class="ln">15317 </span></a> 
<a name="l15318"><span class="ln">15318 </span></a>    .. note:: 
<a name="l15319"><span class="ln">15319 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l15320"><span class="ln">15320 </span></a> 
<a name="l15321"><span class="ln">15321 </span></a>    Arguments: 
<a name="l15322"><span class="ln">15322 </span></a>        window_length (int): the size of returned window 
<a name="l15323"><span class="ln">15323 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l15324"><span class="ln">15324 </span></a>            function. If False, return a symmetric window. 
<a name="l15325"><span class="ln">15325 </span></a> 
<a name="l15326"><span class="ln">15326 </span></a>    Keyword args: 
<a name="l15327"><span class="ln">15327 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l15328"><span class="ln">15328 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l15329"><span class="ln">15329 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l15330"><span class="ln">15330 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l15331"><span class="ln">15331 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l15332"><span class="ln">15332 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l15333"><span class="ln">15333 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l15334"><span class="ln">15334 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l15335"><span class="ln">15335 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l15336"><span class="ln">15336 </span></a>            returned tensor. Default: ``False``. 
<a name="l15337"><span class="ln">15337 </span></a> 
<a name="l15338"><span class="ln">15338 </span></a>    Returns: 
<a name="l15339"><span class="ln">15339 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window 
<a name="l15340"><span class="ln">15340 </span></a>    &quot;&quot;&quot;</span>
<a name="l15341"><span class="ln">15341 </span></a>
<a name="l15342"><span class="ln">15342 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15343"><span class="ln">15343 </span></a><span class="s2">def </span><span class="s1">hann_window</span><span class="s3">(</span>
<a name="l15344"><span class="ln">15344 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l15345"><span class="ln">15345 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l15346"><span class="ln">15346 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15347"><span class="ln">15347 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15348"><span class="ln">15348 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15349"><span class="ln">15349 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15350"><span class="ln">15350 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15351"><span class="ln">15351 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l15352"><span class="ln">15352 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15353"><span class="ln">15353 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15354"><span class="ln">15354 </span></a>    hann_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l15355"><span class="ln">15355 </span></a> 
<a name="l15356"><span class="ln">15356 </span></a>    Hann window function. 
<a name="l15357"><span class="ln">15357 </span></a> 
<a name="l15358"><span class="ln">15358 </span></a>    .. math:: 
<a name="l15359"><span class="ln">15359 </span></a>        w[n] = \frac{1}{2}\ \left[1 - \cos \left( \frac{2 \pi n}{N - 1} \right)\right] = 
<a name="l15360"><span class="ln">15360 </span></a>                \sin^2 \left( \frac{\pi n}{N - 1} \right), 
<a name="l15361"><span class="ln">15361 </span></a> 
<a name="l15362"><span class="ln">15362 </span></a>    where :math:`N` is the full window size. 
<a name="l15363"><span class="ln">15363 </span></a> 
<a name="l15364"><span class="ln">15364 </span></a>    The input :attr:`window_length` is a positive integer controlling the 
<a name="l15365"><span class="ln">15365 </span></a>    returned window size. :attr:`periodic` flag determines whether the returned 
<a name="l15366"><span class="ln">15366 </span></a>    window trims off the last duplicate value from the symmetric window and is 
<a name="l15367"><span class="ln">15367 </span></a>    ready to be used as a periodic window with functions like 
<a name="l15368"><span class="ln">15368 </span></a>    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in 
<a name="l15369"><span class="ln">15369 </span></a>    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have 
<a name="l15370"><span class="ln">15370 </span></a>    ``torch.hann_window(L, periodic=True)`` equal to 
<a name="l15371"><span class="ln">15371 </span></a>    ``torch.hann_window(L + 1, periodic=False)[:-1])``. 
<a name="l15372"><span class="ln">15372 </span></a> 
<a name="l15373"><span class="ln">15373 </span></a>    .. note:: 
<a name="l15374"><span class="ln">15374 </span></a>        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1. 
<a name="l15375"><span class="ln">15375 </span></a> 
<a name="l15376"><span class="ln">15376 </span></a>    Arguments: 
<a name="l15377"><span class="ln">15377 </span></a>        window_length (int): the size of returned window 
<a name="l15378"><span class="ln">15378 </span></a>        periodic (bool, optional): If True, returns a window to be used as periodic 
<a name="l15379"><span class="ln">15379 </span></a>            function. If False, return a symmetric window. 
<a name="l15380"><span class="ln">15380 </span></a> 
<a name="l15381"><span class="ln">15381 </span></a>    Keyword args: 
<a name="l15382"><span class="ln">15382 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l15383"><span class="ln">15383 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported. 
<a name="l15384"><span class="ln">15384 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l15385"><span class="ln">15385 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l15386"><span class="ln">15386 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l15387"><span class="ln">15387 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l15388"><span class="ln">15388 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l15389"><span class="ln">15389 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l15390"><span class="ln">15390 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l15391"><span class="ln">15391 </span></a>            returned tensor. Default: ``False``. 
<a name="l15392"><span class="ln">15392 </span></a> 
<a name="l15393"><span class="ln">15393 </span></a>    Returns: 
<a name="l15394"><span class="ln">15394 </span></a>        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window 
<a name="l15395"><span class="ln">15395 </span></a>    &quot;&quot;&quot;</span>
<a name="l15396"><span class="ln">15396 </span></a>
<a name="l15397"><span class="ln">15397 </span></a><span class="s2">def </span><span class="s1">hardshrink</span><span class="s3">(</span>
<a name="l15398"><span class="ln">15398 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15399"><span class="ln">15399 </span></a>    <span class="s1">lambd</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0.5</span><span class="s3">,</span>
<a name="l15400"><span class="ln">15400 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15401"><span class="ln">15401 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15402"><span class="ln">15402 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l15403"><span class="ln">15403 </span></a><span class="s2">def </span><span class="s1">heaviside</span><span class="s3">(</span>
<a name="l15404"><span class="ln">15404 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15405"><span class="ln">15405 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15406"><span class="ln">15406 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15407"><span class="ln">15407 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15408"><span class="ln">15408 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15409"><span class="ln">15409 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15410"><span class="ln">15410 </span></a>    heaviside(input, values, *, out=None) -&gt; Tensor 
<a name="l15411"><span class="ln">15411 </span></a> 
<a name="l15412"><span class="ln">15412 </span></a>    Computes the Heaviside step function for each element in :attr:`input`. 
<a name="l15413"><span class="ln">15413 </span></a>    The Heaviside step function is defined as: 
<a name="l15414"><span class="ln">15414 </span></a> 
<a name="l15415"><span class="ln">15415 </span></a>    .. math:: 
<a name="l15416"><span class="ln">15416 </span></a>        \text{{heaviside}}(input, values) = \begin{cases} 
<a name="l15417"><span class="ln">15417 </span></a>            0, &amp; \text{if input &lt; 0}\\ 
<a name="l15418"><span class="ln">15418 </span></a>            values, &amp; \text{if input == 0}\\ 
<a name="l15419"><span class="ln">15419 </span></a>            1, &amp; \text{if input &gt; 0} 
<a name="l15420"><span class="ln">15420 </span></a>        \end{cases} 
<a name="l15421"><span class="ln">15421 </span></a> 
<a name="l15422"><span class="ln">15422 </span></a> 
<a name="l15423"><span class="ln">15423 </span></a>    Args: 
<a name="l15424"><span class="ln">15424 </span></a>        input (Tensor): the input tensor. 
<a name="l15425"><span class="ln">15425 </span></a>        values (Tensor): The values to use where :attr:`input` is zero. 
<a name="l15426"><span class="ln">15426 </span></a> 
<a name="l15427"><span class="ln">15427 </span></a>    Keyword arguments: 
<a name="l15428"><span class="ln">15428 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l15429"><span class="ln">15429 </span></a> 
<a name="l15430"><span class="ln">15430 </span></a>    Example:: 
<a name="l15431"><span class="ln">15431 </span></a> 
<a name="l15432"><span class="ln">15432 </span></a>        &gt;&gt;&gt; input = torch.tensor([-1.5, 0, 2.0]) 
<a name="l15433"><span class="ln">15433 </span></a>        &gt;&gt;&gt; values = torch.tensor([0.5]) 
<a name="l15434"><span class="ln">15434 </span></a>        &gt;&gt;&gt; torch.heaviside(input, values) 
<a name="l15435"><span class="ln">15435 </span></a>        tensor([0.0000, 0.5000, 1.0000]) 
<a name="l15436"><span class="ln">15436 </span></a>        &gt;&gt;&gt; values = torch.tensor([1.2, -2.0, 3.5]) 
<a name="l15437"><span class="ln">15437 </span></a>        &gt;&gt;&gt; torch.heaviside(input, values) 
<a name="l15438"><span class="ln">15438 </span></a>        tensor([0., -2., 1.]) 
<a name="l15439"><span class="ln">15439 </span></a>    &quot;&quot;&quot;</span>
<a name="l15440"><span class="ln">15440 </span></a>
<a name="l15441"><span class="ln">15441 </span></a><span class="s2">def </span><span class="s1">hinge_embedding_loss</span><span class="s3">(</span>
<a name="l15442"><span class="ln">15442 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15443"><span class="ln">15443 </span></a>    <span class="s1">target</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15444"><span class="ln">15444 </span></a>    <span class="s1">margin</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1.0</span><span class="s3">,</span>
<a name="l15445"><span class="ln">15445 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l15446"><span class="ln">15446 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l15447"><span class="ln">15447 </span></a><span class="s2">def </span><span class="s1">histc</span><span class="s3">(</span>
<a name="l15448"><span class="ln">15448 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15449"><span class="ln">15449 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">100</span><span class="s3">,</span>
<a name="l15450"><span class="ln">15450 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l15451"><span class="ln">15451 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l15452"><span class="ln">15452 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15453"><span class="ln">15453 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15454"><span class="ln">15454 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15455"><span class="ln">15455 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15456"><span class="ln">15456 </span></a>    histc(input, bins=100, min=0, max=0, *, out=None) -&gt; Tensor 
<a name="l15457"><span class="ln">15457 </span></a> 
<a name="l15458"><span class="ln">15458 </span></a>    Computes the histogram of a tensor. 
<a name="l15459"><span class="ln">15459 </span></a> 
<a name="l15460"><span class="ln">15460 </span></a>    The elements are sorted into equal width bins between :attr:`min` and 
<a name="l15461"><span class="ln">15461 </span></a>    :attr:`max`. If :attr:`min` and :attr:`max` are both zero, the minimum and 
<a name="l15462"><span class="ln">15462 </span></a>    maximum values of the data are used. 
<a name="l15463"><span class="ln">15463 </span></a> 
<a name="l15464"><span class="ln">15464 </span></a>    Elements lower than min and higher than max and ``NaN`` elements are ignored. 
<a name="l15465"><span class="ln">15465 </span></a> 
<a name="l15466"><span class="ln">15466 </span></a>    Args: 
<a name="l15467"><span class="ln">15467 </span></a>        input (Tensor): the input tensor. 
<a name="l15468"><span class="ln">15468 </span></a>        bins (int): number of histogram bins 
<a name="l15469"><span class="ln">15469 </span></a>        min (Scalar): lower end of the range (inclusive) 
<a name="l15470"><span class="ln">15470 </span></a>        max (Scalar): upper end of the range (inclusive) 
<a name="l15471"><span class="ln">15471 </span></a> 
<a name="l15472"><span class="ln">15472 </span></a>    Keyword args: 
<a name="l15473"><span class="ln">15473 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l15474"><span class="ln">15474 </span></a> 
<a name="l15475"><span class="ln">15475 </span></a>    Returns: 
<a name="l15476"><span class="ln">15476 </span></a>        Tensor: Histogram represented as a tensor 
<a name="l15477"><span class="ln">15477 </span></a> 
<a name="l15478"><span class="ln">15478 </span></a>    Example:: 
<a name="l15479"><span class="ln">15479 </span></a> 
<a name="l15480"><span class="ln">15480 </span></a>        &gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3) 
<a name="l15481"><span class="ln">15481 </span></a>        tensor([ 0.,  2.,  1.,  0.]) 
<a name="l15482"><span class="ln">15482 </span></a>    &quot;&quot;&quot;</span>
<a name="l15483"><span class="ln">15483 </span></a>
<a name="l15484"><span class="ln">15484 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15485"><span class="ln">15485 </span></a><span class="s2">def </span><span class="s1">histogram</span><span class="s3">(</span>
<a name="l15486"><span class="ln">15486 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15487"><span class="ln">15487 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15488"><span class="ln">15488 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15489"><span class="ln">15489 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15490"><span class="ln">15490 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l15491"><span class="ln">15491 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15492"><span class="ln">15492 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">histogram</span><span class="s2">:</span>
<a name="l15493"><span class="ln">15493 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15494"><span class="ln">15494 </span></a>    histogram(input, bins, *, range=None, weight=None, density=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l15495"><span class="ln">15495 </span></a> 
<a name="l15496"><span class="ln">15496 </span></a>    Computes a histogram of the values in a tensor. 
<a name="l15497"><span class="ln">15497 </span></a> 
<a name="l15498"><span class="ln">15498 </span></a>    :attr:`bins` can be an integer or a 1D tensor. 
<a name="l15499"><span class="ln">15499 </span></a> 
<a name="l15500"><span class="ln">15500 </span></a>    If :attr:`bins` is an int, it specifies the number of equal-width bins. 
<a name="l15501"><span class="ln">15501 </span></a>    By default, the lower and upper range of the bins is determined by the 
<a name="l15502"><span class="ln">15502 </span></a>    minimum and maximum elements of the input tensor. The :attr:`range` 
<a name="l15503"><span class="ln">15503 </span></a>    argument can be provided to specify a range for the bins. 
<a name="l15504"><span class="ln">15504 </span></a> 
<a name="l15505"><span class="ln">15505 </span></a>    If :attr:`bins` is a 1D tensor, it specifies the sequence of bin edges 
<a name="l15506"><span class="ln">15506 </span></a>    including the rightmost edge. It should contain at least 2 elements 
<a name="l15507"><span class="ln">15507 </span></a>    and its elements should be increasing. 
<a name="l15508"><span class="ln">15508 </span></a> 
<a name="l15509"><span class="ln">15509 </span></a>    Args: 
<a name="l15510"><span class="ln">15510 </span></a>        input (Tensor): the input tensor. 
<a name="l15511"><span class="ln">15511 </span></a>        bins: int or 1D Tensor. If int, defines the number of equal-width bins. If tensor, 
<a name="l15512"><span class="ln">15512 </span></a>              defines the sequence of bin edges including the rightmost edge. 
<a name="l15513"><span class="ln">15513 </span></a> 
<a name="l15514"><span class="ln">15514 </span></a>    Keyword args: 
<a name="l15515"><span class="ln">15515 </span></a>        range (tuple of float): Defines the range of the bins. 
<a name="l15516"><span class="ln">15516 </span></a>        weight (Tensor): If provided, weight should have the same shape as input. Each value in 
<a name="l15517"><span class="ln">15517 </span></a>                         input contributes its associated weight towards its bin's result. 
<a name="l15518"><span class="ln">15518 </span></a>        density (bool): If False, the result will contain the count (or total weight) in each bin. 
<a name="l15519"><span class="ln">15519 </span></a>                        If True, the result is the value of the probability density function over the bins, 
<a name="l15520"><span class="ln">15520 </span></a>                        normalized such that the integral over the range of the bins is 1. 
<a name="l15521"><span class="ln">15521 </span></a>        out (Tensor, optional): the output tensor. (tuple, optional): The result tuple of two output tensors (hist, bin_edges). 
<a name="l15522"><span class="ln">15522 </span></a> 
<a name="l15523"><span class="ln">15523 </span></a>    Returns: 
<a name="l15524"><span class="ln">15524 </span></a>        hist (Tensor): 1D Tensor containing the values of the histogram. 
<a name="l15525"><span class="ln">15525 </span></a>        bin_edges(Tensor): 1D Tensor containing the edges of the histogram bins. 
<a name="l15526"><span class="ln">15526 </span></a> 
<a name="l15527"><span class="ln">15527 </span></a>    Example:: 
<a name="l15528"><span class="ln">15528 </span></a> 
<a name="l15529"><span class="ln">15529 </span></a>        &gt;&gt;&gt; torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.])) 
<a name="l15530"><span class="ln">15530 </span></a>        (tensor([ 0.,  5.,  2.,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.])) 
<a name="l15531"><span class="ln">15531 </span></a>        &gt;&gt;&gt; torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.]), density=True) 
<a name="l15532"><span class="ln">15532 </span></a>        (tensor([ 0.,  0.9524,  0.3810,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.])) 
<a name="l15533"><span class="ln">15533 </span></a>    &quot;&quot;&quot;</span>
<a name="l15534"><span class="ln">15534 </span></a>
<a name="l15535"><span class="ln">15535 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15536"><span class="ln">15536 </span></a><span class="s2">def </span><span class="s1">histogram</span><span class="s3">(</span>
<a name="l15537"><span class="ln">15537 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15538"><span class="ln">15538 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">100</span><span class="s3">,</span>
<a name="l15539"><span class="ln">15539 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15540"><span class="ln">15540 </span></a>    <span class="s1">range</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_float</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15541"><span class="ln">15541 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15542"><span class="ln">15542 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l15543"><span class="ln">15543 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15544"><span class="ln">15544 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">histogram</span><span class="s2">:</span>
<a name="l15545"><span class="ln">15545 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15546"><span class="ln">15546 </span></a>    histogram(input, bins, *, range=None, weight=None, density=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l15547"><span class="ln">15547 </span></a> 
<a name="l15548"><span class="ln">15548 </span></a>    Computes a histogram of the values in a tensor. 
<a name="l15549"><span class="ln">15549 </span></a> 
<a name="l15550"><span class="ln">15550 </span></a>    :attr:`bins` can be an integer or a 1D tensor. 
<a name="l15551"><span class="ln">15551 </span></a> 
<a name="l15552"><span class="ln">15552 </span></a>    If :attr:`bins` is an int, it specifies the number of equal-width bins. 
<a name="l15553"><span class="ln">15553 </span></a>    By default, the lower and upper range of the bins is determined by the 
<a name="l15554"><span class="ln">15554 </span></a>    minimum and maximum elements of the input tensor. The :attr:`range` 
<a name="l15555"><span class="ln">15555 </span></a>    argument can be provided to specify a range for the bins. 
<a name="l15556"><span class="ln">15556 </span></a> 
<a name="l15557"><span class="ln">15557 </span></a>    If :attr:`bins` is a 1D tensor, it specifies the sequence of bin edges 
<a name="l15558"><span class="ln">15558 </span></a>    including the rightmost edge. It should contain at least 2 elements 
<a name="l15559"><span class="ln">15559 </span></a>    and its elements should be increasing. 
<a name="l15560"><span class="ln">15560 </span></a> 
<a name="l15561"><span class="ln">15561 </span></a>    Args: 
<a name="l15562"><span class="ln">15562 </span></a>        input (Tensor): the input tensor. 
<a name="l15563"><span class="ln">15563 </span></a>        bins: int or 1D Tensor. If int, defines the number of equal-width bins. If tensor, 
<a name="l15564"><span class="ln">15564 </span></a>              defines the sequence of bin edges including the rightmost edge. 
<a name="l15565"><span class="ln">15565 </span></a> 
<a name="l15566"><span class="ln">15566 </span></a>    Keyword args: 
<a name="l15567"><span class="ln">15567 </span></a>        range (tuple of float): Defines the range of the bins. 
<a name="l15568"><span class="ln">15568 </span></a>        weight (Tensor): If provided, weight should have the same shape as input. Each value in 
<a name="l15569"><span class="ln">15569 </span></a>                         input contributes its associated weight towards its bin's result. 
<a name="l15570"><span class="ln">15570 </span></a>        density (bool): If False, the result will contain the count (or total weight) in each bin. 
<a name="l15571"><span class="ln">15571 </span></a>                        If True, the result is the value of the probability density function over the bins, 
<a name="l15572"><span class="ln">15572 </span></a>                        normalized such that the integral over the range of the bins is 1. 
<a name="l15573"><span class="ln">15573 </span></a>        out (Tensor, optional): the output tensor. (tuple, optional): The result tuple of two output tensors (hist, bin_edges). 
<a name="l15574"><span class="ln">15574 </span></a> 
<a name="l15575"><span class="ln">15575 </span></a>    Returns: 
<a name="l15576"><span class="ln">15576 </span></a>        hist (Tensor): 1D Tensor containing the values of the histogram. 
<a name="l15577"><span class="ln">15577 </span></a>        bin_edges(Tensor): 1D Tensor containing the edges of the histogram bins. 
<a name="l15578"><span class="ln">15578 </span></a> 
<a name="l15579"><span class="ln">15579 </span></a>    Example:: 
<a name="l15580"><span class="ln">15580 </span></a> 
<a name="l15581"><span class="ln">15581 </span></a>        &gt;&gt;&gt; torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.])) 
<a name="l15582"><span class="ln">15582 </span></a>        (tensor([ 0.,  5.,  2.,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.])) 
<a name="l15583"><span class="ln">15583 </span></a>        &gt;&gt;&gt; torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.]), density=True) 
<a name="l15584"><span class="ln">15584 </span></a>        (tensor([ 0.,  0.9524,  0.3810,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.])) 
<a name="l15585"><span class="ln">15585 </span></a>    &quot;&quot;&quot;</span>
<a name="l15586"><span class="ln">15586 </span></a>
<a name="l15587"><span class="ln">15587 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15588"><span class="ln">15588 </span></a><span class="s2">def </span><span class="s1">histogramdd</span><span class="s3">(</span>
<a name="l15589"><span class="ln">15589 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15590"><span class="ln">15590 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l15591"><span class="ln">15591 </span></a>    <span class="s1">range</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_float</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15592"><span class="ln">15592 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15593"><span class="ln">15593 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l15594"><span class="ln">15594 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">histogramdd</span><span class="s2">:</span>
<a name="l15595"><span class="ln">15595 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15596"><span class="ln">15596 </span></a>    histogramdd(input, bins, *, range=None, weight=None, density=False, out=None) -&gt; (Tensor, Tensor[]) 
<a name="l15597"><span class="ln">15597 </span></a> 
<a name="l15598"><span class="ln">15598 </span></a>    Computes a multi-dimensional histogram of the values in a tensor. 
<a name="l15599"><span class="ln">15599 </span></a> 
<a name="l15600"><span class="ln">15600 </span></a>    Interprets the elements of an input tensor whose innermost dimension has size N 
<a name="l15601"><span class="ln">15601 </span></a>    as a collection of N-dimensional points. Maps each of the points into a set of 
<a name="l15602"><span class="ln">15602 </span></a>    N-dimensional bins and returns the number of points (or total weight) in each bin. 
<a name="l15603"><span class="ln">15603 </span></a> 
<a name="l15604"><span class="ln">15604 </span></a>    :attr:`input` must be a tensor with at least 2 dimensions. 
<a name="l15605"><span class="ln">15605 </span></a>    If input has shape (M, N), each of its M rows defines a point in N-dimensional space. 
<a name="l15606"><span class="ln">15606 </span></a>    If input has three or more dimensions, all but the last dimension are flattened. 
<a name="l15607"><span class="ln">15607 </span></a> 
<a name="l15608"><span class="ln">15608 </span></a>    Each dimension is independently associated with its own strictly increasing sequence 
<a name="l15609"><span class="ln">15609 </span></a>    of bin edges. Bin edges may be specified explicitly by passing a sequence of 1D 
<a name="l15610"><span class="ln">15610 </span></a>    tensors. Alternatively, bin edges may be constructed automatically by passing a 
<a name="l15611"><span class="ln">15611 </span></a>    sequence of integers specifying the number of equal-width bins in each dimension. 
<a name="l15612"><span class="ln">15612 </span></a> 
<a name="l15613"><span class="ln">15613 </span></a>    For each N-dimensional point in input: 
<a name="l15614"><span class="ln">15614 </span></a>        - Each of its coordinates is binned independently among the bin edges 
<a name="l15615"><span class="ln">15615 </span></a>            corresponding to its dimension 
<a name="l15616"><span class="ln">15616 </span></a>        - Binning results are combined to identify the N-dimensional bin (if any) 
<a name="l15617"><span class="ln">15617 </span></a>            into which the point falls 
<a name="l15618"><span class="ln">15618 </span></a>        - If the point falls into a bin, the bin's count (or total weight) is incremented 
<a name="l15619"><span class="ln">15619 </span></a>        - Points which do not fall into any bin do not contribute to the output 
<a name="l15620"><span class="ln">15620 </span></a> 
<a name="l15621"><span class="ln">15621 </span></a>    :attr:`bins` can be a sequence of N 1D tensors, a sequence of N ints, or a single int. 
<a name="l15622"><span class="ln">15622 </span></a> 
<a name="l15623"><span class="ln">15623 </span></a>    If :attr:`bins` is a sequence of N 1D tensors, it explicitly specifies the N sequences 
<a name="l15624"><span class="ln">15624 </span></a>    of bin edges. Each 1D tensor should contain a strictly increasing sequence with at 
<a name="l15625"><span class="ln">15625 </span></a>    least one element. A sequence of K bin edges defines K-1 bins, explicitly specifying 
<a name="l15626"><span class="ln">15626 </span></a>    the left and right edges of all bins. Every bin is exclusive of its left edge. Only 
<a name="l15627"><span class="ln">15627 </span></a>    the rightmost bin is inclusive of its right edge. 
<a name="l15628"><span class="ln">15628 </span></a> 
<a name="l15629"><span class="ln">15629 </span></a>    If :attr:`bins` is a sequence of N ints, it specifies the number of equal-width bins 
<a name="l15630"><span class="ln">15630 </span></a>    in each dimension. By default, the leftmost and rightmost bin edges in each dimension 
<a name="l15631"><span class="ln">15631 </span></a>    are determined by the minimum and maximum elements of the input tensor in the 
<a name="l15632"><span class="ln">15632 </span></a>    corresponding dimension. The :attr:`range` argument can be provided to manually 
<a name="l15633"><span class="ln">15633 </span></a>    specify the leftmost and rightmost bin edges in each dimension. 
<a name="l15634"><span class="ln">15634 </span></a> 
<a name="l15635"><span class="ln">15635 </span></a>    If :attr:`bins` is an int, it specifies the number of equal-width bins for all dimensions. 
<a name="l15636"><span class="ln">15636 </span></a> 
<a name="l15637"><span class="ln">15637 </span></a>    .. note:: 
<a name="l15638"><span class="ln">15638 </span></a>        See also :func:`torch.histogram`, which specifically computes 1D histograms. 
<a name="l15639"><span class="ln">15639 </span></a>        While :func:`torch.histogramdd` infers the dimensionality of its bins and 
<a name="l15640"><span class="ln">15640 </span></a>        binned values from the shape of :attr:`input`, :func:`torch.histogram` 
<a name="l15641"><span class="ln">15641 </span></a>        accepts and flattens :attr:`input` of any shape. 
<a name="l15642"><span class="ln">15642 </span></a> 
<a name="l15643"><span class="ln">15643 </span></a>    Args: 
<a name="l15644"><span class="ln">15644 </span></a>        input (Tensor): the input tensor. 
<a name="l15645"><span class="ln">15645 </span></a>        bins: Tensor[], int[], or int. 
<a name="l15646"><span class="ln">15646 </span></a>                If Tensor[], defines the sequences of bin edges. 
<a name="l15647"><span class="ln">15647 </span></a>                If int[], defines the number of equal-width bins in each dimension. 
<a name="l15648"><span class="ln">15648 </span></a>                If int, defines the number of equal-width bins for all dimensions. 
<a name="l15649"><span class="ln">15649 </span></a>    Keyword args: 
<a name="l15650"><span class="ln">15650 </span></a>        range (sequence of float): Defines the leftmost and rightmost bin edges 
<a name="l15651"><span class="ln">15651 </span></a>                                    in each dimension. 
<a name="l15652"><span class="ln">15652 </span></a>        weight (Tensor): By default, each value in the input has weight 1. If a weight 
<a name="l15653"><span class="ln">15653 </span></a>                            tensor is passed, each N-dimensional coordinate in input 
<a name="l15654"><span class="ln">15654 </span></a>                            contributes its associated weight towards its bin's result. 
<a name="l15655"><span class="ln">15655 </span></a>                            The weight tensor should have the same shape as the :attr:`input` 
<a name="l15656"><span class="ln">15656 </span></a>                            tensor excluding its innermost dimension N. 
<a name="l15657"><span class="ln">15657 </span></a>        density (bool): If False (default), the result will contain the count (or total weight) 
<a name="l15658"><span class="ln">15658 </span></a>                        in each bin. If True, each count (weight) is divided by the total count 
<a name="l15659"><span class="ln">15659 </span></a>                        (total weight), then divided by the volume of its associated bin. 
<a name="l15660"><span class="ln">15660 </span></a>    Returns: 
<a name="l15661"><span class="ln">15661 </span></a>        hist (Tensor): N-dimensional Tensor containing the values of the histogram. 
<a name="l15662"><span class="ln">15662 </span></a>        bin_edges(Tensor[]): sequence of N 1D Tensors containing the bin edges. 
<a name="l15663"><span class="ln">15663 </span></a> 
<a name="l15664"><span class="ln">15664 </span></a>    Example:: 
<a name="l15665"><span class="ln">15665 </span></a> 
<a name="l15666"><span class="ln">15666 </span></a>        &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 1.], [1., 0.], [2., 0.], [2., 2.]]), bins=[3, 3], 
<a name="l15667"><span class="ln">15667 </span></a>        ...                   weight=torch.tensor([1., 2., 4., 8.])) 
<a name="l15668"><span class="ln">15668 </span></a>            torch.return_types.histogramdd( 
<a name="l15669"><span class="ln">15669 </span></a>                hist=tensor([[0., 1., 0.], 
<a name="l15670"><span class="ln">15670 </span></a>                             [2., 0., 0.], 
<a name="l15671"><span class="ln">15671 </span></a>                             [4., 0., 8.]]), 
<a name="l15672"><span class="ln">15672 </span></a>                bin_edges=(tensor([0.0000, 0.6667, 1.3333, 2.0000]), 
<a name="l15673"><span class="ln">15673 </span></a>                           tensor([0.0000, 0.6667, 1.3333, 2.0000]))) 
<a name="l15674"><span class="ln">15674 </span></a> 
<a name="l15675"><span class="ln">15675 </span></a>        &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 0.], [1., 1.], [2., 2.]]), bins=[2, 2], 
<a name="l15676"><span class="ln">15676 </span></a>        ...                   range=[0., 1., 0., 1.], density=True) 
<a name="l15677"><span class="ln">15677 </span></a>            torch.return_types.histogramdd( 
<a name="l15678"><span class="ln">15678 </span></a>               hist=tensor([[2., 0.], 
<a name="l15679"><span class="ln">15679 </span></a>                            [0., 2.]]), 
<a name="l15680"><span class="ln">15680 </span></a>               bin_edges=(tensor([0.0000, 0.5000, 1.0000]), 
<a name="l15681"><span class="ln">15681 </span></a>                          tensor([0.0000, 0.5000, 1.0000]))) 
<a name="l15682"><span class="ln">15682 </span></a>    &quot;&quot;&quot;</span>
<a name="l15683"><span class="ln">15683 </span></a>
<a name="l15684"><span class="ln">15684 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15685"><span class="ln">15685 </span></a><span class="s2">def </span><span class="s1">histogramdd</span><span class="s3">(</span>
<a name="l15686"><span class="ln">15686 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15687"><span class="ln">15687 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l15688"><span class="ln">15688 </span></a>    <span class="s1">range</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_float</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15689"><span class="ln">15689 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15690"><span class="ln">15690 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l15691"><span class="ln">15691 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">histogramdd</span><span class="s2">:</span>
<a name="l15692"><span class="ln">15692 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15693"><span class="ln">15693 </span></a>    histogramdd(input, bins, *, range=None, weight=None, density=False, out=None) -&gt; (Tensor, Tensor[]) 
<a name="l15694"><span class="ln">15694 </span></a> 
<a name="l15695"><span class="ln">15695 </span></a>    Computes a multi-dimensional histogram of the values in a tensor. 
<a name="l15696"><span class="ln">15696 </span></a> 
<a name="l15697"><span class="ln">15697 </span></a>    Interprets the elements of an input tensor whose innermost dimension has size N 
<a name="l15698"><span class="ln">15698 </span></a>    as a collection of N-dimensional points. Maps each of the points into a set of 
<a name="l15699"><span class="ln">15699 </span></a>    N-dimensional bins and returns the number of points (or total weight) in each bin. 
<a name="l15700"><span class="ln">15700 </span></a> 
<a name="l15701"><span class="ln">15701 </span></a>    :attr:`input` must be a tensor with at least 2 dimensions. 
<a name="l15702"><span class="ln">15702 </span></a>    If input has shape (M, N), each of its M rows defines a point in N-dimensional space. 
<a name="l15703"><span class="ln">15703 </span></a>    If input has three or more dimensions, all but the last dimension are flattened. 
<a name="l15704"><span class="ln">15704 </span></a> 
<a name="l15705"><span class="ln">15705 </span></a>    Each dimension is independently associated with its own strictly increasing sequence 
<a name="l15706"><span class="ln">15706 </span></a>    of bin edges. Bin edges may be specified explicitly by passing a sequence of 1D 
<a name="l15707"><span class="ln">15707 </span></a>    tensors. Alternatively, bin edges may be constructed automatically by passing a 
<a name="l15708"><span class="ln">15708 </span></a>    sequence of integers specifying the number of equal-width bins in each dimension. 
<a name="l15709"><span class="ln">15709 </span></a> 
<a name="l15710"><span class="ln">15710 </span></a>    For each N-dimensional point in input: 
<a name="l15711"><span class="ln">15711 </span></a>        - Each of its coordinates is binned independently among the bin edges 
<a name="l15712"><span class="ln">15712 </span></a>            corresponding to its dimension 
<a name="l15713"><span class="ln">15713 </span></a>        - Binning results are combined to identify the N-dimensional bin (if any) 
<a name="l15714"><span class="ln">15714 </span></a>            into which the point falls 
<a name="l15715"><span class="ln">15715 </span></a>        - If the point falls into a bin, the bin's count (or total weight) is incremented 
<a name="l15716"><span class="ln">15716 </span></a>        - Points which do not fall into any bin do not contribute to the output 
<a name="l15717"><span class="ln">15717 </span></a> 
<a name="l15718"><span class="ln">15718 </span></a>    :attr:`bins` can be a sequence of N 1D tensors, a sequence of N ints, or a single int. 
<a name="l15719"><span class="ln">15719 </span></a> 
<a name="l15720"><span class="ln">15720 </span></a>    If :attr:`bins` is a sequence of N 1D tensors, it explicitly specifies the N sequences 
<a name="l15721"><span class="ln">15721 </span></a>    of bin edges. Each 1D tensor should contain a strictly increasing sequence with at 
<a name="l15722"><span class="ln">15722 </span></a>    least one element. A sequence of K bin edges defines K-1 bins, explicitly specifying 
<a name="l15723"><span class="ln">15723 </span></a>    the left and right edges of all bins. Every bin is exclusive of its left edge. Only 
<a name="l15724"><span class="ln">15724 </span></a>    the rightmost bin is inclusive of its right edge. 
<a name="l15725"><span class="ln">15725 </span></a> 
<a name="l15726"><span class="ln">15726 </span></a>    If :attr:`bins` is a sequence of N ints, it specifies the number of equal-width bins 
<a name="l15727"><span class="ln">15727 </span></a>    in each dimension. By default, the leftmost and rightmost bin edges in each dimension 
<a name="l15728"><span class="ln">15728 </span></a>    are determined by the minimum and maximum elements of the input tensor in the 
<a name="l15729"><span class="ln">15729 </span></a>    corresponding dimension. The :attr:`range` argument can be provided to manually 
<a name="l15730"><span class="ln">15730 </span></a>    specify the leftmost and rightmost bin edges in each dimension. 
<a name="l15731"><span class="ln">15731 </span></a> 
<a name="l15732"><span class="ln">15732 </span></a>    If :attr:`bins` is an int, it specifies the number of equal-width bins for all dimensions. 
<a name="l15733"><span class="ln">15733 </span></a> 
<a name="l15734"><span class="ln">15734 </span></a>    .. note:: 
<a name="l15735"><span class="ln">15735 </span></a>        See also :func:`torch.histogram`, which specifically computes 1D histograms. 
<a name="l15736"><span class="ln">15736 </span></a>        While :func:`torch.histogramdd` infers the dimensionality of its bins and 
<a name="l15737"><span class="ln">15737 </span></a>        binned values from the shape of :attr:`input`, :func:`torch.histogram` 
<a name="l15738"><span class="ln">15738 </span></a>        accepts and flattens :attr:`input` of any shape. 
<a name="l15739"><span class="ln">15739 </span></a> 
<a name="l15740"><span class="ln">15740 </span></a>    Args: 
<a name="l15741"><span class="ln">15741 </span></a>        input (Tensor): the input tensor. 
<a name="l15742"><span class="ln">15742 </span></a>        bins: Tensor[], int[], or int. 
<a name="l15743"><span class="ln">15743 </span></a>                If Tensor[], defines the sequences of bin edges. 
<a name="l15744"><span class="ln">15744 </span></a>                If int[], defines the number of equal-width bins in each dimension. 
<a name="l15745"><span class="ln">15745 </span></a>                If int, defines the number of equal-width bins for all dimensions. 
<a name="l15746"><span class="ln">15746 </span></a>    Keyword args: 
<a name="l15747"><span class="ln">15747 </span></a>        range (sequence of float): Defines the leftmost and rightmost bin edges 
<a name="l15748"><span class="ln">15748 </span></a>                                    in each dimension. 
<a name="l15749"><span class="ln">15749 </span></a>        weight (Tensor): By default, each value in the input has weight 1. If a weight 
<a name="l15750"><span class="ln">15750 </span></a>                            tensor is passed, each N-dimensional coordinate in input 
<a name="l15751"><span class="ln">15751 </span></a>                            contributes its associated weight towards its bin's result. 
<a name="l15752"><span class="ln">15752 </span></a>                            The weight tensor should have the same shape as the :attr:`input` 
<a name="l15753"><span class="ln">15753 </span></a>                            tensor excluding its innermost dimension N. 
<a name="l15754"><span class="ln">15754 </span></a>        density (bool): If False (default), the result will contain the count (or total weight) 
<a name="l15755"><span class="ln">15755 </span></a>                        in each bin. If True, each count (weight) is divided by the total count 
<a name="l15756"><span class="ln">15756 </span></a>                        (total weight), then divided by the volume of its associated bin. 
<a name="l15757"><span class="ln">15757 </span></a>    Returns: 
<a name="l15758"><span class="ln">15758 </span></a>        hist (Tensor): N-dimensional Tensor containing the values of the histogram. 
<a name="l15759"><span class="ln">15759 </span></a>        bin_edges(Tensor[]): sequence of N 1D Tensors containing the bin edges. 
<a name="l15760"><span class="ln">15760 </span></a> 
<a name="l15761"><span class="ln">15761 </span></a>    Example:: 
<a name="l15762"><span class="ln">15762 </span></a> 
<a name="l15763"><span class="ln">15763 </span></a>        &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 1.], [1., 0.], [2., 0.], [2., 2.]]), bins=[3, 3], 
<a name="l15764"><span class="ln">15764 </span></a>        ...                   weight=torch.tensor([1., 2., 4., 8.])) 
<a name="l15765"><span class="ln">15765 </span></a>            torch.return_types.histogramdd( 
<a name="l15766"><span class="ln">15766 </span></a>                hist=tensor([[0., 1., 0.], 
<a name="l15767"><span class="ln">15767 </span></a>                             [2., 0., 0.], 
<a name="l15768"><span class="ln">15768 </span></a>                             [4., 0., 8.]]), 
<a name="l15769"><span class="ln">15769 </span></a>                bin_edges=(tensor([0.0000, 0.6667, 1.3333, 2.0000]), 
<a name="l15770"><span class="ln">15770 </span></a>                           tensor([0.0000, 0.6667, 1.3333, 2.0000]))) 
<a name="l15771"><span class="ln">15771 </span></a> 
<a name="l15772"><span class="ln">15772 </span></a>        &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 0.], [1., 1.], [2., 2.]]), bins=[2, 2], 
<a name="l15773"><span class="ln">15773 </span></a>        ...                   range=[0., 1., 0., 1.], density=True) 
<a name="l15774"><span class="ln">15774 </span></a>            torch.return_types.histogramdd( 
<a name="l15775"><span class="ln">15775 </span></a>               hist=tensor([[2., 0.], 
<a name="l15776"><span class="ln">15776 </span></a>                            [0., 2.]]), 
<a name="l15777"><span class="ln">15777 </span></a>               bin_edges=(tensor([0.0000, 0.5000, 1.0000]), 
<a name="l15778"><span class="ln">15778 </span></a>                          tensor([0.0000, 0.5000, 1.0000]))) 
<a name="l15779"><span class="ln">15779 </span></a>    &quot;&quot;&quot;</span>
<a name="l15780"><span class="ln">15780 </span></a>
<a name="l15781"><span class="ln">15781 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15782"><span class="ln">15782 </span></a><span class="s2">def </span><span class="s1">histogramdd</span><span class="s3">(</span>
<a name="l15783"><span class="ln">15783 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15784"><span class="ln">15784 </span></a>    <span class="s1">bins</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l15785"><span class="ln">15785 </span></a>    <span class="s1">range</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_float</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15786"><span class="ln">15786 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15787"><span class="ln">15787 </span></a>    <span class="s1">density</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l15788"><span class="ln">15788 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">histogramdd</span><span class="s2">:</span>
<a name="l15789"><span class="ln">15789 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15790"><span class="ln">15790 </span></a>    histogramdd(input, bins, *, range=None, weight=None, density=False, out=None) -&gt; (Tensor, Tensor[]) 
<a name="l15791"><span class="ln">15791 </span></a> 
<a name="l15792"><span class="ln">15792 </span></a>    Computes a multi-dimensional histogram of the values in a tensor. 
<a name="l15793"><span class="ln">15793 </span></a> 
<a name="l15794"><span class="ln">15794 </span></a>    Interprets the elements of an input tensor whose innermost dimension has size N 
<a name="l15795"><span class="ln">15795 </span></a>    as a collection of N-dimensional points. Maps each of the points into a set of 
<a name="l15796"><span class="ln">15796 </span></a>    N-dimensional bins and returns the number of points (or total weight) in each bin. 
<a name="l15797"><span class="ln">15797 </span></a> 
<a name="l15798"><span class="ln">15798 </span></a>    :attr:`input` must be a tensor with at least 2 dimensions. 
<a name="l15799"><span class="ln">15799 </span></a>    If input has shape (M, N), each of its M rows defines a point in N-dimensional space. 
<a name="l15800"><span class="ln">15800 </span></a>    If input has three or more dimensions, all but the last dimension are flattened. 
<a name="l15801"><span class="ln">15801 </span></a> 
<a name="l15802"><span class="ln">15802 </span></a>    Each dimension is independently associated with its own strictly increasing sequence 
<a name="l15803"><span class="ln">15803 </span></a>    of bin edges. Bin edges may be specified explicitly by passing a sequence of 1D 
<a name="l15804"><span class="ln">15804 </span></a>    tensors. Alternatively, bin edges may be constructed automatically by passing a 
<a name="l15805"><span class="ln">15805 </span></a>    sequence of integers specifying the number of equal-width bins in each dimension. 
<a name="l15806"><span class="ln">15806 </span></a> 
<a name="l15807"><span class="ln">15807 </span></a>    For each N-dimensional point in input: 
<a name="l15808"><span class="ln">15808 </span></a>        - Each of its coordinates is binned independently among the bin edges 
<a name="l15809"><span class="ln">15809 </span></a>            corresponding to its dimension 
<a name="l15810"><span class="ln">15810 </span></a>        - Binning results are combined to identify the N-dimensional bin (if any) 
<a name="l15811"><span class="ln">15811 </span></a>            into which the point falls 
<a name="l15812"><span class="ln">15812 </span></a>        - If the point falls into a bin, the bin's count (or total weight) is incremented 
<a name="l15813"><span class="ln">15813 </span></a>        - Points which do not fall into any bin do not contribute to the output 
<a name="l15814"><span class="ln">15814 </span></a> 
<a name="l15815"><span class="ln">15815 </span></a>    :attr:`bins` can be a sequence of N 1D tensors, a sequence of N ints, or a single int. 
<a name="l15816"><span class="ln">15816 </span></a> 
<a name="l15817"><span class="ln">15817 </span></a>    If :attr:`bins` is a sequence of N 1D tensors, it explicitly specifies the N sequences 
<a name="l15818"><span class="ln">15818 </span></a>    of bin edges. Each 1D tensor should contain a strictly increasing sequence with at 
<a name="l15819"><span class="ln">15819 </span></a>    least one element. A sequence of K bin edges defines K-1 bins, explicitly specifying 
<a name="l15820"><span class="ln">15820 </span></a>    the left and right edges of all bins. Every bin is exclusive of its left edge. Only 
<a name="l15821"><span class="ln">15821 </span></a>    the rightmost bin is inclusive of its right edge. 
<a name="l15822"><span class="ln">15822 </span></a> 
<a name="l15823"><span class="ln">15823 </span></a>    If :attr:`bins` is a sequence of N ints, it specifies the number of equal-width bins 
<a name="l15824"><span class="ln">15824 </span></a>    in each dimension. By default, the leftmost and rightmost bin edges in each dimension 
<a name="l15825"><span class="ln">15825 </span></a>    are determined by the minimum and maximum elements of the input tensor in the 
<a name="l15826"><span class="ln">15826 </span></a>    corresponding dimension. The :attr:`range` argument can be provided to manually 
<a name="l15827"><span class="ln">15827 </span></a>    specify the leftmost and rightmost bin edges in each dimension. 
<a name="l15828"><span class="ln">15828 </span></a> 
<a name="l15829"><span class="ln">15829 </span></a>    If :attr:`bins` is an int, it specifies the number of equal-width bins for all dimensions. 
<a name="l15830"><span class="ln">15830 </span></a> 
<a name="l15831"><span class="ln">15831 </span></a>    .. note:: 
<a name="l15832"><span class="ln">15832 </span></a>        See also :func:`torch.histogram`, which specifically computes 1D histograms. 
<a name="l15833"><span class="ln">15833 </span></a>        While :func:`torch.histogramdd` infers the dimensionality of its bins and 
<a name="l15834"><span class="ln">15834 </span></a>        binned values from the shape of :attr:`input`, :func:`torch.histogram` 
<a name="l15835"><span class="ln">15835 </span></a>        accepts and flattens :attr:`input` of any shape. 
<a name="l15836"><span class="ln">15836 </span></a> 
<a name="l15837"><span class="ln">15837 </span></a>    Args: 
<a name="l15838"><span class="ln">15838 </span></a>        input (Tensor): the input tensor. 
<a name="l15839"><span class="ln">15839 </span></a>        bins: Tensor[], int[], or int. 
<a name="l15840"><span class="ln">15840 </span></a>                If Tensor[], defines the sequences of bin edges. 
<a name="l15841"><span class="ln">15841 </span></a>                If int[], defines the number of equal-width bins in each dimension. 
<a name="l15842"><span class="ln">15842 </span></a>                If int, defines the number of equal-width bins for all dimensions. 
<a name="l15843"><span class="ln">15843 </span></a>    Keyword args: 
<a name="l15844"><span class="ln">15844 </span></a>        range (sequence of float): Defines the leftmost and rightmost bin edges 
<a name="l15845"><span class="ln">15845 </span></a>                                    in each dimension. 
<a name="l15846"><span class="ln">15846 </span></a>        weight (Tensor): By default, each value in the input has weight 1. If a weight 
<a name="l15847"><span class="ln">15847 </span></a>                            tensor is passed, each N-dimensional coordinate in input 
<a name="l15848"><span class="ln">15848 </span></a>                            contributes its associated weight towards its bin's result. 
<a name="l15849"><span class="ln">15849 </span></a>                            The weight tensor should have the same shape as the :attr:`input` 
<a name="l15850"><span class="ln">15850 </span></a>                            tensor excluding its innermost dimension N. 
<a name="l15851"><span class="ln">15851 </span></a>        density (bool): If False (default), the result will contain the count (or total weight) 
<a name="l15852"><span class="ln">15852 </span></a>                        in each bin. If True, each count (weight) is divided by the total count 
<a name="l15853"><span class="ln">15853 </span></a>                        (total weight), then divided by the volume of its associated bin. 
<a name="l15854"><span class="ln">15854 </span></a>    Returns: 
<a name="l15855"><span class="ln">15855 </span></a>        hist (Tensor): N-dimensional Tensor containing the values of the histogram. 
<a name="l15856"><span class="ln">15856 </span></a>        bin_edges(Tensor[]): sequence of N 1D Tensors containing the bin edges. 
<a name="l15857"><span class="ln">15857 </span></a> 
<a name="l15858"><span class="ln">15858 </span></a>    Example:: 
<a name="l15859"><span class="ln">15859 </span></a> 
<a name="l15860"><span class="ln">15860 </span></a>        &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 1.], [1., 0.], [2., 0.], [2., 2.]]), bins=[3, 3], 
<a name="l15861"><span class="ln">15861 </span></a>        ...                   weight=torch.tensor([1., 2., 4., 8.])) 
<a name="l15862"><span class="ln">15862 </span></a>            torch.return_types.histogramdd( 
<a name="l15863"><span class="ln">15863 </span></a>                hist=tensor([[0., 1., 0.], 
<a name="l15864"><span class="ln">15864 </span></a>                             [2., 0., 0.], 
<a name="l15865"><span class="ln">15865 </span></a>                             [4., 0., 8.]]), 
<a name="l15866"><span class="ln">15866 </span></a>                bin_edges=(tensor([0.0000, 0.6667, 1.3333, 2.0000]), 
<a name="l15867"><span class="ln">15867 </span></a>                           tensor([0.0000, 0.6667, 1.3333, 2.0000]))) 
<a name="l15868"><span class="ln">15868 </span></a> 
<a name="l15869"><span class="ln">15869 </span></a>        &gt;&gt;&gt; torch.histogramdd(torch.tensor([[0., 0.], [1., 1.], [2., 2.]]), bins=[2, 2], 
<a name="l15870"><span class="ln">15870 </span></a>        ...                   range=[0., 1., 0., 1.], density=True) 
<a name="l15871"><span class="ln">15871 </span></a>            torch.return_types.histogramdd( 
<a name="l15872"><span class="ln">15872 </span></a>               hist=tensor([[2., 0.], 
<a name="l15873"><span class="ln">15873 </span></a>                            [0., 2.]]), 
<a name="l15874"><span class="ln">15874 </span></a>               bin_edges=(tensor([0.0000, 0.5000, 1.0000]), 
<a name="l15875"><span class="ln">15875 </span></a>                          tensor([0.0000, 0.5000, 1.0000]))) 
<a name="l15876"><span class="ln">15876 </span></a>    &quot;&quot;&quot;</span>
<a name="l15877"><span class="ln">15877 </span></a>
<a name="l15878"><span class="ln">15878 </span></a><span class="s2">def </span><span class="s1">hsmm</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l15879"><span class="ln">15879 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15880"><span class="ln">15880 </span></a><span class="s2">def </span><span class="s1">hsplit</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">sections</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l15881"><span class="ln">15881 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15882"><span class="ln">15882 </span></a>    hsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l15883"><span class="ln">15883 </span></a> 
<a name="l15884"><span class="ln">15884 </span></a>    Splits :attr:`input`, a tensor with one or more dimensions, into multiple tensors 
<a name="l15885"><span class="ln">15885 </span></a>    horizontally according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l15886"><span class="ln">15886 </span></a>    :attr:`input`. 
<a name="l15887"><span class="ln">15887 </span></a> 
<a name="l15888"><span class="ln">15888 </span></a>    If :attr:`input` is one dimensional this is equivalent to calling 
<a name="l15889"><span class="ln">15889 </span></a>    torch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is 
<a name="l15890"><span class="ln">15890 </span></a>    zero), and if :attr:`input` has two or more dimensions it's equivalent to calling 
<a name="l15891"><span class="ln">15891 </span></a>    torch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1), 
<a name="l15892"><span class="ln">15892 </span></a>    except that if :attr:`indices_or_sections` is an integer it must evenly divide 
<a name="l15893"><span class="ln">15893 </span></a>    the split dimension or a runtime error will be thrown. 
<a name="l15894"><span class="ln">15894 </span></a> 
<a name="l15895"><span class="ln">15895 </span></a>    This function is based on NumPy's :func:`numpy.hsplit`. 
<a name="l15896"><span class="ln">15896 </span></a> 
<a name="l15897"><span class="ln">15897 </span></a>    Args: 
<a name="l15898"><span class="ln">15898 </span></a>        input (Tensor): tensor to split. 
<a name="l15899"><span class="ln">15899 </span></a>        indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l15900"><span class="ln">15900 </span></a> 
<a name="l15901"><span class="ln">15901 </span></a>    Example:: 
<a name="l15902"><span class="ln">15902 </span></a> 
<a name="l15903"><span class="ln">15903 </span></a>        &gt;&gt;&gt; t = torch.arange(16.0).reshape(4,4) 
<a name="l15904"><span class="ln">15904 </span></a>        &gt;&gt;&gt; t 
<a name="l15905"><span class="ln">15905 </span></a>        tensor([[ 0.,  1.,  2.,  3.], 
<a name="l15906"><span class="ln">15906 </span></a>                [ 4.,  5.,  6.,  7.], 
<a name="l15907"><span class="ln">15907 </span></a>                [ 8.,  9., 10., 11.], 
<a name="l15908"><span class="ln">15908 </span></a>                [12., 13., 14., 15.]]) 
<a name="l15909"><span class="ln">15909 </span></a>        &gt;&gt;&gt; torch.hsplit(t, 2) 
<a name="l15910"><span class="ln">15910 </span></a>        (tensor([[ 0.,  1.], 
<a name="l15911"><span class="ln">15911 </span></a>                 [ 4.,  5.], 
<a name="l15912"><span class="ln">15912 </span></a>                 [ 8.,  9.], 
<a name="l15913"><span class="ln">15913 </span></a>                 [12., 13.]]), 
<a name="l15914"><span class="ln">15914 </span></a>         tensor([[ 2.,  3.], 
<a name="l15915"><span class="ln">15915 </span></a>                 [ 6.,  7.], 
<a name="l15916"><span class="ln">15916 </span></a>                 [10., 11.], 
<a name="l15917"><span class="ln">15917 </span></a>                 [14., 15.]])) 
<a name="l15918"><span class="ln">15918 </span></a>        &gt;&gt;&gt; torch.hsplit(t, [3, 6]) 
<a name="l15919"><span class="ln">15919 </span></a>        (tensor([[ 0.,  1.,  2.], 
<a name="l15920"><span class="ln">15920 </span></a>                 [ 4.,  5.,  6.], 
<a name="l15921"><span class="ln">15921 </span></a>                 [ 8.,  9., 10.], 
<a name="l15922"><span class="ln">15922 </span></a>                 [12., 13., 14.]]), 
<a name="l15923"><span class="ln">15923 </span></a>         tensor([[ 3.], 
<a name="l15924"><span class="ln">15924 </span></a>                 [ 7.], 
<a name="l15925"><span class="ln">15925 </span></a>                 [11.], 
<a name="l15926"><span class="ln">15926 </span></a>                 [15.]]), 
<a name="l15927"><span class="ln">15927 </span></a>         tensor([], size=(4, 0))) 
<a name="l15928"><span class="ln">15928 </span></a>    &quot;&quot;&quot;</span>
<a name="l15929"><span class="ln">15929 </span></a>
<a name="l15930"><span class="ln">15930 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l15931"><span class="ln">15931 </span></a><span class="s2">def </span><span class="s1">hsplit</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">indices</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l15932"><span class="ln">15932 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15933"><span class="ln">15933 </span></a>    hsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l15934"><span class="ln">15934 </span></a> 
<a name="l15935"><span class="ln">15935 </span></a>    Splits :attr:`input`, a tensor with one or more dimensions, into multiple tensors 
<a name="l15936"><span class="ln">15936 </span></a>    horizontally according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l15937"><span class="ln">15937 </span></a>    :attr:`input`. 
<a name="l15938"><span class="ln">15938 </span></a> 
<a name="l15939"><span class="ln">15939 </span></a>    If :attr:`input` is one dimensional this is equivalent to calling 
<a name="l15940"><span class="ln">15940 </span></a>    torch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is 
<a name="l15941"><span class="ln">15941 </span></a>    zero), and if :attr:`input` has two or more dimensions it's equivalent to calling 
<a name="l15942"><span class="ln">15942 </span></a>    torch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1), 
<a name="l15943"><span class="ln">15943 </span></a>    except that if :attr:`indices_or_sections` is an integer it must evenly divide 
<a name="l15944"><span class="ln">15944 </span></a>    the split dimension or a runtime error will be thrown. 
<a name="l15945"><span class="ln">15945 </span></a> 
<a name="l15946"><span class="ln">15946 </span></a>    This function is based on NumPy's :func:`numpy.hsplit`. 
<a name="l15947"><span class="ln">15947 </span></a> 
<a name="l15948"><span class="ln">15948 </span></a>    Args: 
<a name="l15949"><span class="ln">15949 </span></a>        input (Tensor): tensor to split. 
<a name="l15950"><span class="ln">15950 </span></a>        indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l15951"><span class="ln">15951 </span></a> 
<a name="l15952"><span class="ln">15952 </span></a>    Example:: 
<a name="l15953"><span class="ln">15953 </span></a> 
<a name="l15954"><span class="ln">15954 </span></a>        &gt;&gt;&gt; t = torch.arange(16.0).reshape(4,4) 
<a name="l15955"><span class="ln">15955 </span></a>        &gt;&gt;&gt; t 
<a name="l15956"><span class="ln">15956 </span></a>        tensor([[ 0.,  1.,  2.,  3.], 
<a name="l15957"><span class="ln">15957 </span></a>                [ 4.,  5.,  6.,  7.], 
<a name="l15958"><span class="ln">15958 </span></a>                [ 8.,  9., 10., 11.], 
<a name="l15959"><span class="ln">15959 </span></a>                [12., 13., 14., 15.]]) 
<a name="l15960"><span class="ln">15960 </span></a>        &gt;&gt;&gt; torch.hsplit(t, 2) 
<a name="l15961"><span class="ln">15961 </span></a>        (tensor([[ 0.,  1.], 
<a name="l15962"><span class="ln">15962 </span></a>                 [ 4.,  5.], 
<a name="l15963"><span class="ln">15963 </span></a>                 [ 8.,  9.], 
<a name="l15964"><span class="ln">15964 </span></a>                 [12., 13.]]), 
<a name="l15965"><span class="ln">15965 </span></a>         tensor([[ 2.,  3.], 
<a name="l15966"><span class="ln">15966 </span></a>                 [ 6.,  7.], 
<a name="l15967"><span class="ln">15967 </span></a>                 [10., 11.], 
<a name="l15968"><span class="ln">15968 </span></a>                 [14., 15.]])) 
<a name="l15969"><span class="ln">15969 </span></a>        &gt;&gt;&gt; torch.hsplit(t, [3, 6]) 
<a name="l15970"><span class="ln">15970 </span></a>        (tensor([[ 0.,  1.,  2.], 
<a name="l15971"><span class="ln">15971 </span></a>                 [ 4.,  5.,  6.], 
<a name="l15972"><span class="ln">15972 </span></a>                 [ 8.,  9., 10.], 
<a name="l15973"><span class="ln">15973 </span></a>                 [12., 13., 14.]]), 
<a name="l15974"><span class="ln">15974 </span></a>         tensor([[ 3.], 
<a name="l15975"><span class="ln">15975 </span></a>                 [ 7.], 
<a name="l15976"><span class="ln">15976 </span></a>                 [11.], 
<a name="l15977"><span class="ln">15977 </span></a>                 [15.]]), 
<a name="l15978"><span class="ln">15978 </span></a>         tensor([], size=(4, 0))) 
<a name="l15979"><span class="ln">15979 </span></a>    &quot;&quot;&quot;</span>
<a name="l15980"><span class="ln">15980 </span></a>
<a name="l15981"><span class="ln">15981 </span></a><span class="s2">def </span><span class="s1">hspmm</span><span class="s3">(</span>
<a name="l15982"><span class="ln">15982 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15983"><span class="ln">15983 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l15984"><span class="ln">15984 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l15985"><span class="ln">15985 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l15986"><span class="ln">15986 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l15987"><span class="ln">15987 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l15988"><span class="ln">15988 </span></a>    hspmm(mat1, mat2, *, out=None) -&gt; Tensor 
<a name="l15989"><span class="ln">15989 </span></a> 
<a name="l15990"><span class="ln">15990 </span></a>    Performs a matrix multiplication of a :ref:`sparse COO matrix 
<a name="l15991"><span class="ln">15991 </span></a>    &lt;sparse-coo-docs&gt;` :attr:`mat1` and a strided matrix :attr:`mat2`. The 
<a name="l15992"><span class="ln">15992 </span></a>    result is a (1 + 1)-dimensional :ref:`hybrid COO matrix 
<a name="l15993"><span class="ln">15993 </span></a>    &lt;sparse-hybrid-coo-docs&gt;`. 
<a name="l15994"><span class="ln">15994 </span></a> 
<a name="l15995"><span class="ln">15995 </span></a>    Args: 
<a name="l15996"><span class="ln">15996 </span></a>        mat1 (Tensor): the first sparse matrix to be matrix multiplied 
<a name="l15997"><span class="ln">15997 </span></a>        mat2 (Tensor): the second strided matrix to be matrix multiplied 
<a name="l15998"><span class="ln">15998 </span></a> 
<a name="l15999"><span class="ln">15999 </span></a>    Keyword args: 
<a name="l16000"><span class="ln">16000 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l16001"><span class="ln">16001 </span></a>    &quot;&quot;&quot;</span>
<a name="l16002"><span class="ln">16002 </span></a>
<a name="l16003"><span class="ln">16003 </span></a><span class="s2">def </span><span class="s1">hstack</span><span class="s3">(</span>
<a name="l16004"><span class="ln">16004 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16005"><span class="ln">16005 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16006"><span class="ln">16006 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16007"><span class="ln">16007 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16008"><span class="ln">16008 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16009"><span class="ln">16009 </span></a>    hstack(tensors, *, out=None) -&gt; Tensor 
<a name="l16010"><span class="ln">16010 </span></a> 
<a name="l16011"><span class="ln">16011 </span></a>    Stack tensors in sequence horizontally (column wise). 
<a name="l16012"><span class="ln">16012 </span></a> 
<a name="l16013"><span class="ln">16013 </span></a>    This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors. 
<a name="l16014"><span class="ln">16014 </span></a> 
<a name="l16015"><span class="ln">16015 </span></a>    Args: 
<a name="l16016"><span class="ln">16016 </span></a>        tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l16017"><span class="ln">16017 </span></a> 
<a name="l16018"><span class="ln">16018 </span></a>    Keyword args: 
<a name="l16019"><span class="ln">16019 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l16020"><span class="ln">16020 </span></a> 
<a name="l16021"><span class="ln">16021 </span></a>    Example:: 
<a name="l16022"><span class="ln">16022 </span></a> 
<a name="l16023"><span class="ln">16023 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l16024"><span class="ln">16024 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l16025"><span class="ln">16025 </span></a>        &gt;&gt;&gt; torch.hstack((a,b)) 
<a name="l16026"><span class="ln">16026 </span></a>        tensor([1, 2, 3, 4, 5, 6]) 
<a name="l16027"><span class="ln">16027 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1],[2],[3]]) 
<a name="l16028"><span class="ln">16028 </span></a>        &gt;&gt;&gt; b = torch.tensor([[4],[5],[6]]) 
<a name="l16029"><span class="ln">16029 </span></a>        &gt;&gt;&gt; torch.hstack((a,b)) 
<a name="l16030"><span class="ln">16030 </span></a>        tensor([[1, 4], 
<a name="l16031"><span class="ln">16031 </span></a>                [2, 5], 
<a name="l16032"><span class="ln">16032 </span></a>                [3, 6]]) 
<a name="l16033"><span class="ln">16033 </span></a>    &quot;&quot;&quot;</span>
<a name="l16034"><span class="ln">16034 </span></a>
<a name="l16035"><span class="ln">16035 </span></a><span class="s2">def </span><span class="s1">hypot</span><span class="s3">(</span>
<a name="l16036"><span class="ln">16036 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16037"><span class="ln">16037 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16038"><span class="ln">16038 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16039"><span class="ln">16039 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16040"><span class="ln">16040 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16041"><span class="ln">16041 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16042"><span class="ln">16042 </span></a>    hypot(input, other, *, out=None) -&gt; Tensor 
<a name="l16043"><span class="ln">16043 </span></a> 
<a name="l16044"><span class="ln">16044 </span></a>    Given the legs of a right triangle, return its hypotenuse. 
<a name="l16045"><span class="ln">16045 </span></a> 
<a name="l16046"><span class="ln">16046 </span></a>    .. math:: 
<a name="l16047"><span class="ln">16047 </span></a>        \text{out}_{i} = \sqrt{\text{input}_{i}^{2} + \text{other}_{i}^{2}} 
<a name="l16048"><span class="ln">16048 </span></a> 
<a name="l16049"><span class="ln">16049 </span></a>    The shapes of ``input`` and ``other`` must be 
<a name="l16050"><span class="ln">16050 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l16051"><span class="ln">16051 </span></a> 
<a name="l16052"><span class="ln">16052 </span></a>    Args: 
<a name="l16053"><span class="ln">16053 </span></a>        input (Tensor): the first input tensor 
<a name="l16054"><span class="ln">16054 </span></a>        other (Tensor): the second input tensor 
<a name="l16055"><span class="ln">16055 </span></a> 
<a name="l16056"><span class="ln">16056 </span></a>    Keyword args: 
<a name="l16057"><span class="ln">16057 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l16058"><span class="ln">16058 </span></a> 
<a name="l16059"><span class="ln">16059 </span></a>    Example:: 
<a name="l16060"><span class="ln">16060 </span></a> 
<a name="l16061"><span class="ln">16061 </span></a>        &gt;&gt;&gt; a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0])) 
<a name="l16062"><span class="ln">16062 </span></a>        tensor([5.0000, 5.6569, 6.4031]) 
<a name="l16063"><span class="ln">16063 </span></a>    &quot;&quot;&quot;</span>
<a name="l16064"><span class="ln">16064 </span></a>
<a name="l16065"><span class="ln">16065 </span></a><span class="s2">def </span><span class="s1">i0</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16066"><span class="ln">16066 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16067"><span class="ln">16067 </span></a>    i0(input, *, out=None) -&gt; Tensor 
<a name="l16068"><span class="ln">16068 </span></a> 
<a name="l16069"><span class="ln">16069 </span></a>    Alias for :func:`torch.special.i0`. 
<a name="l16070"><span class="ln">16070 </span></a>    &quot;&quot;&quot;</span>
<a name="l16071"><span class="ln">16071 </span></a>
<a name="l16072"><span class="ln">16072 </span></a><span class="s2">def </span><span class="s1">i0_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16073"><span class="ln">16073 </span></a><span class="s2">def </span><span class="s1">igamma</span><span class="s3">(</span>
<a name="l16074"><span class="ln">16074 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16075"><span class="ln">16075 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16076"><span class="ln">16076 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16077"><span class="ln">16077 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16078"><span class="ln">16078 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16079"><span class="ln">16079 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16080"><span class="ln">16080 </span></a>    igamma(input, other, *, out=None) -&gt; Tensor 
<a name="l16081"><span class="ln">16081 </span></a> 
<a name="l16082"><span class="ln">16082 </span></a>    Alias for :func:`torch.special.gammainc`. 
<a name="l16083"><span class="ln">16083 </span></a>    &quot;&quot;&quot;</span>
<a name="l16084"><span class="ln">16084 </span></a>
<a name="l16085"><span class="ln">16085 </span></a><span class="s2">def </span><span class="s1">igammac</span><span class="s3">(</span>
<a name="l16086"><span class="ln">16086 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16087"><span class="ln">16087 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16088"><span class="ln">16088 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16089"><span class="ln">16089 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16090"><span class="ln">16090 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16091"><span class="ln">16091 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16092"><span class="ln">16092 </span></a>    igammac(input, other, *, out=None) -&gt; Tensor 
<a name="l16093"><span class="ln">16093 </span></a> 
<a name="l16094"><span class="ln">16094 </span></a>    Alias for :func:`torch.special.gammaincc`. 
<a name="l16095"><span class="ln">16095 </span></a>    &quot;&quot;&quot;</span>
<a name="l16096"><span class="ln">16096 </span></a>
<a name="l16097"><span class="ln">16097 </span></a><span class="s2">def </span><span class="s1">imag</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16098"><span class="ln">16098 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16099"><span class="ln">16099 </span></a>    imag(input) -&gt; Tensor 
<a name="l16100"><span class="ln">16100 </span></a> 
<a name="l16101"><span class="ln">16101 </span></a>    Returns a new tensor containing imaginary values of the :attr:`self` tensor. 
<a name="l16102"><span class="ln">16102 </span></a>    The returned tensor and :attr:`self` share the same underlying storage. 
<a name="l16103"><span class="ln">16103 </span></a> 
<a name="l16104"><span class="ln">16104 </span></a>    .. warning:: 
<a name="l16105"><span class="ln">16105 </span></a>        :func:`imag` is only supported for tensors with complex dtypes. 
<a name="l16106"><span class="ln">16106 </span></a> 
<a name="l16107"><span class="ln">16107 </span></a>    Args: 
<a name="l16108"><span class="ln">16108 </span></a>        input (Tensor): the input tensor. 
<a name="l16109"><span class="ln">16109 </span></a> 
<a name="l16110"><span class="ln">16110 </span></a>    Example:: 
<a name="l16111"><span class="ln">16111 </span></a> 
<a name="l16112"><span class="ln">16112 </span></a>        &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l16113"><span class="ln">16113 </span></a>        &gt;&gt;&gt; x 
<a name="l16114"><span class="ln">16114 </span></a>        tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)]) 
<a name="l16115"><span class="ln">16115 </span></a>        &gt;&gt;&gt; x.imag 
<a name="l16116"><span class="ln">16116 </span></a>        tensor([ 0.3553, -0.7896, -0.0633, -0.8119]) 
<a name="l16117"><span class="ln">16117 </span></a>    &quot;&quot;&quot;</span>
<a name="l16118"><span class="ln">16118 </span></a>
<a name="l16119"><span class="ln">16119 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16120"><span class="ln">16120 </span></a><span class="s2">def </span><span class="s1">index_add</span><span class="s3">(</span>
<a name="l16121"><span class="ln">16121 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16122"><span class="ln">16122 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16123"><span class="ln">16123 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16124"><span class="ln">16124 </span></a>    <span class="s1">source</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16125"><span class="ln">16125 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16126"><span class="ln">16126 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l16127"><span class="ln">16127 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16128"><span class="ln">16128 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16129"><span class="ln">16129 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16130"><span class="ln">16130 </span></a>    index_add(input: Tensor, dim: int, index: Tensor, source: Tensor, *, alpha: Union[Number, _complex] = 1, out: Optional[Tensor]) -&gt; Tensor # noqa: B950 
<a name="l16131"><span class="ln">16131 </span></a> 
<a name="l16132"><span class="ln">16132 </span></a>    See :meth:`~Tensor.index_add_` for function description. 
<a name="l16133"><span class="ln">16133 </span></a>    &quot;&quot;&quot;</span>
<a name="l16134"><span class="ln">16134 </span></a>
<a name="l16135"><span class="ln">16135 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16136"><span class="ln">16136 </span></a><span class="s2">def </span><span class="s1">index_add</span><span class="s3">(</span>
<a name="l16137"><span class="ln">16137 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16138"><span class="ln">16138 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16139"><span class="ln">16139 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16140"><span class="ln">16140 </span></a>    <span class="s1">source</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16141"><span class="ln">16141 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16142"><span class="ln">16142 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l16143"><span class="ln">16143 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16144"><span class="ln">16144 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16145"><span class="ln">16145 </span></a>    index_add(input: Tensor, dim: int, index: Tensor, source: Tensor, *, alpha: Union[Number, _complex] = 1, out: Optional[Tensor]) -&gt; Tensor # noqa: B950 
<a name="l16146"><span class="ln">16146 </span></a> 
<a name="l16147"><span class="ln">16147 </span></a>    See :meth:`~Tensor.index_add_` for function description. 
<a name="l16148"><span class="ln">16148 </span></a>    &quot;&quot;&quot;</span>
<a name="l16149"><span class="ln">16149 </span></a>
<a name="l16150"><span class="ln">16150 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16151"><span class="ln">16151 </span></a><span class="s2">def </span><span class="s1">index_copy</span><span class="s3">(</span>
<a name="l16152"><span class="ln">16152 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16153"><span class="ln">16153 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16154"><span class="ln">16154 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16155"><span class="ln">16155 </span></a>    <span class="s1">source</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16156"><span class="ln">16156 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16157"><span class="ln">16157 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16158"><span class="ln">16158 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16159"><span class="ln">16159 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16160"><span class="ln">16160 </span></a>    index_copy(input: Tensor, dim: int, index: Tensor, source: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l16161"><span class="ln">16161 </span></a> 
<a name="l16162"><span class="ln">16162 </span></a>    See :meth:`~Tensor.index_add_` for function description. 
<a name="l16163"><span class="ln">16163 </span></a>    &quot;&quot;&quot;</span>
<a name="l16164"><span class="ln">16164 </span></a>
<a name="l16165"><span class="ln">16165 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16166"><span class="ln">16166 </span></a><span class="s2">def </span><span class="s1">index_copy</span><span class="s3">(</span>
<a name="l16167"><span class="ln">16167 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16168"><span class="ln">16168 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16169"><span class="ln">16169 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16170"><span class="ln">16170 </span></a>    <span class="s1">source</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16171"><span class="ln">16171 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16172"><span class="ln">16172 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16173"><span class="ln">16173 </span></a>    index_copy(input: Tensor, dim: int, index: Tensor, source: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l16174"><span class="ln">16174 </span></a> 
<a name="l16175"><span class="ln">16175 </span></a>    See :meth:`~Tensor.index_add_` for function description. 
<a name="l16176"><span class="ln">16176 </span></a>    &quot;&quot;&quot;</span>
<a name="l16177"><span class="ln">16177 </span></a>
<a name="l16178"><span class="ln">16178 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16179"><span class="ln">16179 </span></a><span class="s2">def </span><span class="s1">index_fill</span><span class="s3">(</span>
<a name="l16180"><span class="ln">16180 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16181"><span class="ln">16181 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16182"><span class="ln">16182 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16183"><span class="ln">16183 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16184"><span class="ln">16184 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16185"><span class="ln">16185 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16186"><span class="ln">16186 </span></a><span class="s2">def </span><span class="s1">index_fill</span><span class="s3">(</span>
<a name="l16187"><span class="ln">16187 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16188"><span class="ln">16188 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16189"><span class="ln">16189 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16190"><span class="ln">16190 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16191"><span class="ln">16191 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16192"><span class="ln">16192 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16193"><span class="ln">16193 </span></a><span class="s2">def </span><span class="s1">index_fill</span><span class="s3">(</span>
<a name="l16194"><span class="ln">16194 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16195"><span class="ln">16195 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16196"><span class="ln">16196 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16197"><span class="ln">16197 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l16198"><span class="ln">16198 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16199"><span class="ln">16199 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16200"><span class="ln">16200 </span></a><span class="s2">def </span><span class="s1">index_fill</span><span class="s3">(</span>
<a name="l16201"><span class="ln">16201 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16202"><span class="ln">16202 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16203"><span class="ln">16203 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16204"><span class="ln">16204 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l16205"><span class="ln">16205 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16206"><span class="ln">16206 </span></a><span class="s2">def </span><span class="s1">index_put</span><span class="s3">(</span>
<a name="l16207"><span class="ln">16207 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16208"><span class="ln">16208 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16209"><span class="ln">16209 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16210"><span class="ln">16210 </span></a>    <span class="s1">accumulate</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16211"><span class="ln">16211 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16212"><span class="ln">16212 </span></a><span class="s2">def </span><span class="s1">index_put_</span><span class="s3">(</span>
<a name="l16213"><span class="ln">16213 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16214"><span class="ln">16214 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16215"><span class="ln">16215 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16216"><span class="ln">16216 </span></a>    <span class="s1">accumulate</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16217"><span class="ln">16217 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16218"><span class="ln">16218 </span></a><span class="s2">def </span><span class="s1">index_reduce</span><span class="s3">(</span>
<a name="l16219"><span class="ln">16219 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16220"><span class="ln">16220 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16221"><span class="ln">16221 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16222"><span class="ln">16222 </span></a>    <span class="s1">source</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16223"><span class="ln">16223 </span></a>    <span class="s1">reduce</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l16224"><span class="ln">16224 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16225"><span class="ln">16225 </span></a>    <span class="s1">include_self</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l16226"><span class="ln">16226 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16227"><span class="ln">16227 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16228"><span class="ln">16228 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16229"><span class="ln">16229 </span></a>    index_reduce(input: Tensor, dim: int, index: Tensor, source: Tensor, reduce: str, *, include_self: bool = True, out: Optional[Tensor]) -&gt; Tensor # noqa: B950 
<a name="l16230"><span class="ln">16230 </span></a> 
<a name="l16231"><span class="ln">16231 </span></a>    See :meth:`~Tensor.index_reduce_` for function description. 
<a name="l16232"><span class="ln">16232 </span></a>    &quot;&quot;&quot;</span>
<a name="l16233"><span class="ln">16233 </span></a>
<a name="l16234"><span class="ln">16234 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16235"><span class="ln">16235 </span></a><span class="s2">def </span><span class="s1">index_select</span><span class="s3">(</span>
<a name="l16236"><span class="ln">16236 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16237"><span class="ln">16237 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16238"><span class="ln">16238 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16239"><span class="ln">16239 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16240"><span class="ln">16240 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16241"><span class="ln">16241 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16242"><span class="ln">16242 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16243"><span class="ln">16243 </span></a>    index_select(input, dim, index, *, out=None) -&gt; Tensor 
<a name="l16244"><span class="ln">16244 </span></a> 
<a name="l16245"><span class="ln">16245 </span></a>    Returns a new tensor which indexes the :attr:`input` tensor along dimension 
<a name="l16246"><span class="ln">16246 </span></a>    :attr:`dim` using the entries in :attr:`index` which is a `LongTensor`. 
<a name="l16247"><span class="ln">16247 </span></a> 
<a name="l16248"><span class="ln">16248 </span></a>    The returned tensor has the same number of dimensions as the original tensor 
<a name="l16249"><span class="ln">16249 </span></a>    (:attr:`input`).  The :attr:`dim`\ th dimension has the same size as the length 
<a name="l16250"><span class="ln">16250 </span></a>    of :attr:`index`; other dimensions have the same size as in the original tensor. 
<a name="l16251"><span class="ln">16251 </span></a> 
<a name="l16252"><span class="ln">16252 </span></a>    .. note:: The returned tensor does **not** use the same storage as the original 
<a name="l16253"><span class="ln">16253 </span></a>              tensor.  If :attr:`out` has a different shape than expected, we 
<a name="l16254"><span class="ln">16254 </span></a>              silently change it to the correct shape, reallocating the underlying 
<a name="l16255"><span class="ln">16255 </span></a>              storage if necessary. 
<a name="l16256"><span class="ln">16256 </span></a> 
<a name="l16257"><span class="ln">16257 </span></a>    Args: 
<a name="l16258"><span class="ln">16258 </span></a>        input (Tensor): the input tensor. 
<a name="l16259"><span class="ln">16259 </span></a>        dim (int): the dimension in which we index 
<a name="l16260"><span class="ln">16260 </span></a>        index (IntTensor or LongTensor): the 1-D tensor containing the indices to index 
<a name="l16261"><span class="ln">16261 </span></a> 
<a name="l16262"><span class="ln">16262 </span></a>    Keyword args: 
<a name="l16263"><span class="ln">16263 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l16264"><span class="ln">16264 </span></a> 
<a name="l16265"><span class="ln">16265 </span></a>    Example:: 
<a name="l16266"><span class="ln">16266 </span></a> 
<a name="l16267"><span class="ln">16267 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l16268"><span class="ln">16268 </span></a>        &gt;&gt;&gt; x 
<a name="l16269"><span class="ln">16269 </span></a>        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009], 
<a name="l16270"><span class="ln">16270 </span></a>                [-0.4664,  0.2647, -0.1228, -1.1068], 
<a name="l16271"><span class="ln">16271 </span></a>                [-1.1734, -0.6571,  0.7230, -0.6004]]) 
<a name="l16272"><span class="ln">16272 </span></a>        &gt;&gt;&gt; indices = torch.tensor([0, 2]) 
<a name="l16273"><span class="ln">16273 </span></a>        &gt;&gt;&gt; torch.index_select(x, 0, indices) 
<a name="l16274"><span class="ln">16274 </span></a>        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009], 
<a name="l16275"><span class="ln">16275 </span></a>                [-1.1734, -0.6571,  0.7230, -0.6004]]) 
<a name="l16276"><span class="ln">16276 </span></a>        &gt;&gt;&gt; torch.index_select(x, 1, indices) 
<a name="l16277"><span class="ln">16277 </span></a>        tensor([[ 0.1427, -0.5414], 
<a name="l16278"><span class="ln">16278 </span></a>                [-0.4664, -0.1228], 
<a name="l16279"><span class="ln">16279 </span></a>                [-1.1734,  0.7230]]) 
<a name="l16280"><span class="ln">16280 </span></a>    &quot;&quot;&quot;</span>
<a name="l16281"><span class="ln">16281 </span></a>
<a name="l16282"><span class="ln">16282 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16283"><span class="ln">16283 </span></a><span class="s2">def </span><span class="s1">index_select</span><span class="s3">(</span>
<a name="l16284"><span class="ln">16284 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16285"><span class="ln">16285 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16286"><span class="ln">16286 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16287"><span class="ln">16287 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16288"><span class="ln">16288 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16289"><span class="ln">16289 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16290"><span class="ln">16290 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16291"><span class="ln">16291 </span></a>    index_select(input, dim, index, *, out=None) -&gt; Tensor 
<a name="l16292"><span class="ln">16292 </span></a> 
<a name="l16293"><span class="ln">16293 </span></a>    Returns a new tensor which indexes the :attr:`input` tensor along dimension 
<a name="l16294"><span class="ln">16294 </span></a>    :attr:`dim` using the entries in :attr:`index` which is a `LongTensor`. 
<a name="l16295"><span class="ln">16295 </span></a> 
<a name="l16296"><span class="ln">16296 </span></a>    The returned tensor has the same number of dimensions as the original tensor 
<a name="l16297"><span class="ln">16297 </span></a>    (:attr:`input`).  The :attr:`dim`\ th dimension has the same size as the length 
<a name="l16298"><span class="ln">16298 </span></a>    of :attr:`index`; other dimensions have the same size as in the original tensor. 
<a name="l16299"><span class="ln">16299 </span></a> 
<a name="l16300"><span class="ln">16300 </span></a>    .. note:: The returned tensor does **not** use the same storage as the original 
<a name="l16301"><span class="ln">16301 </span></a>              tensor.  If :attr:`out` has a different shape than expected, we 
<a name="l16302"><span class="ln">16302 </span></a>              silently change it to the correct shape, reallocating the underlying 
<a name="l16303"><span class="ln">16303 </span></a>              storage if necessary. 
<a name="l16304"><span class="ln">16304 </span></a> 
<a name="l16305"><span class="ln">16305 </span></a>    Args: 
<a name="l16306"><span class="ln">16306 </span></a>        input (Tensor): the input tensor. 
<a name="l16307"><span class="ln">16307 </span></a>        dim (int): the dimension in which we index 
<a name="l16308"><span class="ln">16308 </span></a>        index (IntTensor or LongTensor): the 1-D tensor containing the indices to index 
<a name="l16309"><span class="ln">16309 </span></a> 
<a name="l16310"><span class="ln">16310 </span></a>    Keyword args: 
<a name="l16311"><span class="ln">16311 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l16312"><span class="ln">16312 </span></a> 
<a name="l16313"><span class="ln">16313 </span></a>    Example:: 
<a name="l16314"><span class="ln">16314 </span></a> 
<a name="l16315"><span class="ln">16315 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l16316"><span class="ln">16316 </span></a>        &gt;&gt;&gt; x 
<a name="l16317"><span class="ln">16317 </span></a>        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009], 
<a name="l16318"><span class="ln">16318 </span></a>                [-0.4664,  0.2647, -0.1228, -1.1068], 
<a name="l16319"><span class="ln">16319 </span></a>                [-1.1734, -0.6571,  0.7230, -0.6004]]) 
<a name="l16320"><span class="ln">16320 </span></a>        &gt;&gt;&gt; indices = torch.tensor([0, 2]) 
<a name="l16321"><span class="ln">16321 </span></a>        &gt;&gt;&gt; torch.index_select(x, 0, indices) 
<a name="l16322"><span class="ln">16322 </span></a>        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009], 
<a name="l16323"><span class="ln">16323 </span></a>                [-1.1734, -0.6571,  0.7230, -0.6004]]) 
<a name="l16324"><span class="ln">16324 </span></a>        &gt;&gt;&gt; torch.index_select(x, 1, indices) 
<a name="l16325"><span class="ln">16325 </span></a>        tensor([[ 0.1427, -0.5414], 
<a name="l16326"><span class="ln">16326 </span></a>                [-0.4664, -0.1228], 
<a name="l16327"><span class="ln">16327 </span></a>                [-1.1734,  0.7230]]) 
<a name="l16328"><span class="ln">16328 </span></a>    &quot;&quot;&quot;</span>
<a name="l16329"><span class="ln">16329 </span></a>
<a name="l16330"><span class="ln">16330 </span></a><span class="s2">def </span><span class="s1">indices_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16331"><span class="ln">16331 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16332"><span class="ln">16332 </span></a>    Performs the same operation as :func:`torch.indices`, but all output tensors 
<a name="l16333"><span class="ln">16333 </span></a>    are freshly created instead of aliasing the input. 
<a name="l16334"><span class="ln">16334 </span></a>    &quot;&quot;&quot;</span>
<a name="l16335"><span class="ln">16335 </span></a>
<a name="l16336"><span class="ln">16336 </span></a><span class="s2">def </span><span class="s1">init_num_threads</span><span class="s3">() </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l16337"><span class="ln">16337 </span></a><span class="s2">def </span><span class="s1">inner</span><span class="s3">(</span>
<a name="l16338"><span class="ln">16338 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16339"><span class="ln">16339 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16340"><span class="ln">16340 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16341"><span class="ln">16341 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16342"><span class="ln">16342 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16343"><span class="ln">16343 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16344"><span class="ln">16344 </span></a>    inner(input, other, *, out=None) -&gt; Tensor 
<a name="l16345"><span class="ln">16345 </span></a> 
<a name="l16346"><span class="ln">16346 </span></a>    Computes the dot product for 1D tensors. For higher dimensions, sums the product 
<a name="l16347"><span class="ln">16347 </span></a>    of elements from :attr:`input` and :attr:`other` along their last dimension. 
<a name="l16348"><span class="ln">16348 </span></a> 
<a name="l16349"><span class="ln">16349 </span></a>    .. note:: 
<a name="l16350"><span class="ln">16350 </span></a> 
<a name="l16351"><span class="ln">16351 </span></a>        If either :attr:`input` or :attr:`other` is a scalar, the result is equivalent 
<a name="l16352"><span class="ln">16352 </span></a>        to `torch.mul(input, other)`. 
<a name="l16353"><span class="ln">16353 </span></a> 
<a name="l16354"><span class="ln">16354 </span></a>        If both :attr:`input` and :attr:`other` are non-scalars, the size of their last 
<a name="l16355"><span class="ln">16355 </span></a>        dimension must match and the result is equivalent to `torch.tensordot(input, 
<a name="l16356"><span class="ln">16356 </span></a>        other, dims=([-1], [-1]))` 
<a name="l16357"><span class="ln">16357 </span></a> 
<a name="l16358"><span class="ln">16358 </span></a>    Args: 
<a name="l16359"><span class="ln">16359 </span></a>        input (Tensor): First input tensor 
<a name="l16360"><span class="ln">16360 </span></a>        other (Tensor): Second input tensor 
<a name="l16361"><span class="ln">16361 </span></a> 
<a name="l16362"><span class="ln">16362 </span></a>    Keyword args: 
<a name="l16363"><span class="ln">16363 </span></a>        out (Tensor, optional): Optional output tensor to write result into. The output 
<a name="l16364"><span class="ln">16364 </span></a>                                shape is `input.shape[:-1] + other.shape[:-1]`. 
<a name="l16365"><span class="ln">16365 </span></a> 
<a name="l16366"><span class="ln">16366 </span></a>    Example:: 
<a name="l16367"><span class="ln">16367 </span></a> 
<a name="l16368"><span class="ln">16368 </span></a>        # Dot product 
<a name="l16369"><span class="ln">16369 </span></a>        &gt;&gt;&gt; torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1])) 
<a name="l16370"><span class="ln">16370 </span></a>        tensor(7) 
<a name="l16371"><span class="ln">16371 </span></a> 
<a name="l16372"><span class="ln">16372 </span></a>        # Multidimensional input tensors 
<a name="l16373"><span class="ln">16373 </span></a>        &gt;&gt;&gt; a = torch.randn(2, 3) 
<a name="l16374"><span class="ln">16374 </span></a>        &gt;&gt;&gt; a 
<a name="l16375"><span class="ln">16375 </span></a>        tensor([[0.8173, 1.0874, 1.1784], 
<a name="l16376"><span class="ln">16376 </span></a>                [0.3279, 0.1234, 2.7894]]) 
<a name="l16377"><span class="ln">16377 </span></a>        &gt;&gt;&gt; b = torch.randn(2, 4, 3) 
<a name="l16378"><span class="ln">16378 </span></a>        &gt;&gt;&gt; b 
<a name="l16379"><span class="ln">16379 </span></a>        tensor([[[-0.4682, -0.7159,  0.1506], 
<a name="l16380"><span class="ln">16380 </span></a>                [ 0.4034, -0.3657,  1.0387], 
<a name="l16381"><span class="ln">16381 </span></a>                [ 0.9892, -0.6684,  0.1774], 
<a name="l16382"><span class="ln">16382 </span></a>                [ 0.9482,  1.3261,  0.3917]], 
<a name="l16383"><span class="ln">16383 </span></a> 
<a name="l16384"><span class="ln">16384 </span></a>                [[ 0.4537,  0.7493,  1.1724], 
<a name="l16385"><span class="ln">16385 </span></a>                [ 0.2291,  0.5749, -0.2267], 
<a name="l16386"><span class="ln">16386 </span></a>                [-0.7920,  0.3607, -0.3701], 
<a name="l16387"><span class="ln">16387 </span></a>                [ 1.3666, -0.5850, -1.7242]]]) 
<a name="l16388"><span class="ln">16388 </span></a>        &gt;&gt;&gt; torch.inner(a, b) 
<a name="l16389"><span class="ln">16389 </span></a>        tensor([[[-0.9837,  1.1560,  0.2907,  2.6785], 
<a name="l16390"><span class="ln">16390 </span></a>                [ 2.5671,  0.5452, -0.6912, -1.5509]], 
<a name="l16391"><span class="ln">16391 </span></a> 
<a name="l16392"><span class="ln">16392 </span></a>                [[ 0.1782,  2.9843,  0.7366,  1.5672], 
<a name="l16393"><span class="ln">16393 </span></a>                [ 3.5115, -0.4864, -1.2476, -4.4337]]]) 
<a name="l16394"><span class="ln">16394 </span></a> 
<a name="l16395"><span class="ln">16395 </span></a>        # Scalar input 
<a name="l16396"><span class="ln">16396 </span></a>        &gt;&gt;&gt; torch.inner(a, torch.tensor(2)) 
<a name="l16397"><span class="ln">16397 </span></a>        tensor([[1.6347, 2.1748, 2.3567], 
<a name="l16398"><span class="ln">16398 </span></a>                [0.6558, 0.2469, 5.5787]]) 
<a name="l16399"><span class="ln">16399 </span></a>    &quot;&quot;&quot;</span>
<a name="l16400"><span class="ln">16400 </span></a>
<a name="l16401"><span class="ln">16401 </span></a><span class="s2">def </span><span class="s1">instance_norm</span><span class="s3">(</span>
<a name="l16402"><span class="ln">16402 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16403"><span class="ln">16403 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16404"><span class="ln">16404 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16405"><span class="ln">16405 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16406"><span class="ln">16406 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l16407"><span class="ln">16407 </span></a>    <span class="s1">use_input_stats</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l16408"><span class="ln">16408 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l16409"><span class="ln">16409 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l16410"><span class="ln">16410 </span></a>    <span class="s1">cudnn_enabled</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l16411"><span class="ln">16411 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16412"><span class="ln">16412 </span></a><span class="s2">def </span><span class="s1">int_repr</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16413"><span class="ln">16413 </span></a><span class="s2">def </span><span class="s1">inverse</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16414"><span class="ln">16414 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16415"><span class="ln">16415 </span></a>    inverse(input, *, out=None) -&gt; Tensor 
<a name="l16416"><span class="ln">16416 </span></a> 
<a name="l16417"><span class="ln">16417 </span></a>    Alias for :func:`torch.linalg.inv` 
<a name="l16418"><span class="ln">16418 </span></a>    &quot;&quot;&quot;</span>
<a name="l16419"><span class="ln">16419 </span></a>
<a name="l16420"><span class="ln">16420 </span></a><span class="s2">def </span><span class="s1">is_complex</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l16421"><span class="ln">16421 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16422"><span class="ln">16422 </span></a>    is_complex(input) -&gt; (bool) 
<a name="l16423"><span class="ln">16423 </span></a> 
<a name="l16424"><span class="ln">16424 </span></a>    Returns True if the data type of :attr:`input` is a complex data type i.e., 
<a name="l16425"><span class="ln">16425 </span></a>    one of ``torch.complex64``, and ``torch.complex128``. 
<a name="l16426"><span class="ln">16426 </span></a> 
<a name="l16427"><span class="ln">16427 </span></a>    Args: 
<a name="l16428"><span class="ln">16428 </span></a>        input (Tensor): the input tensor. 
<a name="l16429"><span class="ln">16429 </span></a>    &quot;&quot;&quot;</span>
<a name="l16430"><span class="ln">16430 </span></a>
<a name="l16431"><span class="ln">16431 </span></a><span class="s2">def </span><span class="s1">is_conj</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l16432"><span class="ln">16432 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16433"><span class="ln">16433 </span></a>    is_conj(input) -&gt; (bool) 
<a name="l16434"><span class="ln">16434 </span></a> 
<a name="l16435"><span class="ln">16435 </span></a>    Returns True if the :attr:`input` is a conjugated tensor, i.e. its conjugate bit is set to `True`. 
<a name="l16436"><span class="ln">16436 </span></a> 
<a name="l16437"><span class="ln">16437 </span></a>    Args: 
<a name="l16438"><span class="ln">16438 </span></a>        input (Tensor): the input tensor. 
<a name="l16439"><span class="ln">16439 </span></a>    &quot;&quot;&quot;</span>
<a name="l16440"><span class="ln">16440 </span></a>
<a name="l16441"><span class="ln">16441 </span></a><span class="s2">def </span><span class="s1">is_distributed</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16442"><span class="ln">16442 </span></a><span class="s2">def </span><span class="s1">is_floating_point</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l16443"><span class="ln">16443 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16444"><span class="ln">16444 </span></a>    is_floating_point(input) -&gt; (bool) 
<a name="l16445"><span class="ln">16445 </span></a> 
<a name="l16446"><span class="ln">16446 </span></a>    Returns True if the data type of :attr:`input` is a floating point data type i.e., 
<a name="l16447"><span class="ln">16447 </span></a>    one of ``torch.float64``, ``torch.float32``, ``torch.float16``, and ``torch.bfloat16``. 
<a name="l16448"><span class="ln">16448 </span></a> 
<a name="l16449"><span class="ln">16449 </span></a>    Args: 
<a name="l16450"><span class="ln">16450 </span></a>        input (Tensor): the input tensor. 
<a name="l16451"><span class="ln">16451 </span></a>    &quot;&quot;&quot;</span>
<a name="l16452"><span class="ln">16452 </span></a>
<a name="l16453"><span class="ln">16453 </span></a><span class="s2">def </span><span class="s1">is_grad_enabled</span><span class="s3">() </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l16454"><span class="ln">16454 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16455"><span class="ln">16455 </span></a>    is_grad_enabled() -&gt; (bool) 
<a name="l16456"><span class="ln">16456 </span></a> 
<a name="l16457"><span class="ln">16457 </span></a>    Returns True if grad mode is currently enabled. 
<a name="l16458"><span class="ln">16458 </span></a>    &quot;&quot;&quot;</span>
<a name="l16459"><span class="ln">16459 </span></a>
<a name="l16460"><span class="ln">16460 </span></a><span class="s2">def </span><span class="s1">is_inference</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l16461"><span class="ln">16461 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16462"><span class="ln">16462 </span></a>    is_inference(input) -&gt; (bool) 
<a name="l16463"><span class="ln">16463 </span></a> 
<a name="l16464"><span class="ln">16464 </span></a>    Returns True if :attr:`input` is an inference tensor. 
<a name="l16465"><span class="ln">16465 </span></a> 
<a name="l16466"><span class="ln">16466 </span></a>    A non-view tensor is an inference tensor if and only if it was 
<a name="l16467"><span class="ln">16467 </span></a>    allocated during inference mode. A view tensor is an inference 
<a name="l16468"><span class="ln">16468 </span></a>    tensor if and only if the tensor it is a view of is an inference tensor. 
<a name="l16469"><span class="ln">16469 </span></a> 
<a name="l16470"><span class="ln">16470 </span></a>    For details on inference mode please see 
<a name="l16471"><span class="ln">16471 </span></a>    `Inference Mode &lt;https://pytorch.org/cppdocs/notes/inference_mode.html&gt;`_. 
<a name="l16472"><span class="ln">16472 </span></a> 
<a name="l16473"><span class="ln">16473 </span></a>    Args: 
<a name="l16474"><span class="ln">16474 </span></a>        input (Tensor): the input tensor. 
<a name="l16475"><span class="ln">16475 </span></a>    &quot;&quot;&quot;</span>
<a name="l16476"><span class="ln">16476 </span></a>
<a name="l16477"><span class="ln">16477 </span></a><span class="s2">def </span><span class="s1">is_inference_mode_enabled</span><span class="s3">() </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l16478"><span class="ln">16478 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16479"><span class="ln">16479 </span></a>    is_inference_mode_enabled() -&gt; (bool) 
<a name="l16480"><span class="ln">16480 </span></a> 
<a name="l16481"><span class="ln">16481 </span></a>    Returns True if inference mode is currently enabled. 
<a name="l16482"><span class="ln">16482 </span></a>    &quot;&quot;&quot;</span>
<a name="l16483"><span class="ln">16483 </span></a>
<a name="l16484"><span class="ln">16484 </span></a><span class="s2">def </span><span class="s1">is_neg</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16485"><span class="ln">16485 </span></a><span class="s2">def </span><span class="s1">is_nonzero</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l16486"><span class="ln">16486 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16487"><span class="ln">16487 </span></a>    is_nonzero(input) -&gt; (bool) 
<a name="l16488"><span class="ln">16488 </span></a> 
<a name="l16489"><span class="ln">16489 </span></a>    Returns True if the :attr:`input` is a single element tensor which is not equal to zero 
<a name="l16490"><span class="ln">16490 </span></a>    after type conversions. 
<a name="l16491"><span class="ln">16491 </span></a>    i.e. not equal to ``torch.tensor([0.])`` or ``torch.tensor([0])`` or 
<a name="l16492"><span class="ln">16492 </span></a>    ``torch.tensor([False])``. 
<a name="l16493"><span class="ln">16493 </span></a>    Throws a ``RuntimeError`` if ``torch.numel() != 1`` (even in case 
<a name="l16494"><span class="ln">16494 </span></a>    of sparse tensors). 
<a name="l16495"><span class="ln">16495 </span></a> 
<a name="l16496"><span class="ln">16496 </span></a>    Args: 
<a name="l16497"><span class="ln">16497 </span></a>        input (Tensor): the input tensor. 
<a name="l16498"><span class="ln">16498 </span></a> 
<a name="l16499"><span class="ln">16499 </span></a>    Examples:: 
<a name="l16500"><span class="ln">16500 </span></a> 
<a name="l16501"><span class="ln">16501 </span></a>        &gt;&gt;&gt; torch.is_nonzero(torch.tensor([0.])) 
<a name="l16502"><span class="ln">16502 </span></a>        False 
<a name="l16503"><span class="ln">16503 </span></a>        &gt;&gt;&gt; torch.is_nonzero(torch.tensor([1.5])) 
<a name="l16504"><span class="ln">16504 </span></a>        True 
<a name="l16505"><span class="ln">16505 </span></a>        &gt;&gt;&gt; torch.is_nonzero(torch.tensor([False])) 
<a name="l16506"><span class="ln">16506 </span></a>        False 
<a name="l16507"><span class="ln">16507 </span></a>        &gt;&gt;&gt; torch.is_nonzero(torch.tensor([3])) 
<a name="l16508"><span class="ln">16508 </span></a>        True 
<a name="l16509"><span class="ln">16509 </span></a>        &gt;&gt;&gt; torch.is_nonzero(torch.tensor([1, 3, 5])) 
<a name="l16510"><span class="ln">16510 </span></a>        Traceback (most recent call last): 
<a name="l16511"><span class="ln">16511 </span></a>        ... 
<a name="l16512"><span class="ln">16512 </span></a>        RuntimeError: bool value of Tensor with more than one value is ambiguous 
<a name="l16513"><span class="ln">16513 </span></a>        &gt;&gt;&gt; torch.is_nonzero(torch.tensor([])) 
<a name="l16514"><span class="ln">16514 </span></a>        Traceback (most recent call last): 
<a name="l16515"><span class="ln">16515 </span></a>        ... 
<a name="l16516"><span class="ln">16516 </span></a>        RuntimeError: bool value of Tensor with no values is ambiguous 
<a name="l16517"><span class="ln">16517 </span></a>    &quot;&quot;&quot;</span>
<a name="l16518"><span class="ln">16518 </span></a>
<a name="l16519"><span class="ln">16519 </span></a><span class="s2">def </span><span class="s1">is_same_size</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16520"><span class="ln">16520 </span></a><span class="s2">def </span><span class="s1">is_signed</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16521"><span class="ln">16521 </span></a><span class="s2">def </span><span class="s1">is_vulkan_available</span><span class="s3">() </span><span class="s1">-&gt; _bool</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16522"><span class="ln">16522 </span></a><span class="s2">def </span><span class="s1">isclose</span><span class="s3">(</span>
<a name="l16523"><span class="ln">16523 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16524"><span class="ln">16524 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16525"><span class="ln">16525 </span></a>    <span class="s1">rtol</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-05</span><span class="s3">,</span>
<a name="l16526"><span class="ln">16526 </span></a>    <span class="s1">atol</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-08</span><span class="s3">,</span>
<a name="l16527"><span class="ln">16527 </span></a>    <span class="s1">equal_nan</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16528"><span class="ln">16528 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16529"><span class="ln">16529 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16530"><span class="ln">16530 </span></a>    isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) -&gt; Tensor 
<a name="l16531"><span class="ln">16531 </span></a> 
<a name="l16532"><span class="ln">16532 </span></a>    Returns a new tensor with boolean elements representing if each element of 
<a name="l16533"><span class="ln">16533 </span></a>    :attr:`input` is &quot;close&quot; to the corresponding element of :attr:`other`. 
<a name="l16534"><span class="ln">16534 </span></a>    Closeness is defined as: 
<a name="l16535"><span class="ln">16535 </span></a> 
<a name="l16536"><span class="ln">16536 </span></a>    .. math:: 
<a name="l16537"><span class="ln">16537 </span></a>        \lvert \text{input}_i - \text{other}_i \rvert \leq \texttt{rtol} \times \lvert \text{other}_i \rvert + \texttt{atol} 
<a name="l16538"><span class="ln">16538 </span></a> 
<a name="l16539"><span class="ln">16539 </span></a> 
<a name="l16540"><span class="ln">16540 </span></a>    where :attr:`input` and :attr:`other` are finite. Where :attr:`input` 
<a name="l16541"><span class="ln">16541 </span></a>    and/or :attr:`other` are nonfinite they are close if and only if 
<a name="l16542"><span class="ln">16542 </span></a>    they are equal, with NaNs being considered equal to each other when 
<a name="l16543"><span class="ln">16543 </span></a>    :attr:`equal_nan` is True. 
<a name="l16544"><span class="ln">16544 </span></a> 
<a name="l16545"><span class="ln">16545 </span></a>    Args: 
<a name="l16546"><span class="ln">16546 </span></a>        input (Tensor): first tensor to compare 
<a name="l16547"><span class="ln">16547 </span></a>        other (Tensor): second tensor to compare 
<a name="l16548"><span class="ln">16548 </span></a>        rtol (float, optional): relative tolerance. Default: 1e-05 
<a name="l16549"><span class="ln">16549 </span></a>        atol (float, optional): absolute tolerance. Default: 1e-08 
<a name="l16550"><span class="ln">16550 </span></a>        equal_nan (bool, optional): if ``True``, then two ``NaN`` s will be considered equal. Default: ``False`` 
<a name="l16551"><span class="ln">16551 </span></a> 
<a name="l16552"><span class="ln">16552 </span></a>    Examples:: 
<a name="l16553"><span class="ln">16553 </span></a> 
<a name="l16554"><span class="ln">16554 </span></a>        &gt;&gt;&gt; torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4))) 
<a name="l16555"><span class="ln">16555 </span></a>        tensor([ True, False, False]) 
<a name="l16556"><span class="ln">16556 </span></a>        &gt;&gt;&gt; torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5) 
<a name="l16557"><span class="ln">16557 </span></a>        tensor([True, True]) 
<a name="l16558"><span class="ln">16558 </span></a>    &quot;&quot;&quot;</span>
<a name="l16559"><span class="ln">16559 </span></a>
<a name="l16560"><span class="ln">16560 </span></a><span class="s2">def </span><span class="s1">isfinite</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16561"><span class="ln">16561 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16562"><span class="ln">16562 </span></a>    isfinite(input) -&gt; Tensor 
<a name="l16563"><span class="ln">16563 </span></a> 
<a name="l16564"><span class="ln">16564 </span></a>    Returns a new tensor with boolean elements representing if each element is `finite` or not. 
<a name="l16565"><span class="ln">16565 </span></a> 
<a name="l16566"><span class="ln">16566 </span></a>    Real values are finite when they are not NaN, negative infinity, or infinity. 
<a name="l16567"><span class="ln">16567 </span></a>    Complex values are finite when both their real and imaginary parts are finite. 
<a name="l16568"><span class="ln">16568 </span></a> 
<a name="l16569"><span class="ln">16569 </span></a>    Args: 
<a name="l16570"><span class="ln">16570 </span></a>        input (Tensor): the input tensor. 
<a name="l16571"><span class="ln">16571 </span></a> 
<a name="l16572"><span class="ln">16572 </span></a>    Returns: 
<a name="l16573"><span class="ln">16573 </span></a>        A boolean tensor that is True where :attr:`input` is finite and False elsewhere 
<a name="l16574"><span class="ln">16574 </span></a> 
<a name="l16575"><span class="ln">16575 </span></a>    Example:: 
<a name="l16576"><span class="ln">16576 </span></a> 
<a name="l16577"><span class="ln">16577 </span></a>        &gt;&gt;&gt; torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) 
<a name="l16578"><span class="ln">16578 </span></a>        tensor([True,  False,  True,  False,  False]) 
<a name="l16579"><span class="ln">16579 </span></a>    &quot;&quot;&quot;</span>
<a name="l16580"><span class="ln">16580 </span></a>
<a name="l16581"><span class="ln">16581 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16582"><span class="ln">16582 </span></a><span class="s2">def </span><span class="s1">isin</span><span class="s3">(</span>
<a name="l16583"><span class="ln">16583 </span></a>    <span class="s1">elements</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16584"><span class="ln">16584 </span></a>    <span class="s1">test_elements</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16585"><span class="ln">16585 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16586"><span class="ln">16586 </span></a>    <span class="s1">assume_unique</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16587"><span class="ln">16587 </span></a>    <span class="s1">invert</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16588"><span class="ln">16588 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16589"><span class="ln">16589 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16590"><span class="ln">16590 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16591"><span class="ln">16591 </span></a>    isin(elements, test_elements, *, assume_unique=False, invert=False) -&gt; Tensor 
<a name="l16592"><span class="ln">16592 </span></a> 
<a name="l16593"><span class="ln">16593 </span></a>    Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns 
<a name="l16594"><span class="ln">16594 </span></a>    a boolean tensor of the same shape as :attr:`elements` that is True for elements 
<a name="l16595"><span class="ln">16595 </span></a>    in :attr:`test_elements` and False otherwise. 
<a name="l16596"><span class="ln">16596 </span></a> 
<a name="l16597"><span class="ln">16597 </span></a>    .. note:: 
<a name="l16598"><span class="ln">16598 </span></a>        One of :attr:`elements` or :attr:`test_elements` can be a scalar, but not both. 
<a name="l16599"><span class="ln">16599 </span></a> 
<a name="l16600"><span class="ln">16600 </span></a>    Args: 
<a name="l16601"><span class="ln">16601 </span></a>        elements (Tensor or Scalar): Input elements 
<a name="l16602"><span class="ln">16602 </span></a>        test_elements (Tensor or Scalar): Values against which to test for each input element 
<a name="l16603"><span class="ln">16603 </span></a>        assume_unique (bool, optional): If True, assumes both :attr:`elements` and 
<a name="l16604"><span class="ln">16604 </span></a>            :attr:`test_elements` contain unique elements, which can speed up the 
<a name="l16605"><span class="ln">16605 </span></a>            calculation. Default: False 
<a name="l16606"><span class="ln">16606 </span></a>        invert (bool, optional): If True, inverts the boolean return tensor, resulting in True 
<a name="l16607"><span class="ln">16607 </span></a>            values for elements *not* in :attr:`test_elements`. Default: False 
<a name="l16608"><span class="ln">16608 </span></a> 
<a name="l16609"><span class="ln">16609 </span></a>    Returns: 
<a name="l16610"><span class="ln">16610 </span></a>        A boolean tensor of the same shape as :attr:`elements` that is True for elements in 
<a name="l16611"><span class="ln">16611 </span></a>        :attr:`test_elements` and False otherwise 
<a name="l16612"><span class="ln">16612 </span></a> 
<a name="l16613"><span class="ln">16613 </span></a>    Example: 
<a name="l16614"><span class="ln">16614 </span></a>        &gt;&gt;&gt; torch.isin(torch.tensor([[1, 2], [3, 4]]), torch.tensor([2, 3])) 
<a name="l16615"><span class="ln">16615 </span></a>        tensor([[False,  True], 
<a name="l16616"><span class="ln">16616 </span></a>                [ True, False]]) 
<a name="l16617"><span class="ln">16617 </span></a>    &quot;&quot;&quot;</span>
<a name="l16618"><span class="ln">16618 </span></a>
<a name="l16619"><span class="ln">16619 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16620"><span class="ln">16620 </span></a><span class="s2">def </span><span class="s1">isin</span><span class="s3">(</span>
<a name="l16621"><span class="ln">16621 </span></a>    <span class="s1">element</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l16622"><span class="ln">16622 </span></a>    <span class="s1">test_elements</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16623"><span class="ln">16623 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16624"><span class="ln">16624 </span></a>    <span class="s1">assume_unique</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16625"><span class="ln">16625 </span></a>    <span class="s1">invert</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16626"><span class="ln">16626 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16627"><span class="ln">16627 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16628"><span class="ln">16628 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16629"><span class="ln">16629 </span></a>    isin(elements, test_elements, *, assume_unique=False, invert=False) -&gt; Tensor 
<a name="l16630"><span class="ln">16630 </span></a> 
<a name="l16631"><span class="ln">16631 </span></a>    Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns 
<a name="l16632"><span class="ln">16632 </span></a>    a boolean tensor of the same shape as :attr:`elements` that is True for elements 
<a name="l16633"><span class="ln">16633 </span></a>    in :attr:`test_elements` and False otherwise. 
<a name="l16634"><span class="ln">16634 </span></a> 
<a name="l16635"><span class="ln">16635 </span></a>    .. note:: 
<a name="l16636"><span class="ln">16636 </span></a>        One of :attr:`elements` or :attr:`test_elements` can be a scalar, but not both. 
<a name="l16637"><span class="ln">16637 </span></a> 
<a name="l16638"><span class="ln">16638 </span></a>    Args: 
<a name="l16639"><span class="ln">16639 </span></a>        elements (Tensor or Scalar): Input elements 
<a name="l16640"><span class="ln">16640 </span></a>        test_elements (Tensor or Scalar): Values against which to test for each input element 
<a name="l16641"><span class="ln">16641 </span></a>        assume_unique (bool, optional): If True, assumes both :attr:`elements` and 
<a name="l16642"><span class="ln">16642 </span></a>            :attr:`test_elements` contain unique elements, which can speed up the 
<a name="l16643"><span class="ln">16643 </span></a>            calculation. Default: False 
<a name="l16644"><span class="ln">16644 </span></a>        invert (bool, optional): If True, inverts the boolean return tensor, resulting in True 
<a name="l16645"><span class="ln">16645 </span></a>            values for elements *not* in :attr:`test_elements`. Default: False 
<a name="l16646"><span class="ln">16646 </span></a> 
<a name="l16647"><span class="ln">16647 </span></a>    Returns: 
<a name="l16648"><span class="ln">16648 </span></a>        A boolean tensor of the same shape as :attr:`elements` that is True for elements in 
<a name="l16649"><span class="ln">16649 </span></a>        :attr:`test_elements` and False otherwise 
<a name="l16650"><span class="ln">16650 </span></a> 
<a name="l16651"><span class="ln">16651 </span></a>    Example: 
<a name="l16652"><span class="ln">16652 </span></a>        &gt;&gt;&gt; torch.isin(torch.tensor([[1, 2], [3, 4]]), torch.tensor([2, 3])) 
<a name="l16653"><span class="ln">16653 </span></a>        tensor([[False,  True], 
<a name="l16654"><span class="ln">16654 </span></a>                [ True, False]]) 
<a name="l16655"><span class="ln">16655 </span></a>    &quot;&quot;&quot;</span>
<a name="l16656"><span class="ln">16656 </span></a>
<a name="l16657"><span class="ln">16657 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16658"><span class="ln">16658 </span></a><span class="s2">def </span><span class="s1">isin</span><span class="s3">(</span>
<a name="l16659"><span class="ln">16659 </span></a>    <span class="s1">elements</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16660"><span class="ln">16660 </span></a>    <span class="s1">test_element</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l16661"><span class="ln">16661 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16662"><span class="ln">16662 </span></a>    <span class="s1">assume_unique</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16663"><span class="ln">16663 </span></a>    <span class="s1">invert</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16664"><span class="ln">16664 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16665"><span class="ln">16665 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16666"><span class="ln">16666 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16667"><span class="ln">16667 </span></a>    isin(elements, test_elements, *, assume_unique=False, invert=False) -&gt; Tensor 
<a name="l16668"><span class="ln">16668 </span></a> 
<a name="l16669"><span class="ln">16669 </span></a>    Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns 
<a name="l16670"><span class="ln">16670 </span></a>    a boolean tensor of the same shape as :attr:`elements` that is True for elements 
<a name="l16671"><span class="ln">16671 </span></a>    in :attr:`test_elements` and False otherwise. 
<a name="l16672"><span class="ln">16672 </span></a> 
<a name="l16673"><span class="ln">16673 </span></a>    .. note:: 
<a name="l16674"><span class="ln">16674 </span></a>        One of :attr:`elements` or :attr:`test_elements` can be a scalar, but not both. 
<a name="l16675"><span class="ln">16675 </span></a> 
<a name="l16676"><span class="ln">16676 </span></a>    Args: 
<a name="l16677"><span class="ln">16677 </span></a>        elements (Tensor or Scalar): Input elements 
<a name="l16678"><span class="ln">16678 </span></a>        test_elements (Tensor or Scalar): Values against which to test for each input element 
<a name="l16679"><span class="ln">16679 </span></a>        assume_unique (bool, optional): If True, assumes both :attr:`elements` and 
<a name="l16680"><span class="ln">16680 </span></a>            :attr:`test_elements` contain unique elements, which can speed up the 
<a name="l16681"><span class="ln">16681 </span></a>            calculation. Default: False 
<a name="l16682"><span class="ln">16682 </span></a>        invert (bool, optional): If True, inverts the boolean return tensor, resulting in True 
<a name="l16683"><span class="ln">16683 </span></a>            values for elements *not* in :attr:`test_elements`. Default: False 
<a name="l16684"><span class="ln">16684 </span></a> 
<a name="l16685"><span class="ln">16685 </span></a>    Returns: 
<a name="l16686"><span class="ln">16686 </span></a>        A boolean tensor of the same shape as :attr:`elements` that is True for elements in 
<a name="l16687"><span class="ln">16687 </span></a>        :attr:`test_elements` and False otherwise 
<a name="l16688"><span class="ln">16688 </span></a> 
<a name="l16689"><span class="ln">16689 </span></a>    Example: 
<a name="l16690"><span class="ln">16690 </span></a>        &gt;&gt;&gt; torch.isin(torch.tensor([[1, 2], [3, 4]]), torch.tensor([2, 3])) 
<a name="l16691"><span class="ln">16691 </span></a>        tensor([[False,  True], 
<a name="l16692"><span class="ln">16692 </span></a>                [ True, False]]) 
<a name="l16693"><span class="ln">16693 </span></a>    &quot;&quot;&quot;</span>
<a name="l16694"><span class="ln">16694 </span></a>
<a name="l16695"><span class="ln">16695 </span></a><span class="s2">def </span><span class="s1">isinf</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16696"><span class="ln">16696 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16697"><span class="ln">16697 </span></a>    isinf(input) -&gt; Tensor 
<a name="l16698"><span class="ln">16698 </span></a> 
<a name="l16699"><span class="ln">16699 </span></a>    Tests if each element of :attr:`input` is infinite 
<a name="l16700"><span class="ln">16700 </span></a>    (positive or negative infinity) or not. 
<a name="l16701"><span class="ln">16701 </span></a> 
<a name="l16702"><span class="ln">16702 </span></a>    .. note:: 
<a name="l16703"><span class="ln">16703 </span></a>        Complex values are infinite when their real or imaginary part is 
<a name="l16704"><span class="ln">16704 </span></a>        infinite. 
<a name="l16705"><span class="ln">16705 </span></a> 
<a name="l16706"><span class="ln">16706 </span></a>    Args: 
<a name="l16707"><span class="ln">16707 </span></a>        input (Tensor): the input tensor. 
<a name="l16708"><span class="ln">16708 </span></a> 
<a name="l16709"><span class="ln">16709 </span></a>    Returns: 
<a name="l16710"><span class="ln">16710 </span></a>        A boolean tensor that is True where :attr:`input` is infinite and False elsewhere 
<a name="l16711"><span class="ln">16711 </span></a> 
<a name="l16712"><span class="ln">16712 </span></a>    Example:: 
<a name="l16713"><span class="ln">16713 </span></a> 
<a name="l16714"><span class="ln">16714 </span></a>        &gt;&gt;&gt; torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) 
<a name="l16715"><span class="ln">16715 </span></a>        tensor([False,  True,  False,  True,  False]) 
<a name="l16716"><span class="ln">16716 </span></a>    &quot;&quot;&quot;</span>
<a name="l16717"><span class="ln">16717 </span></a>
<a name="l16718"><span class="ln">16718 </span></a><span class="s2">def </span><span class="s1">isnan</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16719"><span class="ln">16719 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16720"><span class="ln">16720 </span></a>    isnan(input) -&gt; Tensor 
<a name="l16721"><span class="ln">16721 </span></a> 
<a name="l16722"><span class="ln">16722 </span></a>    Returns a new tensor with boolean elements representing if each element of :attr:`input` 
<a name="l16723"><span class="ln">16723 </span></a>    is NaN or not. Complex values are considered NaN when either their real 
<a name="l16724"><span class="ln">16724 </span></a>    and/or imaginary part is NaN. 
<a name="l16725"><span class="ln">16725 </span></a> 
<a name="l16726"><span class="ln">16726 </span></a>    Arguments: 
<a name="l16727"><span class="ln">16727 </span></a>        input (Tensor): the input tensor. 
<a name="l16728"><span class="ln">16728 </span></a> 
<a name="l16729"><span class="ln">16729 </span></a>    Returns: 
<a name="l16730"><span class="ln">16730 </span></a>        A boolean tensor that is True where :attr:`input` is NaN and False elsewhere 
<a name="l16731"><span class="ln">16731 </span></a> 
<a name="l16732"><span class="ln">16732 </span></a>    Example:: 
<a name="l16733"><span class="ln">16733 </span></a> 
<a name="l16734"><span class="ln">16734 </span></a>        &gt;&gt;&gt; torch.isnan(torch.tensor([1, float('nan'), 2])) 
<a name="l16735"><span class="ln">16735 </span></a>        tensor([False, True, False]) 
<a name="l16736"><span class="ln">16736 </span></a>    &quot;&quot;&quot;</span>
<a name="l16737"><span class="ln">16737 </span></a>
<a name="l16738"><span class="ln">16738 </span></a><span class="s2">def </span><span class="s1">isneginf</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16739"><span class="ln">16739 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16740"><span class="ln">16740 </span></a>    isneginf(input, *, out=None) -&gt; Tensor 
<a name="l16741"><span class="ln">16741 </span></a>    Tests if each element of :attr:`input` is negative infinity or not. 
<a name="l16742"><span class="ln">16742 </span></a> 
<a name="l16743"><span class="ln">16743 </span></a>    Args: 
<a name="l16744"><span class="ln">16744 </span></a>      input (Tensor): the input tensor. 
<a name="l16745"><span class="ln">16745 </span></a> 
<a name="l16746"><span class="ln">16746 </span></a>    Keyword args: 
<a name="l16747"><span class="ln">16747 </span></a>      out (Tensor, optional): the output tensor. 
<a name="l16748"><span class="ln">16748 </span></a> 
<a name="l16749"><span class="ln">16749 </span></a>    Example:: 
<a name="l16750"><span class="ln">16750 </span></a> 
<a name="l16751"><span class="ln">16751 </span></a>        &gt;&gt;&gt; a = torch.tensor([-float('inf'), float('inf'), 1.2]) 
<a name="l16752"><span class="ln">16752 </span></a>        &gt;&gt;&gt; torch.isneginf(a) 
<a name="l16753"><span class="ln">16753 </span></a>        tensor([ True, False, False]) 
<a name="l16754"><span class="ln">16754 </span></a>    &quot;&quot;&quot;</span>
<a name="l16755"><span class="ln">16755 </span></a>
<a name="l16756"><span class="ln">16756 </span></a><span class="s2">def </span><span class="s1">isposinf</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16757"><span class="ln">16757 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16758"><span class="ln">16758 </span></a>    isposinf(input, *, out=None) -&gt; Tensor 
<a name="l16759"><span class="ln">16759 </span></a>    Tests if each element of :attr:`input` is positive infinity or not. 
<a name="l16760"><span class="ln">16760 </span></a> 
<a name="l16761"><span class="ln">16761 </span></a>    Args: 
<a name="l16762"><span class="ln">16762 </span></a>      input (Tensor): the input tensor. 
<a name="l16763"><span class="ln">16763 </span></a> 
<a name="l16764"><span class="ln">16764 </span></a>    Keyword args: 
<a name="l16765"><span class="ln">16765 </span></a>      out (Tensor, optional): the output tensor. 
<a name="l16766"><span class="ln">16766 </span></a> 
<a name="l16767"><span class="ln">16767 </span></a>    Example:: 
<a name="l16768"><span class="ln">16768 </span></a> 
<a name="l16769"><span class="ln">16769 </span></a>        &gt;&gt;&gt; a = torch.tensor([-float('inf'), float('inf'), 1.2]) 
<a name="l16770"><span class="ln">16770 </span></a>        &gt;&gt;&gt; torch.isposinf(a) 
<a name="l16771"><span class="ln">16771 </span></a>        tensor([False,  True, False]) 
<a name="l16772"><span class="ln">16772 </span></a>    &quot;&quot;&quot;</span>
<a name="l16773"><span class="ln">16773 </span></a>
<a name="l16774"><span class="ln">16774 </span></a><span class="s2">def </span><span class="s1">isreal</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16775"><span class="ln">16775 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16776"><span class="ln">16776 </span></a>    isreal(input) -&gt; Tensor 
<a name="l16777"><span class="ln">16777 </span></a> 
<a name="l16778"><span class="ln">16778 </span></a>    Returns a new tensor with boolean elements representing if each element of :attr:`input` is real-valued or not. 
<a name="l16779"><span class="ln">16779 </span></a>    All real-valued types are considered real. Complex values are considered real when their imaginary part is 0. 
<a name="l16780"><span class="ln">16780 </span></a> 
<a name="l16781"><span class="ln">16781 </span></a>    Arguments: 
<a name="l16782"><span class="ln">16782 </span></a>        input (Tensor): the input tensor. 
<a name="l16783"><span class="ln">16783 </span></a> 
<a name="l16784"><span class="ln">16784 </span></a>    Returns: 
<a name="l16785"><span class="ln">16785 </span></a>        A boolean tensor that is True where :attr:`input` is real and False elsewhere 
<a name="l16786"><span class="ln">16786 </span></a> 
<a name="l16787"><span class="ln">16787 </span></a>    Example:: 
<a name="l16788"><span class="ln">16788 </span></a> 
<a name="l16789"><span class="ln">16789 </span></a>        &gt;&gt;&gt; torch.isreal(torch.tensor([1, 1+1j, 2+0j])) 
<a name="l16790"><span class="ln">16790 </span></a>        tensor([True, False, True]) 
<a name="l16791"><span class="ln">16791 </span></a>    &quot;&quot;&quot;</span>
<a name="l16792"><span class="ln">16792 </span></a>
<a name="l16793"><span class="ln">16793 </span></a><span class="s2">def </span><span class="s1">istft</span><span class="s3">(</span>
<a name="l16794"><span class="ln">16794 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16795"><span class="ln">16795 </span></a>    <span class="s1">n_fft</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16796"><span class="ln">16796 </span></a>    <span class="s1">hop_length</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16797"><span class="ln">16797 </span></a>    <span class="s1">win_length</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16798"><span class="ln">16798 </span></a>    <span class="s1">window</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16799"><span class="ln">16799 </span></a>    <span class="s1">center</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l16800"><span class="ln">16800 </span></a>    <span class="s1">normalized</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16801"><span class="ln">16801 </span></a>    <span class="s1">onesided</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16802"><span class="ln">16802 </span></a>    <span class="s1">length</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16803"><span class="ln">16803 </span></a>    <span class="s1">return_complex</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16804"><span class="ln">16804 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16805"><span class="ln">16805 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16806"><span class="ln">16806 </span></a><span class="s2">def </span><span class="s1">kaiser_window</span><span class="s3">(</span>
<a name="l16807"><span class="ln">16807 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16808"><span class="ln">16808 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16809"><span class="ln">16809 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16810"><span class="ln">16810 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16811"><span class="ln">16811 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16812"><span class="ln">16812 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l16813"><span class="ln">16813 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l16814"><span class="ln">16814 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16815"><span class="ln">16815 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16816"><span class="ln">16816 </span></a>    kaiser_window(window_length, periodic=True, beta=12.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l16817"><span class="ln">16817 </span></a> 
<a name="l16818"><span class="ln">16818 </span></a>    Computes the Kaiser window with window length :attr:`window_length` and shape parameter :attr:`beta`. 
<a name="l16819"><span class="ln">16819 </span></a> 
<a name="l16820"><span class="ln">16820 </span></a>    Let I_0 be the zeroth order modified Bessel function of the first kind (see :func:`torch.i0`) and 
<a name="l16821"><span class="ln">16821 </span></a>    ``N = L - 1`` if :attr:`periodic` is False and ``L`` if :attr:`periodic` is True, 
<a name="l16822"><span class="ln">16822 </span></a>    where ``L`` is the :attr:`window_length`. This function computes: 
<a name="l16823"><span class="ln">16823 </span></a> 
<a name="l16824"><span class="ln">16824 </span></a>    .. math:: 
<a name="l16825"><span class="ln">16825 </span></a>        out_i = I_0 \left( \beta \sqrt{1 - \left( {\frac{i - N/2}{N/2}} \right) ^2 } \right) / I_0( \beta ) 
<a name="l16826"><span class="ln">16826 </span></a> 
<a name="l16827"><span class="ln">16827 </span></a>    Calling ``torch.kaiser_window(L, B, periodic=True)`` is equivalent to calling 
<a name="l16828"><span class="ln">16828 </span></a>    ``torch.kaiser_window(L + 1, B, periodic=False)[:-1])``. 
<a name="l16829"><span class="ln">16829 </span></a>    The :attr:`periodic` argument is intended as a helpful shorthand 
<a name="l16830"><span class="ln">16830 </span></a>    to produce a periodic window as input to functions like :func:`torch.stft`. 
<a name="l16831"><span class="ln">16831 </span></a> 
<a name="l16832"><span class="ln">16832 </span></a>    .. note:: 
<a name="l16833"><span class="ln">16833 </span></a>        If :attr:`window_length` is one, then the returned window is a single element tensor containing a one. 
<a name="l16834"><span class="ln">16834 </span></a> 
<a name="l16835"><span class="ln">16835 </span></a> 
<a name="l16836"><span class="ln">16836 </span></a>    Args: 
<a name="l16837"><span class="ln">16837 </span></a>        window_length (int): length of the window. 
<a name="l16838"><span class="ln">16838 </span></a>        periodic (bool, optional): If True, returns a periodic window suitable for use in spectral analysis. 
<a name="l16839"><span class="ln">16839 </span></a>            If False, returns a symmetric window suitable for use in filter design. 
<a name="l16840"><span class="ln">16840 </span></a>        beta (float, optional): shape parameter for the window. 
<a name="l16841"><span class="ln">16841 </span></a> 
<a name="l16842"><span class="ln">16842 </span></a>    Keyword args: 
<a name="l16843"><span class="ln">16843 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l16844"><span class="ln">16844 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l16845"><span class="ln">16845 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l16846"><span class="ln">16846 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l16847"><span class="ln">16847 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l16848"><span class="ln">16848 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l16849"><span class="ln">16849 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l16850"><span class="ln">16850 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l16851"><span class="ln">16851 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l16852"><span class="ln">16852 </span></a>            returned tensor. Default: ``False``. 
<a name="l16853"><span class="ln">16853 </span></a>    &quot;&quot;&quot;</span>
<a name="l16854"><span class="ln">16854 </span></a>
<a name="l16855"><span class="ln">16855 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16856"><span class="ln">16856 </span></a><span class="s2">def </span><span class="s1">kaiser_window</span><span class="s3">(</span>
<a name="l16857"><span class="ln">16857 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16858"><span class="ln">16858 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l16859"><span class="ln">16859 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16860"><span class="ln">16860 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16861"><span class="ln">16861 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16862"><span class="ln">16862 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16863"><span class="ln">16863 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l16864"><span class="ln">16864 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l16865"><span class="ln">16865 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16866"><span class="ln">16866 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16867"><span class="ln">16867 </span></a>    kaiser_window(window_length, periodic=True, beta=12.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l16868"><span class="ln">16868 </span></a> 
<a name="l16869"><span class="ln">16869 </span></a>    Computes the Kaiser window with window length :attr:`window_length` and shape parameter :attr:`beta`. 
<a name="l16870"><span class="ln">16870 </span></a> 
<a name="l16871"><span class="ln">16871 </span></a>    Let I_0 be the zeroth order modified Bessel function of the first kind (see :func:`torch.i0`) and 
<a name="l16872"><span class="ln">16872 </span></a>    ``N = L - 1`` if :attr:`periodic` is False and ``L`` if :attr:`periodic` is True, 
<a name="l16873"><span class="ln">16873 </span></a>    where ``L`` is the :attr:`window_length`. This function computes: 
<a name="l16874"><span class="ln">16874 </span></a> 
<a name="l16875"><span class="ln">16875 </span></a>    .. math:: 
<a name="l16876"><span class="ln">16876 </span></a>        out_i = I_0 \left( \beta \sqrt{1 - \left( {\frac{i - N/2}{N/2}} \right) ^2 } \right) / I_0( \beta ) 
<a name="l16877"><span class="ln">16877 </span></a> 
<a name="l16878"><span class="ln">16878 </span></a>    Calling ``torch.kaiser_window(L, B, periodic=True)`` is equivalent to calling 
<a name="l16879"><span class="ln">16879 </span></a>    ``torch.kaiser_window(L + 1, B, periodic=False)[:-1])``. 
<a name="l16880"><span class="ln">16880 </span></a>    The :attr:`periodic` argument is intended as a helpful shorthand 
<a name="l16881"><span class="ln">16881 </span></a>    to produce a periodic window as input to functions like :func:`torch.stft`. 
<a name="l16882"><span class="ln">16882 </span></a> 
<a name="l16883"><span class="ln">16883 </span></a>    .. note:: 
<a name="l16884"><span class="ln">16884 </span></a>        If :attr:`window_length` is one, then the returned window is a single element tensor containing a one. 
<a name="l16885"><span class="ln">16885 </span></a> 
<a name="l16886"><span class="ln">16886 </span></a> 
<a name="l16887"><span class="ln">16887 </span></a>    Args: 
<a name="l16888"><span class="ln">16888 </span></a>        window_length (int): length of the window. 
<a name="l16889"><span class="ln">16889 </span></a>        periodic (bool, optional): If True, returns a periodic window suitable for use in spectral analysis. 
<a name="l16890"><span class="ln">16890 </span></a>            If False, returns a symmetric window suitable for use in filter design. 
<a name="l16891"><span class="ln">16891 </span></a>        beta (float, optional): shape parameter for the window. 
<a name="l16892"><span class="ln">16892 </span></a> 
<a name="l16893"><span class="ln">16893 </span></a>    Keyword args: 
<a name="l16894"><span class="ln">16894 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l16895"><span class="ln">16895 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l16896"><span class="ln">16896 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l16897"><span class="ln">16897 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l16898"><span class="ln">16898 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l16899"><span class="ln">16899 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l16900"><span class="ln">16900 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l16901"><span class="ln">16901 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l16902"><span class="ln">16902 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l16903"><span class="ln">16903 </span></a>            returned tensor. Default: ``False``. 
<a name="l16904"><span class="ln">16904 </span></a>    &quot;&quot;&quot;</span>
<a name="l16905"><span class="ln">16905 </span></a>
<a name="l16906"><span class="ln">16906 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l16907"><span class="ln">16907 </span></a><span class="s2">def </span><span class="s1">kaiser_window</span><span class="s3">(</span>
<a name="l16908"><span class="ln">16908 </span></a>    <span class="s1">window_length</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l16909"><span class="ln">16909 </span></a>    <span class="s1">periodic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l16910"><span class="ln">16910 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l16911"><span class="ln">16911 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16912"><span class="ln">16912 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16913"><span class="ln">16913 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16914"><span class="ln">16914 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16915"><span class="ln">16915 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l16916"><span class="ln">16916 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l16917"><span class="ln">16917 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16918"><span class="ln">16918 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16919"><span class="ln">16919 </span></a>    kaiser_window(window_length, periodic=True, beta=12.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l16920"><span class="ln">16920 </span></a> 
<a name="l16921"><span class="ln">16921 </span></a>    Computes the Kaiser window with window length :attr:`window_length` and shape parameter :attr:`beta`. 
<a name="l16922"><span class="ln">16922 </span></a> 
<a name="l16923"><span class="ln">16923 </span></a>    Let I_0 be the zeroth order modified Bessel function of the first kind (see :func:`torch.i0`) and 
<a name="l16924"><span class="ln">16924 </span></a>    ``N = L - 1`` if :attr:`periodic` is False and ``L`` if :attr:`periodic` is True, 
<a name="l16925"><span class="ln">16925 </span></a>    where ``L`` is the :attr:`window_length`. This function computes: 
<a name="l16926"><span class="ln">16926 </span></a> 
<a name="l16927"><span class="ln">16927 </span></a>    .. math:: 
<a name="l16928"><span class="ln">16928 </span></a>        out_i = I_0 \left( \beta \sqrt{1 - \left( {\frac{i - N/2}{N/2}} \right) ^2 } \right) / I_0( \beta ) 
<a name="l16929"><span class="ln">16929 </span></a> 
<a name="l16930"><span class="ln">16930 </span></a>    Calling ``torch.kaiser_window(L, B, periodic=True)`` is equivalent to calling 
<a name="l16931"><span class="ln">16931 </span></a>    ``torch.kaiser_window(L + 1, B, periodic=False)[:-1])``. 
<a name="l16932"><span class="ln">16932 </span></a>    The :attr:`periodic` argument is intended as a helpful shorthand 
<a name="l16933"><span class="ln">16933 </span></a>    to produce a periodic window as input to functions like :func:`torch.stft`. 
<a name="l16934"><span class="ln">16934 </span></a> 
<a name="l16935"><span class="ln">16935 </span></a>    .. note:: 
<a name="l16936"><span class="ln">16936 </span></a>        If :attr:`window_length` is one, then the returned window is a single element tensor containing a one. 
<a name="l16937"><span class="ln">16937 </span></a> 
<a name="l16938"><span class="ln">16938 </span></a> 
<a name="l16939"><span class="ln">16939 </span></a>    Args: 
<a name="l16940"><span class="ln">16940 </span></a>        window_length (int): length of the window. 
<a name="l16941"><span class="ln">16941 </span></a>        periodic (bool, optional): If True, returns a periodic window suitable for use in spectral analysis. 
<a name="l16942"><span class="ln">16942 </span></a>            If False, returns a symmetric window suitable for use in filter design. 
<a name="l16943"><span class="ln">16943 </span></a>        beta (float, optional): shape parameter for the window. 
<a name="l16944"><span class="ln">16944 </span></a> 
<a name="l16945"><span class="ln">16945 </span></a>    Keyword args: 
<a name="l16946"><span class="ln">16946 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l16947"><span class="ln">16947 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l16948"><span class="ln">16948 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only 
<a name="l16949"><span class="ln">16949 </span></a>              ``torch.strided`` (dense layout) is supported. 
<a name="l16950"><span class="ln">16950 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l16951"><span class="ln">16951 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l16952"><span class="ln">16952 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l16953"><span class="ln">16953 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l16954"><span class="ln">16954 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l16955"><span class="ln">16955 </span></a>            returned tensor. Default: ``False``. 
<a name="l16956"><span class="ln">16956 </span></a>    &quot;&quot;&quot;</span>
<a name="l16957"><span class="ln">16957 </span></a>
<a name="l16958"><span class="ln">16958 </span></a><span class="s2">def </span><span class="s1">kl_div</span><span class="s3">(</span>
<a name="l16959"><span class="ln">16959 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16960"><span class="ln">16960 </span></a>    <span class="s1">target</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16961"><span class="ln">16961 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l16962"><span class="ln">16962 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16963"><span class="ln">16963 </span></a>    <span class="s1">log_target</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l16964"><span class="ln">16964 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l16965"><span class="ln">16965 </span></a><span class="s2">def </span><span class="s1">kron</span><span class="s3">(</span>
<a name="l16966"><span class="ln">16966 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16967"><span class="ln">16967 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l16968"><span class="ln">16968 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l16969"><span class="ln">16969 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l16970"><span class="ln">16970 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l16971"><span class="ln">16971 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l16972"><span class="ln">16972 </span></a>    kron(input, other, *, out=None) -&gt; Tensor 
<a name="l16973"><span class="ln">16973 </span></a> 
<a name="l16974"><span class="ln">16974 </span></a>    Computes the Kronecker product, denoted by :math:`\otimes`, of :attr:`input` and :attr:`other`. 
<a name="l16975"><span class="ln">16975 </span></a> 
<a name="l16976"><span class="ln">16976 </span></a>    If :attr:`input` is a :math:`(a_0 \times a_1 \times \dots \times a_n)` tensor and :attr:`other` is a 
<a name="l16977"><span class="ln">16977 </span></a>    :math:`(b_0 \times b_1 \times \dots \times b_n)` tensor, the result will be a 
<a name="l16978"><span class="ln">16978 </span></a>    :math:`(a_0*b_0 \times a_1*b_1 \times \dots \times a_n*b_n)` tensor with the following entries: 
<a name="l16979"><span class="ln">16979 </span></a> 
<a name="l16980"><span class="ln">16980 </span></a>    .. math:: 
<a name="l16981"><span class="ln">16981 </span></a>        (\text{input} \otimes \text{other})_{k_0, k_1, \dots, k_n} = 
<a name="l16982"><span class="ln">16982 </span></a>            \text{input}_{i_0, i_1, \dots, i_n} * \text{other}_{j_0, j_1, \dots, j_n}, 
<a name="l16983"><span class="ln">16983 </span></a> 
<a name="l16984"><span class="ln">16984 </span></a>    where :math:`k_t = i_t * b_t + j_t` for :math:`0 \leq t \leq n`. 
<a name="l16985"><span class="ln">16985 </span></a>    If one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions. 
<a name="l16986"><span class="ln">16986 </span></a> 
<a name="l16987"><span class="ln">16987 </span></a>    Supports real-valued and complex-valued inputs. 
<a name="l16988"><span class="ln">16988 </span></a> 
<a name="l16989"><span class="ln">16989 </span></a>    .. note:: 
<a name="l16990"><span class="ln">16990 </span></a>        This function generalizes the typical definition of the Kronecker product for two matrices to two tensors, 
<a name="l16991"><span class="ln">16991 </span></a>        as described above. When :attr:`input` is a :math:`(m \times n)` matrix and :attr:`other` is a 
<a name="l16992"><span class="ln">16992 </span></a>        :math:`(p \times q)` matrix, the result will be a :math:`(p*m \times q*n)` block matrix: 
<a name="l16993"><span class="ln">16993 </span></a> 
<a name="l16994"><span class="ln">16994 </span></a>        .. math:: 
<a name="l16995"><span class="ln">16995 </span></a>            \mathbf{A} \otimes \mathbf{B}=\begin{bmatrix} 
<a name="l16996"><span class="ln">16996 </span></a>            a_{11} \mathbf{B} &amp; \cdots &amp; a_{1 n} \mathbf{B} \\ 
<a name="l16997"><span class="ln">16997 </span></a>            \vdots &amp; \ddots &amp; \vdots \\ 
<a name="l16998"><span class="ln">16998 </span></a>            a_{m 1} \mathbf{B} &amp; \cdots &amp; a_{m n} \mathbf{B} \end{bmatrix} 
<a name="l16999"><span class="ln">16999 </span></a> 
<a name="l17000"><span class="ln">17000 </span></a>        where :attr:`input` is :math:`\mathbf{A}` and :attr:`other` is :math:`\mathbf{B}`. 
<a name="l17001"><span class="ln">17001 </span></a> 
<a name="l17002"><span class="ln">17002 </span></a>    Arguments: 
<a name="l17003"><span class="ln">17003 </span></a>        input (Tensor) 
<a name="l17004"><span class="ln">17004 </span></a>        other (Tensor) 
<a name="l17005"><span class="ln">17005 </span></a> 
<a name="l17006"><span class="ln">17006 </span></a>    Keyword args: 
<a name="l17007"><span class="ln">17007 </span></a>        out (Tensor, optional): The output tensor. Ignored if ``None``. Default: ``None`` 
<a name="l17008"><span class="ln">17008 </span></a> 
<a name="l17009"><span class="ln">17009 </span></a>    Examples:: 
<a name="l17010"><span class="ln">17010 </span></a> 
<a name="l17011"><span class="ln">17011 </span></a>        &gt;&gt;&gt; mat1 = torch.eye(2) 
<a name="l17012"><span class="ln">17012 </span></a>        &gt;&gt;&gt; mat2 = torch.ones(2, 2) 
<a name="l17013"><span class="ln">17013 </span></a>        &gt;&gt;&gt; torch.kron(mat1, mat2) 
<a name="l17014"><span class="ln">17014 </span></a>        tensor([[1., 1., 0., 0.], 
<a name="l17015"><span class="ln">17015 </span></a>                [1., 1., 0., 0.], 
<a name="l17016"><span class="ln">17016 </span></a>                [0., 0., 1., 1.], 
<a name="l17017"><span class="ln">17017 </span></a>                [0., 0., 1., 1.]]) 
<a name="l17018"><span class="ln">17018 </span></a> 
<a name="l17019"><span class="ln">17019 </span></a>        &gt;&gt;&gt; mat1 = torch.eye(2) 
<a name="l17020"><span class="ln">17020 </span></a>        &gt;&gt;&gt; mat2 = torch.arange(1, 5).reshape(2, 2) 
<a name="l17021"><span class="ln">17021 </span></a>        &gt;&gt;&gt; torch.kron(mat1, mat2) 
<a name="l17022"><span class="ln">17022 </span></a>        tensor([[1., 2., 0., 0.], 
<a name="l17023"><span class="ln">17023 </span></a>                [3., 4., 0., 0.], 
<a name="l17024"><span class="ln">17024 </span></a>                [0., 0., 1., 2.], 
<a name="l17025"><span class="ln">17025 </span></a>                [0., 0., 3., 4.]]) 
<a name="l17026"><span class="ln">17026 </span></a>    &quot;&quot;&quot;</span>
<a name="l17027"><span class="ln">17027 </span></a>
<a name="l17028"><span class="ln">17028 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17029"><span class="ln">17029 </span></a><span class="s2">def </span><span class="s1">kthvalue</span><span class="s3">(</span>
<a name="l17030"><span class="ln">17030 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17031"><span class="ln">17031 </span></a>    <span class="s1">k</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l17032"><span class="ln">17032 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l17033"><span class="ln">17033 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l17034"><span class="ln">17034 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17035"><span class="ln">17035 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17036"><span class="ln">17036 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">kthvalue</span><span class="s2">:</span>
<a name="l17037"><span class="ln">17037 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17038"><span class="ln">17038 </span></a>    kthvalue(input, k, dim=None, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l17039"><span class="ln">17039 </span></a> 
<a name="l17040"><span class="ln">17040 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the :attr:`k` th 
<a name="l17041"><span class="ln">17041 </span></a>    smallest element of each row of the :attr:`input` tensor in the given dimension 
<a name="l17042"><span class="ln">17042 </span></a>    :attr:`dim`. And ``indices`` is the index location of each element found. 
<a name="l17043"><span class="ln">17043 </span></a> 
<a name="l17044"><span class="ln">17044 </span></a>    If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l17045"><span class="ln">17045 </span></a> 
<a name="l17046"><span class="ln">17046 </span></a>    If :attr:`keepdim` is ``True``, both the :attr:`values` and :attr:`indices` tensors 
<a name="l17047"><span class="ln">17047 </span></a>    are the same size as :attr:`input`, except in the dimension :attr:`dim` where 
<a name="l17048"><span class="ln">17048 </span></a>    they are of size 1. Otherwise, :attr:`dim` is squeezed 
<a name="l17049"><span class="ln">17049 </span></a>    (see :func:`torch.squeeze`), resulting in both the :attr:`values` and 
<a name="l17050"><span class="ln">17050 </span></a>    :attr:`indices` tensors having 1 fewer dimension than the :attr:`input` tensor. 
<a name="l17051"><span class="ln">17051 </span></a> 
<a name="l17052"><span class="ln">17052 </span></a>    .. note:: 
<a name="l17053"><span class="ln">17053 </span></a>        When :attr:`input` is a CUDA tensor and there are multiple valid 
<a name="l17054"><span class="ln">17054 </span></a>        :attr:`k` th values, this function may nondeterministically return 
<a name="l17055"><span class="ln">17055 </span></a>        :attr:`indices` for any of them. 
<a name="l17056"><span class="ln">17056 </span></a> 
<a name="l17057"><span class="ln">17057 </span></a>    Args: 
<a name="l17058"><span class="ln">17058 </span></a>        input (Tensor): the input tensor. 
<a name="l17059"><span class="ln">17059 </span></a>        k (int): k for the k-th smallest element 
<a name="l17060"><span class="ln">17060 </span></a>        dim (int, optional): the dimension to find the kth value along 
<a name="l17061"><span class="ln">17061 </span></a> 
<a name="l17062"><span class="ln">17062 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l17063"><span class="ln">17063 </span></a> 
<a name="l17064"><span class="ln">17064 </span></a> 
<a name="l17065"><span class="ln">17065 </span></a>    Keyword args: 
<a name="l17066"><span class="ln">17066 </span></a>        out (tuple, optional): the output tuple of (Tensor, LongTensor) 
<a name="l17067"><span class="ln">17067 </span></a>                               can be optionally given to be used as output buffers 
<a name="l17068"><span class="ln">17068 </span></a> 
<a name="l17069"><span class="ln">17069 </span></a>    Example:: 
<a name="l17070"><span class="ln">17070 </span></a> 
<a name="l17071"><span class="ln">17071 </span></a>        &gt;&gt;&gt; x = torch.arange(1., 6.) 
<a name="l17072"><span class="ln">17072 </span></a>        &gt;&gt;&gt; x 
<a name="l17073"><span class="ln">17073 </span></a>        tensor([ 1.,  2.,  3.,  4.,  5.]) 
<a name="l17074"><span class="ln">17074 </span></a>        &gt;&gt;&gt; torch.kthvalue(x, 4) 
<a name="l17075"><span class="ln">17075 </span></a>        torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3)) 
<a name="l17076"><span class="ln">17076 </span></a> 
<a name="l17077"><span class="ln">17077 </span></a>        &gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3) 
<a name="l17078"><span class="ln">17078 </span></a>        &gt;&gt;&gt; x 
<a name="l17079"><span class="ln">17079 </span></a>        tensor([[ 1.,  2.,  3.], 
<a name="l17080"><span class="ln">17080 </span></a>                [ 4.,  5.,  6.]]) 
<a name="l17081"><span class="ln">17081 </span></a>        &gt;&gt;&gt; torch.kthvalue(x, 2, 0, True) 
<a name="l17082"><span class="ln">17082 </span></a>        torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]])) 
<a name="l17083"><span class="ln">17083 </span></a>    &quot;&quot;&quot;</span>
<a name="l17084"><span class="ln">17084 </span></a>
<a name="l17085"><span class="ln">17085 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17086"><span class="ln">17086 </span></a><span class="s2">def </span><span class="s1">kthvalue</span><span class="s3">(</span>
<a name="l17087"><span class="ln">17087 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17088"><span class="ln">17088 </span></a>    <span class="s1">k</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l17089"><span class="ln">17089 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l17090"><span class="ln">17090 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l17091"><span class="ln">17091 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17092"><span class="ln">17092 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17093"><span class="ln">17093 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">kthvalue</span><span class="s2">:</span>
<a name="l17094"><span class="ln">17094 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17095"><span class="ln">17095 </span></a>    kthvalue(input, k, dim=None, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l17096"><span class="ln">17096 </span></a> 
<a name="l17097"><span class="ln">17097 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the :attr:`k` th 
<a name="l17098"><span class="ln">17098 </span></a>    smallest element of each row of the :attr:`input` tensor in the given dimension 
<a name="l17099"><span class="ln">17099 </span></a>    :attr:`dim`. And ``indices`` is the index location of each element found. 
<a name="l17100"><span class="ln">17100 </span></a> 
<a name="l17101"><span class="ln">17101 </span></a>    If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l17102"><span class="ln">17102 </span></a> 
<a name="l17103"><span class="ln">17103 </span></a>    If :attr:`keepdim` is ``True``, both the :attr:`values` and :attr:`indices` tensors 
<a name="l17104"><span class="ln">17104 </span></a>    are the same size as :attr:`input`, except in the dimension :attr:`dim` where 
<a name="l17105"><span class="ln">17105 </span></a>    they are of size 1. Otherwise, :attr:`dim` is squeezed 
<a name="l17106"><span class="ln">17106 </span></a>    (see :func:`torch.squeeze`), resulting in both the :attr:`values` and 
<a name="l17107"><span class="ln">17107 </span></a>    :attr:`indices` tensors having 1 fewer dimension than the :attr:`input` tensor. 
<a name="l17108"><span class="ln">17108 </span></a> 
<a name="l17109"><span class="ln">17109 </span></a>    .. note:: 
<a name="l17110"><span class="ln">17110 </span></a>        When :attr:`input` is a CUDA tensor and there are multiple valid 
<a name="l17111"><span class="ln">17111 </span></a>        :attr:`k` th values, this function may nondeterministically return 
<a name="l17112"><span class="ln">17112 </span></a>        :attr:`indices` for any of them. 
<a name="l17113"><span class="ln">17113 </span></a> 
<a name="l17114"><span class="ln">17114 </span></a>    Args: 
<a name="l17115"><span class="ln">17115 </span></a>        input (Tensor): the input tensor. 
<a name="l17116"><span class="ln">17116 </span></a>        k (int): k for the k-th smallest element 
<a name="l17117"><span class="ln">17117 </span></a>        dim (int, optional): the dimension to find the kth value along 
<a name="l17118"><span class="ln">17118 </span></a> 
<a name="l17119"><span class="ln">17119 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l17120"><span class="ln">17120 </span></a> 
<a name="l17121"><span class="ln">17121 </span></a> 
<a name="l17122"><span class="ln">17122 </span></a>    Keyword args: 
<a name="l17123"><span class="ln">17123 </span></a>        out (tuple, optional): the output tuple of (Tensor, LongTensor) 
<a name="l17124"><span class="ln">17124 </span></a>                               can be optionally given to be used as output buffers 
<a name="l17125"><span class="ln">17125 </span></a> 
<a name="l17126"><span class="ln">17126 </span></a>    Example:: 
<a name="l17127"><span class="ln">17127 </span></a> 
<a name="l17128"><span class="ln">17128 </span></a>        &gt;&gt;&gt; x = torch.arange(1., 6.) 
<a name="l17129"><span class="ln">17129 </span></a>        &gt;&gt;&gt; x 
<a name="l17130"><span class="ln">17130 </span></a>        tensor([ 1.,  2.,  3.,  4.,  5.]) 
<a name="l17131"><span class="ln">17131 </span></a>        &gt;&gt;&gt; torch.kthvalue(x, 4) 
<a name="l17132"><span class="ln">17132 </span></a>        torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3)) 
<a name="l17133"><span class="ln">17133 </span></a> 
<a name="l17134"><span class="ln">17134 </span></a>        &gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3) 
<a name="l17135"><span class="ln">17135 </span></a>        &gt;&gt;&gt; x 
<a name="l17136"><span class="ln">17136 </span></a>        tensor([[ 1.,  2.,  3.], 
<a name="l17137"><span class="ln">17137 </span></a>                [ 4.,  5.,  6.]]) 
<a name="l17138"><span class="ln">17138 </span></a>        &gt;&gt;&gt; torch.kthvalue(x, 2, 0, True) 
<a name="l17139"><span class="ln">17139 </span></a>        torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]])) 
<a name="l17140"><span class="ln">17140 </span></a>    &quot;&quot;&quot;</span>
<a name="l17141"><span class="ln">17141 </span></a>
<a name="l17142"><span class="ln">17142 </span></a><span class="s2">def </span><span class="s1">layer_norm</span><span class="s3">(</span>
<a name="l17143"><span class="ln">17143 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17144"><span class="ln">17144 </span></a>    <span class="s1">normalized_shape</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l17145"><span class="ln">17145 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17146"><span class="ln">17146 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17147"><span class="ln">17147 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-05</span><span class="s3">,</span>
<a name="l17148"><span class="ln">17148 </span></a>    <span class="s1">cudnn_enable</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l17149"><span class="ln">17149 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17150"><span class="ln">17150 </span></a><span class="s2">def </span><span class="s1">lcm</span><span class="s3">(</span>
<a name="l17151"><span class="ln">17151 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17152"><span class="ln">17152 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17153"><span class="ln">17153 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17154"><span class="ln">17154 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17155"><span class="ln">17155 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17156"><span class="ln">17156 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17157"><span class="ln">17157 </span></a>    lcm(input, other, *, out=None) -&gt; Tensor 
<a name="l17158"><span class="ln">17158 </span></a> 
<a name="l17159"><span class="ln">17159 </span></a>    Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`. 
<a name="l17160"><span class="ln">17160 </span></a> 
<a name="l17161"><span class="ln">17161 </span></a>    Both :attr:`input` and :attr:`other` must have integer types. 
<a name="l17162"><span class="ln">17162 </span></a> 
<a name="l17163"><span class="ln">17163 </span></a>    .. note:: 
<a name="l17164"><span class="ln">17164 </span></a>        This defines :math:`lcm(0, 0) = 0` and :math:`lcm(0, a) = 0`. 
<a name="l17165"><span class="ln">17165 </span></a> 
<a name="l17166"><span class="ln">17166 </span></a>    Args: 
<a name="l17167"><span class="ln">17167 </span></a>        input (Tensor): the input tensor. 
<a name="l17168"><span class="ln">17168 </span></a>        other (Tensor): the second input tensor 
<a name="l17169"><span class="ln">17169 </span></a> 
<a name="l17170"><span class="ln">17170 </span></a>    Keyword arguments: 
<a name="l17171"><span class="ln">17171 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17172"><span class="ln">17172 </span></a> 
<a name="l17173"><span class="ln">17173 </span></a>    Example:: 
<a name="l17174"><span class="ln">17174 </span></a> 
<a name="l17175"><span class="ln">17175 </span></a>        &gt;&gt;&gt; a = torch.tensor([5, 10, 15]) 
<a name="l17176"><span class="ln">17176 </span></a>        &gt;&gt;&gt; b = torch.tensor([3, 4, 5]) 
<a name="l17177"><span class="ln">17177 </span></a>        &gt;&gt;&gt; torch.lcm(a, b) 
<a name="l17178"><span class="ln">17178 </span></a>        tensor([15, 20, 15]) 
<a name="l17179"><span class="ln">17179 </span></a>        &gt;&gt;&gt; c = torch.tensor([3]) 
<a name="l17180"><span class="ln">17180 </span></a>        &gt;&gt;&gt; torch.lcm(a, c) 
<a name="l17181"><span class="ln">17181 </span></a>        tensor([15, 30, 15]) 
<a name="l17182"><span class="ln">17182 </span></a>    &quot;&quot;&quot;</span>
<a name="l17183"><span class="ln">17183 </span></a>
<a name="l17184"><span class="ln">17184 </span></a><span class="s2">def </span><span class="s1">lcm_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17185"><span class="ln">17185 </span></a><span class="s2">def </span><span class="s1">ldexp</span><span class="s3">(</span>
<a name="l17186"><span class="ln">17186 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17187"><span class="ln">17187 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17188"><span class="ln">17188 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17189"><span class="ln">17189 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17190"><span class="ln">17190 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17191"><span class="ln">17191 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17192"><span class="ln">17192 </span></a>    ldexp(input, other, *, out=None) -&gt; Tensor 
<a name="l17193"><span class="ln">17193 </span></a> 
<a name="l17194"><span class="ln">17194 </span></a>    Multiplies :attr:`input` by 2 ** :attr:`other`. 
<a name="l17195"><span class="ln">17195 </span></a> 
<a name="l17196"><span class="ln">17196 </span></a>    .. math:: 
<a name="l17197"><span class="ln">17197 </span></a>        \text{{out}}_i = \text{{input}}_i * 2^\text{{other}}_i 
<a name="l17198"><span class="ln">17198 </span></a> 
<a name="l17199"><span class="ln">17199 </span></a> 
<a name="l17200"><span class="ln">17200 </span></a>    Typically this function is used to construct floating point numbers by multiplying 
<a name="l17201"><span class="ln">17201 </span></a>    mantissas in :attr:`input` with integral powers of two created from the exponents 
<a name="l17202"><span class="ln">17202 </span></a>    in :attr:`other`. 
<a name="l17203"><span class="ln">17203 </span></a> 
<a name="l17204"><span class="ln">17204 </span></a>    Args: 
<a name="l17205"><span class="ln">17205 </span></a>        input (Tensor): the input tensor. 
<a name="l17206"><span class="ln">17206 </span></a>        other (Tensor): a tensor of exponents, typically integers. 
<a name="l17207"><span class="ln">17207 </span></a> 
<a name="l17208"><span class="ln">17208 </span></a>    Keyword args: 
<a name="l17209"><span class="ln">17209 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17210"><span class="ln">17210 </span></a> 
<a name="l17211"><span class="ln">17211 </span></a>    Example:: 
<a name="l17212"><span class="ln">17212 </span></a> 
<a name="l17213"><span class="ln">17213 </span></a>        &gt;&gt;&gt; torch.ldexp(torch.tensor([1.]), torch.tensor([1])) 
<a name="l17214"><span class="ln">17214 </span></a>        tensor([2.]) 
<a name="l17215"><span class="ln">17215 </span></a>        &gt;&gt;&gt; torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4])) 
<a name="l17216"><span class="ln">17216 </span></a>        tensor([ 2.,  4.,  8., 16.]) 
<a name="l17217"><span class="ln">17217 </span></a>    &quot;&quot;&quot;</span>
<a name="l17218"><span class="ln">17218 </span></a>
<a name="l17219"><span class="ln">17219 </span></a><span class="s2">def </span><span class="s1">ldexp_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17220"><span class="ln">17220 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17221"><span class="ln">17221 </span></a><span class="s2">def </span><span class="s1">le</span><span class="s3">(</span>
<a name="l17222"><span class="ln">17222 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17223"><span class="ln">17223 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17224"><span class="ln">17224 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17225"><span class="ln">17225 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17226"><span class="ln">17226 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17227"><span class="ln">17227 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17228"><span class="ln">17228 </span></a>    le(input, other, *, out=None) -&gt; Tensor 
<a name="l17229"><span class="ln">17229 </span></a> 
<a name="l17230"><span class="ln">17230 </span></a>    Computes :math:`\text{input} \leq \text{other}` element-wise. 
<a name="l17231"><span class="ln">17231 </span></a> 
<a name="l17232"><span class="ln">17232 </span></a> 
<a name="l17233"><span class="ln">17233 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l17234"><span class="ln">17234 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l17235"><span class="ln">17235 </span></a> 
<a name="l17236"><span class="ln">17236 </span></a>    Args: 
<a name="l17237"><span class="ln">17237 </span></a>        input (Tensor): the tensor to compare 
<a name="l17238"><span class="ln">17238 </span></a>        other (Tensor or Scalar): the tensor or value to compare 
<a name="l17239"><span class="ln">17239 </span></a> 
<a name="l17240"><span class="ln">17240 </span></a>    Keyword args: 
<a name="l17241"><span class="ln">17241 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17242"><span class="ln">17242 </span></a> 
<a name="l17243"><span class="ln">17243 </span></a>    Returns: 
<a name="l17244"><span class="ln">17244 </span></a>        A boolean tensor that is True where :attr:`input` is less than or equal to 
<a name="l17245"><span class="ln">17245 </span></a>        :attr:`other` and False elsewhere 
<a name="l17246"><span class="ln">17246 </span></a> 
<a name="l17247"><span class="ln">17247 </span></a>    Example:: 
<a name="l17248"><span class="ln">17248 </span></a> 
<a name="l17249"><span class="ln">17249 </span></a>        &gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l17250"><span class="ln">17250 </span></a>        tensor([[True, False], [True, True]]) 
<a name="l17251"><span class="ln">17251 </span></a>    &quot;&quot;&quot;</span>
<a name="l17252"><span class="ln">17252 </span></a>
<a name="l17253"><span class="ln">17253 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17254"><span class="ln">17254 </span></a><span class="s2">def </span><span class="s1">le</span><span class="s3">(</span>
<a name="l17255"><span class="ln">17255 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17256"><span class="ln">17256 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17257"><span class="ln">17257 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17258"><span class="ln">17258 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17259"><span class="ln">17259 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17260"><span class="ln">17260 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17261"><span class="ln">17261 </span></a>    le(input, other, *, out=None) -&gt; Tensor 
<a name="l17262"><span class="ln">17262 </span></a> 
<a name="l17263"><span class="ln">17263 </span></a>    Computes :math:`\text{input} \leq \text{other}` element-wise. 
<a name="l17264"><span class="ln">17264 </span></a> 
<a name="l17265"><span class="ln">17265 </span></a> 
<a name="l17266"><span class="ln">17266 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l17267"><span class="ln">17267 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l17268"><span class="ln">17268 </span></a> 
<a name="l17269"><span class="ln">17269 </span></a>    Args: 
<a name="l17270"><span class="ln">17270 </span></a>        input (Tensor): the tensor to compare 
<a name="l17271"><span class="ln">17271 </span></a>        other (Tensor or Scalar): the tensor or value to compare 
<a name="l17272"><span class="ln">17272 </span></a> 
<a name="l17273"><span class="ln">17273 </span></a>    Keyword args: 
<a name="l17274"><span class="ln">17274 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17275"><span class="ln">17275 </span></a> 
<a name="l17276"><span class="ln">17276 </span></a>    Returns: 
<a name="l17277"><span class="ln">17277 </span></a>        A boolean tensor that is True where :attr:`input` is less than or equal to 
<a name="l17278"><span class="ln">17278 </span></a>        :attr:`other` and False elsewhere 
<a name="l17279"><span class="ln">17279 </span></a> 
<a name="l17280"><span class="ln">17280 </span></a>    Example:: 
<a name="l17281"><span class="ln">17281 </span></a> 
<a name="l17282"><span class="ln">17282 </span></a>        &gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l17283"><span class="ln">17283 </span></a>        tensor([[True, False], [True, True]]) 
<a name="l17284"><span class="ln">17284 </span></a>    &quot;&quot;&quot;</span>
<a name="l17285"><span class="ln">17285 </span></a>
<a name="l17286"><span class="ln">17286 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17287"><span class="ln">17287 </span></a><span class="s2">def </span><span class="s1">lerp</span><span class="s3">(</span>
<a name="l17288"><span class="ln">17288 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17289"><span class="ln">17289 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17290"><span class="ln">17290 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17291"><span class="ln">17291 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17292"><span class="ln">17292 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17293"><span class="ln">17293 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17294"><span class="ln">17294 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17295"><span class="ln">17295 </span></a>    lerp(input, end, weight, *, out=None) 
<a name="l17296"><span class="ln">17296 </span></a> 
<a name="l17297"><span class="ln">17297 </span></a>    Does a linear interpolation of two tensors :attr:`start` (given by :attr:`input`) and :attr:`end` based 
<a name="l17298"><span class="ln">17298 </span></a>    on a scalar or tensor :attr:`weight` and returns the resulting :attr:`out` tensor. 
<a name="l17299"><span class="ln">17299 </span></a> 
<a name="l17300"><span class="ln">17300 </span></a>    .. math:: 
<a name="l17301"><span class="ln">17301 </span></a>        \text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i) 
<a name="l17302"><span class="ln">17302 </span></a> 
<a name="l17303"><span class="ln">17303 </span></a>    The shapes of :attr:`start` and :attr:`end` must be 
<a name="l17304"><span class="ln">17304 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. If :attr:`weight` is a tensor, then 
<a name="l17305"><span class="ln">17305 </span></a>    the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l17306"><span class="ln">17306 </span></a> 
<a name="l17307"><span class="ln">17307 </span></a>    Args: 
<a name="l17308"><span class="ln">17308 </span></a>        input (Tensor): the tensor with the starting points 
<a name="l17309"><span class="ln">17309 </span></a>        end (Tensor): the tensor with the ending points 
<a name="l17310"><span class="ln">17310 </span></a>        weight (float or tensor): the weight for the interpolation formula 
<a name="l17311"><span class="ln">17311 </span></a> 
<a name="l17312"><span class="ln">17312 </span></a>    Keyword args: 
<a name="l17313"><span class="ln">17313 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17314"><span class="ln">17314 </span></a> 
<a name="l17315"><span class="ln">17315 </span></a>    Example:: 
<a name="l17316"><span class="ln">17316 </span></a> 
<a name="l17317"><span class="ln">17317 </span></a>        &gt;&gt;&gt; start = torch.arange(1., 5.) 
<a name="l17318"><span class="ln">17318 </span></a>        &gt;&gt;&gt; end = torch.empty(4).fill_(10) 
<a name="l17319"><span class="ln">17319 </span></a>        &gt;&gt;&gt; start 
<a name="l17320"><span class="ln">17320 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l17321"><span class="ln">17321 </span></a>        &gt;&gt;&gt; end 
<a name="l17322"><span class="ln">17322 </span></a>        tensor([ 10.,  10.,  10.,  10.]) 
<a name="l17323"><span class="ln">17323 </span></a>        &gt;&gt;&gt; torch.lerp(start, end, 0.5) 
<a name="l17324"><span class="ln">17324 </span></a>        tensor([ 5.5000,  6.0000,  6.5000,  7.0000]) 
<a name="l17325"><span class="ln">17325 </span></a>        &gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5)) 
<a name="l17326"><span class="ln">17326 </span></a>        tensor([ 5.5000,  6.0000,  6.5000,  7.0000]) 
<a name="l17327"><span class="ln">17327 </span></a>    &quot;&quot;&quot;</span>
<a name="l17328"><span class="ln">17328 </span></a>
<a name="l17329"><span class="ln">17329 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17330"><span class="ln">17330 </span></a><span class="s2">def </span><span class="s1">lerp</span><span class="s3">(</span>
<a name="l17331"><span class="ln">17331 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17332"><span class="ln">17332 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17333"><span class="ln">17333 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17334"><span class="ln">17334 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17335"><span class="ln">17335 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17336"><span class="ln">17336 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17337"><span class="ln">17337 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17338"><span class="ln">17338 </span></a>    lerp(input, end, weight, *, out=None) 
<a name="l17339"><span class="ln">17339 </span></a> 
<a name="l17340"><span class="ln">17340 </span></a>    Does a linear interpolation of two tensors :attr:`start` (given by :attr:`input`) and :attr:`end` based 
<a name="l17341"><span class="ln">17341 </span></a>    on a scalar or tensor :attr:`weight` and returns the resulting :attr:`out` tensor. 
<a name="l17342"><span class="ln">17342 </span></a> 
<a name="l17343"><span class="ln">17343 </span></a>    .. math:: 
<a name="l17344"><span class="ln">17344 </span></a>        \text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i) 
<a name="l17345"><span class="ln">17345 </span></a> 
<a name="l17346"><span class="ln">17346 </span></a>    The shapes of :attr:`start` and :attr:`end` must be 
<a name="l17347"><span class="ln">17347 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. If :attr:`weight` is a tensor, then 
<a name="l17348"><span class="ln">17348 </span></a>    the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l17349"><span class="ln">17349 </span></a> 
<a name="l17350"><span class="ln">17350 </span></a>    Args: 
<a name="l17351"><span class="ln">17351 </span></a>        input (Tensor): the tensor with the starting points 
<a name="l17352"><span class="ln">17352 </span></a>        end (Tensor): the tensor with the ending points 
<a name="l17353"><span class="ln">17353 </span></a>        weight (float or tensor): the weight for the interpolation formula 
<a name="l17354"><span class="ln">17354 </span></a> 
<a name="l17355"><span class="ln">17355 </span></a>    Keyword args: 
<a name="l17356"><span class="ln">17356 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17357"><span class="ln">17357 </span></a> 
<a name="l17358"><span class="ln">17358 </span></a>    Example:: 
<a name="l17359"><span class="ln">17359 </span></a> 
<a name="l17360"><span class="ln">17360 </span></a>        &gt;&gt;&gt; start = torch.arange(1., 5.) 
<a name="l17361"><span class="ln">17361 </span></a>        &gt;&gt;&gt; end = torch.empty(4).fill_(10) 
<a name="l17362"><span class="ln">17362 </span></a>        &gt;&gt;&gt; start 
<a name="l17363"><span class="ln">17363 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l17364"><span class="ln">17364 </span></a>        &gt;&gt;&gt; end 
<a name="l17365"><span class="ln">17365 </span></a>        tensor([ 10.,  10.,  10.,  10.]) 
<a name="l17366"><span class="ln">17366 </span></a>        &gt;&gt;&gt; torch.lerp(start, end, 0.5) 
<a name="l17367"><span class="ln">17367 </span></a>        tensor([ 5.5000,  6.0000,  6.5000,  7.0000]) 
<a name="l17368"><span class="ln">17368 </span></a>        &gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5)) 
<a name="l17369"><span class="ln">17369 </span></a>        tensor([ 5.5000,  6.0000,  6.5000,  7.0000]) 
<a name="l17370"><span class="ln">17370 </span></a>    &quot;&quot;&quot;</span>
<a name="l17371"><span class="ln">17371 </span></a>
<a name="l17372"><span class="ln">17372 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17373"><span class="ln">17373 </span></a><span class="s2">def </span><span class="s1">less</span><span class="s3">(</span>
<a name="l17374"><span class="ln">17374 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17375"><span class="ln">17375 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17376"><span class="ln">17376 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17377"><span class="ln">17377 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17378"><span class="ln">17378 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17379"><span class="ln">17379 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17380"><span class="ln">17380 </span></a>    less(input, other, *, out=None) -&gt; Tensor 
<a name="l17381"><span class="ln">17381 </span></a> 
<a name="l17382"><span class="ln">17382 </span></a>    Alias for :func:`torch.lt`. 
<a name="l17383"><span class="ln">17383 </span></a>    &quot;&quot;&quot;</span>
<a name="l17384"><span class="ln">17384 </span></a>
<a name="l17385"><span class="ln">17385 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17386"><span class="ln">17386 </span></a><span class="s2">def </span><span class="s1">less</span><span class="s3">(</span>
<a name="l17387"><span class="ln">17387 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17388"><span class="ln">17388 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17389"><span class="ln">17389 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17390"><span class="ln">17390 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17391"><span class="ln">17391 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17392"><span class="ln">17392 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17393"><span class="ln">17393 </span></a>    less(input, other, *, out=None) -&gt; Tensor 
<a name="l17394"><span class="ln">17394 </span></a> 
<a name="l17395"><span class="ln">17395 </span></a>    Alias for :func:`torch.lt`. 
<a name="l17396"><span class="ln">17396 </span></a>    &quot;&quot;&quot;</span>
<a name="l17397"><span class="ln">17397 </span></a>
<a name="l17398"><span class="ln">17398 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17399"><span class="ln">17399 </span></a><span class="s2">def </span><span class="s1">less_equal</span><span class="s3">(</span>
<a name="l17400"><span class="ln">17400 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17401"><span class="ln">17401 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17402"><span class="ln">17402 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17403"><span class="ln">17403 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17404"><span class="ln">17404 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17405"><span class="ln">17405 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17406"><span class="ln">17406 </span></a>    less_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l17407"><span class="ln">17407 </span></a> 
<a name="l17408"><span class="ln">17408 </span></a>    Alias for :func:`torch.le`. 
<a name="l17409"><span class="ln">17409 </span></a>    &quot;&quot;&quot;</span>
<a name="l17410"><span class="ln">17410 </span></a>
<a name="l17411"><span class="ln">17411 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17412"><span class="ln">17412 </span></a><span class="s2">def </span><span class="s1">less_equal</span><span class="s3">(</span>
<a name="l17413"><span class="ln">17413 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17414"><span class="ln">17414 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17415"><span class="ln">17415 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17416"><span class="ln">17416 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17417"><span class="ln">17417 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17418"><span class="ln">17418 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17419"><span class="ln">17419 </span></a>    less_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l17420"><span class="ln">17420 </span></a> 
<a name="l17421"><span class="ln">17421 </span></a>    Alias for :func:`torch.le`. 
<a name="l17422"><span class="ln">17422 </span></a>    &quot;&quot;&quot;</span>
<a name="l17423"><span class="ln">17423 </span></a>
<a name="l17424"><span class="ln">17424 </span></a><span class="s2">def </span><span class="s1">lgamma</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17425"><span class="ln">17425 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17426"><span class="ln">17426 </span></a>    lgamma(input, *, out=None) -&gt; Tensor 
<a name="l17427"><span class="ln">17427 </span></a> 
<a name="l17428"><span class="ln">17428 </span></a>    Computes the natural logarithm of the absolute value of the gamma function on :attr:`input`. 
<a name="l17429"><span class="ln">17429 </span></a> 
<a name="l17430"><span class="ln">17430 </span></a>    .. math:: 
<a name="l17431"><span class="ln">17431 </span></a>        \text{out}_{i} = \ln |\Gamma(\text{input}_{i})| 
<a name="l17432"><span class="ln">17432 </span></a> 
<a name="l17433"><span class="ln">17433 </span></a>    Args: 
<a name="l17434"><span class="ln">17434 </span></a>        input (Tensor): the input tensor. 
<a name="l17435"><span class="ln">17435 </span></a> 
<a name="l17436"><span class="ln">17436 </span></a>    Keyword args: 
<a name="l17437"><span class="ln">17437 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17438"><span class="ln">17438 </span></a> 
<a name="l17439"><span class="ln">17439 </span></a>    Example:: 
<a name="l17440"><span class="ln">17440 </span></a> 
<a name="l17441"><span class="ln">17441 </span></a>        &gt;&gt;&gt; a = torch.arange(0.5, 2, 0.5) 
<a name="l17442"><span class="ln">17442 </span></a>        &gt;&gt;&gt; torch.lgamma(a) 
<a name="l17443"><span class="ln">17443 </span></a>        tensor([ 0.5724,  0.0000, -0.1208]) 
<a name="l17444"><span class="ln">17444 </span></a>    &quot;&quot;&quot;</span>
<a name="l17445"><span class="ln">17445 </span></a>
<a name="l17446"><span class="ln">17446 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17447"><span class="ln">17447 </span></a><span class="s2">def </span><span class="s1">linspace</span><span class="s3">(</span>
<a name="l17448"><span class="ln">17448 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l17449"><span class="ln">17449 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l17450"><span class="ln">17450 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17451"><span class="ln">17451 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17452"><span class="ln">17452 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17453"><span class="ln">17453 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17454"><span class="ln">17454 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17455"><span class="ln">17455 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l17456"><span class="ln">17456 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l17457"><span class="ln">17457 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17458"><span class="ln">17458 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17459"><span class="ln">17459 </span></a>    linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l17460"><span class="ln">17460 </span></a> 
<a name="l17461"><span class="ln">17461 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l17462"><span class="ln">17462 </span></a>    spaced from :attr:`start` to :attr:`end`, inclusive. That is, the value are: 
<a name="l17463"><span class="ln">17463 </span></a> 
<a name="l17464"><span class="ln">17464 </span></a>    .. math:: 
<a name="l17465"><span class="ln">17465 </span></a>        (\text{start}, 
<a name="l17466"><span class="ln">17466 </span></a>        \text{start} + \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17467"><span class="ln">17467 </span></a>        \ldots, 
<a name="l17468"><span class="ln">17468 </span></a>        \text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17469"><span class="ln">17469 </span></a>        \text{end}) 
<a name="l17470"><span class="ln">17470 </span></a> 
<a name="l17471"><span class="ln">17471 </span></a> 
<a name="l17472"><span class="ln">17472 </span></a>    From PyTorch 1.11 linspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l17473"><span class="ln">17473 </span></a> 
<a name="l17474"><span class="ln">17474 </span></a>    Args: 
<a name="l17475"><span class="ln">17475 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17476"><span class="ln">17476 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17477"><span class="ln">17477 </span></a>        steps (int): size of the constructed tensor 
<a name="l17478"><span class="ln">17478 </span></a> 
<a name="l17479"><span class="ln">17479 </span></a>    Keyword arguments: 
<a name="l17480"><span class="ln">17480 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17481"><span class="ln">17481 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l17482"><span class="ln">17482 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l17483"><span class="ln">17483 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l17484"><span class="ln">17484 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l17485"><span class="ln">17485 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l17486"><span class="ln">17486 </span></a>            Default: ``torch.strided``. 
<a name="l17487"><span class="ln">17487 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l17488"><span class="ln">17488 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l17489"><span class="ln">17489 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l17490"><span class="ln">17490 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l17491"><span class="ln">17491 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l17492"><span class="ln">17492 </span></a>            returned tensor. Default: ``False``. 
<a name="l17493"><span class="ln">17493 </span></a> 
<a name="l17494"><span class="ln">17494 </span></a> 
<a name="l17495"><span class="ln">17495 </span></a>    Example:: 
<a name="l17496"><span class="ln">17496 </span></a> 
<a name="l17497"><span class="ln">17497 </span></a>        &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 
<a name="l17498"><span class="ln">17498 </span></a>        tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000]) 
<a name="l17499"><span class="ln">17499 </span></a>        &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) 
<a name="l17500"><span class="ln">17500 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17501"><span class="ln">17501 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) 
<a name="l17502"><span class="ln">17502 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17503"><span class="ln">17503 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1) 
<a name="l17504"><span class="ln">17504 </span></a>        tensor([-10.]) 
<a name="l17505"><span class="ln">17505 </span></a>    &quot;&quot;&quot;</span>
<a name="l17506"><span class="ln">17506 </span></a>
<a name="l17507"><span class="ln">17507 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17508"><span class="ln">17508 </span></a><span class="s2">def </span><span class="s1">linspace</span><span class="s3">(</span>
<a name="l17509"><span class="ln">17509 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17510"><span class="ln">17510 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17511"><span class="ln">17511 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l17512"><span class="ln">17512 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17513"><span class="ln">17513 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17514"><span class="ln">17514 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17515"><span class="ln">17515 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17516"><span class="ln">17516 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17517"><span class="ln">17517 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17518"><span class="ln">17518 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17519"><span class="ln">17519 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17520"><span class="ln">17520 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17521"><span class="ln">17521 </span></a>    linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l17522"><span class="ln">17522 </span></a> 
<a name="l17523"><span class="ln">17523 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l17524"><span class="ln">17524 </span></a>    spaced from :attr:`start` to :attr:`end`, inclusive. That is, the value are: 
<a name="l17525"><span class="ln">17525 </span></a> 
<a name="l17526"><span class="ln">17526 </span></a>    .. math:: 
<a name="l17527"><span class="ln">17527 </span></a>        (\text{start}, 
<a name="l17528"><span class="ln">17528 </span></a>        \text{start} + \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17529"><span class="ln">17529 </span></a>        \ldots, 
<a name="l17530"><span class="ln">17530 </span></a>        \text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17531"><span class="ln">17531 </span></a>        \text{end}) 
<a name="l17532"><span class="ln">17532 </span></a> 
<a name="l17533"><span class="ln">17533 </span></a> 
<a name="l17534"><span class="ln">17534 </span></a>    From PyTorch 1.11 linspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l17535"><span class="ln">17535 </span></a> 
<a name="l17536"><span class="ln">17536 </span></a>    Args: 
<a name="l17537"><span class="ln">17537 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17538"><span class="ln">17538 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17539"><span class="ln">17539 </span></a>        steps (int): size of the constructed tensor 
<a name="l17540"><span class="ln">17540 </span></a> 
<a name="l17541"><span class="ln">17541 </span></a>    Keyword arguments: 
<a name="l17542"><span class="ln">17542 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17543"><span class="ln">17543 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l17544"><span class="ln">17544 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l17545"><span class="ln">17545 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l17546"><span class="ln">17546 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l17547"><span class="ln">17547 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l17548"><span class="ln">17548 </span></a>            Default: ``torch.strided``. 
<a name="l17549"><span class="ln">17549 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l17550"><span class="ln">17550 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l17551"><span class="ln">17551 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l17552"><span class="ln">17552 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l17553"><span class="ln">17553 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l17554"><span class="ln">17554 </span></a>            returned tensor. Default: ``False``. 
<a name="l17555"><span class="ln">17555 </span></a> 
<a name="l17556"><span class="ln">17556 </span></a> 
<a name="l17557"><span class="ln">17557 </span></a>    Example:: 
<a name="l17558"><span class="ln">17558 </span></a> 
<a name="l17559"><span class="ln">17559 </span></a>        &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 
<a name="l17560"><span class="ln">17560 </span></a>        tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000]) 
<a name="l17561"><span class="ln">17561 </span></a>        &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) 
<a name="l17562"><span class="ln">17562 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17563"><span class="ln">17563 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) 
<a name="l17564"><span class="ln">17564 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17565"><span class="ln">17565 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1) 
<a name="l17566"><span class="ln">17566 </span></a>        tensor([-10.]) 
<a name="l17567"><span class="ln">17567 </span></a>    &quot;&quot;&quot;</span>
<a name="l17568"><span class="ln">17568 </span></a>
<a name="l17569"><span class="ln">17569 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17570"><span class="ln">17570 </span></a><span class="s2">def </span><span class="s1">linspace</span><span class="s3">(</span>
<a name="l17571"><span class="ln">17571 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17572"><span class="ln">17572 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17573"><span class="ln">17573 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l17574"><span class="ln">17574 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17575"><span class="ln">17575 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17576"><span class="ln">17576 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17577"><span class="ln">17577 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17578"><span class="ln">17578 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17579"><span class="ln">17579 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17580"><span class="ln">17580 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17581"><span class="ln">17581 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17582"><span class="ln">17582 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17583"><span class="ln">17583 </span></a>    linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l17584"><span class="ln">17584 </span></a> 
<a name="l17585"><span class="ln">17585 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l17586"><span class="ln">17586 </span></a>    spaced from :attr:`start` to :attr:`end`, inclusive. That is, the value are: 
<a name="l17587"><span class="ln">17587 </span></a> 
<a name="l17588"><span class="ln">17588 </span></a>    .. math:: 
<a name="l17589"><span class="ln">17589 </span></a>        (\text{start}, 
<a name="l17590"><span class="ln">17590 </span></a>        \text{start} + \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17591"><span class="ln">17591 </span></a>        \ldots, 
<a name="l17592"><span class="ln">17592 </span></a>        \text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17593"><span class="ln">17593 </span></a>        \text{end}) 
<a name="l17594"><span class="ln">17594 </span></a> 
<a name="l17595"><span class="ln">17595 </span></a> 
<a name="l17596"><span class="ln">17596 </span></a>    From PyTorch 1.11 linspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l17597"><span class="ln">17597 </span></a> 
<a name="l17598"><span class="ln">17598 </span></a>    Args: 
<a name="l17599"><span class="ln">17599 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17600"><span class="ln">17600 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17601"><span class="ln">17601 </span></a>        steps (int): size of the constructed tensor 
<a name="l17602"><span class="ln">17602 </span></a> 
<a name="l17603"><span class="ln">17603 </span></a>    Keyword arguments: 
<a name="l17604"><span class="ln">17604 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17605"><span class="ln">17605 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l17606"><span class="ln">17606 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l17607"><span class="ln">17607 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l17608"><span class="ln">17608 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l17609"><span class="ln">17609 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l17610"><span class="ln">17610 </span></a>            Default: ``torch.strided``. 
<a name="l17611"><span class="ln">17611 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l17612"><span class="ln">17612 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l17613"><span class="ln">17613 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l17614"><span class="ln">17614 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l17615"><span class="ln">17615 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l17616"><span class="ln">17616 </span></a>            returned tensor. Default: ``False``. 
<a name="l17617"><span class="ln">17617 </span></a> 
<a name="l17618"><span class="ln">17618 </span></a> 
<a name="l17619"><span class="ln">17619 </span></a>    Example:: 
<a name="l17620"><span class="ln">17620 </span></a> 
<a name="l17621"><span class="ln">17621 </span></a>        &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 
<a name="l17622"><span class="ln">17622 </span></a>        tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000]) 
<a name="l17623"><span class="ln">17623 </span></a>        &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) 
<a name="l17624"><span class="ln">17624 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17625"><span class="ln">17625 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) 
<a name="l17626"><span class="ln">17626 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17627"><span class="ln">17627 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1) 
<a name="l17628"><span class="ln">17628 </span></a>        tensor([-10.]) 
<a name="l17629"><span class="ln">17629 </span></a>    &quot;&quot;&quot;</span>
<a name="l17630"><span class="ln">17630 </span></a>
<a name="l17631"><span class="ln">17631 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17632"><span class="ln">17632 </span></a><span class="s2">def </span><span class="s1">linspace</span><span class="s3">(</span>
<a name="l17633"><span class="ln">17633 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17634"><span class="ln">17634 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17635"><span class="ln">17635 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l17636"><span class="ln">17636 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17637"><span class="ln">17637 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17638"><span class="ln">17638 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17639"><span class="ln">17639 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17640"><span class="ln">17640 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17641"><span class="ln">17641 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17642"><span class="ln">17642 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17643"><span class="ln">17643 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17644"><span class="ln">17644 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17645"><span class="ln">17645 </span></a>    linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l17646"><span class="ln">17646 </span></a> 
<a name="l17647"><span class="ln">17647 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l17648"><span class="ln">17648 </span></a>    spaced from :attr:`start` to :attr:`end`, inclusive. That is, the value are: 
<a name="l17649"><span class="ln">17649 </span></a> 
<a name="l17650"><span class="ln">17650 </span></a>    .. math:: 
<a name="l17651"><span class="ln">17651 </span></a>        (\text{start}, 
<a name="l17652"><span class="ln">17652 </span></a>        \text{start} + \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17653"><span class="ln">17653 </span></a>        \ldots, 
<a name="l17654"><span class="ln">17654 </span></a>        \text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17655"><span class="ln">17655 </span></a>        \text{end}) 
<a name="l17656"><span class="ln">17656 </span></a> 
<a name="l17657"><span class="ln">17657 </span></a> 
<a name="l17658"><span class="ln">17658 </span></a>    From PyTorch 1.11 linspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l17659"><span class="ln">17659 </span></a> 
<a name="l17660"><span class="ln">17660 </span></a>    Args: 
<a name="l17661"><span class="ln">17661 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17662"><span class="ln">17662 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17663"><span class="ln">17663 </span></a>        steps (int): size of the constructed tensor 
<a name="l17664"><span class="ln">17664 </span></a> 
<a name="l17665"><span class="ln">17665 </span></a>    Keyword arguments: 
<a name="l17666"><span class="ln">17666 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17667"><span class="ln">17667 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l17668"><span class="ln">17668 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l17669"><span class="ln">17669 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l17670"><span class="ln">17670 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l17671"><span class="ln">17671 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l17672"><span class="ln">17672 </span></a>            Default: ``torch.strided``. 
<a name="l17673"><span class="ln">17673 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l17674"><span class="ln">17674 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l17675"><span class="ln">17675 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l17676"><span class="ln">17676 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l17677"><span class="ln">17677 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l17678"><span class="ln">17678 </span></a>            returned tensor. Default: ``False``. 
<a name="l17679"><span class="ln">17679 </span></a> 
<a name="l17680"><span class="ln">17680 </span></a> 
<a name="l17681"><span class="ln">17681 </span></a>    Example:: 
<a name="l17682"><span class="ln">17682 </span></a> 
<a name="l17683"><span class="ln">17683 </span></a>        &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 
<a name="l17684"><span class="ln">17684 </span></a>        tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000]) 
<a name="l17685"><span class="ln">17685 </span></a>        &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) 
<a name="l17686"><span class="ln">17686 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17687"><span class="ln">17687 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) 
<a name="l17688"><span class="ln">17688 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17689"><span class="ln">17689 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1) 
<a name="l17690"><span class="ln">17690 </span></a>        tensor([-10.]) 
<a name="l17691"><span class="ln">17691 </span></a>    &quot;&quot;&quot;</span>
<a name="l17692"><span class="ln">17692 </span></a>
<a name="l17693"><span class="ln">17693 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17694"><span class="ln">17694 </span></a><span class="s2">def </span><span class="s1">linspace</span><span class="s3">(</span>
<a name="l17695"><span class="ln">17695 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17696"><span class="ln">17696 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l17697"><span class="ln">17697 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l17698"><span class="ln">17698 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17699"><span class="ln">17699 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17700"><span class="ln">17700 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17701"><span class="ln">17701 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17702"><span class="ln">17702 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17703"><span class="ln">17703 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17704"><span class="ln">17704 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l17705"><span class="ln">17705 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17706"><span class="ln">17706 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17707"><span class="ln">17707 </span></a>    linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l17708"><span class="ln">17708 </span></a> 
<a name="l17709"><span class="ln">17709 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l17710"><span class="ln">17710 </span></a>    spaced from :attr:`start` to :attr:`end`, inclusive. That is, the value are: 
<a name="l17711"><span class="ln">17711 </span></a> 
<a name="l17712"><span class="ln">17712 </span></a>    .. math:: 
<a name="l17713"><span class="ln">17713 </span></a>        (\text{start}, 
<a name="l17714"><span class="ln">17714 </span></a>        \text{start} + \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17715"><span class="ln">17715 </span></a>        \ldots, 
<a name="l17716"><span class="ln">17716 </span></a>        \text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{\text{steps} - 1}, 
<a name="l17717"><span class="ln">17717 </span></a>        \text{end}) 
<a name="l17718"><span class="ln">17718 </span></a> 
<a name="l17719"><span class="ln">17719 </span></a> 
<a name="l17720"><span class="ln">17720 </span></a>    From PyTorch 1.11 linspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l17721"><span class="ln">17721 </span></a> 
<a name="l17722"><span class="ln">17722 </span></a>    Args: 
<a name="l17723"><span class="ln">17723 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17724"><span class="ln">17724 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l17725"><span class="ln">17725 </span></a>        steps (int): size of the constructed tensor 
<a name="l17726"><span class="ln">17726 </span></a> 
<a name="l17727"><span class="ln">17727 </span></a>    Keyword arguments: 
<a name="l17728"><span class="ln">17728 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17729"><span class="ln">17729 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l17730"><span class="ln">17730 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l17731"><span class="ln">17731 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l17732"><span class="ln">17732 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l17733"><span class="ln">17733 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l17734"><span class="ln">17734 </span></a>            Default: ``torch.strided``. 
<a name="l17735"><span class="ln">17735 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l17736"><span class="ln">17736 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l17737"><span class="ln">17737 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l17738"><span class="ln">17738 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l17739"><span class="ln">17739 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l17740"><span class="ln">17740 </span></a>            returned tensor. Default: ``False``. 
<a name="l17741"><span class="ln">17741 </span></a> 
<a name="l17742"><span class="ln">17742 </span></a> 
<a name="l17743"><span class="ln">17743 </span></a>    Example:: 
<a name="l17744"><span class="ln">17744 </span></a> 
<a name="l17745"><span class="ln">17745 </span></a>        &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 
<a name="l17746"><span class="ln">17746 </span></a>        tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000]) 
<a name="l17747"><span class="ln">17747 </span></a>        &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) 
<a name="l17748"><span class="ln">17748 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17749"><span class="ln">17749 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) 
<a name="l17750"><span class="ln">17750 </span></a>        tensor([-10.,  -5.,   0.,   5.,  10.]) 
<a name="l17751"><span class="ln">17751 </span></a>        &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1) 
<a name="l17752"><span class="ln">17752 </span></a>        tensor([-10.]) 
<a name="l17753"><span class="ln">17753 </span></a>    &quot;&quot;&quot;</span>
<a name="l17754"><span class="ln">17754 </span></a>
<a name="l17755"><span class="ln">17755 </span></a><span class="s2">def </span><span class="s1">log</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17756"><span class="ln">17756 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17757"><span class="ln">17757 </span></a>    log(input, *, out=None) -&gt; Tensor 
<a name="l17758"><span class="ln">17758 </span></a> 
<a name="l17759"><span class="ln">17759 </span></a>    Returns a new tensor with the natural logarithm of the elements 
<a name="l17760"><span class="ln">17760 </span></a>    of :attr:`input`. 
<a name="l17761"><span class="ln">17761 </span></a> 
<a name="l17762"><span class="ln">17762 </span></a>    .. math:: 
<a name="l17763"><span class="ln">17763 </span></a>        y_{i} = \log_{e} (x_{i}) 
<a name="l17764"><span class="ln">17764 </span></a> 
<a name="l17765"><span class="ln">17765 </span></a> 
<a name="l17766"><span class="ln">17766 </span></a>    Args: 
<a name="l17767"><span class="ln">17767 </span></a>        input (Tensor): the input tensor. 
<a name="l17768"><span class="ln">17768 </span></a> 
<a name="l17769"><span class="ln">17769 </span></a>    Keyword args: 
<a name="l17770"><span class="ln">17770 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17771"><span class="ln">17771 </span></a> 
<a name="l17772"><span class="ln">17772 </span></a>    Example:: 
<a name="l17773"><span class="ln">17773 </span></a> 
<a name="l17774"><span class="ln">17774 </span></a>        &gt;&gt;&gt; a = torch.rand(5) * 5 
<a name="l17775"><span class="ln">17775 </span></a>        &gt;&gt;&gt; a 
<a name="l17776"><span class="ln">17776 </span></a>        tensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739]) 
<a name="l17777"><span class="ln">17777 </span></a>        &gt;&gt;&gt; torch.log(a) 
<a name="l17778"><span class="ln">17778 </span></a>        tensor([ 1.5637,  1.4640,  0.1952, -1.4226,  1.5204]) 
<a name="l17779"><span class="ln">17779 </span></a>    &quot;&quot;&quot;</span>
<a name="l17780"><span class="ln">17780 </span></a>
<a name="l17781"><span class="ln">17781 </span></a><span class="s2">def </span><span class="s1">log10</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17782"><span class="ln">17782 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17783"><span class="ln">17783 </span></a>    log10(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l17784"><span class="ln">17784 </span></a> 
<a name="l17785"><span class="ln">17785 </span></a>    Returns a new tensor with the logarithm to the base 10 of the elements 
<a name="l17786"><span class="ln">17786 </span></a>    of :attr:`input`. 
<a name="l17787"><span class="ln">17787 </span></a> 
<a name="l17788"><span class="ln">17788 </span></a>    .. math:: 
<a name="l17789"><span class="ln">17789 </span></a>        y_{i} = \log_{10} (x_{i}) 
<a name="l17790"><span class="ln">17790 </span></a> 
<a name="l17791"><span class="ln">17791 </span></a> 
<a name="l17792"><span class="ln">17792 </span></a>    Args: 
<a name="l17793"><span class="ln">17793 </span></a>        input (Tensor): the input tensor. 
<a name="l17794"><span class="ln">17794 </span></a> 
<a name="l17795"><span class="ln">17795 </span></a>    Keyword args: 
<a name="l17796"><span class="ln">17796 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17797"><span class="ln">17797 </span></a> 
<a name="l17798"><span class="ln">17798 </span></a>    Example:: 
<a name="l17799"><span class="ln">17799 </span></a> 
<a name="l17800"><span class="ln">17800 </span></a>        &gt;&gt;&gt; a = torch.rand(5) 
<a name="l17801"><span class="ln">17801 </span></a>        &gt;&gt;&gt; a 
<a name="l17802"><span class="ln">17802 </span></a>        tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251]) 
<a name="l17803"><span class="ln">17803 </span></a> 
<a name="l17804"><span class="ln">17804 </span></a> 
<a name="l17805"><span class="ln">17805 </span></a>        &gt;&gt;&gt; torch.log10(a) 
<a name="l17806"><span class="ln">17806 </span></a>        tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476]) 
<a name="l17807"><span class="ln">17807 </span></a>    &quot;&quot;&quot;</span>
<a name="l17808"><span class="ln">17808 </span></a>
<a name="l17809"><span class="ln">17809 </span></a><span class="s2">def </span><span class="s1">log10_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17810"><span class="ln">17810 </span></a><span class="s2">def </span><span class="s1">log1p</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17811"><span class="ln">17811 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17812"><span class="ln">17812 </span></a>    log1p(input, *, out=None) -&gt; Tensor 
<a name="l17813"><span class="ln">17813 </span></a> 
<a name="l17814"><span class="ln">17814 </span></a>    Returns a new tensor with the natural logarithm of (1 + :attr:`input`). 
<a name="l17815"><span class="ln">17815 </span></a> 
<a name="l17816"><span class="ln">17816 </span></a>    .. math:: 
<a name="l17817"><span class="ln">17817 </span></a>        y_i = \log_{e} (x_i + 1) 
<a name="l17818"><span class="ln">17818 </span></a> 
<a name="l17819"><span class="ln">17819 </span></a>    .. note:: This function is more accurate than :func:`torch.log` for small 
<a name="l17820"><span class="ln">17820 </span></a>              values of :attr:`input` 
<a name="l17821"><span class="ln">17821 </span></a> 
<a name="l17822"><span class="ln">17822 </span></a>    Args: 
<a name="l17823"><span class="ln">17823 </span></a>        input (Tensor): the input tensor. 
<a name="l17824"><span class="ln">17824 </span></a> 
<a name="l17825"><span class="ln">17825 </span></a>    Keyword args: 
<a name="l17826"><span class="ln">17826 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17827"><span class="ln">17827 </span></a> 
<a name="l17828"><span class="ln">17828 </span></a>    Example:: 
<a name="l17829"><span class="ln">17829 </span></a> 
<a name="l17830"><span class="ln">17830 </span></a>        &gt;&gt;&gt; a = torch.randn(5) 
<a name="l17831"><span class="ln">17831 </span></a>        &gt;&gt;&gt; a 
<a name="l17832"><span class="ln">17832 </span></a>        tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492]) 
<a name="l17833"><span class="ln">17833 </span></a>        &gt;&gt;&gt; torch.log1p(a) 
<a name="l17834"><span class="ln">17834 </span></a>        tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225]) 
<a name="l17835"><span class="ln">17835 </span></a>    &quot;&quot;&quot;</span>
<a name="l17836"><span class="ln">17836 </span></a>
<a name="l17837"><span class="ln">17837 </span></a><span class="s2">def </span><span class="s1">log1p_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17838"><span class="ln">17838 </span></a><span class="s2">def </span><span class="s1">log2</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17839"><span class="ln">17839 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17840"><span class="ln">17840 </span></a>    log2(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l17841"><span class="ln">17841 </span></a> 
<a name="l17842"><span class="ln">17842 </span></a>    Returns a new tensor with the logarithm to the base 2 of the elements 
<a name="l17843"><span class="ln">17843 </span></a>    of :attr:`input`. 
<a name="l17844"><span class="ln">17844 </span></a> 
<a name="l17845"><span class="ln">17845 </span></a>    .. math:: 
<a name="l17846"><span class="ln">17846 </span></a>        y_{i} = \log_{2} (x_{i}) 
<a name="l17847"><span class="ln">17847 </span></a> 
<a name="l17848"><span class="ln">17848 </span></a> 
<a name="l17849"><span class="ln">17849 </span></a>    Args: 
<a name="l17850"><span class="ln">17850 </span></a>        input (Tensor): the input tensor. 
<a name="l17851"><span class="ln">17851 </span></a> 
<a name="l17852"><span class="ln">17852 </span></a>    Keyword args: 
<a name="l17853"><span class="ln">17853 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17854"><span class="ln">17854 </span></a> 
<a name="l17855"><span class="ln">17855 </span></a>    Example:: 
<a name="l17856"><span class="ln">17856 </span></a> 
<a name="l17857"><span class="ln">17857 </span></a>        &gt;&gt;&gt; a = torch.rand(5) 
<a name="l17858"><span class="ln">17858 </span></a>        &gt;&gt;&gt; a 
<a name="l17859"><span class="ln">17859 </span></a>        tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490]) 
<a name="l17860"><span class="ln">17860 </span></a> 
<a name="l17861"><span class="ln">17861 </span></a> 
<a name="l17862"><span class="ln">17862 </span></a>        &gt;&gt;&gt; torch.log2(a) 
<a name="l17863"><span class="ln">17863 </span></a>        tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504]) 
<a name="l17864"><span class="ln">17864 </span></a>    &quot;&quot;&quot;</span>
<a name="l17865"><span class="ln">17865 </span></a>
<a name="l17866"><span class="ln">17866 </span></a><span class="s2">def </span><span class="s1">log2_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17867"><span class="ln">17867 </span></a><span class="s2">def </span><span class="s1">log_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17868"><span class="ln">17868 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17869"><span class="ln">17869 </span></a><span class="s2">def </span><span class="s1">log_softmax</span><span class="s3">(</span>
<a name="l17870"><span class="ln">17870 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17871"><span class="ln">17871 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l17872"><span class="ln">17872 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17873"><span class="ln">17873 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17874"><span class="ln">17874 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17875"><span class="ln">17875 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17876"><span class="ln">17876 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17877"><span class="ln">17877 </span></a><span class="s2">def </span><span class="s1">log_softmax</span><span class="s3">(</span>
<a name="l17878"><span class="ln">17878 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17879"><span class="ln">17879 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l17880"><span class="ln">17880 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17881"><span class="ln">17881 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17882"><span class="ln">17882 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l17883"><span class="ln">17883 </span></a><span class="s2">def </span><span class="s1">logaddexp</span><span class="s3">(</span>
<a name="l17884"><span class="ln">17884 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17885"><span class="ln">17885 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17886"><span class="ln">17886 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17887"><span class="ln">17887 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17888"><span class="ln">17888 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17889"><span class="ln">17889 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17890"><span class="ln">17890 </span></a>    logaddexp(input, other, *, out=None) -&gt; Tensor 
<a name="l17891"><span class="ln">17891 </span></a> 
<a name="l17892"><span class="ln">17892 </span></a>    Logarithm of the sum of exponentiations of the inputs. 
<a name="l17893"><span class="ln">17893 </span></a> 
<a name="l17894"><span class="ln">17894 </span></a>    Calculates pointwise :math:`\log\left(e^x + e^y\right)`. This function is useful 
<a name="l17895"><span class="ln">17895 </span></a>    in statistics where the calculated probabilities of events may be so small as to 
<a name="l17896"><span class="ln">17896 </span></a>    exceed the range of normal floating point numbers. In such cases the logarithm 
<a name="l17897"><span class="ln">17897 </span></a>    of the calculated probability is stored. This function allows adding 
<a name="l17898"><span class="ln">17898 </span></a>    probabilities stored in such a fashion. 
<a name="l17899"><span class="ln">17899 </span></a> 
<a name="l17900"><span class="ln">17900 </span></a>    This op should be disambiguated with :func:`torch.logsumexp` which performs a 
<a name="l17901"><span class="ln">17901 </span></a>    reduction on a single tensor. 
<a name="l17902"><span class="ln">17902 </span></a> 
<a name="l17903"><span class="ln">17903 </span></a>    Args: 
<a name="l17904"><span class="ln">17904 </span></a>        input (Tensor): the input tensor. 
<a name="l17905"><span class="ln">17905 </span></a>        other (Tensor): the second input tensor 
<a name="l17906"><span class="ln">17906 </span></a> 
<a name="l17907"><span class="ln">17907 </span></a>    Keyword arguments: 
<a name="l17908"><span class="ln">17908 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17909"><span class="ln">17909 </span></a> 
<a name="l17910"><span class="ln">17910 </span></a>    Example:: 
<a name="l17911"><span class="ln">17911 </span></a> 
<a name="l17912"><span class="ln">17912 </span></a>        &gt;&gt;&gt; torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3])) 
<a name="l17913"><span class="ln">17913 </span></a>        tensor([-0.3069, -0.6867, -0.8731]) 
<a name="l17914"><span class="ln">17914 </span></a>        &gt;&gt;&gt; torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3])) 
<a name="l17915"><span class="ln">17915 </span></a>        tensor([-1., -2., -3.]) 
<a name="l17916"><span class="ln">17916 </span></a>        &gt;&gt;&gt; torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3])) 
<a name="l17917"><span class="ln">17917 </span></a>        tensor([1.1269e+00, 2.0000e+03, 3.0000e+04]) 
<a name="l17918"><span class="ln">17918 </span></a>    &quot;&quot;&quot;</span>
<a name="l17919"><span class="ln">17919 </span></a>
<a name="l17920"><span class="ln">17920 </span></a><span class="s2">def </span><span class="s1">logaddexp2</span><span class="s3">(</span>
<a name="l17921"><span class="ln">17921 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17922"><span class="ln">17922 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17923"><span class="ln">17923 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17924"><span class="ln">17924 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17925"><span class="ln">17925 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17926"><span class="ln">17926 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17927"><span class="ln">17927 </span></a>    logaddexp2(input, other, *, out=None) -&gt; Tensor 
<a name="l17928"><span class="ln">17928 </span></a> 
<a name="l17929"><span class="ln">17929 </span></a>    Logarithm of the sum of exponentiations of the inputs in base-2. 
<a name="l17930"><span class="ln">17930 </span></a> 
<a name="l17931"><span class="ln">17931 </span></a>    Calculates pointwise :math:`\log_2\left(2^x + 2^y\right)`. See 
<a name="l17932"><span class="ln">17932 </span></a>    :func:`torch.logaddexp` for more details. 
<a name="l17933"><span class="ln">17933 </span></a> 
<a name="l17934"><span class="ln">17934 </span></a>    Args: 
<a name="l17935"><span class="ln">17935 </span></a>        input (Tensor): the input tensor. 
<a name="l17936"><span class="ln">17936 </span></a>        other (Tensor): the second input tensor 
<a name="l17937"><span class="ln">17937 </span></a> 
<a name="l17938"><span class="ln">17938 </span></a>    Keyword arguments: 
<a name="l17939"><span class="ln">17939 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17940"><span class="ln">17940 </span></a>    &quot;&quot;&quot;</span>
<a name="l17941"><span class="ln">17941 </span></a>
<a name="l17942"><span class="ln">17942 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17943"><span class="ln">17943 </span></a><span class="s2">def </span><span class="s1">logcumsumexp</span><span class="s3">(</span>
<a name="l17944"><span class="ln">17944 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17945"><span class="ln">17945 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l17946"><span class="ln">17946 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17947"><span class="ln">17947 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17948"><span class="ln">17948 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17949"><span class="ln">17949 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17950"><span class="ln">17950 </span></a>    logcumsumexp(input, dim, *, out=None) -&gt; Tensor 
<a name="l17951"><span class="ln">17951 </span></a>    Returns the logarithm of the cumulative summation of the exponentiation of 
<a name="l17952"><span class="ln">17952 </span></a>    elements of :attr:`input` in the dimension :attr:`dim`. 
<a name="l17953"><span class="ln">17953 </span></a> 
<a name="l17954"><span class="ln">17954 </span></a>    For summation index :math:`j` given by `dim` and other indices :math:`i`, the result is 
<a name="l17955"><span class="ln">17955 </span></a> 
<a name="l17956"><span class="ln">17956 </span></a>        .. math:: 
<a name="l17957"><span class="ln">17957 </span></a>            \text{logcumsumexp}(x)_{ij} = \log \sum\limits_{k=0}^{j} \exp(x_{ik}) 
<a name="l17958"><span class="ln">17958 </span></a> 
<a name="l17959"><span class="ln">17959 </span></a>    Args: 
<a name="l17960"><span class="ln">17960 </span></a>        input (Tensor): the input tensor. 
<a name="l17961"><span class="ln">17961 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l17962"><span class="ln">17962 </span></a> 
<a name="l17963"><span class="ln">17963 </span></a>    Keyword args: 
<a name="l17964"><span class="ln">17964 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17965"><span class="ln">17965 </span></a> 
<a name="l17966"><span class="ln">17966 </span></a>    Example:: 
<a name="l17967"><span class="ln">17967 </span></a> 
<a name="l17968"><span class="ln">17968 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l17969"><span class="ln">17969 </span></a>        &gt;&gt;&gt; torch.logcumsumexp(a, dim=0) 
<a name="l17970"><span class="ln">17970 </span></a>        tensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811, 
<a name="l17971"><span class="ln">17971 </span></a>                 1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475])) 
<a name="l17972"><span class="ln">17972 </span></a>    &quot;&quot;&quot;</span>
<a name="l17973"><span class="ln">17973 </span></a>
<a name="l17974"><span class="ln">17974 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l17975"><span class="ln">17975 </span></a><span class="s2">def </span><span class="s1">logcumsumexp</span><span class="s3">(</span>
<a name="l17976"><span class="ln">17976 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l17977"><span class="ln">17977 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l17978"><span class="ln">17978 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l17979"><span class="ln">17979 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l17980"><span class="ln">17980 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l17981"><span class="ln">17981 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l17982"><span class="ln">17982 </span></a>    logcumsumexp(input, dim, *, out=None) -&gt; Tensor 
<a name="l17983"><span class="ln">17983 </span></a>    Returns the logarithm of the cumulative summation of the exponentiation of 
<a name="l17984"><span class="ln">17984 </span></a>    elements of :attr:`input` in the dimension :attr:`dim`. 
<a name="l17985"><span class="ln">17985 </span></a> 
<a name="l17986"><span class="ln">17986 </span></a>    For summation index :math:`j` given by `dim` and other indices :math:`i`, the result is 
<a name="l17987"><span class="ln">17987 </span></a> 
<a name="l17988"><span class="ln">17988 </span></a>        .. math:: 
<a name="l17989"><span class="ln">17989 </span></a>            \text{logcumsumexp}(x)_{ij} = \log \sum\limits_{k=0}^{j} \exp(x_{ik}) 
<a name="l17990"><span class="ln">17990 </span></a> 
<a name="l17991"><span class="ln">17991 </span></a>    Args: 
<a name="l17992"><span class="ln">17992 </span></a>        input (Tensor): the input tensor. 
<a name="l17993"><span class="ln">17993 </span></a>        dim  (int): the dimension to do the operation over 
<a name="l17994"><span class="ln">17994 </span></a> 
<a name="l17995"><span class="ln">17995 </span></a>    Keyword args: 
<a name="l17996"><span class="ln">17996 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l17997"><span class="ln">17997 </span></a> 
<a name="l17998"><span class="ln">17998 </span></a>    Example:: 
<a name="l17999"><span class="ln">17999 </span></a> 
<a name="l18000"><span class="ln">18000 </span></a>        &gt;&gt;&gt; a = torch.randn(10) 
<a name="l18001"><span class="ln">18001 </span></a>        &gt;&gt;&gt; torch.logcumsumexp(a, dim=0) 
<a name="l18002"><span class="ln">18002 </span></a>        tensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811, 
<a name="l18003"><span class="ln">18003 </span></a>                 1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475])) 
<a name="l18004"><span class="ln">18004 </span></a>    &quot;&quot;&quot;</span>
<a name="l18005"><span class="ln">18005 </span></a>
<a name="l18006"><span class="ln">18006 </span></a><span class="s2">def </span><span class="s1">logdet</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18007"><span class="ln">18007 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18008"><span class="ln">18008 </span></a>    logdet(input) -&gt; Tensor 
<a name="l18009"><span class="ln">18009 </span></a> 
<a name="l18010"><span class="ln">18010 </span></a>    Calculates log determinant of a square matrix or batches of square matrices. 
<a name="l18011"><span class="ln">18011 </span></a> 
<a name="l18012"><span class="ln">18012 </span></a>    It returns ``-inf`` if the input has a determinant of zero, and ``NaN`` if it has 
<a name="l18013"><span class="ln">18013 </span></a>    a negative determinant. 
<a name="l18014"><span class="ln">18014 </span></a> 
<a name="l18015"><span class="ln">18015 </span></a>    .. note:: 
<a name="l18016"><span class="ln">18016 </span></a>        Backward through :meth:`logdet` internally uses SVD results when :attr:`input` 
<a name="l18017"><span class="ln">18017 </span></a>        is not invertible. In this case, double backward through :meth:`logdet` will 
<a name="l18018"><span class="ln">18018 </span></a>        be unstable in when :attr:`input` doesn't have distinct singular values. See 
<a name="l18019"><span class="ln">18019 </span></a>        :func:`torch.linalg.svd` for details. 
<a name="l18020"><span class="ln">18020 </span></a> 
<a name="l18021"><span class="ln">18021 </span></a>    .. seealso:: 
<a name="l18022"><span class="ln">18022 </span></a> 
<a name="l18023"><span class="ln">18023 </span></a>            :func:`torch.linalg.slogdet` computes the sign (resp. angle) and natural logarithm of the 
<a name="l18024"><span class="ln">18024 </span></a>            absolute value of the determinant of real-valued (resp. complex) square matrices. 
<a name="l18025"><span class="ln">18025 </span></a> 
<a name="l18026"><span class="ln">18026 </span></a>    Arguments: 
<a name="l18027"><span class="ln">18027 </span></a>        input (Tensor): the input tensor of size ``(*, n, n)`` where ``*`` is zero or more 
<a name="l18028"><span class="ln">18028 </span></a>                    batch dimensions. 
<a name="l18029"><span class="ln">18029 </span></a> 
<a name="l18030"><span class="ln">18030 </span></a>    Example:: 
<a name="l18031"><span class="ln">18031 </span></a> 
<a name="l18032"><span class="ln">18032 </span></a>        &gt;&gt;&gt; A = torch.randn(3, 3) 
<a name="l18033"><span class="ln">18033 </span></a>        &gt;&gt;&gt; torch.det(A) 
<a name="l18034"><span class="ln">18034 </span></a>        tensor(0.2611) 
<a name="l18035"><span class="ln">18035 </span></a>        &gt;&gt;&gt; torch.logdet(A) 
<a name="l18036"><span class="ln">18036 </span></a>        tensor(-1.3430) 
<a name="l18037"><span class="ln">18037 </span></a>        &gt;&gt;&gt; A 
<a name="l18038"><span class="ln">18038 </span></a>        tensor([[[ 0.9254, -0.6213], 
<a name="l18039"><span class="ln">18039 </span></a>                 [-0.5787,  1.6843]], 
<a name="l18040"><span class="ln">18040 </span></a> 
<a name="l18041"><span class="ln">18041 </span></a>                [[ 0.3242, -0.9665], 
<a name="l18042"><span class="ln">18042 </span></a>                 [ 0.4539, -0.0887]], 
<a name="l18043"><span class="ln">18043 </span></a> 
<a name="l18044"><span class="ln">18044 </span></a>                [[ 1.1336, -0.4025], 
<a name="l18045"><span class="ln">18045 </span></a>                 [-0.7089,  0.9032]]]) 
<a name="l18046"><span class="ln">18046 </span></a>        &gt;&gt;&gt; A.det() 
<a name="l18047"><span class="ln">18047 </span></a>        tensor([1.1990, 0.4099, 0.7386]) 
<a name="l18048"><span class="ln">18048 </span></a>        &gt;&gt;&gt; A.det().log() 
<a name="l18049"><span class="ln">18049 </span></a>        tensor([ 0.1815, -0.8917, -0.3031]) 
<a name="l18050"><span class="ln">18050 </span></a>    &quot;&quot;&quot;</span>
<a name="l18051"><span class="ln">18051 </span></a>
<a name="l18052"><span class="ln">18052 </span></a><span class="s2">def </span><span class="s1">logical_and</span><span class="s3">(</span>
<a name="l18053"><span class="ln">18053 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18054"><span class="ln">18054 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18055"><span class="ln">18055 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18056"><span class="ln">18056 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18057"><span class="ln">18057 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18058"><span class="ln">18058 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18059"><span class="ln">18059 </span></a>    logical_and(input, other, *, out=None) -&gt; Tensor 
<a name="l18060"><span class="ln">18060 </span></a> 
<a name="l18061"><span class="ln">18061 </span></a>    Computes the element-wise logical AND of the given input tensors. Zeros are treated as ``False`` and nonzeros are 
<a name="l18062"><span class="ln">18062 </span></a>    treated as ``True``. 
<a name="l18063"><span class="ln">18063 </span></a> 
<a name="l18064"><span class="ln">18064 </span></a>    Args: 
<a name="l18065"><span class="ln">18065 </span></a>        input (Tensor): the input tensor. 
<a name="l18066"><span class="ln">18066 </span></a>        other (Tensor): the tensor to compute AND with 
<a name="l18067"><span class="ln">18067 </span></a> 
<a name="l18068"><span class="ln">18068 </span></a>    Keyword args: 
<a name="l18069"><span class="ln">18069 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18070"><span class="ln">18070 </span></a> 
<a name="l18071"><span class="ln">18071 </span></a>    Example:: 
<a name="l18072"><span class="ln">18072 </span></a> 
<a name="l18073"><span class="ln">18073 </span></a>        &gt;&gt;&gt; torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False])) 
<a name="l18074"><span class="ln">18074 </span></a>        tensor([ True, False, False]) 
<a name="l18075"><span class="ln">18075 </span></a>        &gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8) 
<a name="l18076"><span class="ln">18076 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8) 
<a name="l18077"><span class="ln">18077 </span></a>        &gt;&gt;&gt; torch.logical_and(a, b) 
<a name="l18078"><span class="ln">18078 </span></a>        tensor([False, False,  True, False]) 
<a name="l18079"><span class="ln">18079 </span></a>        &gt;&gt;&gt; torch.logical_and(a.double(), b.double()) 
<a name="l18080"><span class="ln">18080 </span></a>        tensor([False, False,  True, False]) 
<a name="l18081"><span class="ln">18081 </span></a>        &gt;&gt;&gt; torch.logical_and(a.double(), b) 
<a name="l18082"><span class="ln">18082 </span></a>        tensor([False, False,  True, False]) 
<a name="l18083"><span class="ln">18083 </span></a>        &gt;&gt;&gt; torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool)) 
<a name="l18084"><span class="ln">18084 </span></a>        tensor([False, False,  True, False]) 
<a name="l18085"><span class="ln">18085 </span></a>    &quot;&quot;&quot;</span>
<a name="l18086"><span class="ln">18086 </span></a>
<a name="l18087"><span class="ln">18087 </span></a><span class="s2">def </span><span class="s1">logical_not</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18088"><span class="ln">18088 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18089"><span class="ln">18089 </span></a>    logical_not(input, *, out=None) -&gt; Tensor 
<a name="l18090"><span class="ln">18090 </span></a> 
<a name="l18091"><span class="ln">18091 </span></a>    Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool 
<a name="l18092"><span class="ln">18092 </span></a>    dtype. If the input tensor is not a bool tensor, zeros are treated as ``False`` and non-zeros are treated as ``True``. 
<a name="l18093"><span class="ln">18093 </span></a> 
<a name="l18094"><span class="ln">18094 </span></a>    Args: 
<a name="l18095"><span class="ln">18095 </span></a>        input (Tensor): the input tensor. 
<a name="l18096"><span class="ln">18096 </span></a> 
<a name="l18097"><span class="ln">18097 </span></a>    Keyword args: 
<a name="l18098"><span class="ln">18098 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18099"><span class="ln">18099 </span></a> 
<a name="l18100"><span class="ln">18100 </span></a>    Example:: 
<a name="l18101"><span class="ln">18101 </span></a> 
<a name="l18102"><span class="ln">18102 </span></a>        &gt;&gt;&gt; torch.logical_not(torch.tensor([True, False])) 
<a name="l18103"><span class="ln">18103 </span></a>        tensor([False,  True]) 
<a name="l18104"><span class="ln">18104 </span></a>        &gt;&gt;&gt; torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8)) 
<a name="l18105"><span class="ln">18105 </span></a>        tensor([ True, False, False]) 
<a name="l18106"><span class="ln">18106 </span></a>        &gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double)) 
<a name="l18107"><span class="ln">18107 </span></a>        tensor([ True, False, False]) 
<a name="l18108"><span class="ln">18108 </span></a>        &gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16)) 
<a name="l18109"><span class="ln">18109 </span></a>        tensor([1, 0, 0], dtype=torch.int16) 
<a name="l18110"><span class="ln">18110 </span></a>    &quot;&quot;&quot;</span>
<a name="l18111"><span class="ln">18111 </span></a>
<a name="l18112"><span class="ln">18112 </span></a><span class="s2">def </span><span class="s1">logical_or</span><span class="s3">(</span>
<a name="l18113"><span class="ln">18113 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18114"><span class="ln">18114 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18115"><span class="ln">18115 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18116"><span class="ln">18116 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18117"><span class="ln">18117 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18118"><span class="ln">18118 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18119"><span class="ln">18119 </span></a>    logical_or(input, other, *, out=None) -&gt; Tensor 
<a name="l18120"><span class="ln">18120 </span></a> 
<a name="l18121"><span class="ln">18121 </span></a>    Computes the element-wise logical OR of the given input tensors. Zeros are treated as ``False`` and nonzeros are 
<a name="l18122"><span class="ln">18122 </span></a>    treated as ``True``. 
<a name="l18123"><span class="ln">18123 </span></a> 
<a name="l18124"><span class="ln">18124 </span></a>    Args: 
<a name="l18125"><span class="ln">18125 </span></a>        input (Tensor): the input tensor. 
<a name="l18126"><span class="ln">18126 </span></a>        other (Tensor): the tensor to compute OR with 
<a name="l18127"><span class="ln">18127 </span></a> 
<a name="l18128"><span class="ln">18128 </span></a>    Keyword args: 
<a name="l18129"><span class="ln">18129 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18130"><span class="ln">18130 </span></a> 
<a name="l18131"><span class="ln">18131 </span></a>    Example:: 
<a name="l18132"><span class="ln">18132 </span></a> 
<a name="l18133"><span class="ln">18133 </span></a>        &gt;&gt;&gt; torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False])) 
<a name="l18134"><span class="ln">18134 </span></a>        tensor([ True, False,  True]) 
<a name="l18135"><span class="ln">18135 </span></a>        &gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8) 
<a name="l18136"><span class="ln">18136 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8) 
<a name="l18137"><span class="ln">18137 </span></a>        &gt;&gt;&gt; torch.logical_or(a, b) 
<a name="l18138"><span class="ln">18138 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l18139"><span class="ln">18139 </span></a>        &gt;&gt;&gt; torch.logical_or(a.double(), b.double()) 
<a name="l18140"><span class="ln">18140 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l18141"><span class="ln">18141 </span></a>        &gt;&gt;&gt; torch.logical_or(a.double(), b) 
<a name="l18142"><span class="ln">18142 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l18143"><span class="ln">18143 </span></a>        &gt;&gt;&gt; torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool)) 
<a name="l18144"><span class="ln">18144 </span></a>        tensor([ True,  True,  True, False]) 
<a name="l18145"><span class="ln">18145 </span></a>    &quot;&quot;&quot;</span>
<a name="l18146"><span class="ln">18146 </span></a>
<a name="l18147"><span class="ln">18147 </span></a><span class="s2">def </span><span class="s1">logical_xor</span><span class="s3">(</span>
<a name="l18148"><span class="ln">18148 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18149"><span class="ln">18149 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18150"><span class="ln">18150 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18151"><span class="ln">18151 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18152"><span class="ln">18152 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18153"><span class="ln">18153 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18154"><span class="ln">18154 </span></a>    logical_xor(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l18155"><span class="ln">18155 </span></a> 
<a name="l18156"><span class="ln">18156 </span></a>    Computes the element-wise logical XOR of the given input tensors. Zeros are treated as ``False`` and nonzeros are 
<a name="l18157"><span class="ln">18157 </span></a>    treated as ``True``. 
<a name="l18158"><span class="ln">18158 </span></a> 
<a name="l18159"><span class="ln">18159 </span></a>    Args: 
<a name="l18160"><span class="ln">18160 </span></a>        input (Tensor): the input tensor. 
<a name="l18161"><span class="ln">18161 </span></a>        other (Tensor): the tensor to compute XOR with 
<a name="l18162"><span class="ln">18162 </span></a> 
<a name="l18163"><span class="ln">18163 </span></a>    Keyword args: 
<a name="l18164"><span class="ln">18164 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18165"><span class="ln">18165 </span></a> 
<a name="l18166"><span class="ln">18166 </span></a>    Example:: 
<a name="l18167"><span class="ln">18167 </span></a> 
<a name="l18168"><span class="ln">18168 </span></a>        &gt;&gt;&gt; torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False])) 
<a name="l18169"><span class="ln">18169 </span></a>        tensor([False, False,  True]) 
<a name="l18170"><span class="ln">18170 </span></a>        &gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8) 
<a name="l18171"><span class="ln">18171 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8) 
<a name="l18172"><span class="ln">18172 </span></a>        &gt;&gt;&gt; torch.logical_xor(a, b) 
<a name="l18173"><span class="ln">18173 </span></a>        tensor([ True,  True, False, False]) 
<a name="l18174"><span class="ln">18174 </span></a>        &gt;&gt;&gt; torch.logical_xor(a.double(), b.double()) 
<a name="l18175"><span class="ln">18175 </span></a>        tensor([ True,  True, False, False]) 
<a name="l18176"><span class="ln">18176 </span></a>        &gt;&gt;&gt; torch.logical_xor(a.double(), b) 
<a name="l18177"><span class="ln">18177 </span></a>        tensor([ True,  True, False, False]) 
<a name="l18178"><span class="ln">18178 </span></a>        &gt;&gt;&gt; torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool)) 
<a name="l18179"><span class="ln">18179 </span></a>        tensor([ True,  True, False, False]) 
<a name="l18180"><span class="ln">18180 </span></a>    &quot;&quot;&quot;</span>
<a name="l18181"><span class="ln">18181 </span></a>
<a name="l18182"><span class="ln">18182 </span></a><span class="s2">def </span><span class="s1">logit</span><span class="s3">(</span>
<a name="l18183"><span class="ln">18183 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18184"><span class="ln">18184 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18185"><span class="ln">18185 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18186"><span class="ln">18186 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18187"><span class="ln">18187 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18188"><span class="ln">18188 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18189"><span class="ln">18189 </span></a>    logit(input, eps=None, *, out=None) -&gt; Tensor 
<a name="l18190"><span class="ln">18190 </span></a> 
<a name="l18191"><span class="ln">18191 </span></a>    Alias for :func:`torch.special.logit`. 
<a name="l18192"><span class="ln">18192 </span></a>    &quot;&quot;&quot;</span>
<a name="l18193"><span class="ln">18193 </span></a>
<a name="l18194"><span class="ln">18194 </span></a><span class="s2">def </span><span class="s1">logit_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18195"><span class="ln">18195 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18196"><span class="ln">18196 </span></a><span class="s2">def </span><span class="s1">logspace</span><span class="s3">(</span>
<a name="l18197"><span class="ln">18197 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l18198"><span class="ln">18198 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l18199"><span class="ln">18199 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18200"><span class="ln">18200 </span></a>    <span class="s1">base</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">10.0</span><span class="s3">,</span>
<a name="l18201"><span class="ln">18201 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18202"><span class="ln">18202 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18203"><span class="ln">18203 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18204"><span class="ln">18204 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18205"><span class="ln">18205 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l18206"><span class="ln">18206 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l18207"><span class="ln">18207 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18208"><span class="ln">18208 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18209"><span class="ln">18209 </span></a>    logspace(start, end, steps, base=10.0, *,          out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l18210"><span class="ln">18210 </span></a> 
<a name="l18211"><span class="ln">18211 </span></a> 
<a name="l18212"><span class="ln">18212 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l18213"><span class="ln">18213 </span></a>    spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to 
<a name="l18214"><span class="ln">18214 </span></a>    :math:`{{\text{{base}}}}^{{\text{{end}}}}`, inclusive, on a logarithmic scale 
<a name="l18215"><span class="ln">18215 </span></a>    with base :attr:`base`. That is, the values are: 
<a name="l18216"><span class="ln">18216 </span></a> 
<a name="l18217"><span class="ln">18217 </span></a>    .. math:: 
<a name="l18218"><span class="ln">18218 </span></a>        (\text{base}^{\text{start}}, 
<a name="l18219"><span class="ln">18219 </span></a>        \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18220"><span class="ln">18220 </span></a>        \ldots, 
<a name="l18221"><span class="ln">18221 </span></a>        \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18222"><span class="ln">18222 </span></a>        \text{base}^{\text{end}}) 
<a name="l18223"><span class="ln">18223 </span></a> 
<a name="l18224"><span class="ln">18224 </span></a> 
<a name="l18225"><span class="ln">18225 </span></a> 
<a name="l18226"><span class="ln">18226 </span></a>    From PyTorch 1.11 logspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l18227"><span class="ln">18227 </span></a> 
<a name="l18228"><span class="ln">18228 </span></a>    Args: 
<a name="l18229"><span class="ln">18229 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18230"><span class="ln">18230 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18231"><span class="ln">18231 </span></a>        steps (int): size of the constructed tensor 
<a name="l18232"><span class="ln">18232 </span></a>        base (float, optional): base of the logarithm function. Default: ``10.0``. 
<a name="l18233"><span class="ln">18233 </span></a> 
<a name="l18234"><span class="ln">18234 </span></a>    Keyword arguments: 
<a name="l18235"><span class="ln">18235 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18236"><span class="ln">18236 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l18237"><span class="ln">18237 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l18238"><span class="ln">18238 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l18239"><span class="ln">18239 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l18240"><span class="ln">18240 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l18241"><span class="ln">18241 </span></a>            Default: ``torch.strided``. 
<a name="l18242"><span class="ln">18242 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l18243"><span class="ln">18243 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l18244"><span class="ln">18244 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l18245"><span class="ln">18245 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l18246"><span class="ln">18246 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l18247"><span class="ln">18247 </span></a>            returned tensor. Default: ``False``. 
<a name="l18248"><span class="ln">18248 </span></a> 
<a name="l18249"><span class="ln">18249 </span></a>    Example:: 
<a name="l18250"><span class="ln">18250 </span></a> 
<a name="l18251"><span class="ln">18251 </span></a>        &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 
<a name="l18252"><span class="ln">18252 </span></a>        tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) 
<a name="l18253"><span class="ln">18253 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 
<a name="l18254"><span class="ln">18254 </span></a>        tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000]) 
<a name="l18255"><span class="ln">18255 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1) 
<a name="l18256"><span class="ln">18256 </span></a>        tensor([1.2589]) 
<a name="l18257"><span class="ln">18257 </span></a>        &gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2) 
<a name="l18258"><span class="ln">18258 </span></a>        tensor([4.0]) 
<a name="l18259"><span class="ln">18259 </span></a>    &quot;&quot;&quot;</span>
<a name="l18260"><span class="ln">18260 </span></a>
<a name="l18261"><span class="ln">18261 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18262"><span class="ln">18262 </span></a><span class="s2">def </span><span class="s1">logspace</span><span class="s3">(</span>
<a name="l18263"><span class="ln">18263 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18264"><span class="ln">18264 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18265"><span class="ln">18265 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l18266"><span class="ln">18266 </span></a>    <span class="s1">base</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">10.0</span><span class="s3">,</span>
<a name="l18267"><span class="ln">18267 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18268"><span class="ln">18268 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18269"><span class="ln">18269 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18270"><span class="ln">18270 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18271"><span class="ln">18271 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18272"><span class="ln">18272 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18273"><span class="ln">18273 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18274"><span class="ln">18274 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18275"><span class="ln">18275 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18276"><span class="ln">18276 </span></a>    logspace(start, end, steps, base=10.0, *,          out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l18277"><span class="ln">18277 </span></a> 
<a name="l18278"><span class="ln">18278 </span></a> 
<a name="l18279"><span class="ln">18279 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l18280"><span class="ln">18280 </span></a>    spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to 
<a name="l18281"><span class="ln">18281 </span></a>    :math:`{{\text{{base}}}}^{{\text{{end}}}}`, inclusive, on a logarithmic scale 
<a name="l18282"><span class="ln">18282 </span></a>    with base :attr:`base`. That is, the values are: 
<a name="l18283"><span class="ln">18283 </span></a> 
<a name="l18284"><span class="ln">18284 </span></a>    .. math:: 
<a name="l18285"><span class="ln">18285 </span></a>        (\text{base}^{\text{start}}, 
<a name="l18286"><span class="ln">18286 </span></a>        \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18287"><span class="ln">18287 </span></a>        \ldots, 
<a name="l18288"><span class="ln">18288 </span></a>        \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18289"><span class="ln">18289 </span></a>        \text{base}^{\text{end}}) 
<a name="l18290"><span class="ln">18290 </span></a> 
<a name="l18291"><span class="ln">18291 </span></a> 
<a name="l18292"><span class="ln">18292 </span></a> 
<a name="l18293"><span class="ln">18293 </span></a>    From PyTorch 1.11 logspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l18294"><span class="ln">18294 </span></a> 
<a name="l18295"><span class="ln">18295 </span></a>    Args: 
<a name="l18296"><span class="ln">18296 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18297"><span class="ln">18297 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18298"><span class="ln">18298 </span></a>        steps (int): size of the constructed tensor 
<a name="l18299"><span class="ln">18299 </span></a>        base (float, optional): base of the logarithm function. Default: ``10.0``. 
<a name="l18300"><span class="ln">18300 </span></a> 
<a name="l18301"><span class="ln">18301 </span></a>    Keyword arguments: 
<a name="l18302"><span class="ln">18302 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18303"><span class="ln">18303 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l18304"><span class="ln">18304 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l18305"><span class="ln">18305 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l18306"><span class="ln">18306 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l18307"><span class="ln">18307 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l18308"><span class="ln">18308 </span></a>            Default: ``torch.strided``. 
<a name="l18309"><span class="ln">18309 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l18310"><span class="ln">18310 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l18311"><span class="ln">18311 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l18312"><span class="ln">18312 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l18313"><span class="ln">18313 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l18314"><span class="ln">18314 </span></a>            returned tensor. Default: ``False``. 
<a name="l18315"><span class="ln">18315 </span></a> 
<a name="l18316"><span class="ln">18316 </span></a>    Example:: 
<a name="l18317"><span class="ln">18317 </span></a> 
<a name="l18318"><span class="ln">18318 </span></a>        &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 
<a name="l18319"><span class="ln">18319 </span></a>        tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) 
<a name="l18320"><span class="ln">18320 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 
<a name="l18321"><span class="ln">18321 </span></a>        tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000]) 
<a name="l18322"><span class="ln">18322 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1) 
<a name="l18323"><span class="ln">18323 </span></a>        tensor([1.2589]) 
<a name="l18324"><span class="ln">18324 </span></a>        &gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2) 
<a name="l18325"><span class="ln">18325 </span></a>        tensor([4.0]) 
<a name="l18326"><span class="ln">18326 </span></a>    &quot;&quot;&quot;</span>
<a name="l18327"><span class="ln">18327 </span></a>
<a name="l18328"><span class="ln">18328 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18329"><span class="ln">18329 </span></a><span class="s2">def </span><span class="s1">logspace</span><span class="s3">(</span>
<a name="l18330"><span class="ln">18330 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l18331"><span class="ln">18331 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18332"><span class="ln">18332 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l18333"><span class="ln">18333 </span></a>    <span class="s1">base</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">10.0</span><span class="s3">,</span>
<a name="l18334"><span class="ln">18334 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18335"><span class="ln">18335 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18336"><span class="ln">18336 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18337"><span class="ln">18337 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18338"><span class="ln">18338 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18339"><span class="ln">18339 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18340"><span class="ln">18340 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18341"><span class="ln">18341 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18342"><span class="ln">18342 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18343"><span class="ln">18343 </span></a>    logspace(start, end, steps, base=10.0, *,          out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l18344"><span class="ln">18344 </span></a> 
<a name="l18345"><span class="ln">18345 </span></a> 
<a name="l18346"><span class="ln">18346 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l18347"><span class="ln">18347 </span></a>    spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to 
<a name="l18348"><span class="ln">18348 </span></a>    :math:`{{\text{{base}}}}^{{\text{{end}}}}`, inclusive, on a logarithmic scale 
<a name="l18349"><span class="ln">18349 </span></a>    with base :attr:`base`. That is, the values are: 
<a name="l18350"><span class="ln">18350 </span></a> 
<a name="l18351"><span class="ln">18351 </span></a>    .. math:: 
<a name="l18352"><span class="ln">18352 </span></a>        (\text{base}^{\text{start}}, 
<a name="l18353"><span class="ln">18353 </span></a>        \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18354"><span class="ln">18354 </span></a>        \ldots, 
<a name="l18355"><span class="ln">18355 </span></a>        \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18356"><span class="ln">18356 </span></a>        \text{base}^{\text{end}}) 
<a name="l18357"><span class="ln">18357 </span></a> 
<a name="l18358"><span class="ln">18358 </span></a> 
<a name="l18359"><span class="ln">18359 </span></a> 
<a name="l18360"><span class="ln">18360 </span></a>    From PyTorch 1.11 logspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l18361"><span class="ln">18361 </span></a> 
<a name="l18362"><span class="ln">18362 </span></a>    Args: 
<a name="l18363"><span class="ln">18363 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18364"><span class="ln">18364 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18365"><span class="ln">18365 </span></a>        steps (int): size of the constructed tensor 
<a name="l18366"><span class="ln">18366 </span></a>        base (float, optional): base of the logarithm function. Default: ``10.0``. 
<a name="l18367"><span class="ln">18367 </span></a> 
<a name="l18368"><span class="ln">18368 </span></a>    Keyword arguments: 
<a name="l18369"><span class="ln">18369 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18370"><span class="ln">18370 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l18371"><span class="ln">18371 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l18372"><span class="ln">18372 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l18373"><span class="ln">18373 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l18374"><span class="ln">18374 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l18375"><span class="ln">18375 </span></a>            Default: ``torch.strided``. 
<a name="l18376"><span class="ln">18376 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l18377"><span class="ln">18377 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l18378"><span class="ln">18378 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l18379"><span class="ln">18379 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l18380"><span class="ln">18380 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l18381"><span class="ln">18381 </span></a>            returned tensor. Default: ``False``. 
<a name="l18382"><span class="ln">18382 </span></a> 
<a name="l18383"><span class="ln">18383 </span></a>    Example:: 
<a name="l18384"><span class="ln">18384 </span></a> 
<a name="l18385"><span class="ln">18385 </span></a>        &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 
<a name="l18386"><span class="ln">18386 </span></a>        tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) 
<a name="l18387"><span class="ln">18387 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 
<a name="l18388"><span class="ln">18388 </span></a>        tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000]) 
<a name="l18389"><span class="ln">18389 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1) 
<a name="l18390"><span class="ln">18390 </span></a>        tensor([1.2589]) 
<a name="l18391"><span class="ln">18391 </span></a>        &gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2) 
<a name="l18392"><span class="ln">18392 </span></a>        tensor([4.0]) 
<a name="l18393"><span class="ln">18393 </span></a>    &quot;&quot;&quot;</span>
<a name="l18394"><span class="ln">18394 </span></a>
<a name="l18395"><span class="ln">18395 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18396"><span class="ln">18396 </span></a><span class="s2">def </span><span class="s1">logspace</span><span class="s3">(</span>
<a name="l18397"><span class="ln">18397 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18398"><span class="ln">18398 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l18399"><span class="ln">18399 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l18400"><span class="ln">18400 </span></a>    <span class="s1">base</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">10.0</span><span class="s3">,</span>
<a name="l18401"><span class="ln">18401 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18402"><span class="ln">18402 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18403"><span class="ln">18403 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18404"><span class="ln">18404 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18405"><span class="ln">18405 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18406"><span class="ln">18406 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18407"><span class="ln">18407 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18408"><span class="ln">18408 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18409"><span class="ln">18409 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18410"><span class="ln">18410 </span></a>    logspace(start, end, steps, base=10.0, *,          out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l18411"><span class="ln">18411 </span></a> 
<a name="l18412"><span class="ln">18412 </span></a> 
<a name="l18413"><span class="ln">18413 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l18414"><span class="ln">18414 </span></a>    spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to 
<a name="l18415"><span class="ln">18415 </span></a>    :math:`{{\text{{base}}}}^{{\text{{end}}}}`, inclusive, on a logarithmic scale 
<a name="l18416"><span class="ln">18416 </span></a>    with base :attr:`base`. That is, the values are: 
<a name="l18417"><span class="ln">18417 </span></a> 
<a name="l18418"><span class="ln">18418 </span></a>    .. math:: 
<a name="l18419"><span class="ln">18419 </span></a>        (\text{base}^{\text{start}}, 
<a name="l18420"><span class="ln">18420 </span></a>        \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18421"><span class="ln">18421 </span></a>        \ldots, 
<a name="l18422"><span class="ln">18422 </span></a>        \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18423"><span class="ln">18423 </span></a>        \text{base}^{\text{end}}) 
<a name="l18424"><span class="ln">18424 </span></a> 
<a name="l18425"><span class="ln">18425 </span></a> 
<a name="l18426"><span class="ln">18426 </span></a> 
<a name="l18427"><span class="ln">18427 </span></a>    From PyTorch 1.11 logspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l18428"><span class="ln">18428 </span></a> 
<a name="l18429"><span class="ln">18429 </span></a>    Args: 
<a name="l18430"><span class="ln">18430 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18431"><span class="ln">18431 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18432"><span class="ln">18432 </span></a>        steps (int): size of the constructed tensor 
<a name="l18433"><span class="ln">18433 </span></a>        base (float, optional): base of the logarithm function. Default: ``10.0``. 
<a name="l18434"><span class="ln">18434 </span></a> 
<a name="l18435"><span class="ln">18435 </span></a>    Keyword arguments: 
<a name="l18436"><span class="ln">18436 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18437"><span class="ln">18437 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l18438"><span class="ln">18438 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l18439"><span class="ln">18439 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l18440"><span class="ln">18440 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l18441"><span class="ln">18441 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l18442"><span class="ln">18442 </span></a>            Default: ``torch.strided``. 
<a name="l18443"><span class="ln">18443 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l18444"><span class="ln">18444 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l18445"><span class="ln">18445 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l18446"><span class="ln">18446 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l18447"><span class="ln">18447 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l18448"><span class="ln">18448 </span></a>            returned tensor. Default: ``False``. 
<a name="l18449"><span class="ln">18449 </span></a> 
<a name="l18450"><span class="ln">18450 </span></a>    Example:: 
<a name="l18451"><span class="ln">18451 </span></a> 
<a name="l18452"><span class="ln">18452 </span></a>        &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 
<a name="l18453"><span class="ln">18453 </span></a>        tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) 
<a name="l18454"><span class="ln">18454 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 
<a name="l18455"><span class="ln">18455 </span></a>        tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000]) 
<a name="l18456"><span class="ln">18456 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1) 
<a name="l18457"><span class="ln">18457 </span></a>        tensor([1.2589]) 
<a name="l18458"><span class="ln">18458 </span></a>        &gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2) 
<a name="l18459"><span class="ln">18459 </span></a>        tensor([4.0]) 
<a name="l18460"><span class="ln">18460 </span></a>    &quot;&quot;&quot;</span>
<a name="l18461"><span class="ln">18461 </span></a>
<a name="l18462"><span class="ln">18462 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18463"><span class="ln">18463 </span></a><span class="s2">def </span><span class="s1">logspace</span><span class="s3">(</span>
<a name="l18464"><span class="ln">18464 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l18465"><span class="ln">18465 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l18466"><span class="ln">18466 </span></a>    <span class="s1">steps</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l18467"><span class="ln">18467 </span></a>    <span class="s1">base</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">10.0</span><span class="s3">,</span>
<a name="l18468"><span class="ln">18468 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18469"><span class="ln">18469 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18470"><span class="ln">18470 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18471"><span class="ln">18471 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18472"><span class="ln">18472 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18473"><span class="ln">18473 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18474"><span class="ln">18474 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l18475"><span class="ln">18475 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18476"><span class="ln">18476 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18477"><span class="ln">18477 </span></a>    logspace(start, end, steps, base=10.0, *,          out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l18478"><span class="ln">18478 </span></a> 
<a name="l18479"><span class="ln">18479 </span></a> 
<a name="l18480"><span class="ln">18480 </span></a>    Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly 
<a name="l18481"><span class="ln">18481 </span></a>    spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to 
<a name="l18482"><span class="ln">18482 </span></a>    :math:`{{\text{{base}}}}^{{\text{{end}}}}`, inclusive, on a logarithmic scale 
<a name="l18483"><span class="ln">18483 </span></a>    with base :attr:`base`. That is, the values are: 
<a name="l18484"><span class="ln">18484 </span></a> 
<a name="l18485"><span class="ln">18485 </span></a>    .. math:: 
<a name="l18486"><span class="ln">18486 </span></a>        (\text{base}^{\text{start}}, 
<a name="l18487"><span class="ln">18487 </span></a>        \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18488"><span class="ln">18488 </span></a>        \ldots, 
<a name="l18489"><span class="ln">18489 </span></a>        \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, 
<a name="l18490"><span class="ln">18490 </span></a>        \text{base}^{\text{end}}) 
<a name="l18491"><span class="ln">18491 </span></a> 
<a name="l18492"><span class="ln">18492 </span></a> 
<a name="l18493"><span class="ln">18493 </span></a> 
<a name="l18494"><span class="ln">18494 </span></a>    From PyTorch 1.11 logspace requires the steps argument. Use steps=100 to restore the previous behavior. 
<a name="l18495"><span class="ln">18495 </span></a> 
<a name="l18496"><span class="ln">18496 </span></a>    Args: 
<a name="l18497"><span class="ln">18497 </span></a>        start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18498"><span class="ln">18498 </span></a>        end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional 
<a name="l18499"><span class="ln">18499 </span></a>        steps (int): size of the constructed tensor 
<a name="l18500"><span class="ln">18500 </span></a>        base (float, optional): base of the logarithm function. Default: ``10.0``. 
<a name="l18501"><span class="ln">18501 </span></a> 
<a name="l18502"><span class="ln">18502 </span></a>    Keyword arguments: 
<a name="l18503"><span class="ln">18503 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18504"><span class="ln">18504 </span></a>        dtype (torch.dtype, optional): the data type to perform the computation in. 
<a name="l18505"><span class="ln">18505 </span></a>            Default: if None, uses the global default dtype (see torch.get_default_dtype()) 
<a name="l18506"><span class="ln">18506 </span></a>            when both :attr:`start` and :attr:`end` are real, 
<a name="l18507"><span class="ln">18507 </span></a>            and corresponding complex dtype when either is complex. 
<a name="l18508"><span class="ln">18508 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l18509"><span class="ln">18509 </span></a>            Default: ``torch.strided``. 
<a name="l18510"><span class="ln">18510 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l18511"><span class="ln">18511 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l18512"><span class="ln">18512 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l18513"><span class="ln">18513 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l18514"><span class="ln">18514 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l18515"><span class="ln">18515 </span></a>            returned tensor. Default: ``False``. 
<a name="l18516"><span class="ln">18516 </span></a> 
<a name="l18517"><span class="ln">18517 </span></a>    Example:: 
<a name="l18518"><span class="ln">18518 </span></a> 
<a name="l18519"><span class="ln">18519 </span></a>        &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 
<a name="l18520"><span class="ln">18520 </span></a>        tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) 
<a name="l18521"><span class="ln">18521 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 
<a name="l18522"><span class="ln">18522 </span></a>        tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000]) 
<a name="l18523"><span class="ln">18523 </span></a>        &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1) 
<a name="l18524"><span class="ln">18524 </span></a>        tensor([1.2589]) 
<a name="l18525"><span class="ln">18525 </span></a>        &gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2) 
<a name="l18526"><span class="ln">18526 </span></a>        tensor([4.0]) 
<a name="l18527"><span class="ln">18527 </span></a>    &quot;&quot;&quot;</span>
<a name="l18528"><span class="ln">18528 </span></a>
<a name="l18529"><span class="ln">18529 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18530"><span class="ln">18530 </span></a><span class="s2">def </span><span class="s1">logsumexp</span><span class="s3">(</span>
<a name="l18531"><span class="ln">18531 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18532"><span class="ln">18532 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l18533"><span class="ln">18533 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l18534"><span class="ln">18534 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18535"><span class="ln">18535 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18536"><span class="ln">18536 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18537"><span class="ln">18537 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18538"><span class="ln">18538 </span></a>    logsumexp(input, dim, keepdim=False, *, out=None) 
<a name="l18539"><span class="ln">18539 </span></a> 
<a name="l18540"><span class="ln">18540 </span></a>    Returns the log of summed exponentials of each row of the :attr:`input` 
<a name="l18541"><span class="ln">18541 </span></a>    tensor in the given dimension :attr:`dim`. The computation is numerically 
<a name="l18542"><span class="ln">18542 </span></a>    stabilized. 
<a name="l18543"><span class="ln">18543 </span></a> 
<a name="l18544"><span class="ln">18544 </span></a>    For summation index :math:`j` given by `dim` and other indices :math:`i`, the result is 
<a name="l18545"><span class="ln">18545 </span></a> 
<a name="l18546"><span class="ln">18546 </span></a>        .. math:: 
<a name="l18547"><span class="ln">18547 </span></a>            \text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij}) 
<a name="l18548"><span class="ln">18548 </span></a> 
<a name="l18549"><span class="ln">18549 </span></a> 
<a name="l18550"><span class="ln">18550 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l18551"><span class="ln">18551 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l18552"><span class="ln">18552 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l18553"><span class="ln">18553 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l18554"><span class="ln">18554 </span></a> 
<a name="l18555"><span class="ln">18555 </span></a> 
<a name="l18556"><span class="ln">18556 </span></a>    Args: 
<a name="l18557"><span class="ln">18557 </span></a>        input (Tensor): the input tensor. 
<a name="l18558"><span class="ln">18558 </span></a>        dim (int or tuple of ints): the dimension or dimensions to reduce. 
<a name="l18559"><span class="ln">18559 </span></a> 
<a name="l18560"><span class="ln">18560 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l18561"><span class="ln">18561 </span></a> 
<a name="l18562"><span class="ln">18562 </span></a> 
<a name="l18563"><span class="ln">18563 </span></a>    Keyword args: 
<a name="l18564"><span class="ln">18564 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18565"><span class="ln">18565 </span></a> 
<a name="l18566"><span class="ln">18566 </span></a>    Example:: 
<a name="l18567"><span class="ln">18567 </span></a> 
<a name="l18568"><span class="ln">18568 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l18569"><span class="ln">18569 </span></a>        &gt;&gt;&gt; torch.logsumexp(a, 1) 
<a name="l18570"><span class="ln">18570 </span></a>        tensor([1.4907, 1.0593, 1.5696]) 
<a name="l18571"><span class="ln">18571 </span></a>        &gt;&gt;&gt; torch.dist(torch.logsumexp(a, 1), torch.log(torch.sum(torch.exp(a), 1))) 
<a name="l18572"><span class="ln">18572 </span></a>        tensor(1.6859e-07) 
<a name="l18573"><span class="ln">18573 </span></a>    &quot;&quot;&quot;</span>
<a name="l18574"><span class="ln">18574 </span></a>
<a name="l18575"><span class="ln">18575 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18576"><span class="ln">18576 </span></a><span class="s2">def </span><span class="s1">logsumexp</span><span class="s3">(</span>
<a name="l18577"><span class="ln">18577 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18578"><span class="ln">18578 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l18579"><span class="ln">18579 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l18580"><span class="ln">18580 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18581"><span class="ln">18581 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18582"><span class="ln">18582 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18583"><span class="ln">18583 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18584"><span class="ln">18584 </span></a>    logsumexp(input, dim, keepdim=False, *, out=None) 
<a name="l18585"><span class="ln">18585 </span></a> 
<a name="l18586"><span class="ln">18586 </span></a>    Returns the log of summed exponentials of each row of the :attr:`input` 
<a name="l18587"><span class="ln">18587 </span></a>    tensor in the given dimension :attr:`dim`. The computation is numerically 
<a name="l18588"><span class="ln">18588 </span></a>    stabilized. 
<a name="l18589"><span class="ln">18589 </span></a> 
<a name="l18590"><span class="ln">18590 </span></a>    For summation index :math:`j` given by `dim` and other indices :math:`i`, the result is 
<a name="l18591"><span class="ln">18591 </span></a> 
<a name="l18592"><span class="ln">18592 </span></a>        .. math:: 
<a name="l18593"><span class="ln">18593 </span></a>            \text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij}) 
<a name="l18594"><span class="ln">18594 </span></a> 
<a name="l18595"><span class="ln">18595 </span></a> 
<a name="l18596"><span class="ln">18596 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l18597"><span class="ln">18597 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l18598"><span class="ln">18598 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l18599"><span class="ln">18599 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l18600"><span class="ln">18600 </span></a> 
<a name="l18601"><span class="ln">18601 </span></a> 
<a name="l18602"><span class="ln">18602 </span></a>    Args: 
<a name="l18603"><span class="ln">18603 </span></a>        input (Tensor): the input tensor. 
<a name="l18604"><span class="ln">18604 </span></a>        dim (int or tuple of ints): the dimension or dimensions to reduce. 
<a name="l18605"><span class="ln">18605 </span></a> 
<a name="l18606"><span class="ln">18606 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l18607"><span class="ln">18607 </span></a> 
<a name="l18608"><span class="ln">18608 </span></a> 
<a name="l18609"><span class="ln">18609 </span></a>    Keyword args: 
<a name="l18610"><span class="ln">18610 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18611"><span class="ln">18611 </span></a> 
<a name="l18612"><span class="ln">18612 </span></a>    Example:: 
<a name="l18613"><span class="ln">18613 </span></a> 
<a name="l18614"><span class="ln">18614 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l18615"><span class="ln">18615 </span></a>        &gt;&gt;&gt; torch.logsumexp(a, 1) 
<a name="l18616"><span class="ln">18616 </span></a>        tensor([1.4907, 1.0593, 1.5696]) 
<a name="l18617"><span class="ln">18617 </span></a>        &gt;&gt;&gt; torch.dist(torch.logsumexp(a, 1), torch.log(torch.sum(torch.exp(a), 1))) 
<a name="l18618"><span class="ln">18618 </span></a>        tensor(1.6859e-07) 
<a name="l18619"><span class="ln">18619 </span></a>    &quot;&quot;&quot;</span>
<a name="l18620"><span class="ln">18620 </span></a>
<a name="l18621"><span class="ln">18621 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18622"><span class="ln">18622 </span></a><span class="s2">def </span><span class="s1">lstm</span><span class="s3">(</span>
<a name="l18623"><span class="ln">18623 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18624"><span class="ln">18624 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18625"><span class="ln">18625 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l18626"><span class="ln">18626 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l18627"><span class="ln">18627 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l18628"><span class="ln">18628 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l18629"><span class="ln">18629 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l18630"><span class="ln">18630 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l18631"><span class="ln">18631 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l18632"><span class="ln">18632 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18633"><span class="ln">18633 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18634"><span class="ln">18634 </span></a><span class="s2">def </span><span class="s1">lstm</span><span class="s3">(</span>
<a name="l18635"><span class="ln">18635 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18636"><span class="ln">18636 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l18637"><span class="ln">18637 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l18638"><span class="ln">18638 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l18639"><span class="ln">18639 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l18640"><span class="ln">18640 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l18641"><span class="ln">18641 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l18642"><span class="ln">18642 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l18643"><span class="ln">18643 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l18644"><span class="ln">18644 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18645"><span class="ln">18645 </span></a><span class="s2">def </span><span class="s1">lstm_cell</span><span class="s3">(</span>
<a name="l18646"><span class="ln">18646 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18647"><span class="ln">18647 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l18648"><span class="ln">18648 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18649"><span class="ln">18649 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18650"><span class="ln">18650 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18651"><span class="ln">18651 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18652"><span class="ln">18652 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18653"><span class="ln">18653 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18654"><span class="ln">18654 </span></a><span class="s2">def </span><span class="s1">lt</span><span class="s3">(</span>
<a name="l18655"><span class="ln">18655 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18656"><span class="ln">18656 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18657"><span class="ln">18657 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18658"><span class="ln">18658 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18659"><span class="ln">18659 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18660"><span class="ln">18660 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18661"><span class="ln">18661 </span></a>    lt(input, other, *, out=None) -&gt; Tensor 
<a name="l18662"><span class="ln">18662 </span></a> 
<a name="l18663"><span class="ln">18663 </span></a>    Computes :math:`\text{input} &lt; \text{other}` element-wise. 
<a name="l18664"><span class="ln">18664 </span></a> 
<a name="l18665"><span class="ln">18665 </span></a> 
<a name="l18666"><span class="ln">18666 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l18667"><span class="ln">18667 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l18668"><span class="ln">18668 </span></a> 
<a name="l18669"><span class="ln">18669 </span></a>    Args: 
<a name="l18670"><span class="ln">18670 </span></a>        input (Tensor): the tensor to compare 
<a name="l18671"><span class="ln">18671 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l18672"><span class="ln">18672 </span></a> 
<a name="l18673"><span class="ln">18673 </span></a>    Keyword args: 
<a name="l18674"><span class="ln">18674 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18675"><span class="ln">18675 </span></a> 
<a name="l18676"><span class="ln">18676 </span></a>    Returns: 
<a name="l18677"><span class="ln">18677 </span></a>        A boolean tensor that is True where :attr:`input` is less than :attr:`other` and False elsewhere 
<a name="l18678"><span class="ln">18678 </span></a> 
<a name="l18679"><span class="ln">18679 </span></a>    Example:: 
<a name="l18680"><span class="ln">18680 </span></a> 
<a name="l18681"><span class="ln">18681 </span></a>        &gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l18682"><span class="ln">18682 </span></a>        tensor([[False, False], [True, False]]) 
<a name="l18683"><span class="ln">18683 </span></a>    &quot;&quot;&quot;</span>
<a name="l18684"><span class="ln">18684 </span></a>
<a name="l18685"><span class="ln">18685 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18686"><span class="ln">18686 </span></a><span class="s2">def </span><span class="s1">lt</span><span class="s3">(</span>
<a name="l18687"><span class="ln">18687 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18688"><span class="ln">18688 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l18689"><span class="ln">18689 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18690"><span class="ln">18690 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18691"><span class="ln">18691 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18692"><span class="ln">18692 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18693"><span class="ln">18693 </span></a>    lt(input, other, *, out=None) -&gt; Tensor 
<a name="l18694"><span class="ln">18694 </span></a> 
<a name="l18695"><span class="ln">18695 </span></a>    Computes :math:`\text{input} &lt; \text{other}` element-wise. 
<a name="l18696"><span class="ln">18696 </span></a> 
<a name="l18697"><span class="ln">18697 </span></a> 
<a name="l18698"><span class="ln">18698 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l18699"><span class="ln">18699 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l18700"><span class="ln">18700 </span></a> 
<a name="l18701"><span class="ln">18701 </span></a>    Args: 
<a name="l18702"><span class="ln">18702 </span></a>        input (Tensor): the tensor to compare 
<a name="l18703"><span class="ln">18703 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l18704"><span class="ln">18704 </span></a> 
<a name="l18705"><span class="ln">18705 </span></a>    Keyword args: 
<a name="l18706"><span class="ln">18706 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18707"><span class="ln">18707 </span></a> 
<a name="l18708"><span class="ln">18708 </span></a>    Returns: 
<a name="l18709"><span class="ln">18709 </span></a>        A boolean tensor that is True where :attr:`input` is less than :attr:`other` and False elsewhere 
<a name="l18710"><span class="ln">18710 </span></a> 
<a name="l18711"><span class="ln">18711 </span></a>    Example:: 
<a name="l18712"><span class="ln">18712 </span></a> 
<a name="l18713"><span class="ln">18713 </span></a>        &gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l18714"><span class="ln">18714 </span></a>        tensor([[False, False], [True, False]]) 
<a name="l18715"><span class="ln">18715 </span></a>    &quot;&quot;&quot;</span>
<a name="l18716"><span class="ln">18716 </span></a>
<a name="l18717"><span class="ln">18717 </span></a><span class="s2">def </span><span class="s1">lu_solve</span><span class="s3">(</span>
<a name="l18718"><span class="ln">18718 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18719"><span class="ln">18719 </span></a>    <span class="s1">LU_data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18720"><span class="ln">18720 </span></a>    <span class="s1">LU_pivots</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18721"><span class="ln">18721 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18722"><span class="ln">18722 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18723"><span class="ln">18723 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18724"><span class="ln">18724 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18725"><span class="ln">18725 </span></a>    lu_solve(b, LU_data, LU_pivots, *, out=None) -&gt; Tensor 
<a name="l18726"><span class="ln">18726 </span></a> 
<a name="l18727"><span class="ln">18727 </span></a>    Returns the LU solve of the linear system :math:`Ax = b` using the partially pivoted 
<a name="l18728"><span class="ln">18728 </span></a>    LU factorization of A from :func:`~linalg.lu_factor`. 
<a name="l18729"><span class="ln">18729 </span></a> 
<a name="l18730"><span class="ln">18730 </span></a>    This function supports ``float``, ``double``, ``cfloat`` and ``cdouble`` dtypes for :attr:`input`. 
<a name="l18731"><span class="ln">18731 </span></a> 
<a name="l18732"><span class="ln">18732 </span></a>    .. warning:: 
<a name="l18733"><span class="ln">18733 </span></a> 
<a name="l18734"><span class="ln">18734 </span></a>        :func:`torch.lu_solve` is deprecated in favor of :func:`torch.linalg.lu_solve`. 
<a name="l18735"><span class="ln">18735 </span></a>        :func:`torch.lu_solve` will be removed in a future PyTorch release. 
<a name="l18736"><span class="ln">18736 </span></a>        ``X = torch.lu_solve(B, LU, pivots)`` should be replaced with 
<a name="l18737"><span class="ln">18737 </span></a> 
<a name="l18738"><span class="ln">18738 </span></a>        .. code:: python 
<a name="l18739"><span class="ln">18739 </span></a> 
<a name="l18740"><span class="ln">18740 </span></a>            X = linalg.lu_solve(LU, pivots, B) 
<a name="l18741"><span class="ln">18741 </span></a> 
<a name="l18742"><span class="ln">18742 </span></a>    Arguments: 
<a name="l18743"><span class="ln">18743 </span></a>        b (Tensor): the RHS tensor of size :math:`(*, m, k)`, where :math:`*` 
<a name="l18744"><span class="ln">18744 </span></a>                    is zero or more batch dimensions. 
<a name="l18745"><span class="ln">18745 </span></a>        LU_data (Tensor): the pivoted LU factorization of A from :meth:`~linalg.lu_factor` of size :math:`(*, m, m)`, 
<a name="l18746"><span class="ln">18746 </span></a>                           where :math:`*` is zero or more batch dimensions. 
<a name="l18747"><span class="ln">18747 </span></a>        LU_pivots (IntTensor): the pivots of the LU factorization from :meth:`~linalg.lu_factor` of size :math:`(*, m)`, 
<a name="l18748"><span class="ln">18748 </span></a>                               where :math:`*` is zero or more batch dimensions. 
<a name="l18749"><span class="ln">18749 </span></a>                               The batch dimensions of :attr:`LU_pivots` must be equal to the batch dimensions of 
<a name="l18750"><span class="ln">18750 </span></a>                               :attr:`LU_data`. 
<a name="l18751"><span class="ln">18751 </span></a> 
<a name="l18752"><span class="ln">18752 </span></a>    Keyword args: 
<a name="l18753"><span class="ln">18753 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18754"><span class="ln">18754 </span></a> 
<a name="l18755"><span class="ln">18755 </span></a>    Example:: 
<a name="l18756"><span class="ln">18756 </span></a> 
<a name="l18757"><span class="ln">18757 </span></a>        &gt;&gt;&gt; A = torch.randn(2, 3, 3) 
<a name="l18758"><span class="ln">18758 </span></a>        &gt;&gt;&gt; b = torch.randn(2, 3, 1) 
<a name="l18759"><span class="ln">18759 </span></a>        &gt;&gt;&gt; LU, pivots = torch.linalg.lu_factor(A) 
<a name="l18760"><span class="ln">18760 </span></a>        &gt;&gt;&gt; x = torch.lu_solve(b, LU, pivots) 
<a name="l18761"><span class="ln">18761 </span></a>        &gt;&gt;&gt; torch.dist(A @ x, b) 
<a name="l18762"><span class="ln">18762 </span></a>        tensor(1.00000e-07 * 
<a name="l18763"><span class="ln">18763 </span></a>               2.8312) 
<a name="l18764"><span class="ln">18764 </span></a>    &quot;&quot;&quot;</span>
<a name="l18765"><span class="ln">18765 </span></a>
<a name="l18766"><span class="ln">18766 </span></a><span class="s2">def </span><span class="s1">lu_unpack</span><span class="s3">(</span>
<a name="l18767"><span class="ln">18767 </span></a>    <span class="s1">LU_data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18768"><span class="ln">18768 </span></a>    <span class="s1">LU_pivots</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18769"><span class="ln">18769 </span></a>    <span class="s1">unpack_data</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l18770"><span class="ln">18770 </span></a>    <span class="s1">unpack_pivots</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l18771"><span class="ln">18771 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18772"><span class="ln">18772 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18773"><span class="ln">18773 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">lu_unpack</span><span class="s2">:</span>
<a name="l18774"><span class="ln">18774 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18775"><span class="ln">18775 </span></a>    lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None) -&gt; (Tensor, Tensor, Tensor) 
<a name="l18776"><span class="ln">18776 </span></a> 
<a name="l18777"><span class="ln">18777 </span></a>    Unpacks the LU decomposition returned by :func:`~linalg.lu_factor` into the `P, L, U` matrices. 
<a name="l18778"><span class="ln">18778 </span></a> 
<a name="l18779"><span class="ln">18779 </span></a>    .. seealso:: 
<a name="l18780"><span class="ln">18780 </span></a> 
<a name="l18781"><span class="ln">18781 </span></a>        :func:`~linalg.lu` returns the matrices from the LU decomposition. Its gradient formula is more efficient 
<a name="l18782"><span class="ln">18782 </span></a>        than that of doing :func:`~linalg.lu_factor` followed by :func:`~linalg.lu_unpack`. 
<a name="l18783"><span class="ln">18783 </span></a> 
<a name="l18784"><span class="ln">18784 </span></a>    Args: 
<a name="l18785"><span class="ln">18785 </span></a>        LU_data (Tensor): the packed LU factorization data 
<a name="l18786"><span class="ln">18786 </span></a>        LU_pivots (Tensor): the packed LU factorization pivots 
<a name="l18787"><span class="ln">18787 </span></a>        unpack_data (bool): flag indicating if the data should be unpacked. 
<a name="l18788"><span class="ln">18788 </span></a>                            If ``False``, then the returned ``L`` and ``U`` are empty tensors. 
<a name="l18789"><span class="ln">18789 </span></a>                            Default: ``True`` 
<a name="l18790"><span class="ln">18790 </span></a>        unpack_pivots (bool): flag indicating if the pivots should be unpacked into a permutation matrix ``P``. 
<a name="l18791"><span class="ln">18791 </span></a>                              If ``False``, then the returned ``P`` is  an empty tensor. 
<a name="l18792"><span class="ln">18792 </span></a>                              Default: ``True`` 
<a name="l18793"><span class="ln">18793 </span></a> 
<a name="l18794"><span class="ln">18794 </span></a>    Keyword args: 
<a name="l18795"><span class="ln">18795 </span></a>        out (tuple, optional): output tuple of three tensors. Ignored if `None`. 
<a name="l18796"><span class="ln">18796 </span></a> 
<a name="l18797"><span class="ln">18797 </span></a>    Returns: 
<a name="l18798"><span class="ln">18798 </span></a>        A namedtuple ``(P, L, U)`` 
<a name="l18799"><span class="ln">18799 </span></a> 
<a name="l18800"><span class="ln">18800 </span></a>    Examples:: 
<a name="l18801"><span class="ln">18801 </span></a> 
<a name="l18802"><span class="ln">18802 </span></a>        &gt;&gt;&gt; A = torch.randn(2, 3, 3) 
<a name="l18803"><span class="ln">18803 </span></a>        &gt;&gt;&gt; LU, pivots = torch.linalg.lu_factor(A) 
<a name="l18804"><span class="ln">18804 </span></a>        &gt;&gt;&gt; P, L, U = torch.lu_unpack(LU, pivots) 
<a name="l18805"><span class="ln">18805 </span></a>        &gt;&gt;&gt; # We can recover A from the factorization 
<a name="l18806"><span class="ln">18806 </span></a>        &gt;&gt;&gt; A_ = P @ L @ U 
<a name="l18807"><span class="ln">18807 </span></a>        &gt;&gt;&gt; torch.allclose(A, A_) 
<a name="l18808"><span class="ln">18808 </span></a>        True 
<a name="l18809"><span class="ln">18809 </span></a> 
<a name="l18810"><span class="ln">18810 </span></a>        &gt;&gt;&gt; # LU factorization of a rectangular matrix: 
<a name="l18811"><span class="ln">18811 </span></a>        &gt;&gt;&gt; A = torch.randn(2, 3, 2) 
<a name="l18812"><span class="ln">18812 </span></a>        &gt;&gt;&gt; LU, pivots = torch.linalg.lu_factor(A) 
<a name="l18813"><span class="ln">18813 </span></a>        &gt;&gt;&gt; P, L, U = torch.lu_unpack(LU, pivots) 
<a name="l18814"><span class="ln">18814 </span></a>        &gt;&gt;&gt; # P, L, U are the same as returned by linalg.lu 
<a name="l18815"><span class="ln">18815 </span></a>        &gt;&gt;&gt; P_, L_, U_ = torch.linalg.lu(A) 
<a name="l18816"><span class="ln">18816 </span></a>        &gt;&gt;&gt; torch.allclose(P, P_) and torch.allclose(L, L_) and torch.allclose(U, U_) 
<a name="l18817"><span class="ln">18817 </span></a>        True 
<a name="l18818"><span class="ln">18818 </span></a>    &quot;&quot;&quot;</span>
<a name="l18819"><span class="ln">18819 </span></a>
<a name="l18820"><span class="ln">18820 </span></a><span class="s2">def </span><span class="s1">margin_ranking_loss</span><span class="s3">(</span>
<a name="l18821"><span class="ln">18821 </span></a>    <span class="s1">input1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18822"><span class="ln">18822 </span></a>    <span class="s1">input2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18823"><span class="ln">18823 </span></a>    <span class="s1">target</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18824"><span class="ln">18824 </span></a>    <span class="s1">margin</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">0.0</span><span class="s3">,</span>
<a name="l18825"><span class="ln">18825 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l18826"><span class="ln">18826 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18827"><span class="ln">18827 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18828"><span class="ln">18828 </span></a><span class="s2">def </span><span class="s1">masked_fill</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">value</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18829"><span class="ln">18829 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18830"><span class="ln">18830 </span></a><span class="s2">def </span><span class="s1">masked_fill</span><span class="s3">(</span>
<a name="l18831"><span class="ln">18831 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18832"><span class="ln">18832 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18833"><span class="ln">18833 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l18834"><span class="ln">18834 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18835"><span class="ln">18835 </span></a><span class="s2">def </span><span class="s1">masked_scatter</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">source</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l18836"><span class="ln">18836 </span></a><span class="s2">def </span><span class="s1">masked_select</span><span class="s3">(</span>
<a name="l18837"><span class="ln">18837 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18838"><span class="ln">18838 </span></a>    <span class="s1">mask</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18839"><span class="ln">18839 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18840"><span class="ln">18840 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18841"><span class="ln">18841 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18842"><span class="ln">18842 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18843"><span class="ln">18843 </span></a>    masked_select(input, mask, *, out=None) -&gt; Tensor 
<a name="l18844"><span class="ln">18844 </span></a> 
<a name="l18845"><span class="ln">18845 </span></a>    Returns a new 1-D tensor which indexes the :attr:`input` tensor according to 
<a name="l18846"><span class="ln">18846 </span></a>    the boolean mask :attr:`mask` which is a `BoolTensor`. 
<a name="l18847"><span class="ln">18847 </span></a> 
<a name="l18848"><span class="ln">18848 </span></a>    The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need 
<a name="l18849"><span class="ln">18849 </span></a>    to match, but they must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l18850"><span class="ln">18850 </span></a> 
<a name="l18851"><span class="ln">18851 </span></a>    .. note:: The returned tensor does **not** use the same storage 
<a name="l18852"><span class="ln">18852 </span></a>              as the original tensor 
<a name="l18853"><span class="ln">18853 </span></a> 
<a name="l18854"><span class="ln">18854 </span></a>    Args: 
<a name="l18855"><span class="ln">18855 </span></a>        input (Tensor): the input tensor. 
<a name="l18856"><span class="ln">18856 </span></a>        mask  (BoolTensor): the tensor containing the binary mask to index with 
<a name="l18857"><span class="ln">18857 </span></a> 
<a name="l18858"><span class="ln">18858 </span></a>    Keyword args: 
<a name="l18859"><span class="ln">18859 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18860"><span class="ln">18860 </span></a> 
<a name="l18861"><span class="ln">18861 </span></a>    Example:: 
<a name="l18862"><span class="ln">18862 </span></a> 
<a name="l18863"><span class="ln">18863 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l18864"><span class="ln">18864 </span></a>        &gt;&gt;&gt; x 
<a name="l18865"><span class="ln">18865 </span></a>        tensor([[ 0.3552, -2.3825, -0.8297,  0.3477], 
<a name="l18866"><span class="ln">18866 </span></a>                [-1.2035,  1.2252,  0.5002,  0.6248], 
<a name="l18867"><span class="ln">18867 </span></a>                [ 0.1307, -2.0608,  0.1244,  2.0139]]) 
<a name="l18868"><span class="ln">18868 </span></a>        &gt;&gt;&gt; mask = x.ge(0.5) 
<a name="l18869"><span class="ln">18869 </span></a>        &gt;&gt;&gt; mask 
<a name="l18870"><span class="ln">18870 </span></a>        tensor([[False, False, False, False], 
<a name="l18871"><span class="ln">18871 </span></a>                [False, True, True, True], 
<a name="l18872"><span class="ln">18872 </span></a>                [False, False, False, True]]) 
<a name="l18873"><span class="ln">18873 </span></a>        &gt;&gt;&gt; torch.masked_select(x, mask) 
<a name="l18874"><span class="ln">18874 </span></a>        tensor([ 1.2252,  0.5002,  0.6248,  2.0139]) 
<a name="l18875"><span class="ln">18875 </span></a>    &quot;&quot;&quot;</span>
<a name="l18876"><span class="ln">18876 </span></a>
<a name="l18877"><span class="ln">18877 </span></a><span class="s2">def </span><span class="s1">matmul</span><span class="s3">(</span>
<a name="l18878"><span class="ln">18878 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18879"><span class="ln">18879 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18880"><span class="ln">18880 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18881"><span class="ln">18881 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18882"><span class="ln">18882 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18883"><span class="ln">18883 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18884"><span class="ln">18884 </span></a>    matmul(input, other, *, out=None) -&gt; Tensor 
<a name="l18885"><span class="ln">18885 </span></a> 
<a name="l18886"><span class="ln">18886 </span></a>    Matrix product of two tensors. 
<a name="l18887"><span class="ln">18887 </span></a> 
<a name="l18888"><span class="ln">18888 </span></a>    The behavior depends on the dimensionality of the tensors as follows: 
<a name="l18889"><span class="ln">18889 </span></a> 
<a name="l18890"><span class="ln">18890 </span></a>    - If both tensors are 1-dimensional, the dot product (scalar) is returned. 
<a name="l18891"><span class="ln">18891 </span></a>    - If both arguments are 2-dimensional, the matrix-matrix product is returned. 
<a name="l18892"><span class="ln">18892 </span></a>    - If the first argument is 1-dimensional and the second argument is 2-dimensional, 
<a name="l18893"><span class="ln">18893 </span></a>      a 1 is prepended to its dimension for the purpose of the matrix multiply. 
<a name="l18894"><span class="ln">18894 </span></a>      After the matrix multiply, the prepended dimension is removed. 
<a name="l18895"><span class="ln">18895 </span></a>    - If the first argument is 2-dimensional and the second argument is 1-dimensional, 
<a name="l18896"><span class="ln">18896 </span></a>      the matrix-vector product is returned. 
<a name="l18897"><span class="ln">18897 </span></a>    - If both arguments are at least 1-dimensional and at least one argument is 
<a name="l18898"><span class="ln">18898 </span></a>      N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first 
<a name="l18899"><span class="ln">18899 </span></a>      argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the 
<a name="l18900"><span class="ln">18900 </span></a>      batched matrix multiply and removed after.  If the second argument is 1-dimensional, a 
<a name="l18901"><span class="ln">18901 </span></a>      1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. 
<a name="l18902"><span class="ln">18902 </span></a>      The non-matrix (i.e. batch) dimensions are :ref:`broadcasted &lt;broadcasting-semantics&gt;` (and thus 
<a name="l18903"><span class="ln">18903 </span></a>      must be broadcastable).  For example, if :attr:`input` is a 
<a name="l18904"><span class="ln">18904 </span></a>      :math:`(j \times 1 \times n \times n)` tensor and :attr:`other` is a :math:`(k \times n \times n)` 
<a name="l18905"><span class="ln">18905 </span></a>      tensor, :attr:`out` will be a :math:`(j \times k \times n \times n)` tensor. 
<a name="l18906"><span class="ln">18906 </span></a> 
<a name="l18907"><span class="ln">18907 </span></a>      Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs 
<a name="l18908"><span class="ln">18908 </span></a>      are broadcastable, and not the matrix dimensions. For example, if :attr:`input` is a 
<a name="l18909"><span class="ln">18909 </span></a>      :math:`(j \times 1 \times n \times m)` tensor and :attr:`other` is a :math:`(k \times m \times p)` 
<a name="l18910"><span class="ln">18910 </span></a>      tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the 
<a name="l18911"><span class="ln">18911 </span></a>      matrix dimensions) are different. :attr:`out` will be a :math:`(j \times k \times n \times p)` tensor. 
<a name="l18912"><span class="ln">18912 </span></a> 
<a name="l18913"><span class="ln">18913 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. In particular the 
<a name="l18914"><span class="ln">18914 </span></a>    matrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions 
<a name="l18915"><span class="ln">18915 </span></a>    as :func:`torch.mm` 
<a name="l18916"><span class="ln">18916 </span></a> 
<a name="l18917"><span class="ln">18917 </span></a> 
<a name="l18918"><span class="ln">18918 </span></a>    .. warning:: 
<a name="l18919"><span class="ln">18919 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l18920"><span class="ln">18920 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l18921"><span class="ln">18921 </span></a>        open a feature request. 
<a name="l18922"><span class="ln">18922 </span></a> 
<a name="l18923"><span class="ln">18923 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l18924"><span class="ln">18924 </span></a> 
<a name="l18925"><span class="ln">18925 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l18926"><span class="ln">18926 </span></a> 
<a name="l18927"><span class="ln">18927 </span></a>    .. note:: 
<a name="l18928"><span class="ln">18928 </span></a> 
<a name="l18929"><span class="ln">18929 </span></a>        The 1-dimensional dot product version of this function does not support an :attr:`out` parameter. 
<a name="l18930"><span class="ln">18930 </span></a> 
<a name="l18931"><span class="ln">18931 </span></a>    Arguments: 
<a name="l18932"><span class="ln">18932 </span></a>        input (Tensor): the first tensor to be multiplied 
<a name="l18933"><span class="ln">18933 </span></a>        other (Tensor): the second tensor to be multiplied 
<a name="l18934"><span class="ln">18934 </span></a> 
<a name="l18935"><span class="ln">18935 </span></a>    Keyword args: 
<a name="l18936"><span class="ln">18936 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18937"><span class="ln">18937 </span></a> 
<a name="l18938"><span class="ln">18938 </span></a>    Example:: 
<a name="l18939"><span class="ln">18939 </span></a> 
<a name="l18940"><span class="ln">18940 </span></a>        &gt;&gt;&gt; # vector x vector 
<a name="l18941"><span class="ln">18941 </span></a>        &gt;&gt;&gt; tensor1 = torch.randn(3) 
<a name="l18942"><span class="ln">18942 </span></a>        &gt;&gt;&gt; tensor2 = torch.randn(3) 
<a name="l18943"><span class="ln">18943 </span></a>        &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l18944"><span class="ln">18944 </span></a>        torch.Size([]) 
<a name="l18945"><span class="ln">18945 </span></a>        &gt;&gt;&gt; # matrix x vector 
<a name="l18946"><span class="ln">18946 </span></a>        &gt;&gt;&gt; tensor1 = torch.randn(3, 4) 
<a name="l18947"><span class="ln">18947 </span></a>        &gt;&gt;&gt; tensor2 = torch.randn(4) 
<a name="l18948"><span class="ln">18948 </span></a>        &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l18949"><span class="ln">18949 </span></a>        torch.Size([3]) 
<a name="l18950"><span class="ln">18950 </span></a>        &gt;&gt;&gt; # batched matrix x broadcasted vector 
<a name="l18951"><span class="ln">18951 </span></a>        &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) 
<a name="l18952"><span class="ln">18952 </span></a>        &gt;&gt;&gt; tensor2 = torch.randn(4) 
<a name="l18953"><span class="ln">18953 </span></a>        &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l18954"><span class="ln">18954 </span></a>        torch.Size([10, 3]) 
<a name="l18955"><span class="ln">18955 </span></a>        &gt;&gt;&gt; # batched matrix x batched matrix 
<a name="l18956"><span class="ln">18956 </span></a>        &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) 
<a name="l18957"><span class="ln">18957 </span></a>        &gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5) 
<a name="l18958"><span class="ln">18958 </span></a>        &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l18959"><span class="ln">18959 </span></a>        torch.Size([10, 3, 5]) 
<a name="l18960"><span class="ln">18960 </span></a>        &gt;&gt;&gt; # batched matrix x broadcasted matrix 
<a name="l18961"><span class="ln">18961 </span></a>        &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) 
<a name="l18962"><span class="ln">18962 </span></a>        &gt;&gt;&gt; tensor2 = torch.randn(4, 5) 
<a name="l18963"><span class="ln">18963 </span></a>        &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() 
<a name="l18964"><span class="ln">18964 </span></a>        torch.Size([10, 3, 5]) 
<a name="l18965"><span class="ln">18965 </span></a>    &quot;&quot;&quot;</span>
<a name="l18966"><span class="ln">18966 </span></a>
<a name="l18967"><span class="ln">18967 </span></a><span class="s2">def </span><span class="s1">matrix_exp</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18968"><span class="ln">18968 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18969"><span class="ln">18969 </span></a>    matrix_exp(A) -&gt; Tensor 
<a name="l18970"><span class="ln">18970 </span></a> 
<a name="l18971"><span class="ln">18971 </span></a>    Alias for :func:`torch.linalg.matrix_exp`. 
<a name="l18972"><span class="ln">18972 </span></a>    &quot;&quot;&quot;</span>
<a name="l18973"><span class="ln">18973 </span></a>
<a name="l18974"><span class="ln">18974 </span></a><span class="s2">def </span><span class="s1">matrix_power</span><span class="s3">(</span>
<a name="l18975"><span class="ln">18975 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l18976"><span class="ln">18976 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l18977"><span class="ln">18977 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l18978"><span class="ln">18978 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l18979"><span class="ln">18979 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18980"><span class="ln">18980 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18981"><span class="ln">18981 </span></a>    matrix_power(input, n, *, out=None) -&gt; Tensor 
<a name="l18982"><span class="ln">18982 </span></a> 
<a name="l18983"><span class="ln">18983 </span></a>    Alias for :func:`torch.linalg.matrix_power` 
<a name="l18984"><span class="ln">18984 </span></a>    &quot;&quot;&quot;</span>
<a name="l18985"><span class="ln">18985 </span></a>
<a name="l18986"><span class="ln">18986 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l18987"><span class="ln">18987 </span></a><span class="s2">def </span><span class="s1">max</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l18988"><span class="ln">18988 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l18989"><span class="ln">18989 </span></a>    max(input, *, out=None) -&gt; Tensor 
<a name="l18990"><span class="ln">18990 </span></a> 
<a name="l18991"><span class="ln">18991 </span></a>    Returns the maximum value of all elements in the ``input`` tensor. 
<a name="l18992"><span class="ln">18992 </span></a> 
<a name="l18993"><span class="ln">18993 </span></a>    Args: 
<a name="l18994"><span class="ln">18994 </span></a>        input (Tensor): the input tensor. 
<a name="l18995"><span class="ln">18995 </span></a> 
<a name="l18996"><span class="ln">18996 </span></a>    Keyword args: 
<a name="l18997"><span class="ln">18997 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l18998"><span class="ln">18998 </span></a> 
<a name="l18999"><span class="ln">18999 </span></a>    Example:: 
<a name="l19000"><span class="ln">19000 </span></a> 
<a name="l19001"><span class="ln">19001 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19002"><span class="ln">19002 </span></a>        &gt;&gt;&gt; a 
<a name="l19003"><span class="ln">19003 </span></a>        tensor([[ 0.6763,  0.7445, -2.2369]]) 
<a name="l19004"><span class="ln">19004 </span></a>        &gt;&gt;&gt; torch.max(a) 
<a name="l19005"><span class="ln">19005 </span></a>        tensor(0.7445) 
<a name="l19006"><span class="ln">19006 </span></a> 
<a name="l19007"><span class="ln">19007 </span></a>    .. function:: max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19008"><span class="ln">19008 </span></a>       :noindex: 
<a name="l19009"><span class="ln">19009 </span></a> 
<a name="l19010"><span class="ln">19010 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum 
<a name="l19011"><span class="ln">19011 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l19012"><span class="ln">19012 </span></a>    :attr:`dim`. And ``indices`` is the index location of each maximum value found 
<a name="l19013"><span class="ln">19013 </span></a>    (argmax). 
<a name="l19014"><span class="ln">19014 </span></a> 
<a name="l19015"><span class="ln">19015 </span></a>    If ``keepdim`` is ``True``, the output tensors are of the same size 
<a name="l19016"><span class="ln">19016 </span></a>    as ``input`` except in the dimension ``dim`` where they are of size 1. 
<a name="l19017"><span class="ln">19017 </span></a>    Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l19018"><span class="ln">19018 </span></a>    in the output tensors having 1 fewer dimension than ``input``. 
<a name="l19019"><span class="ln">19019 </span></a> 
<a name="l19020"><span class="ln">19020 </span></a>    .. note:: If there are multiple maximal values in a reduced row then 
<a name="l19021"><span class="ln">19021 </span></a>              the indices of the first maximal value are returned. 
<a name="l19022"><span class="ln">19022 </span></a> 
<a name="l19023"><span class="ln">19023 </span></a>    Args: 
<a name="l19024"><span class="ln">19024 </span></a>        input (Tensor): the input tensor. 
<a name="l19025"><span class="ln">19025 </span></a> 
<a name="l19026"><span class="ln">19026 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l19027"><span class="ln">19027 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19028"><span class="ln">19028 </span></a> 
<a name="l19029"><span class="ln">19029 </span></a> 
<a name="l19030"><span class="ln">19030 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19031"><span class="ln">19031 </span></a> 
<a name="l19032"><span class="ln">19032 </span></a> 
<a name="l19033"><span class="ln">19033 </span></a>    Keyword args: 
<a name="l19034"><span class="ln">19034 </span></a>        out (tuple, optional): the result tuple of two output tensors (max, max_indices) 
<a name="l19035"><span class="ln">19035 </span></a> 
<a name="l19036"><span class="ln">19036 </span></a>    Example:: 
<a name="l19037"><span class="ln">19037 </span></a> 
<a name="l19038"><span class="ln">19038 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19039"><span class="ln">19039 </span></a>        &gt;&gt;&gt; a 
<a name="l19040"><span class="ln">19040 </span></a>        tensor([[-1.2360, -0.2942, -0.1222,  0.8475], 
<a name="l19041"><span class="ln">19041 </span></a>                [ 1.1949, -1.1127, -2.2379, -0.6702], 
<a name="l19042"><span class="ln">19042 </span></a>                [ 1.5717, -0.9207,  0.1297, -1.8768], 
<a name="l19043"><span class="ln">19043 </span></a>                [-0.6172,  1.0036, -0.6060, -0.2432]]) 
<a name="l19044"><span class="ln">19044 </span></a>        &gt;&gt;&gt; torch.max(a, 1) 
<a name="l19045"><span class="ln">19045 </span></a>        torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1])) 
<a name="l19046"><span class="ln">19046 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) 
<a name="l19047"><span class="ln">19047 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=True) 
<a name="l19048"><span class="ln">19048 </span></a>        torch.return_types.max( 
<a name="l19049"><span class="ln">19049 </span></a>        values=tensor([[2.], [4.]]), 
<a name="l19050"><span class="ln">19050 </span></a>        indices=tensor([[1], [1]])) 
<a name="l19051"><span class="ln">19051 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=False) 
<a name="l19052"><span class="ln">19052 </span></a>        torch.return_types.max( 
<a name="l19053"><span class="ln">19053 </span></a>        values=tensor([2., 4.]), 
<a name="l19054"><span class="ln">19054 </span></a>        indices=tensor([1, 1])) 
<a name="l19055"><span class="ln">19055 </span></a> 
<a name="l19056"><span class="ln">19056 </span></a>    .. function:: max(input, other, *, out=None) -&gt; Tensor 
<a name="l19057"><span class="ln">19057 </span></a>       :noindex: 
<a name="l19058"><span class="ln">19058 </span></a> 
<a name="l19059"><span class="ln">19059 </span></a>    See :func:`torch.maximum`. 
<a name="l19060"><span class="ln">19060 </span></a>    &quot;&quot;&quot;</span>
<a name="l19061"><span class="ln">19061 </span></a>
<a name="l19062"><span class="ln">19062 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19063"><span class="ln">19063 </span></a><span class="s2">def </span><span class="s1">max</span><span class="s3">(</span>
<a name="l19064"><span class="ln">19064 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19065"><span class="ln">19065 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19066"><span class="ln">19066 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19067"><span class="ln">19067 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19068"><span class="ln">19068 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19069"><span class="ln">19069 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19070"><span class="ln">19070 </span></a>    max(input, *, out=None) -&gt; Tensor 
<a name="l19071"><span class="ln">19071 </span></a> 
<a name="l19072"><span class="ln">19072 </span></a>    Returns the maximum value of all elements in the ``input`` tensor. 
<a name="l19073"><span class="ln">19073 </span></a> 
<a name="l19074"><span class="ln">19074 </span></a>    Args: 
<a name="l19075"><span class="ln">19075 </span></a>        input (Tensor): the input tensor. 
<a name="l19076"><span class="ln">19076 </span></a> 
<a name="l19077"><span class="ln">19077 </span></a>    Keyword args: 
<a name="l19078"><span class="ln">19078 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19079"><span class="ln">19079 </span></a> 
<a name="l19080"><span class="ln">19080 </span></a>    Example:: 
<a name="l19081"><span class="ln">19081 </span></a> 
<a name="l19082"><span class="ln">19082 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19083"><span class="ln">19083 </span></a>        &gt;&gt;&gt; a 
<a name="l19084"><span class="ln">19084 </span></a>        tensor([[ 0.6763,  0.7445, -2.2369]]) 
<a name="l19085"><span class="ln">19085 </span></a>        &gt;&gt;&gt; torch.max(a) 
<a name="l19086"><span class="ln">19086 </span></a>        tensor(0.7445) 
<a name="l19087"><span class="ln">19087 </span></a> 
<a name="l19088"><span class="ln">19088 </span></a>    .. function:: max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19089"><span class="ln">19089 </span></a>       :noindex: 
<a name="l19090"><span class="ln">19090 </span></a> 
<a name="l19091"><span class="ln">19091 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum 
<a name="l19092"><span class="ln">19092 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l19093"><span class="ln">19093 </span></a>    :attr:`dim`. And ``indices`` is the index location of each maximum value found 
<a name="l19094"><span class="ln">19094 </span></a>    (argmax). 
<a name="l19095"><span class="ln">19095 </span></a> 
<a name="l19096"><span class="ln">19096 </span></a>    If ``keepdim`` is ``True``, the output tensors are of the same size 
<a name="l19097"><span class="ln">19097 </span></a>    as ``input`` except in the dimension ``dim`` where they are of size 1. 
<a name="l19098"><span class="ln">19098 </span></a>    Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l19099"><span class="ln">19099 </span></a>    in the output tensors having 1 fewer dimension than ``input``. 
<a name="l19100"><span class="ln">19100 </span></a> 
<a name="l19101"><span class="ln">19101 </span></a>    .. note:: If there are multiple maximal values in a reduced row then 
<a name="l19102"><span class="ln">19102 </span></a>              the indices of the first maximal value are returned. 
<a name="l19103"><span class="ln">19103 </span></a> 
<a name="l19104"><span class="ln">19104 </span></a>    Args: 
<a name="l19105"><span class="ln">19105 </span></a>        input (Tensor): the input tensor. 
<a name="l19106"><span class="ln">19106 </span></a> 
<a name="l19107"><span class="ln">19107 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l19108"><span class="ln">19108 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19109"><span class="ln">19109 </span></a> 
<a name="l19110"><span class="ln">19110 </span></a> 
<a name="l19111"><span class="ln">19111 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19112"><span class="ln">19112 </span></a> 
<a name="l19113"><span class="ln">19113 </span></a> 
<a name="l19114"><span class="ln">19114 </span></a>    Keyword args: 
<a name="l19115"><span class="ln">19115 </span></a>        out (tuple, optional): the result tuple of two output tensors (max, max_indices) 
<a name="l19116"><span class="ln">19116 </span></a> 
<a name="l19117"><span class="ln">19117 </span></a>    Example:: 
<a name="l19118"><span class="ln">19118 </span></a> 
<a name="l19119"><span class="ln">19119 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19120"><span class="ln">19120 </span></a>        &gt;&gt;&gt; a 
<a name="l19121"><span class="ln">19121 </span></a>        tensor([[-1.2360, -0.2942, -0.1222,  0.8475], 
<a name="l19122"><span class="ln">19122 </span></a>                [ 1.1949, -1.1127, -2.2379, -0.6702], 
<a name="l19123"><span class="ln">19123 </span></a>                [ 1.5717, -0.9207,  0.1297, -1.8768], 
<a name="l19124"><span class="ln">19124 </span></a>                [-0.6172,  1.0036, -0.6060, -0.2432]]) 
<a name="l19125"><span class="ln">19125 </span></a>        &gt;&gt;&gt; torch.max(a, 1) 
<a name="l19126"><span class="ln">19126 </span></a>        torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1])) 
<a name="l19127"><span class="ln">19127 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) 
<a name="l19128"><span class="ln">19128 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=True) 
<a name="l19129"><span class="ln">19129 </span></a>        torch.return_types.max( 
<a name="l19130"><span class="ln">19130 </span></a>        values=tensor([[2.], [4.]]), 
<a name="l19131"><span class="ln">19131 </span></a>        indices=tensor([[1], [1]])) 
<a name="l19132"><span class="ln">19132 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=False) 
<a name="l19133"><span class="ln">19133 </span></a>        torch.return_types.max( 
<a name="l19134"><span class="ln">19134 </span></a>        values=tensor([2., 4.]), 
<a name="l19135"><span class="ln">19135 </span></a>        indices=tensor([1, 1])) 
<a name="l19136"><span class="ln">19136 </span></a> 
<a name="l19137"><span class="ln">19137 </span></a>    .. function:: max(input, other, *, out=None) -&gt; Tensor 
<a name="l19138"><span class="ln">19138 </span></a>       :noindex: 
<a name="l19139"><span class="ln">19139 </span></a> 
<a name="l19140"><span class="ln">19140 </span></a>    See :func:`torch.maximum`. 
<a name="l19141"><span class="ln">19141 </span></a>    &quot;&quot;&quot;</span>
<a name="l19142"><span class="ln">19142 </span></a>
<a name="l19143"><span class="ln">19143 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19144"><span class="ln">19144 </span></a><span class="s2">def </span><span class="s1">max</span><span class="s3">(</span>
<a name="l19145"><span class="ln">19145 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19146"><span class="ln">19146 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l19147"><span class="ln">19147 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19148"><span class="ln">19148 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19149"><span class="ln">19149 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19150"><span class="ln">19150 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">max</span><span class="s2">:</span>
<a name="l19151"><span class="ln">19151 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19152"><span class="ln">19152 </span></a>    max(input, *, out=None) -&gt; Tensor 
<a name="l19153"><span class="ln">19153 </span></a> 
<a name="l19154"><span class="ln">19154 </span></a>    Returns the maximum value of all elements in the ``input`` tensor. 
<a name="l19155"><span class="ln">19155 </span></a> 
<a name="l19156"><span class="ln">19156 </span></a>    Args: 
<a name="l19157"><span class="ln">19157 </span></a>        input (Tensor): the input tensor. 
<a name="l19158"><span class="ln">19158 </span></a> 
<a name="l19159"><span class="ln">19159 </span></a>    Keyword args: 
<a name="l19160"><span class="ln">19160 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19161"><span class="ln">19161 </span></a> 
<a name="l19162"><span class="ln">19162 </span></a>    Example:: 
<a name="l19163"><span class="ln">19163 </span></a> 
<a name="l19164"><span class="ln">19164 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19165"><span class="ln">19165 </span></a>        &gt;&gt;&gt; a 
<a name="l19166"><span class="ln">19166 </span></a>        tensor([[ 0.6763,  0.7445, -2.2369]]) 
<a name="l19167"><span class="ln">19167 </span></a>        &gt;&gt;&gt; torch.max(a) 
<a name="l19168"><span class="ln">19168 </span></a>        tensor(0.7445) 
<a name="l19169"><span class="ln">19169 </span></a> 
<a name="l19170"><span class="ln">19170 </span></a>    .. function:: max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19171"><span class="ln">19171 </span></a>       :noindex: 
<a name="l19172"><span class="ln">19172 </span></a> 
<a name="l19173"><span class="ln">19173 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum 
<a name="l19174"><span class="ln">19174 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l19175"><span class="ln">19175 </span></a>    :attr:`dim`. And ``indices`` is the index location of each maximum value found 
<a name="l19176"><span class="ln">19176 </span></a>    (argmax). 
<a name="l19177"><span class="ln">19177 </span></a> 
<a name="l19178"><span class="ln">19178 </span></a>    If ``keepdim`` is ``True``, the output tensors are of the same size 
<a name="l19179"><span class="ln">19179 </span></a>    as ``input`` except in the dimension ``dim`` where they are of size 1. 
<a name="l19180"><span class="ln">19180 </span></a>    Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l19181"><span class="ln">19181 </span></a>    in the output tensors having 1 fewer dimension than ``input``. 
<a name="l19182"><span class="ln">19182 </span></a> 
<a name="l19183"><span class="ln">19183 </span></a>    .. note:: If there are multiple maximal values in a reduced row then 
<a name="l19184"><span class="ln">19184 </span></a>              the indices of the first maximal value are returned. 
<a name="l19185"><span class="ln">19185 </span></a> 
<a name="l19186"><span class="ln">19186 </span></a>    Args: 
<a name="l19187"><span class="ln">19187 </span></a>        input (Tensor): the input tensor. 
<a name="l19188"><span class="ln">19188 </span></a> 
<a name="l19189"><span class="ln">19189 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l19190"><span class="ln">19190 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19191"><span class="ln">19191 </span></a> 
<a name="l19192"><span class="ln">19192 </span></a> 
<a name="l19193"><span class="ln">19193 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19194"><span class="ln">19194 </span></a> 
<a name="l19195"><span class="ln">19195 </span></a> 
<a name="l19196"><span class="ln">19196 </span></a>    Keyword args: 
<a name="l19197"><span class="ln">19197 </span></a>        out (tuple, optional): the result tuple of two output tensors (max, max_indices) 
<a name="l19198"><span class="ln">19198 </span></a> 
<a name="l19199"><span class="ln">19199 </span></a>    Example:: 
<a name="l19200"><span class="ln">19200 </span></a> 
<a name="l19201"><span class="ln">19201 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19202"><span class="ln">19202 </span></a>        &gt;&gt;&gt; a 
<a name="l19203"><span class="ln">19203 </span></a>        tensor([[-1.2360, -0.2942, -0.1222,  0.8475], 
<a name="l19204"><span class="ln">19204 </span></a>                [ 1.1949, -1.1127, -2.2379, -0.6702], 
<a name="l19205"><span class="ln">19205 </span></a>                [ 1.5717, -0.9207,  0.1297, -1.8768], 
<a name="l19206"><span class="ln">19206 </span></a>                [-0.6172,  1.0036, -0.6060, -0.2432]]) 
<a name="l19207"><span class="ln">19207 </span></a>        &gt;&gt;&gt; torch.max(a, 1) 
<a name="l19208"><span class="ln">19208 </span></a>        torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1])) 
<a name="l19209"><span class="ln">19209 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) 
<a name="l19210"><span class="ln">19210 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=True) 
<a name="l19211"><span class="ln">19211 </span></a>        torch.return_types.max( 
<a name="l19212"><span class="ln">19212 </span></a>        values=tensor([[2.], [4.]]), 
<a name="l19213"><span class="ln">19213 </span></a>        indices=tensor([[1], [1]])) 
<a name="l19214"><span class="ln">19214 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=False) 
<a name="l19215"><span class="ln">19215 </span></a>        torch.return_types.max( 
<a name="l19216"><span class="ln">19216 </span></a>        values=tensor([2., 4.]), 
<a name="l19217"><span class="ln">19217 </span></a>        indices=tensor([1, 1])) 
<a name="l19218"><span class="ln">19218 </span></a> 
<a name="l19219"><span class="ln">19219 </span></a>    .. function:: max(input, other, *, out=None) -&gt; Tensor 
<a name="l19220"><span class="ln">19220 </span></a>       :noindex: 
<a name="l19221"><span class="ln">19221 </span></a> 
<a name="l19222"><span class="ln">19222 </span></a>    See :func:`torch.maximum`. 
<a name="l19223"><span class="ln">19223 </span></a>    &quot;&quot;&quot;</span>
<a name="l19224"><span class="ln">19224 </span></a>
<a name="l19225"><span class="ln">19225 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19226"><span class="ln">19226 </span></a><span class="s2">def </span><span class="s1">max</span><span class="s3">(</span>
<a name="l19227"><span class="ln">19227 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19228"><span class="ln">19228 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l19229"><span class="ln">19229 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19230"><span class="ln">19230 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19231"><span class="ln">19231 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19232"><span class="ln">19232 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">max</span><span class="s2">:</span>
<a name="l19233"><span class="ln">19233 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19234"><span class="ln">19234 </span></a>    max(input, *, out=None) -&gt; Tensor 
<a name="l19235"><span class="ln">19235 </span></a> 
<a name="l19236"><span class="ln">19236 </span></a>    Returns the maximum value of all elements in the ``input`` tensor. 
<a name="l19237"><span class="ln">19237 </span></a> 
<a name="l19238"><span class="ln">19238 </span></a>    Args: 
<a name="l19239"><span class="ln">19239 </span></a>        input (Tensor): the input tensor. 
<a name="l19240"><span class="ln">19240 </span></a> 
<a name="l19241"><span class="ln">19241 </span></a>    Keyword args: 
<a name="l19242"><span class="ln">19242 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19243"><span class="ln">19243 </span></a> 
<a name="l19244"><span class="ln">19244 </span></a>    Example:: 
<a name="l19245"><span class="ln">19245 </span></a> 
<a name="l19246"><span class="ln">19246 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19247"><span class="ln">19247 </span></a>        &gt;&gt;&gt; a 
<a name="l19248"><span class="ln">19248 </span></a>        tensor([[ 0.6763,  0.7445, -2.2369]]) 
<a name="l19249"><span class="ln">19249 </span></a>        &gt;&gt;&gt; torch.max(a) 
<a name="l19250"><span class="ln">19250 </span></a>        tensor(0.7445) 
<a name="l19251"><span class="ln">19251 </span></a> 
<a name="l19252"><span class="ln">19252 </span></a>    .. function:: max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19253"><span class="ln">19253 </span></a>       :noindex: 
<a name="l19254"><span class="ln">19254 </span></a> 
<a name="l19255"><span class="ln">19255 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum 
<a name="l19256"><span class="ln">19256 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l19257"><span class="ln">19257 </span></a>    :attr:`dim`. And ``indices`` is the index location of each maximum value found 
<a name="l19258"><span class="ln">19258 </span></a>    (argmax). 
<a name="l19259"><span class="ln">19259 </span></a> 
<a name="l19260"><span class="ln">19260 </span></a>    If ``keepdim`` is ``True``, the output tensors are of the same size 
<a name="l19261"><span class="ln">19261 </span></a>    as ``input`` except in the dimension ``dim`` where they are of size 1. 
<a name="l19262"><span class="ln">19262 </span></a>    Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l19263"><span class="ln">19263 </span></a>    in the output tensors having 1 fewer dimension than ``input``. 
<a name="l19264"><span class="ln">19264 </span></a> 
<a name="l19265"><span class="ln">19265 </span></a>    .. note:: If there are multiple maximal values in a reduced row then 
<a name="l19266"><span class="ln">19266 </span></a>              the indices of the first maximal value are returned. 
<a name="l19267"><span class="ln">19267 </span></a> 
<a name="l19268"><span class="ln">19268 </span></a>    Args: 
<a name="l19269"><span class="ln">19269 </span></a>        input (Tensor): the input tensor. 
<a name="l19270"><span class="ln">19270 </span></a> 
<a name="l19271"><span class="ln">19271 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l19272"><span class="ln">19272 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19273"><span class="ln">19273 </span></a> 
<a name="l19274"><span class="ln">19274 </span></a> 
<a name="l19275"><span class="ln">19275 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19276"><span class="ln">19276 </span></a> 
<a name="l19277"><span class="ln">19277 </span></a> 
<a name="l19278"><span class="ln">19278 </span></a>    Keyword args: 
<a name="l19279"><span class="ln">19279 </span></a>        out (tuple, optional): the result tuple of two output tensors (max, max_indices) 
<a name="l19280"><span class="ln">19280 </span></a> 
<a name="l19281"><span class="ln">19281 </span></a>    Example:: 
<a name="l19282"><span class="ln">19282 </span></a> 
<a name="l19283"><span class="ln">19283 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19284"><span class="ln">19284 </span></a>        &gt;&gt;&gt; a 
<a name="l19285"><span class="ln">19285 </span></a>        tensor([[-1.2360, -0.2942, -0.1222,  0.8475], 
<a name="l19286"><span class="ln">19286 </span></a>                [ 1.1949, -1.1127, -2.2379, -0.6702], 
<a name="l19287"><span class="ln">19287 </span></a>                [ 1.5717, -0.9207,  0.1297, -1.8768], 
<a name="l19288"><span class="ln">19288 </span></a>                [-0.6172,  1.0036, -0.6060, -0.2432]]) 
<a name="l19289"><span class="ln">19289 </span></a>        &gt;&gt;&gt; torch.max(a, 1) 
<a name="l19290"><span class="ln">19290 </span></a>        torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1])) 
<a name="l19291"><span class="ln">19291 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) 
<a name="l19292"><span class="ln">19292 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=True) 
<a name="l19293"><span class="ln">19293 </span></a>        torch.return_types.max( 
<a name="l19294"><span class="ln">19294 </span></a>        values=tensor([[2.], [4.]]), 
<a name="l19295"><span class="ln">19295 </span></a>        indices=tensor([[1], [1]])) 
<a name="l19296"><span class="ln">19296 </span></a>        &gt;&gt;&gt; a.max(dim=1, keepdim=False) 
<a name="l19297"><span class="ln">19297 </span></a>        torch.return_types.max( 
<a name="l19298"><span class="ln">19298 </span></a>        values=tensor([2., 4.]), 
<a name="l19299"><span class="ln">19299 </span></a>        indices=tensor([1, 1])) 
<a name="l19300"><span class="ln">19300 </span></a> 
<a name="l19301"><span class="ln">19301 </span></a>    .. function:: max(input, other, *, out=None) -&gt; Tensor 
<a name="l19302"><span class="ln">19302 </span></a>       :noindex: 
<a name="l19303"><span class="ln">19303 </span></a> 
<a name="l19304"><span class="ln">19304 </span></a>    See :func:`torch.maximum`. 
<a name="l19305"><span class="ln">19305 </span></a>    &quot;&quot;&quot;</span>
<a name="l19306"><span class="ln">19306 </span></a>
<a name="l19307"><span class="ln">19307 </span></a><span class="s2">def </span><span class="s1">max_pool1d</span><span class="s3">(</span>
<a name="l19308"><span class="ln">19308 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19309"><span class="ln">19309 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l19310"><span class="ln">19310 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l19311"><span class="ln">19311 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l19312"><span class="ln">19312 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l19313"><span class="ln">19313 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19314"><span class="ln">19314 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l19315"><span class="ln">19315 </span></a><span class="s2">def </span><span class="s1">max_pool1d_with_indices</span><span class="s3">(</span>
<a name="l19316"><span class="ln">19316 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19317"><span class="ln">19317 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l19318"><span class="ln">19318 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l19319"><span class="ln">19319 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l19320"><span class="ln">19320 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l19321"><span class="ln">19321 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19322"><span class="ln">19322 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l19323"><span class="ln">19323 </span></a><span class="s2">def </span><span class="s1">max_pool2d</span><span class="s3">(</span>
<a name="l19324"><span class="ln">19324 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19325"><span class="ln">19325 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l19326"><span class="ln">19326 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l19327"><span class="ln">19327 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l19328"><span class="ln">19328 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l19329"><span class="ln">19329 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19330"><span class="ln">19330 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l19331"><span class="ln">19331 </span></a><span class="s2">def </span><span class="s1">max_pool3d</span><span class="s3">(</span>
<a name="l19332"><span class="ln">19332 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19333"><span class="ln">19333 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l19334"><span class="ln">19334 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l19335"><span class="ln">19335 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l19336"><span class="ln">19336 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l19337"><span class="ln">19337 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19338"><span class="ln">19338 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l19339"><span class="ln">19339 </span></a><span class="s2">def </span><span class="s1">maximum</span><span class="s3">(</span>
<a name="l19340"><span class="ln">19340 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19341"><span class="ln">19341 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19342"><span class="ln">19342 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19343"><span class="ln">19343 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19344"><span class="ln">19344 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19345"><span class="ln">19345 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19346"><span class="ln">19346 </span></a>    maximum(input, other, *, out=None) -&gt; Tensor 
<a name="l19347"><span class="ln">19347 </span></a> 
<a name="l19348"><span class="ln">19348 </span></a>    Computes the element-wise maximum of :attr:`input` and :attr:`other`. 
<a name="l19349"><span class="ln">19349 </span></a> 
<a name="l19350"><span class="ln">19350 </span></a>    .. note:: 
<a name="l19351"><span class="ln">19351 </span></a>        If one of the elements being compared is a NaN, then that element is returned. 
<a name="l19352"><span class="ln">19352 </span></a>        :func:`maximum` is not supported for tensors with complex dtypes. 
<a name="l19353"><span class="ln">19353 </span></a> 
<a name="l19354"><span class="ln">19354 </span></a>    Args: 
<a name="l19355"><span class="ln">19355 </span></a>        input (Tensor): the input tensor. 
<a name="l19356"><span class="ln">19356 </span></a>        other (Tensor): the second input tensor 
<a name="l19357"><span class="ln">19357 </span></a> 
<a name="l19358"><span class="ln">19358 </span></a>    Keyword args: 
<a name="l19359"><span class="ln">19359 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19360"><span class="ln">19360 </span></a> 
<a name="l19361"><span class="ln">19361 </span></a>    Example:: 
<a name="l19362"><span class="ln">19362 </span></a> 
<a name="l19363"><span class="ln">19363 </span></a>        &gt;&gt;&gt; a = torch.tensor((1, 2, -1)) 
<a name="l19364"><span class="ln">19364 </span></a>        &gt;&gt;&gt; b = torch.tensor((3, 0, 4)) 
<a name="l19365"><span class="ln">19365 </span></a>        &gt;&gt;&gt; torch.maximum(a, b) 
<a name="l19366"><span class="ln">19366 </span></a>        tensor([3, 2, 4]) 
<a name="l19367"><span class="ln">19367 </span></a>    &quot;&quot;&quot;</span>
<a name="l19368"><span class="ln">19368 </span></a>
<a name="l19369"><span class="ln">19369 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19370"><span class="ln">19370 </span></a><span class="s2">def </span><span class="s1">mean</span><span class="s3">(</span>
<a name="l19371"><span class="ln">19371 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19372"><span class="ln">19372 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19373"><span class="ln">19373 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19374"><span class="ln">19374 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19375"><span class="ln">19375 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19376"><span class="ln">19376 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19377"><span class="ln">19377 </span></a>    mean(input, *, dtype=None) -&gt; Tensor 
<a name="l19378"><span class="ln">19378 </span></a> 
<a name="l19379"><span class="ln">19379 </span></a>    .. note:: 
<a name="l19380"><span class="ln">19380 </span></a>        If the `input` tensor is empty, ``torch.mean()`` returns ``nan``. 
<a name="l19381"><span class="ln">19381 </span></a>        This behavior is consistent with NumPy and follows the definition 
<a name="l19382"><span class="ln">19382 </span></a>        that the mean over an empty set is undefined. 
<a name="l19383"><span class="ln">19383 </span></a> 
<a name="l19384"><span class="ln">19384 </span></a> 
<a name="l19385"><span class="ln">19385 </span></a>    Returns the mean value of all elements in the :attr:`input` tensor. Input must be floating point or complex. 
<a name="l19386"><span class="ln">19386 </span></a> 
<a name="l19387"><span class="ln">19387 </span></a>    Args: 
<a name="l19388"><span class="ln">19388 </span></a>        input (Tensor): 
<a name="l19389"><span class="ln">19389 </span></a>          the input tensor, either of floating point or complex dtype 
<a name="l19390"><span class="ln">19390 </span></a> 
<a name="l19391"><span class="ln">19391 </span></a>    Keyword args: 
<a name="l19392"><span class="ln">19392 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l19393"><span class="ln">19393 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l19394"><span class="ln">19394 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l19395"><span class="ln">19395 </span></a> 
<a name="l19396"><span class="ln">19396 </span></a>    Example:: 
<a name="l19397"><span class="ln">19397 </span></a> 
<a name="l19398"><span class="ln">19398 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19399"><span class="ln">19399 </span></a>        &gt;&gt;&gt; a 
<a name="l19400"><span class="ln">19400 </span></a>        tensor([[ 0.2294, -0.5481,  1.3288]]) 
<a name="l19401"><span class="ln">19401 </span></a>        &gt;&gt;&gt; torch.mean(a) 
<a name="l19402"><span class="ln">19402 </span></a>        tensor(0.3367) 
<a name="l19403"><span class="ln">19403 </span></a> 
<a name="l19404"><span class="ln">19404 </span></a>    .. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -&gt; Tensor 
<a name="l19405"><span class="ln">19405 </span></a>       :noindex: 
<a name="l19406"><span class="ln">19406 </span></a> 
<a name="l19407"><span class="ln">19407 </span></a>    Returns the mean value of each row of the :attr:`input` tensor in the given 
<a name="l19408"><span class="ln">19408 </span></a>    dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l19409"><span class="ln">19409 </span></a>    reduce over all of them. 
<a name="l19410"><span class="ln">19410 </span></a> 
<a name="l19411"><span class="ln">19411 </span></a> 
<a name="l19412"><span class="ln">19412 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l19413"><span class="ln">19413 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l19414"><span class="ln">19414 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l19415"><span class="ln">19415 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l19416"><span class="ln">19416 </span></a> 
<a name="l19417"><span class="ln">19417 </span></a> 
<a name="l19418"><span class="ln">19418 </span></a>    Args: 
<a name="l19419"><span class="ln">19419 </span></a>        input (Tensor): the input tensor. 
<a name="l19420"><span class="ln">19420 </span></a> 
<a name="l19421"><span class="ln">19421 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l19422"><span class="ln">19422 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19423"><span class="ln">19423 </span></a> 
<a name="l19424"><span class="ln">19424 </span></a> 
<a name="l19425"><span class="ln">19425 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19426"><span class="ln">19426 </span></a> 
<a name="l19427"><span class="ln">19427 </span></a> 
<a name="l19428"><span class="ln">19428 </span></a>    Keyword args: 
<a name="l19429"><span class="ln">19429 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l19430"><span class="ln">19430 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l19431"><span class="ln">19431 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l19432"><span class="ln">19432 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19433"><span class="ln">19433 </span></a> 
<a name="l19434"><span class="ln">19434 </span></a>    .. seealso:: 
<a name="l19435"><span class="ln">19435 </span></a> 
<a name="l19436"><span class="ln">19436 </span></a>        :func:`torch.nanmean` computes the mean value of `non-NaN` elements. 
<a name="l19437"><span class="ln">19437 </span></a> 
<a name="l19438"><span class="ln">19438 </span></a>    Example:: 
<a name="l19439"><span class="ln">19439 </span></a> 
<a name="l19440"><span class="ln">19440 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19441"><span class="ln">19441 </span></a>        &gt;&gt;&gt; a 
<a name="l19442"><span class="ln">19442 </span></a>        tensor([[-0.3841,  0.6320,  0.4254, -0.7384], 
<a name="l19443"><span class="ln">19443 </span></a>                [-0.9644,  1.0131, -0.6549, -1.4279], 
<a name="l19444"><span class="ln">19444 </span></a>                [-0.2951, -1.3350, -0.7694,  0.5600], 
<a name="l19445"><span class="ln">19445 </span></a>                [ 1.0842, -0.9580,  0.3623,  0.2343]]) 
<a name="l19446"><span class="ln">19446 </span></a>        &gt;&gt;&gt; torch.mean(a, 1) 
<a name="l19447"><span class="ln">19447 </span></a>        tensor([-0.0163, -0.5085, -0.4599,  0.1807]) 
<a name="l19448"><span class="ln">19448 </span></a>        &gt;&gt;&gt; torch.mean(a, 1, True) 
<a name="l19449"><span class="ln">19449 </span></a>        tensor([[-0.0163], 
<a name="l19450"><span class="ln">19450 </span></a>                [-0.5085], 
<a name="l19451"><span class="ln">19451 </span></a>                [-0.4599], 
<a name="l19452"><span class="ln">19452 </span></a>                [ 0.1807]]) 
<a name="l19453"><span class="ln">19453 </span></a>    &quot;&quot;&quot;</span>
<a name="l19454"><span class="ln">19454 </span></a>
<a name="l19455"><span class="ln">19455 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19456"><span class="ln">19456 </span></a><span class="s2">def </span><span class="s1">mean</span><span class="s3">(</span>
<a name="l19457"><span class="ln">19457 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19458"><span class="ln">19458 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l19459"><span class="ln">19459 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19460"><span class="ln">19460 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19461"><span class="ln">19461 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19462"><span class="ln">19462 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19463"><span class="ln">19463 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19464"><span class="ln">19464 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19465"><span class="ln">19465 </span></a>    mean(input, *, dtype=None) -&gt; Tensor 
<a name="l19466"><span class="ln">19466 </span></a> 
<a name="l19467"><span class="ln">19467 </span></a>    .. note:: 
<a name="l19468"><span class="ln">19468 </span></a>        If the `input` tensor is empty, ``torch.mean()`` returns ``nan``. 
<a name="l19469"><span class="ln">19469 </span></a>        This behavior is consistent with NumPy and follows the definition 
<a name="l19470"><span class="ln">19470 </span></a>        that the mean over an empty set is undefined. 
<a name="l19471"><span class="ln">19471 </span></a> 
<a name="l19472"><span class="ln">19472 </span></a> 
<a name="l19473"><span class="ln">19473 </span></a>    Returns the mean value of all elements in the :attr:`input` tensor. Input must be floating point or complex. 
<a name="l19474"><span class="ln">19474 </span></a> 
<a name="l19475"><span class="ln">19475 </span></a>    Args: 
<a name="l19476"><span class="ln">19476 </span></a>        input (Tensor): 
<a name="l19477"><span class="ln">19477 </span></a>          the input tensor, either of floating point or complex dtype 
<a name="l19478"><span class="ln">19478 </span></a> 
<a name="l19479"><span class="ln">19479 </span></a>    Keyword args: 
<a name="l19480"><span class="ln">19480 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l19481"><span class="ln">19481 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l19482"><span class="ln">19482 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l19483"><span class="ln">19483 </span></a> 
<a name="l19484"><span class="ln">19484 </span></a>    Example:: 
<a name="l19485"><span class="ln">19485 </span></a> 
<a name="l19486"><span class="ln">19486 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19487"><span class="ln">19487 </span></a>        &gt;&gt;&gt; a 
<a name="l19488"><span class="ln">19488 </span></a>        tensor([[ 0.2294, -0.5481,  1.3288]]) 
<a name="l19489"><span class="ln">19489 </span></a>        &gt;&gt;&gt; torch.mean(a) 
<a name="l19490"><span class="ln">19490 </span></a>        tensor(0.3367) 
<a name="l19491"><span class="ln">19491 </span></a> 
<a name="l19492"><span class="ln">19492 </span></a>    .. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -&gt; Tensor 
<a name="l19493"><span class="ln">19493 </span></a>       :noindex: 
<a name="l19494"><span class="ln">19494 </span></a> 
<a name="l19495"><span class="ln">19495 </span></a>    Returns the mean value of each row of the :attr:`input` tensor in the given 
<a name="l19496"><span class="ln">19496 </span></a>    dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l19497"><span class="ln">19497 </span></a>    reduce over all of them. 
<a name="l19498"><span class="ln">19498 </span></a> 
<a name="l19499"><span class="ln">19499 </span></a> 
<a name="l19500"><span class="ln">19500 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l19501"><span class="ln">19501 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l19502"><span class="ln">19502 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l19503"><span class="ln">19503 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l19504"><span class="ln">19504 </span></a> 
<a name="l19505"><span class="ln">19505 </span></a> 
<a name="l19506"><span class="ln">19506 </span></a>    Args: 
<a name="l19507"><span class="ln">19507 </span></a>        input (Tensor): the input tensor. 
<a name="l19508"><span class="ln">19508 </span></a> 
<a name="l19509"><span class="ln">19509 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l19510"><span class="ln">19510 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19511"><span class="ln">19511 </span></a> 
<a name="l19512"><span class="ln">19512 </span></a> 
<a name="l19513"><span class="ln">19513 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19514"><span class="ln">19514 </span></a> 
<a name="l19515"><span class="ln">19515 </span></a> 
<a name="l19516"><span class="ln">19516 </span></a>    Keyword args: 
<a name="l19517"><span class="ln">19517 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l19518"><span class="ln">19518 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l19519"><span class="ln">19519 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l19520"><span class="ln">19520 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19521"><span class="ln">19521 </span></a> 
<a name="l19522"><span class="ln">19522 </span></a>    .. seealso:: 
<a name="l19523"><span class="ln">19523 </span></a> 
<a name="l19524"><span class="ln">19524 </span></a>        :func:`torch.nanmean` computes the mean value of `non-NaN` elements. 
<a name="l19525"><span class="ln">19525 </span></a> 
<a name="l19526"><span class="ln">19526 </span></a>    Example:: 
<a name="l19527"><span class="ln">19527 </span></a> 
<a name="l19528"><span class="ln">19528 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19529"><span class="ln">19529 </span></a>        &gt;&gt;&gt; a 
<a name="l19530"><span class="ln">19530 </span></a>        tensor([[-0.3841,  0.6320,  0.4254, -0.7384], 
<a name="l19531"><span class="ln">19531 </span></a>                [-0.9644,  1.0131, -0.6549, -1.4279], 
<a name="l19532"><span class="ln">19532 </span></a>                [-0.2951, -1.3350, -0.7694,  0.5600], 
<a name="l19533"><span class="ln">19533 </span></a>                [ 1.0842, -0.9580,  0.3623,  0.2343]]) 
<a name="l19534"><span class="ln">19534 </span></a>        &gt;&gt;&gt; torch.mean(a, 1) 
<a name="l19535"><span class="ln">19535 </span></a>        tensor([-0.0163, -0.5085, -0.4599,  0.1807]) 
<a name="l19536"><span class="ln">19536 </span></a>        &gt;&gt;&gt; torch.mean(a, 1, True) 
<a name="l19537"><span class="ln">19537 </span></a>        tensor([[-0.0163], 
<a name="l19538"><span class="ln">19538 </span></a>                [-0.5085], 
<a name="l19539"><span class="ln">19539 </span></a>                [-0.4599], 
<a name="l19540"><span class="ln">19540 </span></a>                [ 0.1807]]) 
<a name="l19541"><span class="ln">19541 </span></a>    &quot;&quot;&quot;</span>
<a name="l19542"><span class="ln">19542 </span></a>
<a name="l19543"><span class="ln">19543 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19544"><span class="ln">19544 </span></a><span class="s2">def </span><span class="s1">mean</span><span class="s3">(</span>
<a name="l19545"><span class="ln">19545 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19546"><span class="ln">19546 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l19547"><span class="ln">19547 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19548"><span class="ln">19548 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19549"><span class="ln">19549 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19550"><span class="ln">19550 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19551"><span class="ln">19551 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19552"><span class="ln">19552 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19553"><span class="ln">19553 </span></a>    mean(input, *, dtype=None) -&gt; Tensor 
<a name="l19554"><span class="ln">19554 </span></a> 
<a name="l19555"><span class="ln">19555 </span></a>    .. note:: 
<a name="l19556"><span class="ln">19556 </span></a>        If the `input` tensor is empty, ``torch.mean()`` returns ``nan``. 
<a name="l19557"><span class="ln">19557 </span></a>        This behavior is consistent with NumPy and follows the definition 
<a name="l19558"><span class="ln">19558 </span></a>        that the mean over an empty set is undefined. 
<a name="l19559"><span class="ln">19559 </span></a> 
<a name="l19560"><span class="ln">19560 </span></a> 
<a name="l19561"><span class="ln">19561 </span></a>    Returns the mean value of all elements in the :attr:`input` tensor. Input must be floating point or complex. 
<a name="l19562"><span class="ln">19562 </span></a> 
<a name="l19563"><span class="ln">19563 </span></a>    Args: 
<a name="l19564"><span class="ln">19564 </span></a>        input (Tensor): 
<a name="l19565"><span class="ln">19565 </span></a>          the input tensor, either of floating point or complex dtype 
<a name="l19566"><span class="ln">19566 </span></a> 
<a name="l19567"><span class="ln">19567 </span></a>    Keyword args: 
<a name="l19568"><span class="ln">19568 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l19569"><span class="ln">19569 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l19570"><span class="ln">19570 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l19571"><span class="ln">19571 </span></a> 
<a name="l19572"><span class="ln">19572 </span></a>    Example:: 
<a name="l19573"><span class="ln">19573 </span></a> 
<a name="l19574"><span class="ln">19574 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19575"><span class="ln">19575 </span></a>        &gt;&gt;&gt; a 
<a name="l19576"><span class="ln">19576 </span></a>        tensor([[ 0.2294, -0.5481,  1.3288]]) 
<a name="l19577"><span class="ln">19577 </span></a>        &gt;&gt;&gt; torch.mean(a) 
<a name="l19578"><span class="ln">19578 </span></a>        tensor(0.3367) 
<a name="l19579"><span class="ln">19579 </span></a> 
<a name="l19580"><span class="ln">19580 </span></a>    .. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -&gt; Tensor 
<a name="l19581"><span class="ln">19581 </span></a>       :noindex: 
<a name="l19582"><span class="ln">19582 </span></a> 
<a name="l19583"><span class="ln">19583 </span></a>    Returns the mean value of each row of the :attr:`input` tensor in the given 
<a name="l19584"><span class="ln">19584 </span></a>    dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l19585"><span class="ln">19585 </span></a>    reduce over all of them. 
<a name="l19586"><span class="ln">19586 </span></a> 
<a name="l19587"><span class="ln">19587 </span></a> 
<a name="l19588"><span class="ln">19588 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l19589"><span class="ln">19589 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l19590"><span class="ln">19590 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l19591"><span class="ln">19591 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l19592"><span class="ln">19592 </span></a> 
<a name="l19593"><span class="ln">19593 </span></a> 
<a name="l19594"><span class="ln">19594 </span></a>    Args: 
<a name="l19595"><span class="ln">19595 </span></a>        input (Tensor): the input tensor. 
<a name="l19596"><span class="ln">19596 </span></a> 
<a name="l19597"><span class="ln">19597 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l19598"><span class="ln">19598 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19599"><span class="ln">19599 </span></a> 
<a name="l19600"><span class="ln">19600 </span></a> 
<a name="l19601"><span class="ln">19601 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19602"><span class="ln">19602 </span></a> 
<a name="l19603"><span class="ln">19603 </span></a> 
<a name="l19604"><span class="ln">19604 </span></a>    Keyword args: 
<a name="l19605"><span class="ln">19605 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l19606"><span class="ln">19606 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l19607"><span class="ln">19607 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l19608"><span class="ln">19608 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19609"><span class="ln">19609 </span></a> 
<a name="l19610"><span class="ln">19610 </span></a>    .. seealso:: 
<a name="l19611"><span class="ln">19611 </span></a> 
<a name="l19612"><span class="ln">19612 </span></a>        :func:`torch.nanmean` computes the mean value of `non-NaN` elements. 
<a name="l19613"><span class="ln">19613 </span></a> 
<a name="l19614"><span class="ln">19614 </span></a>    Example:: 
<a name="l19615"><span class="ln">19615 </span></a> 
<a name="l19616"><span class="ln">19616 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19617"><span class="ln">19617 </span></a>        &gt;&gt;&gt; a 
<a name="l19618"><span class="ln">19618 </span></a>        tensor([[-0.3841,  0.6320,  0.4254, -0.7384], 
<a name="l19619"><span class="ln">19619 </span></a>                [-0.9644,  1.0131, -0.6549, -1.4279], 
<a name="l19620"><span class="ln">19620 </span></a>                [-0.2951, -1.3350, -0.7694,  0.5600], 
<a name="l19621"><span class="ln">19621 </span></a>                [ 1.0842, -0.9580,  0.3623,  0.2343]]) 
<a name="l19622"><span class="ln">19622 </span></a>        &gt;&gt;&gt; torch.mean(a, 1) 
<a name="l19623"><span class="ln">19623 </span></a>        tensor([-0.0163, -0.5085, -0.4599,  0.1807]) 
<a name="l19624"><span class="ln">19624 </span></a>        &gt;&gt;&gt; torch.mean(a, 1, True) 
<a name="l19625"><span class="ln">19625 </span></a>        tensor([[-0.0163], 
<a name="l19626"><span class="ln">19626 </span></a>                [-0.5085], 
<a name="l19627"><span class="ln">19627 </span></a>                [-0.4599], 
<a name="l19628"><span class="ln">19628 </span></a>                [ 0.1807]]) 
<a name="l19629"><span class="ln">19629 </span></a>    &quot;&quot;&quot;</span>
<a name="l19630"><span class="ln">19630 </span></a>
<a name="l19631"><span class="ln">19631 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19632"><span class="ln">19632 </span></a><span class="s2">def </span><span class="s1">median</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19633"><span class="ln">19633 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19634"><span class="ln">19634 </span></a>    median(input) -&gt; Tensor 
<a name="l19635"><span class="ln">19635 </span></a> 
<a name="l19636"><span class="ln">19636 </span></a>    Returns the median of the values in :attr:`input`. 
<a name="l19637"><span class="ln">19637 </span></a> 
<a name="l19638"><span class="ln">19638 </span></a>    .. note:: 
<a name="l19639"><span class="ln">19639 </span></a>        The median is not unique for :attr:`input` tensors with an even number 
<a name="l19640"><span class="ln">19640 </span></a>        of elements. In this case the lower of the two medians is returned. To 
<a name="l19641"><span class="ln">19641 </span></a>        compute the mean of both medians, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l19642"><span class="ln">19642 </span></a> 
<a name="l19643"><span class="ln">19643 </span></a>    .. warning:: 
<a name="l19644"><span class="ln">19644 </span></a>        This function produces deterministic (sub)gradients unlike ``median(dim=0)`` 
<a name="l19645"><span class="ln">19645 </span></a> 
<a name="l19646"><span class="ln">19646 </span></a>    Args: 
<a name="l19647"><span class="ln">19647 </span></a>        input (Tensor): the input tensor. 
<a name="l19648"><span class="ln">19648 </span></a> 
<a name="l19649"><span class="ln">19649 </span></a>    Example:: 
<a name="l19650"><span class="ln">19650 </span></a> 
<a name="l19651"><span class="ln">19651 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19652"><span class="ln">19652 </span></a>        &gt;&gt;&gt; a 
<a name="l19653"><span class="ln">19653 </span></a>        tensor([[ 1.5219, -1.5212,  0.2202]]) 
<a name="l19654"><span class="ln">19654 </span></a>        &gt;&gt;&gt; torch.median(a) 
<a name="l19655"><span class="ln">19655 </span></a>        tensor(0.2202) 
<a name="l19656"><span class="ln">19656 </span></a> 
<a name="l19657"><span class="ln">19657 </span></a>    .. function:: median(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19658"><span class="ln">19658 </span></a>       :noindex: 
<a name="l19659"><span class="ln">19659 </span></a> 
<a name="l19660"><span class="ln">19660 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l19661"><span class="ln">19661 </span></a>    in the dimension :attr:`dim`, and ``indices`` contains the index of the median values found in the dimension :attr:`dim`. 
<a name="l19662"><span class="ln">19662 </span></a> 
<a name="l19663"><span class="ln">19663 </span></a>    By default, :attr:`dim` is the last dimension of the :attr:`input` tensor. 
<a name="l19664"><span class="ln">19664 </span></a> 
<a name="l19665"><span class="ln">19665 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size 
<a name="l19666"><span class="ln">19666 </span></a>    as :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l19667"><span class="ln">19667 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l19668"><span class="ln">19668 </span></a>    the outputs tensor having 1 fewer dimension than :attr:`input`. 
<a name="l19669"><span class="ln">19669 </span></a> 
<a name="l19670"><span class="ln">19670 </span></a>    .. note:: 
<a name="l19671"><span class="ln">19671 </span></a>        The median is not unique for :attr:`input` tensors with an even number 
<a name="l19672"><span class="ln">19672 </span></a>        of elements in the dimension :attr:`dim`. In this case the lower of the 
<a name="l19673"><span class="ln">19673 </span></a>        two medians is returned. To compute the mean of both medians in 
<a name="l19674"><span class="ln">19674 </span></a>        :attr:`input`, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l19675"><span class="ln">19675 </span></a> 
<a name="l19676"><span class="ln">19676 </span></a>    .. warning:: 
<a name="l19677"><span class="ln">19677 </span></a>        ``indices`` does not necessarily contain the first occurrence of each 
<a name="l19678"><span class="ln">19678 </span></a>        median value found, unless it is unique. 
<a name="l19679"><span class="ln">19679 </span></a>        The exact implementation details are device-specific. 
<a name="l19680"><span class="ln">19680 </span></a>        Do not expect the same result when run on CPU and GPU in general. 
<a name="l19681"><span class="ln">19681 </span></a>        For the same reason do not expect the gradients to be deterministic. 
<a name="l19682"><span class="ln">19682 </span></a> 
<a name="l19683"><span class="ln">19683 </span></a>    Args: 
<a name="l19684"><span class="ln">19684 </span></a>        input (Tensor): the input tensor. 
<a name="l19685"><span class="ln">19685 </span></a> 
<a name="l19686"><span class="ln">19686 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l19687"><span class="ln">19687 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19688"><span class="ln">19688 </span></a> 
<a name="l19689"><span class="ln">19689 </span></a> 
<a name="l19690"><span class="ln">19690 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19691"><span class="ln">19691 </span></a> 
<a name="l19692"><span class="ln">19692 </span></a> 
<a name="l19693"><span class="ln">19693 </span></a>    Keyword args: 
<a name="l19694"><span class="ln">19694 </span></a>        out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l19695"><span class="ln">19695 </span></a>                                          tensor, which must have dtype long, with their indices in the dimension 
<a name="l19696"><span class="ln">19696 </span></a>                                          :attr:`dim` of :attr:`input`. 
<a name="l19697"><span class="ln">19697 </span></a> 
<a name="l19698"><span class="ln">19698 </span></a>    Example:: 
<a name="l19699"><span class="ln">19699 </span></a> 
<a name="l19700"><span class="ln">19700 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 5) 
<a name="l19701"><span class="ln">19701 </span></a>        &gt;&gt;&gt; a 
<a name="l19702"><span class="ln">19702 </span></a>        tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131], 
<a name="l19703"><span class="ln">19703 </span></a>                [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270], 
<a name="l19704"><span class="ln">19704 </span></a>                [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488], 
<a name="l19705"><span class="ln">19705 </span></a>                [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]]) 
<a name="l19706"><span class="ln">19706 </span></a>        &gt;&gt;&gt; torch.median(a, 1) 
<a name="l19707"><span class="ln">19707 </span></a>        torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3])) 
<a name="l19708"><span class="ln">19708 </span></a>    &quot;&quot;&quot;</span>
<a name="l19709"><span class="ln">19709 </span></a>
<a name="l19710"><span class="ln">19710 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19711"><span class="ln">19711 </span></a><span class="s2">def </span><span class="s1">median</span><span class="s3">(</span>
<a name="l19712"><span class="ln">19712 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19713"><span class="ln">19713 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l19714"><span class="ln">19714 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19715"><span class="ln">19715 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19716"><span class="ln">19716 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19717"><span class="ln">19717 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">median</span><span class="s2">:</span>
<a name="l19718"><span class="ln">19718 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19719"><span class="ln">19719 </span></a>    median(input) -&gt; Tensor 
<a name="l19720"><span class="ln">19720 </span></a> 
<a name="l19721"><span class="ln">19721 </span></a>    Returns the median of the values in :attr:`input`. 
<a name="l19722"><span class="ln">19722 </span></a> 
<a name="l19723"><span class="ln">19723 </span></a>    .. note:: 
<a name="l19724"><span class="ln">19724 </span></a>        The median is not unique for :attr:`input` tensors with an even number 
<a name="l19725"><span class="ln">19725 </span></a>        of elements. In this case the lower of the two medians is returned. To 
<a name="l19726"><span class="ln">19726 </span></a>        compute the mean of both medians, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l19727"><span class="ln">19727 </span></a> 
<a name="l19728"><span class="ln">19728 </span></a>    .. warning:: 
<a name="l19729"><span class="ln">19729 </span></a>        This function produces deterministic (sub)gradients unlike ``median(dim=0)`` 
<a name="l19730"><span class="ln">19730 </span></a> 
<a name="l19731"><span class="ln">19731 </span></a>    Args: 
<a name="l19732"><span class="ln">19732 </span></a>        input (Tensor): the input tensor. 
<a name="l19733"><span class="ln">19733 </span></a> 
<a name="l19734"><span class="ln">19734 </span></a>    Example:: 
<a name="l19735"><span class="ln">19735 </span></a> 
<a name="l19736"><span class="ln">19736 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19737"><span class="ln">19737 </span></a>        &gt;&gt;&gt; a 
<a name="l19738"><span class="ln">19738 </span></a>        tensor([[ 1.5219, -1.5212,  0.2202]]) 
<a name="l19739"><span class="ln">19739 </span></a>        &gt;&gt;&gt; torch.median(a) 
<a name="l19740"><span class="ln">19740 </span></a>        tensor(0.2202) 
<a name="l19741"><span class="ln">19741 </span></a> 
<a name="l19742"><span class="ln">19742 </span></a>    .. function:: median(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19743"><span class="ln">19743 </span></a>       :noindex: 
<a name="l19744"><span class="ln">19744 </span></a> 
<a name="l19745"><span class="ln">19745 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l19746"><span class="ln">19746 </span></a>    in the dimension :attr:`dim`, and ``indices`` contains the index of the median values found in the dimension :attr:`dim`. 
<a name="l19747"><span class="ln">19747 </span></a> 
<a name="l19748"><span class="ln">19748 </span></a>    By default, :attr:`dim` is the last dimension of the :attr:`input` tensor. 
<a name="l19749"><span class="ln">19749 </span></a> 
<a name="l19750"><span class="ln">19750 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size 
<a name="l19751"><span class="ln">19751 </span></a>    as :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l19752"><span class="ln">19752 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l19753"><span class="ln">19753 </span></a>    the outputs tensor having 1 fewer dimension than :attr:`input`. 
<a name="l19754"><span class="ln">19754 </span></a> 
<a name="l19755"><span class="ln">19755 </span></a>    .. note:: 
<a name="l19756"><span class="ln">19756 </span></a>        The median is not unique for :attr:`input` tensors with an even number 
<a name="l19757"><span class="ln">19757 </span></a>        of elements in the dimension :attr:`dim`. In this case the lower of the 
<a name="l19758"><span class="ln">19758 </span></a>        two medians is returned. To compute the mean of both medians in 
<a name="l19759"><span class="ln">19759 </span></a>        :attr:`input`, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l19760"><span class="ln">19760 </span></a> 
<a name="l19761"><span class="ln">19761 </span></a>    .. warning:: 
<a name="l19762"><span class="ln">19762 </span></a>        ``indices`` does not necessarily contain the first occurrence of each 
<a name="l19763"><span class="ln">19763 </span></a>        median value found, unless it is unique. 
<a name="l19764"><span class="ln">19764 </span></a>        The exact implementation details are device-specific. 
<a name="l19765"><span class="ln">19765 </span></a>        Do not expect the same result when run on CPU and GPU in general. 
<a name="l19766"><span class="ln">19766 </span></a>        For the same reason do not expect the gradients to be deterministic. 
<a name="l19767"><span class="ln">19767 </span></a> 
<a name="l19768"><span class="ln">19768 </span></a>    Args: 
<a name="l19769"><span class="ln">19769 </span></a>        input (Tensor): the input tensor. 
<a name="l19770"><span class="ln">19770 </span></a> 
<a name="l19771"><span class="ln">19771 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l19772"><span class="ln">19772 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19773"><span class="ln">19773 </span></a> 
<a name="l19774"><span class="ln">19774 </span></a> 
<a name="l19775"><span class="ln">19775 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19776"><span class="ln">19776 </span></a> 
<a name="l19777"><span class="ln">19777 </span></a> 
<a name="l19778"><span class="ln">19778 </span></a>    Keyword args: 
<a name="l19779"><span class="ln">19779 </span></a>        out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l19780"><span class="ln">19780 </span></a>                                          tensor, which must have dtype long, with their indices in the dimension 
<a name="l19781"><span class="ln">19781 </span></a>                                          :attr:`dim` of :attr:`input`. 
<a name="l19782"><span class="ln">19782 </span></a> 
<a name="l19783"><span class="ln">19783 </span></a>    Example:: 
<a name="l19784"><span class="ln">19784 </span></a> 
<a name="l19785"><span class="ln">19785 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 5) 
<a name="l19786"><span class="ln">19786 </span></a>        &gt;&gt;&gt; a 
<a name="l19787"><span class="ln">19787 </span></a>        tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131], 
<a name="l19788"><span class="ln">19788 </span></a>                [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270], 
<a name="l19789"><span class="ln">19789 </span></a>                [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488], 
<a name="l19790"><span class="ln">19790 </span></a>                [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]]) 
<a name="l19791"><span class="ln">19791 </span></a>        &gt;&gt;&gt; torch.median(a, 1) 
<a name="l19792"><span class="ln">19792 </span></a>        torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3])) 
<a name="l19793"><span class="ln">19793 </span></a>    &quot;&quot;&quot;</span>
<a name="l19794"><span class="ln">19794 </span></a>
<a name="l19795"><span class="ln">19795 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19796"><span class="ln">19796 </span></a><span class="s2">def </span><span class="s1">median</span><span class="s3">(</span>
<a name="l19797"><span class="ln">19797 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19798"><span class="ln">19798 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l19799"><span class="ln">19799 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l19800"><span class="ln">19800 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19801"><span class="ln">19801 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19802"><span class="ln">19802 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">median</span><span class="s2">:</span>
<a name="l19803"><span class="ln">19803 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19804"><span class="ln">19804 </span></a>    median(input) -&gt; Tensor 
<a name="l19805"><span class="ln">19805 </span></a> 
<a name="l19806"><span class="ln">19806 </span></a>    Returns the median of the values in :attr:`input`. 
<a name="l19807"><span class="ln">19807 </span></a> 
<a name="l19808"><span class="ln">19808 </span></a>    .. note:: 
<a name="l19809"><span class="ln">19809 </span></a>        The median is not unique for :attr:`input` tensors with an even number 
<a name="l19810"><span class="ln">19810 </span></a>        of elements. In this case the lower of the two medians is returned. To 
<a name="l19811"><span class="ln">19811 </span></a>        compute the mean of both medians, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l19812"><span class="ln">19812 </span></a> 
<a name="l19813"><span class="ln">19813 </span></a>    .. warning:: 
<a name="l19814"><span class="ln">19814 </span></a>        This function produces deterministic (sub)gradients unlike ``median(dim=0)`` 
<a name="l19815"><span class="ln">19815 </span></a> 
<a name="l19816"><span class="ln">19816 </span></a>    Args: 
<a name="l19817"><span class="ln">19817 </span></a>        input (Tensor): the input tensor. 
<a name="l19818"><span class="ln">19818 </span></a> 
<a name="l19819"><span class="ln">19819 </span></a>    Example:: 
<a name="l19820"><span class="ln">19820 </span></a> 
<a name="l19821"><span class="ln">19821 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19822"><span class="ln">19822 </span></a>        &gt;&gt;&gt; a 
<a name="l19823"><span class="ln">19823 </span></a>        tensor([[ 1.5219, -1.5212,  0.2202]]) 
<a name="l19824"><span class="ln">19824 </span></a>        &gt;&gt;&gt; torch.median(a) 
<a name="l19825"><span class="ln">19825 </span></a>        tensor(0.2202) 
<a name="l19826"><span class="ln">19826 </span></a> 
<a name="l19827"><span class="ln">19827 </span></a>    .. function:: median(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19828"><span class="ln">19828 </span></a>       :noindex: 
<a name="l19829"><span class="ln">19829 </span></a> 
<a name="l19830"><span class="ln">19830 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l19831"><span class="ln">19831 </span></a>    in the dimension :attr:`dim`, and ``indices`` contains the index of the median values found in the dimension :attr:`dim`. 
<a name="l19832"><span class="ln">19832 </span></a> 
<a name="l19833"><span class="ln">19833 </span></a>    By default, :attr:`dim` is the last dimension of the :attr:`input` tensor. 
<a name="l19834"><span class="ln">19834 </span></a> 
<a name="l19835"><span class="ln">19835 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size 
<a name="l19836"><span class="ln">19836 </span></a>    as :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l19837"><span class="ln">19837 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l19838"><span class="ln">19838 </span></a>    the outputs tensor having 1 fewer dimension than :attr:`input`. 
<a name="l19839"><span class="ln">19839 </span></a> 
<a name="l19840"><span class="ln">19840 </span></a>    .. note:: 
<a name="l19841"><span class="ln">19841 </span></a>        The median is not unique for :attr:`input` tensors with an even number 
<a name="l19842"><span class="ln">19842 </span></a>        of elements in the dimension :attr:`dim`. In this case the lower of the 
<a name="l19843"><span class="ln">19843 </span></a>        two medians is returned. To compute the mean of both medians in 
<a name="l19844"><span class="ln">19844 </span></a>        :attr:`input`, use :func:`torch.quantile` with ``q=0.5`` instead. 
<a name="l19845"><span class="ln">19845 </span></a> 
<a name="l19846"><span class="ln">19846 </span></a>    .. warning:: 
<a name="l19847"><span class="ln">19847 </span></a>        ``indices`` does not necessarily contain the first occurrence of each 
<a name="l19848"><span class="ln">19848 </span></a>        median value found, unless it is unique. 
<a name="l19849"><span class="ln">19849 </span></a>        The exact implementation details are device-specific. 
<a name="l19850"><span class="ln">19850 </span></a>        Do not expect the same result when run on CPU and GPU in general. 
<a name="l19851"><span class="ln">19851 </span></a>        For the same reason do not expect the gradients to be deterministic. 
<a name="l19852"><span class="ln">19852 </span></a> 
<a name="l19853"><span class="ln">19853 </span></a>    Args: 
<a name="l19854"><span class="ln">19854 </span></a>        input (Tensor): the input tensor. 
<a name="l19855"><span class="ln">19855 </span></a> 
<a name="l19856"><span class="ln">19856 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l19857"><span class="ln">19857 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19858"><span class="ln">19858 </span></a> 
<a name="l19859"><span class="ln">19859 </span></a> 
<a name="l19860"><span class="ln">19860 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19861"><span class="ln">19861 </span></a> 
<a name="l19862"><span class="ln">19862 </span></a> 
<a name="l19863"><span class="ln">19863 </span></a>    Keyword args: 
<a name="l19864"><span class="ln">19864 </span></a>        out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l19865"><span class="ln">19865 </span></a>                                          tensor, which must have dtype long, with their indices in the dimension 
<a name="l19866"><span class="ln">19866 </span></a>                                          :attr:`dim` of :attr:`input`. 
<a name="l19867"><span class="ln">19867 </span></a> 
<a name="l19868"><span class="ln">19868 </span></a>    Example:: 
<a name="l19869"><span class="ln">19869 </span></a> 
<a name="l19870"><span class="ln">19870 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 5) 
<a name="l19871"><span class="ln">19871 </span></a>        &gt;&gt;&gt; a 
<a name="l19872"><span class="ln">19872 </span></a>        tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131], 
<a name="l19873"><span class="ln">19873 </span></a>                [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270], 
<a name="l19874"><span class="ln">19874 </span></a>                [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488], 
<a name="l19875"><span class="ln">19875 </span></a>                [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]]) 
<a name="l19876"><span class="ln">19876 </span></a>        &gt;&gt;&gt; torch.median(a, 1) 
<a name="l19877"><span class="ln">19877 </span></a>        torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3])) 
<a name="l19878"><span class="ln">19878 </span></a>    &quot;&quot;&quot;</span>
<a name="l19879"><span class="ln">19879 </span></a>
<a name="l19880"><span class="ln">19880 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19881"><span class="ln">19881 </span></a><span class="s2">def </span><span class="s1">min</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19882"><span class="ln">19882 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19883"><span class="ln">19883 </span></a>    min(input, *, out=None) -&gt; Tensor 
<a name="l19884"><span class="ln">19884 </span></a> 
<a name="l19885"><span class="ln">19885 </span></a>    Returns the minimum value of all elements in the :attr:`input` tensor. 
<a name="l19886"><span class="ln">19886 </span></a> 
<a name="l19887"><span class="ln">19887 </span></a>    Args: 
<a name="l19888"><span class="ln">19888 </span></a>        input (Tensor): the input tensor. 
<a name="l19889"><span class="ln">19889 </span></a> 
<a name="l19890"><span class="ln">19890 </span></a>    Keyword args: 
<a name="l19891"><span class="ln">19891 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19892"><span class="ln">19892 </span></a> 
<a name="l19893"><span class="ln">19893 </span></a>    Example:: 
<a name="l19894"><span class="ln">19894 </span></a> 
<a name="l19895"><span class="ln">19895 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19896"><span class="ln">19896 </span></a>        &gt;&gt;&gt; a 
<a name="l19897"><span class="ln">19897 </span></a>        tensor([[ 0.6750,  1.0857,  1.7197]]) 
<a name="l19898"><span class="ln">19898 </span></a>        &gt;&gt;&gt; torch.min(a) 
<a name="l19899"><span class="ln">19899 </span></a>        tensor(0.6750) 
<a name="l19900"><span class="ln">19900 </span></a> 
<a name="l19901"><span class="ln">19901 </span></a>    .. function:: min(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19902"><span class="ln">19902 </span></a>       :noindex: 
<a name="l19903"><span class="ln">19903 </span></a> 
<a name="l19904"><span class="ln">19904 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum 
<a name="l19905"><span class="ln">19905 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l19906"><span class="ln">19906 </span></a>    :attr:`dim`. And ``indices`` is the index location of each minimum value found 
<a name="l19907"><span class="ln">19907 </span></a>    (argmin). 
<a name="l19908"><span class="ln">19908 </span></a> 
<a name="l19909"><span class="ln">19909 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l19910"><span class="ln">19910 </span></a>    :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l19911"><span class="ln">19911 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l19912"><span class="ln">19912 </span></a>    the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l19913"><span class="ln">19913 </span></a> 
<a name="l19914"><span class="ln">19914 </span></a>    .. note:: If there are multiple minimal values in a reduced row then 
<a name="l19915"><span class="ln">19915 </span></a>              the indices of the first minimal value are returned. 
<a name="l19916"><span class="ln">19916 </span></a> 
<a name="l19917"><span class="ln">19917 </span></a>    Args: 
<a name="l19918"><span class="ln">19918 </span></a>        input (Tensor): the input tensor. 
<a name="l19919"><span class="ln">19919 </span></a> 
<a name="l19920"><span class="ln">19920 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l19921"><span class="ln">19921 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19922"><span class="ln">19922 </span></a> 
<a name="l19923"><span class="ln">19923 </span></a> 
<a name="l19924"><span class="ln">19924 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19925"><span class="ln">19925 </span></a> 
<a name="l19926"><span class="ln">19926 </span></a> 
<a name="l19927"><span class="ln">19927 </span></a>    Keyword args: 
<a name="l19928"><span class="ln">19928 </span></a>        out (tuple, optional): the tuple of two output tensors (min, min_indices) 
<a name="l19929"><span class="ln">19929 </span></a> 
<a name="l19930"><span class="ln">19930 </span></a>    Example:: 
<a name="l19931"><span class="ln">19931 </span></a> 
<a name="l19932"><span class="ln">19932 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l19933"><span class="ln">19933 </span></a>        &gt;&gt;&gt; a 
<a name="l19934"><span class="ln">19934 </span></a>        tensor([[-0.6248,  1.1334, -1.1899, -0.2803], 
<a name="l19935"><span class="ln">19935 </span></a>                [-1.4644, -0.2635, -0.3651,  0.6134], 
<a name="l19936"><span class="ln">19936 </span></a>                [ 0.2457,  0.0384,  1.0128,  0.7015], 
<a name="l19937"><span class="ln">19937 </span></a>                [-0.1153,  2.9849,  2.1458,  0.5788]]) 
<a name="l19938"><span class="ln">19938 </span></a>        &gt;&gt;&gt; torch.min(a, 1) 
<a name="l19939"><span class="ln">19939 </span></a>        torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0])) 
<a name="l19940"><span class="ln">19940 </span></a> 
<a name="l19941"><span class="ln">19941 </span></a>    .. function:: min(input, other, *, out=None) -&gt; Tensor 
<a name="l19942"><span class="ln">19942 </span></a>       :noindex: 
<a name="l19943"><span class="ln">19943 </span></a> 
<a name="l19944"><span class="ln">19944 </span></a>    See :func:`torch.minimum`. 
<a name="l19945"><span class="ln">19945 </span></a>    &quot;&quot;&quot;</span>
<a name="l19946"><span class="ln">19946 </span></a>
<a name="l19947"><span class="ln">19947 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l19948"><span class="ln">19948 </span></a><span class="s2">def </span><span class="s1">min</span><span class="s3">(</span>
<a name="l19949"><span class="ln">19949 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19950"><span class="ln">19950 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l19951"><span class="ln">19951 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l19952"><span class="ln">19952 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l19953"><span class="ln">19953 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l19954"><span class="ln">19954 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l19955"><span class="ln">19955 </span></a>    min(input, *, out=None) -&gt; Tensor 
<a name="l19956"><span class="ln">19956 </span></a> 
<a name="l19957"><span class="ln">19957 </span></a>    Returns the minimum value of all elements in the :attr:`input` tensor. 
<a name="l19958"><span class="ln">19958 </span></a> 
<a name="l19959"><span class="ln">19959 </span></a>    Args: 
<a name="l19960"><span class="ln">19960 </span></a>        input (Tensor): the input tensor. 
<a name="l19961"><span class="ln">19961 </span></a> 
<a name="l19962"><span class="ln">19962 </span></a>    Keyword args: 
<a name="l19963"><span class="ln">19963 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l19964"><span class="ln">19964 </span></a> 
<a name="l19965"><span class="ln">19965 </span></a>    Example:: 
<a name="l19966"><span class="ln">19966 </span></a> 
<a name="l19967"><span class="ln">19967 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l19968"><span class="ln">19968 </span></a>        &gt;&gt;&gt; a 
<a name="l19969"><span class="ln">19969 </span></a>        tensor([[ 0.6750,  1.0857,  1.7197]]) 
<a name="l19970"><span class="ln">19970 </span></a>        &gt;&gt;&gt; torch.min(a) 
<a name="l19971"><span class="ln">19971 </span></a>        tensor(0.6750) 
<a name="l19972"><span class="ln">19972 </span></a> 
<a name="l19973"><span class="ln">19973 </span></a>    .. function:: min(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l19974"><span class="ln">19974 </span></a>       :noindex: 
<a name="l19975"><span class="ln">19975 </span></a> 
<a name="l19976"><span class="ln">19976 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum 
<a name="l19977"><span class="ln">19977 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l19978"><span class="ln">19978 </span></a>    :attr:`dim`. And ``indices`` is the index location of each minimum value found 
<a name="l19979"><span class="ln">19979 </span></a>    (argmin). 
<a name="l19980"><span class="ln">19980 </span></a> 
<a name="l19981"><span class="ln">19981 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l19982"><span class="ln">19982 </span></a>    :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l19983"><span class="ln">19983 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l19984"><span class="ln">19984 </span></a>    the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l19985"><span class="ln">19985 </span></a> 
<a name="l19986"><span class="ln">19986 </span></a>    .. note:: If there are multiple minimal values in a reduced row then 
<a name="l19987"><span class="ln">19987 </span></a>              the indices of the first minimal value are returned. 
<a name="l19988"><span class="ln">19988 </span></a> 
<a name="l19989"><span class="ln">19989 </span></a>    Args: 
<a name="l19990"><span class="ln">19990 </span></a>        input (Tensor): the input tensor. 
<a name="l19991"><span class="ln">19991 </span></a> 
<a name="l19992"><span class="ln">19992 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l19993"><span class="ln">19993 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l19994"><span class="ln">19994 </span></a> 
<a name="l19995"><span class="ln">19995 </span></a> 
<a name="l19996"><span class="ln">19996 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l19997"><span class="ln">19997 </span></a> 
<a name="l19998"><span class="ln">19998 </span></a> 
<a name="l19999"><span class="ln">19999 </span></a>    Keyword args: 
<a name="l20000"><span class="ln">20000 </span></a>        out (tuple, optional): the tuple of two output tensors (min, min_indices) 
<a name="l20001"><span class="ln">20001 </span></a> 
<a name="l20002"><span class="ln">20002 </span></a>    Example:: 
<a name="l20003"><span class="ln">20003 </span></a> 
<a name="l20004"><span class="ln">20004 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l20005"><span class="ln">20005 </span></a>        &gt;&gt;&gt; a 
<a name="l20006"><span class="ln">20006 </span></a>        tensor([[-0.6248,  1.1334, -1.1899, -0.2803], 
<a name="l20007"><span class="ln">20007 </span></a>                [-1.4644, -0.2635, -0.3651,  0.6134], 
<a name="l20008"><span class="ln">20008 </span></a>                [ 0.2457,  0.0384,  1.0128,  0.7015], 
<a name="l20009"><span class="ln">20009 </span></a>                [-0.1153,  2.9849,  2.1458,  0.5788]]) 
<a name="l20010"><span class="ln">20010 </span></a>        &gt;&gt;&gt; torch.min(a, 1) 
<a name="l20011"><span class="ln">20011 </span></a>        torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0])) 
<a name="l20012"><span class="ln">20012 </span></a> 
<a name="l20013"><span class="ln">20013 </span></a>    .. function:: min(input, other, *, out=None) -&gt; Tensor 
<a name="l20014"><span class="ln">20014 </span></a>       :noindex: 
<a name="l20015"><span class="ln">20015 </span></a> 
<a name="l20016"><span class="ln">20016 </span></a>    See :func:`torch.minimum`. 
<a name="l20017"><span class="ln">20017 </span></a>    &quot;&quot;&quot;</span>
<a name="l20018"><span class="ln">20018 </span></a>
<a name="l20019"><span class="ln">20019 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20020"><span class="ln">20020 </span></a><span class="s2">def </span><span class="s1">min</span><span class="s3">(</span>
<a name="l20021"><span class="ln">20021 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20022"><span class="ln">20022 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20023"><span class="ln">20023 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20024"><span class="ln">20024 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20025"><span class="ln">20025 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20026"><span class="ln">20026 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">min</span><span class="s2">:</span>
<a name="l20027"><span class="ln">20027 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20028"><span class="ln">20028 </span></a>    min(input, *, out=None) -&gt; Tensor 
<a name="l20029"><span class="ln">20029 </span></a> 
<a name="l20030"><span class="ln">20030 </span></a>    Returns the minimum value of all elements in the :attr:`input` tensor. 
<a name="l20031"><span class="ln">20031 </span></a> 
<a name="l20032"><span class="ln">20032 </span></a>    Args: 
<a name="l20033"><span class="ln">20033 </span></a>        input (Tensor): the input tensor. 
<a name="l20034"><span class="ln">20034 </span></a> 
<a name="l20035"><span class="ln">20035 </span></a>    Keyword args: 
<a name="l20036"><span class="ln">20036 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20037"><span class="ln">20037 </span></a> 
<a name="l20038"><span class="ln">20038 </span></a>    Example:: 
<a name="l20039"><span class="ln">20039 </span></a> 
<a name="l20040"><span class="ln">20040 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l20041"><span class="ln">20041 </span></a>        &gt;&gt;&gt; a 
<a name="l20042"><span class="ln">20042 </span></a>        tensor([[ 0.6750,  1.0857,  1.7197]]) 
<a name="l20043"><span class="ln">20043 </span></a>        &gt;&gt;&gt; torch.min(a) 
<a name="l20044"><span class="ln">20044 </span></a>        tensor(0.6750) 
<a name="l20045"><span class="ln">20045 </span></a> 
<a name="l20046"><span class="ln">20046 </span></a>    .. function:: min(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l20047"><span class="ln">20047 </span></a>       :noindex: 
<a name="l20048"><span class="ln">20048 </span></a> 
<a name="l20049"><span class="ln">20049 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum 
<a name="l20050"><span class="ln">20050 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l20051"><span class="ln">20051 </span></a>    :attr:`dim`. And ``indices`` is the index location of each minimum value found 
<a name="l20052"><span class="ln">20052 </span></a>    (argmin). 
<a name="l20053"><span class="ln">20053 </span></a> 
<a name="l20054"><span class="ln">20054 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l20055"><span class="ln">20055 </span></a>    :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l20056"><span class="ln">20056 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l20057"><span class="ln">20057 </span></a>    the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l20058"><span class="ln">20058 </span></a> 
<a name="l20059"><span class="ln">20059 </span></a>    .. note:: If there are multiple minimal values in a reduced row then 
<a name="l20060"><span class="ln">20060 </span></a>              the indices of the first minimal value are returned. 
<a name="l20061"><span class="ln">20061 </span></a> 
<a name="l20062"><span class="ln">20062 </span></a>    Args: 
<a name="l20063"><span class="ln">20063 </span></a>        input (Tensor): the input tensor. 
<a name="l20064"><span class="ln">20064 </span></a> 
<a name="l20065"><span class="ln">20065 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l20066"><span class="ln">20066 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l20067"><span class="ln">20067 </span></a> 
<a name="l20068"><span class="ln">20068 </span></a> 
<a name="l20069"><span class="ln">20069 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l20070"><span class="ln">20070 </span></a> 
<a name="l20071"><span class="ln">20071 </span></a> 
<a name="l20072"><span class="ln">20072 </span></a>    Keyword args: 
<a name="l20073"><span class="ln">20073 </span></a>        out (tuple, optional): the tuple of two output tensors (min, min_indices) 
<a name="l20074"><span class="ln">20074 </span></a> 
<a name="l20075"><span class="ln">20075 </span></a>    Example:: 
<a name="l20076"><span class="ln">20076 </span></a> 
<a name="l20077"><span class="ln">20077 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l20078"><span class="ln">20078 </span></a>        &gt;&gt;&gt; a 
<a name="l20079"><span class="ln">20079 </span></a>        tensor([[-0.6248,  1.1334, -1.1899, -0.2803], 
<a name="l20080"><span class="ln">20080 </span></a>                [-1.4644, -0.2635, -0.3651,  0.6134], 
<a name="l20081"><span class="ln">20081 </span></a>                [ 0.2457,  0.0384,  1.0128,  0.7015], 
<a name="l20082"><span class="ln">20082 </span></a>                [-0.1153,  2.9849,  2.1458,  0.5788]]) 
<a name="l20083"><span class="ln">20083 </span></a>        &gt;&gt;&gt; torch.min(a, 1) 
<a name="l20084"><span class="ln">20084 </span></a>        torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0])) 
<a name="l20085"><span class="ln">20085 </span></a> 
<a name="l20086"><span class="ln">20086 </span></a>    .. function:: min(input, other, *, out=None) -&gt; Tensor 
<a name="l20087"><span class="ln">20087 </span></a>       :noindex: 
<a name="l20088"><span class="ln">20088 </span></a> 
<a name="l20089"><span class="ln">20089 </span></a>    See :func:`torch.minimum`. 
<a name="l20090"><span class="ln">20090 </span></a>    &quot;&quot;&quot;</span>
<a name="l20091"><span class="ln">20091 </span></a>
<a name="l20092"><span class="ln">20092 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20093"><span class="ln">20093 </span></a><span class="s2">def </span><span class="s1">min</span><span class="s3">(</span>
<a name="l20094"><span class="ln">20094 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20095"><span class="ln">20095 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20096"><span class="ln">20096 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20097"><span class="ln">20097 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20098"><span class="ln">20098 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20099"><span class="ln">20099 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">min</span><span class="s2">:</span>
<a name="l20100"><span class="ln">20100 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20101"><span class="ln">20101 </span></a>    min(input, *, out=None) -&gt; Tensor 
<a name="l20102"><span class="ln">20102 </span></a> 
<a name="l20103"><span class="ln">20103 </span></a>    Returns the minimum value of all elements in the :attr:`input` tensor. 
<a name="l20104"><span class="ln">20104 </span></a> 
<a name="l20105"><span class="ln">20105 </span></a>    Args: 
<a name="l20106"><span class="ln">20106 </span></a>        input (Tensor): the input tensor. 
<a name="l20107"><span class="ln">20107 </span></a> 
<a name="l20108"><span class="ln">20108 </span></a>    Keyword args: 
<a name="l20109"><span class="ln">20109 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20110"><span class="ln">20110 </span></a> 
<a name="l20111"><span class="ln">20111 </span></a>    Example:: 
<a name="l20112"><span class="ln">20112 </span></a> 
<a name="l20113"><span class="ln">20113 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l20114"><span class="ln">20114 </span></a>        &gt;&gt;&gt; a 
<a name="l20115"><span class="ln">20115 </span></a>        tensor([[ 0.6750,  1.0857,  1.7197]]) 
<a name="l20116"><span class="ln">20116 </span></a>        &gt;&gt;&gt; torch.min(a) 
<a name="l20117"><span class="ln">20117 </span></a>        tensor(0.6750) 
<a name="l20118"><span class="ln">20118 </span></a> 
<a name="l20119"><span class="ln">20119 </span></a>    .. function:: min(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l20120"><span class="ln">20120 </span></a>       :noindex: 
<a name="l20121"><span class="ln">20121 </span></a> 
<a name="l20122"><span class="ln">20122 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum 
<a name="l20123"><span class="ln">20123 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l20124"><span class="ln">20124 </span></a>    :attr:`dim`. And ``indices`` is the index location of each minimum value found 
<a name="l20125"><span class="ln">20125 </span></a>    (argmin). 
<a name="l20126"><span class="ln">20126 </span></a> 
<a name="l20127"><span class="ln">20127 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l20128"><span class="ln">20128 </span></a>    :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l20129"><span class="ln">20129 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l20130"><span class="ln">20130 </span></a>    the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l20131"><span class="ln">20131 </span></a> 
<a name="l20132"><span class="ln">20132 </span></a>    .. note:: If there are multiple minimal values in a reduced row then 
<a name="l20133"><span class="ln">20133 </span></a>              the indices of the first minimal value are returned. 
<a name="l20134"><span class="ln">20134 </span></a> 
<a name="l20135"><span class="ln">20135 </span></a>    Args: 
<a name="l20136"><span class="ln">20136 </span></a>        input (Tensor): the input tensor. 
<a name="l20137"><span class="ln">20137 </span></a> 
<a name="l20138"><span class="ln">20138 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l20139"><span class="ln">20139 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l20140"><span class="ln">20140 </span></a> 
<a name="l20141"><span class="ln">20141 </span></a> 
<a name="l20142"><span class="ln">20142 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l20143"><span class="ln">20143 </span></a> 
<a name="l20144"><span class="ln">20144 </span></a> 
<a name="l20145"><span class="ln">20145 </span></a>    Keyword args: 
<a name="l20146"><span class="ln">20146 </span></a>        out (tuple, optional): the tuple of two output tensors (min, min_indices) 
<a name="l20147"><span class="ln">20147 </span></a> 
<a name="l20148"><span class="ln">20148 </span></a>    Example:: 
<a name="l20149"><span class="ln">20149 </span></a> 
<a name="l20150"><span class="ln">20150 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l20151"><span class="ln">20151 </span></a>        &gt;&gt;&gt; a 
<a name="l20152"><span class="ln">20152 </span></a>        tensor([[-0.6248,  1.1334, -1.1899, -0.2803], 
<a name="l20153"><span class="ln">20153 </span></a>                [-1.4644, -0.2635, -0.3651,  0.6134], 
<a name="l20154"><span class="ln">20154 </span></a>                [ 0.2457,  0.0384,  1.0128,  0.7015], 
<a name="l20155"><span class="ln">20155 </span></a>                [-0.1153,  2.9849,  2.1458,  0.5788]]) 
<a name="l20156"><span class="ln">20156 </span></a>        &gt;&gt;&gt; torch.min(a, 1) 
<a name="l20157"><span class="ln">20157 </span></a>        torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0])) 
<a name="l20158"><span class="ln">20158 </span></a> 
<a name="l20159"><span class="ln">20159 </span></a>    .. function:: min(input, other, *, out=None) -&gt; Tensor 
<a name="l20160"><span class="ln">20160 </span></a>       :noindex: 
<a name="l20161"><span class="ln">20161 </span></a> 
<a name="l20162"><span class="ln">20162 </span></a>    See :func:`torch.minimum`. 
<a name="l20163"><span class="ln">20163 </span></a>    &quot;&quot;&quot;</span>
<a name="l20164"><span class="ln">20164 </span></a>
<a name="l20165"><span class="ln">20165 </span></a><span class="s2">def </span><span class="s1">minimum</span><span class="s3">(</span>
<a name="l20166"><span class="ln">20166 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20167"><span class="ln">20167 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20168"><span class="ln">20168 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20169"><span class="ln">20169 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20170"><span class="ln">20170 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20171"><span class="ln">20171 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20172"><span class="ln">20172 </span></a>    minimum(input, other, *, out=None) -&gt; Tensor 
<a name="l20173"><span class="ln">20173 </span></a> 
<a name="l20174"><span class="ln">20174 </span></a>    Computes the element-wise minimum of :attr:`input` and :attr:`other`. 
<a name="l20175"><span class="ln">20175 </span></a> 
<a name="l20176"><span class="ln">20176 </span></a>    .. note:: 
<a name="l20177"><span class="ln">20177 </span></a>        If one of the elements being compared is a NaN, then that element is returned. 
<a name="l20178"><span class="ln">20178 </span></a>        :func:`minimum` is not supported for tensors with complex dtypes. 
<a name="l20179"><span class="ln">20179 </span></a> 
<a name="l20180"><span class="ln">20180 </span></a>    Args: 
<a name="l20181"><span class="ln">20181 </span></a>        input (Tensor): the input tensor. 
<a name="l20182"><span class="ln">20182 </span></a>        other (Tensor): the second input tensor 
<a name="l20183"><span class="ln">20183 </span></a> 
<a name="l20184"><span class="ln">20184 </span></a>    Keyword args: 
<a name="l20185"><span class="ln">20185 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20186"><span class="ln">20186 </span></a> 
<a name="l20187"><span class="ln">20187 </span></a>    Example:: 
<a name="l20188"><span class="ln">20188 </span></a> 
<a name="l20189"><span class="ln">20189 </span></a>        &gt;&gt;&gt; a = torch.tensor((1, 2, -1)) 
<a name="l20190"><span class="ln">20190 </span></a>        &gt;&gt;&gt; b = torch.tensor((3, 0, 4)) 
<a name="l20191"><span class="ln">20191 </span></a>        &gt;&gt;&gt; torch.minimum(a, b) 
<a name="l20192"><span class="ln">20192 </span></a>        tensor([1, 0, -1]) 
<a name="l20193"><span class="ln">20193 </span></a>    &quot;&quot;&quot;</span>
<a name="l20194"><span class="ln">20194 </span></a>
<a name="l20195"><span class="ln">20195 </span></a><span class="s2">def </span><span class="s1">miopen_batch_norm</span><span class="s3">(</span>
<a name="l20196"><span class="ln">20196 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20197"><span class="ln">20197 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20198"><span class="ln">20198 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20199"><span class="ln">20199 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20200"><span class="ln">20200 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20201"><span class="ln">20201 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20202"><span class="ln">20202 </span></a>    <span class="s1">exponential_average_factor</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l20203"><span class="ln">20203 </span></a>    <span class="s1">epsilon</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l20204"><span class="ln">20204 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20205"><span class="ln">20205 </span></a><span class="s2">def </span><span class="s1">miopen_convolution</span><span class="s3">(</span>
<a name="l20206"><span class="ln">20206 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20207"><span class="ln">20207 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20208"><span class="ln">20208 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20209"><span class="ln">20209 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20210"><span class="ln">20210 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20211"><span class="ln">20211 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20212"><span class="ln">20212 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l20213"><span class="ln">20213 </span></a>    <span class="s1">benchmark</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20214"><span class="ln">20214 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20215"><span class="ln">20215 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20216"><span class="ln">20216 </span></a><span class="s2">def </span><span class="s1">miopen_convolution_add_relu</span><span class="s3">(</span>
<a name="l20217"><span class="ln">20217 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20218"><span class="ln">20218 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20219"><span class="ln">20219 </span></a>    <span class="s1">z</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20220"><span class="ln">20220 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20221"><span class="ln">20221 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20222"><span class="ln">20222 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20223"><span class="ln">20223 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20224"><span class="ln">20224 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20225"><span class="ln">20225 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l20226"><span class="ln">20226 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20227"><span class="ln">20227 </span></a><span class="s2">def </span><span class="s1">miopen_convolution_relu</span><span class="s3">(</span>
<a name="l20228"><span class="ln">20228 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20229"><span class="ln">20229 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20230"><span class="ln">20230 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20231"><span class="ln">20231 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20232"><span class="ln">20232 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20233"><span class="ln">20233 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20234"><span class="ln">20234 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l20235"><span class="ln">20235 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20236"><span class="ln">20236 </span></a><span class="s2">def </span><span class="s1">miopen_convolution_transpose</span><span class="s3">(</span>
<a name="l20237"><span class="ln">20237 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20238"><span class="ln">20238 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20239"><span class="ln">20239 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20240"><span class="ln">20240 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20241"><span class="ln">20241 </span></a>    <span class="s1">output_padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20242"><span class="ln">20242 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20243"><span class="ln">20243 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20244"><span class="ln">20244 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l20245"><span class="ln">20245 </span></a>    <span class="s1">benchmark</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20246"><span class="ln">20246 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20247"><span class="ln">20247 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20248"><span class="ln">20248 </span></a><span class="s2">def </span><span class="s1">miopen_depthwise_convolution</span><span class="s3">(</span>
<a name="l20249"><span class="ln">20249 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20250"><span class="ln">20250 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20251"><span class="ln">20251 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20252"><span class="ln">20252 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20253"><span class="ln">20253 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20254"><span class="ln">20254 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20255"><span class="ln">20255 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l20256"><span class="ln">20256 </span></a>    <span class="s1">benchmark</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20257"><span class="ln">20257 </span></a>    <span class="s1">deterministic</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20258"><span class="ln">20258 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20259"><span class="ln">20259 </span></a><span class="s2">def </span><span class="s1">miopen_rnn</span><span class="s3">(</span>
<a name="l20260"><span class="ln">20260 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20261"><span class="ln">20261 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20262"><span class="ln">20262 </span></a>    <span class="s1">weight_stride0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20263"><span class="ln">20263 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20264"><span class="ln">20264 </span></a>    <span class="s1">cx</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20265"><span class="ln">20265 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20266"><span class="ln">20266 </span></a>    <span class="s1">hidden_size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20267"><span class="ln">20267 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20268"><span class="ln">20268 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20269"><span class="ln">20269 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l20270"><span class="ln">20270 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20271"><span class="ln">20271 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20272"><span class="ln">20272 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l20273"><span class="ln">20273 </span></a>    <span class="s1">dropout_state</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20274"><span class="ln">20274 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20275"><span class="ln">20275 </span></a><span class="s2">def </span><span class="s1">mkldnn_adaptive_avg_pool2d</span><span class="s3">(</span>
<a name="l20276"><span class="ln">20276 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20277"><span class="ln">20277 </span></a>    <span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l20278"><span class="ln">20278 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20279"><span class="ln">20279 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20280"><span class="ln">20280 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20281"><span class="ln">20281 </span></a><span class="s2">def </span><span class="s1">mkldnn_convolution</span><span class="s3">(</span>
<a name="l20282"><span class="ln">20282 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20283"><span class="ln">20283 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20284"><span class="ln">20284 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20285"><span class="ln">20285 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20286"><span class="ln">20286 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20287"><span class="ln">20287 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l20288"><span class="ln">20288 </span></a>    <span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l20289"><span class="ln">20289 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20290"><span class="ln">20290 </span></a><span class="s2">def </span><span class="s1">mkldnn_linear_backward_weights</span><span class="s3">(</span>
<a name="l20291"><span class="ln">20291 </span></a>    <span class="s1">grad_output</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20292"><span class="ln">20292 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20293"><span class="ln">20293 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20294"><span class="ln">20294 </span></a>    <span class="s1">bias_defined</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20295"><span class="ln">20295 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20296"><span class="ln">20296 </span></a><span class="s2">def </span><span class="s1">mkldnn_max_pool2d</span><span class="s3">(</span>
<a name="l20297"><span class="ln">20297 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20298"><span class="ln">20298 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l20299"><span class="ln">20299 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l20300"><span class="ln">20300 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l20301"><span class="ln">20301 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l20302"><span class="ln">20302 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20303"><span class="ln">20303 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20304"><span class="ln">20304 </span></a><span class="s2">def </span><span class="s1">mkldnn_max_pool3d</span><span class="s3">(</span>
<a name="l20305"><span class="ln">20305 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20306"><span class="ln">20306 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l20307"><span class="ln">20307 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l20308"><span class="ln">20308 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l20309"><span class="ln">20309 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l20310"><span class="ln">20310 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20311"><span class="ln">20311 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20312"><span class="ln">20312 </span></a><span class="s2">def </span><span class="s1">mkldnn_rnn_layer</span><span class="s3">(</span>
<a name="l20313"><span class="ln">20313 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20314"><span class="ln">20314 </span></a>    <span class="s1">weight0</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20315"><span class="ln">20315 </span></a>    <span class="s1">weight1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20316"><span class="ln">20316 </span></a>    <span class="s1">weight2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20317"><span class="ln">20317 </span></a>    <span class="s1">weight3</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20318"><span class="ln">20318 </span></a>    <span class="s1">hx_</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20319"><span class="ln">20319 </span></a>    <span class="s1">cx_</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20320"><span class="ln">20320 </span></a>    <span class="s1">reverse</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20321"><span class="ln">20321 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l20322"><span class="ln">20322 </span></a>    <span class="s1">mode</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20323"><span class="ln">20323 </span></a>    <span class="s1">hidden_size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20324"><span class="ln">20324 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20325"><span class="ln">20325 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20326"><span class="ln">20326 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20327"><span class="ln">20327 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20328"><span class="ln">20328 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l20329"><span class="ln">20329 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20330"><span class="ln">20330 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20331"><span class="ln">20331 </span></a><span class="s2">def </span><span class="s1">mm</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20332"><span class="ln">20332 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20333"><span class="ln">20333 </span></a>    mm(input, mat2, out_dtype=None, *, out=None) -&gt; Tensor 
<a name="l20334"><span class="ln">20334 </span></a> 
<a name="l20335"><span class="ln">20335 </span></a>    Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`. 
<a name="l20336"><span class="ln">20336 </span></a> 
<a name="l20337"><span class="ln">20337 </span></a>    If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l20338"><span class="ln">20338 </span></a>    :math:`(m \times p)` tensor, :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l20339"><span class="ln">20339 </span></a> 
<a name="l20340"><span class="ln">20340 </span></a>    .. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l20341"><span class="ln">20341 </span></a>              For broadcasting matrix products, see :func:`torch.matmul`. 
<a name="l20342"><span class="ln">20342 </span></a> 
<a name="l20343"><span class="ln">20343 </span></a>    Supports strided and sparse 2-D tensors as inputs, autograd with 
<a name="l20344"><span class="ln">20344 </span></a>    respect to strided inputs. 
<a name="l20345"><span class="ln">20345 </span></a> 
<a name="l20346"><span class="ln">20346 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. 
<a name="l20347"><span class="ln">20347 </span></a>    If :attr:`out` is provided its layout will be used. Otherwise, the result 
<a name="l20348"><span class="ln">20348 </span></a>    layout will be deduced from that of :attr:`input`. 
<a name="l20349"><span class="ln">20349 </span></a> 
<a name="l20350"><span class="ln">20350 </span></a> 
<a name="l20351"><span class="ln">20351 </span></a>    .. warning:: 
<a name="l20352"><span class="ln">20352 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l20353"><span class="ln">20353 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l20354"><span class="ln">20354 </span></a>        open a feature request. 
<a name="l20355"><span class="ln">20355 </span></a> 
<a name="l20356"><span class="ln">20356 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l20357"><span class="ln">20357 </span></a> 
<a name="l20358"><span class="ln">20358 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l20359"><span class="ln">20359 </span></a> 
<a name="l20360"><span class="ln">20360 </span></a>    Args: 
<a name="l20361"><span class="ln">20361 </span></a>        input (Tensor): the first matrix to be matrix multiplied 
<a name="l20362"><span class="ln">20362 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l20363"><span class="ln">20363 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l20364"><span class="ln">20364 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l20365"><span class="ln">20365 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l20366"><span class="ln">20366 </span></a> 
<a name="l20367"><span class="ln">20367 </span></a>    Keyword args: 
<a name="l20368"><span class="ln">20368 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20369"><span class="ln">20369 </span></a> 
<a name="l20370"><span class="ln">20370 </span></a>    Example:: 
<a name="l20371"><span class="ln">20371 </span></a> 
<a name="l20372"><span class="ln">20372 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l20373"><span class="ln">20373 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l20374"><span class="ln">20374 </span></a>        &gt;&gt;&gt; torch.mm(mat1, mat2) 
<a name="l20375"><span class="ln">20375 </span></a>        tensor([[ 0.4851,  0.5037, -0.3633], 
<a name="l20376"><span class="ln">20376 </span></a>                [-0.0760, -3.6705,  2.4784]]) 
<a name="l20377"><span class="ln">20377 </span></a>    &quot;&quot;&quot;</span>
<a name="l20378"><span class="ln">20378 </span></a>
<a name="l20379"><span class="ln">20379 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20380"><span class="ln">20380 </span></a><span class="s2">def </span><span class="s1">mm</span><span class="s3">(</span>
<a name="l20381"><span class="ln">20381 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20382"><span class="ln">20382 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20383"><span class="ln">20383 </span></a>    <span class="s1">out_dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l20384"><span class="ln">20384 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20385"><span class="ln">20385 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20386"><span class="ln">20386 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20387"><span class="ln">20387 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20388"><span class="ln">20388 </span></a>    mm(input, mat2, out_dtype=None, *, out=None) -&gt; Tensor 
<a name="l20389"><span class="ln">20389 </span></a> 
<a name="l20390"><span class="ln">20390 </span></a>    Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`. 
<a name="l20391"><span class="ln">20391 </span></a> 
<a name="l20392"><span class="ln">20392 </span></a>    If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`mat2` is a 
<a name="l20393"><span class="ln">20393 </span></a>    :math:`(m \times p)` tensor, :attr:`out` will be a :math:`(n \times p)` tensor. 
<a name="l20394"><span class="ln">20394 </span></a> 
<a name="l20395"><span class="ln">20395 </span></a>    .. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l20396"><span class="ln">20396 </span></a>              For broadcasting matrix products, see :func:`torch.matmul`. 
<a name="l20397"><span class="ln">20397 </span></a> 
<a name="l20398"><span class="ln">20398 </span></a>    Supports strided and sparse 2-D tensors as inputs, autograd with 
<a name="l20399"><span class="ln">20399 </span></a>    respect to strided inputs. 
<a name="l20400"><span class="ln">20400 </span></a> 
<a name="l20401"><span class="ln">20401 </span></a>    This operation has support for arguments with :ref:`sparse layouts&lt;sparse-docs&gt;`. 
<a name="l20402"><span class="ln">20402 </span></a>    If :attr:`out` is provided its layout will be used. Otherwise, the result 
<a name="l20403"><span class="ln">20403 </span></a>    layout will be deduced from that of :attr:`input`. 
<a name="l20404"><span class="ln">20404 </span></a> 
<a name="l20405"><span class="ln">20405 </span></a> 
<a name="l20406"><span class="ln">20406 </span></a>    .. warning:: 
<a name="l20407"><span class="ln">20407 </span></a>        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, 
<a name="l20408"><span class="ln">20408 </span></a>        or may not have autograd support. If you notice missing functionality please 
<a name="l20409"><span class="ln">20409 </span></a>        open a feature request. 
<a name="l20410"><span class="ln">20410 </span></a> 
<a name="l20411"><span class="ln">20411 </span></a>    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`. 
<a name="l20412"><span class="ln">20412 </span></a> 
<a name="l20413"><span class="ln">20413 </span></a>    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward. 
<a name="l20414"><span class="ln">20414 </span></a> 
<a name="l20415"><span class="ln">20415 </span></a>    Args: 
<a name="l20416"><span class="ln">20416 </span></a>        input (Tensor): the first matrix to be matrix multiplied 
<a name="l20417"><span class="ln">20417 </span></a>        mat2 (Tensor): the second matrix to be matrix multiplied 
<a name="l20418"><span class="ln">20418 </span></a>        out_dtype (dtype, optional): the dtype of the output tensor, 
<a name="l20419"><span class="ln">20419 </span></a>            Supported only on CUDA and for torch.float32 given 
<a name="l20420"><span class="ln">20420 </span></a>            torch.float16/torch.bfloat16 input dtypes 
<a name="l20421"><span class="ln">20421 </span></a> 
<a name="l20422"><span class="ln">20422 </span></a>    Keyword args: 
<a name="l20423"><span class="ln">20423 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20424"><span class="ln">20424 </span></a> 
<a name="l20425"><span class="ln">20425 </span></a>    Example:: 
<a name="l20426"><span class="ln">20426 </span></a> 
<a name="l20427"><span class="ln">20427 </span></a>        &gt;&gt;&gt; mat1 = torch.randn(2, 3) 
<a name="l20428"><span class="ln">20428 </span></a>        &gt;&gt;&gt; mat2 = torch.randn(3, 3) 
<a name="l20429"><span class="ln">20429 </span></a>        &gt;&gt;&gt; torch.mm(mat1, mat2) 
<a name="l20430"><span class="ln">20430 </span></a>        tensor([[ 0.4851,  0.5037, -0.3633], 
<a name="l20431"><span class="ln">20431 </span></a>                [-0.0760, -3.6705,  2.4784]]) 
<a name="l20432"><span class="ln">20432 </span></a>    &quot;&quot;&quot;</span>
<a name="l20433"><span class="ln">20433 </span></a>
<a name="l20434"><span class="ln">20434 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20435"><span class="ln">20435 </span></a><span class="s2">def </span><span class="s1">mode</span><span class="s3">(</span>
<a name="l20436"><span class="ln">20436 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20437"><span class="ln">20437 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l20438"><span class="ln">20438 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20439"><span class="ln">20439 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20440"><span class="ln">20440 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20441"><span class="ln">20441 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">mode</span><span class="s2">:</span>
<a name="l20442"><span class="ln">20442 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20443"><span class="ln">20443 </span></a>    mode(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l20444"><span class="ln">20444 </span></a> 
<a name="l20445"><span class="ln">20445 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the mode 
<a name="l20446"><span class="ln">20446 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l20447"><span class="ln">20447 </span></a>    :attr:`dim`, i.e. a value which appears most often 
<a name="l20448"><span class="ln">20448 </span></a>    in that row, and ``indices`` is the index location of each mode value found. 
<a name="l20449"><span class="ln">20449 </span></a> 
<a name="l20450"><span class="ln">20450 </span></a>    By default, :attr:`dim` is the last dimension of the :attr:`input` tensor. 
<a name="l20451"><span class="ln">20451 </span></a> 
<a name="l20452"><span class="ln">20452 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l20453"><span class="ln">20453 </span></a>    :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l20454"><span class="ln">20454 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l20455"><span class="ln">20455 </span></a>    in the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l20456"><span class="ln">20456 </span></a> 
<a name="l20457"><span class="ln">20457 </span></a>    .. note:: This function is not defined for ``torch.cuda.Tensor`` yet. 
<a name="l20458"><span class="ln">20458 </span></a> 
<a name="l20459"><span class="ln">20459 </span></a>    Args: 
<a name="l20460"><span class="ln">20460 </span></a>        input (Tensor): the input tensor. 
<a name="l20461"><span class="ln">20461 </span></a> 
<a name="l20462"><span class="ln">20462 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l20463"><span class="ln">20463 </span></a> 
<a name="l20464"><span class="ln">20464 </span></a> 
<a name="l20465"><span class="ln">20465 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l20466"><span class="ln">20466 </span></a> 
<a name="l20467"><span class="ln">20467 </span></a> 
<a name="l20468"><span class="ln">20468 </span></a>    Keyword args: 
<a name="l20469"><span class="ln">20469 </span></a>        out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l20470"><span class="ln">20470 </span></a> 
<a name="l20471"><span class="ln">20471 </span></a>    Example:: 
<a name="l20472"><span class="ln">20472 </span></a> 
<a name="l20473"><span class="ln">20473 </span></a>        &gt;&gt;&gt; b = torch.tensor([[0, 0, 0, 2, 0, 0, 2], 
<a name="l20474"><span class="ln">20474 </span></a>        ...                   [0, 3, 0, 0, 2, 0, 1], 
<a name="l20475"><span class="ln">20475 </span></a>        ...                   [2, 2, 2, 0, 0, 0, 3], 
<a name="l20476"><span class="ln">20476 </span></a>        ...                   [2, 2, 3, 0, 1, 1, 0], 
<a name="l20477"><span class="ln">20477 </span></a>        ...                   [1, 1, 0, 0, 2, 0, 2]]) 
<a name="l20478"><span class="ln">20478 </span></a>        &gt;&gt;&gt; torch.mode(b, 0) 
<a name="l20479"><span class="ln">20479 </span></a>        torch.return_types.mode( 
<a name="l20480"><span class="ln">20480 </span></a>        values=tensor([0, 2, 0, 0, 0, 0, 2]), 
<a name="l20481"><span class="ln">20481 </span></a>        indices=tensor([1, 3, 4, 4, 2, 4, 4])) 
<a name="l20482"><span class="ln">20482 </span></a>    &quot;&quot;&quot;</span>
<a name="l20483"><span class="ln">20483 </span></a>
<a name="l20484"><span class="ln">20484 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20485"><span class="ln">20485 </span></a><span class="s2">def </span><span class="s1">mode</span><span class="s3">(</span>
<a name="l20486"><span class="ln">20486 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20487"><span class="ln">20487 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l20488"><span class="ln">20488 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20489"><span class="ln">20489 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20490"><span class="ln">20490 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20491"><span class="ln">20491 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">mode</span><span class="s2">:</span>
<a name="l20492"><span class="ln">20492 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20493"><span class="ln">20493 </span></a>    mode(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l20494"><span class="ln">20494 </span></a> 
<a name="l20495"><span class="ln">20495 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` is the mode 
<a name="l20496"><span class="ln">20496 </span></a>    value of each row of the :attr:`input` tensor in the given dimension 
<a name="l20497"><span class="ln">20497 </span></a>    :attr:`dim`, i.e. a value which appears most often 
<a name="l20498"><span class="ln">20498 </span></a>    in that row, and ``indices`` is the index location of each mode value found. 
<a name="l20499"><span class="ln">20499 </span></a> 
<a name="l20500"><span class="ln">20500 </span></a>    By default, :attr:`dim` is the last dimension of the :attr:`input` tensor. 
<a name="l20501"><span class="ln">20501 </span></a> 
<a name="l20502"><span class="ln">20502 </span></a>    If :attr:`keepdim` is ``True``, the output tensors are of the same size as 
<a name="l20503"><span class="ln">20503 </span></a>    :attr:`input` except in the dimension :attr:`dim` where they are of size 1. 
<a name="l20504"><span class="ln">20504 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting 
<a name="l20505"><span class="ln">20505 </span></a>    in the output tensors having 1 fewer dimension than :attr:`input`. 
<a name="l20506"><span class="ln">20506 </span></a> 
<a name="l20507"><span class="ln">20507 </span></a>    .. note:: This function is not defined for ``torch.cuda.Tensor`` yet. 
<a name="l20508"><span class="ln">20508 </span></a> 
<a name="l20509"><span class="ln">20509 </span></a>    Args: 
<a name="l20510"><span class="ln">20510 </span></a>        input (Tensor): the input tensor. 
<a name="l20511"><span class="ln">20511 </span></a> 
<a name="l20512"><span class="ln">20512 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l20513"><span class="ln">20513 </span></a> 
<a name="l20514"><span class="ln">20514 </span></a> 
<a name="l20515"><span class="ln">20515 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l20516"><span class="ln">20516 </span></a> 
<a name="l20517"><span class="ln">20517 </span></a> 
<a name="l20518"><span class="ln">20518 </span></a>    Keyword args: 
<a name="l20519"><span class="ln">20519 </span></a>        out (tuple, optional): the result tuple of two output tensors (values, indices) 
<a name="l20520"><span class="ln">20520 </span></a> 
<a name="l20521"><span class="ln">20521 </span></a>    Example:: 
<a name="l20522"><span class="ln">20522 </span></a> 
<a name="l20523"><span class="ln">20523 </span></a>        &gt;&gt;&gt; b = torch.tensor([[0, 0, 0, 2, 0, 0, 2], 
<a name="l20524"><span class="ln">20524 </span></a>        ...                   [0, 3, 0, 0, 2, 0, 1], 
<a name="l20525"><span class="ln">20525 </span></a>        ...                   [2, 2, 2, 0, 0, 0, 3], 
<a name="l20526"><span class="ln">20526 </span></a>        ...                   [2, 2, 3, 0, 1, 1, 0], 
<a name="l20527"><span class="ln">20527 </span></a>        ...                   [1, 1, 0, 0, 2, 0, 2]]) 
<a name="l20528"><span class="ln">20528 </span></a>        &gt;&gt;&gt; torch.mode(b, 0) 
<a name="l20529"><span class="ln">20529 </span></a>        torch.return_types.mode( 
<a name="l20530"><span class="ln">20530 </span></a>        values=tensor([0, 2, 0, 0, 0, 0, 2]), 
<a name="l20531"><span class="ln">20531 </span></a>        indices=tensor([1, 3, 4, 4, 2, 4, 4])) 
<a name="l20532"><span class="ln">20532 </span></a>    &quot;&quot;&quot;</span>
<a name="l20533"><span class="ln">20533 </span></a>
<a name="l20534"><span class="ln">20534 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20535"><span class="ln">20535 </span></a><span class="s2">def </span><span class="s1">moveaxis</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">source</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">destination</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20536"><span class="ln">20536 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20537"><span class="ln">20537 </span></a>    moveaxis(input, source, destination) -&gt; Tensor 
<a name="l20538"><span class="ln">20538 </span></a> 
<a name="l20539"><span class="ln">20539 </span></a>    Alias for :func:`torch.movedim`. 
<a name="l20540"><span class="ln">20540 </span></a> 
<a name="l20541"><span class="ln">20541 </span></a>    This function is equivalent to NumPy's moveaxis function. 
<a name="l20542"><span class="ln">20542 </span></a> 
<a name="l20543"><span class="ln">20543 </span></a>    Examples:: 
<a name="l20544"><span class="ln">20544 </span></a> 
<a name="l20545"><span class="ln">20545 </span></a>        &gt;&gt;&gt; t = torch.randn(3,2,1) 
<a name="l20546"><span class="ln">20546 </span></a>        &gt;&gt;&gt; t 
<a name="l20547"><span class="ln">20547 </span></a>        tensor([[[-0.3362], 
<a name="l20548"><span class="ln">20548 </span></a>                [-0.8437]], 
<a name="l20549"><span class="ln">20549 </span></a> 
<a name="l20550"><span class="ln">20550 </span></a>                [[-0.9627], 
<a name="l20551"><span class="ln">20551 </span></a>                [ 0.1727]], 
<a name="l20552"><span class="ln">20552 </span></a> 
<a name="l20553"><span class="ln">20553 </span></a>                [[ 0.5173], 
<a name="l20554"><span class="ln">20554 </span></a>                [-0.1398]]]) 
<a name="l20555"><span class="ln">20555 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, 1, 0).shape 
<a name="l20556"><span class="ln">20556 </span></a>        torch.Size([2, 3, 1]) 
<a name="l20557"><span class="ln">20557 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, 1, 0) 
<a name="l20558"><span class="ln">20558 </span></a>        tensor([[[-0.3362], 
<a name="l20559"><span class="ln">20559 </span></a>                [-0.9627], 
<a name="l20560"><span class="ln">20560 </span></a>                [ 0.5173]], 
<a name="l20561"><span class="ln">20561 </span></a> 
<a name="l20562"><span class="ln">20562 </span></a>                [[-0.8437], 
<a name="l20563"><span class="ln">20563 </span></a>                [ 0.1727], 
<a name="l20564"><span class="ln">20564 </span></a>                [-0.1398]]]) 
<a name="l20565"><span class="ln">20565 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1)).shape 
<a name="l20566"><span class="ln">20566 </span></a>        torch.Size([2, 1, 3]) 
<a name="l20567"><span class="ln">20567 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1)) 
<a name="l20568"><span class="ln">20568 </span></a>        tensor([[[-0.3362, -0.9627,  0.5173]], 
<a name="l20569"><span class="ln">20569 </span></a> 
<a name="l20570"><span class="ln">20570 </span></a>                [[-0.8437,  0.1727, -0.1398]]]) 
<a name="l20571"><span class="ln">20571 </span></a>    &quot;&quot;&quot;</span>
<a name="l20572"><span class="ln">20572 </span></a>
<a name="l20573"><span class="ln">20573 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20574"><span class="ln">20574 </span></a><span class="s2">def </span><span class="s1">moveaxis</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">source</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">, </span><span class="s1">destination</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20575"><span class="ln">20575 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20576"><span class="ln">20576 </span></a>    moveaxis(input, source, destination) -&gt; Tensor 
<a name="l20577"><span class="ln">20577 </span></a> 
<a name="l20578"><span class="ln">20578 </span></a>    Alias for :func:`torch.movedim`. 
<a name="l20579"><span class="ln">20579 </span></a> 
<a name="l20580"><span class="ln">20580 </span></a>    This function is equivalent to NumPy's moveaxis function. 
<a name="l20581"><span class="ln">20581 </span></a> 
<a name="l20582"><span class="ln">20582 </span></a>    Examples:: 
<a name="l20583"><span class="ln">20583 </span></a> 
<a name="l20584"><span class="ln">20584 </span></a>        &gt;&gt;&gt; t = torch.randn(3,2,1) 
<a name="l20585"><span class="ln">20585 </span></a>        &gt;&gt;&gt; t 
<a name="l20586"><span class="ln">20586 </span></a>        tensor([[[-0.3362], 
<a name="l20587"><span class="ln">20587 </span></a>                [-0.8437]], 
<a name="l20588"><span class="ln">20588 </span></a> 
<a name="l20589"><span class="ln">20589 </span></a>                [[-0.9627], 
<a name="l20590"><span class="ln">20590 </span></a>                [ 0.1727]], 
<a name="l20591"><span class="ln">20591 </span></a> 
<a name="l20592"><span class="ln">20592 </span></a>                [[ 0.5173], 
<a name="l20593"><span class="ln">20593 </span></a>                [-0.1398]]]) 
<a name="l20594"><span class="ln">20594 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, 1, 0).shape 
<a name="l20595"><span class="ln">20595 </span></a>        torch.Size([2, 3, 1]) 
<a name="l20596"><span class="ln">20596 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, 1, 0) 
<a name="l20597"><span class="ln">20597 </span></a>        tensor([[[-0.3362], 
<a name="l20598"><span class="ln">20598 </span></a>                [-0.9627], 
<a name="l20599"><span class="ln">20599 </span></a>                [ 0.5173]], 
<a name="l20600"><span class="ln">20600 </span></a> 
<a name="l20601"><span class="ln">20601 </span></a>                [[-0.8437], 
<a name="l20602"><span class="ln">20602 </span></a>                [ 0.1727], 
<a name="l20603"><span class="ln">20603 </span></a>                [-0.1398]]]) 
<a name="l20604"><span class="ln">20604 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1)).shape 
<a name="l20605"><span class="ln">20605 </span></a>        torch.Size([2, 1, 3]) 
<a name="l20606"><span class="ln">20606 </span></a>        &gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1)) 
<a name="l20607"><span class="ln">20607 </span></a>        tensor([[[-0.3362, -0.9627,  0.5173]], 
<a name="l20608"><span class="ln">20608 </span></a> 
<a name="l20609"><span class="ln">20609 </span></a>                [[-0.8437,  0.1727, -0.1398]]]) 
<a name="l20610"><span class="ln">20610 </span></a>    &quot;&quot;&quot;</span>
<a name="l20611"><span class="ln">20611 </span></a>
<a name="l20612"><span class="ln">20612 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20613"><span class="ln">20613 </span></a><span class="s2">def </span><span class="s1">movedim</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">source</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">destination</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20614"><span class="ln">20614 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20615"><span class="ln">20615 </span></a>    movedim(input, source, destination) -&gt; Tensor 
<a name="l20616"><span class="ln">20616 </span></a> 
<a name="l20617"><span class="ln">20617 </span></a>    Moves the dimension(s) of :attr:`input` at the position(s) in :attr:`source` 
<a name="l20618"><span class="ln">20618 </span></a>    to the position(s) in :attr:`destination`. 
<a name="l20619"><span class="ln">20619 </span></a> 
<a name="l20620"><span class="ln">20620 </span></a>    Other dimensions of :attr:`input` that are not explicitly moved remain in 
<a name="l20621"><span class="ln">20621 </span></a>    their original order and appear at the positions not specified in :attr:`destination`. 
<a name="l20622"><span class="ln">20622 </span></a> 
<a name="l20623"><span class="ln">20623 </span></a>    Args: 
<a name="l20624"><span class="ln">20624 </span></a>        input (Tensor): the input tensor. 
<a name="l20625"><span class="ln">20625 </span></a>        source (int or tuple of ints): Original positions of the dims to move. These must be unique. 
<a name="l20626"><span class="ln">20626 </span></a>        destination (int or tuple of ints): Destination positions for each of the original dims. These must also be unique. 
<a name="l20627"><span class="ln">20627 </span></a> 
<a name="l20628"><span class="ln">20628 </span></a>    Examples:: 
<a name="l20629"><span class="ln">20629 </span></a> 
<a name="l20630"><span class="ln">20630 </span></a>        &gt;&gt;&gt; t = torch.randn(3,2,1) 
<a name="l20631"><span class="ln">20631 </span></a>        &gt;&gt;&gt; t 
<a name="l20632"><span class="ln">20632 </span></a>        tensor([[[-0.3362], 
<a name="l20633"><span class="ln">20633 </span></a>                [-0.8437]], 
<a name="l20634"><span class="ln">20634 </span></a> 
<a name="l20635"><span class="ln">20635 </span></a>                [[-0.9627], 
<a name="l20636"><span class="ln">20636 </span></a>                [ 0.1727]], 
<a name="l20637"><span class="ln">20637 </span></a> 
<a name="l20638"><span class="ln">20638 </span></a>                [[ 0.5173], 
<a name="l20639"><span class="ln">20639 </span></a>                [-0.1398]]]) 
<a name="l20640"><span class="ln">20640 </span></a>        &gt;&gt;&gt; torch.movedim(t, 1, 0).shape 
<a name="l20641"><span class="ln">20641 </span></a>        torch.Size([2, 3, 1]) 
<a name="l20642"><span class="ln">20642 </span></a>        &gt;&gt;&gt; torch.movedim(t, 1, 0) 
<a name="l20643"><span class="ln">20643 </span></a>        tensor([[[-0.3362], 
<a name="l20644"><span class="ln">20644 </span></a>                [-0.9627], 
<a name="l20645"><span class="ln">20645 </span></a>                [ 0.5173]], 
<a name="l20646"><span class="ln">20646 </span></a> 
<a name="l20647"><span class="ln">20647 </span></a>                [[-0.8437], 
<a name="l20648"><span class="ln">20648 </span></a>                [ 0.1727], 
<a name="l20649"><span class="ln">20649 </span></a>                [-0.1398]]]) 
<a name="l20650"><span class="ln">20650 </span></a>        &gt;&gt;&gt; torch.movedim(t, (1, 2), (0, 1)).shape 
<a name="l20651"><span class="ln">20651 </span></a>        torch.Size([2, 1, 3]) 
<a name="l20652"><span class="ln">20652 </span></a>        &gt;&gt;&gt; torch.movedim(t, (1, 2), (0, 1)) 
<a name="l20653"><span class="ln">20653 </span></a>        tensor([[[-0.3362, -0.9627,  0.5173]], 
<a name="l20654"><span class="ln">20654 </span></a> 
<a name="l20655"><span class="ln">20655 </span></a>                [[-0.8437,  0.1727, -0.1398]]]) 
<a name="l20656"><span class="ln">20656 </span></a>    &quot;&quot;&quot;</span>
<a name="l20657"><span class="ln">20657 </span></a>
<a name="l20658"><span class="ln">20658 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20659"><span class="ln">20659 </span></a><span class="s2">def </span><span class="s1">movedim</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">source</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">, </span><span class="s1">destination</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20660"><span class="ln">20660 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20661"><span class="ln">20661 </span></a>    movedim(input, source, destination) -&gt; Tensor 
<a name="l20662"><span class="ln">20662 </span></a> 
<a name="l20663"><span class="ln">20663 </span></a>    Moves the dimension(s) of :attr:`input` at the position(s) in :attr:`source` 
<a name="l20664"><span class="ln">20664 </span></a>    to the position(s) in :attr:`destination`. 
<a name="l20665"><span class="ln">20665 </span></a> 
<a name="l20666"><span class="ln">20666 </span></a>    Other dimensions of :attr:`input` that are not explicitly moved remain in 
<a name="l20667"><span class="ln">20667 </span></a>    their original order and appear at the positions not specified in :attr:`destination`. 
<a name="l20668"><span class="ln">20668 </span></a> 
<a name="l20669"><span class="ln">20669 </span></a>    Args: 
<a name="l20670"><span class="ln">20670 </span></a>        input (Tensor): the input tensor. 
<a name="l20671"><span class="ln">20671 </span></a>        source (int or tuple of ints): Original positions of the dims to move. These must be unique. 
<a name="l20672"><span class="ln">20672 </span></a>        destination (int or tuple of ints): Destination positions for each of the original dims. These must also be unique. 
<a name="l20673"><span class="ln">20673 </span></a> 
<a name="l20674"><span class="ln">20674 </span></a>    Examples:: 
<a name="l20675"><span class="ln">20675 </span></a> 
<a name="l20676"><span class="ln">20676 </span></a>        &gt;&gt;&gt; t = torch.randn(3,2,1) 
<a name="l20677"><span class="ln">20677 </span></a>        &gt;&gt;&gt; t 
<a name="l20678"><span class="ln">20678 </span></a>        tensor([[[-0.3362], 
<a name="l20679"><span class="ln">20679 </span></a>                [-0.8437]], 
<a name="l20680"><span class="ln">20680 </span></a> 
<a name="l20681"><span class="ln">20681 </span></a>                [[-0.9627], 
<a name="l20682"><span class="ln">20682 </span></a>                [ 0.1727]], 
<a name="l20683"><span class="ln">20683 </span></a> 
<a name="l20684"><span class="ln">20684 </span></a>                [[ 0.5173], 
<a name="l20685"><span class="ln">20685 </span></a>                [-0.1398]]]) 
<a name="l20686"><span class="ln">20686 </span></a>        &gt;&gt;&gt; torch.movedim(t, 1, 0).shape 
<a name="l20687"><span class="ln">20687 </span></a>        torch.Size([2, 3, 1]) 
<a name="l20688"><span class="ln">20688 </span></a>        &gt;&gt;&gt; torch.movedim(t, 1, 0) 
<a name="l20689"><span class="ln">20689 </span></a>        tensor([[[-0.3362], 
<a name="l20690"><span class="ln">20690 </span></a>                [-0.9627], 
<a name="l20691"><span class="ln">20691 </span></a>                [ 0.5173]], 
<a name="l20692"><span class="ln">20692 </span></a> 
<a name="l20693"><span class="ln">20693 </span></a>                [[-0.8437], 
<a name="l20694"><span class="ln">20694 </span></a>                [ 0.1727], 
<a name="l20695"><span class="ln">20695 </span></a>                [-0.1398]]]) 
<a name="l20696"><span class="ln">20696 </span></a>        &gt;&gt;&gt; torch.movedim(t, (1, 2), (0, 1)).shape 
<a name="l20697"><span class="ln">20697 </span></a>        torch.Size([2, 1, 3]) 
<a name="l20698"><span class="ln">20698 </span></a>        &gt;&gt;&gt; torch.movedim(t, (1, 2), (0, 1)) 
<a name="l20699"><span class="ln">20699 </span></a>        tensor([[[-0.3362, -0.9627,  0.5173]], 
<a name="l20700"><span class="ln">20700 </span></a> 
<a name="l20701"><span class="ln">20701 </span></a>                [[-0.8437,  0.1727, -0.1398]]]) 
<a name="l20702"><span class="ln">20702 </span></a>    &quot;&quot;&quot;</span>
<a name="l20703"><span class="ln">20703 </span></a>
<a name="l20704"><span class="ln">20704 </span></a><span class="s2">def </span><span class="s1">msort</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20705"><span class="ln">20705 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20706"><span class="ln">20706 </span></a>    msort(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l20707"><span class="ln">20707 </span></a> 
<a name="l20708"><span class="ln">20708 </span></a>    Sorts the elements of the :attr:`input` tensor along its first dimension 
<a name="l20709"><span class="ln">20709 </span></a>    in ascending order by value. 
<a name="l20710"><span class="ln">20710 </span></a> 
<a name="l20711"><span class="ln">20711 </span></a>    .. note:: `torch.msort(t)` is equivalent to `torch.sort(t, dim=0)[0]`. 
<a name="l20712"><span class="ln">20712 </span></a>              See also :func:`torch.sort`. 
<a name="l20713"><span class="ln">20713 </span></a> 
<a name="l20714"><span class="ln">20714 </span></a>    Args: 
<a name="l20715"><span class="ln">20715 </span></a>        input (Tensor): the input tensor. 
<a name="l20716"><span class="ln">20716 </span></a> 
<a name="l20717"><span class="ln">20717 </span></a>    Keyword args: 
<a name="l20718"><span class="ln">20718 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20719"><span class="ln">20719 </span></a> 
<a name="l20720"><span class="ln">20720 </span></a>    Example:: 
<a name="l20721"><span class="ln">20721 </span></a> 
<a name="l20722"><span class="ln">20722 </span></a>        &gt;&gt;&gt; t = torch.randn(3, 4) 
<a name="l20723"><span class="ln">20723 </span></a>        &gt;&gt;&gt; t 
<a name="l20724"><span class="ln">20724 </span></a>        tensor([[-0.1321,  0.4370, -1.2631, -1.1289], 
<a name="l20725"><span class="ln">20725 </span></a>                [-2.0527, -1.1250,  0.2275,  0.3077], 
<a name="l20726"><span class="ln">20726 </span></a>                [-0.0881, -0.1259, -0.5495,  1.0284]]) 
<a name="l20727"><span class="ln">20727 </span></a>        &gt;&gt;&gt; torch.msort(t) 
<a name="l20728"><span class="ln">20728 </span></a>        tensor([[-2.0527, -1.1250, -1.2631, -1.1289], 
<a name="l20729"><span class="ln">20729 </span></a>                [-0.1321, -0.1259, -0.5495,  0.3077], 
<a name="l20730"><span class="ln">20730 </span></a>                [-0.0881,  0.4370,  0.2275,  1.0284]]) 
<a name="l20731"><span class="ln">20731 </span></a>    &quot;&quot;&quot;</span>
<a name="l20732"><span class="ln">20732 </span></a>
<a name="l20733"><span class="ln">20733 </span></a><span class="s2">def </span><span class="s1">mul</span><span class="s3">(</span>
<a name="l20734"><span class="ln">20734 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l20735"><span class="ln">20735 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l20736"><span class="ln">20736 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20737"><span class="ln">20737 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20738"><span class="ln">20738 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20739"><span class="ln">20739 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20740"><span class="ln">20740 </span></a>    mul(input, other, *, out=None) -&gt; Tensor 
<a name="l20741"><span class="ln">20741 </span></a> 
<a name="l20742"><span class="ln">20742 </span></a>    Multiplies :attr:`input` by :attr:`other`. 
<a name="l20743"><span class="ln">20743 </span></a> 
<a name="l20744"><span class="ln">20744 </span></a> 
<a name="l20745"><span class="ln">20745 </span></a>    .. math:: 
<a name="l20746"><span class="ln">20746 </span></a>        \text{out}_i = \text{input}_i \times \text{other}_i 
<a name="l20747"><span class="ln">20747 </span></a> 
<a name="l20748"><span class="ln">20748 </span></a> 
<a name="l20749"><span class="ln">20749 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l20750"><span class="ln">20750 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l20751"><span class="ln">20751 </span></a> 
<a name="l20752"><span class="ln">20752 </span></a>    Args: 
<a name="l20753"><span class="ln">20753 </span></a>        input (Tensor): the input tensor. 
<a name="l20754"><span class="ln">20754 </span></a>        other (Tensor or Number) - the tensor or number to multiply input by. 
<a name="l20755"><span class="ln">20755 </span></a> 
<a name="l20756"><span class="ln">20756 </span></a>    Keyword args: 
<a name="l20757"><span class="ln">20757 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20758"><span class="ln">20758 </span></a> 
<a name="l20759"><span class="ln">20759 </span></a>    Examples:: 
<a name="l20760"><span class="ln">20760 </span></a> 
<a name="l20761"><span class="ln">20761 </span></a>        &gt;&gt;&gt; a = torch.randn(3) 
<a name="l20762"><span class="ln">20762 </span></a>        &gt;&gt;&gt; a 
<a name="l20763"><span class="ln">20763 </span></a>        tensor([ 0.2015, -0.4255,  2.6087]) 
<a name="l20764"><span class="ln">20764 </span></a>        &gt;&gt;&gt; torch.mul(a, 100) 
<a name="l20765"><span class="ln">20765 </span></a>        tensor([  20.1494,  -42.5491,  260.8663]) 
<a name="l20766"><span class="ln">20766 </span></a> 
<a name="l20767"><span class="ln">20767 </span></a>        &gt;&gt;&gt; b = torch.randn(4, 1) 
<a name="l20768"><span class="ln">20768 </span></a>        &gt;&gt;&gt; b 
<a name="l20769"><span class="ln">20769 </span></a>        tensor([[ 1.1207], 
<a name="l20770"><span class="ln">20770 </span></a>                [-0.3137], 
<a name="l20771"><span class="ln">20771 </span></a>                [ 0.0700], 
<a name="l20772"><span class="ln">20772 </span></a>                [ 0.8378]]) 
<a name="l20773"><span class="ln">20773 </span></a>        &gt;&gt;&gt; c = torch.randn(1, 4) 
<a name="l20774"><span class="ln">20774 </span></a>        &gt;&gt;&gt; c 
<a name="l20775"><span class="ln">20775 </span></a>        tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]]) 
<a name="l20776"><span class="ln">20776 </span></a>        &gt;&gt;&gt; torch.mul(b, c) 
<a name="l20777"><span class="ln">20777 </span></a>        tensor([[ 0.5767,  0.1363, -0.5877,  2.5083], 
<a name="l20778"><span class="ln">20778 </span></a>                [-0.1614, -0.0382,  0.1645, -0.7021], 
<a name="l20779"><span class="ln">20779 </span></a>                [ 0.0360,  0.0085, -0.0367,  0.1567], 
<a name="l20780"><span class="ln">20780 </span></a>                [ 0.4312,  0.1019, -0.4394,  1.8753]]) 
<a name="l20781"><span class="ln">20781 </span></a>    &quot;&quot;&quot;</span>
<a name="l20782"><span class="ln">20782 </span></a>
<a name="l20783"><span class="ln">20783 </span></a><span class="s2">def </span><span class="s1">multinomial</span><span class="s3">(</span>
<a name="l20784"><span class="ln">20784 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20785"><span class="ln">20785 </span></a>    <span class="s1">num_samples</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l20786"><span class="ln">20786 </span></a>    <span class="s1">replacement</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20787"><span class="ln">20787 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20788"><span class="ln">20788 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20789"><span class="ln">20789 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20790"><span class="ln">20790 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20791"><span class="ln">20791 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20792"><span class="ln">20792 </span></a>    multinomial(input, num_samples, replacement=False, *, generator=None, out=None) -&gt; LongTensor 
<a name="l20793"><span class="ln">20793 </span></a> 
<a name="l20794"><span class="ln">20794 </span></a>    Returns a tensor where each row contains :attr:`num_samples` indices sampled 
<a name="l20795"><span class="ln">20795 </span></a>    from the multinomial (a stricter definition would be multivariate, 
<a name="l20796"><span class="ln">20796 </span></a>    refer to :class:`torch.distributions.multinomial.Multinomial` for more details) 
<a name="l20797"><span class="ln">20797 </span></a>    probability distribution located in the corresponding row 
<a name="l20798"><span class="ln">20798 </span></a>    of tensor :attr:`input`. 
<a name="l20799"><span class="ln">20799 </span></a> 
<a name="l20800"><span class="ln">20800 </span></a>    .. note:: 
<a name="l20801"><span class="ln">20801 </span></a>        The rows of :attr:`input` do not need to sum to one (in which case we use 
<a name="l20802"><span class="ln">20802 </span></a>        the values as weights), but must be non-negative, finite and have 
<a name="l20803"><span class="ln">20803 </span></a>        a non-zero sum. 
<a name="l20804"><span class="ln">20804 </span></a> 
<a name="l20805"><span class="ln">20805 </span></a>    Indices are ordered from left to right according to when each was sampled 
<a name="l20806"><span class="ln">20806 </span></a>    (first samples are placed in first column). 
<a name="l20807"><span class="ln">20807 </span></a> 
<a name="l20808"><span class="ln">20808 </span></a>    If :attr:`input` is a vector, :attr:`out` is a vector of size :attr:`num_samples`. 
<a name="l20809"><span class="ln">20809 </span></a> 
<a name="l20810"><span class="ln">20810 </span></a>    If :attr:`input` is a matrix with `m` rows, :attr:`out` is an matrix of shape 
<a name="l20811"><span class="ln">20811 </span></a>    :math:`(m \times \text{num\_samples})`. 
<a name="l20812"><span class="ln">20812 </span></a> 
<a name="l20813"><span class="ln">20813 </span></a>    If replacement is ``True``, samples are drawn with replacement. 
<a name="l20814"><span class="ln">20814 </span></a> 
<a name="l20815"><span class="ln">20815 </span></a>    If not, they are drawn without replacement, which means that when a 
<a name="l20816"><span class="ln">20816 </span></a>    sample index is drawn for a row, it cannot be drawn again for that row. 
<a name="l20817"><span class="ln">20817 </span></a> 
<a name="l20818"><span class="ln">20818 </span></a>    .. note:: 
<a name="l20819"><span class="ln">20819 </span></a>        When drawn without replacement, :attr:`num_samples` must be lower than 
<a name="l20820"><span class="ln">20820 </span></a>        number of non-zero elements in :attr:`input` (or the min number of non-zero 
<a name="l20821"><span class="ln">20821 </span></a>        elements in each row of :attr:`input` if it is a matrix). 
<a name="l20822"><span class="ln">20822 </span></a> 
<a name="l20823"><span class="ln">20823 </span></a>    Args: 
<a name="l20824"><span class="ln">20824 </span></a>        input (Tensor): the input tensor containing probabilities 
<a name="l20825"><span class="ln">20825 </span></a>        num_samples (int): number of samples to draw 
<a name="l20826"><span class="ln">20826 </span></a>        replacement (bool, optional): whether to draw with replacement or not 
<a name="l20827"><span class="ln">20827 </span></a> 
<a name="l20828"><span class="ln">20828 </span></a>    Keyword args: 
<a name="l20829"><span class="ln">20829 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l20830"><span class="ln">20830 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20831"><span class="ln">20831 </span></a> 
<a name="l20832"><span class="ln">20832 </span></a>    Example:: 
<a name="l20833"><span class="ln">20833 </span></a> 
<a name="l20834"><span class="ln">20834 </span></a>        &gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights 
<a name="l20835"><span class="ln">20835 </span></a>        &gt;&gt;&gt; torch.multinomial(weights, 2) 
<a name="l20836"><span class="ln">20836 </span></a>        tensor([1, 2]) 
<a name="l20837"><span class="ln">20837 </span></a>        &gt;&gt;&gt; torch.multinomial(weights, 5) # ERROR! 
<a name="l20838"><span class="ln">20838 </span></a>        RuntimeError: cannot sample n_sample &gt; prob_dist.size(-1) samples without replacement 
<a name="l20839"><span class="ln">20839 </span></a>        &gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True) 
<a name="l20840"><span class="ln">20840 </span></a>        tensor([ 2,  1,  1,  1]) 
<a name="l20841"><span class="ln">20841 </span></a>    &quot;&quot;&quot;</span>
<a name="l20842"><span class="ln">20842 </span></a>
<a name="l20843"><span class="ln">20843 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20844"><span class="ln">20844 </span></a><span class="s2">def </span><span class="s1">multiply</span><span class="s3">(</span>
<a name="l20845"><span class="ln">20845 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20846"><span class="ln">20846 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20847"><span class="ln">20847 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20848"><span class="ln">20848 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20849"><span class="ln">20849 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20850"><span class="ln">20850 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20851"><span class="ln">20851 </span></a>    multiply(input, other, *, out=None) 
<a name="l20852"><span class="ln">20852 </span></a> 
<a name="l20853"><span class="ln">20853 </span></a>    Alias for :func:`torch.mul`. 
<a name="l20854"><span class="ln">20854 </span></a>    &quot;&quot;&quot;</span>
<a name="l20855"><span class="ln">20855 </span></a>
<a name="l20856"><span class="ln">20856 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l20857"><span class="ln">20857 </span></a><span class="s2">def </span><span class="s1">multiply</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20858"><span class="ln">20858 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20859"><span class="ln">20859 </span></a>    multiply(input, other, *, out=None) 
<a name="l20860"><span class="ln">20860 </span></a> 
<a name="l20861"><span class="ln">20861 </span></a>    Alias for :func:`torch.mul`. 
<a name="l20862"><span class="ln">20862 </span></a>    &quot;&quot;&quot;</span>
<a name="l20863"><span class="ln">20863 </span></a>
<a name="l20864"><span class="ln">20864 </span></a><span class="s2">def </span><span class="s1">mv</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">vec</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20865"><span class="ln">20865 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20866"><span class="ln">20866 </span></a>    mv(input, vec, *, out=None) -&gt; Tensor 
<a name="l20867"><span class="ln">20867 </span></a> 
<a name="l20868"><span class="ln">20868 </span></a>    Performs a matrix-vector product of the matrix :attr:`input` and the vector 
<a name="l20869"><span class="ln">20869 </span></a>    :attr:`vec`. 
<a name="l20870"><span class="ln">20870 </span></a> 
<a name="l20871"><span class="ln">20871 </span></a>    If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of 
<a name="l20872"><span class="ln">20872 </span></a>    size :math:`m`, :attr:`out` will be 1-D of size :math:`n`. 
<a name="l20873"><span class="ln">20873 </span></a> 
<a name="l20874"><span class="ln">20874 </span></a>    .. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l20875"><span class="ln">20875 </span></a> 
<a name="l20876"><span class="ln">20876 </span></a>    Args: 
<a name="l20877"><span class="ln">20877 </span></a>        input (Tensor): matrix to be multiplied 
<a name="l20878"><span class="ln">20878 </span></a>        vec (Tensor): vector to be multiplied 
<a name="l20879"><span class="ln">20879 </span></a> 
<a name="l20880"><span class="ln">20880 </span></a>    Keyword args: 
<a name="l20881"><span class="ln">20881 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20882"><span class="ln">20882 </span></a> 
<a name="l20883"><span class="ln">20883 </span></a>    Example:: 
<a name="l20884"><span class="ln">20884 </span></a> 
<a name="l20885"><span class="ln">20885 </span></a>        &gt;&gt;&gt; mat = torch.randn(2, 3) 
<a name="l20886"><span class="ln">20886 </span></a>        &gt;&gt;&gt; vec = torch.randn(3) 
<a name="l20887"><span class="ln">20887 </span></a>        &gt;&gt;&gt; torch.mv(mat, vec) 
<a name="l20888"><span class="ln">20888 </span></a>        tensor([ 1.0404, -0.6361]) 
<a name="l20889"><span class="ln">20889 </span></a>    &quot;&quot;&quot;</span>
<a name="l20890"><span class="ln">20890 </span></a>
<a name="l20891"><span class="ln">20891 </span></a><span class="s2">def </span><span class="s1">mvlgamma</span><span class="s3">(</span>
<a name="l20892"><span class="ln">20892 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20893"><span class="ln">20893 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l20894"><span class="ln">20894 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20895"><span class="ln">20895 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20896"><span class="ln">20896 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20897"><span class="ln">20897 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20898"><span class="ln">20898 </span></a>    mvlgamma(input, p, *, out=None) -&gt; Tensor 
<a name="l20899"><span class="ln">20899 </span></a> 
<a name="l20900"><span class="ln">20900 </span></a>    Alias for :func:`torch.special.multigammaln`. 
<a name="l20901"><span class="ln">20901 </span></a>    &quot;&quot;&quot;</span>
<a name="l20902"><span class="ln">20902 </span></a>
<a name="l20903"><span class="ln">20903 </span></a><span class="s2">def </span><span class="s1">nan_to_num</span><span class="s3">(</span>
<a name="l20904"><span class="ln">20904 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20905"><span class="ln">20905 </span></a>    <span class="s1">nan</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20906"><span class="ln">20906 </span></a>    <span class="s1">posinf</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20907"><span class="ln">20907 </span></a>    <span class="s1">neginf</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20908"><span class="ln">20908 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20909"><span class="ln">20909 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20910"><span class="ln">20910 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20911"><span class="ln">20911 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20912"><span class="ln">20912 </span></a>    nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -&gt; Tensor 
<a name="l20913"><span class="ln">20913 </span></a> 
<a name="l20914"><span class="ln">20914 </span></a>    Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input` 
<a name="l20915"><span class="ln">20915 </span></a>    with the values specified by :attr:`nan`, :attr:`posinf`, and :attr:`neginf`, respectively. 
<a name="l20916"><span class="ln">20916 </span></a>    By default, :literal:`NaN`\ s are replaced with zero, positive infinity is replaced with the 
<a name="l20917"><span class="ln">20917 </span></a>    greatest finite value representable by :attr:`input`'s dtype, and negative infinity 
<a name="l20918"><span class="ln">20918 </span></a>    is replaced with the least finite value representable by :attr:`input`'s dtype. 
<a name="l20919"><span class="ln">20919 </span></a> 
<a name="l20920"><span class="ln">20920 </span></a>    Args: 
<a name="l20921"><span class="ln">20921 </span></a>        input (Tensor): the input tensor. 
<a name="l20922"><span class="ln">20922 </span></a>        nan (Number, optional): the value to replace :literal:`NaN`\s with. Default is zero. 
<a name="l20923"><span class="ln">20923 </span></a>        posinf (Number, optional): if a Number, the value to replace positive infinity values with. 
<a name="l20924"><span class="ln">20924 </span></a>            If None, positive infinity values are replaced with the greatest finite value representable by :attr:`input`'s dtype. 
<a name="l20925"><span class="ln">20925 </span></a>            Default is None. 
<a name="l20926"><span class="ln">20926 </span></a>        neginf (Number, optional): if a Number, the value to replace negative infinity values with. 
<a name="l20927"><span class="ln">20927 </span></a>            If None, negative infinity values are replaced with the lowest finite value representable by :attr:`input`'s dtype. 
<a name="l20928"><span class="ln">20928 </span></a>            Default is None. 
<a name="l20929"><span class="ln">20929 </span></a> 
<a name="l20930"><span class="ln">20930 </span></a>    Keyword args: 
<a name="l20931"><span class="ln">20931 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20932"><span class="ln">20932 </span></a> 
<a name="l20933"><span class="ln">20933 </span></a>    Example:: 
<a name="l20934"><span class="ln">20934 </span></a> 
<a name="l20935"><span class="ln">20935 </span></a>        &gt;&gt;&gt; x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14]) 
<a name="l20936"><span class="ln">20936 </span></a>        &gt;&gt;&gt; torch.nan_to_num(x) 
<a name="l20937"><span class="ln">20937 </span></a>        tensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00]) 
<a name="l20938"><span class="ln">20938 </span></a>        &gt;&gt;&gt; torch.nan_to_num(x, nan=2.0) 
<a name="l20939"><span class="ln">20939 </span></a>        tensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00]) 
<a name="l20940"><span class="ln">20940 </span></a>        &gt;&gt;&gt; torch.nan_to_num(x, nan=2.0, posinf=1.0) 
<a name="l20941"><span class="ln">20941 </span></a>        tensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00]) 
<a name="l20942"><span class="ln">20942 </span></a>    &quot;&quot;&quot;</span>
<a name="l20943"><span class="ln">20943 </span></a>
<a name="l20944"><span class="ln">20944 </span></a><span class="s2">def </span><span class="s1">nan_to_num_</span><span class="s3">(</span>
<a name="l20945"><span class="ln">20945 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20946"><span class="ln">20946 </span></a>    <span class="s1">nan</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20947"><span class="ln">20947 </span></a>    <span class="s1">posinf</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20948"><span class="ln">20948 </span></a>    <span class="s1">neginf</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20949"><span class="ln">20949 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l20950"><span class="ln">20950 </span></a><span class="s2">def </span><span class="s1">nanmean</span><span class="s3">(</span>
<a name="l20951"><span class="ln">20951 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l20952"><span class="ln">20952 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20953"><span class="ln">20953 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l20954"><span class="ln">20954 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l20955"><span class="ln">20955 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20956"><span class="ln">20956 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l20957"><span class="ln">20957 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l20958"><span class="ln">20958 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l20959"><span class="ln">20959 </span></a>    nanmean(input, dim=None, keepdim=False, *, dtype=None, out=None) -&gt; Tensor 
<a name="l20960"><span class="ln">20960 </span></a> 
<a name="l20961"><span class="ln">20961 </span></a>    Computes the mean of all `non-NaN` elements along the specified dimensions. 
<a name="l20962"><span class="ln">20962 </span></a>    Input must be floating point or complex. 
<a name="l20963"><span class="ln">20963 </span></a> 
<a name="l20964"><span class="ln">20964 </span></a>    This function is identical to :func:`torch.mean` when there are no `NaN` values 
<a name="l20965"><span class="ln">20965 </span></a>    in the :attr:`input` tensor. In the presence of `NaN`, :func:`torch.mean` will 
<a name="l20966"><span class="ln">20966 </span></a>    propagate the `NaN` to the output whereas :func:`torch.nanmean` will ignore the 
<a name="l20967"><span class="ln">20967 </span></a>    `NaN` values (`torch.nanmean(a)` is equivalent to `torch.mean(a[~a.isnan()])`). 
<a name="l20968"><span class="ln">20968 </span></a> 
<a name="l20969"><span class="ln">20969 </span></a> 
<a name="l20970"><span class="ln">20970 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l20971"><span class="ln">20971 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l20972"><span class="ln">20972 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l20973"><span class="ln">20973 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l20974"><span class="ln">20974 </span></a> 
<a name="l20975"><span class="ln">20975 </span></a> 
<a name="l20976"><span class="ln">20976 </span></a>    Args: 
<a name="l20977"><span class="ln">20977 </span></a>        input (Tensor): the input tensor, either of floating point or complex dtype 
<a name="l20978"><span class="ln">20978 </span></a> 
<a name="l20979"><span class="ln">20979 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l20980"><span class="ln">20980 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l20981"><span class="ln">20981 </span></a> 
<a name="l20982"><span class="ln">20982 </span></a> 
<a name="l20983"><span class="ln">20983 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l20984"><span class="ln">20984 </span></a> 
<a name="l20985"><span class="ln">20985 </span></a> 
<a name="l20986"><span class="ln">20986 </span></a>    Keyword args: 
<a name="l20987"><span class="ln">20987 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l20988"><span class="ln">20988 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l20989"><span class="ln">20989 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l20990"><span class="ln">20990 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l20991"><span class="ln">20991 </span></a> 
<a name="l20992"><span class="ln">20992 </span></a>    .. seealso:: 
<a name="l20993"><span class="ln">20993 </span></a> 
<a name="l20994"><span class="ln">20994 </span></a>        :func:`torch.mean` computes the mean value, propagating `NaN`. 
<a name="l20995"><span class="ln">20995 </span></a> 
<a name="l20996"><span class="ln">20996 </span></a>    Example:: 
<a name="l20997"><span class="ln">20997 </span></a> 
<a name="l20998"><span class="ln">20998 </span></a>        &gt;&gt;&gt; x = torch.tensor([[torch.nan, 1, 2], [1, 2, 3]]) 
<a name="l20999"><span class="ln">20999 </span></a>        &gt;&gt;&gt; x.mean() 
<a name="l21000"><span class="ln">21000 </span></a>        tensor(nan) 
<a name="l21001"><span class="ln">21001 </span></a>        &gt;&gt;&gt; x.nanmean() 
<a name="l21002"><span class="ln">21002 </span></a>        tensor(1.8000) 
<a name="l21003"><span class="ln">21003 </span></a>        &gt;&gt;&gt; x.mean(dim=0) 
<a name="l21004"><span class="ln">21004 </span></a>        tensor([   nan, 1.5000, 2.5000]) 
<a name="l21005"><span class="ln">21005 </span></a>        &gt;&gt;&gt; x.nanmean(dim=0) 
<a name="l21006"><span class="ln">21006 </span></a>        tensor([1.0000, 1.5000, 2.5000]) 
<a name="l21007"><span class="ln">21007 </span></a> 
<a name="l21008"><span class="ln">21008 </span></a>        # If all elements in the reduced dimensions are NaN then the result is NaN 
<a name="l21009"><span class="ln">21009 </span></a>        &gt;&gt;&gt; torch.tensor([torch.nan]).nanmean() 
<a name="l21010"><span class="ln">21010 </span></a>        tensor(nan) 
<a name="l21011"><span class="ln">21011 </span></a>    &quot;&quot;&quot;</span>
<a name="l21012"><span class="ln">21012 </span></a>
<a name="l21013"><span class="ln">21013 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21014"><span class="ln">21014 </span></a><span class="s2">def </span><span class="s1">nanmedian</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21015"><span class="ln">21015 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21016"><span class="ln">21016 </span></a>    nanmedian(input) -&gt; Tensor 
<a name="l21017"><span class="ln">21017 </span></a> 
<a name="l21018"><span class="ln">21018 </span></a>    Returns the median of the values in :attr:`input`, ignoring ``NaN`` values. 
<a name="l21019"><span class="ln">21019 </span></a> 
<a name="l21020"><span class="ln">21020 </span></a>    This function is identical to :func:`torch.median` when there are no ``NaN`` values in :attr:`input`. 
<a name="l21021"><span class="ln">21021 </span></a>    When :attr:`input` has one or more ``NaN`` values, :func:`torch.median` will always return ``NaN``, 
<a name="l21022"><span class="ln">21022 </span></a>    while this function will return the median of the non-``NaN`` elements in :attr:`input`. 
<a name="l21023"><span class="ln">21023 </span></a>    If all the elements in :attr:`input` are ``NaN`` it will also return ``NaN``. 
<a name="l21024"><span class="ln">21024 </span></a> 
<a name="l21025"><span class="ln">21025 </span></a>    Args: 
<a name="l21026"><span class="ln">21026 </span></a>        input (Tensor): the input tensor. 
<a name="l21027"><span class="ln">21027 </span></a> 
<a name="l21028"><span class="ln">21028 </span></a>    Example:: 
<a name="l21029"><span class="ln">21029 </span></a> 
<a name="l21030"><span class="ln">21030 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, float('nan'), 3, 2]) 
<a name="l21031"><span class="ln">21031 </span></a>        &gt;&gt;&gt; a.median() 
<a name="l21032"><span class="ln">21032 </span></a>        tensor(nan) 
<a name="l21033"><span class="ln">21033 </span></a>        &gt;&gt;&gt; a.nanmedian() 
<a name="l21034"><span class="ln">21034 </span></a>        tensor(2.) 
<a name="l21035"><span class="ln">21035 </span></a> 
<a name="l21036"><span class="ln">21036 </span></a>    .. function:: nanmedian(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l21037"><span class="ln">21037 </span></a>       :noindex: 
<a name="l21038"><span class="ln">21038 </span></a> 
<a name="l21039"><span class="ln">21039 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l21040"><span class="ln">21040 </span></a>    in the dimension :attr:`dim`, ignoring ``NaN`` values, and ``indices`` contains the index of the median values 
<a name="l21041"><span class="ln">21041 </span></a>    found in the dimension :attr:`dim`. 
<a name="l21042"><span class="ln">21042 </span></a> 
<a name="l21043"><span class="ln">21043 </span></a>    This function is identical to :func:`torch.median` when there are no ``NaN`` values in a reduced row. When a reduced row has 
<a name="l21044"><span class="ln">21044 </span></a>    one or more ``NaN`` values, :func:`torch.median` will always reduce it to ``NaN``, while this function will reduce it to the 
<a name="l21045"><span class="ln">21045 </span></a>    median of the non-``NaN`` elements. If all the elements in a reduced row are ``NaN`` then it will be reduced to ``NaN``, too. 
<a name="l21046"><span class="ln">21046 </span></a> 
<a name="l21047"><span class="ln">21047 </span></a>    Args: 
<a name="l21048"><span class="ln">21048 </span></a>        input (Tensor): the input tensor. 
<a name="l21049"><span class="ln">21049 </span></a> 
<a name="l21050"><span class="ln">21050 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l21051"><span class="ln">21051 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l21052"><span class="ln">21052 </span></a> 
<a name="l21053"><span class="ln">21053 </span></a> 
<a name="l21054"><span class="ln">21054 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l21055"><span class="ln">21055 </span></a> 
<a name="l21056"><span class="ln">21056 </span></a> 
<a name="l21057"><span class="ln">21057 </span></a>    Keyword args: 
<a name="l21058"><span class="ln">21058 </span></a>        out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l21059"><span class="ln">21059 </span></a>                                          tensor, which must have dtype long, with their indices in the dimension 
<a name="l21060"><span class="ln">21060 </span></a>                                          :attr:`dim` of :attr:`input`. 
<a name="l21061"><span class="ln">21061 </span></a> 
<a name="l21062"><span class="ln">21062 </span></a>    Example:: 
<a name="l21063"><span class="ln">21063 </span></a> 
<a name="l21064"><span class="ln">21064 </span></a>        &gt;&gt;&gt; a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]]) 
<a name="l21065"><span class="ln">21065 </span></a>        &gt;&gt;&gt; a 
<a name="l21066"><span class="ln">21066 </span></a>        tensor([[2., 3., 1.], 
<a name="l21067"><span class="ln">21067 </span></a>                [nan, 1., nan]]) 
<a name="l21068"><span class="ln">21068 </span></a>        &gt;&gt;&gt; a.median(0) 
<a name="l21069"><span class="ln">21069 </span></a>        torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1])) 
<a name="l21070"><span class="ln">21070 </span></a>        &gt;&gt;&gt; a.nanmedian(0) 
<a name="l21071"><span class="ln">21071 </span></a>        torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0])) 
<a name="l21072"><span class="ln">21072 </span></a>    &quot;&quot;&quot;</span>
<a name="l21073"><span class="ln">21073 </span></a>
<a name="l21074"><span class="ln">21074 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21075"><span class="ln">21075 </span></a><span class="s2">def </span><span class="s1">nanmedian</span><span class="s3">(</span>
<a name="l21076"><span class="ln">21076 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21077"><span class="ln">21077 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l21078"><span class="ln">21078 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l21079"><span class="ln">21079 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21080"><span class="ln">21080 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21081"><span class="ln">21081 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">nanmedian</span><span class="s2">:</span>
<a name="l21082"><span class="ln">21082 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21083"><span class="ln">21083 </span></a>    nanmedian(input) -&gt; Tensor 
<a name="l21084"><span class="ln">21084 </span></a> 
<a name="l21085"><span class="ln">21085 </span></a>    Returns the median of the values in :attr:`input`, ignoring ``NaN`` values. 
<a name="l21086"><span class="ln">21086 </span></a> 
<a name="l21087"><span class="ln">21087 </span></a>    This function is identical to :func:`torch.median` when there are no ``NaN`` values in :attr:`input`. 
<a name="l21088"><span class="ln">21088 </span></a>    When :attr:`input` has one or more ``NaN`` values, :func:`torch.median` will always return ``NaN``, 
<a name="l21089"><span class="ln">21089 </span></a>    while this function will return the median of the non-``NaN`` elements in :attr:`input`. 
<a name="l21090"><span class="ln">21090 </span></a>    If all the elements in :attr:`input` are ``NaN`` it will also return ``NaN``. 
<a name="l21091"><span class="ln">21091 </span></a> 
<a name="l21092"><span class="ln">21092 </span></a>    Args: 
<a name="l21093"><span class="ln">21093 </span></a>        input (Tensor): the input tensor. 
<a name="l21094"><span class="ln">21094 </span></a> 
<a name="l21095"><span class="ln">21095 </span></a>    Example:: 
<a name="l21096"><span class="ln">21096 </span></a> 
<a name="l21097"><span class="ln">21097 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, float('nan'), 3, 2]) 
<a name="l21098"><span class="ln">21098 </span></a>        &gt;&gt;&gt; a.median() 
<a name="l21099"><span class="ln">21099 </span></a>        tensor(nan) 
<a name="l21100"><span class="ln">21100 </span></a>        &gt;&gt;&gt; a.nanmedian() 
<a name="l21101"><span class="ln">21101 </span></a>        tensor(2.) 
<a name="l21102"><span class="ln">21102 </span></a> 
<a name="l21103"><span class="ln">21103 </span></a>    .. function:: nanmedian(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l21104"><span class="ln">21104 </span></a>       :noindex: 
<a name="l21105"><span class="ln">21105 </span></a> 
<a name="l21106"><span class="ln">21106 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l21107"><span class="ln">21107 </span></a>    in the dimension :attr:`dim`, ignoring ``NaN`` values, and ``indices`` contains the index of the median values 
<a name="l21108"><span class="ln">21108 </span></a>    found in the dimension :attr:`dim`. 
<a name="l21109"><span class="ln">21109 </span></a> 
<a name="l21110"><span class="ln">21110 </span></a>    This function is identical to :func:`torch.median` when there are no ``NaN`` values in a reduced row. When a reduced row has 
<a name="l21111"><span class="ln">21111 </span></a>    one or more ``NaN`` values, :func:`torch.median` will always reduce it to ``NaN``, while this function will reduce it to the 
<a name="l21112"><span class="ln">21112 </span></a>    median of the non-``NaN`` elements. If all the elements in a reduced row are ``NaN`` then it will be reduced to ``NaN``, too. 
<a name="l21113"><span class="ln">21113 </span></a> 
<a name="l21114"><span class="ln">21114 </span></a>    Args: 
<a name="l21115"><span class="ln">21115 </span></a>        input (Tensor): the input tensor. 
<a name="l21116"><span class="ln">21116 </span></a> 
<a name="l21117"><span class="ln">21117 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l21118"><span class="ln">21118 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l21119"><span class="ln">21119 </span></a> 
<a name="l21120"><span class="ln">21120 </span></a> 
<a name="l21121"><span class="ln">21121 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l21122"><span class="ln">21122 </span></a> 
<a name="l21123"><span class="ln">21123 </span></a> 
<a name="l21124"><span class="ln">21124 </span></a>    Keyword args: 
<a name="l21125"><span class="ln">21125 </span></a>        out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l21126"><span class="ln">21126 </span></a>                                          tensor, which must have dtype long, with their indices in the dimension 
<a name="l21127"><span class="ln">21127 </span></a>                                          :attr:`dim` of :attr:`input`. 
<a name="l21128"><span class="ln">21128 </span></a> 
<a name="l21129"><span class="ln">21129 </span></a>    Example:: 
<a name="l21130"><span class="ln">21130 </span></a> 
<a name="l21131"><span class="ln">21131 </span></a>        &gt;&gt;&gt; a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]]) 
<a name="l21132"><span class="ln">21132 </span></a>        &gt;&gt;&gt; a 
<a name="l21133"><span class="ln">21133 </span></a>        tensor([[2., 3., 1.], 
<a name="l21134"><span class="ln">21134 </span></a>                [nan, 1., nan]]) 
<a name="l21135"><span class="ln">21135 </span></a>        &gt;&gt;&gt; a.median(0) 
<a name="l21136"><span class="ln">21136 </span></a>        torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1])) 
<a name="l21137"><span class="ln">21137 </span></a>        &gt;&gt;&gt; a.nanmedian(0) 
<a name="l21138"><span class="ln">21138 </span></a>        torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0])) 
<a name="l21139"><span class="ln">21139 </span></a>    &quot;&quot;&quot;</span>
<a name="l21140"><span class="ln">21140 </span></a>
<a name="l21141"><span class="ln">21141 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21142"><span class="ln">21142 </span></a><span class="s2">def </span><span class="s1">nanmedian</span><span class="s3">(</span>
<a name="l21143"><span class="ln">21143 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21144"><span class="ln">21144 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21145"><span class="ln">21145 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l21146"><span class="ln">21146 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21147"><span class="ln">21147 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21148"><span class="ln">21148 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">nanmedian</span><span class="s2">:</span>
<a name="l21149"><span class="ln">21149 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21150"><span class="ln">21150 </span></a>    nanmedian(input) -&gt; Tensor 
<a name="l21151"><span class="ln">21151 </span></a> 
<a name="l21152"><span class="ln">21152 </span></a>    Returns the median of the values in :attr:`input`, ignoring ``NaN`` values. 
<a name="l21153"><span class="ln">21153 </span></a> 
<a name="l21154"><span class="ln">21154 </span></a>    This function is identical to :func:`torch.median` when there are no ``NaN`` values in :attr:`input`. 
<a name="l21155"><span class="ln">21155 </span></a>    When :attr:`input` has one or more ``NaN`` values, :func:`torch.median` will always return ``NaN``, 
<a name="l21156"><span class="ln">21156 </span></a>    while this function will return the median of the non-``NaN`` elements in :attr:`input`. 
<a name="l21157"><span class="ln">21157 </span></a>    If all the elements in :attr:`input` are ``NaN`` it will also return ``NaN``. 
<a name="l21158"><span class="ln">21158 </span></a> 
<a name="l21159"><span class="ln">21159 </span></a>    Args: 
<a name="l21160"><span class="ln">21160 </span></a>        input (Tensor): the input tensor. 
<a name="l21161"><span class="ln">21161 </span></a> 
<a name="l21162"><span class="ln">21162 </span></a>    Example:: 
<a name="l21163"><span class="ln">21163 </span></a> 
<a name="l21164"><span class="ln">21164 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, float('nan'), 3, 2]) 
<a name="l21165"><span class="ln">21165 </span></a>        &gt;&gt;&gt; a.median() 
<a name="l21166"><span class="ln">21166 </span></a>        tensor(nan) 
<a name="l21167"><span class="ln">21167 </span></a>        &gt;&gt;&gt; a.nanmedian() 
<a name="l21168"><span class="ln">21168 </span></a>        tensor(2.) 
<a name="l21169"><span class="ln">21169 </span></a> 
<a name="l21170"><span class="ln">21170 </span></a>    .. function:: nanmedian(input, dim=-1, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l21171"><span class="ln">21171 </span></a>       :noindex: 
<a name="l21172"><span class="ln">21172 </span></a> 
<a name="l21173"><span class="ln">21173 </span></a>    Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input` 
<a name="l21174"><span class="ln">21174 </span></a>    in the dimension :attr:`dim`, ignoring ``NaN`` values, and ``indices`` contains the index of the median values 
<a name="l21175"><span class="ln">21175 </span></a>    found in the dimension :attr:`dim`. 
<a name="l21176"><span class="ln">21176 </span></a> 
<a name="l21177"><span class="ln">21177 </span></a>    This function is identical to :func:`torch.median` when there are no ``NaN`` values in a reduced row. When a reduced row has 
<a name="l21178"><span class="ln">21178 </span></a>    one or more ``NaN`` values, :func:`torch.median` will always reduce it to ``NaN``, while this function will reduce it to the 
<a name="l21179"><span class="ln">21179 </span></a>    median of the non-``NaN`` elements. If all the elements in a reduced row are ``NaN`` then it will be reduced to ``NaN``, too. 
<a name="l21180"><span class="ln">21180 </span></a> 
<a name="l21181"><span class="ln">21181 </span></a>    Args: 
<a name="l21182"><span class="ln">21182 </span></a>        input (Tensor): the input tensor. 
<a name="l21183"><span class="ln">21183 </span></a> 
<a name="l21184"><span class="ln">21184 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l21185"><span class="ln">21185 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l21186"><span class="ln">21186 </span></a> 
<a name="l21187"><span class="ln">21187 </span></a> 
<a name="l21188"><span class="ln">21188 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l21189"><span class="ln">21189 </span></a> 
<a name="l21190"><span class="ln">21190 </span></a> 
<a name="l21191"><span class="ln">21191 </span></a>    Keyword args: 
<a name="l21192"><span class="ln">21192 </span></a>        out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second 
<a name="l21193"><span class="ln">21193 </span></a>                                          tensor, which must have dtype long, with their indices in the dimension 
<a name="l21194"><span class="ln">21194 </span></a>                                          :attr:`dim` of :attr:`input`. 
<a name="l21195"><span class="ln">21195 </span></a> 
<a name="l21196"><span class="ln">21196 </span></a>    Example:: 
<a name="l21197"><span class="ln">21197 </span></a> 
<a name="l21198"><span class="ln">21198 </span></a>        &gt;&gt;&gt; a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]]) 
<a name="l21199"><span class="ln">21199 </span></a>        &gt;&gt;&gt; a 
<a name="l21200"><span class="ln">21200 </span></a>        tensor([[2., 3., 1.], 
<a name="l21201"><span class="ln">21201 </span></a>                [nan, 1., nan]]) 
<a name="l21202"><span class="ln">21202 </span></a>        &gt;&gt;&gt; a.median(0) 
<a name="l21203"><span class="ln">21203 </span></a>        torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1])) 
<a name="l21204"><span class="ln">21204 </span></a>        &gt;&gt;&gt; a.nanmedian(0) 
<a name="l21205"><span class="ln">21205 </span></a>        torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0])) 
<a name="l21206"><span class="ln">21206 </span></a>    &quot;&quot;&quot;</span>
<a name="l21207"><span class="ln">21207 </span></a>
<a name="l21208"><span class="ln">21208 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21209"><span class="ln">21209 </span></a><span class="s2">def </span><span class="s1">nanquantile</span><span class="s3">(</span>
<a name="l21210"><span class="ln">21210 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21211"><span class="ln">21211 </span></a>    <span class="s1">q</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21212"><span class="ln">21212 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21213"><span class="ln">21213 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l21214"><span class="ln">21214 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21215"><span class="ln">21215 </span></a>    <span class="s1">interpolation</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;linear&quot;</span><span class="s3">,</span>
<a name="l21216"><span class="ln">21216 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21217"><span class="ln">21217 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21218"><span class="ln">21218 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21219"><span class="ln">21219 </span></a>    nanquantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -&gt; Tensor 
<a name="l21220"><span class="ln">21220 </span></a> 
<a name="l21221"><span class="ln">21221 </span></a>    This is a variant of :func:`torch.quantile` that &quot;ignores&quot; ``NaN`` values, 
<a name="l21222"><span class="ln">21222 </span></a>    computing the quantiles :attr:`q` as if ``NaN`` values in :attr:`input` did 
<a name="l21223"><span class="ln">21223 </span></a>    not exist. If all values in a reduced row are ``NaN`` then the quantiles for 
<a name="l21224"><span class="ln">21224 </span></a>    that reduction will be ``NaN``. See the documentation for :func:`torch.quantile`. 
<a name="l21225"><span class="ln">21225 </span></a> 
<a name="l21226"><span class="ln">21226 </span></a>    Args: 
<a name="l21227"><span class="ln">21227 </span></a>        input (Tensor): the input tensor. 
<a name="l21228"><span class="ln">21228 </span></a>        q (float or Tensor): a scalar or 1D tensor of quantile values in the range [0, 1] 
<a name="l21229"><span class="ln">21229 </span></a> 
<a name="l21230"><span class="ln">21230 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l21231"><span class="ln">21231 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l21232"><span class="ln">21232 </span></a> 
<a name="l21233"><span class="ln">21233 </span></a> 
<a name="l21234"><span class="ln">21234 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l21235"><span class="ln">21235 </span></a> 
<a name="l21236"><span class="ln">21236 </span></a> 
<a name="l21237"><span class="ln">21237 </span></a>    Keyword arguments: 
<a name="l21238"><span class="ln">21238 </span></a>        interpolation (str): interpolation method to use when the desired quantile lies between two data points. 
<a name="l21239"><span class="ln">21239 </span></a>                                Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``. 
<a name="l21240"><span class="ln">21240 </span></a>                                Default is ``linear``. 
<a name="l21241"><span class="ln">21241 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21242"><span class="ln">21242 </span></a> 
<a name="l21243"><span class="ln">21243 </span></a>    Example:: 
<a name="l21244"><span class="ln">21244 </span></a> 
<a name="l21245"><span class="ln">21245 </span></a>        &gt;&gt;&gt; t = torch.tensor([float('nan'), 1, 2]) 
<a name="l21246"><span class="ln">21246 </span></a>        &gt;&gt;&gt; t.quantile(0.5) 
<a name="l21247"><span class="ln">21247 </span></a>        tensor(nan) 
<a name="l21248"><span class="ln">21248 </span></a>        &gt;&gt;&gt; t.nanquantile(0.5) 
<a name="l21249"><span class="ln">21249 </span></a>        tensor(1.5000) 
<a name="l21250"><span class="ln">21250 </span></a>        &gt;&gt;&gt; t = torch.tensor([[float('nan'), float('nan')], [1, 2]]) 
<a name="l21251"><span class="ln">21251 </span></a>        &gt;&gt;&gt; t 
<a name="l21252"><span class="ln">21252 </span></a>        tensor([[nan, nan], 
<a name="l21253"><span class="ln">21253 </span></a>                [1., 2.]]) 
<a name="l21254"><span class="ln">21254 </span></a>        &gt;&gt;&gt; t.nanquantile(0.5, dim=0) 
<a name="l21255"><span class="ln">21255 </span></a>        tensor([1., 2.]) 
<a name="l21256"><span class="ln">21256 </span></a>        &gt;&gt;&gt; t.nanquantile(0.5, dim=1) 
<a name="l21257"><span class="ln">21257 </span></a>        tensor([   nan, 1.5000]) 
<a name="l21258"><span class="ln">21258 </span></a>    &quot;&quot;&quot;</span>
<a name="l21259"><span class="ln">21259 </span></a>
<a name="l21260"><span class="ln">21260 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21261"><span class="ln">21261 </span></a><span class="s2">def </span><span class="s1">nanquantile</span><span class="s3">(</span>
<a name="l21262"><span class="ln">21262 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21263"><span class="ln">21263 </span></a>    <span class="s1">q</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l21264"><span class="ln">21264 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21265"><span class="ln">21265 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l21266"><span class="ln">21266 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21267"><span class="ln">21267 </span></a>    <span class="s1">interpolation</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;linear&quot;</span><span class="s3">,</span>
<a name="l21268"><span class="ln">21268 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21269"><span class="ln">21269 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21270"><span class="ln">21270 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21271"><span class="ln">21271 </span></a>    nanquantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -&gt; Tensor 
<a name="l21272"><span class="ln">21272 </span></a> 
<a name="l21273"><span class="ln">21273 </span></a>    This is a variant of :func:`torch.quantile` that &quot;ignores&quot; ``NaN`` values, 
<a name="l21274"><span class="ln">21274 </span></a>    computing the quantiles :attr:`q` as if ``NaN`` values in :attr:`input` did 
<a name="l21275"><span class="ln">21275 </span></a>    not exist. If all values in a reduced row are ``NaN`` then the quantiles for 
<a name="l21276"><span class="ln">21276 </span></a>    that reduction will be ``NaN``. See the documentation for :func:`torch.quantile`. 
<a name="l21277"><span class="ln">21277 </span></a> 
<a name="l21278"><span class="ln">21278 </span></a>    Args: 
<a name="l21279"><span class="ln">21279 </span></a>        input (Tensor): the input tensor. 
<a name="l21280"><span class="ln">21280 </span></a>        q (float or Tensor): a scalar or 1D tensor of quantile values in the range [0, 1] 
<a name="l21281"><span class="ln">21281 </span></a> 
<a name="l21282"><span class="ln">21282 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l21283"><span class="ln">21283 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l21284"><span class="ln">21284 </span></a> 
<a name="l21285"><span class="ln">21285 </span></a> 
<a name="l21286"><span class="ln">21286 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l21287"><span class="ln">21287 </span></a> 
<a name="l21288"><span class="ln">21288 </span></a> 
<a name="l21289"><span class="ln">21289 </span></a>    Keyword arguments: 
<a name="l21290"><span class="ln">21290 </span></a>        interpolation (str): interpolation method to use when the desired quantile lies between two data points. 
<a name="l21291"><span class="ln">21291 </span></a>                                Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``. 
<a name="l21292"><span class="ln">21292 </span></a>                                Default is ``linear``. 
<a name="l21293"><span class="ln">21293 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21294"><span class="ln">21294 </span></a> 
<a name="l21295"><span class="ln">21295 </span></a>    Example:: 
<a name="l21296"><span class="ln">21296 </span></a> 
<a name="l21297"><span class="ln">21297 </span></a>        &gt;&gt;&gt; t = torch.tensor([float('nan'), 1, 2]) 
<a name="l21298"><span class="ln">21298 </span></a>        &gt;&gt;&gt; t.quantile(0.5) 
<a name="l21299"><span class="ln">21299 </span></a>        tensor(nan) 
<a name="l21300"><span class="ln">21300 </span></a>        &gt;&gt;&gt; t.nanquantile(0.5) 
<a name="l21301"><span class="ln">21301 </span></a>        tensor(1.5000) 
<a name="l21302"><span class="ln">21302 </span></a>        &gt;&gt;&gt; t = torch.tensor([[float('nan'), float('nan')], [1, 2]]) 
<a name="l21303"><span class="ln">21303 </span></a>        &gt;&gt;&gt; t 
<a name="l21304"><span class="ln">21304 </span></a>        tensor([[nan, nan], 
<a name="l21305"><span class="ln">21305 </span></a>                [1., 2.]]) 
<a name="l21306"><span class="ln">21306 </span></a>        &gt;&gt;&gt; t.nanquantile(0.5, dim=0) 
<a name="l21307"><span class="ln">21307 </span></a>        tensor([1., 2.]) 
<a name="l21308"><span class="ln">21308 </span></a>        &gt;&gt;&gt; t.nanquantile(0.5, dim=1) 
<a name="l21309"><span class="ln">21309 </span></a>        tensor([   nan, 1.5000]) 
<a name="l21310"><span class="ln">21310 </span></a>    &quot;&quot;&quot;</span>
<a name="l21311"><span class="ln">21311 </span></a>
<a name="l21312"><span class="ln">21312 </span></a><span class="s2">def </span><span class="s1">nansum</span><span class="s3">(</span>
<a name="l21313"><span class="ln">21313 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21314"><span class="ln">21314 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21315"><span class="ln">21315 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l21316"><span class="ln">21316 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21317"><span class="ln">21317 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21318"><span class="ln">21318 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21319"><span class="ln">21319 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21320"><span class="ln">21320 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21321"><span class="ln">21321 </span></a>    nansum(input, *, dtype=None) -&gt; Tensor 
<a name="l21322"><span class="ln">21322 </span></a> 
<a name="l21323"><span class="ln">21323 </span></a>    Returns the sum of all elements, treating Not a Numbers (NaNs) as zero. 
<a name="l21324"><span class="ln">21324 </span></a> 
<a name="l21325"><span class="ln">21325 </span></a>    Args: 
<a name="l21326"><span class="ln">21326 </span></a>        input (Tensor): the input tensor. 
<a name="l21327"><span class="ln">21327 </span></a> 
<a name="l21328"><span class="ln">21328 </span></a>    Keyword args: 
<a name="l21329"><span class="ln">21329 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l21330"><span class="ln">21330 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l21331"><span class="ln">21331 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l21332"><span class="ln">21332 </span></a> 
<a name="l21333"><span class="ln">21333 </span></a>    Example:: 
<a name="l21334"><span class="ln">21334 </span></a> 
<a name="l21335"><span class="ln">21335 </span></a>        &gt;&gt;&gt; a = torch.tensor([1., 2., float('nan'), 4.]) 
<a name="l21336"><span class="ln">21336 </span></a>        &gt;&gt;&gt; torch.nansum(a) 
<a name="l21337"><span class="ln">21337 </span></a>        tensor(7.) 
<a name="l21338"><span class="ln">21338 </span></a> 
<a name="l21339"><span class="ln">21339 </span></a>    .. function:: nansum(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l21340"><span class="ln">21340 </span></a>       :noindex: 
<a name="l21341"><span class="ln">21341 </span></a> 
<a name="l21342"><span class="ln">21342 </span></a>    Returns the sum of each row of the :attr:`input` tensor in the given 
<a name="l21343"><span class="ln">21343 </span></a>    dimension :attr:`dim`, treating Not a Numbers (NaNs) as zero. 
<a name="l21344"><span class="ln">21344 </span></a>    If :attr:`dim` is a list of dimensions, reduce over all of them. 
<a name="l21345"><span class="ln">21345 </span></a> 
<a name="l21346"><span class="ln">21346 </span></a> 
<a name="l21347"><span class="ln">21347 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l21348"><span class="ln">21348 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l21349"><span class="ln">21349 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l21350"><span class="ln">21350 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l21351"><span class="ln">21351 </span></a> 
<a name="l21352"><span class="ln">21352 </span></a> 
<a name="l21353"><span class="ln">21353 </span></a>    Args: 
<a name="l21354"><span class="ln">21354 </span></a>        input (Tensor): the input tensor. 
<a name="l21355"><span class="ln">21355 </span></a> 
<a name="l21356"><span class="ln">21356 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l21357"><span class="ln">21357 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l21358"><span class="ln">21358 </span></a> 
<a name="l21359"><span class="ln">21359 </span></a> 
<a name="l21360"><span class="ln">21360 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l21361"><span class="ln">21361 </span></a> 
<a name="l21362"><span class="ln">21362 </span></a> 
<a name="l21363"><span class="ln">21363 </span></a>    Keyword args: 
<a name="l21364"><span class="ln">21364 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l21365"><span class="ln">21365 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l21366"><span class="ln">21366 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l21367"><span class="ln">21367 </span></a> 
<a name="l21368"><span class="ln">21368 </span></a>    Example:: 
<a name="l21369"><span class="ln">21369 </span></a> 
<a name="l21370"><span class="ln">21370 </span></a>        &gt;&gt;&gt; torch.nansum(torch.tensor([1., float(&quot;nan&quot;)])) 
<a name="l21371"><span class="ln">21371 </span></a>        tensor(1.) 
<a name="l21372"><span class="ln">21372 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1, 2], [3., float(&quot;nan&quot;)]]) 
<a name="l21373"><span class="ln">21373 </span></a>        &gt;&gt;&gt; torch.nansum(a) 
<a name="l21374"><span class="ln">21374 </span></a>        tensor(6.) 
<a name="l21375"><span class="ln">21375 </span></a>        &gt;&gt;&gt; torch.nansum(a, dim=0) 
<a name="l21376"><span class="ln">21376 </span></a>        tensor([4., 2.]) 
<a name="l21377"><span class="ln">21377 </span></a>        &gt;&gt;&gt; torch.nansum(a, dim=1) 
<a name="l21378"><span class="ln">21378 </span></a>        tensor([3., 3.]) 
<a name="l21379"><span class="ln">21379 </span></a>    &quot;&quot;&quot;</span>
<a name="l21380"><span class="ln">21380 </span></a>
<a name="l21381"><span class="ln">21381 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21382"><span class="ln">21382 </span></a><span class="s2">def </span><span class="s1">narrow</span><span class="s3">(</span>
<a name="l21383"><span class="ln">21383 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21384"><span class="ln">21384 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l21385"><span class="ln">21385 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21386"><span class="ln">21386 </span></a>    <span class="s1">length</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21387"><span class="ln">21387 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21388"><span class="ln">21388 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21389"><span class="ln">21389 </span></a>    narrow(input, dim, start, length) -&gt; Tensor 
<a name="l21390"><span class="ln">21390 </span></a> 
<a name="l21391"><span class="ln">21391 </span></a>    Returns a new tensor that is a narrowed version of :attr:`input` tensor. The 
<a name="l21392"><span class="ln">21392 </span></a>    dimension :attr:`dim` is input from :attr:`start` to ``start + length``. The 
<a name="l21393"><span class="ln">21393 </span></a>    returned tensor and :attr:`input` tensor share the same underlying storage. 
<a name="l21394"><span class="ln">21394 </span></a> 
<a name="l21395"><span class="ln">21395 </span></a>    Args: 
<a name="l21396"><span class="ln">21396 </span></a>        input (Tensor): the tensor to narrow 
<a name="l21397"><span class="ln">21397 </span></a>        dim (int): the dimension along which to narrow 
<a name="l21398"><span class="ln">21398 </span></a>        start (int or Tensor): index of the element to start the narrowed dimension 
<a name="l21399"><span class="ln">21399 </span></a>            from. Can be negative, which means indexing from the end of `dim`. If 
<a name="l21400"><span class="ln">21400 </span></a>            `Tensor`, it must be an 0-dim integral `Tensor` (bools not allowed) 
<a name="l21401"><span class="ln">21401 </span></a>        length (int): length of the narrowed dimension, must be weakly positive 
<a name="l21402"><span class="ln">21402 </span></a> 
<a name="l21403"><span class="ln">21403 </span></a>    Example:: 
<a name="l21404"><span class="ln">21404 </span></a> 
<a name="l21405"><span class="ln">21405 </span></a>        &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
<a name="l21406"><span class="ln">21406 </span></a>        &gt;&gt;&gt; torch.narrow(x, 0, 0, 2) 
<a name="l21407"><span class="ln">21407 </span></a>        tensor([[ 1,  2,  3], 
<a name="l21408"><span class="ln">21408 </span></a>                [ 4,  5,  6]]) 
<a name="l21409"><span class="ln">21409 </span></a>        &gt;&gt;&gt; torch.narrow(x, 1, 1, 2) 
<a name="l21410"><span class="ln">21410 </span></a>        tensor([[ 2,  3], 
<a name="l21411"><span class="ln">21411 </span></a>                [ 5,  6], 
<a name="l21412"><span class="ln">21412 </span></a>                [ 8,  9]]) 
<a name="l21413"><span class="ln">21413 </span></a>        &gt;&gt;&gt; torch.narrow(x, -1, torch.tensor(-1), 1) 
<a name="l21414"><span class="ln">21414 </span></a>        tensor([[3], 
<a name="l21415"><span class="ln">21415 </span></a>                [6], 
<a name="l21416"><span class="ln">21416 </span></a>                [9]]) 
<a name="l21417"><span class="ln">21417 </span></a>    &quot;&quot;&quot;</span>
<a name="l21418"><span class="ln">21418 </span></a>
<a name="l21419"><span class="ln">21419 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21420"><span class="ln">21420 </span></a><span class="s2">def </span><span class="s1">narrow</span><span class="s3">(</span>
<a name="l21421"><span class="ln">21421 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21422"><span class="ln">21422 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l21423"><span class="ln">21423 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21424"><span class="ln">21424 </span></a>    <span class="s1">length</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21425"><span class="ln">21425 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21426"><span class="ln">21426 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21427"><span class="ln">21427 </span></a>    narrow(input, dim, start, length) -&gt; Tensor 
<a name="l21428"><span class="ln">21428 </span></a> 
<a name="l21429"><span class="ln">21429 </span></a>    Returns a new tensor that is a narrowed version of :attr:`input` tensor. The 
<a name="l21430"><span class="ln">21430 </span></a>    dimension :attr:`dim` is input from :attr:`start` to ``start + length``. The 
<a name="l21431"><span class="ln">21431 </span></a>    returned tensor and :attr:`input` tensor share the same underlying storage. 
<a name="l21432"><span class="ln">21432 </span></a> 
<a name="l21433"><span class="ln">21433 </span></a>    Args: 
<a name="l21434"><span class="ln">21434 </span></a>        input (Tensor): the tensor to narrow 
<a name="l21435"><span class="ln">21435 </span></a>        dim (int): the dimension along which to narrow 
<a name="l21436"><span class="ln">21436 </span></a>        start (int or Tensor): index of the element to start the narrowed dimension 
<a name="l21437"><span class="ln">21437 </span></a>            from. Can be negative, which means indexing from the end of `dim`. If 
<a name="l21438"><span class="ln">21438 </span></a>            `Tensor`, it must be an 0-dim integral `Tensor` (bools not allowed) 
<a name="l21439"><span class="ln">21439 </span></a>        length (int): length of the narrowed dimension, must be weakly positive 
<a name="l21440"><span class="ln">21440 </span></a> 
<a name="l21441"><span class="ln">21441 </span></a>    Example:: 
<a name="l21442"><span class="ln">21442 </span></a> 
<a name="l21443"><span class="ln">21443 </span></a>        &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
<a name="l21444"><span class="ln">21444 </span></a>        &gt;&gt;&gt; torch.narrow(x, 0, 0, 2) 
<a name="l21445"><span class="ln">21445 </span></a>        tensor([[ 1,  2,  3], 
<a name="l21446"><span class="ln">21446 </span></a>                [ 4,  5,  6]]) 
<a name="l21447"><span class="ln">21447 </span></a>        &gt;&gt;&gt; torch.narrow(x, 1, 1, 2) 
<a name="l21448"><span class="ln">21448 </span></a>        tensor([[ 2,  3], 
<a name="l21449"><span class="ln">21449 </span></a>                [ 5,  6], 
<a name="l21450"><span class="ln">21450 </span></a>                [ 8,  9]]) 
<a name="l21451"><span class="ln">21451 </span></a>        &gt;&gt;&gt; torch.narrow(x, -1, torch.tensor(-1), 1) 
<a name="l21452"><span class="ln">21452 </span></a>        tensor([[3], 
<a name="l21453"><span class="ln">21453 </span></a>                [6], 
<a name="l21454"><span class="ln">21454 </span></a>                [9]]) 
<a name="l21455"><span class="ln">21455 </span></a>    &quot;&quot;&quot;</span>
<a name="l21456"><span class="ln">21456 </span></a>
<a name="l21457"><span class="ln">21457 </span></a><span class="s2">def </span><span class="s1">narrow_copy</span><span class="s3">(</span>
<a name="l21458"><span class="ln">21458 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21459"><span class="ln">21459 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l21460"><span class="ln">21460 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21461"><span class="ln">21461 </span></a>    <span class="s1">length</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21462"><span class="ln">21462 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21463"><span class="ln">21463 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21464"><span class="ln">21464 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21465"><span class="ln">21465 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21466"><span class="ln">21466 </span></a>    narrow_copy(input, dim, start, length, *, out=None) -&gt; Tensor 
<a name="l21467"><span class="ln">21467 </span></a> 
<a name="l21468"><span class="ln">21468 </span></a>    Same as :meth:`Tensor.narrow` except this returns a copy rather 
<a name="l21469"><span class="ln">21469 </span></a>    than shared storage. This is primarily for sparse tensors, which 
<a name="l21470"><span class="ln">21470 </span></a>    do not have a shared-storage narrow method. 
<a name="l21471"><span class="ln">21471 </span></a> 
<a name="l21472"><span class="ln">21472 </span></a>    Args: 
<a name="l21473"><span class="ln">21473 </span></a>        input (Tensor): the tensor to narrow 
<a name="l21474"><span class="ln">21474 </span></a>        dim (int): the dimension along which to narrow 
<a name="l21475"><span class="ln">21475 </span></a>        start (int): index of the element to start the narrowed dimension from. Can 
<a name="l21476"><span class="ln">21476 </span></a>            be negative, which means indexing from the end of `dim` 
<a name="l21477"><span class="ln">21477 </span></a>        length (int): length of the narrowed dimension, must be weakly positive 
<a name="l21478"><span class="ln">21478 </span></a> 
<a name="l21479"><span class="ln">21479 </span></a>    Keyword args: 
<a name="l21480"><span class="ln">21480 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21481"><span class="ln">21481 </span></a> 
<a name="l21482"><span class="ln">21482 </span></a>    Example:: 
<a name="l21483"><span class="ln">21483 </span></a> 
<a name="l21484"><span class="ln">21484 </span></a>        &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
<a name="l21485"><span class="ln">21485 </span></a>        &gt;&gt;&gt; torch.narrow_copy(x, 0, 0, 2) 
<a name="l21486"><span class="ln">21486 </span></a>        tensor([[ 1,  2,  3], 
<a name="l21487"><span class="ln">21487 </span></a>                [ 4,  5,  6]]) 
<a name="l21488"><span class="ln">21488 </span></a>        &gt;&gt;&gt; torch.narrow_copy(x, 1, 1, 2) 
<a name="l21489"><span class="ln">21489 </span></a>        tensor([[ 2,  3], 
<a name="l21490"><span class="ln">21490 </span></a>                [ 5,  6], 
<a name="l21491"><span class="ln">21491 </span></a>                [ 8,  9]]) 
<a name="l21492"><span class="ln">21492 </span></a>        &gt;&gt;&gt; s = torch.arange(16).reshape(2, 2, 2, 2).to_sparse(2) 
<a name="l21493"><span class="ln">21493 </span></a>        &gt;&gt;&gt; torch.narrow_copy(s, 0, 0, 1) 
<a name="l21494"><span class="ln">21494 </span></a>        tensor(indices=tensor([[0, 0], 
<a name="l21495"><span class="ln">21495 </span></a>                               [0, 1]]), 
<a name="l21496"><span class="ln">21496 </span></a>               values=tensor([[[0, 1], 
<a name="l21497"><span class="ln">21497 </span></a>                               [2, 3]], 
<a name="l21498"><span class="ln">21498 </span></a> 
<a name="l21499"><span class="ln">21499 </span></a>                              [[4, 5], 
<a name="l21500"><span class="ln">21500 </span></a>                               [6, 7]]]), 
<a name="l21501"><span class="ln">21501 </span></a>               size=(1, 2, 2, 2), nnz=2, layout=torch.sparse_coo) 
<a name="l21502"><span class="ln">21502 </span></a> 
<a name="l21503"><span class="ln">21503 </span></a>    .. seealso:: 
<a name="l21504"><span class="ln">21504 </span></a> 
<a name="l21505"><span class="ln">21505 </span></a>            :func:`torch.narrow` for a non copy variant 
<a name="l21506"><span class="ln">21506 </span></a>    &quot;&quot;&quot;</span>
<a name="l21507"><span class="ln">21507 </span></a>
<a name="l21508"><span class="ln">21508 </span></a><span class="s2">def </span><span class="s1">native_batch_norm</span><span class="s3">(</span>
<a name="l21509"><span class="ln">21509 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21510"><span class="ln">21510 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21511"><span class="ln">21511 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21512"><span class="ln">21512 </span></a>    <span class="s1">running_mean</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21513"><span class="ln">21513 </span></a>    <span class="s1">running_var</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21514"><span class="ln">21514 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l21515"><span class="ln">21515 </span></a>    <span class="s1">momentum</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l21516"><span class="ln">21516 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l21517"><span class="ln">21517 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21518"><span class="ln">21518 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21519"><span class="ln">21519 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21520"><span class="ln">21520 </span></a><span class="s2">def </span><span class="s1">native_channel_shuffle</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">groups</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21521"><span class="ln">21521 </span></a><span class="s2">def </span><span class="s1">native_dropout</span><span class="s3">(</span>
<a name="l21522"><span class="ln">21522 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21523"><span class="ln">21523 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l21524"><span class="ln">21524 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21525"><span class="ln">21525 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21526"><span class="ln">21526 </span></a><span class="s2">def </span><span class="s1">native_group_norm</span><span class="s3">(</span>
<a name="l21527"><span class="ln">21527 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21528"><span class="ln">21528 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21529"><span class="ln">21529 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21530"><span class="ln">21530 </span></a>    <span class="s1">N</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21531"><span class="ln">21531 </span></a>    <span class="s1">C</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21532"><span class="ln">21532 </span></a>    <span class="s1">HxW</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21533"><span class="ln">21533 </span></a>    <span class="s1">group</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l21534"><span class="ln">21534 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l21535"><span class="ln">21535 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21536"><span class="ln">21536 </span></a><span class="s2">def </span><span class="s1">native_layer_norm</span><span class="s3">(</span>
<a name="l21537"><span class="ln">21537 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21538"><span class="ln">21538 </span></a>    <span class="s1">normalized_shape</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l21539"><span class="ln">21539 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21540"><span class="ln">21540 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21541"><span class="ln">21541 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l21542"><span class="ln">21542 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21543"><span class="ln">21543 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21544"><span class="ln">21544 </span></a><span class="s2">def </span><span class="s1">native_norm</span><span class="s3">(</span>
<a name="l21545"><span class="ln">21545 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21546"><span class="ln">21546 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21547"><span class="ln">21547 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l21548"><span class="ln">21548 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l21549"><span class="ln">21549 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l21550"><span class="ln">21550 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21551"><span class="ln">21551 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21552"><span class="ln">21552 </span></a><span class="s2">def </span><span class="s1">native_norm</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">2</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21553"><span class="ln">21553 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21554"><span class="ln">21554 </span></a><span class="s2">def </span><span class="s1">ne</span><span class="s3">(</span>
<a name="l21555"><span class="ln">21555 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21556"><span class="ln">21556 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21557"><span class="ln">21557 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21558"><span class="ln">21558 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21559"><span class="ln">21559 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21560"><span class="ln">21560 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21561"><span class="ln">21561 </span></a>    ne(input, other, *, out=None) -&gt; Tensor 
<a name="l21562"><span class="ln">21562 </span></a> 
<a name="l21563"><span class="ln">21563 </span></a>    Computes :math:`\text{input} \neq \text{other}` element-wise. 
<a name="l21564"><span class="ln">21564 </span></a> 
<a name="l21565"><span class="ln">21565 </span></a> 
<a name="l21566"><span class="ln">21566 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l21567"><span class="ln">21567 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l21568"><span class="ln">21568 </span></a> 
<a name="l21569"><span class="ln">21569 </span></a>    Args: 
<a name="l21570"><span class="ln">21570 </span></a>        input (Tensor): the tensor to compare 
<a name="l21571"><span class="ln">21571 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l21572"><span class="ln">21572 </span></a> 
<a name="l21573"><span class="ln">21573 </span></a>    Keyword args: 
<a name="l21574"><span class="ln">21574 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21575"><span class="ln">21575 </span></a> 
<a name="l21576"><span class="ln">21576 </span></a>    Returns: 
<a name="l21577"><span class="ln">21577 </span></a>        A boolean tensor that is True where :attr:`input` is not equal to :attr:`other` and False elsewhere 
<a name="l21578"><span class="ln">21578 </span></a> 
<a name="l21579"><span class="ln">21579 </span></a>    Example:: 
<a name="l21580"><span class="ln">21580 </span></a> 
<a name="l21581"><span class="ln">21581 </span></a>        &gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l21582"><span class="ln">21582 </span></a>        tensor([[False, True], [True, False]]) 
<a name="l21583"><span class="ln">21583 </span></a>    &quot;&quot;&quot;</span>
<a name="l21584"><span class="ln">21584 </span></a>
<a name="l21585"><span class="ln">21585 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21586"><span class="ln">21586 </span></a><span class="s2">def </span><span class="s1">ne</span><span class="s3">(</span>
<a name="l21587"><span class="ln">21587 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21588"><span class="ln">21588 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l21589"><span class="ln">21589 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21590"><span class="ln">21590 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21591"><span class="ln">21591 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21592"><span class="ln">21592 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21593"><span class="ln">21593 </span></a>    ne(input, other, *, out=None) -&gt; Tensor 
<a name="l21594"><span class="ln">21594 </span></a> 
<a name="l21595"><span class="ln">21595 </span></a>    Computes :math:`\text{input} \neq \text{other}` element-wise. 
<a name="l21596"><span class="ln">21596 </span></a> 
<a name="l21597"><span class="ln">21597 </span></a> 
<a name="l21598"><span class="ln">21598 </span></a>    The second argument can be a number or a tensor whose shape is 
<a name="l21599"><span class="ln">21599 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;` with the first argument. 
<a name="l21600"><span class="ln">21600 </span></a> 
<a name="l21601"><span class="ln">21601 </span></a>    Args: 
<a name="l21602"><span class="ln">21602 </span></a>        input (Tensor): the tensor to compare 
<a name="l21603"><span class="ln">21603 </span></a>        other (Tensor or float): the tensor or value to compare 
<a name="l21604"><span class="ln">21604 </span></a> 
<a name="l21605"><span class="ln">21605 </span></a>    Keyword args: 
<a name="l21606"><span class="ln">21606 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21607"><span class="ln">21607 </span></a> 
<a name="l21608"><span class="ln">21608 </span></a>    Returns: 
<a name="l21609"><span class="ln">21609 </span></a>        A boolean tensor that is True where :attr:`input` is not equal to :attr:`other` and False elsewhere 
<a name="l21610"><span class="ln">21610 </span></a> 
<a name="l21611"><span class="ln">21611 </span></a>    Example:: 
<a name="l21612"><span class="ln">21612 </span></a> 
<a name="l21613"><span class="ln">21613 </span></a>        &gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) 
<a name="l21614"><span class="ln">21614 </span></a>        tensor([[False, True], [True, False]]) 
<a name="l21615"><span class="ln">21615 </span></a>    &quot;&quot;&quot;</span>
<a name="l21616"><span class="ln">21616 </span></a>
<a name="l21617"><span class="ln">21617 </span></a><span class="s2">def </span><span class="s1">neg</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21618"><span class="ln">21618 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21619"><span class="ln">21619 </span></a>    neg(input, *, out=None) -&gt; Tensor 
<a name="l21620"><span class="ln">21620 </span></a> 
<a name="l21621"><span class="ln">21621 </span></a>    Returns a new tensor with the negative of the elements of :attr:`input`. 
<a name="l21622"><span class="ln">21622 </span></a> 
<a name="l21623"><span class="ln">21623 </span></a>    .. math:: 
<a name="l21624"><span class="ln">21624 </span></a>        \text{out} = -1 \times \text{input} 
<a name="l21625"><span class="ln">21625 </span></a> 
<a name="l21626"><span class="ln">21626 </span></a>    Args: 
<a name="l21627"><span class="ln">21627 </span></a>        input (Tensor): the input tensor. 
<a name="l21628"><span class="ln">21628 </span></a> 
<a name="l21629"><span class="ln">21629 </span></a>    Keyword args: 
<a name="l21630"><span class="ln">21630 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21631"><span class="ln">21631 </span></a> 
<a name="l21632"><span class="ln">21632 </span></a>    Example:: 
<a name="l21633"><span class="ln">21633 </span></a> 
<a name="l21634"><span class="ln">21634 </span></a>        &gt;&gt;&gt; a = torch.randn(5) 
<a name="l21635"><span class="ln">21635 </span></a>        &gt;&gt;&gt; a 
<a name="l21636"><span class="ln">21636 </span></a>        tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940]) 
<a name="l21637"><span class="ln">21637 </span></a>        &gt;&gt;&gt; torch.neg(a) 
<a name="l21638"><span class="ln">21638 </span></a>        tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940]) 
<a name="l21639"><span class="ln">21639 </span></a>    &quot;&quot;&quot;</span>
<a name="l21640"><span class="ln">21640 </span></a>
<a name="l21641"><span class="ln">21641 </span></a><span class="s2">def </span><span class="s1">neg_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21642"><span class="ln">21642 </span></a><span class="s2">def </span><span class="s1">negative</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21643"><span class="ln">21643 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21644"><span class="ln">21644 </span></a>    negative(input, *, out=None) -&gt; Tensor 
<a name="l21645"><span class="ln">21645 </span></a> 
<a name="l21646"><span class="ln">21646 </span></a>    Alias for :func:`torch.neg` 
<a name="l21647"><span class="ln">21647 </span></a>    &quot;&quot;&quot;</span>
<a name="l21648"><span class="ln">21648 </span></a>
<a name="l21649"><span class="ln">21649 </span></a><span class="s2">def </span><span class="s1">negative_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21650"><span class="ln">21650 </span></a><span class="s2">def </span><span class="s1">nextafter</span><span class="s3">(</span>
<a name="l21651"><span class="ln">21651 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21652"><span class="ln">21652 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21653"><span class="ln">21653 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21654"><span class="ln">21654 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21655"><span class="ln">21655 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21656"><span class="ln">21656 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21657"><span class="ln">21657 </span></a>    nextafter(input, other, *, out=None) -&gt; Tensor 
<a name="l21658"><span class="ln">21658 </span></a> 
<a name="l21659"><span class="ln">21659 </span></a>    Return the next floating-point value after :attr:`input` towards :attr:`other`, elementwise. 
<a name="l21660"><span class="ln">21660 </span></a> 
<a name="l21661"><span class="ln">21661 </span></a>    The shapes of ``input`` and ``other`` must be 
<a name="l21662"><span class="ln">21662 </span></a>    :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l21663"><span class="ln">21663 </span></a> 
<a name="l21664"><span class="ln">21664 </span></a>    Args: 
<a name="l21665"><span class="ln">21665 </span></a>        input (Tensor): the first input tensor 
<a name="l21666"><span class="ln">21666 </span></a>        other (Tensor): the second input tensor 
<a name="l21667"><span class="ln">21667 </span></a> 
<a name="l21668"><span class="ln">21668 </span></a>    Keyword args: 
<a name="l21669"><span class="ln">21669 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21670"><span class="ln">21670 </span></a> 
<a name="l21671"><span class="ln">21671 </span></a>    Example:: 
<a name="l21672"><span class="ln">21672 </span></a> 
<a name="l21673"><span class="ln">21673 </span></a>        &gt;&gt;&gt; eps = torch.finfo(torch.float32).eps 
<a name="l21674"><span class="ln">21674 </span></a>        &gt;&gt;&gt; torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps]) 
<a name="l21675"><span class="ln">21675 </span></a>        tensor([True, True]) 
<a name="l21676"><span class="ln">21676 </span></a>    &quot;&quot;&quot;</span>
<a name="l21677"><span class="ln">21677 </span></a>
<a name="l21678"><span class="ln">21678 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21679"><span class="ln">21679 </span></a><span class="s2">def </span><span class="s1">nonzero</span><span class="s3">(</span>
<a name="l21680"><span class="ln">21680 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21681"><span class="ln">21681 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21682"><span class="ln">21682 </span></a>    <span class="s1">as_tuple</span><span class="s2">: </span><span class="s1">Literal</span><span class="s3">[</span><span class="s2">False</span><span class="s3">] </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l21683"><span class="ln">21683 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21684"><span class="ln">21684 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21685"><span class="ln">21685 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21686"><span class="ln">21686 </span></a>    nonzero(input, *, out=None, as_tuple=False) -&gt; LongTensor or tuple of LongTensors 
<a name="l21687"><span class="ln">21687 </span></a> 
<a name="l21688"><span class="ln">21688 </span></a>    .. note:: 
<a name="l21689"><span class="ln">21689 </span></a>        :func:`torch.nonzero(..., as_tuple=False) &lt;torch.nonzero&gt;` (default) returns a 
<a name="l21690"><span class="ln">21690 </span></a>        2-D tensor where each row is the index for a nonzero value. 
<a name="l21691"><span class="ln">21691 </span></a> 
<a name="l21692"><span class="ln">21692 </span></a>        :func:`torch.nonzero(..., as_tuple=True) &lt;torch.nonzero&gt;` returns a tuple of 1-D 
<a name="l21693"><span class="ln">21693 </span></a>        index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]`` 
<a name="l21694"><span class="ln">21694 </span></a>        gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor 
<a name="l21695"><span class="ln">21695 </span></a>        contains nonzero indices for a certain dimension. 
<a name="l21696"><span class="ln">21696 </span></a> 
<a name="l21697"><span class="ln">21697 </span></a>        See below for more details on the two behaviors. 
<a name="l21698"><span class="ln">21698 </span></a> 
<a name="l21699"><span class="ln">21699 </span></a>        When :attr:`input` is on CUDA, :func:`torch.nonzero() &lt;torch.nonzero&gt;` causes 
<a name="l21700"><span class="ln">21700 </span></a>        host-device synchronization. 
<a name="l21701"><span class="ln">21701 </span></a> 
<a name="l21702"><span class="ln">21702 </span></a>    **When** :attr:`as_tuple` **is** ``False`` **(default)**: 
<a name="l21703"><span class="ln">21703 </span></a> 
<a name="l21704"><span class="ln">21704 </span></a>    Returns a tensor containing the indices of all non-zero elements of 
<a name="l21705"><span class="ln">21705 </span></a>    :attr:`input`.  Each row in the result contains the indices of a non-zero 
<a name="l21706"><span class="ln">21706 </span></a>    element in :attr:`input`. The result is sorted lexicographically, with 
<a name="l21707"><span class="ln">21707 </span></a>    the last index changing the fastest (C-style). 
<a name="l21708"><span class="ln">21708 </span></a> 
<a name="l21709"><span class="ln">21709 </span></a>    If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor 
<a name="l21710"><span class="ln">21710 </span></a>    :attr:`out` is of size :math:`(z \times n)`, where :math:`z` is the total number of 
<a name="l21711"><span class="ln">21711 </span></a>    non-zero elements in the :attr:`input` tensor. 
<a name="l21712"><span class="ln">21712 </span></a> 
<a name="l21713"><span class="ln">21713 </span></a>    **When** :attr:`as_tuple` **is** ``True``: 
<a name="l21714"><span class="ln">21714 </span></a> 
<a name="l21715"><span class="ln">21715 </span></a>    Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`, 
<a name="l21716"><span class="ln">21716 </span></a>    each containing the indices (in that dimension) of all non-zero elements of 
<a name="l21717"><span class="ln">21717 </span></a>    :attr:`input` . 
<a name="l21718"><span class="ln">21718 </span></a> 
<a name="l21719"><span class="ln">21719 </span></a>    If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n` 
<a name="l21720"><span class="ln">21720 </span></a>    tensors of size :math:`z`, where :math:`z` is the total number of 
<a name="l21721"><span class="ln">21721 </span></a>    non-zero elements in the :attr:`input` tensor. 
<a name="l21722"><span class="ln">21722 </span></a> 
<a name="l21723"><span class="ln">21723 </span></a>    As a special case, when :attr:`input` has zero dimensions and a nonzero scalar 
<a name="l21724"><span class="ln">21724 </span></a>    value, it is treated as a one-dimensional tensor with one element. 
<a name="l21725"><span class="ln">21725 </span></a> 
<a name="l21726"><span class="ln">21726 </span></a>    Args: 
<a name="l21727"><span class="ln">21727 </span></a>        input (Tensor): the input tensor. 
<a name="l21728"><span class="ln">21728 </span></a> 
<a name="l21729"><span class="ln">21729 </span></a>    Keyword args: 
<a name="l21730"><span class="ln">21730 </span></a>        out (LongTensor, optional): the output tensor containing indices 
<a name="l21731"><span class="ln">21731 </span></a> 
<a name="l21732"><span class="ln">21732 </span></a>    Returns: 
<a name="l21733"><span class="ln">21733 </span></a>        LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output 
<a name="l21734"><span class="ln">21734 </span></a>        tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for 
<a name="l21735"><span class="ln">21735 </span></a>        each dimension, containing the indices of each nonzero element along that 
<a name="l21736"><span class="ln">21736 </span></a>        dimension. 
<a name="l21737"><span class="ln">21737 </span></a> 
<a name="l21738"><span class="ln">21738 </span></a>    Example:: 
<a name="l21739"><span class="ln">21739 </span></a> 
<a name="l21740"><span class="ln">21740 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1])) 
<a name="l21741"><span class="ln">21741 </span></a>        tensor([[ 0], 
<a name="l21742"><span class="ln">21742 </span></a>                [ 1], 
<a name="l21743"><span class="ln">21743 </span></a>                [ 2], 
<a name="l21744"><span class="ln">21744 </span></a>                [ 4]]) 
<a name="l21745"><span class="ln">21745 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], 
<a name="l21746"><span class="ln">21746 </span></a>        ...                             [0.0, 0.4, 0.0, 0.0], 
<a name="l21747"><span class="ln">21747 </span></a>        ...                             [0.0, 0.0, 1.2, 0.0], 
<a name="l21748"><span class="ln">21748 </span></a>        ...                             [0.0, 0.0, 0.0,-0.4]])) 
<a name="l21749"><span class="ln">21749 </span></a>        tensor([[ 0,  0], 
<a name="l21750"><span class="ln">21750 </span></a>                [ 1,  1], 
<a name="l21751"><span class="ln">21751 </span></a>                [ 2,  2], 
<a name="l21752"><span class="ln">21752 </span></a>                [ 3,  3]]) 
<a name="l21753"><span class="ln">21753 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True) 
<a name="l21754"><span class="ln">21754 </span></a>        (tensor([0, 1, 2, 4]),) 
<a name="l21755"><span class="ln">21755 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], 
<a name="l21756"><span class="ln">21756 </span></a>        ...                             [0.0, 0.4, 0.0, 0.0], 
<a name="l21757"><span class="ln">21757 </span></a>        ...                             [0.0, 0.0, 1.2, 0.0], 
<a name="l21758"><span class="ln">21758 </span></a>        ...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True) 
<a name="l21759"><span class="ln">21759 </span></a>        (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])) 
<a name="l21760"><span class="ln">21760 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True) 
<a name="l21761"><span class="ln">21761 </span></a>        (tensor([0]),) 
<a name="l21762"><span class="ln">21762 </span></a>    &quot;&quot;&quot;</span>
<a name="l21763"><span class="ln">21763 </span></a>
<a name="l21764"><span class="ln">21764 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21765"><span class="ln">21765 </span></a><span class="s2">def </span><span class="s1">nonzero</span><span class="s3">(</span>
<a name="l21766"><span class="ln">21766 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21767"><span class="ln">21767 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21768"><span class="ln">21768 </span></a>    <span class="s1">as_tuple</span><span class="s2">: </span><span class="s1">Literal</span><span class="s3">[</span><span class="s2">True</span><span class="s3">],</span>
<a name="l21769"><span class="ln">21769 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l21770"><span class="ln">21770 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21771"><span class="ln">21771 </span></a>    nonzero(input, *, out=None, as_tuple=False) -&gt; LongTensor or tuple of LongTensors 
<a name="l21772"><span class="ln">21772 </span></a> 
<a name="l21773"><span class="ln">21773 </span></a>    .. note:: 
<a name="l21774"><span class="ln">21774 </span></a>        :func:`torch.nonzero(..., as_tuple=False) &lt;torch.nonzero&gt;` (default) returns a 
<a name="l21775"><span class="ln">21775 </span></a>        2-D tensor where each row is the index for a nonzero value. 
<a name="l21776"><span class="ln">21776 </span></a> 
<a name="l21777"><span class="ln">21777 </span></a>        :func:`torch.nonzero(..., as_tuple=True) &lt;torch.nonzero&gt;` returns a tuple of 1-D 
<a name="l21778"><span class="ln">21778 </span></a>        index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]`` 
<a name="l21779"><span class="ln">21779 </span></a>        gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor 
<a name="l21780"><span class="ln">21780 </span></a>        contains nonzero indices for a certain dimension. 
<a name="l21781"><span class="ln">21781 </span></a> 
<a name="l21782"><span class="ln">21782 </span></a>        See below for more details on the two behaviors. 
<a name="l21783"><span class="ln">21783 </span></a> 
<a name="l21784"><span class="ln">21784 </span></a>        When :attr:`input` is on CUDA, :func:`torch.nonzero() &lt;torch.nonzero&gt;` causes 
<a name="l21785"><span class="ln">21785 </span></a>        host-device synchronization. 
<a name="l21786"><span class="ln">21786 </span></a> 
<a name="l21787"><span class="ln">21787 </span></a>    **When** :attr:`as_tuple` **is** ``False`` **(default)**: 
<a name="l21788"><span class="ln">21788 </span></a> 
<a name="l21789"><span class="ln">21789 </span></a>    Returns a tensor containing the indices of all non-zero elements of 
<a name="l21790"><span class="ln">21790 </span></a>    :attr:`input`.  Each row in the result contains the indices of a non-zero 
<a name="l21791"><span class="ln">21791 </span></a>    element in :attr:`input`. The result is sorted lexicographically, with 
<a name="l21792"><span class="ln">21792 </span></a>    the last index changing the fastest (C-style). 
<a name="l21793"><span class="ln">21793 </span></a> 
<a name="l21794"><span class="ln">21794 </span></a>    If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor 
<a name="l21795"><span class="ln">21795 </span></a>    :attr:`out` is of size :math:`(z \times n)`, where :math:`z` is the total number of 
<a name="l21796"><span class="ln">21796 </span></a>    non-zero elements in the :attr:`input` tensor. 
<a name="l21797"><span class="ln">21797 </span></a> 
<a name="l21798"><span class="ln">21798 </span></a>    **When** :attr:`as_tuple` **is** ``True``: 
<a name="l21799"><span class="ln">21799 </span></a> 
<a name="l21800"><span class="ln">21800 </span></a>    Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`, 
<a name="l21801"><span class="ln">21801 </span></a>    each containing the indices (in that dimension) of all non-zero elements of 
<a name="l21802"><span class="ln">21802 </span></a>    :attr:`input` . 
<a name="l21803"><span class="ln">21803 </span></a> 
<a name="l21804"><span class="ln">21804 </span></a>    If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n` 
<a name="l21805"><span class="ln">21805 </span></a>    tensors of size :math:`z`, where :math:`z` is the total number of 
<a name="l21806"><span class="ln">21806 </span></a>    non-zero elements in the :attr:`input` tensor. 
<a name="l21807"><span class="ln">21807 </span></a> 
<a name="l21808"><span class="ln">21808 </span></a>    As a special case, when :attr:`input` has zero dimensions and a nonzero scalar 
<a name="l21809"><span class="ln">21809 </span></a>    value, it is treated as a one-dimensional tensor with one element. 
<a name="l21810"><span class="ln">21810 </span></a> 
<a name="l21811"><span class="ln">21811 </span></a>    Args: 
<a name="l21812"><span class="ln">21812 </span></a>        input (Tensor): the input tensor. 
<a name="l21813"><span class="ln">21813 </span></a> 
<a name="l21814"><span class="ln">21814 </span></a>    Keyword args: 
<a name="l21815"><span class="ln">21815 </span></a>        out (LongTensor, optional): the output tensor containing indices 
<a name="l21816"><span class="ln">21816 </span></a> 
<a name="l21817"><span class="ln">21817 </span></a>    Returns: 
<a name="l21818"><span class="ln">21818 </span></a>        LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output 
<a name="l21819"><span class="ln">21819 </span></a>        tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for 
<a name="l21820"><span class="ln">21820 </span></a>        each dimension, containing the indices of each nonzero element along that 
<a name="l21821"><span class="ln">21821 </span></a>        dimension. 
<a name="l21822"><span class="ln">21822 </span></a> 
<a name="l21823"><span class="ln">21823 </span></a>    Example:: 
<a name="l21824"><span class="ln">21824 </span></a> 
<a name="l21825"><span class="ln">21825 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1])) 
<a name="l21826"><span class="ln">21826 </span></a>        tensor([[ 0], 
<a name="l21827"><span class="ln">21827 </span></a>                [ 1], 
<a name="l21828"><span class="ln">21828 </span></a>                [ 2], 
<a name="l21829"><span class="ln">21829 </span></a>                [ 4]]) 
<a name="l21830"><span class="ln">21830 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], 
<a name="l21831"><span class="ln">21831 </span></a>        ...                             [0.0, 0.4, 0.0, 0.0], 
<a name="l21832"><span class="ln">21832 </span></a>        ...                             [0.0, 0.0, 1.2, 0.0], 
<a name="l21833"><span class="ln">21833 </span></a>        ...                             [0.0, 0.0, 0.0,-0.4]])) 
<a name="l21834"><span class="ln">21834 </span></a>        tensor([[ 0,  0], 
<a name="l21835"><span class="ln">21835 </span></a>                [ 1,  1], 
<a name="l21836"><span class="ln">21836 </span></a>                [ 2,  2], 
<a name="l21837"><span class="ln">21837 </span></a>                [ 3,  3]]) 
<a name="l21838"><span class="ln">21838 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True) 
<a name="l21839"><span class="ln">21839 </span></a>        (tensor([0, 1, 2, 4]),) 
<a name="l21840"><span class="ln">21840 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], 
<a name="l21841"><span class="ln">21841 </span></a>        ...                             [0.0, 0.4, 0.0, 0.0], 
<a name="l21842"><span class="ln">21842 </span></a>        ...                             [0.0, 0.0, 1.2, 0.0], 
<a name="l21843"><span class="ln">21843 </span></a>        ...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True) 
<a name="l21844"><span class="ln">21844 </span></a>        (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])) 
<a name="l21845"><span class="ln">21845 </span></a>        &gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True) 
<a name="l21846"><span class="ln">21846 </span></a>        (tensor([0]),) 
<a name="l21847"><span class="ln">21847 </span></a>    &quot;&quot;&quot;</span>
<a name="l21848"><span class="ln">21848 </span></a>
<a name="l21849"><span class="ln">21849 </span></a><span class="s2">def </span><span class="s1">nonzero_static</span><span class="s3">(</span>
<a name="l21850"><span class="ln">21850 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21851"><span class="ln">21851 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21852"><span class="ln">21852 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l21853"><span class="ln">21853 </span></a>    <span class="s1">fill_value</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l21854"><span class="ln">21854 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21855"><span class="ln">21855 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21856"><span class="ln">21856 </span></a><span class="s2">def </span><span class="s1">norm_except_dim</span><span class="s3">(</span><span class="s1">v</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">pow</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">2</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l21857"><span class="ln">21857 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21858"><span class="ln">21858 </span></a><span class="s2">def </span><span class="s1">normal</span><span class="s3">(</span>
<a name="l21859"><span class="ln">21859 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21860"><span class="ln">21860 </span></a>    <span class="s1">std</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21861"><span class="ln">21861 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21862"><span class="ln">21862 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21863"><span class="ln">21863 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21864"><span class="ln">21864 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21865"><span class="ln">21865 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21866"><span class="ln">21866 </span></a>    normal(mean, std, *, generator=None, out=None) -&gt; Tensor 
<a name="l21867"><span class="ln">21867 </span></a> 
<a name="l21868"><span class="ln">21868 </span></a>    Returns a tensor of random numbers drawn from separate normal distributions 
<a name="l21869"><span class="ln">21869 </span></a>    whose mean and standard deviation are given. 
<a name="l21870"><span class="ln">21870 </span></a> 
<a name="l21871"><span class="ln">21871 </span></a>    The :attr:`mean` is a tensor with the mean of 
<a name="l21872"><span class="ln">21872 </span></a>    each output element's normal distribution 
<a name="l21873"><span class="ln">21873 </span></a> 
<a name="l21874"><span class="ln">21874 </span></a>    The :attr:`std` is a tensor with the standard deviation of 
<a name="l21875"><span class="ln">21875 </span></a>    each output element's normal distribution 
<a name="l21876"><span class="ln">21876 </span></a> 
<a name="l21877"><span class="ln">21877 </span></a>    The shapes of :attr:`mean` and :attr:`std` don't need to match, but the 
<a name="l21878"><span class="ln">21878 </span></a>    total number of elements in each tensor need to be the same. 
<a name="l21879"><span class="ln">21879 </span></a> 
<a name="l21880"><span class="ln">21880 </span></a>    .. note:: When the shapes do not match, the shape of :attr:`mean` 
<a name="l21881"><span class="ln">21881 </span></a>              is used as the shape for the returned output tensor 
<a name="l21882"><span class="ln">21882 </span></a> 
<a name="l21883"><span class="ln">21883 </span></a>    .. note:: When :attr:`std` is a CUDA tensor, this function synchronizes 
<a name="l21884"><span class="ln">21884 </span></a>              its device with the CPU. 
<a name="l21885"><span class="ln">21885 </span></a> 
<a name="l21886"><span class="ln">21886 </span></a>    Args: 
<a name="l21887"><span class="ln">21887 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l21888"><span class="ln">21888 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l21889"><span class="ln">21889 </span></a> 
<a name="l21890"><span class="ln">21890 </span></a>    Keyword args: 
<a name="l21891"><span class="ln">21891 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l21892"><span class="ln">21892 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21893"><span class="ln">21893 </span></a> 
<a name="l21894"><span class="ln">21894 </span></a>    Example:: 
<a name="l21895"><span class="ln">21895 </span></a> 
<a name="l21896"><span class="ln">21896 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) 
<a name="l21897"><span class="ln">21897 </span></a>        tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134, 
<a name="l21898"><span class="ln">21898 </span></a>                  8.0505,   8.1408,   9.0563,  10.0566]) 
<a name="l21899"><span class="ln">21899 </span></a> 
<a name="l21900"><span class="ln">21900 </span></a>    .. function:: normal(mean=0.0, std, *, out=None) -&gt; Tensor 
<a name="l21901"><span class="ln">21901 </span></a>       :noindex: 
<a name="l21902"><span class="ln">21902 </span></a> 
<a name="l21903"><span class="ln">21903 </span></a>    Similar to the function above, but the means are shared among all drawn 
<a name="l21904"><span class="ln">21904 </span></a>    elements. 
<a name="l21905"><span class="ln">21905 </span></a> 
<a name="l21906"><span class="ln">21906 </span></a>    Args: 
<a name="l21907"><span class="ln">21907 </span></a>        mean (float, optional): the mean for all distributions 
<a name="l21908"><span class="ln">21908 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l21909"><span class="ln">21909 </span></a> 
<a name="l21910"><span class="ln">21910 </span></a>    Keyword args: 
<a name="l21911"><span class="ln">21911 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21912"><span class="ln">21912 </span></a> 
<a name="l21913"><span class="ln">21913 </span></a>    Example:: 
<a name="l21914"><span class="ln">21914 </span></a> 
<a name="l21915"><span class="ln">21915 </span></a>        &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.)) 
<a name="l21916"><span class="ln">21916 </span></a>        tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303]) 
<a name="l21917"><span class="ln">21917 </span></a> 
<a name="l21918"><span class="ln">21918 </span></a>    .. function:: normal(mean, std=1.0, *, out=None) -&gt; Tensor 
<a name="l21919"><span class="ln">21919 </span></a>       :noindex: 
<a name="l21920"><span class="ln">21920 </span></a> 
<a name="l21921"><span class="ln">21921 </span></a>    Similar to the function above, but the standard deviations are shared among 
<a name="l21922"><span class="ln">21922 </span></a>    all drawn elements. 
<a name="l21923"><span class="ln">21923 </span></a> 
<a name="l21924"><span class="ln">21924 </span></a>    Args: 
<a name="l21925"><span class="ln">21925 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l21926"><span class="ln">21926 </span></a>        std (float, optional): the standard deviation for all distributions 
<a name="l21927"><span class="ln">21927 </span></a> 
<a name="l21928"><span class="ln">21928 </span></a>    Keyword args: 
<a name="l21929"><span class="ln">21929 </span></a>        out (Tensor, optional): the output tensor 
<a name="l21930"><span class="ln">21930 </span></a> 
<a name="l21931"><span class="ln">21931 </span></a>    Example:: 
<a name="l21932"><span class="ln">21932 </span></a> 
<a name="l21933"><span class="ln">21933 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.)) 
<a name="l21934"><span class="ln">21934 </span></a>        tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361]) 
<a name="l21935"><span class="ln">21935 </span></a> 
<a name="l21936"><span class="ln">21936 </span></a>    .. function:: normal(mean, std, size, *, out=None) -&gt; Tensor 
<a name="l21937"><span class="ln">21937 </span></a>       :noindex: 
<a name="l21938"><span class="ln">21938 </span></a> 
<a name="l21939"><span class="ln">21939 </span></a>    Similar to the function above, but the means and standard deviations are shared 
<a name="l21940"><span class="ln">21940 </span></a>    among all drawn elements. The resulting tensor has size given by :attr:`size`. 
<a name="l21941"><span class="ln">21941 </span></a> 
<a name="l21942"><span class="ln">21942 </span></a>    Args: 
<a name="l21943"><span class="ln">21943 </span></a>        mean (float): the mean for all distributions 
<a name="l21944"><span class="ln">21944 </span></a>        std (float): the standard deviation for all distributions 
<a name="l21945"><span class="ln">21945 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l21946"><span class="ln">21946 </span></a> 
<a name="l21947"><span class="ln">21947 </span></a>    Keyword args: 
<a name="l21948"><span class="ln">21948 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21949"><span class="ln">21949 </span></a> 
<a name="l21950"><span class="ln">21950 </span></a>    Example:: 
<a name="l21951"><span class="ln">21951 </span></a> 
<a name="l21952"><span class="ln">21952 </span></a>        &gt;&gt;&gt; torch.normal(2, 3, size=(1, 4)) 
<a name="l21953"><span class="ln">21953 </span></a>        tensor([[-1.3987, -1.9544,  3.6048,  0.7909]]) 
<a name="l21954"><span class="ln">21954 </span></a>    &quot;&quot;&quot;</span>
<a name="l21955"><span class="ln">21955 </span></a>
<a name="l21956"><span class="ln">21956 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l21957"><span class="ln">21957 </span></a><span class="s2">def </span><span class="s1">normal</span><span class="s3">(</span>
<a name="l21958"><span class="ln">21958 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l21959"><span class="ln">21959 </span></a>    <span class="s1">std</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l21960"><span class="ln">21960 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l21961"><span class="ln">21961 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21962"><span class="ln">21962 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l21963"><span class="ln">21963 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l21964"><span class="ln">21964 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l21965"><span class="ln">21965 </span></a>    normal(mean, std, *, generator=None, out=None) -&gt; Tensor 
<a name="l21966"><span class="ln">21966 </span></a> 
<a name="l21967"><span class="ln">21967 </span></a>    Returns a tensor of random numbers drawn from separate normal distributions 
<a name="l21968"><span class="ln">21968 </span></a>    whose mean and standard deviation are given. 
<a name="l21969"><span class="ln">21969 </span></a> 
<a name="l21970"><span class="ln">21970 </span></a>    The :attr:`mean` is a tensor with the mean of 
<a name="l21971"><span class="ln">21971 </span></a>    each output element's normal distribution 
<a name="l21972"><span class="ln">21972 </span></a> 
<a name="l21973"><span class="ln">21973 </span></a>    The :attr:`std` is a tensor with the standard deviation of 
<a name="l21974"><span class="ln">21974 </span></a>    each output element's normal distribution 
<a name="l21975"><span class="ln">21975 </span></a> 
<a name="l21976"><span class="ln">21976 </span></a>    The shapes of :attr:`mean` and :attr:`std` don't need to match, but the 
<a name="l21977"><span class="ln">21977 </span></a>    total number of elements in each tensor need to be the same. 
<a name="l21978"><span class="ln">21978 </span></a> 
<a name="l21979"><span class="ln">21979 </span></a>    .. note:: When the shapes do not match, the shape of :attr:`mean` 
<a name="l21980"><span class="ln">21980 </span></a>              is used as the shape for the returned output tensor 
<a name="l21981"><span class="ln">21981 </span></a> 
<a name="l21982"><span class="ln">21982 </span></a>    .. note:: When :attr:`std` is a CUDA tensor, this function synchronizes 
<a name="l21983"><span class="ln">21983 </span></a>              its device with the CPU. 
<a name="l21984"><span class="ln">21984 </span></a> 
<a name="l21985"><span class="ln">21985 </span></a>    Args: 
<a name="l21986"><span class="ln">21986 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l21987"><span class="ln">21987 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l21988"><span class="ln">21988 </span></a> 
<a name="l21989"><span class="ln">21989 </span></a>    Keyword args: 
<a name="l21990"><span class="ln">21990 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l21991"><span class="ln">21991 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l21992"><span class="ln">21992 </span></a> 
<a name="l21993"><span class="ln">21993 </span></a>    Example:: 
<a name="l21994"><span class="ln">21994 </span></a> 
<a name="l21995"><span class="ln">21995 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) 
<a name="l21996"><span class="ln">21996 </span></a>        tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134, 
<a name="l21997"><span class="ln">21997 </span></a>                  8.0505,   8.1408,   9.0563,  10.0566]) 
<a name="l21998"><span class="ln">21998 </span></a> 
<a name="l21999"><span class="ln">21999 </span></a>    .. function:: normal(mean=0.0, std, *, out=None) -&gt; Tensor 
<a name="l22000"><span class="ln">22000 </span></a>       :noindex: 
<a name="l22001"><span class="ln">22001 </span></a> 
<a name="l22002"><span class="ln">22002 </span></a>    Similar to the function above, but the means are shared among all drawn 
<a name="l22003"><span class="ln">22003 </span></a>    elements. 
<a name="l22004"><span class="ln">22004 </span></a> 
<a name="l22005"><span class="ln">22005 </span></a>    Args: 
<a name="l22006"><span class="ln">22006 </span></a>        mean (float, optional): the mean for all distributions 
<a name="l22007"><span class="ln">22007 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l22008"><span class="ln">22008 </span></a> 
<a name="l22009"><span class="ln">22009 </span></a>    Keyword args: 
<a name="l22010"><span class="ln">22010 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22011"><span class="ln">22011 </span></a> 
<a name="l22012"><span class="ln">22012 </span></a>    Example:: 
<a name="l22013"><span class="ln">22013 </span></a> 
<a name="l22014"><span class="ln">22014 </span></a>        &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.)) 
<a name="l22015"><span class="ln">22015 </span></a>        tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303]) 
<a name="l22016"><span class="ln">22016 </span></a> 
<a name="l22017"><span class="ln">22017 </span></a>    .. function:: normal(mean, std=1.0, *, out=None) -&gt; Tensor 
<a name="l22018"><span class="ln">22018 </span></a>       :noindex: 
<a name="l22019"><span class="ln">22019 </span></a> 
<a name="l22020"><span class="ln">22020 </span></a>    Similar to the function above, but the standard deviations are shared among 
<a name="l22021"><span class="ln">22021 </span></a>    all drawn elements. 
<a name="l22022"><span class="ln">22022 </span></a> 
<a name="l22023"><span class="ln">22023 </span></a>    Args: 
<a name="l22024"><span class="ln">22024 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l22025"><span class="ln">22025 </span></a>        std (float, optional): the standard deviation for all distributions 
<a name="l22026"><span class="ln">22026 </span></a> 
<a name="l22027"><span class="ln">22027 </span></a>    Keyword args: 
<a name="l22028"><span class="ln">22028 </span></a>        out (Tensor, optional): the output tensor 
<a name="l22029"><span class="ln">22029 </span></a> 
<a name="l22030"><span class="ln">22030 </span></a>    Example:: 
<a name="l22031"><span class="ln">22031 </span></a> 
<a name="l22032"><span class="ln">22032 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.)) 
<a name="l22033"><span class="ln">22033 </span></a>        tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361]) 
<a name="l22034"><span class="ln">22034 </span></a> 
<a name="l22035"><span class="ln">22035 </span></a>    .. function:: normal(mean, std, size, *, out=None) -&gt; Tensor 
<a name="l22036"><span class="ln">22036 </span></a>       :noindex: 
<a name="l22037"><span class="ln">22037 </span></a> 
<a name="l22038"><span class="ln">22038 </span></a>    Similar to the function above, but the means and standard deviations are shared 
<a name="l22039"><span class="ln">22039 </span></a>    among all drawn elements. The resulting tensor has size given by :attr:`size`. 
<a name="l22040"><span class="ln">22040 </span></a> 
<a name="l22041"><span class="ln">22041 </span></a>    Args: 
<a name="l22042"><span class="ln">22042 </span></a>        mean (float): the mean for all distributions 
<a name="l22043"><span class="ln">22043 </span></a>        std (float): the standard deviation for all distributions 
<a name="l22044"><span class="ln">22044 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l22045"><span class="ln">22045 </span></a> 
<a name="l22046"><span class="ln">22046 </span></a>    Keyword args: 
<a name="l22047"><span class="ln">22047 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22048"><span class="ln">22048 </span></a> 
<a name="l22049"><span class="ln">22049 </span></a>    Example:: 
<a name="l22050"><span class="ln">22050 </span></a> 
<a name="l22051"><span class="ln">22051 </span></a>        &gt;&gt;&gt; torch.normal(2, 3, size=(1, 4)) 
<a name="l22052"><span class="ln">22052 </span></a>        tensor([[-1.3987, -1.9544,  3.6048,  0.7909]]) 
<a name="l22053"><span class="ln">22053 </span></a>    &quot;&quot;&quot;</span>
<a name="l22054"><span class="ln">22054 </span></a>
<a name="l22055"><span class="ln">22055 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22056"><span class="ln">22056 </span></a><span class="s2">def </span><span class="s1">normal</span><span class="s3">(</span>
<a name="l22057"><span class="ln">22057 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l22058"><span class="ln">22058 </span></a>    <span class="s1">std</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22059"><span class="ln">22059 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22060"><span class="ln">22060 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22061"><span class="ln">22061 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22062"><span class="ln">22062 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22063"><span class="ln">22063 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22064"><span class="ln">22064 </span></a>    normal(mean, std, *, generator=None, out=None) -&gt; Tensor 
<a name="l22065"><span class="ln">22065 </span></a> 
<a name="l22066"><span class="ln">22066 </span></a>    Returns a tensor of random numbers drawn from separate normal distributions 
<a name="l22067"><span class="ln">22067 </span></a>    whose mean and standard deviation are given. 
<a name="l22068"><span class="ln">22068 </span></a> 
<a name="l22069"><span class="ln">22069 </span></a>    The :attr:`mean` is a tensor with the mean of 
<a name="l22070"><span class="ln">22070 </span></a>    each output element's normal distribution 
<a name="l22071"><span class="ln">22071 </span></a> 
<a name="l22072"><span class="ln">22072 </span></a>    The :attr:`std` is a tensor with the standard deviation of 
<a name="l22073"><span class="ln">22073 </span></a>    each output element's normal distribution 
<a name="l22074"><span class="ln">22074 </span></a> 
<a name="l22075"><span class="ln">22075 </span></a>    The shapes of :attr:`mean` and :attr:`std` don't need to match, but the 
<a name="l22076"><span class="ln">22076 </span></a>    total number of elements in each tensor need to be the same. 
<a name="l22077"><span class="ln">22077 </span></a> 
<a name="l22078"><span class="ln">22078 </span></a>    .. note:: When the shapes do not match, the shape of :attr:`mean` 
<a name="l22079"><span class="ln">22079 </span></a>              is used as the shape for the returned output tensor 
<a name="l22080"><span class="ln">22080 </span></a> 
<a name="l22081"><span class="ln">22081 </span></a>    .. note:: When :attr:`std` is a CUDA tensor, this function synchronizes 
<a name="l22082"><span class="ln">22082 </span></a>              its device with the CPU. 
<a name="l22083"><span class="ln">22083 </span></a> 
<a name="l22084"><span class="ln">22084 </span></a>    Args: 
<a name="l22085"><span class="ln">22085 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l22086"><span class="ln">22086 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l22087"><span class="ln">22087 </span></a> 
<a name="l22088"><span class="ln">22088 </span></a>    Keyword args: 
<a name="l22089"><span class="ln">22089 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l22090"><span class="ln">22090 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22091"><span class="ln">22091 </span></a> 
<a name="l22092"><span class="ln">22092 </span></a>    Example:: 
<a name="l22093"><span class="ln">22093 </span></a> 
<a name="l22094"><span class="ln">22094 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) 
<a name="l22095"><span class="ln">22095 </span></a>        tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134, 
<a name="l22096"><span class="ln">22096 </span></a>                  8.0505,   8.1408,   9.0563,  10.0566]) 
<a name="l22097"><span class="ln">22097 </span></a> 
<a name="l22098"><span class="ln">22098 </span></a>    .. function:: normal(mean=0.0, std, *, out=None) -&gt; Tensor 
<a name="l22099"><span class="ln">22099 </span></a>       :noindex: 
<a name="l22100"><span class="ln">22100 </span></a> 
<a name="l22101"><span class="ln">22101 </span></a>    Similar to the function above, but the means are shared among all drawn 
<a name="l22102"><span class="ln">22102 </span></a>    elements. 
<a name="l22103"><span class="ln">22103 </span></a> 
<a name="l22104"><span class="ln">22104 </span></a>    Args: 
<a name="l22105"><span class="ln">22105 </span></a>        mean (float, optional): the mean for all distributions 
<a name="l22106"><span class="ln">22106 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l22107"><span class="ln">22107 </span></a> 
<a name="l22108"><span class="ln">22108 </span></a>    Keyword args: 
<a name="l22109"><span class="ln">22109 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22110"><span class="ln">22110 </span></a> 
<a name="l22111"><span class="ln">22111 </span></a>    Example:: 
<a name="l22112"><span class="ln">22112 </span></a> 
<a name="l22113"><span class="ln">22113 </span></a>        &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.)) 
<a name="l22114"><span class="ln">22114 </span></a>        tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303]) 
<a name="l22115"><span class="ln">22115 </span></a> 
<a name="l22116"><span class="ln">22116 </span></a>    .. function:: normal(mean, std=1.0, *, out=None) -&gt; Tensor 
<a name="l22117"><span class="ln">22117 </span></a>       :noindex: 
<a name="l22118"><span class="ln">22118 </span></a> 
<a name="l22119"><span class="ln">22119 </span></a>    Similar to the function above, but the standard deviations are shared among 
<a name="l22120"><span class="ln">22120 </span></a>    all drawn elements. 
<a name="l22121"><span class="ln">22121 </span></a> 
<a name="l22122"><span class="ln">22122 </span></a>    Args: 
<a name="l22123"><span class="ln">22123 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l22124"><span class="ln">22124 </span></a>        std (float, optional): the standard deviation for all distributions 
<a name="l22125"><span class="ln">22125 </span></a> 
<a name="l22126"><span class="ln">22126 </span></a>    Keyword args: 
<a name="l22127"><span class="ln">22127 </span></a>        out (Tensor, optional): the output tensor 
<a name="l22128"><span class="ln">22128 </span></a> 
<a name="l22129"><span class="ln">22129 </span></a>    Example:: 
<a name="l22130"><span class="ln">22130 </span></a> 
<a name="l22131"><span class="ln">22131 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.)) 
<a name="l22132"><span class="ln">22132 </span></a>        tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361]) 
<a name="l22133"><span class="ln">22133 </span></a> 
<a name="l22134"><span class="ln">22134 </span></a>    .. function:: normal(mean, std, size, *, out=None) -&gt; Tensor 
<a name="l22135"><span class="ln">22135 </span></a>       :noindex: 
<a name="l22136"><span class="ln">22136 </span></a> 
<a name="l22137"><span class="ln">22137 </span></a>    Similar to the function above, but the means and standard deviations are shared 
<a name="l22138"><span class="ln">22138 </span></a>    among all drawn elements. The resulting tensor has size given by :attr:`size`. 
<a name="l22139"><span class="ln">22139 </span></a> 
<a name="l22140"><span class="ln">22140 </span></a>    Args: 
<a name="l22141"><span class="ln">22141 </span></a>        mean (float): the mean for all distributions 
<a name="l22142"><span class="ln">22142 </span></a>        std (float): the standard deviation for all distributions 
<a name="l22143"><span class="ln">22143 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l22144"><span class="ln">22144 </span></a> 
<a name="l22145"><span class="ln">22145 </span></a>    Keyword args: 
<a name="l22146"><span class="ln">22146 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22147"><span class="ln">22147 </span></a> 
<a name="l22148"><span class="ln">22148 </span></a>    Example:: 
<a name="l22149"><span class="ln">22149 </span></a> 
<a name="l22150"><span class="ln">22150 </span></a>        &gt;&gt;&gt; torch.normal(2, 3, size=(1, 4)) 
<a name="l22151"><span class="ln">22151 </span></a>        tensor([[-1.3987, -1.9544,  3.6048,  0.7909]]) 
<a name="l22152"><span class="ln">22152 </span></a>    &quot;&quot;&quot;</span>
<a name="l22153"><span class="ln">22153 </span></a>
<a name="l22154"><span class="ln">22154 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22155"><span class="ln">22155 </span></a><span class="s2">def </span><span class="s1">normal</span><span class="s3">(</span>
<a name="l22156"><span class="ln">22156 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l22157"><span class="ln">22157 </span></a>    <span class="s1">std</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l22158"><span class="ln">22158 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l22159"><span class="ln">22159 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22160"><span class="ln">22160 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22161"><span class="ln">22161 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22162"><span class="ln">22162 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22163"><span class="ln">22163 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22164"><span class="ln">22164 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22165"><span class="ln">22165 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22166"><span class="ln">22166 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22167"><span class="ln">22167 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22168"><span class="ln">22168 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22169"><span class="ln">22169 </span></a>    normal(mean, std, *, generator=None, out=None) -&gt; Tensor 
<a name="l22170"><span class="ln">22170 </span></a> 
<a name="l22171"><span class="ln">22171 </span></a>    Returns a tensor of random numbers drawn from separate normal distributions 
<a name="l22172"><span class="ln">22172 </span></a>    whose mean and standard deviation are given. 
<a name="l22173"><span class="ln">22173 </span></a> 
<a name="l22174"><span class="ln">22174 </span></a>    The :attr:`mean` is a tensor with the mean of 
<a name="l22175"><span class="ln">22175 </span></a>    each output element's normal distribution 
<a name="l22176"><span class="ln">22176 </span></a> 
<a name="l22177"><span class="ln">22177 </span></a>    The :attr:`std` is a tensor with the standard deviation of 
<a name="l22178"><span class="ln">22178 </span></a>    each output element's normal distribution 
<a name="l22179"><span class="ln">22179 </span></a> 
<a name="l22180"><span class="ln">22180 </span></a>    The shapes of :attr:`mean` and :attr:`std` don't need to match, but the 
<a name="l22181"><span class="ln">22181 </span></a>    total number of elements in each tensor need to be the same. 
<a name="l22182"><span class="ln">22182 </span></a> 
<a name="l22183"><span class="ln">22183 </span></a>    .. note:: When the shapes do not match, the shape of :attr:`mean` 
<a name="l22184"><span class="ln">22184 </span></a>              is used as the shape for the returned output tensor 
<a name="l22185"><span class="ln">22185 </span></a> 
<a name="l22186"><span class="ln">22186 </span></a>    .. note:: When :attr:`std` is a CUDA tensor, this function synchronizes 
<a name="l22187"><span class="ln">22187 </span></a>              its device with the CPU. 
<a name="l22188"><span class="ln">22188 </span></a> 
<a name="l22189"><span class="ln">22189 </span></a>    Args: 
<a name="l22190"><span class="ln">22190 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l22191"><span class="ln">22191 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l22192"><span class="ln">22192 </span></a> 
<a name="l22193"><span class="ln">22193 </span></a>    Keyword args: 
<a name="l22194"><span class="ln">22194 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l22195"><span class="ln">22195 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22196"><span class="ln">22196 </span></a> 
<a name="l22197"><span class="ln">22197 </span></a>    Example:: 
<a name="l22198"><span class="ln">22198 </span></a> 
<a name="l22199"><span class="ln">22199 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) 
<a name="l22200"><span class="ln">22200 </span></a>        tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134, 
<a name="l22201"><span class="ln">22201 </span></a>                  8.0505,   8.1408,   9.0563,  10.0566]) 
<a name="l22202"><span class="ln">22202 </span></a> 
<a name="l22203"><span class="ln">22203 </span></a>    .. function:: normal(mean=0.0, std, *, out=None) -&gt; Tensor 
<a name="l22204"><span class="ln">22204 </span></a>       :noindex: 
<a name="l22205"><span class="ln">22205 </span></a> 
<a name="l22206"><span class="ln">22206 </span></a>    Similar to the function above, but the means are shared among all drawn 
<a name="l22207"><span class="ln">22207 </span></a>    elements. 
<a name="l22208"><span class="ln">22208 </span></a> 
<a name="l22209"><span class="ln">22209 </span></a>    Args: 
<a name="l22210"><span class="ln">22210 </span></a>        mean (float, optional): the mean for all distributions 
<a name="l22211"><span class="ln">22211 </span></a>        std (Tensor): the tensor of per-element standard deviations 
<a name="l22212"><span class="ln">22212 </span></a> 
<a name="l22213"><span class="ln">22213 </span></a>    Keyword args: 
<a name="l22214"><span class="ln">22214 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22215"><span class="ln">22215 </span></a> 
<a name="l22216"><span class="ln">22216 </span></a>    Example:: 
<a name="l22217"><span class="ln">22217 </span></a> 
<a name="l22218"><span class="ln">22218 </span></a>        &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.)) 
<a name="l22219"><span class="ln">22219 </span></a>        tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303]) 
<a name="l22220"><span class="ln">22220 </span></a> 
<a name="l22221"><span class="ln">22221 </span></a>    .. function:: normal(mean, std=1.0, *, out=None) -&gt; Tensor 
<a name="l22222"><span class="ln">22222 </span></a>       :noindex: 
<a name="l22223"><span class="ln">22223 </span></a> 
<a name="l22224"><span class="ln">22224 </span></a>    Similar to the function above, but the standard deviations are shared among 
<a name="l22225"><span class="ln">22225 </span></a>    all drawn elements. 
<a name="l22226"><span class="ln">22226 </span></a> 
<a name="l22227"><span class="ln">22227 </span></a>    Args: 
<a name="l22228"><span class="ln">22228 </span></a>        mean (Tensor): the tensor of per-element means 
<a name="l22229"><span class="ln">22229 </span></a>        std (float, optional): the standard deviation for all distributions 
<a name="l22230"><span class="ln">22230 </span></a> 
<a name="l22231"><span class="ln">22231 </span></a>    Keyword args: 
<a name="l22232"><span class="ln">22232 </span></a>        out (Tensor, optional): the output tensor 
<a name="l22233"><span class="ln">22233 </span></a> 
<a name="l22234"><span class="ln">22234 </span></a>    Example:: 
<a name="l22235"><span class="ln">22235 </span></a> 
<a name="l22236"><span class="ln">22236 </span></a>        &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.)) 
<a name="l22237"><span class="ln">22237 </span></a>        tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361]) 
<a name="l22238"><span class="ln">22238 </span></a> 
<a name="l22239"><span class="ln">22239 </span></a>    .. function:: normal(mean, std, size, *, out=None) -&gt; Tensor 
<a name="l22240"><span class="ln">22240 </span></a>       :noindex: 
<a name="l22241"><span class="ln">22241 </span></a> 
<a name="l22242"><span class="ln">22242 </span></a>    Similar to the function above, but the means and standard deviations are shared 
<a name="l22243"><span class="ln">22243 </span></a>    among all drawn elements. The resulting tensor has size given by :attr:`size`. 
<a name="l22244"><span class="ln">22244 </span></a> 
<a name="l22245"><span class="ln">22245 </span></a>    Args: 
<a name="l22246"><span class="ln">22246 </span></a>        mean (float): the mean for all distributions 
<a name="l22247"><span class="ln">22247 </span></a>        std (float): the standard deviation for all distributions 
<a name="l22248"><span class="ln">22248 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l22249"><span class="ln">22249 </span></a> 
<a name="l22250"><span class="ln">22250 </span></a>    Keyword args: 
<a name="l22251"><span class="ln">22251 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22252"><span class="ln">22252 </span></a> 
<a name="l22253"><span class="ln">22253 </span></a>    Example:: 
<a name="l22254"><span class="ln">22254 </span></a> 
<a name="l22255"><span class="ln">22255 </span></a>        &gt;&gt;&gt; torch.normal(2, 3, size=(1, 4)) 
<a name="l22256"><span class="ln">22256 </span></a>        tensor([[-1.3987, -1.9544,  3.6048,  0.7909]]) 
<a name="l22257"><span class="ln">22257 </span></a>    &quot;&quot;&quot;</span>
<a name="l22258"><span class="ln">22258 </span></a>
<a name="l22259"><span class="ln">22259 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22260"><span class="ln">22260 </span></a><span class="s2">def </span><span class="s1">not_equal</span><span class="s3">(</span>
<a name="l22261"><span class="ln">22261 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22262"><span class="ln">22262 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22263"><span class="ln">22263 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22264"><span class="ln">22264 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22265"><span class="ln">22265 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22266"><span class="ln">22266 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22267"><span class="ln">22267 </span></a>    not_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l22268"><span class="ln">22268 </span></a> 
<a name="l22269"><span class="ln">22269 </span></a>    Alias for :func:`torch.ne`. 
<a name="l22270"><span class="ln">22270 </span></a>    &quot;&quot;&quot;</span>
<a name="l22271"><span class="ln">22271 </span></a>
<a name="l22272"><span class="ln">22272 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22273"><span class="ln">22273 </span></a><span class="s2">def </span><span class="s1">not_equal</span><span class="s3">(</span>
<a name="l22274"><span class="ln">22274 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22275"><span class="ln">22275 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l22276"><span class="ln">22276 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22277"><span class="ln">22277 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22278"><span class="ln">22278 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22279"><span class="ln">22279 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22280"><span class="ln">22280 </span></a>    not_equal(input, other, *, out=None) -&gt; Tensor 
<a name="l22281"><span class="ln">22281 </span></a> 
<a name="l22282"><span class="ln">22282 </span></a>    Alias for :func:`torch.ne`. 
<a name="l22283"><span class="ln">22283 </span></a>    &quot;&quot;&quot;</span>
<a name="l22284"><span class="ln">22284 </span></a>
<a name="l22285"><span class="ln">22285 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22286"><span class="ln">22286 </span></a><span class="s2">def </span><span class="s1">nuclear_norm</span><span class="s3">(</span>
<a name="l22287"><span class="ln">22287 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22288"><span class="ln">22288 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l22289"><span class="ln">22289 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l22290"><span class="ln">22290 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22291"><span class="ln">22291 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22292"><span class="ln">22292 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l22293"><span class="ln">22293 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22294"><span class="ln">22294 </span></a><span class="s2">def </span><span class="s1">nuclear_norm</span><span class="s3">(</span>
<a name="l22295"><span class="ln">22295 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22296"><span class="ln">22296 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l22297"><span class="ln">22297 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22298"><span class="ln">22298 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22299"><span class="ln">22299 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l22300"><span class="ln">22300 </span></a><span class="s2">def </span><span class="s1">numel</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">:</span>
<a name="l22301"><span class="ln">22301 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22302"><span class="ln">22302 </span></a>    numel(input: Tensor) -&gt; int 
<a name="l22303"><span class="ln">22303 </span></a> 
<a name="l22304"><span class="ln">22304 </span></a>    Returns the total number of elements in the :attr:`input` tensor. 
<a name="l22305"><span class="ln">22305 </span></a> 
<a name="l22306"><span class="ln">22306 </span></a>    Args: 
<a name="l22307"><span class="ln">22307 </span></a>        input (Tensor): the input tensor. 
<a name="l22308"><span class="ln">22308 </span></a> 
<a name="l22309"><span class="ln">22309 </span></a>    Example:: 
<a name="l22310"><span class="ln">22310 </span></a> 
<a name="l22311"><span class="ln">22311 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5) 
<a name="l22312"><span class="ln">22312 </span></a>        &gt;&gt;&gt; torch.numel(a) 
<a name="l22313"><span class="ln">22313 </span></a>        120 
<a name="l22314"><span class="ln">22314 </span></a>        &gt;&gt;&gt; a = torch.zeros(4,4) 
<a name="l22315"><span class="ln">22315 </span></a>        &gt;&gt;&gt; torch.numel(a) 
<a name="l22316"><span class="ln">22316 </span></a>        16 
<a name="l22317"><span class="ln">22317 </span></a>    &quot;&quot;&quot;</span>
<a name="l22318"><span class="ln">22318 </span></a>
<a name="l22319"><span class="ln">22319 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22320"><span class="ln">22320 </span></a><span class="s2">def </span><span class="s1">ones</span><span class="s3">(</span>
<a name="l22321"><span class="ln">22321 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l22322"><span class="ln">22322 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22323"><span class="ln">22323 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22324"><span class="ln">22324 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22325"><span class="ln">22325 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22326"><span class="ln">22326 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22327"><span class="ln">22327 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22328"><span class="ln">22328 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22329"><span class="ln">22329 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22330"><span class="ln">22330 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22331"><span class="ln">22331 </span></a>    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l22332"><span class="ln">22332 </span></a> 
<a name="l22333"><span class="ln">22333 </span></a>    Returns a tensor filled with the scalar value `1`, with the shape defined 
<a name="l22334"><span class="ln">22334 </span></a>    by the variable argument :attr:`size`. 
<a name="l22335"><span class="ln">22335 </span></a> 
<a name="l22336"><span class="ln">22336 </span></a>    Args: 
<a name="l22337"><span class="ln">22337 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l22338"><span class="ln">22338 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l22339"><span class="ln">22339 </span></a> 
<a name="l22340"><span class="ln">22340 </span></a>    Keyword arguments: 
<a name="l22341"><span class="ln">22341 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22342"><span class="ln">22342 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l22343"><span class="ln">22343 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l22344"><span class="ln">22344 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l22345"><span class="ln">22345 </span></a>            Default: ``torch.strided``. 
<a name="l22346"><span class="ln">22346 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l22347"><span class="ln">22347 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l22348"><span class="ln">22348 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l22349"><span class="ln">22349 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l22350"><span class="ln">22350 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l22351"><span class="ln">22351 </span></a>            returned tensor. Default: ``False``. 
<a name="l22352"><span class="ln">22352 </span></a> 
<a name="l22353"><span class="ln">22353 </span></a>    Example:: 
<a name="l22354"><span class="ln">22354 </span></a> 
<a name="l22355"><span class="ln">22355 </span></a>        &gt;&gt;&gt; torch.ones(2, 3) 
<a name="l22356"><span class="ln">22356 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l22357"><span class="ln">22357 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l22358"><span class="ln">22358 </span></a> 
<a name="l22359"><span class="ln">22359 </span></a>        &gt;&gt;&gt; torch.ones(5) 
<a name="l22360"><span class="ln">22360 </span></a>        tensor([ 1.,  1.,  1.,  1.,  1.]) 
<a name="l22361"><span class="ln">22361 </span></a>    &quot;&quot;&quot;</span>
<a name="l22362"><span class="ln">22362 </span></a>
<a name="l22363"><span class="ln">22363 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22364"><span class="ln">22364 </span></a><span class="s2">def </span><span class="s1">ones</span><span class="s3">(</span>
<a name="l22365"><span class="ln">22365 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l22366"><span class="ln">22366 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22367"><span class="ln">22367 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22368"><span class="ln">22368 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22369"><span class="ln">22369 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22370"><span class="ln">22370 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22371"><span class="ln">22371 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22372"><span class="ln">22372 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22373"><span class="ln">22373 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22374"><span class="ln">22374 </span></a>    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l22375"><span class="ln">22375 </span></a> 
<a name="l22376"><span class="ln">22376 </span></a>    Returns a tensor filled with the scalar value `1`, with the shape defined 
<a name="l22377"><span class="ln">22377 </span></a>    by the variable argument :attr:`size`. 
<a name="l22378"><span class="ln">22378 </span></a> 
<a name="l22379"><span class="ln">22379 </span></a>    Args: 
<a name="l22380"><span class="ln">22380 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l22381"><span class="ln">22381 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l22382"><span class="ln">22382 </span></a> 
<a name="l22383"><span class="ln">22383 </span></a>    Keyword arguments: 
<a name="l22384"><span class="ln">22384 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22385"><span class="ln">22385 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l22386"><span class="ln">22386 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l22387"><span class="ln">22387 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l22388"><span class="ln">22388 </span></a>            Default: ``torch.strided``. 
<a name="l22389"><span class="ln">22389 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l22390"><span class="ln">22390 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l22391"><span class="ln">22391 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l22392"><span class="ln">22392 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l22393"><span class="ln">22393 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l22394"><span class="ln">22394 </span></a>            returned tensor. Default: ``False``. 
<a name="l22395"><span class="ln">22395 </span></a> 
<a name="l22396"><span class="ln">22396 </span></a>    Example:: 
<a name="l22397"><span class="ln">22397 </span></a> 
<a name="l22398"><span class="ln">22398 </span></a>        &gt;&gt;&gt; torch.ones(2, 3) 
<a name="l22399"><span class="ln">22399 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l22400"><span class="ln">22400 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l22401"><span class="ln">22401 </span></a> 
<a name="l22402"><span class="ln">22402 </span></a>        &gt;&gt;&gt; torch.ones(5) 
<a name="l22403"><span class="ln">22403 </span></a>        tensor([ 1.,  1.,  1.,  1.,  1.]) 
<a name="l22404"><span class="ln">22404 </span></a>    &quot;&quot;&quot;</span>
<a name="l22405"><span class="ln">22405 </span></a>
<a name="l22406"><span class="ln">22406 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22407"><span class="ln">22407 </span></a><span class="s2">def </span><span class="s1">ones</span><span class="s3">(</span>
<a name="l22408"><span class="ln">22408 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l22409"><span class="ln">22409 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22410"><span class="ln">22410 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l22411"><span class="ln">22411 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22412"><span class="ln">22412 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22413"><span class="ln">22413 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22414"><span class="ln">22414 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22415"><span class="ln">22415 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22416"><span class="ln">22416 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22417"><span class="ln">22417 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22418"><span class="ln">22418 </span></a>    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l22419"><span class="ln">22419 </span></a> 
<a name="l22420"><span class="ln">22420 </span></a>    Returns a tensor filled with the scalar value `1`, with the shape defined 
<a name="l22421"><span class="ln">22421 </span></a>    by the variable argument :attr:`size`. 
<a name="l22422"><span class="ln">22422 </span></a> 
<a name="l22423"><span class="ln">22423 </span></a>    Args: 
<a name="l22424"><span class="ln">22424 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l22425"><span class="ln">22425 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l22426"><span class="ln">22426 </span></a> 
<a name="l22427"><span class="ln">22427 </span></a>    Keyword arguments: 
<a name="l22428"><span class="ln">22428 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22429"><span class="ln">22429 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l22430"><span class="ln">22430 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l22431"><span class="ln">22431 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l22432"><span class="ln">22432 </span></a>            Default: ``torch.strided``. 
<a name="l22433"><span class="ln">22433 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l22434"><span class="ln">22434 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l22435"><span class="ln">22435 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l22436"><span class="ln">22436 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l22437"><span class="ln">22437 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l22438"><span class="ln">22438 </span></a>            returned tensor. Default: ``False``. 
<a name="l22439"><span class="ln">22439 </span></a> 
<a name="l22440"><span class="ln">22440 </span></a>    Example:: 
<a name="l22441"><span class="ln">22441 </span></a> 
<a name="l22442"><span class="ln">22442 </span></a>        &gt;&gt;&gt; torch.ones(2, 3) 
<a name="l22443"><span class="ln">22443 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l22444"><span class="ln">22444 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l22445"><span class="ln">22445 </span></a> 
<a name="l22446"><span class="ln">22446 </span></a>        &gt;&gt;&gt; torch.ones(5) 
<a name="l22447"><span class="ln">22447 </span></a>        tensor([ 1.,  1.,  1.,  1.,  1.]) 
<a name="l22448"><span class="ln">22448 </span></a>    &quot;&quot;&quot;</span>
<a name="l22449"><span class="ln">22449 </span></a>
<a name="l22450"><span class="ln">22450 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22451"><span class="ln">22451 </span></a><span class="s2">def </span><span class="s1">ones</span><span class="s3">(</span>
<a name="l22452"><span class="ln">22452 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l22453"><span class="ln">22453 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l22454"><span class="ln">22454 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22455"><span class="ln">22455 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22456"><span class="ln">22456 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22457"><span class="ln">22457 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22458"><span class="ln">22458 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22459"><span class="ln">22459 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22460"><span class="ln">22460 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22461"><span class="ln">22461 </span></a>    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l22462"><span class="ln">22462 </span></a> 
<a name="l22463"><span class="ln">22463 </span></a>    Returns a tensor filled with the scalar value `1`, with the shape defined 
<a name="l22464"><span class="ln">22464 </span></a>    by the variable argument :attr:`size`. 
<a name="l22465"><span class="ln">22465 </span></a> 
<a name="l22466"><span class="ln">22466 </span></a>    Args: 
<a name="l22467"><span class="ln">22467 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l22468"><span class="ln">22468 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l22469"><span class="ln">22469 </span></a> 
<a name="l22470"><span class="ln">22470 </span></a>    Keyword arguments: 
<a name="l22471"><span class="ln">22471 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22472"><span class="ln">22472 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l22473"><span class="ln">22473 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l22474"><span class="ln">22474 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l22475"><span class="ln">22475 </span></a>            Default: ``torch.strided``. 
<a name="l22476"><span class="ln">22476 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l22477"><span class="ln">22477 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l22478"><span class="ln">22478 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l22479"><span class="ln">22479 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l22480"><span class="ln">22480 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l22481"><span class="ln">22481 </span></a>            returned tensor. Default: ``False``. 
<a name="l22482"><span class="ln">22482 </span></a> 
<a name="l22483"><span class="ln">22483 </span></a>    Example:: 
<a name="l22484"><span class="ln">22484 </span></a> 
<a name="l22485"><span class="ln">22485 </span></a>        &gt;&gt;&gt; torch.ones(2, 3) 
<a name="l22486"><span class="ln">22486 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l22487"><span class="ln">22487 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l22488"><span class="ln">22488 </span></a> 
<a name="l22489"><span class="ln">22489 </span></a>        &gt;&gt;&gt; torch.ones(5) 
<a name="l22490"><span class="ln">22490 </span></a>        tensor([ 1.,  1.,  1.,  1.,  1.]) 
<a name="l22491"><span class="ln">22491 </span></a>    &quot;&quot;&quot;</span>
<a name="l22492"><span class="ln">22492 </span></a>
<a name="l22493"><span class="ln">22493 </span></a><span class="s2">def </span><span class="s1">ones_like</span><span class="s3">(</span>
<a name="l22494"><span class="ln">22494 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22495"><span class="ln">22495 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22496"><span class="ln">22496 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22497"><span class="ln">22497 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22498"><span class="ln">22498 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22499"><span class="ln">22499 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22500"><span class="ln">22500 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22501"><span class="ln">22501 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l22502"><span class="ln">22502 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22503"><span class="ln">22503 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22504"><span class="ln">22504 </span></a>    ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l22505"><span class="ln">22505 </span></a> 
<a name="l22506"><span class="ln">22506 </span></a>    Returns a tensor filled with the scalar value `1`, with the same size as 
<a name="l22507"><span class="ln">22507 </span></a>    :attr:`input`. ``torch.ones_like(input)`` is equivalent to 
<a name="l22508"><span class="ln">22508 </span></a>    ``torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l22509"><span class="ln">22509 </span></a> 
<a name="l22510"><span class="ln">22510 </span></a>    .. warning:: 
<a name="l22511"><span class="ln">22511 </span></a>        As of 0.4, this function does not support an :attr:`out` keyword. As an alternative, 
<a name="l22512"><span class="ln">22512 </span></a>        the old ``torch.ones_like(input, out=output)`` is equivalent to 
<a name="l22513"><span class="ln">22513 </span></a>        ``torch.ones(input.size(), out=output)``. 
<a name="l22514"><span class="ln">22514 </span></a> 
<a name="l22515"><span class="ln">22515 </span></a>    Args: 
<a name="l22516"><span class="ln">22516 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l22517"><span class="ln">22517 </span></a> 
<a name="l22518"><span class="ln">22518 </span></a>    Keyword arguments: 
<a name="l22519"><span class="ln">22519 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l22520"><span class="ln">22520 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l22521"><span class="ln">22521 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l22522"><span class="ln">22522 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l22523"><span class="ln">22523 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l22524"><span class="ln">22524 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l22525"><span class="ln">22525 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l22526"><span class="ln">22526 </span></a>            returned tensor. Default: ``False``. 
<a name="l22527"><span class="ln">22527 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l22528"><span class="ln">22528 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l22529"><span class="ln">22529 </span></a> 
<a name="l22530"><span class="ln">22530 </span></a>    Example:: 
<a name="l22531"><span class="ln">22531 </span></a> 
<a name="l22532"><span class="ln">22532 </span></a>        &gt;&gt;&gt; input = torch.empty(2, 3) 
<a name="l22533"><span class="ln">22533 </span></a>        &gt;&gt;&gt; torch.ones_like(input) 
<a name="l22534"><span class="ln">22534 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l22535"><span class="ln">22535 </span></a>                [ 1.,  1.,  1.]]) 
<a name="l22536"><span class="ln">22536 </span></a>    &quot;&quot;&quot;</span>
<a name="l22537"><span class="ln">22537 </span></a>
<a name="l22538"><span class="ln">22538 </span></a><span class="s2">def </span><span class="s1">orgqr</span><span class="s3">(</span>
<a name="l22539"><span class="ln">22539 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22540"><span class="ln">22540 </span></a>    <span class="s1">input2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22541"><span class="ln">22541 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22542"><span class="ln">22542 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22543"><span class="ln">22543 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22544"><span class="ln">22544 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22545"><span class="ln">22545 </span></a>    orgqr(input, tau) -&gt; Tensor 
<a name="l22546"><span class="ln">22546 </span></a> 
<a name="l22547"><span class="ln">22547 </span></a>    Alias for :func:`torch.linalg.householder_product`. 
<a name="l22548"><span class="ln">22548 </span></a>    &quot;&quot;&quot;</span>
<a name="l22549"><span class="ln">22549 </span></a>
<a name="l22550"><span class="ln">22550 </span></a><span class="s2">def </span><span class="s1">ormqr</span><span class="s3">(</span>
<a name="l22551"><span class="ln">22551 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22552"><span class="ln">22552 </span></a>    <span class="s1">input2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22553"><span class="ln">22553 </span></a>    <span class="s1">input3</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22554"><span class="ln">22554 </span></a>    <span class="s1">left</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l22555"><span class="ln">22555 </span></a>    <span class="s1">transpose</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l22556"><span class="ln">22556 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22557"><span class="ln">22557 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22558"><span class="ln">22558 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22559"><span class="ln">22559 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22560"><span class="ln">22560 </span></a>    ormqr(input, tau, other, left=True, transpose=False, *, out=None) -&gt; Tensor 
<a name="l22561"><span class="ln">22561 </span></a> 
<a name="l22562"><span class="ln">22562 </span></a>    Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. 
<a name="l22563"><span class="ln">22563 </span></a> 
<a name="l22564"><span class="ln">22564 </span></a>    Multiplies a :math:`m \times n` matrix `C` (given by :attr:`other`) with a matrix `Q`, 
<a name="l22565"><span class="ln">22565 </span></a>    where `Q` is represented using Householder reflectors `(input, tau)`. 
<a name="l22566"><span class="ln">22566 </span></a>    See `Representation of Orthogonal or Unitary Matrices`_ for further details. 
<a name="l22567"><span class="ln">22567 </span></a> 
<a name="l22568"><span class="ln">22568 </span></a>    If :attr:`left` is `True` then `op(Q)` times `C` is computed, otherwise the result is `C` times `op(Q)`. 
<a name="l22569"><span class="ln">22569 </span></a>    When :attr:`left` is `True`, the implicit matrix `Q` has size :math:`m \times m`. 
<a name="l22570"><span class="ln">22570 </span></a>    It has size :math:`n \times n` otherwise. 
<a name="l22571"><span class="ln">22571 </span></a>    If :attr:`transpose` is `True` then `op` is the conjugate transpose operation, otherwise it's a no-op. 
<a name="l22572"><span class="ln">22572 </span></a> 
<a name="l22573"><span class="ln">22573 </span></a>    Supports inputs of float, double, cfloat and cdouble dtypes. 
<a name="l22574"><span class="ln">22574 </span></a>    Also supports batched inputs, and, if the input is batched, the output is batched with the same dimensions. 
<a name="l22575"><span class="ln">22575 </span></a> 
<a name="l22576"><span class="ln">22576 </span></a>    .. seealso:: 
<a name="l22577"><span class="ln">22577 </span></a>            :func:`torch.geqrf` can be used to form the Householder representation `(input, tau)` of matrix `Q` 
<a name="l22578"><span class="ln">22578 </span></a>            from the QR decomposition. 
<a name="l22579"><span class="ln">22579 </span></a> 
<a name="l22580"><span class="ln">22580 </span></a>    .. note:: 
<a name="l22581"><span class="ln">22581 </span></a>            This function supports backward but it is only fast when ``(input, tau)`` do not require gradients 
<a name="l22582"><span class="ln">22582 </span></a>            and/or ``tau.size(-1)`` is very small. 
<a name="l22583"><span class="ln">22583 </span></a>            `` 
<a name="l22584"><span class="ln">22584 </span></a> 
<a name="l22585"><span class="ln">22585 </span></a>    Args: 
<a name="l22586"><span class="ln">22586 </span></a>        input (Tensor): tensor of shape `(*, mn, k)` where `*` is zero or more batch dimensions 
<a name="l22587"><span class="ln">22587 </span></a>                        and `mn` equals to `m` or `n` depending on the :attr:`left`. 
<a name="l22588"><span class="ln">22588 </span></a>        tau (Tensor): tensor of shape `(*, min(mn, k))` where `*` is zero or more batch dimensions. 
<a name="l22589"><span class="ln">22589 </span></a>        other (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions. 
<a name="l22590"><span class="ln">22590 </span></a>        left (bool): controls the order of multiplication. 
<a name="l22591"><span class="ln">22591 </span></a>        transpose (bool): controls whether the matrix `Q` is conjugate transposed or not. 
<a name="l22592"><span class="ln">22592 </span></a> 
<a name="l22593"><span class="ln">22593 </span></a>    Keyword args: 
<a name="l22594"><span class="ln">22594 </span></a>        out (Tensor, optional): the output Tensor. Ignored if `None`. Default: `None`. 
<a name="l22595"><span class="ln">22595 </span></a> 
<a name="l22596"><span class="ln">22596 </span></a>    .. _Representation of Orthogonal or Unitary Matrices: 
<a name="l22597"><span class="ln">22597 </span></a>        https://www.netlib.org/lapack/lug/node128.html 
<a name="l22598"><span class="ln">22598 </span></a>    &quot;&quot;&quot;</span>
<a name="l22599"><span class="ln">22599 </span></a>
<a name="l22600"><span class="ln">22600 </span></a><span class="s2">def </span><span class="s1">outer</span><span class="s3">(</span>
<a name="l22601"><span class="ln">22601 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22602"><span class="ln">22602 </span></a>    <span class="s1">vec2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22603"><span class="ln">22603 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22604"><span class="ln">22604 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22605"><span class="ln">22605 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22606"><span class="ln">22606 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22607"><span class="ln">22607 </span></a>    outer(input, vec2, *, out=None) -&gt; Tensor 
<a name="l22608"><span class="ln">22608 </span></a> 
<a name="l22609"><span class="ln">22609 </span></a>    Outer product of :attr:`input` and :attr:`vec2`. 
<a name="l22610"><span class="ln">22610 </span></a>    If :attr:`input` is a vector of size :math:`n` and :attr:`vec2` is a vector of 
<a name="l22611"><span class="ln">22611 </span></a>    size :math:`m`, then :attr:`out` must be a matrix of size :math:`(n \times m)`. 
<a name="l22612"><span class="ln">22612 </span></a> 
<a name="l22613"><span class="ln">22613 </span></a>    .. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics&gt;`. 
<a name="l22614"><span class="ln">22614 </span></a> 
<a name="l22615"><span class="ln">22615 </span></a>    Args: 
<a name="l22616"><span class="ln">22616 </span></a>        input (Tensor): 1-D input vector 
<a name="l22617"><span class="ln">22617 </span></a>        vec2 (Tensor): 1-D input vector 
<a name="l22618"><span class="ln">22618 </span></a> 
<a name="l22619"><span class="ln">22619 </span></a>    Keyword args: 
<a name="l22620"><span class="ln">22620 </span></a>        out (Tensor, optional): optional output matrix 
<a name="l22621"><span class="ln">22621 </span></a> 
<a name="l22622"><span class="ln">22622 </span></a>    Example:: 
<a name="l22623"><span class="ln">22623 </span></a> 
<a name="l22624"><span class="ln">22624 </span></a>        &gt;&gt;&gt; v1 = torch.arange(1., 5.) 
<a name="l22625"><span class="ln">22625 </span></a>        &gt;&gt;&gt; v2 = torch.arange(1., 4.) 
<a name="l22626"><span class="ln">22626 </span></a>        &gt;&gt;&gt; torch.outer(v1, v2) 
<a name="l22627"><span class="ln">22627 </span></a>        tensor([[  1.,   2.,   3.], 
<a name="l22628"><span class="ln">22628 </span></a>                [  2.,   4.,   6.], 
<a name="l22629"><span class="ln">22629 </span></a>                [  3.,   6.,   9.], 
<a name="l22630"><span class="ln">22630 </span></a>                [  4.,   8.,  12.]]) 
<a name="l22631"><span class="ln">22631 </span></a>    &quot;&quot;&quot;</span>
<a name="l22632"><span class="ln">22632 </span></a>
<a name="l22633"><span class="ln">22633 </span></a><span class="s2">def </span><span class="s1">pairwise_distance</span><span class="s3">(</span>
<a name="l22634"><span class="ln">22634 </span></a>    <span class="s1">x1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22635"><span class="ln">22635 </span></a>    <span class="s1">x2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22636"><span class="ln">22636 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">2</span><span class="s3">,</span>
<a name="l22637"><span class="ln">22637 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-06</span><span class="s3">,</span>
<a name="l22638"><span class="ln">22638 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l22639"><span class="ln">22639 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l22640"><span class="ln">22640 </span></a><span class="s2">def </span><span class="s1">pdist</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">2</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l22641"><span class="ln">22641 </span></a><span class="s2">def </span><span class="s1">permute</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dims</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22642"><span class="ln">22642 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22643"><span class="ln">22643 </span></a>    permute(input, dims) -&gt; Tensor 
<a name="l22644"><span class="ln">22644 </span></a> 
<a name="l22645"><span class="ln">22645 </span></a>    Returns a view of the original tensor :attr:`input` with its dimensions permuted. 
<a name="l22646"><span class="ln">22646 </span></a> 
<a name="l22647"><span class="ln">22647 </span></a>    Args: 
<a name="l22648"><span class="ln">22648 </span></a>        input (Tensor): the input tensor. 
<a name="l22649"><span class="ln">22649 </span></a>        dims (tuple of int): The desired ordering of dimensions 
<a name="l22650"><span class="ln">22650 </span></a> 
<a name="l22651"><span class="ln">22651 </span></a>    Example: 
<a name="l22652"><span class="ln">22652 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 3, 5) 
<a name="l22653"><span class="ln">22653 </span></a>        &gt;&gt;&gt; x.size() 
<a name="l22654"><span class="ln">22654 </span></a>        torch.Size([2, 3, 5]) 
<a name="l22655"><span class="ln">22655 </span></a>        &gt;&gt;&gt; torch.permute(x, (2, 0, 1)).size() 
<a name="l22656"><span class="ln">22656 </span></a>        torch.Size([5, 2, 3]) 
<a name="l22657"><span class="ln">22657 </span></a>    &quot;&quot;&quot;</span>
<a name="l22658"><span class="ln">22658 </span></a>
<a name="l22659"><span class="ln">22659 </span></a><span class="s2">def </span><span class="s1">permute_copy</span><span class="s3">(</span>
<a name="l22660"><span class="ln">22660 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22661"><span class="ln">22661 </span></a>    <span class="s1">dims</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l22662"><span class="ln">22662 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22663"><span class="ln">22663 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22664"><span class="ln">22664 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22665"><span class="ln">22665 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22666"><span class="ln">22666 </span></a>    Performs the same operation as :func:`torch.permute`, but all output tensors 
<a name="l22667"><span class="ln">22667 </span></a>    are freshly created instead of aliasing the input. 
<a name="l22668"><span class="ln">22668 </span></a>    &quot;&quot;&quot;</span>
<a name="l22669"><span class="ln">22669 </span></a>
<a name="l22670"><span class="ln">22670 </span></a><span class="s2">def </span><span class="s1">pinverse</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">rcond</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-15</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22671"><span class="ln">22671 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22672"><span class="ln">22672 </span></a>    pinverse(input, rcond=1e-15) -&gt; Tensor 
<a name="l22673"><span class="ln">22673 </span></a> 
<a name="l22674"><span class="ln">22674 </span></a>    Alias for :func:`torch.linalg.pinv` 
<a name="l22675"><span class="ln">22675 </span></a>    &quot;&quot;&quot;</span>
<a name="l22676"><span class="ln">22676 </span></a>
<a name="l22677"><span class="ln">22677 </span></a><span class="s2">def </span><span class="s1">pixel_shuffle</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">upscale_factor</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l22678"><span class="ln">22678 </span></a><span class="s2">def </span><span class="s1">pixel_unshuffle</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">downscale_factor</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l22679"><span class="ln">22679 </span></a><span class="s2">def </span><span class="s1">poisson</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22680"><span class="ln">22680 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22681"><span class="ln">22681 </span></a>    poisson(input, generator=None) -&gt; Tensor 
<a name="l22682"><span class="ln">22682 </span></a> 
<a name="l22683"><span class="ln">22683 </span></a>    Returns a tensor of the same size as :attr:`input` with each element 
<a name="l22684"><span class="ln">22684 </span></a>    sampled from a Poisson distribution with rate parameter given by the corresponding 
<a name="l22685"><span class="ln">22685 </span></a>    element in :attr:`input` i.e., 
<a name="l22686"><span class="ln">22686 </span></a> 
<a name="l22687"><span class="ln">22687 </span></a>    .. math:: 
<a name="l22688"><span class="ln">22688 </span></a>        \text{out}_i \sim \text{Poisson}(\text{input}_i) 
<a name="l22689"><span class="ln">22689 </span></a> 
<a name="l22690"><span class="ln">22690 </span></a>    :attr:`input` must be non-negative. 
<a name="l22691"><span class="ln">22691 </span></a> 
<a name="l22692"><span class="ln">22692 </span></a>    Args: 
<a name="l22693"><span class="ln">22693 </span></a>        input (Tensor): the input tensor containing the rates of the Poisson distribution 
<a name="l22694"><span class="ln">22694 </span></a> 
<a name="l22695"><span class="ln">22695 </span></a>    Keyword args: 
<a name="l22696"><span class="ln">22696 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l22697"><span class="ln">22697 </span></a> 
<a name="l22698"><span class="ln">22698 </span></a>    Example:: 
<a name="l22699"><span class="ln">22699 </span></a> 
<a name="l22700"><span class="ln">22700 </span></a>        &gt;&gt;&gt; rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5 
<a name="l22701"><span class="ln">22701 </span></a>        &gt;&gt;&gt; torch.poisson(rates) 
<a name="l22702"><span class="ln">22702 </span></a>        tensor([[9., 1., 3., 5.], 
<a name="l22703"><span class="ln">22703 </span></a>                [8., 6., 6., 0.], 
<a name="l22704"><span class="ln">22704 </span></a>                [0., 4., 5., 3.], 
<a name="l22705"><span class="ln">22705 </span></a>                [2., 1., 4., 2.]]) 
<a name="l22706"><span class="ln">22706 </span></a>    &quot;&quot;&quot;</span>
<a name="l22707"><span class="ln">22707 </span></a>
<a name="l22708"><span class="ln">22708 </span></a><span class="s2">def </span><span class="s1">poisson_nll_loss</span><span class="s3">(</span>
<a name="l22709"><span class="ln">22709 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22710"><span class="ln">22710 </span></a>    <span class="s1">target</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22711"><span class="ln">22711 </span></a>    <span class="s1">log_input</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l22712"><span class="ln">22712 </span></a>    <span class="s1">full</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l22713"><span class="ln">22713 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l22714"><span class="ln">22714 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l22715"><span class="ln">22715 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l22716"><span class="ln">22716 </span></a><span class="s2">def </span><span class="s1">polar</span><span class="s3">(</span>
<a name="l22717"><span class="ln">22717 </span></a>    <span class="s1">abs</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22718"><span class="ln">22718 </span></a>    <span class="s1">angle</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22719"><span class="ln">22719 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22720"><span class="ln">22720 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22721"><span class="ln">22721 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22722"><span class="ln">22722 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22723"><span class="ln">22723 </span></a>    polar(abs, angle, *, out=None) -&gt; Tensor 
<a name="l22724"><span class="ln">22724 </span></a> 
<a name="l22725"><span class="ln">22725 </span></a>    Constructs a complex tensor whose elements are Cartesian coordinates 
<a name="l22726"><span class="ln">22726 </span></a>    corresponding to the polar coordinates with absolute value :attr:`abs` and angle 
<a name="l22727"><span class="ln">22727 </span></a>    :attr:`angle`. 
<a name="l22728"><span class="ln">22728 </span></a> 
<a name="l22729"><span class="ln">22729 </span></a>    .. math:: 
<a name="l22730"><span class="ln">22730 </span></a>        \text{out} = \text{abs} \cdot \cos(\text{angle}) + \text{abs} \cdot \sin(\text{angle}) \cdot j 
<a name="l22731"><span class="ln">22731 </span></a> 
<a name="l22732"><span class="ln">22732 </span></a>    .. note:: 
<a name="l22733"><span class="ln">22733 </span></a>        `torch.polar` is similar to 
<a name="l22734"><span class="ln">22734 </span></a>        `std::polar &lt;https://en.cppreference.com/w/cpp/numeric/complex/polar&gt;`_ 
<a name="l22735"><span class="ln">22735 </span></a>        and does not compute the polar decomposition 
<a name="l22736"><span class="ln">22736 </span></a>        of a complex tensor like Python's `cmath.polar` and SciPy's `linalg.polar` do. 
<a name="l22737"><span class="ln">22737 </span></a>        The behavior of this function is undefined if `abs` is negative or NaN, or if `angle` is 
<a name="l22738"><span class="ln">22738 </span></a>        infinite. 
<a name="l22739"><span class="ln">22739 </span></a> 
<a name="l22740"><span class="ln">22740 </span></a> 
<a name="l22741"><span class="ln">22741 </span></a>    Args: 
<a name="l22742"><span class="ln">22742 </span></a>        abs (Tensor): The absolute value the complex tensor. Must be float or double. 
<a name="l22743"><span class="ln">22743 </span></a>        angle (Tensor): The angle of the complex tensor. Must be same dtype as 
<a name="l22744"><span class="ln">22744 </span></a>            :attr:`abs`. 
<a name="l22745"><span class="ln">22745 </span></a> 
<a name="l22746"><span class="ln">22746 </span></a>    Keyword args: 
<a name="l22747"><span class="ln">22747 </span></a>        out (Tensor): If the inputs are ``torch.float32``, must be 
<a name="l22748"><span class="ln">22748 </span></a>            ``torch.complex64``. If the inputs are ``torch.float64``, must be 
<a name="l22749"><span class="ln">22749 </span></a>            ``torch.complex128``. 
<a name="l22750"><span class="ln">22750 </span></a> 
<a name="l22751"><span class="ln">22751 </span></a>    Example:: 
<a name="l22752"><span class="ln">22752 </span></a> 
<a name="l22753"><span class="ln">22753 </span></a>        &gt;&gt;&gt; import numpy as np 
<a name="l22754"><span class="ln">22754 </span></a>        &gt;&gt;&gt; abs = torch.tensor([1, 2], dtype=torch.float64) 
<a name="l22755"><span class="ln">22755 </span></a>        &gt;&gt;&gt; angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64) 
<a name="l22756"><span class="ln">22756 </span></a>        &gt;&gt;&gt; z = torch.polar(abs, angle) 
<a name="l22757"><span class="ln">22757 </span></a>        &gt;&gt;&gt; z 
<a name="l22758"><span class="ln">22758 </span></a>        tensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128) 
<a name="l22759"><span class="ln">22759 </span></a>    &quot;&quot;&quot;</span>
<a name="l22760"><span class="ln">22760 </span></a>
<a name="l22761"><span class="ln">22761 </span></a><span class="s2">def </span><span class="s1">polygamma</span><span class="s3">(</span>
<a name="l22762"><span class="ln">22762 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l22763"><span class="ln">22763 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22764"><span class="ln">22764 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22765"><span class="ln">22765 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22766"><span class="ln">22766 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22767"><span class="ln">22767 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22768"><span class="ln">22768 </span></a>    polygamma(n, input, *, out=None) -&gt; Tensor 
<a name="l22769"><span class="ln">22769 </span></a> 
<a name="l22770"><span class="ln">22770 </span></a>    Alias for :func:`torch.special.polygamma`. 
<a name="l22771"><span class="ln">22771 </span></a>    &quot;&quot;&quot;</span>
<a name="l22772"><span class="ln">22772 </span></a>
<a name="l22773"><span class="ln">22773 </span></a><span class="s2">def </span><span class="s1">positive</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22774"><span class="ln">22774 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22775"><span class="ln">22775 </span></a>    positive(input) -&gt; Tensor 
<a name="l22776"><span class="ln">22776 </span></a> 
<a name="l22777"><span class="ln">22777 </span></a>    Returns :attr:`input`. 
<a name="l22778"><span class="ln">22778 </span></a>    Throws a runtime error if :attr:`input` is a bool tensor. 
<a name="l22779"><span class="ln">22779 </span></a> 
<a name="l22780"><span class="ln">22780 </span></a>    Args: 
<a name="l22781"><span class="ln">22781 </span></a>        input (Tensor): the input tensor. 
<a name="l22782"><span class="ln">22782 </span></a> 
<a name="l22783"><span class="ln">22783 </span></a>    Example:: 
<a name="l22784"><span class="ln">22784 </span></a> 
<a name="l22785"><span class="ln">22785 </span></a>        &gt;&gt;&gt; t = torch.randn(5) 
<a name="l22786"><span class="ln">22786 </span></a>        &gt;&gt;&gt; t 
<a name="l22787"><span class="ln">22787 </span></a>        tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940]) 
<a name="l22788"><span class="ln">22788 </span></a>        &gt;&gt;&gt; torch.positive(t) 
<a name="l22789"><span class="ln">22789 </span></a>        tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940]) 
<a name="l22790"><span class="ln">22790 </span></a>    &quot;&quot;&quot;</span>
<a name="l22791"><span class="ln">22791 </span></a>
<a name="l22792"><span class="ln">22792 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22793"><span class="ln">22793 </span></a><span class="s2">def </span><span class="s1">pow</span><span class="s3">(</span>
<a name="l22794"><span class="ln">22794 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22795"><span class="ln">22795 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22796"><span class="ln">22796 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22797"><span class="ln">22797 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22798"><span class="ln">22798 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22799"><span class="ln">22799 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22800"><span class="ln">22800 </span></a>    pow(input, exponent, *, out=None) -&gt; Tensor 
<a name="l22801"><span class="ln">22801 </span></a> 
<a name="l22802"><span class="ln">22802 </span></a>    Takes the power of each element in :attr:`input` with :attr:`exponent` and 
<a name="l22803"><span class="ln">22803 </span></a>    returns a tensor with the result. 
<a name="l22804"><span class="ln">22804 </span></a> 
<a name="l22805"><span class="ln">22805 </span></a>    :attr:`exponent` can be either a single ``float`` number or a `Tensor` 
<a name="l22806"><span class="ln">22806 </span></a>    with the same number of elements as :attr:`input`. 
<a name="l22807"><span class="ln">22807 </span></a> 
<a name="l22808"><span class="ln">22808 </span></a>    When :attr:`exponent` is a scalar value, the operation applied is: 
<a name="l22809"><span class="ln">22809 </span></a> 
<a name="l22810"><span class="ln">22810 </span></a>    .. math:: 
<a name="l22811"><span class="ln">22811 </span></a>        \text{out}_i = x_i ^ \text{exponent} 
<a name="l22812"><span class="ln">22812 </span></a> 
<a name="l22813"><span class="ln">22813 </span></a>    When :attr:`exponent` is a tensor, the operation applied is: 
<a name="l22814"><span class="ln">22814 </span></a> 
<a name="l22815"><span class="ln">22815 </span></a>    .. math:: 
<a name="l22816"><span class="ln">22816 </span></a>        \text{out}_i = x_i ^ {\text{exponent}_i} 
<a name="l22817"><span class="ln">22817 </span></a> 
<a name="l22818"><span class="ln">22818 </span></a>    When :attr:`exponent` is a tensor, the shapes of :attr:`input` 
<a name="l22819"><span class="ln">22819 </span></a>    and :attr:`exponent` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l22820"><span class="ln">22820 </span></a> 
<a name="l22821"><span class="ln">22821 </span></a>    Args: 
<a name="l22822"><span class="ln">22822 </span></a>        input (Tensor): the input tensor. 
<a name="l22823"><span class="ln">22823 </span></a>        exponent (float or tensor): the exponent value 
<a name="l22824"><span class="ln">22824 </span></a> 
<a name="l22825"><span class="ln">22825 </span></a>    Keyword args: 
<a name="l22826"><span class="ln">22826 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22827"><span class="ln">22827 </span></a> 
<a name="l22828"><span class="ln">22828 </span></a>    Example:: 
<a name="l22829"><span class="ln">22829 </span></a> 
<a name="l22830"><span class="ln">22830 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l22831"><span class="ln">22831 </span></a>        &gt;&gt;&gt; a 
<a name="l22832"><span class="ln">22832 </span></a>        tensor([ 0.4331,  1.2475,  0.6834, -0.2791]) 
<a name="l22833"><span class="ln">22833 </span></a>        &gt;&gt;&gt; torch.pow(a, 2) 
<a name="l22834"><span class="ln">22834 </span></a>        tensor([ 0.1875,  1.5561,  0.4670,  0.0779]) 
<a name="l22835"><span class="ln">22835 </span></a>        &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l22836"><span class="ln">22836 </span></a> 
<a name="l22837"><span class="ln">22837 </span></a>        &gt;&gt;&gt; a = torch.arange(1., 5.) 
<a name="l22838"><span class="ln">22838 </span></a>        &gt;&gt;&gt; a 
<a name="l22839"><span class="ln">22839 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l22840"><span class="ln">22840 </span></a>        &gt;&gt;&gt; exp 
<a name="l22841"><span class="ln">22841 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l22842"><span class="ln">22842 </span></a>        &gt;&gt;&gt; torch.pow(a, exp) 
<a name="l22843"><span class="ln">22843 </span></a>        tensor([   1.,    4.,   27.,  256.]) 
<a name="l22844"><span class="ln">22844 </span></a> 
<a name="l22845"><span class="ln">22845 </span></a>    .. function:: pow(self, exponent, *, out=None) -&gt; Tensor 
<a name="l22846"><span class="ln">22846 </span></a>       :noindex: 
<a name="l22847"><span class="ln">22847 </span></a> 
<a name="l22848"><span class="ln">22848 </span></a>    :attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor. 
<a name="l22849"><span class="ln">22849 </span></a>    The returned tensor :attr:`out` is of the same shape as :attr:`exponent` 
<a name="l22850"><span class="ln">22850 </span></a> 
<a name="l22851"><span class="ln">22851 </span></a>    The operation applied is: 
<a name="l22852"><span class="ln">22852 </span></a> 
<a name="l22853"><span class="ln">22853 </span></a>    .. math:: 
<a name="l22854"><span class="ln">22854 </span></a>        \text{out}_i = \text{self} ^ {\text{exponent}_i} 
<a name="l22855"><span class="ln">22855 </span></a> 
<a name="l22856"><span class="ln">22856 </span></a>    Args: 
<a name="l22857"><span class="ln">22857 </span></a>        self (float): the scalar base value for the power operation 
<a name="l22858"><span class="ln">22858 </span></a>        exponent (Tensor): the exponent tensor 
<a name="l22859"><span class="ln">22859 </span></a> 
<a name="l22860"><span class="ln">22860 </span></a>    Keyword args: 
<a name="l22861"><span class="ln">22861 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22862"><span class="ln">22862 </span></a> 
<a name="l22863"><span class="ln">22863 </span></a>    Example:: 
<a name="l22864"><span class="ln">22864 </span></a> 
<a name="l22865"><span class="ln">22865 </span></a>        &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l22866"><span class="ln">22866 </span></a>        &gt;&gt;&gt; base = 2 
<a name="l22867"><span class="ln">22867 </span></a>        &gt;&gt;&gt; torch.pow(base, exp) 
<a name="l22868"><span class="ln">22868 </span></a>        tensor([  2.,   4.,   8.,  16.]) 
<a name="l22869"><span class="ln">22869 </span></a>    &quot;&quot;&quot;</span>
<a name="l22870"><span class="ln">22870 </span></a>
<a name="l22871"><span class="ln">22871 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22872"><span class="ln">22872 </span></a><span class="s2">def </span><span class="s1">pow</span><span class="s3">(</span>
<a name="l22873"><span class="ln">22873 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l22874"><span class="ln">22874 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22875"><span class="ln">22875 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22876"><span class="ln">22876 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22877"><span class="ln">22877 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22878"><span class="ln">22878 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22879"><span class="ln">22879 </span></a>    pow(input, exponent, *, out=None) -&gt; Tensor 
<a name="l22880"><span class="ln">22880 </span></a> 
<a name="l22881"><span class="ln">22881 </span></a>    Takes the power of each element in :attr:`input` with :attr:`exponent` and 
<a name="l22882"><span class="ln">22882 </span></a>    returns a tensor with the result. 
<a name="l22883"><span class="ln">22883 </span></a> 
<a name="l22884"><span class="ln">22884 </span></a>    :attr:`exponent` can be either a single ``float`` number or a `Tensor` 
<a name="l22885"><span class="ln">22885 </span></a>    with the same number of elements as :attr:`input`. 
<a name="l22886"><span class="ln">22886 </span></a> 
<a name="l22887"><span class="ln">22887 </span></a>    When :attr:`exponent` is a scalar value, the operation applied is: 
<a name="l22888"><span class="ln">22888 </span></a> 
<a name="l22889"><span class="ln">22889 </span></a>    .. math:: 
<a name="l22890"><span class="ln">22890 </span></a>        \text{out}_i = x_i ^ \text{exponent} 
<a name="l22891"><span class="ln">22891 </span></a> 
<a name="l22892"><span class="ln">22892 </span></a>    When :attr:`exponent` is a tensor, the operation applied is: 
<a name="l22893"><span class="ln">22893 </span></a> 
<a name="l22894"><span class="ln">22894 </span></a>    .. math:: 
<a name="l22895"><span class="ln">22895 </span></a>        \text{out}_i = x_i ^ {\text{exponent}_i} 
<a name="l22896"><span class="ln">22896 </span></a> 
<a name="l22897"><span class="ln">22897 </span></a>    When :attr:`exponent` is a tensor, the shapes of :attr:`input` 
<a name="l22898"><span class="ln">22898 </span></a>    and :attr:`exponent` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l22899"><span class="ln">22899 </span></a> 
<a name="l22900"><span class="ln">22900 </span></a>    Args: 
<a name="l22901"><span class="ln">22901 </span></a>        input (Tensor): the input tensor. 
<a name="l22902"><span class="ln">22902 </span></a>        exponent (float or tensor): the exponent value 
<a name="l22903"><span class="ln">22903 </span></a> 
<a name="l22904"><span class="ln">22904 </span></a>    Keyword args: 
<a name="l22905"><span class="ln">22905 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22906"><span class="ln">22906 </span></a> 
<a name="l22907"><span class="ln">22907 </span></a>    Example:: 
<a name="l22908"><span class="ln">22908 </span></a> 
<a name="l22909"><span class="ln">22909 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l22910"><span class="ln">22910 </span></a>        &gt;&gt;&gt; a 
<a name="l22911"><span class="ln">22911 </span></a>        tensor([ 0.4331,  1.2475,  0.6834, -0.2791]) 
<a name="l22912"><span class="ln">22912 </span></a>        &gt;&gt;&gt; torch.pow(a, 2) 
<a name="l22913"><span class="ln">22913 </span></a>        tensor([ 0.1875,  1.5561,  0.4670,  0.0779]) 
<a name="l22914"><span class="ln">22914 </span></a>        &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l22915"><span class="ln">22915 </span></a> 
<a name="l22916"><span class="ln">22916 </span></a>        &gt;&gt;&gt; a = torch.arange(1., 5.) 
<a name="l22917"><span class="ln">22917 </span></a>        &gt;&gt;&gt; a 
<a name="l22918"><span class="ln">22918 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l22919"><span class="ln">22919 </span></a>        &gt;&gt;&gt; exp 
<a name="l22920"><span class="ln">22920 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l22921"><span class="ln">22921 </span></a>        &gt;&gt;&gt; torch.pow(a, exp) 
<a name="l22922"><span class="ln">22922 </span></a>        tensor([   1.,    4.,   27.,  256.]) 
<a name="l22923"><span class="ln">22923 </span></a> 
<a name="l22924"><span class="ln">22924 </span></a>    .. function:: pow(self, exponent, *, out=None) -&gt; Tensor 
<a name="l22925"><span class="ln">22925 </span></a>       :noindex: 
<a name="l22926"><span class="ln">22926 </span></a> 
<a name="l22927"><span class="ln">22927 </span></a>    :attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor. 
<a name="l22928"><span class="ln">22928 </span></a>    The returned tensor :attr:`out` is of the same shape as :attr:`exponent` 
<a name="l22929"><span class="ln">22929 </span></a> 
<a name="l22930"><span class="ln">22930 </span></a>    The operation applied is: 
<a name="l22931"><span class="ln">22931 </span></a> 
<a name="l22932"><span class="ln">22932 </span></a>    .. math:: 
<a name="l22933"><span class="ln">22933 </span></a>        \text{out}_i = \text{self} ^ {\text{exponent}_i} 
<a name="l22934"><span class="ln">22934 </span></a> 
<a name="l22935"><span class="ln">22935 </span></a>    Args: 
<a name="l22936"><span class="ln">22936 </span></a>        self (float): the scalar base value for the power operation 
<a name="l22937"><span class="ln">22937 </span></a>        exponent (Tensor): the exponent tensor 
<a name="l22938"><span class="ln">22938 </span></a> 
<a name="l22939"><span class="ln">22939 </span></a>    Keyword args: 
<a name="l22940"><span class="ln">22940 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22941"><span class="ln">22941 </span></a> 
<a name="l22942"><span class="ln">22942 </span></a>    Example:: 
<a name="l22943"><span class="ln">22943 </span></a> 
<a name="l22944"><span class="ln">22944 </span></a>        &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l22945"><span class="ln">22945 </span></a>        &gt;&gt;&gt; base = 2 
<a name="l22946"><span class="ln">22946 </span></a>        &gt;&gt;&gt; torch.pow(base, exp) 
<a name="l22947"><span class="ln">22947 </span></a>        tensor([  2.,   4.,   8.,  16.]) 
<a name="l22948"><span class="ln">22948 </span></a>    &quot;&quot;&quot;</span>
<a name="l22949"><span class="ln">22949 </span></a>
<a name="l22950"><span class="ln">22950 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l22951"><span class="ln">22951 </span></a><span class="s2">def </span><span class="s1">pow</span><span class="s3">(</span>
<a name="l22952"><span class="ln">22952 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l22953"><span class="ln">22953 </span></a>    <span class="s1">exponent</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l22954"><span class="ln">22954 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l22955"><span class="ln">22955 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l22956"><span class="ln">22956 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l22957"><span class="ln">22957 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l22958"><span class="ln">22958 </span></a>    pow(input, exponent, *, out=None) -&gt; Tensor 
<a name="l22959"><span class="ln">22959 </span></a> 
<a name="l22960"><span class="ln">22960 </span></a>    Takes the power of each element in :attr:`input` with :attr:`exponent` and 
<a name="l22961"><span class="ln">22961 </span></a>    returns a tensor with the result. 
<a name="l22962"><span class="ln">22962 </span></a> 
<a name="l22963"><span class="ln">22963 </span></a>    :attr:`exponent` can be either a single ``float`` number or a `Tensor` 
<a name="l22964"><span class="ln">22964 </span></a>    with the same number of elements as :attr:`input`. 
<a name="l22965"><span class="ln">22965 </span></a> 
<a name="l22966"><span class="ln">22966 </span></a>    When :attr:`exponent` is a scalar value, the operation applied is: 
<a name="l22967"><span class="ln">22967 </span></a> 
<a name="l22968"><span class="ln">22968 </span></a>    .. math:: 
<a name="l22969"><span class="ln">22969 </span></a>        \text{out}_i = x_i ^ \text{exponent} 
<a name="l22970"><span class="ln">22970 </span></a> 
<a name="l22971"><span class="ln">22971 </span></a>    When :attr:`exponent` is a tensor, the operation applied is: 
<a name="l22972"><span class="ln">22972 </span></a> 
<a name="l22973"><span class="ln">22973 </span></a>    .. math:: 
<a name="l22974"><span class="ln">22974 </span></a>        \text{out}_i = x_i ^ {\text{exponent}_i} 
<a name="l22975"><span class="ln">22975 </span></a> 
<a name="l22976"><span class="ln">22976 </span></a>    When :attr:`exponent` is a tensor, the shapes of :attr:`input` 
<a name="l22977"><span class="ln">22977 </span></a>    and :attr:`exponent` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l22978"><span class="ln">22978 </span></a> 
<a name="l22979"><span class="ln">22979 </span></a>    Args: 
<a name="l22980"><span class="ln">22980 </span></a>        input (Tensor): the input tensor. 
<a name="l22981"><span class="ln">22981 </span></a>        exponent (float or tensor): the exponent value 
<a name="l22982"><span class="ln">22982 </span></a> 
<a name="l22983"><span class="ln">22983 </span></a>    Keyword args: 
<a name="l22984"><span class="ln">22984 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l22985"><span class="ln">22985 </span></a> 
<a name="l22986"><span class="ln">22986 </span></a>    Example:: 
<a name="l22987"><span class="ln">22987 </span></a> 
<a name="l22988"><span class="ln">22988 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l22989"><span class="ln">22989 </span></a>        &gt;&gt;&gt; a 
<a name="l22990"><span class="ln">22990 </span></a>        tensor([ 0.4331,  1.2475,  0.6834, -0.2791]) 
<a name="l22991"><span class="ln">22991 </span></a>        &gt;&gt;&gt; torch.pow(a, 2) 
<a name="l22992"><span class="ln">22992 </span></a>        tensor([ 0.1875,  1.5561,  0.4670,  0.0779]) 
<a name="l22993"><span class="ln">22993 </span></a>        &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l22994"><span class="ln">22994 </span></a> 
<a name="l22995"><span class="ln">22995 </span></a>        &gt;&gt;&gt; a = torch.arange(1., 5.) 
<a name="l22996"><span class="ln">22996 </span></a>        &gt;&gt;&gt; a 
<a name="l22997"><span class="ln">22997 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l22998"><span class="ln">22998 </span></a>        &gt;&gt;&gt; exp 
<a name="l22999"><span class="ln">22999 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l23000"><span class="ln">23000 </span></a>        &gt;&gt;&gt; torch.pow(a, exp) 
<a name="l23001"><span class="ln">23001 </span></a>        tensor([   1.,    4.,   27.,  256.]) 
<a name="l23002"><span class="ln">23002 </span></a> 
<a name="l23003"><span class="ln">23003 </span></a>    .. function:: pow(self, exponent, *, out=None) -&gt; Tensor 
<a name="l23004"><span class="ln">23004 </span></a>       :noindex: 
<a name="l23005"><span class="ln">23005 </span></a> 
<a name="l23006"><span class="ln">23006 </span></a>    :attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor. 
<a name="l23007"><span class="ln">23007 </span></a>    The returned tensor :attr:`out` is of the same shape as :attr:`exponent` 
<a name="l23008"><span class="ln">23008 </span></a> 
<a name="l23009"><span class="ln">23009 </span></a>    The operation applied is: 
<a name="l23010"><span class="ln">23010 </span></a> 
<a name="l23011"><span class="ln">23011 </span></a>    .. math:: 
<a name="l23012"><span class="ln">23012 </span></a>        \text{out}_i = \text{self} ^ {\text{exponent}_i} 
<a name="l23013"><span class="ln">23013 </span></a> 
<a name="l23014"><span class="ln">23014 </span></a>    Args: 
<a name="l23015"><span class="ln">23015 </span></a>        self (float): the scalar base value for the power operation 
<a name="l23016"><span class="ln">23016 </span></a>        exponent (Tensor): the exponent tensor 
<a name="l23017"><span class="ln">23017 </span></a> 
<a name="l23018"><span class="ln">23018 </span></a>    Keyword args: 
<a name="l23019"><span class="ln">23019 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l23020"><span class="ln">23020 </span></a> 
<a name="l23021"><span class="ln">23021 </span></a>    Example:: 
<a name="l23022"><span class="ln">23022 </span></a> 
<a name="l23023"><span class="ln">23023 </span></a>        &gt;&gt;&gt; exp = torch.arange(1., 5.) 
<a name="l23024"><span class="ln">23024 </span></a>        &gt;&gt;&gt; base = 2 
<a name="l23025"><span class="ln">23025 </span></a>        &gt;&gt;&gt; torch.pow(base, exp) 
<a name="l23026"><span class="ln">23026 </span></a>        tensor([  2.,   4.,   8.,  16.]) 
<a name="l23027"><span class="ln">23027 </span></a>    &quot;&quot;&quot;</span>
<a name="l23028"><span class="ln">23028 </span></a>
<a name="l23029"><span class="ln">23029 </span></a><span class="s2">def </span><span class="s1">prelu</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23030"><span class="ln">23030 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23031"><span class="ln">23031 </span></a><span class="s2">def </span><span class="s1">prod</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23032"><span class="ln">23032 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23033"><span class="ln">23033 </span></a>    prod(input: Tensor, *, dtype: Optional[_dtype]) -&gt; Tensor 
<a name="l23034"><span class="ln">23034 </span></a> 
<a name="l23035"><span class="ln">23035 </span></a>    Returns the product of all elements in the :attr:`input` tensor. 
<a name="l23036"><span class="ln">23036 </span></a> 
<a name="l23037"><span class="ln">23037 </span></a>    Args: 
<a name="l23038"><span class="ln">23038 </span></a>        input (Tensor): the input tensor. 
<a name="l23039"><span class="ln">23039 </span></a> 
<a name="l23040"><span class="ln">23040 </span></a>    Keyword args: 
<a name="l23041"><span class="ln">23041 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23042"><span class="ln">23042 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l23043"><span class="ln">23043 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l23044"><span class="ln">23044 </span></a> 
<a name="l23045"><span class="ln">23045 </span></a>    Example:: 
<a name="l23046"><span class="ln">23046 </span></a> 
<a name="l23047"><span class="ln">23047 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l23048"><span class="ln">23048 </span></a>        &gt;&gt;&gt; a 
<a name="l23049"><span class="ln">23049 </span></a>        tensor([[-0.8020,  0.5428, -1.5854]]) 
<a name="l23050"><span class="ln">23050 </span></a>        &gt;&gt;&gt; torch.prod(a) 
<a name="l23051"><span class="ln">23051 </span></a>        tensor(0.6902) 
<a name="l23052"><span class="ln">23052 </span></a> 
<a name="l23053"><span class="ln">23053 </span></a>    .. function:: prod(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l23054"><span class="ln">23054 </span></a>       :noindex: 
<a name="l23055"><span class="ln">23055 </span></a> 
<a name="l23056"><span class="ln">23056 </span></a>    Returns the product of each row of the :attr:`input` tensor in the given 
<a name="l23057"><span class="ln">23057 </span></a>    dimension :attr:`dim`. 
<a name="l23058"><span class="ln">23058 </span></a> 
<a name="l23059"><span class="ln">23059 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l23060"><span class="ln">23060 </span></a>    as :attr:`input` except in the dimension :attr:`dim` where it is of size 1. 
<a name="l23061"><span class="ln">23061 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l23062"><span class="ln">23062 </span></a>    the output tensor having 1 fewer dimension than :attr:`input`. 
<a name="l23063"><span class="ln">23063 </span></a> 
<a name="l23064"><span class="ln">23064 </span></a>    Args: 
<a name="l23065"><span class="ln">23065 </span></a>        input (Tensor): the input tensor. 
<a name="l23066"><span class="ln">23066 </span></a> 
<a name="l23067"><span class="ln">23067 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l23068"><span class="ln">23068 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l23069"><span class="ln">23069 </span></a> 
<a name="l23070"><span class="ln">23070 </span></a> 
<a name="l23071"><span class="ln">23071 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l23072"><span class="ln">23072 </span></a> 
<a name="l23073"><span class="ln">23073 </span></a> 
<a name="l23074"><span class="ln">23074 </span></a>    Keyword args: 
<a name="l23075"><span class="ln">23075 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23076"><span class="ln">23076 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l23077"><span class="ln">23077 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l23078"><span class="ln">23078 </span></a> 
<a name="l23079"><span class="ln">23079 </span></a>    Example:: 
<a name="l23080"><span class="ln">23080 </span></a> 
<a name="l23081"><span class="ln">23081 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 2) 
<a name="l23082"><span class="ln">23082 </span></a>        &gt;&gt;&gt; a 
<a name="l23083"><span class="ln">23083 </span></a>        tensor([[ 0.5261, -0.3837], 
<a name="l23084"><span class="ln">23084 </span></a>                [ 1.1857, -0.2498], 
<a name="l23085"><span class="ln">23085 </span></a>                [-1.1646,  0.0705], 
<a name="l23086"><span class="ln">23086 </span></a>                [ 1.1131, -1.0629]]) 
<a name="l23087"><span class="ln">23087 </span></a>        &gt;&gt;&gt; torch.prod(a, 1) 
<a name="l23088"><span class="ln">23088 </span></a>        tensor([-0.2018, -0.2962, -0.0821, -1.1831]) 
<a name="l23089"><span class="ln">23089 </span></a>    &quot;&quot;&quot;</span>
<a name="l23090"><span class="ln">23090 </span></a>
<a name="l23091"><span class="ln">23091 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23092"><span class="ln">23092 </span></a><span class="s2">def </span><span class="s1">prod</span><span class="s3">(</span>
<a name="l23093"><span class="ln">23093 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23094"><span class="ln">23094 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l23095"><span class="ln">23095 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23096"><span class="ln">23096 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l23097"><span class="ln">23097 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23098"><span class="ln">23098 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23099"><span class="ln">23099 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23100"><span class="ln">23100 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23101"><span class="ln">23101 </span></a>    prod(input: Tensor, *, dtype: Optional[_dtype]) -&gt; Tensor 
<a name="l23102"><span class="ln">23102 </span></a> 
<a name="l23103"><span class="ln">23103 </span></a>    Returns the product of all elements in the :attr:`input` tensor. 
<a name="l23104"><span class="ln">23104 </span></a> 
<a name="l23105"><span class="ln">23105 </span></a>    Args: 
<a name="l23106"><span class="ln">23106 </span></a>        input (Tensor): the input tensor. 
<a name="l23107"><span class="ln">23107 </span></a> 
<a name="l23108"><span class="ln">23108 </span></a>    Keyword args: 
<a name="l23109"><span class="ln">23109 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23110"><span class="ln">23110 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l23111"><span class="ln">23111 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l23112"><span class="ln">23112 </span></a> 
<a name="l23113"><span class="ln">23113 </span></a>    Example:: 
<a name="l23114"><span class="ln">23114 </span></a> 
<a name="l23115"><span class="ln">23115 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l23116"><span class="ln">23116 </span></a>        &gt;&gt;&gt; a 
<a name="l23117"><span class="ln">23117 </span></a>        tensor([[-0.8020,  0.5428, -1.5854]]) 
<a name="l23118"><span class="ln">23118 </span></a>        &gt;&gt;&gt; torch.prod(a) 
<a name="l23119"><span class="ln">23119 </span></a>        tensor(0.6902) 
<a name="l23120"><span class="ln">23120 </span></a> 
<a name="l23121"><span class="ln">23121 </span></a>    .. function:: prod(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l23122"><span class="ln">23122 </span></a>       :noindex: 
<a name="l23123"><span class="ln">23123 </span></a> 
<a name="l23124"><span class="ln">23124 </span></a>    Returns the product of each row of the :attr:`input` tensor in the given 
<a name="l23125"><span class="ln">23125 </span></a>    dimension :attr:`dim`. 
<a name="l23126"><span class="ln">23126 </span></a> 
<a name="l23127"><span class="ln">23127 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l23128"><span class="ln">23128 </span></a>    as :attr:`input` except in the dimension :attr:`dim` where it is of size 1. 
<a name="l23129"><span class="ln">23129 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l23130"><span class="ln">23130 </span></a>    the output tensor having 1 fewer dimension than :attr:`input`. 
<a name="l23131"><span class="ln">23131 </span></a> 
<a name="l23132"><span class="ln">23132 </span></a>    Args: 
<a name="l23133"><span class="ln">23133 </span></a>        input (Tensor): the input tensor. 
<a name="l23134"><span class="ln">23134 </span></a> 
<a name="l23135"><span class="ln">23135 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l23136"><span class="ln">23136 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l23137"><span class="ln">23137 </span></a> 
<a name="l23138"><span class="ln">23138 </span></a> 
<a name="l23139"><span class="ln">23139 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l23140"><span class="ln">23140 </span></a> 
<a name="l23141"><span class="ln">23141 </span></a> 
<a name="l23142"><span class="ln">23142 </span></a>    Keyword args: 
<a name="l23143"><span class="ln">23143 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23144"><span class="ln">23144 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l23145"><span class="ln">23145 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l23146"><span class="ln">23146 </span></a> 
<a name="l23147"><span class="ln">23147 </span></a>    Example:: 
<a name="l23148"><span class="ln">23148 </span></a> 
<a name="l23149"><span class="ln">23149 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 2) 
<a name="l23150"><span class="ln">23150 </span></a>        &gt;&gt;&gt; a 
<a name="l23151"><span class="ln">23151 </span></a>        tensor([[ 0.5261, -0.3837], 
<a name="l23152"><span class="ln">23152 </span></a>                [ 1.1857, -0.2498], 
<a name="l23153"><span class="ln">23153 </span></a>                [-1.1646,  0.0705], 
<a name="l23154"><span class="ln">23154 </span></a>                [ 1.1131, -1.0629]]) 
<a name="l23155"><span class="ln">23155 </span></a>        &gt;&gt;&gt; torch.prod(a, 1) 
<a name="l23156"><span class="ln">23156 </span></a>        tensor([-0.2018, -0.2962, -0.0821, -1.1831]) 
<a name="l23157"><span class="ln">23157 </span></a>    &quot;&quot;&quot;</span>
<a name="l23158"><span class="ln">23158 </span></a>
<a name="l23159"><span class="ln">23159 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23160"><span class="ln">23160 </span></a><span class="s2">def </span><span class="s1">prod</span><span class="s3">(</span>
<a name="l23161"><span class="ln">23161 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23162"><span class="ln">23162 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23163"><span class="ln">23163 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23164"><span class="ln">23164 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l23165"><span class="ln">23165 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23166"><span class="ln">23166 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23167"><span class="ln">23167 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23168"><span class="ln">23168 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23169"><span class="ln">23169 </span></a>    prod(input: Tensor, *, dtype: Optional[_dtype]) -&gt; Tensor 
<a name="l23170"><span class="ln">23170 </span></a> 
<a name="l23171"><span class="ln">23171 </span></a>    Returns the product of all elements in the :attr:`input` tensor. 
<a name="l23172"><span class="ln">23172 </span></a> 
<a name="l23173"><span class="ln">23173 </span></a>    Args: 
<a name="l23174"><span class="ln">23174 </span></a>        input (Tensor): the input tensor. 
<a name="l23175"><span class="ln">23175 </span></a> 
<a name="l23176"><span class="ln">23176 </span></a>    Keyword args: 
<a name="l23177"><span class="ln">23177 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23178"><span class="ln">23178 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l23179"><span class="ln">23179 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l23180"><span class="ln">23180 </span></a> 
<a name="l23181"><span class="ln">23181 </span></a>    Example:: 
<a name="l23182"><span class="ln">23182 </span></a> 
<a name="l23183"><span class="ln">23183 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l23184"><span class="ln">23184 </span></a>        &gt;&gt;&gt; a 
<a name="l23185"><span class="ln">23185 </span></a>        tensor([[-0.8020,  0.5428, -1.5854]]) 
<a name="l23186"><span class="ln">23186 </span></a>        &gt;&gt;&gt; torch.prod(a) 
<a name="l23187"><span class="ln">23187 </span></a>        tensor(0.6902) 
<a name="l23188"><span class="ln">23188 </span></a> 
<a name="l23189"><span class="ln">23189 </span></a>    .. function:: prod(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l23190"><span class="ln">23190 </span></a>       :noindex: 
<a name="l23191"><span class="ln">23191 </span></a> 
<a name="l23192"><span class="ln">23192 </span></a>    Returns the product of each row of the :attr:`input` tensor in the given 
<a name="l23193"><span class="ln">23193 </span></a>    dimension :attr:`dim`. 
<a name="l23194"><span class="ln">23194 </span></a> 
<a name="l23195"><span class="ln">23195 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l23196"><span class="ln">23196 </span></a>    as :attr:`input` except in the dimension :attr:`dim` where it is of size 1. 
<a name="l23197"><span class="ln">23197 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in 
<a name="l23198"><span class="ln">23198 </span></a>    the output tensor having 1 fewer dimension than :attr:`input`. 
<a name="l23199"><span class="ln">23199 </span></a> 
<a name="l23200"><span class="ln">23200 </span></a>    Args: 
<a name="l23201"><span class="ln">23201 </span></a>        input (Tensor): the input tensor. 
<a name="l23202"><span class="ln">23202 </span></a> 
<a name="l23203"><span class="ln">23203 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l23204"><span class="ln">23204 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l23205"><span class="ln">23205 </span></a> 
<a name="l23206"><span class="ln">23206 </span></a> 
<a name="l23207"><span class="ln">23207 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l23208"><span class="ln">23208 </span></a> 
<a name="l23209"><span class="ln">23209 </span></a> 
<a name="l23210"><span class="ln">23210 </span></a>    Keyword args: 
<a name="l23211"><span class="ln">23211 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23212"><span class="ln">23212 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l23213"><span class="ln">23213 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l23214"><span class="ln">23214 </span></a> 
<a name="l23215"><span class="ln">23215 </span></a>    Example:: 
<a name="l23216"><span class="ln">23216 </span></a> 
<a name="l23217"><span class="ln">23217 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 2) 
<a name="l23218"><span class="ln">23218 </span></a>        &gt;&gt;&gt; a 
<a name="l23219"><span class="ln">23219 </span></a>        tensor([[ 0.5261, -0.3837], 
<a name="l23220"><span class="ln">23220 </span></a>                [ 1.1857, -0.2498], 
<a name="l23221"><span class="ln">23221 </span></a>                [-1.1646,  0.0705], 
<a name="l23222"><span class="ln">23222 </span></a>                [ 1.1131, -1.0629]]) 
<a name="l23223"><span class="ln">23223 </span></a>        &gt;&gt;&gt; torch.prod(a, 1) 
<a name="l23224"><span class="ln">23224 </span></a>        tensor([-0.2018, -0.2962, -0.0821, -1.1831]) 
<a name="l23225"><span class="ln">23225 </span></a>    &quot;&quot;&quot;</span>
<a name="l23226"><span class="ln">23226 </span></a>
<a name="l23227"><span class="ln">23227 </span></a><span class="s2">def </span><span class="s1">promote_types</span><span class="s3">(</span><span class="s1">type1</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">, </span><span class="s1">type2</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">) </span><span class="s1">-&gt; _dtype</span><span class="s2">:</span>
<a name="l23228"><span class="ln">23228 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23229"><span class="ln">23229 </span></a>    promote_types(type1, type2) -&gt; dtype 
<a name="l23230"><span class="ln">23230 </span></a> 
<a name="l23231"><span class="ln">23231 </span></a>    Returns the :class:`torch.dtype` with the smallest size and scalar kind that is 
<a name="l23232"><span class="ln">23232 </span></a>    not smaller nor of lower kind than either `type1` or `type2`. See type promotion 
<a name="l23233"><span class="ln">23233 </span></a>    :ref:`documentation &lt;type-promotion-doc&gt;` for more information on the type 
<a name="l23234"><span class="ln">23234 </span></a>    promotion logic. 
<a name="l23235"><span class="ln">23235 </span></a> 
<a name="l23236"><span class="ln">23236 </span></a>    Args: 
<a name="l23237"><span class="ln">23237 </span></a>        type1 (:class:`torch.dtype`) 
<a name="l23238"><span class="ln">23238 </span></a>        type2 (:class:`torch.dtype`) 
<a name="l23239"><span class="ln">23239 </span></a> 
<a name="l23240"><span class="ln">23240 </span></a>    Example:: 
<a name="l23241"><span class="ln">23241 </span></a> 
<a name="l23242"><span class="ln">23242 </span></a>        &gt;&gt;&gt; torch.promote_types(torch.int32, torch.float32) 
<a name="l23243"><span class="ln">23243 </span></a>        torch.float32 
<a name="l23244"><span class="ln">23244 </span></a>        &gt;&gt;&gt; torch.promote_types(torch.uint8, torch.long) 
<a name="l23245"><span class="ln">23245 </span></a>        torch.long 
<a name="l23246"><span class="ln">23246 </span></a>    &quot;&quot;&quot;</span>
<a name="l23247"><span class="ln">23247 </span></a>
<a name="l23248"><span class="ln">23248 </span></a><span class="s2">def </span><span class="s1">put</span><span class="s3">(</span>
<a name="l23249"><span class="ln">23249 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23250"><span class="ln">23250 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23251"><span class="ln">23251 </span></a>    <span class="s1">source</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23252"><span class="ln">23252 </span></a>    <span class="s1">accumulate</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23253"><span class="ln">23253 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23254"><span class="ln">23254 </span></a><span class="s2">def </span><span class="s1">q_per_channel_axis</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23255"><span class="ln">23255 </span></a><span class="s2">def </span><span class="s1">q_per_channel_scales</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23256"><span class="ln">23256 </span></a><span class="s2">def </span><span class="s1">q_per_channel_zero_points</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23257"><span class="ln">23257 </span></a><span class="s2">def </span><span class="s1">q_scale</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _float</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23258"><span class="ln">23258 </span></a><span class="s2">def </span><span class="s1">q_zero_point</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _int</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23259"><span class="ln">23259 </span></a><span class="s2">def </span><span class="s1">qr</span><span class="s3">(</span>
<a name="l23260"><span class="ln">23260 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23261"><span class="ln">23261 </span></a>    <span class="s1">some</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l23262"><span class="ln">23262 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l23263"><span class="ln">23263 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23264"><span class="ln">23264 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">qr</span><span class="s2">:</span>
<a name="l23265"><span class="ln">23265 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23266"><span class="ln">23266 </span></a>    qr(input: Tensor, some: bool = True, *, out: Union[Tensor, Tuple[Tensor, ...], List[Tensor], None]) -&gt; (Tensor, Tensor) 
<a name="l23267"><span class="ln">23267 </span></a> 
<a name="l23268"><span class="ln">23268 </span></a>    Computes the QR decomposition of a matrix or a batch of matrices :attr:`input`, 
<a name="l23269"><span class="ln">23269 </span></a>    and returns a namedtuple (Q, R) of tensors such that :math:`\text{input} = Q R` 
<a name="l23270"><span class="ln">23270 </span></a>    with :math:`Q` being an orthogonal matrix or batch of orthogonal matrices and 
<a name="l23271"><span class="ln">23271 </span></a>    :math:`R` being an upper triangular matrix or batch of upper triangular matrices. 
<a name="l23272"><span class="ln">23272 </span></a> 
<a name="l23273"><span class="ln">23273 </span></a>    If :attr:`some` is ``True``, then this function returns the thin (reduced) QR factorization. 
<a name="l23274"><span class="ln">23274 </span></a>    Otherwise, if :attr:`some` is ``False``, this function returns the complete QR factorization. 
<a name="l23275"><span class="ln">23275 </span></a> 
<a name="l23276"><span class="ln">23276 </span></a>    .. warning:: 
<a name="l23277"><span class="ln">23277 </span></a> 
<a name="l23278"><span class="ln">23278 </span></a>        :func:`torch.qr` is deprecated in favor of :func:`torch.linalg.qr` 
<a name="l23279"><span class="ln">23279 </span></a>        and will be removed in a future PyTorch release. The boolean parameter :attr:`some` has been 
<a name="l23280"><span class="ln">23280 </span></a>        replaced with a string parameter :attr:`mode`. 
<a name="l23281"><span class="ln">23281 </span></a> 
<a name="l23282"><span class="ln">23282 </span></a>        ``Q, R = torch.qr(A)`` should be replaced with 
<a name="l23283"><span class="ln">23283 </span></a> 
<a name="l23284"><span class="ln">23284 </span></a>        .. code:: python 
<a name="l23285"><span class="ln">23285 </span></a> 
<a name="l23286"><span class="ln">23286 </span></a>            Q, R = torch.linalg.qr(A) 
<a name="l23287"><span class="ln">23287 </span></a> 
<a name="l23288"><span class="ln">23288 </span></a>        ``Q, R = torch.qr(A, some=False)`` should be replaced with 
<a name="l23289"><span class="ln">23289 </span></a> 
<a name="l23290"><span class="ln">23290 </span></a>        .. code:: python 
<a name="l23291"><span class="ln">23291 </span></a> 
<a name="l23292"><span class="ln">23292 </span></a>            Q, R = torch.linalg.qr(A, mode=&quot;complete&quot;) 
<a name="l23293"><span class="ln">23293 </span></a> 
<a name="l23294"><span class="ln">23294 </span></a>    .. warning:: 
<a name="l23295"><span class="ln">23295 </span></a>              If you plan to backpropagate through QR, note that the current backward implementation 
<a name="l23296"><span class="ln">23296 </span></a>              is only well-defined when the first :math:`\min(input.size(-1), input.size(-2))` 
<a name="l23297"><span class="ln">23297 </span></a>              columns of :attr:`input` are linearly independent. 
<a name="l23298"><span class="ln">23298 </span></a>              This behavior will probably change once QR supports pivoting. 
<a name="l23299"><span class="ln">23299 </span></a> 
<a name="l23300"><span class="ln">23300 </span></a>    .. note:: This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, 
<a name="l23301"><span class="ln">23301 </span></a>              and may produce different (valid) decompositions on different device types 
<a name="l23302"><span class="ln">23302 </span></a>              or different platforms. 
<a name="l23303"><span class="ln">23303 </span></a> 
<a name="l23304"><span class="ln">23304 </span></a>    Args: 
<a name="l23305"><span class="ln">23305 </span></a>        input (Tensor): the input tensor of size :math:`(*, m, n)` where `*` is zero or more 
<a name="l23306"><span class="ln">23306 </span></a>                    batch dimensions consisting of matrices of dimension :math:`m \times n`. 
<a name="l23307"><span class="ln">23307 </span></a>        some (bool, optional): Set to ``True`` for reduced QR decomposition and ``False`` for 
<a name="l23308"><span class="ln">23308 </span></a>                    complete QR decomposition. If `k = min(m, n)` then: 
<a name="l23309"><span class="ln">23309 </span></a> 
<a name="l23310"><span class="ln">23310 </span></a>                      * ``some=True`` : returns `(Q, R)` with dimensions (m, k), (k, n) (default) 
<a name="l23311"><span class="ln">23311 </span></a> 
<a name="l23312"><span class="ln">23312 </span></a>                      * ``'some=False'``: returns `(Q, R)` with dimensions (m, m), (m, n) 
<a name="l23313"><span class="ln">23313 </span></a> 
<a name="l23314"><span class="ln">23314 </span></a>    Keyword args: 
<a name="l23315"><span class="ln">23315 </span></a>        out (tuple, optional): tuple of `Q` and `R` tensors. 
<a name="l23316"><span class="ln">23316 </span></a>                    The dimensions of `Q` and `R` are detailed in the description of :attr:`some` above. 
<a name="l23317"><span class="ln">23317 </span></a> 
<a name="l23318"><span class="ln">23318 </span></a>    Example:: 
<a name="l23319"><span class="ln">23319 </span></a> 
<a name="l23320"><span class="ln">23320 </span></a>        &gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]]) 
<a name="l23321"><span class="ln">23321 </span></a>        &gt;&gt;&gt; q, r = torch.qr(a) 
<a name="l23322"><span class="ln">23322 </span></a>        &gt;&gt;&gt; q 
<a name="l23323"><span class="ln">23323 </span></a>        tensor([[-0.8571,  0.3943,  0.3314], 
<a name="l23324"><span class="ln">23324 </span></a>                [-0.4286, -0.9029, -0.0343], 
<a name="l23325"><span class="ln">23325 </span></a>                [ 0.2857, -0.1714,  0.9429]]) 
<a name="l23326"><span class="ln">23326 </span></a>        &gt;&gt;&gt; r 
<a name="l23327"><span class="ln">23327 </span></a>        tensor([[ -14.0000,  -21.0000,   14.0000], 
<a name="l23328"><span class="ln">23328 </span></a>                [   0.0000, -175.0000,   70.0000], 
<a name="l23329"><span class="ln">23329 </span></a>                [   0.0000,    0.0000,  -35.0000]]) 
<a name="l23330"><span class="ln">23330 </span></a>        &gt;&gt;&gt; torch.mm(q, r).round() 
<a name="l23331"><span class="ln">23331 </span></a>        tensor([[  12.,  -51.,    4.], 
<a name="l23332"><span class="ln">23332 </span></a>                [   6.,  167.,  -68.], 
<a name="l23333"><span class="ln">23333 </span></a>                [  -4.,   24.,  -41.]]) 
<a name="l23334"><span class="ln">23334 </span></a>        &gt;&gt;&gt; torch.mm(q.t(), q).round() 
<a name="l23335"><span class="ln">23335 </span></a>        tensor([[ 1.,  0.,  0.], 
<a name="l23336"><span class="ln">23336 </span></a>                [ 0.,  1., -0.], 
<a name="l23337"><span class="ln">23337 </span></a>                [ 0., -0.,  1.]]) 
<a name="l23338"><span class="ln">23338 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 4, 5) 
<a name="l23339"><span class="ln">23339 </span></a>        &gt;&gt;&gt; q, r = torch.qr(a, some=False) 
<a name="l23340"><span class="ln">23340 </span></a>        &gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a) 
<a name="l23341"><span class="ln">23341 </span></a>        True 
<a name="l23342"><span class="ln">23342 </span></a>        &gt;&gt;&gt; torch.allclose(torch.matmul(q.mT, q), torch.eye(5)) 
<a name="l23343"><span class="ln">23343 </span></a>        True 
<a name="l23344"><span class="ln">23344 </span></a>    &quot;&quot;&quot;</span>
<a name="l23345"><span class="ln">23345 </span></a>
<a name="l23346"><span class="ln">23346 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23347"><span class="ln">23347 </span></a><span class="s2">def </span><span class="s1">quantile</span><span class="s3">(</span>
<a name="l23348"><span class="ln">23348 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23349"><span class="ln">23349 </span></a>    <span class="s1">q</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23350"><span class="ln">23350 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23351"><span class="ln">23351 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23352"><span class="ln">23352 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l23353"><span class="ln">23353 </span></a>    <span class="s1">interpolation</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;linear&quot;</span><span class="s3">,</span>
<a name="l23354"><span class="ln">23354 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23355"><span class="ln">23355 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23356"><span class="ln">23356 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23357"><span class="ln">23357 </span></a>    quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -&gt; Tensor 
<a name="l23358"><span class="ln">23358 </span></a> 
<a name="l23359"><span class="ln">23359 </span></a>    Computes the q-th quantiles of each row of the :attr:`input` tensor along the dimension :attr:`dim`. 
<a name="l23360"><span class="ln">23360 </span></a> 
<a name="l23361"><span class="ln">23361 </span></a>    To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location 
<a name="l23362"><span class="ln">23362 </span></a>    of the quantile in the sorted input. If the quantile lies between two data points ``a &lt; b`` with 
<a name="l23363"><span class="ln">23363 </span></a>    indices ``i`` and ``j`` in the sorted order, result is computed according to the given 
<a name="l23364"><span class="ln">23364 </span></a>    :attr:`interpolation` method as follows: 
<a name="l23365"><span class="ln">23365 </span></a> 
<a name="l23366"><span class="ln">23366 </span></a>    - ``linear``: ``a + (b - a) * fraction``, where ``fraction`` is the fractional part of the computed quantile index. 
<a name="l23367"><span class="ln">23367 </span></a>    - ``lower``: ``a``. 
<a name="l23368"><span class="ln">23368 </span></a>    - ``higher``: ``b``. 
<a name="l23369"><span class="ln">23369 </span></a>    - ``nearest``: ``a`` or ``b``, whichever's index is closer to the computed quantile index (rounding down for .5 fractions). 
<a name="l23370"><span class="ln">23370 </span></a>    - ``midpoint``: ``(a + b) / 2``. 
<a name="l23371"><span class="ln">23371 </span></a> 
<a name="l23372"><span class="ln">23372 </span></a>    If :attr:`q` is a 1D tensor, the first dimension of the output represents the quantiles and has size 
<a name="l23373"><span class="ln">23373 </span></a>    equal to the size of :attr:`q`, the remaining dimensions are what remains from the reduction. 
<a name="l23374"><span class="ln">23374 </span></a> 
<a name="l23375"><span class="ln">23375 </span></a>    .. note:: 
<a name="l23376"><span class="ln">23376 </span></a>        By default :attr:`dim` is ``None`` resulting in the :attr:`input` tensor being flattened before computation. 
<a name="l23377"><span class="ln">23377 </span></a> 
<a name="l23378"><span class="ln">23378 </span></a>    Args: 
<a name="l23379"><span class="ln">23379 </span></a>        input (Tensor): the input tensor. 
<a name="l23380"><span class="ln">23380 </span></a>        q (float or Tensor): a scalar or 1D tensor of values in the range [0, 1]. 
<a name="l23381"><span class="ln">23381 </span></a> 
<a name="l23382"><span class="ln">23382 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l23383"><span class="ln">23383 </span></a> 
<a name="l23384"><span class="ln">23384 </span></a> 
<a name="l23385"><span class="ln">23385 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l23386"><span class="ln">23386 </span></a> 
<a name="l23387"><span class="ln">23387 </span></a> 
<a name="l23388"><span class="ln">23388 </span></a>    Keyword arguments: 
<a name="l23389"><span class="ln">23389 </span></a>        interpolation (str): interpolation method to use when the desired quantile lies between two data points. 
<a name="l23390"><span class="ln">23390 </span></a>                                Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``. 
<a name="l23391"><span class="ln">23391 </span></a>                                Default is ``linear``. 
<a name="l23392"><span class="ln">23392 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l23393"><span class="ln">23393 </span></a> 
<a name="l23394"><span class="ln">23394 </span></a>    Example:: 
<a name="l23395"><span class="ln">23395 </span></a> 
<a name="l23396"><span class="ln">23396 </span></a>        &gt;&gt;&gt; a = torch.randn(2, 3) 
<a name="l23397"><span class="ln">23397 </span></a>        &gt;&gt;&gt; a 
<a name="l23398"><span class="ln">23398 </span></a>        tensor([[ 0.0795, -1.2117,  0.9765], 
<a name="l23399"><span class="ln">23399 </span></a>                [ 1.1707,  0.6706,  0.4884]]) 
<a name="l23400"><span class="ln">23400 </span></a>        &gt;&gt;&gt; q = torch.tensor([0.25, 0.5, 0.75]) 
<a name="l23401"><span class="ln">23401 </span></a>        &gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True) 
<a name="l23402"><span class="ln">23402 </span></a>        tensor([[[-0.5661], 
<a name="l23403"><span class="ln">23403 </span></a>                [ 0.5795]], 
<a name="l23404"><span class="ln">23404 </span></a> 
<a name="l23405"><span class="ln">23405 </span></a>                [[ 0.0795], 
<a name="l23406"><span class="ln">23406 </span></a>                [ 0.6706]], 
<a name="l23407"><span class="ln">23407 </span></a> 
<a name="l23408"><span class="ln">23408 </span></a>                [[ 0.5280], 
<a name="l23409"><span class="ln">23409 </span></a>                [ 0.9206]]]) 
<a name="l23410"><span class="ln">23410 </span></a>        &gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True).shape 
<a name="l23411"><span class="ln">23411 </span></a>        torch.Size([3, 2, 1]) 
<a name="l23412"><span class="ln">23412 </span></a>        &gt;&gt;&gt; a = torch.arange(4.) 
<a name="l23413"><span class="ln">23413 </span></a>        &gt;&gt;&gt; a 
<a name="l23414"><span class="ln">23414 </span></a>        tensor([0., 1., 2., 3.]) 
<a name="l23415"><span class="ln">23415 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='linear') 
<a name="l23416"><span class="ln">23416 </span></a>        tensor(1.8000) 
<a name="l23417"><span class="ln">23417 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='lower') 
<a name="l23418"><span class="ln">23418 </span></a>        tensor(1.) 
<a name="l23419"><span class="ln">23419 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='higher') 
<a name="l23420"><span class="ln">23420 </span></a>        tensor(2.) 
<a name="l23421"><span class="ln">23421 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='midpoint') 
<a name="l23422"><span class="ln">23422 </span></a>        tensor(1.5000) 
<a name="l23423"><span class="ln">23423 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='nearest') 
<a name="l23424"><span class="ln">23424 </span></a>        tensor(2.) 
<a name="l23425"><span class="ln">23425 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.4, interpolation='nearest') 
<a name="l23426"><span class="ln">23426 </span></a>        tensor(1.) 
<a name="l23427"><span class="ln">23427 </span></a>    &quot;&quot;&quot;</span>
<a name="l23428"><span class="ln">23428 </span></a>
<a name="l23429"><span class="ln">23429 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23430"><span class="ln">23430 </span></a><span class="s2">def </span><span class="s1">quantile</span><span class="s3">(</span>
<a name="l23431"><span class="ln">23431 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23432"><span class="ln">23432 </span></a>    <span class="s1">q</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l23433"><span class="ln">23433 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23434"><span class="ln">23434 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23435"><span class="ln">23435 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l23436"><span class="ln">23436 </span></a>    <span class="s1">interpolation</span><span class="s2">: </span><span class="s1">str </span><span class="s2">= </span><span class="s4">&quot;linear&quot;</span><span class="s3">,</span>
<a name="l23437"><span class="ln">23437 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23438"><span class="ln">23438 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23439"><span class="ln">23439 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23440"><span class="ln">23440 </span></a>    quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -&gt; Tensor 
<a name="l23441"><span class="ln">23441 </span></a> 
<a name="l23442"><span class="ln">23442 </span></a>    Computes the q-th quantiles of each row of the :attr:`input` tensor along the dimension :attr:`dim`. 
<a name="l23443"><span class="ln">23443 </span></a> 
<a name="l23444"><span class="ln">23444 </span></a>    To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location 
<a name="l23445"><span class="ln">23445 </span></a>    of the quantile in the sorted input. If the quantile lies between two data points ``a &lt; b`` with 
<a name="l23446"><span class="ln">23446 </span></a>    indices ``i`` and ``j`` in the sorted order, result is computed according to the given 
<a name="l23447"><span class="ln">23447 </span></a>    :attr:`interpolation` method as follows: 
<a name="l23448"><span class="ln">23448 </span></a> 
<a name="l23449"><span class="ln">23449 </span></a>    - ``linear``: ``a + (b - a) * fraction``, where ``fraction`` is the fractional part of the computed quantile index. 
<a name="l23450"><span class="ln">23450 </span></a>    - ``lower``: ``a``. 
<a name="l23451"><span class="ln">23451 </span></a>    - ``higher``: ``b``. 
<a name="l23452"><span class="ln">23452 </span></a>    - ``nearest``: ``a`` or ``b``, whichever's index is closer to the computed quantile index (rounding down for .5 fractions). 
<a name="l23453"><span class="ln">23453 </span></a>    - ``midpoint``: ``(a + b) / 2``. 
<a name="l23454"><span class="ln">23454 </span></a> 
<a name="l23455"><span class="ln">23455 </span></a>    If :attr:`q` is a 1D tensor, the first dimension of the output represents the quantiles and has size 
<a name="l23456"><span class="ln">23456 </span></a>    equal to the size of :attr:`q`, the remaining dimensions are what remains from the reduction. 
<a name="l23457"><span class="ln">23457 </span></a> 
<a name="l23458"><span class="ln">23458 </span></a>    .. note:: 
<a name="l23459"><span class="ln">23459 </span></a>        By default :attr:`dim` is ``None`` resulting in the :attr:`input` tensor being flattened before computation. 
<a name="l23460"><span class="ln">23460 </span></a> 
<a name="l23461"><span class="ln">23461 </span></a>    Args: 
<a name="l23462"><span class="ln">23462 </span></a>        input (Tensor): the input tensor. 
<a name="l23463"><span class="ln">23463 </span></a>        q (float or Tensor): a scalar or 1D tensor of values in the range [0, 1]. 
<a name="l23464"><span class="ln">23464 </span></a> 
<a name="l23465"><span class="ln">23465 </span></a>        dim (int, optional): the dimension to reduce. 
<a name="l23466"><span class="ln">23466 </span></a> 
<a name="l23467"><span class="ln">23467 </span></a> 
<a name="l23468"><span class="ln">23468 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l23469"><span class="ln">23469 </span></a> 
<a name="l23470"><span class="ln">23470 </span></a> 
<a name="l23471"><span class="ln">23471 </span></a>    Keyword arguments: 
<a name="l23472"><span class="ln">23472 </span></a>        interpolation (str): interpolation method to use when the desired quantile lies between two data points. 
<a name="l23473"><span class="ln">23473 </span></a>                                Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``. 
<a name="l23474"><span class="ln">23474 </span></a>                                Default is ``linear``. 
<a name="l23475"><span class="ln">23475 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l23476"><span class="ln">23476 </span></a> 
<a name="l23477"><span class="ln">23477 </span></a>    Example:: 
<a name="l23478"><span class="ln">23478 </span></a> 
<a name="l23479"><span class="ln">23479 </span></a>        &gt;&gt;&gt; a = torch.randn(2, 3) 
<a name="l23480"><span class="ln">23480 </span></a>        &gt;&gt;&gt; a 
<a name="l23481"><span class="ln">23481 </span></a>        tensor([[ 0.0795, -1.2117,  0.9765], 
<a name="l23482"><span class="ln">23482 </span></a>                [ 1.1707,  0.6706,  0.4884]]) 
<a name="l23483"><span class="ln">23483 </span></a>        &gt;&gt;&gt; q = torch.tensor([0.25, 0.5, 0.75]) 
<a name="l23484"><span class="ln">23484 </span></a>        &gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True) 
<a name="l23485"><span class="ln">23485 </span></a>        tensor([[[-0.5661], 
<a name="l23486"><span class="ln">23486 </span></a>                [ 0.5795]], 
<a name="l23487"><span class="ln">23487 </span></a> 
<a name="l23488"><span class="ln">23488 </span></a>                [[ 0.0795], 
<a name="l23489"><span class="ln">23489 </span></a>                [ 0.6706]], 
<a name="l23490"><span class="ln">23490 </span></a> 
<a name="l23491"><span class="ln">23491 </span></a>                [[ 0.5280], 
<a name="l23492"><span class="ln">23492 </span></a>                [ 0.9206]]]) 
<a name="l23493"><span class="ln">23493 </span></a>        &gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True).shape 
<a name="l23494"><span class="ln">23494 </span></a>        torch.Size([3, 2, 1]) 
<a name="l23495"><span class="ln">23495 </span></a>        &gt;&gt;&gt; a = torch.arange(4.) 
<a name="l23496"><span class="ln">23496 </span></a>        &gt;&gt;&gt; a 
<a name="l23497"><span class="ln">23497 </span></a>        tensor([0., 1., 2., 3.]) 
<a name="l23498"><span class="ln">23498 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='linear') 
<a name="l23499"><span class="ln">23499 </span></a>        tensor(1.8000) 
<a name="l23500"><span class="ln">23500 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='lower') 
<a name="l23501"><span class="ln">23501 </span></a>        tensor(1.) 
<a name="l23502"><span class="ln">23502 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='higher') 
<a name="l23503"><span class="ln">23503 </span></a>        tensor(2.) 
<a name="l23504"><span class="ln">23504 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='midpoint') 
<a name="l23505"><span class="ln">23505 </span></a>        tensor(1.5000) 
<a name="l23506"><span class="ln">23506 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='nearest') 
<a name="l23507"><span class="ln">23507 </span></a>        tensor(2.) 
<a name="l23508"><span class="ln">23508 </span></a>        &gt;&gt;&gt; torch.quantile(a, 0.4, interpolation='nearest') 
<a name="l23509"><span class="ln">23509 </span></a>        tensor(1.) 
<a name="l23510"><span class="ln">23510 </span></a>    &quot;&quot;&quot;</span>
<a name="l23511"><span class="ln">23511 </span></a>
<a name="l23512"><span class="ln">23512 </span></a><span class="s2">def </span><span class="s1">quantize_per_channel</span><span class="s3">(</span>
<a name="l23513"><span class="ln">23513 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23514"><span class="ln">23514 </span></a>    <span class="s1">scales</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23515"><span class="ln">23515 </span></a>    <span class="s1">zero_points</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23516"><span class="ln">23516 </span></a>    <span class="s1">axis</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l23517"><span class="ln">23517 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l23518"><span class="ln">23518 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23519"><span class="ln">23519 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23520"><span class="ln">23520 </span></a>    quantize_per_channel(input, scales, zero_points, axis, dtype) -&gt; Tensor 
<a name="l23521"><span class="ln">23521 </span></a> 
<a name="l23522"><span class="ln">23522 </span></a>    Converts a float tensor to a per-channel quantized tensor with given scales and zero points. 
<a name="l23523"><span class="ln">23523 </span></a> 
<a name="l23524"><span class="ln">23524 </span></a>    Arguments: 
<a name="l23525"><span class="ln">23525 </span></a>        input (Tensor): float tensor to quantize 
<a name="l23526"><span class="ln">23526 </span></a>        scales (Tensor): float 1D tensor of scales to use, size should match ``input.size(axis)`` 
<a name="l23527"><span class="ln">23527 </span></a>        zero_points (int): integer 1D tensor of offset to use, size should match ``input.size(axis)`` 
<a name="l23528"><span class="ln">23528 </span></a>        axis (int): dimension on which apply per-channel quantization 
<a name="l23529"><span class="ln">23529 </span></a>        dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l23530"><span class="ln">23530 </span></a>            Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32`` 
<a name="l23531"><span class="ln">23531 </span></a> 
<a name="l23532"><span class="ln">23532 </span></a>    Returns: 
<a name="l23533"><span class="ln">23533 </span></a>        Tensor: A newly quantized tensor 
<a name="l23534"><span class="ln">23534 </span></a> 
<a name="l23535"><span class="ln">23535 </span></a>    Example:: 
<a name="l23536"><span class="ln">23536 </span></a> 
<a name="l23537"><span class="ln">23537 </span></a>        &gt;&gt;&gt; x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]]) 
<a name="l23538"><span class="ln">23538 </span></a>        &gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8) 
<a name="l23539"><span class="ln">23539 </span></a>        tensor([[-1.,  0.], 
<a name="l23540"><span class="ln">23540 </span></a>                [ 1.,  2.]], size=(2, 2), dtype=torch.quint8, 
<a name="l23541"><span class="ln">23541 </span></a>               quantization_scheme=torch.per_channel_affine, 
<a name="l23542"><span class="ln">23542 </span></a>               scale=tensor([0.1000, 0.0100], dtype=torch.float64), 
<a name="l23543"><span class="ln">23543 </span></a>               zero_point=tensor([10,  0]), axis=0) 
<a name="l23544"><span class="ln">23544 </span></a>        &gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr() 
<a name="l23545"><span class="ln">23545 </span></a>        tensor([[  0,  10], 
<a name="l23546"><span class="ln">23546 </span></a>                [100, 200]], dtype=torch.uint8) 
<a name="l23547"><span class="ln">23547 </span></a>    &quot;&quot;&quot;</span>
<a name="l23548"><span class="ln">23548 </span></a>
<a name="l23549"><span class="ln">23549 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23550"><span class="ln">23550 </span></a><span class="s2">def </span><span class="s1">quantize_per_tensor</span><span class="s3">(</span>
<a name="l23551"><span class="ln">23551 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23552"><span class="ln">23552 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23553"><span class="ln">23553 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23554"><span class="ln">23554 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l23555"><span class="ln">23555 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23556"><span class="ln">23556 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23557"><span class="ln">23557 </span></a>    quantize_per_tensor(input, scale, zero_point, dtype) -&gt; Tensor 
<a name="l23558"><span class="ln">23558 </span></a> 
<a name="l23559"><span class="ln">23559 </span></a>    Converts a float tensor to a quantized tensor with given scale and zero point. 
<a name="l23560"><span class="ln">23560 </span></a> 
<a name="l23561"><span class="ln">23561 </span></a>    Arguments: 
<a name="l23562"><span class="ln">23562 </span></a>        input (Tensor): float tensor or list of tensors to quantize 
<a name="l23563"><span class="ln">23563 </span></a>        scale (float or Tensor): scale to apply in quantization formula 
<a name="l23564"><span class="ln">23564 </span></a>        zero_point (int or Tensor): offset in integer value that maps to float zero 
<a name="l23565"><span class="ln">23565 </span></a>        dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l23566"><span class="ln">23566 </span></a>            Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32`` 
<a name="l23567"><span class="ln">23567 </span></a> 
<a name="l23568"><span class="ln">23568 </span></a>    Returns: 
<a name="l23569"><span class="ln">23569 </span></a>        Tensor: A newly quantized tensor or list of quantized tensors. 
<a name="l23570"><span class="ln">23570 </span></a> 
<a name="l23571"><span class="ln">23571 </span></a>    Example:: 
<a name="l23572"><span class="ln">23572 </span></a> 
<a name="l23573"><span class="ln">23573 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8) 
<a name="l23574"><span class="ln">23574 </span></a>        tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l23575"><span class="ln">23575 </span></a>               quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10) 
<a name="l23576"><span class="ln">23576 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr() 
<a name="l23577"><span class="ln">23577 </span></a>        tensor([ 0, 10, 20, 30], dtype=torch.uint8) 
<a name="l23578"><span class="ln">23578 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor([torch.tensor([-1.0, 0.0]), torch.tensor([-2.0, 2.0])], 
<a name="l23579"><span class="ln">23579 </span></a>        &gt;&gt;&gt; torch.tensor([0.1, 0.2]), torch.tensor([10, 20]), torch.quint8) 
<a name="l23580"><span class="ln">23580 </span></a>        (tensor([-1.,  0.], size=(2,), dtype=torch.quint8, 
<a name="l23581"><span class="ln">23581 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10), 
<a name="l23582"><span class="ln">23582 </span></a>            tensor([-2.,  2.], size=(2,), dtype=torch.quint8, 
<a name="l23583"><span class="ln">23583 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=0.2, zero_point=20)) 
<a name="l23584"><span class="ln">23584 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8) 
<a name="l23585"><span class="ln">23585 </span></a>        tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l23586"><span class="ln">23586 </span></a>           quantization_scheme=torch.per_tensor_affine, scale=0.10, zero_point=10) 
<a name="l23587"><span class="ln">23587 </span></a>    &quot;&quot;&quot;</span>
<a name="l23588"><span class="ln">23588 </span></a>
<a name="l23589"><span class="ln">23589 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23590"><span class="ln">23590 </span></a><span class="s2">def </span><span class="s1">quantize_per_tensor</span><span class="s3">(</span>
<a name="l23591"><span class="ln">23591 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23592"><span class="ln">23592 </span></a>    <span class="s1">scale</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l23593"><span class="ln">23593 </span></a>    <span class="s1">zero_point</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l23594"><span class="ln">23594 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l23595"><span class="ln">23595 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23596"><span class="ln">23596 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23597"><span class="ln">23597 </span></a>    quantize_per_tensor(input, scale, zero_point, dtype) -&gt; Tensor 
<a name="l23598"><span class="ln">23598 </span></a> 
<a name="l23599"><span class="ln">23599 </span></a>    Converts a float tensor to a quantized tensor with given scale and zero point. 
<a name="l23600"><span class="ln">23600 </span></a> 
<a name="l23601"><span class="ln">23601 </span></a>    Arguments: 
<a name="l23602"><span class="ln">23602 </span></a>        input (Tensor): float tensor or list of tensors to quantize 
<a name="l23603"><span class="ln">23603 </span></a>        scale (float or Tensor): scale to apply in quantization formula 
<a name="l23604"><span class="ln">23604 </span></a>        zero_point (int or Tensor): offset in integer value that maps to float zero 
<a name="l23605"><span class="ln">23605 </span></a>        dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l23606"><span class="ln">23606 </span></a>            Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32`` 
<a name="l23607"><span class="ln">23607 </span></a> 
<a name="l23608"><span class="ln">23608 </span></a>    Returns: 
<a name="l23609"><span class="ln">23609 </span></a>        Tensor: A newly quantized tensor or list of quantized tensors. 
<a name="l23610"><span class="ln">23610 </span></a> 
<a name="l23611"><span class="ln">23611 </span></a>    Example:: 
<a name="l23612"><span class="ln">23612 </span></a> 
<a name="l23613"><span class="ln">23613 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8) 
<a name="l23614"><span class="ln">23614 </span></a>        tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l23615"><span class="ln">23615 </span></a>               quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10) 
<a name="l23616"><span class="ln">23616 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr() 
<a name="l23617"><span class="ln">23617 </span></a>        tensor([ 0, 10, 20, 30], dtype=torch.uint8) 
<a name="l23618"><span class="ln">23618 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor([torch.tensor([-1.0, 0.0]), torch.tensor([-2.0, 2.0])], 
<a name="l23619"><span class="ln">23619 </span></a>        &gt;&gt;&gt; torch.tensor([0.1, 0.2]), torch.tensor([10, 20]), torch.quint8) 
<a name="l23620"><span class="ln">23620 </span></a>        (tensor([-1.,  0.], size=(2,), dtype=torch.quint8, 
<a name="l23621"><span class="ln">23621 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10), 
<a name="l23622"><span class="ln">23622 </span></a>            tensor([-2.,  2.], size=(2,), dtype=torch.quint8, 
<a name="l23623"><span class="ln">23623 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=0.2, zero_point=20)) 
<a name="l23624"><span class="ln">23624 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8) 
<a name="l23625"><span class="ln">23625 </span></a>        tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l23626"><span class="ln">23626 </span></a>           quantization_scheme=torch.per_tensor_affine, scale=0.10, zero_point=10) 
<a name="l23627"><span class="ln">23627 </span></a>    &quot;&quot;&quot;</span>
<a name="l23628"><span class="ln">23628 </span></a>
<a name="l23629"><span class="ln">23629 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23630"><span class="ln">23630 </span></a><span class="s2">def </span><span class="s1">quantize_per_tensor</span><span class="s3">(</span>
<a name="l23631"><span class="ln">23631 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23632"><span class="ln">23632 </span></a>    <span class="s1">scales</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23633"><span class="ln">23633 </span></a>    <span class="s1">zero_points</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23634"><span class="ln">23634 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l23635"><span class="ln">23635 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l23636"><span class="ln">23636 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23637"><span class="ln">23637 </span></a>    quantize_per_tensor(input, scale, zero_point, dtype) -&gt; Tensor 
<a name="l23638"><span class="ln">23638 </span></a> 
<a name="l23639"><span class="ln">23639 </span></a>    Converts a float tensor to a quantized tensor with given scale and zero point. 
<a name="l23640"><span class="ln">23640 </span></a> 
<a name="l23641"><span class="ln">23641 </span></a>    Arguments: 
<a name="l23642"><span class="ln">23642 </span></a>        input (Tensor): float tensor or list of tensors to quantize 
<a name="l23643"><span class="ln">23643 </span></a>        scale (float or Tensor): scale to apply in quantization formula 
<a name="l23644"><span class="ln">23644 </span></a>        zero_point (int or Tensor): offset in integer value that maps to float zero 
<a name="l23645"><span class="ln">23645 </span></a>        dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l23646"><span class="ln">23646 </span></a>            Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32`` 
<a name="l23647"><span class="ln">23647 </span></a> 
<a name="l23648"><span class="ln">23648 </span></a>    Returns: 
<a name="l23649"><span class="ln">23649 </span></a>        Tensor: A newly quantized tensor or list of quantized tensors. 
<a name="l23650"><span class="ln">23650 </span></a> 
<a name="l23651"><span class="ln">23651 </span></a>    Example:: 
<a name="l23652"><span class="ln">23652 </span></a> 
<a name="l23653"><span class="ln">23653 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8) 
<a name="l23654"><span class="ln">23654 </span></a>        tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l23655"><span class="ln">23655 </span></a>               quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10) 
<a name="l23656"><span class="ln">23656 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr() 
<a name="l23657"><span class="ln">23657 </span></a>        tensor([ 0, 10, 20, 30], dtype=torch.uint8) 
<a name="l23658"><span class="ln">23658 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor([torch.tensor([-1.0, 0.0]), torch.tensor([-2.0, 2.0])], 
<a name="l23659"><span class="ln">23659 </span></a>        &gt;&gt;&gt; torch.tensor([0.1, 0.2]), torch.tensor([10, 20]), torch.quint8) 
<a name="l23660"><span class="ln">23660 </span></a>        (tensor([-1.,  0.], size=(2,), dtype=torch.quint8, 
<a name="l23661"><span class="ln">23661 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10), 
<a name="l23662"><span class="ln">23662 </span></a>            tensor([-2.,  2.], size=(2,), dtype=torch.quint8, 
<a name="l23663"><span class="ln">23663 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=0.2, zero_point=20)) 
<a name="l23664"><span class="ln">23664 </span></a>        &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8) 
<a name="l23665"><span class="ln">23665 </span></a>        tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l23666"><span class="ln">23666 </span></a>           quantization_scheme=torch.per_tensor_affine, scale=0.10, zero_point=10) 
<a name="l23667"><span class="ln">23667 </span></a>    &quot;&quot;&quot;</span>
<a name="l23668"><span class="ln">23668 </span></a>
<a name="l23669"><span class="ln">23669 </span></a><span class="s2">def </span><span class="s1">quantize_per_tensor_dynamic</span><span class="s3">(</span>
<a name="l23670"><span class="ln">23670 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23671"><span class="ln">23671 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l23672"><span class="ln">23672 </span></a>    <span class="s1">reduce_range</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l23673"><span class="ln">23673 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23674"><span class="ln">23674 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23675"><span class="ln">23675 </span></a>    quantize_per_tensor_dynamic(input, dtype, reduce_range) -&gt; Tensor 
<a name="l23676"><span class="ln">23676 </span></a> 
<a name="l23677"><span class="ln">23677 </span></a>    Converts a float tensor to a quantized tensor with scale and zero_point calculated 
<a name="l23678"><span class="ln">23678 </span></a>    dynamically based on the input. 
<a name="l23679"><span class="ln">23679 </span></a> 
<a name="l23680"><span class="ln">23680 </span></a>    Arguments: 
<a name="l23681"><span class="ln">23681 </span></a>        input (Tensor): float tensor or list of tensors to quantize 
<a name="l23682"><span class="ln">23682 </span></a>        dtype (:class:`torch.dtype`): the desired data type of returned tensor. 
<a name="l23683"><span class="ln">23683 </span></a>            Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8`` 
<a name="l23684"><span class="ln">23684 </span></a>        reduce_range (bool): a flag to indicate whether to reduce the range of quantized 
<a name="l23685"><span class="ln">23685 </span></a>        data by 1 bit, it's required to avoid instruction overflow for some hardwares 
<a name="l23686"><span class="ln">23686 </span></a> 
<a name="l23687"><span class="ln">23687 </span></a>    Returns: 
<a name="l23688"><span class="ln">23688 </span></a>        Tensor: A newly (dynamically) quantized tensor 
<a name="l23689"><span class="ln">23689 </span></a> 
<a name="l23690"><span class="ln">23690 </span></a>    Example:: 
<a name="l23691"><span class="ln">23691 </span></a> 
<a name="l23692"><span class="ln">23692 </span></a>        &gt;&gt;&gt; t = torch.quantize_per_tensor_dynamic(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.quint8, False) 
<a name="l23693"><span class="ln">23693 </span></a>        &gt;&gt;&gt; print(t) 
<a name="l23694"><span class="ln">23694 </span></a>        tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8, 
<a name="l23695"><span class="ln">23695 </span></a>               quantization_scheme=torch.per_tensor_affine, scale=0.011764705882352941, 
<a name="l23696"><span class="ln">23696 </span></a>               zero_point=85) 
<a name="l23697"><span class="ln">23697 </span></a>        &gt;&gt;&gt; t.int_repr() 
<a name="l23698"><span class="ln">23698 </span></a>        tensor([  0,  85, 170, 255], dtype=torch.uint8) 
<a name="l23699"><span class="ln">23699 </span></a>    &quot;&quot;&quot;</span>
<a name="l23700"><span class="ln">23700 </span></a>
<a name="l23701"><span class="ln">23701 </span></a><span class="s2">def </span><span class="s1">quantized_batch_norm</span><span class="s3">(</span>
<a name="l23702"><span class="ln">23702 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23703"><span class="ln">23703 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23704"><span class="ln">23704 </span></a>    <span class="s1">bias</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23705"><span class="ln">23705 </span></a>    <span class="s1">mean</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23706"><span class="ln">23706 </span></a>    <span class="s1">var</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23707"><span class="ln">23707 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l23708"><span class="ln">23708 </span></a>    <span class="s1">output_scale</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l23709"><span class="ln">23709 </span></a>    <span class="s1">output_zero_point</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l23710"><span class="ln">23710 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23711"><span class="ln">23711 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23712"><span class="ln">23712 </span></a>    quantized_batch_norm(input, weight=None, bias=None, mean, var, eps, output_scale, output_zero_point) -&gt; Tensor 
<a name="l23713"><span class="ln">23713 </span></a> 
<a name="l23714"><span class="ln">23714 </span></a>    Applies batch normalization on a 4D (NCHW) quantized tensor. 
<a name="l23715"><span class="ln">23715 </span></a> 
<a name="l23716"><span class="ln">23716 </span></a>    .. math:: 
<a name="l23717"><span class="ln">23717 </span></a> 
<a name="l23718"><span class="ln">23718 </span></a>            y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta 
<a name="l23719"><span class="ln">23719 </span></a> 
<a name="l23720"><span class="ln">23720 </span></a>    Arguments: 
<a name="l23721"><span class="ln">23721 </span></a>        input (Tensor): quantized tensor 
<a name="l23722"><span class="ln">23722 </span></a>        weight (Tensor): float tensor that corresponds to the gamma, size C 
<a name="l23723"><span class="ln">23723 </span></a>        bias (Tensor):  float tensor that corresponds to the beta, size C 
<a name="l23724"><span class="ln">23724 </span></a>        mean (Tensor): float mean value in batch normalization, size C 
<a name="l23725"><span class="ln">23725 </span></a>        var (Tensor): float tensor for variance, size C 
<a name="l23726"><span class="ln">23726 </span></a>        eps (float): a value added to the denominator for numerical stability. 
<a name="l23727"><span class="ln">23727 </span></a>        output_scale (float): output quantized tensor scale 
<a name="l23728"><span class="ln">23728 </span></a>        output_zero_point (int): output quantized tensor zero_point 
<a name="l23729"><span class="ln">23729 </span></a> 
<a name="l23730"><span class="ln">23730 </span></a>    Returns: 
<a name="l23731"><span class="ln">23731 </span></a>        Tensor: A quantized tensor with batch normalization applied. 
<a name="l23732"><span class="ln">23732 </span></a> 
<a name="l23733"><span class="ln">23733 </span></a>    Example:: 
<a name="l23734"><span class="ln">23734 </span></a> 
<a name="l23735"><span class="ln">23735 </span></a>        &gt;&gt;&gt; qx = torch.quantize_per_tensor(torch.rand(2, 2, 2, 2), 1.5, 3, torch.quint8) 
<a name="l23736"><span class="ln">23736 </span></a>        &gt;&gt;&gt; torch.quantized_batch_norm(qx, torch.ones(2), torch.zeros(2), torch.rand(2), torch.rand(2), 0.00001, 0.2, 2) 
<a name="l23737"><span class="ln">23737 </span></a>        tensor([[[[-0.2000, -0.2000], 
<a name="l23738"><span class="ln">23738 </span></a>              [ 1.6000, -0.2000]], 
<a name="l23739"><span class="ln">23739 </span></a> 
<a name="l23740"><span class="ln">23740 </span></a>             [[-0.4000, -0.4000], 
<a name="l23741"><span class="ln">23741 </span></a>              [-0.4000,  0.6000]]], 
<a name="l23742"><span class="ln">23742 </span></a> 
<a name="l23743"><span class="ln">23743 </span></a> 
<a name="l23744"><span class="ln">23744 </span></a>            [[[-0.2000, -0.2000], 
<a name="l23745"><span class="ln">23745 </span></a>              [-0.2000, -0.2000]], 
<a name="l23746"><span class="ln">23746 </span></a> 
<a name="l23747"><span class="ln">23747 </span></a>             [[ 0.6000, -0.4000], 
<a name="l23748"><span class="ln">23748 </span></a>              [ 0.6000, -0.4000]]]], size=(2, 2, 2, 2), dtype=torch.quint8, 
<a name="l23749"><span class="ln">23749 </span></a>           quantization_scheme=torch.per_tensor_affine, scale=0.2, zero_point=2) 
<a name="l23750"><span class="ln">23750 </span></a>    &quot;&quot;&quot;</span>
<a name="l23751"><span class="ln">23751 </span></a>
<a name="l23752"><span class="ln">23752 </span></a><span class="s2">def </span><span class="s1">quantized_gru_cell</span><span class="s3">(</span>
<a name="l23753"><span class="ln">23753 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23754"><span class="ln">23754 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23755"><span class="ln">23755 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23756"><span class="ln">23756 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23757"><span class="ln">23757 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23758"><span class="ln">23758 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23759"><span class="ln">23759 </span></a>    <span class="s1">packed_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23760"><span class="ln">23760 </span></a>    <span class="s1">packed_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23761"><span class="ln">23761 </span></a>    <span class="s1">col_offsets_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23762"><span class="ln">23762 </span></a>    <span class="s1">col_offsets_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23763"><span class="ln">23763 </span></a>    <span class="s1">scale_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23764"><span class="ln">23764 </span></a>    <span class="s1">scale_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23765"><span class="ln">23765 </span></a>    <span class="s1">zero_point_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23766"><span class="ln">23766 </span></a>    <span class="s1">zero_point_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23767"><span class="ln">23767 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23768"><span class="ln">23768 </span></a><span class="s2">def </span><span class="s1">quantized_lstm_cell</span><span class="s3">(</span>
<a name="l23769"><span class="ln">23769 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23770"><span class="ln">23770 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23771"><span class="ln">23771 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23772"><span class="ln">23772 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23773"><span class="ln">23773 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23774"><span class="ln">23774 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23775"><span class="ln">23775 </span></a>    <span class="s1">packed_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23776"><span class="ln">23776 </span></a>    <span class="s1">packed_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23777"><span class="ln">23777 </span></a>    <span class="s1">col_offsets_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23778"><span class="ln">23778 </span></a>    <span class="s1">col_offsets_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23779"><span class="ln">23779 </span></a>    <span class="s1">scale_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23780"><span class="ln">23780 </span></a>    <span class="s1">scale_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23781"><span class="ln">23781 </span></a>    <span class="s1">zero_point_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23782"><span class="ln">23782 </span></a>    <span class="s1">zero_point_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23783"><span class="ln">23783 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23784"><span class="ln">23784 </span></a><span class="s2">def </span><span class="s1">quantized_max_pool1d</span><span class="s3">(</span>
<a name="l23785"><span class="ln">23785 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23786"><span class="ln">23786 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l23787"><span class="ln">23787 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l23788"><span class="ln">23788 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l23789"><span class="ln">23789 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l23790"><span class="ln">23790 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23791"><span class="ln">23791 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23792"><span class="ln">23792 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23793"><span class="ln">23793 </span></a>    quantized_max_pool1d(input, kernel_size, stride=[], padding=0, dilation=1, ceil_mode=False) -&gt; Tensor 
<a name="l23794"><span class="ln">23794 </span></a> 
<a name="l23795"><span class="ln">23795 </span></a>    Applies a 1D max pooling over an input quantized tensor composed of several input planes. 
<a name="l23796"><span class="ln">23796 </span></a> 
<a name="l23797"><span class="ln">23797 </span></a>    Arguments: 
<a name="l23798"><span class="ln">23798 </span></a>        input (Tensor): quantized tensor 
<a name="l23799"><span class="ln">23799 </span></a>        kernel_size (list of int): the size of the sliding window 
<a name="l23800"><span class="ln">23800 </span></a>        stride (``list of int``, optional): the stride of the sliding window 
<a name="l23801"><span class="ln">23801 </span></a>        padding (``list of int``, optional): padding to be added on both sides, must be &gt;= 0 and &lt;= kernel_size / 2 
<a name="l23802"><span class="ln">23802 </span></a>        dilation (``list of int``, optional): The stride between elements within a sliding window, must be &gt; 0. Default 1 
<a name="l23803"><span class="ln">23803 </span></a>        ceil_mode (bool, optional):  If True, will use ceil instead of floor to compute the output shape. 
<a name="l23804"><span class="ln">23804 </span></a>            Defaults to False. 
<a name="l23805"><span class="ln">23805 </span></a> 
<a name="l23806"><span class="ln">23806 </span></a> 
<a name="l23807"><span class="ln">23807 </span></a>    Returns: 
<a name="l23808"><span class="ln">23808 </span></a>        Tensor: A quantized tensor with max_pool1d applied. 
<a name="l23809"><span class="ln">23809 </span></a> 
<a name="l23810"><span class="ln">23810 </span></a>    Example:: 
<a name="l23811"><span class="ln">23811 </span></a> 
<a name="l23812"><span class="ln">23812 </span></a>        &gt;&gt;&gt; qx = torch.quantize_per_tensor(torch.rand(2, 2), 1.5, 3, torch.quint8) 
<a name="l23813"><span class="ln">23813 </span></a>        &gt;&gt;&gt; torch.quantized_max_pool1d(qx, [2]) 
<a name="l23814"><span class="ln">23814 </span></a>        tensor([[0.0000], 
<a name="l23815"><span class="ln">23815 </span></a>                [1.5000]], size=(2, 1), dtype=torch.quint8, 
<a name="l23816"><span class="ln">23816 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=1.5, zero_point=3) 
<a name="l23817"><span class="ln">23817 </span></a>    &quot;&quot;&quot;</span>
<a name="l23818"><span class="ln">23818 </span></a>
<a name="l23819"><span class="ln">23819 </span></a><span class="s2">def </span><span class="s1">quantized_max_pool2d</span><span class="s3">(</span>
<a name="l23820"><span class="ln">23820 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23821"><span class="ln">23821 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l23822"><span class="ln">23822 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l23823"><span class="ln">23823 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l23824"><span class="ln">23824 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l23825"><span class="ln">23825 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23826"><span class="ln">23826 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23827"><span class="ln">23827 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23828"><span class="ln">23828 </span></a>    quantized_max_pool2d(input, kernel_size, stride=[], padding=0, dilation=1, ceil_mode=False) -&gt; Tensor 
<a name="l23829"><span class="ln">23829 </span></a> 
<a name="l23830"><span class="ln">23830 </span></a>    Applies a 2D max pooling over an input quantized tensor composed of several input planes. 
<a name="l23831"><span class="ln">23831 </span></a> 
<a name="l23832"><span class="ln">23832 </span></a>    Arguments: 
<a name="l23833"><span class="ln">23833 </span></a>        input (Tensor): quantized tensor 
<a name="l23834"><span class="ln">23834 </span></a>        kernel_size (``list of int``): the size of the sliding window 
<a name="l23835"><span class="ln">23835 </span></a>        stride (``list of int``, optional): the stride of the sliding window 
<a name="l23836"><span class="ln">23836 </span></a>        padding (``list of int``, optional): padding to be added on both sides, must be &gt;= 0 and &lt;= kernel_size / 2 
<a name="l23837"><span class="ln">23837 </span></a>        dilation (``list of int``, optional): The stride between elements within a sliding window, must be &gt; 0. Default 1 
<a name="l23838"><span class="ln">23838 </span></a>        ceil_mode (bool, optional):  If True, will use ceil instead of floor to compute the output shape. 
<a name="l23839"><span class="ln">23839 </span></a>            Defaults to False. 
<a name="l23840"><span class="ln">23840 </span></a> 
<a name="l23841"><span class="ln">23841 </span></a> 
<a name="l23842"><span class="ln">23842 </span></a>    Returns: 
<a name="l23843"><span class="ln">23843 </span></a>        Tensor: A quantized tensor with max_pool2d applied. 
<a name="l23844"><span class="ln">23844 </span></a> 
<a name="l23845"><span class="ln">23845 </span></a>    Example:: 
<a name="l23846"><span class="ln">23846 </span></a> 
<a name="l23847"><span class="ln">23847 </span></a>        &gt;&gt;&gt; qx = torch.quantize_per_tensor(torch.rand(2, 2, 2, 2), 1.5, 3, torch.quint8) 
<a name="l23848"><span class="ln">23848 </span></a>        &gt;&gt;&gt; torch.quantized_max_pool2d(qx, [2,2]) 
<a name="l23849"><span class="ln">23849 </span></a>        tensor([[[[1.5000]], 
<a name="l23850"><span class="ln">23850 </span></a> 
<a name="l23851"><span class="ln">23851 </span></a>                [[1.5000]]], 
<a name="l23852"><span class="ln">23852 </span></a> 
<a name="l23853"><span class="ln">23853 </span></a> 
<a name="l23854"><span class="ln">23854 </span></a>                [[[0.0000]], 
<a name="l23855"><span class="ln">23855 </span></a> 
<a name="l23856"><span class="ln">23856 </span></a>                [[0.0000]]]], size=(2, 2, 1, 1), dtype=torch.quint8, 
<a name="l23857"><span class="ln">23857 </span></a>            quantization_scheme=torch.per_tensor_affine, scale=1.5, zero_point=3) 
<a name="l23858"><span class="ln">23858 </span></a>    &quot;&quot;&quot;</span>
<a name="l23859"><span class="ln">23859 </span></a>
<a name="l23860"><span class="ln">23860 </span></a><span class="s2">def </span><span class="s1">quantized_max_pool3d</span><span class="s3">(</span>
<a name="l23861"><span class="ln">23861 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23862"><span class="ln">23862 </span></a>    <span class="s1">kernel_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l23863"><span class="ln">23863 </span></a>    <span class="s1">stride</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l23864"><span class="ln">23864 </span></a>    <span class="s1">padding</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l23865"><span class="ln">23865 </span></a>    <span class="s1">dilation</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l23866"><span class="ln">23866 </span></a>    <span class="s1">ceil_mode</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l23867"><span class="ln">23867 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23868"><span class="ln">23868 </span></a><span class="s2">def </span><span class="s1">quantized_rnn_relu_cell</span><span class="s3">(</span>
<a name="l23869"><span class="ln">23869 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23870"><span class="ln">23870 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23871"><span class="ln">23871 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23872"><span class="ln">23872 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23873"><span class="ln">23873 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23874"><span class="ln">23874 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23875"><span class="ln">23875 </span></a>    <span class="s1">packed_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23876"><span class="ln">23876 </span></a>    <span class="s1">packed_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23877"><span class="ln">23877 </span></a>    <span class="s1">col_offsets_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23878"><span class="ln">23878 </span></a>    <span class="s1">col_offsets_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23879"><span class="ln">23879 </span></a>    <span class="s1">scale_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23880"><span class="ln">23880 </span></a>    <span class="s1">scale_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23881"><span class="ln">23881 </span></a>    <span class="s1">zero_point_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23882"><span class="ln">23882 </span></a>    <span class="s1">zero_point_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23883"><span class="ln">23883 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23884"><span class="ln">23884 </span></a><span class="s2">def </span><span class="s1">quantized_rnn_tanh_cell</span><span class="s3">(</span>
<a name="l23885"><span class="ln">23885 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23886"><span class="ln">23886 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23887"><span class="ln">23887 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23888"><span class="ln">23888 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23889"><span class="ln">23889 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23890"><span class="ln">23890 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23891"><span class="ln">23891 </span></a>    <span class="s1">packed_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23892"><span class="ln">23892 </span></a>    <span class="s1">packed_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23893"><span class="ln">23893 </span></a>    <span class="s1">col_offsets_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23894"><span class="ln">23894 </span></a>    <span class="s1">col_offsets_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l23895"><span class="ln">23895 </span></a>    <span class="s1">scale_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23896"><span class="ln">23896 </span></a>    <span class="s1">scale_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23897"><span class="ln">23897 </span></a>    <span class="s1">zero_point_ih</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23898"><span class="ln">23898 </span></a>    <span class="s1">zero_point_hh</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l23899"><span class="ln">23899 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23900"><span class="ln">23900 </span></a><span class="s2">def </span><span class="s1">rad2deg</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23901"><span class="ln">23901 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23902"><span class="ln">23902 </span></a>    rad2deg(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l23903"><span class="ln">23903 </span></a> 
<a name="l23904"><span class="ln">23904 </span></a>    Returns a new tensor with each of the elements of :attr:`input` 
<a name="l23905"><span class="ln">23905 </span></a>    converted from angles in radians to degrees. 
<a name="l23906"><span class="ln">23906 </span></a> 
<a name="l23907"><span class="ln">23907 </span></a>    Args: 
<a name="l23908"><span class="ln">23908 </span></a>        input (Tensor): the input tensor. 
<a name="l23909"><span class="ln">23909 </span></a> 
<a name="l23910"><span class="ln">23910 </span></a>    Keyword arguments: 
<a name="l23911"><span class="ln">23911 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l23912"><span class="ln">23912 </span></a> 
<a name="l23913"><span class="ln">23913 </span></a>    Example:: 
<a name="l23914"><span class="ln">23914 </span></a> 
<a name="l23915"><span class="ln">23915 </span></a>        &gt;&gt;&gt; a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]]) 
<a name="l23916"><span class="ln">23916 </span></a>        &gt;&gt;&gt; torch.rad2deg(a) 
<a name="l23917"><span class="ln">23917 </span></a>        tensor([[ 180.0233, -180.0233], 
<a name="l23918"><span class="ln">23918 </span></a>                [ 359.9894, -359.9894], 
<a name="l23919"><span class="ln">23919 </span></a>                [  89.9544,  -89.9544]]) 
<a name="l23920"><span class="ln">23920 </span></a>    &quot;&quot;&quot;</span>
<a name="l23921"><span class="ln">23921 </span></a>
<a name="l23922"><span class="ln">23922 </span></a><span class="s2">def </span><span class="s1">rad2deg_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l23923"><span class="ln">23923 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23924"><span class="ln">23924 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l23925"><span class="ln">23925 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l23926"><span class="ln">23926 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l23927"><span class="ln">23927 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23928"><span class="ln">23928 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23929"><span class="ln">23929 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23930"><span class="ln">23930 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23931"><span class="ln">23931 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23932"><span class="ln">23932 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l23933"><span class="ln">23933 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l23934"><span class="ln">23934 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23935"><span class="ln">23935 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23936"><span class="ln">23936 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l23937"><span class="ln">23937 </span></a> 
<a name="l23938"><span class="ln">23938 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l23939"><span class="ln">23939 </span></a>    on the interval :math:`[0, 1)` 
<a name="l23940"><span class="ln">23940 </span></a> 
<a name="l23941"><span class="ln">23941 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l23942"><span class="ln">23942 </span></a> 
<a name="l23943"><span class="ln">23943 </span></a>    Args: 
<a name="l23944"><span class="ln">23944 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l23945"><span class="ln">23945 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l23946"><span class="ln">23946 </span></a> 
<a name="l23947"><span class="ln">23947 </span></a>    Keyword args: 
<a name="l23948"><span class="ln">23948 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l23949"><span class="ln">23949 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l23950"><span class="ln">23950 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23951"><span class="ln">23951 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l23952"><span class="ln">23952 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l23953"><span class="ln">23953 </span></a>            Default: ``torch.strided``. 
<a name="l23954"><span class="ln">23954 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l23955"><span class="ln">23955 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l23956"><span class="ln">23956 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l23957"><span class="ln">23957 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l23958"><span class="ln">23958 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l23959"><span class="ln">23959 </span></a>            returned tensor. Default: ``False``. 
<a name="l23960"><span class="ln">23960 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l23961"><span class="ln">23961 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l23962"><span class="ln">23962 </span></a> 
<a name="l23963"><span class="ln">23963 </span></a>    Example:: 
<a name="l23964"><span class="ln">23964 </span></a> 
<a name="l23965"><span class="ln">23965 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l23966"><span class="ln">23966 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l23967"><span class="ln">23967 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l23968"><span class="ln">23968 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l23969"><span class="ln">23969 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l23970"><span class="ln">23970 </span></a>    &quot;&quot;&quot;</span>
<a name="l23971"><span class="ln">23971 </span></a>
<a name="l23972"><span class="ln">23972 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l23973"><span class="ln">23973 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l23974"><span class="ln">23974 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l23975"><span class="ln">23975 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23976"><span class="ln">23976 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l23977"><span class="ln">23977 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23978"><span class="ln">23978 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23979"><span class="ln">23979 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l23980"><span class="ln">23980 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l23981"><span class="ln">23981 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l23982"><span class="ln">23982 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l23983"><span class="ln">23983 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l23984"><span class="ln">23984 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l23985"><span class="ln">23985 </span></a> 
<a name="l23986"><span class="ln">23986 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l23987"><span class="ln">23987 </span></a>    on the interval :math:`[0, 1)` 
<a name="l23988"><span class="ln">23988 </span></a> 
<a name="l23989"><span class="ln">23989 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l23990"><span class="ln">23990 </span></a> 
<a name="l23991"><span class="ln">23991 </span></a>    Args: 
<a name="l23992"><span class="ln">23992 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l23993"><span class="ln">23993 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l23994"><span class="ln">23994 </span></a> 
<a name="l23995"><span class="ln">23995 </span></a>    Keyword args: 
<a name="l23996"><span class="ln">23996 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l23997"><span class="ln">23997 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l23998"><span class="ln">23998 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l23999"><span class="ln">23999 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24000"><span class="ln">24000 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24001"><span class="ln">24001 </span></a>            Default: ``torch.strided``. 
<a name="l24002"><span class="ln">24002 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24003"><span class="ln">24003 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24004"><span class="ln">24004 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24005"><span class="ln">24005 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24006"><span class="ln">24006 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24007"><span class="ln">24007 </span></a>            returned tensor. Default: ``False``. 
<a name="l24008"><span class="ln">24008 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24009"><span class="ln">24009 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24010"><span class="ln">24010 </span></a> 
<a name="l24011"><span class="ln">24011 </span></a>    Example:: 
<a name="l24012"><span class="ln">24012 </span></a> 
<a name="l24013"><span class="ln">24013 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l24014"><span class="ln">24014 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l24015"><span class="ln">24015 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l24016"><span class="ln">24016 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l24017"><span class="ln">24017 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l24018"><span class="ln">24018 </span></a>    &quot;&quot;&quot;</span>
<a name="l24019"><span class="ln">24019 </span></a>
<a name="l24020"><span class="ln">24020 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24021"><span class="ln">24021 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l24022"><span class="ln">24022 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24023"><span class="ln">24023 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24024"><span class="ln">24024 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24025"><span class="ln">24025 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24026"><span class="ln">24026 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24027"><span class="ln">24027 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24028"><span class="ln">24028 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24029"><span class="ln">24029 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24030"><span class="ln">24030 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24031"><span class="ln">24031 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24032"><span class="ln">24032 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24033"><span class="ln">24033 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24034"><span class="ln">24034 </span></a> 
<a name="l24035"><span class="ln">24035 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l24036"><span class="ln">24036 </span></a>    on the interval :math:`[0, 1)` 
<a name="l24037"><span class="ln">24037 </span></a> 
<a name="l24038"><span class="ln">24038 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24039"><span class="ln">24039 </span></a> 
<a name="l24040"><span class="ln">24040 </span></a>    Args: 
<a name="l24041"><span class="ln">24041 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24042"><span class="ln">24042 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24043"><span class="ln">24043 </span></a> 
<a name="l24044"><span class="ln">24044 </span></a>    Keyword args: 
<a name="l24045"><span class="ln">24045 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24046"><span class="ln">24046 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24047"><span class="ln">24047 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24048"><span class="ln">24048 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24049"><span class="ln">24049 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24050"><span class="ln">24050 </span></a>            Default: ``torch.strided``. 
<a name="l24051"><span class="ln">24051 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24052"><span class="ln">24052 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24053"><span class="ln">24053 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24054"><span class="ln">24054 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24055"><span class="ln">24055 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24056"><span class="ln">24056 </span></a>            returned tensor. Default: ``False``. 
<a name="l24057"><span class="ln">24057 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24058"><span class="ln">24058 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24059"><span class="ln">24059 </span></a> 
<a name="l24060"><span class="ln">24060 </span></a>    Example:: 
<a name="l24061"><span class="ln">24061 </span></a> 
<a name="l24062"><span class="ln">24062 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l24063"><span class="ln">24063 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l24064"><span class="ln">24064 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l24065"><span class="ln">24065 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l24066"><span class="ln">24066 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l24067"><span class="ln">24067 </span></a>    &quot;&quot;&quot;</span>
<a name="l24068"><span class="ln">24068 </span></a>
<a name="l24069"><span class="ln">24069 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24070"><span class="ln">24070 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l24071"><span class="ln">24071 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24072"><span class="ln">24072 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24073"><span class="ln">24073 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24074"><span class="ln">24074 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24075"><span class="ln">24075 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24076"><span class="ln">24076 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24077"><span class="ln">24077 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24078"><span class="ln">24078 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24079"><span class="ln">24079 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24080"><span class="ln">24080 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24081"><span class="ln">24081 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24082"><span class="ln">24082 </span></a> 
<a name="l24083"><span class="ln">24083 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l24084"><span class="ln">24084 </span></a>    on the interval :math:`[0, 1)` 
<a name="l24085"><span class="ln">24085 </span></a> 
<a name="l24086"><span class="ln">24086 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24087"><span class="ln">24087 </span></a> 
<a name="l24088"><span class="ln">24088 </span></a>    Args: 
<a name="l24089"><span class="ln">24089 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24090"><span class="ln">24090 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24091"><span class="ln">24091 </span></a> 
<a name="l24092"><span class="ln">24092 </span></a>    Keyword args: 
<a name="l24093"><span class="ln">24093 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24094"><span class="ln">24094 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24095"><span class="ln">24095 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24096"><span class="ln">24096 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24097"><span class="ln">24097 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24098"><span class="ln">24098 </span></a>            Default: ``torch.strided``. 
<a name="l24099"><span class="ln">24099 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24100"><span class="ln">24100 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24101"><span class="ln">24101 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24102"><span class="ln">24102 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24103"><span class="ln">24103 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24104"><span class="ln">24104 </span></a>            returned tensor. Default: ``False``. 
<a name="l24105"><span class="ln">24105 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24106"><span class="ln">24106 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24107"><span class="ln">24107 </span></a> 
<a name="l24108"><span class="ln">24108 </span></a>    Example:: 
<a name="l24109"><span class="ln">24109 </span></a> 
<a name="l24110"><span class="ln">24110 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l24111"><span class="ln">24111 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l24112"><span class="ln">24112 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l24113"><span class="ln">24113 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l24114"><span class="ln">24114 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l24115"><span class="ln">24115 </span></a>    &quot;&quot;&quot;</span>
<a name="l24116"><span class="ln">24116 </span></a>
<a name="l24117"><span class="ln">24117 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24118"><span class="ln">24118 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l24119"><span class="ln">24119 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24120"><span class="ln">24120 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24121"><span class="ln">24121 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24122"><span class="ln">24122 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24123"><span class="ln">24123 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24124"><span class="ln">24124 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24125"><span class="ln">24125 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24126"><span class="ln">24126 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24127"><span class="ln">24127 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24128"><span class="ln">24128 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24129"><span class="ln">24129 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24130"><span class="ln">24130 </span></a> 
<a name="l24131"><span class="ln">24131 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l24132"><span class="ln">24132 </span></a>    on the interval :math:`[0, 1)` 
<a name="l24133"><span class="ln">24133 </span></a> 
<a name="l24134"><span class="ln">24134 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24135"><span class="ln">24135 </span></a> 
<a name="l24136"><span class="ln">24136 </span></a>    Args: 
<a name="l24137"><span class="ln">24137 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24138"><span class="ln">24138 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24139"><span class="ln">24139 </span></a> 
<a name="l24140"><span class="ln">24140 </span></a>    Keyword args: 
<a name="l24141"><span class="ln">24141 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24142"><span class="ln">24142 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24143"><span class="ln">24143 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24144"><span class="ln">24144 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24145"><span class="ln">24145 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24146"><span class="ln">24146 </span></a>            Default: ``torch.strided``. 
<a name="l24147"><span class="ln">24147 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24148"><span class="ln">24148 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24149"><span class="ln">24149 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24150"><span class="ln">24150 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24151"><span class="ln">24151 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24152"><span class="ln">24152 </span></a>            returned tensor. Default: ``False``. 
<a name="l24153"><span class="ln">24153 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24154"><span class="ln">24154 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24155"><span class="ln">24155 </span></a> 
<a name="l24156"><span class="ln">24156 </span></a>    Example:: 
<a name="l24157"><span class="ln">24157 </span></a> 
<a name="l24158"><span class="ln">24158 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l24159"><span class="ln">24159 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l24160"><span class="ln">24160 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l24161"><span class="ln">24161 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l24162"><span class="ln">24162 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l24163"><span class="ln">24163 </span></a>    &quot;&quot;&quot;</span>
<a name="l24164"><span class="ln">24164 </span></a>
<a name="l24165"><span class="ln">24165 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24166"><span class="ln">24166 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l24167"><span class="ln">24167 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24168"><span class="ln">24168 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24169"><span class="ln">24169 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24170"><span class="ln">24170 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24171"><span class="ln">24171 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24172"><span class="ln">24172 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24173"><span class="ln">24173 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24174"><span class="ln">24174 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24175"><span class="ln">24175 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24176"><span class="ln">24176 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24177"><span class="ln">24177 </span></a> 
<a name="l24178"><span class="ln">24178 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l24179"><span class="ln">24179 </span></a>    on the interval :math:`[0, 1)` 
<a name="l24180"><span class="ln">24180 </span></a> 
<a name="l24181"><span class="ln">24181 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24182"><span class="ln">24182 </span></a> 
<a name="l24183"><span class="ln">24183 </span></a>    Args: 
<a name="l24184"><span class="ln">24184 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24185"><span class="ln">24185 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24186"><span class="ln">24186 </span></a> 
<a name="l24187"><span class="ln">24187 </span></a>    Keyword args: 
<a name="l24188"><span class="ln">24188 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24189"><span class="ln">24189 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24190"><span class="ln">24190 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24191"><span class="ln">24191 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24192"><span class="ln">24192 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24193"><span class="ln">24193 </span></a>            Default: ``torch.strided``. 
<a name="l24194"><span class="ln">24194 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24195"><span class="ln">24195 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24196"><span class="ln">24196 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24197"><span class="ln">24197 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24198"><span class="ln">24198 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24199"><span class="ln">24199 </span></a>            returned tensor. Default: ``False``. 
<a name="l24200"><span class="ln">24200 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24201"><span class="ln">24201 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24202"><span class="ln">24202 </span></a> 
<a name="l24203"><span class="ln">24203 </span></a>    Example:: 
<a name="l24204"><span class="ln">24204 </span></a> 
<a name="l24205"><span class="ln">24205 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l24206"><span class="ln">24206 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l24207"><span class="ln">24207 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l24208"><span class="ln">24208 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l24209"><span class="ln">24209 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l24210"><span class="ln">24210 </span></a>    &quot;&quot;&quot;</span>
<a name="l24211"><span class="ln">24211 </span></a>
<a name="l24212"><span class="ln">24212 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24213"><span class="ln">24213 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l24214"><span class="ln">24214 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24215"><span class="ln">24215 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24216"><span class="ln">24216 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24217"><span class="ln">24217 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24218"><span class="ln">24218 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24219"><span class="ln">24219 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24220"><span class="ln">24220 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24221"><span class="ln">24221 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24222"><span class="ln">24222 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24223"><span class="ln">24223 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24224"><span class="ln">24224 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24225"><span class="ln">24225 </span></a> 
<a name="l24226"><span class="ln">24226 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l24227"><span class="ln">24227 </span></a>    on the interval :math:`[0, 1)` 
<a name="l24228"><span class="ln">24228 </span></a> 
<a name="l24229"><span class="ln">24229 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24230"><span class="ln">24230 </span></a> 
<a name="l24231"><span class="ln">24231 </span></a>    Args: 
<a name="l24232"><span class="ln">24232 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24233"><span class="ln">24233 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24234"><span class="ln">24234 </span></a> 
<a name="l24235"><span class="ln">24235 </span></a>    Keyword args: 
<a name="l24236"><span class="ln">24236 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24237"><span class="ln">24237 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24238"><span class="ln">24238 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24239"><span class="ln">24239 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24240"><span class="ln">24240 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24241"><span class="ln">24241 </span></a>            Default: ``torch.strided``. 
<a name="l24242"><span class="ln">24242 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24243"><span class="ln">24243 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24244"><span class="ln">24244 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24245"><span class="ln">24245 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24246"><span class="ln">24246 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24247"><span class="ln">24247 </span></a>            returned tensor. Default: ``False``. 
<a name="l24248"><span class="ln">24248 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24249"><span class="ln">24249 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24250"><span class="ln">24250 </span></a> 
<a name="l24251"><span class="ln">24251 </span></a>    Example:: 
<a name="l24252"><span class="ln">24252 </span></a> 
<a name="l24253"><span class="ln">24253 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l24254"><span class="ln">24254 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l24255"><span class="ln">24255 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l24256"><span class="ln">24256 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l24257"><span class="ln">24257 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l24258"><span class="ln">24258 </span></a>    &quot;&quot;&quot;</span>
<a name="l24259"><span class="ln">24259 </span></a>
<a name="l24260"><span class="ln">24260 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24261"><span class="ln">24261 </span></a><span class="s2">def </span><span class="s1">rand</span><span class="s3">(</span>
<a name="l24262"><span class="ln">24262 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24263"><span class="ln">24263 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24264"><span class="ln">24264 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24265"><span class="ln">24265 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24266"><span class="ln">24266 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24267"><span class="ln">24267 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24268"><span class="ln">24268 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24269"><span class="ln">24269 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24270"><span class="ln">24270 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24271"><span class="ln">24271 </span></a>    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24272"><span class="ln">24272 </span></a> 
<a name="l24273"><span class="ln">24273 </span></a>    Returns a tensor filled with random numbers from a uniform distribution 
<a name="l24274"><span class="ln">24274 </span></a>    on the interval :math:`[0, 1)` 
<a name="l24275"><span class="ln">24275 </span></a> 
<a name="l24276"><span class="ln">24276 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24277"><span class="ln">24277 </span></a> 
<a name="l24278"><span class="ln">24278 </span></a>    Args: 
<a name="l24279"><span class="ln">24279 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24280"><span class="ln">24280 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24281"><span class="ln">24281 </span></a> 
<a name="l24282"><span class="ln">24282 </span></a>    Keyword args: 
<a name="l24283"><span class="ln">24283 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24284"><span class="ln">24284 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24285"><span class="ln">24285 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24286"><span class="ln">24286 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24287"><span class="ln">24287 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24288"><span class="ln">24288 </span></a>            Default: ``torch.strided``. 
<a name="l24289"><span class="ln">24289 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24290"><span class="ln">24290 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24291"><span class="ln">24291 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24292"><span class="ln">24292 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24293"><span class="ln">24293 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24294"><span class="ln">24294 </span></a>            returned tensor. Default: ``False``. 
<a name="l24295"><span class="ln">24295 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24296"><span class="ln">24296 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24297"><span class="ln">24297 </span></a> 
<a name="l24298"><span class="ln">24298 </span></a>    Example:: 
<a name="l24299"><span class="ln">24299 </span></a> 
<a name="l24300"><span class="ln">24300 </span></a>        &gt;&gt;&gt; torch.rand(4) 
<a name="l24301"><span class="ln">24301 </span></a>        tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) 
<a name="l24302"><span class="ln">24302 </span></a>        &gt;&gt;&gt; torch.rand(2, 3) 
<a name="l24303"><span class="ln">24303 </span></a>        tensor([[ 0.8237,  0.5781,  0.6879], 
<a name="l24304"><span class="ln">24304 </span></a>                [ 0.3816,  0.7249,  0.0998]]) 
<a name="l24305"><span class="ln">24305 </span></a>    &quot;&quot;&quot;</span>
<a name="l24306"><span class="ln">24306 </span></a>
<a name="l24307"><span class="ln">24307 </span></a><span class="s2">def </span><span class="s1">rand_like</span><span class="s3">(</span>
<a name="l24308"><span class="ln">24308 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l24309"><span class="ln">24309 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24310"><span class="ln">24310 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24311"><span class="ln">24311 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24312"><span class="ln">24312 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24313"><span class="ln">24313 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24314"><span class="ln">24314 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24315"><span class="ln">24315 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24316"><span class="ln">24316 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24317"><span class="ln">24317 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24318"><span class="ln">24318 </span></a>    rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l24319"><span class="ln">24319 </span></a> 
<a name="l24320"><span class="ln">24320 </span></a>    Returns a tensor with the same size as :attr:`input` that is filled with 
<a name="l24321"><span class="ln">24321 </span></a>    random numbers from a uniform distribution on the interval :math:`[0, 1)`. 
<a name="l24322"><span class="ln">24322 </span></a>    ``torch.rand_like(input)`` is equivalent to 
<a name="l24323"><span class="ln">24323 </span></a>    ``torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l24324"><span class="ln">24324 </span></a> 
<a name="l24325"><span class="ln">24325 </span></a>    Args: 
<a name="l24326"><span class="ln">24326 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l24327"><span class="ln">24327 </span></a> 
<a name="l24328"><span class="ln">24328 </span></a>    Keyword args: 
<a name="l24329"><span class="ln">24329 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l24330"><span class="ln">24330 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l24331"><span class="ln">24331 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l24332"><span class="ln">24332 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l24333"><span class="ln">24333 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24334"><span class="ln">24334 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l24335"><span class="ln">24335 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24336"><span class="ln">24336 </span></a>            returned tensor. Default: ``False``. 
<a name="l24337"><span class="ln">24337 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l24338"><span class="ln">24338 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l24339"><span class="ln">24339 </span></a>    &quot;&quot;&quot;</span>
<a name="l24340"><span class="ln">24340 </span></a>
<a name="l24341"><span class="ln">24341 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24342"><span class="ln">24342 </span></a><span class="s2">def </span><span class="s1">randint</span><span class="s3">(</span>
<a name="l24343"><span class="ln">24343 </span></a>    <span class="s1">low</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l24344"><span class="ln">24344 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l24345"><span class="ln">24345 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l24346"><span class="ln">24346 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24347"><span class="ln">24347 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24348"><span class="ln">24348 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24349"><span class="ln">24349 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24350"><span class="ln">24350 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l24351"><span class="ln">24351 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l24352"><span class="ln">24352 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24353"><span class="ln">24353 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24354"><span class="ln">24354 </span></a>    randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l24355"><span class="ln">24355 </span></a> 
<a name="l24356"><span class="ln">24356 </span></a>    Returns a tensor filled with random integers generated uniformly 
<a name="l24357"><span class="ln">24357 </span></a>    between :attr:`low` (inclusive) and :attr:`high` (exclusive). 
<a name="l24358"><span class="ln">24358 </span></a> 
<a name="l24359"><span class="ln">24359 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24360"><span class="ln">24360 </span></a> 
<a name="l24361"><span class="ln">24361 </span></a>    .. note:: 
<a name="l24362"><span class="ln">24362 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24363"><span class="ln">24363 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24364"><span class="ln">24364 </span></a> 
<a name="l24365"><span class="ln">24365 </span></a>    Args: 
<a name="l24366"><span class="ln">24366 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24367"><span class="ln">24367 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24368"><span class="ln">24368 </span></a>        size (tuple): a tuple defining the shape of the output tensor. 
<a name="l24369"><span class="ln">24369 </span></a> 
<a name="l24370"><span class="ln">24370 </span></a>    Keyword args: 
<a name="l24371"><span class="ln">24371 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24372"><span class="ln">24372 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24373"><span class="ln">24373 </span></a>        dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``, 
<a name="l24374"><span class="ln">24374 </span></a>            this function returns a tensor with dtype ``torch.int64``. 
<a name="l24375"><span class="ln">24375 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24376"><span class="ln">24376 </span></a>            Default: ``torch.strided``. 
<a name="l24377"><span class="ln">24377 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24378"><span class="ln">24378 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24379"><span class="ln">24379 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24380"><span class="ln">24380 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24381"><span class="ln">24381 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24382"><span class="ln">24382 </span></a>            returned tensor. Default: ``False``. 
<a name="l24383"><span class="ln">24383 </span></a> 
<a name="l24384"><span class="ln">24384 </span></a>    Example:: 
<a name="l24385"><span class="ln">24385 </span></a> 
<a name="l24386"><span class="ln">24386 </span></a>        &gt;&gt;&gt; torch.randint(3, 5, (3,)) 
<a name="l24387"><span class="ln">24387 </span></a>        tensor([4, 3, 4]) 
<a name="l24388"><span class="ln">24388 </span></a> 
<a name="l24389"><span class="ln">24389 </span></a> 
<a name="l24390"><span class="ln">24390 </span></a>        &gt;&gt;&gt; torch.randint(10, (2, 2)) 
<a name="l24391"><span class="ln">24391 </span></a>        tensor([[0, 2], 
<a name="l24392"><span class="ln">24392 </span></a>                [5, 5]]) 
<a name="l24393"><span class="ln">24393 </span></a> 
<a name="l24394"><span class="ln">24394 </span></a> 
<a name="l24395"><span class="ln">24395 </span></a>        &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) 
<a name="l24396"><span class="ln">24396 </span></a>        tensor([[4, 5], 
<a name="l24397"><span class="ln">24397 </span></a>                [6, 7]]) 
<a name="l24398"><span class="ln">24398 </span></a>    &quot;&quot;&quot;</span>
<a name="l24399"><span class="ln">24399 </span></a>
<a name="l24400"><span class="ln">24400 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24401"><span class="ln">24401 </span></a><span class="s2">def </span><span class="s1">randint</span><span class="s3">(</span>
<a name="l24402"><span class="ln">24402 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l24403"><span class="ln">24403 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l24404"><span class="ln">24404 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24405"><span class="ln">24405 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24406"><span class="ln">24406 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24407"><span class="ln">24407 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24408"><span class="ln">24408 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l24409"><span class="ln">24409 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l24410"><span class="ln">24410 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24411"><span class="ln">24411 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24412"><span class="ln">24412 </span></a>    randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l24413"><span class="ln">24413 </span></a> 
<a name="l24414"><span class="ln">24414 </span></a>    Returns a tensor filled with random integers generated uniformly 
<a name="l24415"><span class="ln">24415 </span></a>    between :attr:`low` (inclusive) and :attr:`high` (exclusive). 
<a name="l24416"><span class="ln">24416 </span></a> 
<a name="l24417"><span class="ln">24417 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24418"><span class="ln">24418 </span></a> 
<a name="l24419"><span class="ln">24419 </span></a>    .. note:: 
<a name="l24420"><span class="ln">24420 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24421"><span class="ln">24421 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24422"><span class="ln">24422 </span></a> 
<a name="l24423"><span class="ln">24423 </span></a>    Args: 
<a name="l24424"><span class="ln">24424 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24425"><span class="ln">24425 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24426"><span class="ln">24426 </span></a>        size (tuple): a tuple defining the shape of the output tensor. 
<a name="l24427"><span class="ln">24427 </span></a> 
<a name="l24428"><span class="ln">24428 </span></a>    Keyword args: 
<a name="l24429"><span class="ln">24429 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24430"><span class="ln">24430 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24431"><span class="ln">24431 </span></a>        dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``, 
<a name="l24432"><span class="ln">24432 </span></a>            this function returns a tensor with dtype ``torch.int64``. 
<a name="l24433"><span class="ln">24433 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24434"><span class="ln">24434 </span></a>            Default: ``torch.strided``. 
<a name="l24435"><span class="ln">24435 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24436"><span class="ln">24436 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24437"><span class="ln">24437 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24438"><span class="ln">24438 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24439"><span class="ln">24439 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24440"><span class="ln">24440 </span></a>            returned tensor. Default: ``False``. 
<a name="l24441"><span class="ln">24441 </span></a> 
<a name="l24442"><span class="ln">24442 </span></a>    Example:: 
<a name="l24443"><span class="ln">24443 </span></a> 
<a name="l24444"><span class="ln">24444 </span></a>        &gt;&gt;&gt; torch.randint(3, 5, (3,)) 
<a name="l24445"><span class="ln">24445 </span></a>        tensor([4, 3, 4]) 
<a name="l24446"><span class="ln">24446 </span></a> 
<a name="l24447"><span class="ln">24447 </span></a> 
<a name="l24448"><span class="ln">24448 </span></a>        &gt;&gt;&gt; torch.randint(10, (2, 2)) 
<a name="l24449"><span class="ln">24449 </span></a>        tensor([[0, 2], 
<a name="l24450"><span class="ln">24450 </span></a>                [5, 5]]) 
<a name="l24451"><span class="ln">24451 </span></a> 
<a name="l24452"><span class="ln">24452 </span></a> 
<a name="l24453"><span class="ln">24453 </span></a>        &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) 
<a name="l24454"><span class="ln">24454 </span></a>        tensor([[4, 5], 
<a name="l24455"><span class="ln">24455 </span></a>                [6, 7]]) 
<a name="l24456"><span class="ln">24456 </span></a>    &quot;&quot;&quot;</span>
<a name="l24457"><span class="ln">24457 </span></a>
<a name="l24458"><span class="ln">24458 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24459"><span class="ln">24459 </span></a><span class="s2">def </span><span class="s1">randint</span><span class="s3">(</span>
<a name="l24460"><span class="ln">24460 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24461"><span class="ln">24461 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24462"><span class="ln">24462 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24463"><span class="ln">24463 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24464"><span class="ln">24464 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24465"><span class="ln">24465 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24466"><span class="ln">24466 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24467"><span class="ln">24467 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24468"><span class="ln">24468 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24469"><span class="ln">24469 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24470"><span class="ln">24470 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24471"><span class="ln">24471 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24472"><span class="ln">24472 </span></a>    randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l24473"><span class="ln">24473 </span></a> 
<a name="l24474"><span class="ln">24474 </span></a>    Returns a tensor filled with random integers generated uniformly 
<a name="l24475"><span class="ln">24475 </span></a>    between :attr:`low` (inclusive) and :attr:`high` (exclusive). 
<a name="l24476"><span class="ln">24476 </span></a> 
<a name="l24477"><span class="ln">24477 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24478"><span class="ln">24478 </span></a> 
<a name="l24479"><span class="ln">24479 </span></a>    .. note:: 
<a name="l24480"><span class="ln">24480 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24481"><span class="ln">24481 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24482"><span class="ln">24482 </span></a> 
<a name="l24483"><span class="ln">24483 </span></a>    Args: 
<a name="l24484"><span class="ln">24484 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24485"><span class="ln">24485 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24486"><span class="ln">24486 </span></a>        size (tuple): a tuple defining the shape of the output tensor. 
<a name="l24487"><span class="ln">24487 </span></a> 
<a name="l24488"><span class="ln">24488 </span></a>    Keyword args: 
<a name="l24489"><span class="ln">24489 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24490"><span class="ln">24490 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24491"><span class="ln">24491 </span></a>        dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``, 
<a name="l24492"><span class="ln">24492 </span></a>            this function returns a tensor with dtype ``torch.int64``. 
<a name="l24493"><span class="ln">24493 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24494"><span class="ln">24494 </span></a>            Default: ``torch.strided``. 
<a name="l24495"><span class="ln">24495 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24496"><span class="ln">24496 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24497"><span class="ln">24497 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24498"><span class="ln">24498 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24499"><span class="ln">24499 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24500"><span class="ln">24500 </span></a>            returned tensor. Default: ``False``. 
<a name="l24501"><span class="ln">24501 </span></a> 
<a name="l24502"><span class="ln">24502 </span></a>    Example:: 
<a name="l24503"><span class="ln">24503 </span></a> 
<a name="l24504"><span class="ln">24504 </span></a>        &gt;&gt;&gt; torch.randint(3, 5, (3,)) 
<a name="l24505"><span class="ln">24505 </span></a>        tensor([4, 3, 4]) 
<a name="l24506"><span class="ln">24506 </span></a> 
<a name="l24507"><span class="ln">24507 </span></a> 
<a name="l24508"><span class="ln">24508 </span></a>        &gt;&gt;&gt; torch.randint(10, (2, 2)) 
<a name="l24509"><span class="ln">24509 </span></a>        tensor([[0, 2], 
<a name="l24510"><span class="ln">24510 </span></a>                [5, 5]]) 
<a name="l24511"><span class="ln">24511 </span></a> 
<a name="l24512"><span class="ln">24512 </span></a> 
<a name="l24513"><span class="ln">24513 </span></a>        &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) 
<a name="l24514"><span class="ln">24514 </span></a>        tensor([[4, 5], 
<a name="l24515"><span class="ln">24515 </span></a>                [6, 7]]) 
<a name="l24516"><span class="ln">24516 </span></a>    &quot;&quot;&quot;</span>
<a name="l24517"><span class="ln">24517 </span></a>
<a name="l24518"><span class="ln">24518 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24519"><span class="ln">24519 </span></a><span class="s2">def </span><span class="s1">randint</span><span class="s3">(</span>
<a name="l24520"><span class="ln">24520 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24521"><span class="ln">24521 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24522"><span class="ln">24522 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24523"><span class="ln">24523 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24524"><span class="ln">24524 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24525"><span class="ln">24525 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24526"><span class="ln">24526 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24527"><span class="ln">24527 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24528"><span class="ln">24528 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24529"><span class="ln">24529 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24530"><span class="ln">24530 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24531"><span class="ln">24531 </span></a>    randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l24532"><span class="ln">24532 </span></a> 
<a name="l24533"><span class="ln">24533 </span></a>    Returns a tensor filled with random integers generated uniformly 
<a name="l24534"><span class="ln">24534 </span></a>    between :attr:`low` (inclusive) and :attr:`high` (exclusive). 
<a name="l24535"><span class="ln">24535 </span></a> 
<a name="l24536"><span class="ln">24536 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24537"><span class="ln">24537 </span></a> 
<a name="l24538"><span class="ln">24538 </span></a>    .. note:: 
<a name="l24539"><span class="ln">24539 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24540"><span class="ln">24540 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24541"><span class="ln">24541 </span></a> 
<a name="l24542"><span class="ln">24542 </span></a>    Args: 
<a name="l24543"><span class="ln">24543 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24544"><span class="ln">24544 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24545"><span class="ln">24545 </span></a>        size (tuple): a tuple defining the shape of the output tensor. 
<a name="l24546"><span class="ln">24546 </span></a> 
<a name="l24547"><span class="ln">24547 </span></a>    Keyword args: 
<a name="l24548"><span class="ln">24548 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24549"><span class="ln">24549 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24550"><span class="ln">24550 </span></a>        dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``, 
<a name="l24551"><span class="ln">24551 </span></a>            this function returns a tensor with dtype ``torch.int64``. 
<a name="l24552"><span class="ln">24552 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24553"><span class="ln">24553 </span></a>            Default: ``torch.strided``. 
<a name="l24554"><span class="ln">24554 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24555"><span class="ln">24555 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24556"><span class="ln">24556 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24557"><span class="ln">24557 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24558"><span class="ln">24558 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24559"><span class="ln">24559 </span></a>            returned tensor. Default: ``False``. 
<a name="l24560"><span class="ln">24560 </span></a> 
<a name="l24561"><span class="ln">24561 </span></a>    Example:: 
<a name="l24562"><span class="ln">24562 </span></a> 
<a name="l24563"><span class="ln">24563 </span></a>        &gt;&gt;&gt; torch.randint(3, 5, (3,)) 
<a name="l24564"><span class="ln">24564 </span></a>        tensor([4, 3, 4]) 
<a name="l24565"><span class="ln">24565 </span></a> 
<a name="l24566"><span class="ln">24566 </span></a> 
<a name="l24567"><span class="ln">24567 </span></a>        &gt;&gt;&gt; torch.randint(10, (2, 2)) 
<a name="l24568"><span class="ln">24568 </span></a>        tensor([[0, 2], 
<a name="l24569"><span class="ln">24569 </span></a>                [5, 5]]) 
<a name="l24570"><span class="ln">24570 </span></a> 
<a name="l24571"><span class="ln">24571 </span></a> 
<a name="l24572"><span class="ln">24572 </span></a>        &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) 
<a name="l24573"><span class="ln">24573 </span></a>        tensor([[4, 5], 
<a name="l24574"><span class="ln">24574 </span></a>                [6, 7]]) 
<a name="l24575"><span class="ln">24575 </span></a>    &quot;&quot;&quot;</span>
<a name="l24576"><span class="ln">24576 </span></a>
<a name="l24577"><span class="ln">24577 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24578"><span class="ln">24578 </span></a><span class="s2">def </span><span class="s1">randint</span><span class="s3">(</span>
<a name="l24579"><span class="ln">24579 </span></a>    <span class="s1">low</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24580"><span class="ln">24580 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24581"><span class="ln">24581 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24582"><span class="ln">24582 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24583"><span class="ln">24583 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24584"><span class="ln">24584 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24585"><span class="ln">24585 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24586"><span class="ln">24586 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24587"><span class="ln">24587 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24588"><span class="ln">24588 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24589"><span class="ln">24589 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24590"><span class="ln">24590 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24591"><span class="ln">24591 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24592"><span class="ln">24592 </span></a>    randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l24593"><span class="ln">24593 </span></a> 
<a name="l24594"><span class="ln">24594 </span></a>    Returns a tensor filled with random integers generated uniformly 
<a name="l24595"><span class="ln">24595 </span></a>    between :attr:`low` (inclusive) and :attr:`high` (exclusive). 
<a name="l24596"><span class="ln">24596 </span></a> 
<a name="l24597"><span class="ln">24597 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24598"><span class="ln">24598 </span></a> 
<a name="l24599"><span class="ln">24599 </span></a>    .. note:: 
<a name="l24600"><span class="ln">24600 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24601"><span class="ln">24601 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24602"><span class="ln">24602 </span></a> 
<a name="l24603"><span class="ln">24603 </span></a>    Args: 
<a name="l24604"><span class="ln">24604 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24605"><span class="ln">24605 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24606"><span class="ln">24606 </span></a>        size (tuple): a tuple defining the shape of the output tensor. 
<a name="l24607"><span class="ln">24607 </span></a> 
<a name="l24608"><span class="ln">24608 </span></a>    Keyword args: 
<a name="l24609"><span class="ln">24609 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24610"><span class="ln">24610 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24611"><span class="ln">24611 </span></a>        dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``, 
<a name="l24612"><span class="ln">24612 </span></a>            this function returns a tensor with dtype ``torch.int64``. 
<a name="l24613"><span class="ln">24613 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24614"><span class="ln">24614 </span></a>            Default: ``torch.strided``. 
<a name="l24615"><span class="ln">24615 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24616"><span class="ln">24616 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24617"><span class="ln">24617 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24618"><span class="ln">24618 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24619"><span class="ln">24619 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24620"><span class="ln">24620 </span></a>            returned tensor. Default: ``False``. 
<a name="l24621"><span class="ln">24621 </span></a> 
<a name="l24622"><span class="ln">24622 </span></a>    Example:: 
<a name="l24623"><span class="ln">24623 </span></a> 
<a name="l24624"><span class="ln">24624 </span></a>        &gt;&gt;&gt; torch.randint(3, 5, (3,)) 
<a name="l24625"><span class="ln">24625 </span></a>        tensor([4, 3, 4]) 
<a name="l24626"><span class="ln">24626 </span></a> 
<a name="l24627"><span class="ln">24627 </span></a> 
<a name="l24628"><span class="ln">24628 </span></a>        &gt;&gt;&gt; torch.randint(10, (2, 2)) 
<a name="l24629"><span class="ln">24629 </span></a>        tensor([[0, 2], 
<a name="l24630"><span class="ln">24630 </span></a>                [5, 5]]) 
<a name="l24631"><span class="ln">24631 </span></a> 
<a name="l24632"><span class="ln">24632 </span></a> 
<a name="l24633"><span class="ln">24633 </span></a>        &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) 
<a name="l24634"><span class="ln">24634 </span></a>        tensor([[4, 5], 
<a name="l24635"><span class="ln">24635 </span></a>                [6, 7]]) 
<a name="l24636"><span class="ln">24636 </span></a>    &quot;&quot;&quot;</span>
<a name="l24637"><span class="ln">24637 </span></a>
<a name="l24638"><span class="ln">24638 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24639"><span class="ln">24639 </span></a><span class="s2">def </span><span class="s1">randint</span><span class="s3">(</span>
<a name="l24640"><span class="ln">24640 </span></a>    <span class="s1">low</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24641"><span class="ln">24641 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24642"><span class="ln">24642 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24643"><span class="ln">24643 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24644"><span class="ln">24644 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24645"><span class="ln">24645 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24646"><span class="ln">24646 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24647"><span class="ln">24647 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24648"><span class="ln">24648 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24649"><span class="ln">24649 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24650"><span class="ln">24650 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24651"><span class="ln">24651 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24652"><span class="ln">24652 </span></a>    randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l24653"><span class="ln">24653 </span></a> 
<a name="l24654"><span class="ln">24654 </span></a>    Returns a tensor filled with random integers generated uniformly 
<a name="l24655"><span class="ln">24655 </span></a>    between :attr:`low` (inclusive) and :attr:`high` (exclusive). 
<a name="l24656"><span class="ln">24656 </span></a> 
<a name="l24657"><span class="ln">24657 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24658"><span class="ln">24658 </span></a> 
<a name="l24659"><span class="ln">24659 </span></a>    .. note:: 
<a name="l24660"><span class="ln">24660 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24661"><span class="ln">24661 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24662"><span class="ln">24662 </span></a> 
<a name="l24663"><span class="ln">24663 </span></a>    Args: 
<a name="l24664"><span class="ln">24664 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24665"><span class="ln">24665 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24666"><span class="ln">24666 </span></a>        size (tuple): a tuple defining the shape of the output tensor. 
<a name="l24667"><span class="ln">24667 </span></a> 
<a name="l24668"><span class="ln">24668 </span></a>    Keyword args: 
<a name="l24669"><span class="ln">24669 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24670"><span class="ln">24670 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24671"><span class="ln">24671 </span></a>        dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``, 
<a name="l24672"><span class="ln">24672 </span></a>            this function returns a tensor with dtype ``torch.int64``. 
<a name="l24673"><span class="ln">24673 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24674"><span class="ln">24674 </span></a>            Default: ``torch.strided``. 
<a name="l24675"><span class="ln">24675 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24676"><span class="ln">24676 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24677"><span class="ln">24677 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24678"><span class="ln">24678 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24679"><span class="ln">24679 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24680"><span class="ln">24680 </span></a>            returned tensor. Default: ``False``. 
<a name="l24681"><span class="ln">24681 </span></a> 
<a name="l24682"><span class="ln">24682 </span></a>    Example:: 
<a name="l24683"><span class="ln">24683 </span></a> 
<a name="l24684"><span class="ln">24684 </span></a>        &gt;&gt;&gt; torch.randint(3, 5, (3,)) 
<a name="l24685"><span class="ln">24685 </span></a>        tensor([4, 3, 4]) 
<a name="l24686"><span class="ln">24686 </span></a> 
<a name="l24687"><span class="ln">24687 </span></a> 
<a name="l24688"><span class="ln">24688 </span></a>        &gt;&gt;&gt; torch.randint(10, (2, 2)) 
<a name="l24689"><span class="ln">24689 </span></a>        tensor([[0, 2], 
<a name="l24690"><span class="ln">24690 </span></a>                [5, 5]]) 
<a name="l24691"><span class="ln">24691 </span></a> 
<a name="l24692"><span class="ln">24692 </span></a> 
<a name="l24693"><span class="ln">24693 </span></a>        &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) 
<a name="l24694"><span class="ln">24694 </span></a>        tensor([[4, 5], 
<a name="l24695"><span class="ln">24695 </span></a>                [6, 7]]) 
<a name="l24696"><span class="ln">24696 </span></a>    &quot;&quot;&quot;</span>
<a name="l24697"><span class="ln">24697 </span></a>
<a name="l24698"><span class="ln">24698 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24699"><span class="ln">24699 </span></a><span class="s2">def </span><span class="s1">randint_like</span><span class="s3">(</span>
<a name="l24700"><span class="ln">24700 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l24701"><span class="ln">24701 </span></a>    <span class="s1">low</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24702"><span class="ln">24702 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24703"><span class="ln">24703 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24704"><span class="ln">24704 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24705"><span class="ln">24705 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24706"><span class="ln">24706 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24707"><span class="ln">24707 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24708"><span class="ln">24708 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24709"><span class="ln">24709 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24710"><span class="ln">24710 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24711"><span class="ln">24711 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24712"><span class="ln">24712 </span></a>    randint_like(input, low=0, high, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l24713"><span class="ln">24713 </span></a> 
<a name="l24714"><span class="ln">24714 </span></a>    Returns a tensor with the same shape as Tensor :attr:`input` filled with 
<a name="l24715"><span class="ln">24715 </span></a>    random integers generated uniformly between :attr:`low` (inclusive) and 
<a name="l24716"><span class="ln">24716 </span></a>    :attr:`high` (exclusive). 
<a name="l24717"><span class="ln">24717 </span></a> 
<a name="l24718"><span class="ln">24718 </span></a>    .. note: 
<a name="l24719"><span class="ln">24719 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24720"><span class="ln">24720 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24721"><span class="ln">24721 </span></a> 
<a name="l24722"><span class="ln">24722 </span></a>    Args: 
<a name="l24723"><span class="ln">24723 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l24724"><span class="ln">24724 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24725"><span class="ln">24725 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24726"><span class="ln">24726 </span></a> 
<a name="l24727"><span class="ln">24727 </span></a>    Keyword args: 
<a name="l24728"><span class="ln">24728 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l24729"><span class="ln">24729 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l24730"><span class="ln">24730 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l24731"><span class="ln">24731 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l24732"><span class="ln">24732 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24733"><span class="ln">24733 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l24734"><span class="ln">24734 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24735"><span class="ln">24735 </span></a>            returned tensor. Default: ``False``. 
<a name="l24736"><span class="ln">24736 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l24737"><span class="ln">24737 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l24738"><span class="ln">24738 </span></a>    &quot;&quot;&quot;</span>
<a name="l24739"><span class="ln">24739 </span></a>
<a name="l24740"><span class="ln">24740 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24741"><span class="ln">24741 </span></a><span class="s2">def </span><span class="s1">randint_like</span><span class="s3">(</span>
<a name="l24742"><span class="ln">24742 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l24743"><span class="ln">24743 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l24744"><span class="ln">24744 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24745"><span class="ln">24745 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24746"><span class="ln">24746 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24747"><span class="ln">24747 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24748"><span class="ln">24748 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24749"><span class="ln">24749 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24750"><span class="ln">24750 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24751"><span class="ln">24751 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24752"><span class="ln">24752 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24753"><span class="ln">24753 </span></a>    randint_like(input, low=0, high, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l24754"><span class="ln">24754 </span></a> 
<a name="l24755"><span class="ln">24755 </span></a>    Returns a tensor with the same shape as Tensor :attr:`input` filled with 
<a name="l24756"><span class="ln">24756 </span></a>    random integers generated uniformly between :attr:`low` (inclusive) and 
<a name="l24757"><span class="ln">24757 </span></a>    :attr:`high` (exclusive). 
<a name="l24758"><span class="ln">24758 </span></a> 
<a name="l24759"><span class="ln">24759 </span></a>    .. note: 
<a name="l24760"><span class="ln">24760 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24761"><span class="ln">24761 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24762"><span class="ln">24762 </span></a> 
<a name="l24763"><span class="ln">24763 </span></a>    Args: 
<a name="l24764"><span class="ln">24764 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l24765"><span class="ln">24765 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24766"><span class="ln">24766 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24767"><span class="ln">24767 </span></a> 
<a name="l24768"><span class="ln">24768 </span></a>    Keyword args: 
<a name="l24769"><span class="ln">24769 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l24770"><span class="ln">24770 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l24771"><span class="ln">24771 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l24772"><span class="ln">24772 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l24773"><span class="ln">24773 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24774"><span class="ln">24774 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l24775"><span class="ln">24775 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24776"><span class="ln">24776 </span></a>            returned tensor. Default: ``False``. 
<a name="l24777"><span class="ln">24777 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l24778"><span class="ln">24778 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l24779"><span class="ln">24779 </span></a>    &quot;&quot;&quot;</span>
<a name="l24780"><span class="ln">24780 </span></a>
<a name="l24781"><span class="ln">24781 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24782"><span class="ln">24782 </span></a><span class="s2">def </span><span class="s1">randint_like</span><span class="s3">(</span>
<a name="l24783"><span class="ln">24783 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l24784"><span class="ln">24784 </span></a>    <span class="s1">high</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24785"><span class="ln">24785 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24786"><span class="ln">24786 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24787"><span class="ln">24787 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24788"><span class="ln">24788 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24789"><span class="ln">24789 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24790"><span class="ln">24790 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24791"><span class="ln">24791 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24792"><span class="ln">24792 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24793"><span class="ln">24793 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24794"><span class="ln">24794 </span></a>    randint_like(input, low=0, high, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l24795"><span class="ln">24795 </span></a> 
<a name="l24796"><span class="ln">24796 </span></a>    Returns a tensor with the same shape as Tensor :attr:`input` filled with 
<a name="l24797"><span class="ln">24797 </span></a>    random integers generated uniformly between :attr:`low` (inclusive) and 
<a name="l24798"><span class="ln">24798 </span></a>    :attr:`high` (exclusive). 
<a name="l24799"><span class="ln">24799 </span></a> 
<a name="l24800"><span class="ln">24800 </span></a>    .. note: 
<a name="l24801"><span class="ln">24801 </span></a>        With the global dtype default (``torch.float32``), this function returns 
<a name="l24802"><span class="ln">24802 </span></a>        a tensor with dtype ``torch.int64``. 
<a name="l24803"><span class="ln">24803 </span></a> 
<a name="l24804"><span class="ln">24804 </span></a>    Args: 
<a name="l24805"><span class="ln">24805 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l24806"><span class="ln">24806 </span></a>        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0. 
<a name="l24807"><span class="ln">24807 </span></a>        high (int): One above the highest integer to be drawn from the distribution. 
<a name="l24808"><span class="ln">24808 </span></a> 
<a name="l24809"><span class="ln">24809 </span></a>    Keyword args: 
<a name="l24810"><span class="ln">24810 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l24811"><span class="ln">24811 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l24812"><span class="ln">24812 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l24813"><span class="ln">24813 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l24814"><span class="ln">24814 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24815"><span class="ln">24815 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l24816"><span class="ln">24816 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24817"><span class="ln">24817 </span></a>            returned tensor. Default: ``False``. 
<a name="l24818"><span class="ln">24818 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l24819"><span class="ln">24819 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l24820"><span class="ln">24820 </span></a>    &quot;&quot;&quot;</span>
<a name="l24821"><span class="ln">24821 </span></a>
<a name="l24822"><span class="ln">24822 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24823"><span class="ln">24823 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l24824"><span class="ln">24824 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24825"><span class="ln">24825 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24826"><span class="ln">24826 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24827"><span class="ln">24827 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24828"><span class="ln">24828 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24829"><span class="ln">24829 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24830"><span class="ln">24830 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24831"><span class="ln">24831 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24832"><span class="ln">24832 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24833"><span class="ln">24833 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24834"><span class="ln">24834 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24835"><span class="ln">24835 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24836"><span class="ln">24836 </span></a> 
<a name="l24837"><span class="ln">24837 </span></a> 
<a name="l24838"><span class="ln">24838 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l24839"><span class="ln">24839 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l24840"><span class="ln">24840 </span></a>    distribution). 
<a name="l24841"><span class="ln">24841 </span></a> 
<a name="l24842"><span class="ln">24842 </span></a>    .. math:: 
<a name="l24843"><span class="ln">24843 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l24844"><span class="ln">24844 </span></a> 
<a name="l24845"><span class="ln">24845 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l24846"><span class="ln">24846 </span></a>    unit variance as 
<a name="l24847"><span class="ln">24847 </span></a> 
<a name="l24848"><span class="ln">24848 </span></a>    .. math:: 
<a name="l24849"><span class="ln">24849 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l24850"><span class="ln">24850 </span></a> 
<a name="l24851"><span class="ln">24851 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l24852"><span class="ln">24852 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l24853"><span class="ln">24853 </span></a> 
<a name="l24854"><span class="ln">24854 </span></a>    .. math:: 
<a name="l24855"><span class="ln">24855 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l24856"><span class="ln">24856 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l24857"><span class="ln">24857 </span></a> 
<a name="l24858"><span class="ln">24858 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24859"><span class="ln">24859 </span></a> 
<a name="l24860"><span class="ln">24860 </span></a> 
<a name="l24861"><span class="ln">24861 </span></a>    Args: 
<a name="l24862"><span class="ln">24862 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24863"><span class="ln">24863 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24864"><span class="ln">24864 </span></a> 
<a name="l24865"><span class="ln">24865 </span></a>    Keyword args: 
<a name="l24866"><span class="ln">24866 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24867"><span class="ln">24867 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24868"><span class="ln">24868 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24869"><span class="ln">24869 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24870"><span class="ln">24870 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24871"><span class="ln">24871 </span></a>            Default: ``torch.strided``. 
<a name="l24872"><span class="ln">24872 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24873"><span class="ln">24873 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24874"><span class="ln">24874 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24875"><span class="ln">24875 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24876"><span class="ln">24876 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24877"><span class="ln">24877 </span></a>            returned tensor. Default: ``False``. 
<a name="l24878"><span class="ln">24878 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24879"><span class="ln">24879 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24880"><span class="ln">24880 </span></a> 
<a name="l24881"><span class="ln">24881 </span></a>    Example:: 
<a name="l24882"><span class="ln">24882 </span></a> 
<a name="l24883"><span class="ln">24883 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l24884"><span class="ln">24884 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l24885"><span class="ln">24885 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l24886"><span class="ln">24886 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l24887"><span class="ln">24887 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l24888"><span class="ln">24888 </span></a> 
<a name="l24889"><span class="ln">24889 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l24890"><span class="ln">24890 </span></a>    &quot;&quot;&quot;</span>
<a name="l24891"><span class="ln">24891 </span></a>
<a name="l24892"><span class="ln">24892 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24893"><span class="ln">24893 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l24894"><span class="ln">24894 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l24895"><span class="ln">24895 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24896"><span class="ln">24896 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24897"><span class="ln">24897 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24898"><span class="ln">24898 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24899"><span class="ln">24899 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24900"><span class="ln">24900 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24901"><span class="ln">24901 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24902"><span class="ln">24902 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24903"><span class="ln">24903 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24904"><span class="ln">24904 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24905"><span class="ln">24905 </span></a> 
<a name="l24906"><span class="ln">24906 </span></a> 
<a name="l24907"><span class="ln">24907 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l24908"><span class="ln">24908 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l24909"><span class="ln">24909 </span></a>    distribution). 
<a name="l24910"><span class="ln">24910 </span></a> 
<a name="l24911"><span class="ln">24911 </span></a>    .. math:: 
<a name="l24912"><span class="ln">24912 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l24913"><span class="ln">24913 </span></a> 
<a name="l24914"><span class="ln">24914 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l24915"><span class="ln">24915 </span></a>    unit variance as 
<a name="l24916"><span class="ln">24916 </span></a> 
<a name="l24917"><span class="ln">24917 </span></a>    .. math:: 
<a name="l24918"><span class="ln">24918 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l24919"><span class="ln">24919 </span></a> 
<a name="l24920"><span class="ln">24920 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l24921"><span class="ln">24921 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l24922"><span class="ln">24922 </span></a> 
<a name="l24923"><span class="ln">24923 </span></a>    .. math:: 
<a name="l24924"><span class="ln">24924 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l24925"><span class="ln">24925 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l24926"><span class="ln">24926 </span></a> 
<a name="l24927"><span class="ln">24927 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24928"><span class="ln">24928 </span></a> 
<a name="l24929"><span class="ln">24929 </span></a> 
<a name="l24930"><span class="ln">24930 </span></a>    Args: 
<a name="l24931"><span class="ln">24931 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l24932"><span class="ln">24932 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l24933"><span class="ln">24933 </span></a> 
<a name="l24934"><span class="ln">24934 </span></a>    Keyword args: 
<a name="l24935"><span class="ln">24935 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l24936"><span class="ln">24936 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l24937"><span class="ln">24937 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l24938"><span class="ln">24938 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l24939"><span class="ln">24939 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l24940"><span class="ln">24940 </span></a>            Default: ``torch.strided``. 
<a name="l24941"><span class="ln">24941 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l24942"><span class="ln">24942 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l24943"><span class="ln">24943 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l24944"><span class="ln">24944 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l24945"><span class="ln">24945 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l24946"><span class="ln">24946 </span></a>            returned tensor. Default: ``False``. 
<a name="l24947"><span class="ln">24947 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l24948"><span class="ln">24948 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l24949"><span class="ln">24949 </span></a> 
<a name="l24950"><span class="ln">24950 </span></a>    Example:: 
<a name="l24951"><span class="ln">24951 </span></a> 
<a name="l24952"><span class="ln">24952 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l24953"><span class="ln">24953 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l24954"><span class="ln">24954 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l24955"><span class="ln">24955 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l24956"><span class="ln">24956 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l24957"><span class="ln">24957 </span></a> 
<a name="l24958"><span class="ln">24958 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l24959"><span class="ln">24959 </span></a>    &quot;&quot;&quot;</span>
<a name="l24960"><span class="ln">24960 </span></a>
<a name="l24961"><span class="ln">24961 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l24962"><span class="ln">24962 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l24963"><span class="ln">24963 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l24964"><span class="ln">24964 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l24965"><span class="ln">24965 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l24966"><span class="ln">24966 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24967"><span class="ln">24967 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24968"><span class="ln">24968 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24969"><span class="ln">24969 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l24970"><span class="ln">24970 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24971"><span class="ln">24971 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l24972"><span class="ln">24972 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l24973"><span class="ln">24973 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l24974"><span class="ln">24974 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l24975"><span class="ln">24975 </span></a> 
<a name="l24976"><span class="ln">24976 </span></a> 
<a name="l24977"><span class="ln">24977 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l24978"><span class="ln">24978 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l24979"><span class="ln">24979 </span></a>    distribution). 
<a name="l24980"><span class="ln">24980 </span></a> 
<a name="l24981"><span class="ln">24981 </span></a>    .. math:: 
<a name="l24982"><span class="ln">24982 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l24983"><span class="ln">24983 </span></a> 
<a name="l24984"><span class="ln">24984 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l24985"><span class="ln">24985 </span></a>    unit variance as 
<a name="l24986"><span class="ln">24986 </span></a> 
<a name="l24987"><span class="ln">24987 </span></a>    .. math:: 
<a name="l24988"><span class="ln">24988 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l24989"><span class="ln">24989 </span></a> 
<a name="l24990"><span class="ln">24990 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l24991"><span class="ln">24991 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l24992"><span class="ln">24992 </span></a> 
<a name="l24993"><span class="ln">24993 </span></a>    .. math:: 
<a name="l24994"><span class="ln">24994 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l24995"><span class="ln">24995 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l24996"><span class="ln">24996 </span></a> 
<a name="l24997"><span class="ln">24997 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l24998"><span class="ln">24998 </span></a> 
<a name="l24999"><span class="ln">24999 </span></a> 
<a name="l25000"><span class="ln">25000 </span></a>    Args: 
<a name="l25001"><span class="ln">25001 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l25002"><span class="ln">25002 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l25003"><span class="ln">25003 </span></a> 
<a name="l25004"><span class="ln">25004 </span></a>    Keyword args: 
<a name="l25005"><span class="ln">25005 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25006"><span class="ln">25006 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25007"><span class="ln">25007 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25008"><span class="ln">25008 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l25009"><span class="ln">25009 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25010"><span class="ln">25010 </span></a>            Default: ``torch.strided``. 
<a name="l25011"><span class="ln">25011 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25012"><span class="ln">25012 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25013"><span class="ln">25013 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25014"><span class="ln">25014 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25015"><span class="ln">25015 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25016"><span class="ln">25016 </span></a>            returned tensor. Default: ``False``. 
<a name="l25017"><span class="ln">25017 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25018"><span class="ln">25018 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25019"><span class="ln">25019 </span></a> 
<a name="l25020"><span class="ln">25020 </span></a>    Example:: 
<a name="l25021"><span class="ln">25021 </span></a> 
<a name="l25022"><span class="ln">25022 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l25023"><span class="ln">25023 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l25024"><span class="ln">25024 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l25025"><span class="ln">25025 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l25026"><span class="ln">25026 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l25027"><span class="ln">25027 </span></a> 
<a name="l25028"><span class="ln">25028 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l25029"><span class="ln">25029 </span></a>    &quot;&quot;&quot;</span>
<a name="l25030"><span class="ln">25030 </span></a>
<a name="l25031"><span class="ln">25031 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25032"><span class="ln">25032 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l25033"><span class="ln">25033 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l25034"><span class="ln">25034 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l25035"><span class="ln">25035 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25036"><span class="ln">25036 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25037"><span class="ln">25037 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25038"><span class="ln">25038 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25039"><span class="ln">25039 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25040"><span class="ln">25040 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25041"><span class="ln">25041 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25042"><span class="ln">25042 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25043"><span class="ln">25043 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l25044"><span class="ln">25044 </span></a> 
<a name="l25045"><span class="ln">25045 </span></a> 
<a name="l25046"><span class="ln">25046 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l25047"><span class="ln">25047 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l25048"><span class="ln">25048 </span></a>    distribution). 
<a name="l25049"><span class="ln">25049 </span></a> 
<a name="l25050"><span class="ln">25050 </span></a>    .. math:: 
<a name="l25051"><span class="ln">25051 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l25052"><span class="ln">25052 </span></a> 
<a name="l25053"><span class="ln">25053 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l25054"><span class="ln">25054 </span></a>    unit variance as 
<a name="l25055"><span class="ln">25055 </span></a> 
<a name="l25056"><span class="ln">25056 </span></a>    .. math:: 
<a name="l25057"><span class="ln">25057 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l25058"><span class="ln">25058 </span></a> 
<a name="l25059"><span class="ln">25059 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l25060"><span class="ln">25060 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l25061"><span class="ln">25061 </span></a> 
<a name="l25062"><span class="ln">25062 </span></a>    .. math:: 
<a name="l25063"><span class="ln">25063 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l25064"><span class="ln">25064 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l25065"><span class="ln">25065 </span></a> 
<a name="l25066"><span class="ln">25066 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l25067"><span class="ln">25067 </span></a> 
<a name="l25068"><span class="ln">25068 </span></a> 
<a name="l25069"><span class="ln">25069 </span></a>    Args: 
<a name="l25070"><span class="ln">25070 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l25071"><span class="ln">25071 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l25072"><span class="ln">25072 </span></a> 
<a name="l25073"><span class="ln">25073 </span></a>    Keyword args: 
<a name="l25074"><span class="ln">25074 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25075"><span class="ln">25075 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25076"><span class="ln">25076 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25077"><span class="ln">25077 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l25078"><span class="ln">25078 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25079"><span class="ln">25079 </span></a>            Default: ``torch.strided``. 
<a name="l25080"><span class="ln">25080 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25081"><span class="ln">25081 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25082"><span class="ln">25082 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25083"><span class="ln">25083 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25084"><span class="ln">25084 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25085"><span class="ln">25085 </span></a>            returned tensor. Default: ``False``. 
<a name="l25086"><span class="ln">25086 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25087"><span class="ln">25087 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25088"><span class="ln">25088 </span></a> 
<a name="l25089"><span class="ln">25089 </span></a>    Example:: 
<a name="l25090"><span class="ln">25090 </span></a> 
<a name="l25091"><span class="ln">25091 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l25092"><span class="ln">25092 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l25093"><span class="ln">25093 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l25094"><span class="ln">25094 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l25095"><span class="ln">25095 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l25096"><span class="ln">25096 </span></a> 
<a name="l25097"><span class="ln">25097 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l25098"><span class="ln">25098 </span></a>    &quot;&quot;&quot;</span>
<a name="l25099"><span class="ln">25099 </span></a>
<a name="l25100"><span class="ln">25100 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25101"><span class="ln">25101 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l25102"><span class="ln">25102 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l25103"><span class="ln">25103 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25104"><span class="ln">25104 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25105"><span class="ln">25105 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25106"><span class="ln">25106 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25107"><span class="ln">25107 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25108"><span class="ln">25108 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25109"><span class="ln">25109 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25110"><span class="ln">25110 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25111"><span class="ln">25111 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25112"><span class="ln">25112 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l25113"><span class="ln">25113 </span></a> 
<a name="l25114"><span class="ln">25114 </span></a> 
<a name="l25115"><span class="ln">25115 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l25116"><span class="ln">25116 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l25117"><span class="ln">25117 </span></a>    distribution). 
<a name="l25118"><span class="ln">25118 </span></a> 
<a name="l25119"><span class="ln">25119 </span></a>    .. math:: 
<a name="l25120"><span class="ln">25120 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l25121"><span class="ln">25121 </span></a> 
<a name="l25122"><span class="ln">25122 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l25123"><span class="ln">25123 </span></a>    unit variance as 
<a name="l25124"><span class="ln">25124 </span></a> 
<a name="l25125"><span class="ln">25125 </span></a>    .. math:: 
<a name="l25126"><span class="ln">25126 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l25127"><span class="ln">25127 </span></a> 
<a name="l25128"><span class="ln">25128 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l25129"><span class="ln">25129 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l25130"><span class="ln">25130 </span></a> 
<a name="l25131"><span class="ln">25131 </span></a>    .. math:: 
<a name="l25132"><span class="ln">25132 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l25133"><span class="ln">25133 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l25134"><span class="ln">25134 </span></a> 
<a name="l25135"><span class="ln">25135 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l25136"><span class="ln">25136 </span></a> 
<a name="l25137"><span class="ln">25137 </span></a> 
<a name="l25138"><span class="ln">25138 </span></a>    Args: 
<a name="l25139"><span class="ln">25139 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l25140"><span class="ln">25140 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l25141"><span class="ln">25141 </span></a> 
<a name="l25142"><span class="ln">25142 </span></a>    Keyword args: 
<a name="l25143"><span class="ln">25143 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25144"><span class="ln">25144 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25145"><span class="ln">25145 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25146"><span class="ln">25146 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l25147"><span class="ln">25147 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25148"><span class="ln">25148 </span></a>            Default: ``torch.strided``. 
<a name="l25149"><span class="ln">25149 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25150"><span class="ln">25150 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25151"><span class="ln">25151 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25152"><span class="ln">25152 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25153"><span class="ln">25153 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25154"><span class="ln">25154 </span></a>            returned tensor. Default: ``False``. 
<a name="l25155"><span class="ln">25155 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25156"><span class="ln">25156 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25157"><span class="ln">25157 </span></a> 
<a name="l25158"><span class="ln">25158 </span></a>    Example:: 
<a name="l25159"><span class="ln">25159 </span></a> 
<a name="l25160"><span class="ln">25160 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l25161"><span class="ln">25161 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l25162"><span class="ln">25162 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l25163"><span class="ln">25163 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l25164"><span class="ln">25164 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l25165"><span class="ln">25165 </span></a> 
<a name="l25166"><span class="ln">25166 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l25167"><span class="ln">25167 </span></a>    &quot;&quot;&quot;</span>
<a name="l25168"><span class="ln">25168 </span></a>
<a name="l25169"><span class="ln">25169 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25170"><span class="ln">25170 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l25171"><span class="ln">25171 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l25172"><span class="ln">25172 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25173"><span class="ln">25173 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25174"><span class="ln">25174 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25175"><span class="ln">25175 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25176"><span class="ln">25176 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25177"><span class="ln">25177 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25178"><span class="ln">25178 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25179"><span class="ln">25179 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25180"><span class="ln">25180 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l25181"><span class="ln">25181 </span></a> 
<a name="l25182"><span class="ln">25182 </span></a> 
<a name="l25183"><span class="ln">25183 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l25184"><span class="ln">25184 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l25185"><span class="ln">25185 </span></a>    distribution). 
<a name="l25186"><span class="ln">25186 </span></a> 
<a name="l25187"><span class="ln">25187 </span></a>    .. math:: 
<a name="l25188"><span class="ln">25188 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l25189"><span class="ln">25189 </span></a> 
<a name="l25190"><span class="ln">25190 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l25191"><span class="ln">25191 </span></a>    unit variance as 
<a name="l25192"><span class="ln">25192 </span></a> 
<a name="l25193"><span class="ln">25193 </span></a>    .. math:: 
<a name="l25194"><span class="ln">25194 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l25195"><span class="ln">25195 </span></a> 
<a name="l25196"><span class="ln">25196 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l25197"><span class="ln">25197 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l25198"><span class="ln">25198 </span></a> 
<a name="l25199"><span class="ln">25199 </span></a>    .. math:: 
<a name="l25200"><span class="ln">25200 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l25201"><span class="ln">25201 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l25202"><span class="ln">25202 </span></a> 
<a name="l25203"><span class="ln">25203 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l25204"><span class="ln">25204 </span></a> 
<a name="l25205"><span class="ln">25205 </span></a> 
<a name="l25206"><span class="ln">25206 </span></a>    Args: 
<a name="l25207"><span class="ln">25207 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l25208"><span class="ln">25208 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l25209"><span class="ln">25209 </span></a> 
<a name="l25210"><span class="ln">25210 </span></a>    Keyword args: 
<a name="l25211"><span class="ln">25211 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25212"><span class="ln">25212 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25213"><span class="ln">25213 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25214"><span class="ln">25214 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l25215"><span class="ln">25215 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25216"><span class="ln">25216 </span></a>            Default: ``torch.strided``. 
<a name="l25217"><span class="ln">25217 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25218"><span class="ln">25218 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25219"><span class="ln">25219 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25220"><span class="ln">25220 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25221"><span class="ln">25221 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25222"><span class="ln">25222 </span></a>            returned tensor. Default: ``False``. 
<a name="l25223"><span class="ln">25223 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25224"><span class="ln">25224 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25225"><span class="ln">25225 </span></a> 
<a name="l25226"><span class="ln">25226 </span></a>    Example:: 
<a name="l25227"><span class="ln">25227 </span></a> 
<a name="l25228"><span class="ln">25228 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l25229"><span class="ln">25229 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l25230"><span class="ln">25230 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l25231"><span class="ln">25231 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l25232"><span class="ln">25232 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l25233"><span class="ln">25233 </span></a> 
<a name="l25234"><span class="ln">25234 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l25235"><span class="ln">25235 </span></a>    &quot;&quot;&quot;</span>
<a name="l25236"><span class="ln">25236 </span></a>
<a name="l25237"><span class="ln">25237 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25238"><span class="ln">25238 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l25239"><span class="ln">25239 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l25240"><span class="ln">25240 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25241"><span class="ln">25241 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l25242"><span class="ln">25242 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25243"><span class="ln">25243 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25244"><span class="ln">25244 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25245"><span class="ln">25245 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25246"><span class="ln">25246 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25247"><span class="ln">25247 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25248"><span class="ln">25248 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25249"><span class="ln">25249 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l25250"><span class="ln">25250 </span></a> 
<a name="l25251"><span class="ln">25251 </span></a> 
<a name="l25252"><span class="ln">25252 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l25253"><span class="ln">25253 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l25254"><span class="ln">25254 </span></a>    distribution). 
<a name="l25255"><span class="ln">25255 </span></a> 
<a name="l25256"><span class="ln">25256 </span></a>    .. math:: 
<a name="l25257"><span class="ln">25257 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l25258"><span class="ln">25258 </span></a> 
<a name="l25259"><span class="ln">25259 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l25260"><span class="ln">25260 </span></a>    unit variance as 
<a name="l25261"><span class="ln">25261 </span></a> 
<a name="l25262"><span class="ln">25262 </span></a>    .. math:: 
<a name="l25263"><span class="ln">25263 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l25264"><span class="ln">25264 </span></a> 
<a name="l25265"><span class="ln">25265 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l25266"><span class="ln">25266 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l25267"><span class="ln">25267 </span></a> 
<a name="l25268"><span class="ln">25268 </span></a>    .. math:: 
<a name="l25269"><span class="ln">25269 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l25270"><span class="ln">25270 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l25271"><span class="ln">25271 </span></a> 
<a name="l25272"><span class="ln">25272 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l25273"><span class="ln">25273 </span></a> 
<a name="l25274"><span class="ln">25274 </span></a> 
<a name="l25275"><span class="ln">25275 </span></a>    Args: 
<a name="l25276"><span class="ln">25276 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l25277"><span class="ln">25277 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l25278"><span class="ln">25278 </span></a> 
<a name="l25279"><span class="ln">25279 </span></a>    Keyword args: 
<a name="l25280"><span class="ln">25280 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25281"><span class="ln">25281 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25282"><span class="ln">25282 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25283"><span class="ln">25283 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l25284"><span class="ln">25284 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25285"><span class="ln">25285 </span></a>            Default: ``torch.strided``. 
<a name="l25286"><span class="ln">25286 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25287"><span class="ln">25287 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25288"><span class="ln">25288 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25289"><span class="ln">25289 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25290"><span class="ln">25290 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25291"><span class="ln">25291 </span></a>            returned tensor. Default: ``False``. 
<a name="l25292"><span class="ln">25292 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25293"><span class="ln">25293 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25294"><span class="ln">25294 </span></a> 
<a name="l25295"><span class="ln">25295 </span></a>    Example:: 
<a name="l25296"><span class="ln">25296 </span></a> 
<a name="l25297"><span class="ln">25297 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l25298"><span class="ln">25298 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l25299"><span class="ln">25299 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l25300"><span class="ln">25300 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l25301"><span class="ln">25301 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l25302"><span class="ln">25302 </span></a> 
<a name="l25303"><span class="ln">25303 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l25304"><span class="ln">25304 </span></a>    &quot;&quot;&quot;</span>
<a name="l25305"><span class="ln">25305 </span></a>
<a name="l25306"><span class="ln">25306 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25307"><span class="ln">25307 </span></a><span class="s2">def </span><span class="s1">randn</span><span class="s3">(</span>
<a name="l25308"><span class="ln">25308 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l25309"><span class="ln">25309 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l25310"><span class="ln">25310 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25311"><span class="ln">25311 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25312"><span class="ln">25312 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25313"><span class="ln">25313 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25314"><span class="ln">25314 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25315"><span class="ln">25315 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25316"><span class="ln">25316 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25317"><span class="ln">25317 </span></a>    randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l25318"><span class="ln">25318 </span></a> 
<a name="l25319"><span class="ln">25319 </span></a> 
<a name="l25320"><span class="ln">25320 </span></a>    Returns a tensor filled with random numbers from a normal distribution 
<a name="l25321"><span class="ln">25321 </span></a>    with mean `0` and variance `1` (also called the standard normal 
<a name="l25322"><span class="ln">25322 </span></a>    distribution). 
<a name="l25323"><span class="ln">25323 </span></a> 
<a name="l25324"><span class="ln">25324 </span></a>    .. math:: 
<a name="l25325"><span class="ln">25325 </span></a>        \text{out}_{i} \sim \mathcal{N}(0, 1) 
<a name="l25326"><span class="ln">25326 </span></a> 
<a name="l25327"><span class="ln">25327 </span></a>    For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and 
<a name="l25328"><span class="ln">25328 </span></a>    unit variance as 
<a name="l25329"><span class="ln">25329 </span></a> 
<a name="l25330"><span class="ln">25330 </span></a>    .. math:: 
<a name="l25331"><span class="ln">25331 </span></a>        \text{out}_{i} \sim \mathcal{CN}(0, 1) 
<a name="l25332"><span class="ln">25332 </span></a> 
<a name="l25333"><span class="ln">25333 </span></a>    This is equivalent to separately sampling the real :math:`(\operatorname{Re})` and imaginary 
<a name="l25334"><span class="ln">25334 </span></a>    :math:`(\operatorname{Im})` part of :math:`\text{out}_i` as 
<a name="l25335"><span class="ln">25335 </span></a> 
<a name="l25336"><span class="ln">25336 </span></a>    .. math:: 
<a name="l25337"><span class="ln">25337 </span></a>        \operatorname{Re}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}),\quad 
<a name="l25338"><span class="ln">25338 </span></a>        \operatorname{Im}(\text{out}_{i}) \sim \mathcal{N}(0, \frac{1}{2}) 
<a name="l25339"><span class="ln">25339 </span></a> 
<a name="l25340"><span class="ln">25340 </span></a>    The shape of the tensor is defined by the variable argument :attr:`size`. 
<a name="l25341"><span class="ln">25341 </span></a> 
<a name="l25342"><span class="ln">25342 </span></a> 
<a name="l25343"><span class="ln">25343 </span></a>    Args: 
<a name="l25344"><span class="ln">25344 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l25345"><span class="ln">25345 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l25346"><span class="ln">25346 </span></a> 
<a name="l25347"><span class="ln">25347 </span></a>    Keyword args: 
<a name="l25348"><span class="ln">25348 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25349"><span class="ln">25349 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25350"><span class="ln">25350 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25351"><span class="ln">25351 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l25352"><span class="ln">25352 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25353"><span class="ln">25353 </span></a>            Default: ``torch.strided``. 
<a name="l25354"><span class="ln">25354 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25355"><span class="ln">25355 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25356"><span class="ln">25356 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25357"><span class="ln">25357 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25358"><span class="ln">25358 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25359"><span class="ln">25359 </span></a>            returned tensor. Default: ``False``. 
<a name="l25360"><span class="ln">25360 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25361"><span class="ln">25361 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25362"><span class="ln">25362 </span></a> 
<a name="l25363"><span class="ln">25363 </span></a>    Example:: 
<a name="l25364"><span class="ln">25364 </span></a> 
<a name="l25365"><span class="ln">25365 </span></a>        &gt;&gt;&gt; torch.randn(4) 
<a name="l25366"><span class="ln">25366 </span></a>        tensor([-2.1436,  0.9966,  2.3426, -0.6366]) 
<a name="l25367"><span class="ln">25367 </span></a>        &gt;&gt;&gt; torch.randn(2, 3) 
<a name="l25368"><span class="ln">25368 </span></a>        tensor([[ 1.5954,  2.8929, -1.0923], 
<a name="l25369"><span class="ln">25369 </span></a>                [ 1.1719, -0.4709, -0.1996]]) 
<a name="l25370"><span class="ln">25370 </span></a> 
<a name="l25371"><span class="ln">25371 </span></a>    .. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution 
<a name="l25372"><span class="ln">25372 </span></a>    &quot;&quot;&quot;</span>
<a name="l25373"><span class="ln">25373 </span></a>
<a name="l25374"><span class="ln">25374 </span></a><span class="s2">def </span><span class="s1">randn_like</span><span class="s3">(</span>
<a name="l25375"><span class="ln">25375 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25376"><span class="ln">25376 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25377"><span class="ln">25377 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25378"><span class="ln">25378 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25379"><span class="ln">25379 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25380"><span class="ln">25380 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25381"><span class="ln">25381 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25382"><span class="ln">25382 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25383"><span class="ln">25383 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25384"><span class="ln">25384 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25385"><span class="ln">25385 </span></a>    randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l25386"><span class="ln">25386 </span></a> 
<a name="l25387"><span class="ln">25387 </span></a>    Returns a tensor with the same size as :attr:`input` that is filled with 
<a name="l25388"><span class="ln">25388 </span></a>    random numbers from a normal distribution with mean 0 and variance 1. Please refer to :func:`torch.randn` for the 
<a name="l25389"><span class="ln">25389 </span></a>    sampling process of complex dtypes. ``torch.randn_like(input)`` is equivalent to 
<a name="l25390"><span class="ln">25390 </span></a>    ``torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l25391"><span class="ln">25391 </span></a> 
<a name="l25392"><span class="ln">25392 </span></a>    Args: 
<a name="l25393"><span class="ln">25393 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l25394"><span class="ln">25394 </span></a> 
<a name="l25395"><span class="ln">25395 </span></a>    Keyword args: 
<a name="l25396"><span class="ln">25396 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l25397"><span class="ln">25397 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l25398"><span class="ln">25398 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l25399"><span class="ln">25399 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l25400"><span class="ln">25400 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25401"><span class="ln">25401 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l25402"><span class="ln">25402 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25403"><span class="ln">25403 </span></a>            returned tensor. Default: ``False``. 
<a name="l25404"><span class="ln">25404 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l25405"><span class="ln">25405 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l25406"><span class="ln">25406 </span></a>    &quot;&quot;&quot;</span>
<a name="l25407"><span class="ln">25407 </span></a>
<a name="l25408"><span class="ln">25408 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25409"><span class="ln">25409 </span></a><span class="s2">def </span><span class="s1">randperm</span><span class="s3">(</span>
<a name="l25410"><span class="ln">25410 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l25411"><span class="ln">25411 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25412"><span class="ln">25412 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l25413"><span class="ln">25413 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25414"><span class="ln">25414 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25415"><span class="ln">25415 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25416"><span class="ln">25416 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25417"><span class="ln">25417 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25418"><span class="ln">25418 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25419"><span class="ln">25419 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25420"><span class="ln">25420 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25421"><span class="ln">25421 </span></a>    randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l25422"><span class="ln">25422 </span></a> 
<a name="l25423"><span class="ln">25423 </span></a>    Returns a random permutation of integers from ``0`` to ``n - 1``. 
<a name="l25424"><span class="ln">25424 </span></a> 
<a name="l25425"><span class="ln">25425 </span></a>    Args: 
<a name="l25426"><span class="ln">25426 </span></a>        n (int): the upper bound (exclusive) 
<a name="l25427"><span class="ln">25427 </span></a> 
<a name="l25428"><span class="ln">25428 </span></a>    Keyword args: 
<a name="l25429"><span class="ln">25429 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25430"><span class="ln">25430 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25431"><span class="ln">25431 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25432"><span class="ln">25432 </span></a>            Default: ``torch.int64``. 
<a name="l25433"><span class="ln">25433 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25434"><span class="ln">25434 </span></a>            Default: ``torch.strided``. 
<a name="l25435"><span class="ln">25435 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25436"><span class="ln">25436 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25437"><span class="ln">25437 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25438"><span class="ln">25438 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25439"><span class="ln">25439 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25440"><span class="ln">25440 </span></a>            returned tensor. Default: ``False``. 
<a name="l25441"><span class="ln">25441 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25442"><span class="ln">25442 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25443"><span class="ln">25443 </span></a> 
<a name="l25444"><span class="ln">25444 </span></a>    Example:: 
<a name="l25445"><span class="ln">25445 </span></a> 
<a name="l25446"><span class="ln">25446 </span></a>        &gt;&gt;&gt; torch.randperm(4) 
<a name="l25447"><span class="ln">25447 </span></a>        tensor([2, 1, 0, 3]) 
<a name="l25448"><span class="ln">25448 </span></a>    &quot;&quot;&quot;</span>
<a name="l25449"><span class="ln">25449 </span></a>
<a name="l25450"><span class="ln">25450 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25451"><span class="ln">25451 </span></a><span class="s2">def </span><span class="s1">randperm</span><span class="s3">(</span>
<a name="l25452"><span class="ln">25452 </span></a>    <span class="s1">n</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l25453"><span class="ln">25453 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25454"><span class="ln">25454 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25455"><span class="ln">25455 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25456"><span class="ln">25456 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25457"><span class="ln">25457 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25458"><span class="ln">25458 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25459"><span class="ln">25459 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l25460"><span class="ln">25460 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25461"><span class="ln">25461 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25462"><span class="ln">25462 </span></a>    randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l25463"><span class="ln">25463 </span></a> 
<a name="l25464"><span class="ln">25464 </span></a>    Returns a random permutation of integers from ``0`` to ``n - 1``. 
<a name="l25465"><span class="ln">25465 </span></a> 
<a name="l25466"><span class="ln">25466 </span></a>    Args: 
<a name="l25467"><span class="ln">25467 </span></a>        n (int): the upper bound (exclusive) 
<a name="l25468"><span class="ln">25468 </span></a> 
<a name="l25469"><span class="ln">25469 </span></a>    Keyword args: 
<a name="l25470"><span class="ln">25470 </span></a>        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling 
<a name="l25471"><span class="ln">25471 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25472"><span class="ln">25472 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25473"><span class="ln">25473 </span></a>            Default: ``torch.int64``. 
<a name="l25474"><span class="ln">25474 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25475"><span class="ln">25475 </span></a>            Default: ``torch.strided``. 
<a name="l25476"><span class="ln">25476 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25477"><span class="ln">25477 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25478"><span class="ln">25478 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25479"><span class="ln">25479 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25480"><span class="ln">25480 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25481"><span class="ln">25481 </span></a>            returned tensor. Default: ``False``. 
<a name="l25482"><span class="ln">25482 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l25483"><span class="ln">25483 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l25484"><span class="ln">25484 </span></a> 
<a name="l25485"><span class="ln">25485 </span></a>    Example:: 
<a name="l25486"><span class="ln">25486 </span></a> 
<a name="l25487"><span class="ln">25487 </span></a>        &gt;&gt;&gt; torch.randperm(4) 
<a name="l25488"><span class="ln">25488 </span></a>        tensor([2, 1, 0, 3]) 
<a name="l25489"><span class="ln">25489 </span></a>    &quot;&quot;&quot;</span>
<a name="l25490"><span class="ln">25490 </span></a>
<a name="l25491"><span class="ln">25491 </span></a><span class="s2">def </span><span class="s1">range</span><span class="s3">(</span>
<a name="l25492"><span class="ln">25492 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l25493"><span class="ln">25493 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l25494"><span class="ln">25494 </span></a>    <span class="s1">step</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l25495"><span class="ln">25495 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25496"><span class="ln">25496 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25497"><span class="ln">25497 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25498"><span class="ln">25498 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25499"><span class="ln">25499 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l25500"><span class="ln">25500 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l25501"><span class="ln">25501 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25502"><span class="ln">25502 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25503"><span class="ln">25503 </span></a>    range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l25504"><span class="ln">25504 </span></a> 
<a name="l25505"><span class="ln">25505 </span></a>    Returns a 1-D tensor of size :math:`\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1` 
<a name="l25506"><span class="ln">25506 </span></a>    with values from :attr:`start` to :attr:`end` with step :attr:`step`. Step is 
<a name="l25507"><span class="ln">25507 </span></a>    the gap between two values in the tensor. 
<a name="l25508"><span class="ln">25508 </span></a> 
<a name="l25509"><span class="ln">25509 </span></a>    .. math:: 
<a name="l25510"><span class="ln">25510 </span></a>        \text{out}_{i+1} = \text{out}_i + \text{step}. 
<a name="l25511"><span class="ln">25511 </span></a> 
<a name="l25512"><span class="ln">25512 </span></a>    .. warning:: 
<a name="l25513"><span class="ln">25513 </span></a>        This function is deprecated and will be removed in a future release because its behavior is inconsistent with 
<a name="l25514"><span class="ln">25514 </span></a>        Python's range builtin. Instead, use :func:`torch.arange`, which produces values in [start, end). 
<a name="l25515"><span class="ln">25515 </span></a> 
<a name="l25516"><span class="ln">25516 </span></a>    Args: 
<a name="l25517"><span class="ln">25517 </span></a>        start (float, optional): the starting value for the set of points. Default: ``0``. 
<a name="l25518"><span class="ln">25518 </span></a>        end (float): the ending value for the set of points 
<a name="l25519"><span class="ln">25519 </span></a>        step (float, optional): the gap between each pair of adjacent points. Default: ``1``. 
<a name="l25520"><span class="ln">25520 </span></a> 
<a name="l25521"><span class="ln">25521 </span></a>    Keyword args: 
<a name="l25522"><span class="ln">25522 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25523"><span class="ln">25523 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l25524"><span class="ln">25524 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input 
<a name="l25525"><span class="ln">25525 </span></a>            arguments. If any of `start`, `end`, or `step` are floating-point, the 
<a name="l25526"><span class="ln">25526 </span></a>            `dtype` is inferred to be the default dtype, see 
<a name="l25527"><span class="ln">25527 </span></a>            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to 
<a name="l25528"><span class="ln">25528 </span></a>            be `torch.int64`. 
<a name="l25529"><span class="ln">25529 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l25530"><span class="ln">25530 </span></a>            Default: ``torch.strided``. 
<a name="l25531"><span class="ln">25531 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l25532"><span class="ln">25532 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l25533"><span class="ln">25533 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l25534"><span class="ln">25534 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l25535"><span class="ln">25535 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l25536"><span class="ln">25536 </span></a>            returned tensor. Default: ``False``. 
<a name="l25537"><span class="ln">25537 </span></a> 
<a name="l25538"><span class="ln">25538 </span></a>    Example:: 
<a name="l25539"><span class="ln">25539 </span></a> 
<a name="l25540"><span class="ln">25540 </span></a>        &gt;&gt;&gt; torch.range(1, 4) 
<a name="l25541"><span class="ln">25541 </span></a>        tensor([ 1.,  2.,  3.,  4.]) 
<a name="l25542"><span class="ln">25542 </span></a>        &gt;&gt;&gt; torch.range(1, 4, 0.5) 
<a name="l25543"><span class="ln">25543 </span></a>        tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000]) 
<a name="l25544"><span class="ln">25544 </span></a>    &quot;&quot;&quot;</span>
<a name="l25545"><span class="ln">25545 </span></a>
<a name="l25546"><span class="ln">25546 </span></a><span class="s2">def </span><span class="s1">ravel</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25547"><span class="ln">25547 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25548"><span class="ln">25548 </span></a>    ravel(input) -&gt; Tensor 
<a name="l25549"><span class="ln">25549 </span></a> 
<a name="l25550"><span class="ln">25550 </span></a>    Return a contiguous flattened tensor. A copy is made only if needed. 
<a name="l25551"><span class="ln">25551 </span></a> 
<a name="l25552"><span class="ln">25552 </span></a>    Args: 
<a name="l25553"><span class="ln">25553 </span></a>        input (Tensor): the input tensor. 
<a name="l25554"><span class="ln">25554 </span></a> 
<a name="l25555"><span class="ln">25555 </span></a>    Example:: 
<a name="l25556"><span class="ln">25556 </span></a> 
<a name="l25557"><span class="ln">25557 </span></a>        &gt;&gt;&gt; t = torch.tensor([[[1, 2], 
<a name="l25558"><span class="ln">25558 </span></a>        ...                    [3, 4]], 
<a name="l25559"><span class="ln">25559 </span></a>        ...                   [[5, 6], 
<a name="l25560"><span class="ln">25560 </span></a>        ...                    [7, 8]]]) 
<a name="l25561"><span class="ln">25561 </span></a>        &gt;&gt;&gt; torch.ravel(t) 
<a name="l25562"><span class="ln">25562 </span></a>        tensor([1, 2, 3, 4, 5, 6, 7, 8]) 
<a name="l25563"><span class="ln">25563 </span></a>    &quot;&quot;&quot;</span>
<a name="l25564"><span class="ln">25564 </span></a>
<a name="l25565"><span class="ln">25565 </span></a><span class="s2">def </span><span class="s1">real</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25566"><span class="ln">25566 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25567"><span class="ln">25567 </span></a>    real(input) -&gt; Tensor 
<a name="l25568"><span class="ln">25568 </span></a> 
<a name="l25569"><span class="ln">25569 </span></a>    Returns a new tensor containing real values of the :attr:`self` tensor. 
<a name="l25570"><span class="ln">25570 </span></a>    The returned tensor and :attr:`self` share the same underlying storage. 
<a name="l25571"><span class="ln">25571 </span></a> 
<a name="l25572"><span class="ln">25572 </span></a>    Args: 
<a name="l25573"><span class="ln">25573 </span></a>        input (Tensor): the input tensor. 
<a name="l25574"><span class="ln">25574 </span></a> 
<a name="l25575"><span class="ln">25575 </span></a>    Example:: 
<a name="l25576"><span class="ln">25576 </span></a> 
<a name="l25577"><span class="ln">25577 </span></a>        &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l25578"><span class="ln">25578 </span></a>        &gt;&gt;&gt; x 
<a name="l25579"><span class="ln">25579 </span></a>        tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)]) 
<a name="l25580"><span class="ln">25580 </span></a>        &gt;&gt;&gt; x.real 
<a name="l25581"><span class="ln">25581 </span></a>        tensor([ 0.3100, -0.5445, -1.6492, -0.0638]) 
<a name="l25582"><span class="ln">25582 </span></a>    &quot;&quot;&quot;</span>
<a name="l25583"><span class="ln">25583 </span></a>
<a name="l25584"><span class="ln">25584 </span></a><span class="s2">def </span><span class="s1">reciprocal</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25585"><span class="ln">25585 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25586"><span class="ln">25586 </span></a>    reciprocal(input, *, out=None) -&gt; Tensor 
<a name="l25587"><span class="ln">25587 </span></a> 
<a name="l25588"><span class="ln">25588 </span></a>    Returns a new tensor with the reciprocal of the elements of :attr:`input` 
<a name="l25589"><span class="ln">25589 </span></a> 
<a name="l25590"><span class="ln">25590 </span></a>    .. math:: 
<a name="l25591"><span class="ln">25591 </span></a>        \text{out}_{i} = \frac{1}{\text{input}_{i}} 
<a name="l25592"><span class="ln">25592 </span></a> 
<a name="l25593"><span class="ln">25593 </span></a>    .. note:: 
<a name="l25594"><span class="ln">25594 </span></a>        Unlike NumPy's reciprocal, torch.reciprocal supports integral inputs. Integral 
<a name="l25595"><span class="ln">25595 </span></a>        inputs to reciprocal are automatically :ref:`promoted &lt;type-promotion-doc&gt;` to 
<a name="l25596"><span class="ln">25596 </span></a>        the default scalar type. 
<a name="l25597"><span class="ln">25597 </span></a> 
<a name="l25598"><span class="ln">25598 </span></a>    Args: 
<a name="l25599"><span class="ln">25599 </span></a>        input (Tensor): the input tensor. 
<a name="l25600"><span class="ln">25600 </span></a> 
<a name="l25601"><span class="ln">25601 </span></a>    Keyword args: 
<a name="l25602"><span class="ln">25602 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25603"><span class="ln">25603 </span></a> 
<a name="l25604"><span class="ln">25604 </span></a>    Example:: 
<a name="l25605"><span class="ln">25605 </span></a> 
<a name="l25606"><span class="ln">25606 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l25607"><span class="ln">25607 </span></a>        &gt;&gt;&gt; a 
<a name="l25608"><span class="ln">25608 </span></a>        tensor([-0.4595, -2.1219, -1.4314,  0.7298]) 
<a name="l25609"><span class="ln">25609 </span></a>        &gt;&gt;&gt; torch.reciprocal(a) 
<a name="l25610"><span class="ln">25610 </span></a>        tensor([-2.1763, -0.4713, -0.6986,  1.3702]) 
<a name="l25611"><span class="ln">25611 </span></a>    &quot;&quot;&quot;</span>
<a name="l25612"><span class="ln">25612 </span></a>
<a name="l25613"><span class="ln">25613 </span></a><span class="s2">def </span><span class="s1">reciprocal_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l25614"><span class="ln">25614 </span></a><span class="s2">def </span><span class="s1">relu</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l25615"><span class="ln">25615 </span></a><span class="s2">def </span><span class="s1">relu_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l25616"><span class="ln">25616 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25617"><span class="ln">25617 </span></a><span class="s2">def </span><span class="s1">remainder</span><span class="s3">(</span>
<a name="l25618"><span class="ln">25618 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25619"><span class="ln">25619 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25620"><span class="ln">25620 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25621"><span class="ln">25621 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25622"><span class="ln">25622 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25623"><span class="ln">25623 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25624"><span class="ln">25624 </span></a>    remainder(input, other, *, out=None) -&gt; Tensor 
<a name="l25625"><span class="ln">25625 </span></a> 
<a name="l25626"><span class="ln">25626 </span></a>    Computes 
<a name="l25627"><span class="ln">25627 </span></a>    `Python's modulus operation &lt;https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations&gt;`_ 
<a name="l25628"><span class="ln">25628 </span></a>    entrywise.  The result has the same sign as the divisor :attr:`other` and its absolute value 
<a name="l25629"><span class="ln">25629 </span></a>    is less than that of :attr:`other`. 
<a name="l25630"><span class="ln">25630 </span></a> 
<a name="l25631"><span class="ln">25631 </span></a>    It may also be defined in terms of :func:`torch.div` as 
<a name="l25632"><span class="ln">25632 </span></a> 
<a name="l25633"><span class="ln">25633 </span></a>    .. code:: python 
<a name="l25634"><span class="ln">25634 </span></a> 
<a name="l25635"><span class="ln">25635 </span></a>        torch.remainder(a, b) == a - a.div(b, rounding_mode=&quot;floor&quot;) * b 
<a name="l25636"><span class="ln">25636 </span></a> 
<a name="l25637"><span class="ln">25637 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l25638"><span class="ln">25638 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and float inputs. 
<a name="l25639"><span class="ln">25639 </span></a> 
<a name="l25640"><span class="ln">25640 </span></a>    .. note:: 
<a name="l25641"><span class="ln">25641 </span></a>        Complex inputs are not supported. In some cases, it is not mathematically 
<a name="l25642"><span class="ln">25642 </span></a>        possible to satisfy the definition of a modulo operation with complex numbers. 
<a name="l25643"><span class="ln">25643 </span></a>        See :func:`torch.fmod` for how division by zero is handled. 
<a name="l25644"><span class="ln">25644 </span></a> 
<a name="l25645"><span class="ln">25645 </span></a>    .. seealso:: 
<a name="l25646"><span class="ln">25646 </span></a> 
<a name="l25647"><span class="ln">25647 </span></a>        :func:`torch.fmod` which implements C++'s `std::fmod &lt;https://en.cppreference.com/w/cpp/numeric/math/fmod&gt;`_. 
<a name="l25648"><span class="ln">25648 </span></a>        This one is defined in terms of division rounding towards zero. 
<a name="l25649"><span class="ln">25649 </span></a> 
<a name="l25650"><span class="ln">25650 </span></a>    Args: 
<a name="l25651"><span class="ln">25651 </span></a>        input (Tensor or Scalar): the dividend 
<a name="l25652"><span class="ln">25652 </span></a>        other (Tensor or Scalar): the divisor 
<a name="l25653"><span class="ln">25653 </span></a> 
<a name="l25654"><span class="ln">25654 </span></a>    Keyword args: 
<a name="l25655"><span class="ln">25655 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25656"><span class="ln">25656 </span></a> 
<a name="l25657"><span class="ln">25657 </span></a>    Example:: 
<a name="l25658"><span class="ln">25658 </span></a> 
<a name="l25659"><span class="ln">25659 </span></a>        &gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) 
<a name="l25660"><span class="ln">25660 </span></a>        tensor([ 1.,  0.,  1.,  1.,  0.,  1.]) 
<a name="l25661"><span class="ln">25661 </span></a>        &gt;&gt;&gt; torch.remainder(torch.tensor([1, 2, 3, 4, 5]), -1.5) 
<a name="l25662"><span class="ln">25662 </span></a>        tensor([ -0.5000, -1.0000,  0.0000, -0.5000, -1.0000 ]) 
<a name="l25663"><span class="ln">25663 </span></a>    &quot;&quot;&quot;</span>
<a name="l25664"><span class="ln">25664 </span></a>
<a name="l25665"><span class="ln">25665 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25666"><span class="ln">25666 </span></a><span class="s2">def </span><span class="s1">remainder</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25667"><span class="ln">25667 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25668"><span class="ln">25668 </span></a>    remainder(input, other, *, out=None) -&gt; Tensor 
<a name="l25669"><span class="ln">25669 </span></a> 
<a name="l25670"><span class="ln">25670 </span></a>    Computes 
<a name="l25671"><span class="ln">25671 </span></a>    `Python's modulus operation &lt;https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations&gt;`_ 
<a name="l25672"><span class="ln">25672 </span></a>    entrywise.  The result has the same sign as the divisor :attr:`other` and its absolute value 
<a name="l25673"><span class="ln">25673 </span></a>    is less than that of :attr:`other`. 
<a name="l25674"><span class="ln">25674 </span></a> 
<a name="l25675"><span class="ln">25675 </span></a>    It may also be defined in terms of :func:`torch.div` as 
<a name="l25676"><span class="ln">25676 </span></a> 
<a name="l25677"><span class="ln">25677 </span></a>    .. code:: python 
<a name="l25678"><span class="ln">25678 </span></a> 
<a name="l25679"><span class="ln">25679 </span></a>        torch.remainder(a, b) == a - a.div(b, rounding_mode=&quot;floor&quot;) * b 
<a name="l25680"><span class="ln">25680 </span></a> 
<a name="l25681"><span class="ln">25681 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l25682"><span class="ln">25682 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and float inputs. 
<a name="l25683"><span class="ln">25683 </span></a> 
<a name="l25684"><span class="ln">25684 </span></a>    .. note:: 
<a name="l25685"><span class="ln">25685 </span></a>        Complex inputs are not supported. In some cases, it is not mathematically 
<a name="l25686"><span class="ln">25686 </span></a>        possible to satisfy the definition of a modulo operation with complex numbers. 
<a name="l25687"><span class="ln">25687 </span></a>        See :func:`torch.fmod` for how division by zero is handled. 
<a name="l25688"><span class="ln">25688 </span></a> 
<a name="l25689"><span class="ln">25689 </span></a>    .. seealso:: 
<a name="l25690"><span class="ln">25690 </span></a> 
<a name="l25691"><span class="ln">25691 </span></a>        :func:`torch.fmod` which implements C++'s `std::fmod &lt;https://en.cppreference.com/w/cpp/numeric/math/fmod&gt;`_. 
<a name="l25692"><span class="ln">25692 </span></a>        This one is defined in terms of division rounding towards zero. 
<a name="l25693"><span class="ln">25693 </span></a> 
<a name="l25694"><span class="ln">25694 </span></a>    Args: 
<a name="l25695"><span class="ln">25695 </span></a>        input (Tensor or Scalar): the dividend 
<a name="l25696"><span class="ln">25696 </span></a>        other (Tensor or Scalar): the divisor 
<a name="l25697"><span class="ln">25697 </span></a> 
<a name="l25698"><span class="ln">25698 </span></a>    Keyword args: 
<a name="l25699"><span class="ln">25699 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25700"><span class="ln">25700 </span></a> 
<a name="l25701"><span class="ln">25701 </span></a>    Example:: 
<a name="l25702"><span class="ln">25702 </span></a> 
<a name="l25703"><span class="ln">25703 </span></a>        &gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) 
<a name="l25704"><span class="ln">25704 </span></a>        tensor([ 1.,  0.,  1.,  1.,  0.,  1.]) 
<a name="l25705"><span class="ln">25705 </span></a>        &gt;&gt;&gt; torch.remainder(torch.tensor([1, 2, 3, 4, 5]), -1.5) 
<a name="l25706"><span class="ln">25706 </span></a>        tensor([ -0.5000, -1.0000,  0.0000, -0.5000, -1.0000 ]) 
<a name="l25707"><span class="ln">25707 </span></a>    &quot;&quot;&quot;</span>
<a name="l25708"><span class="ln">25708 </span></a>
<a name="l25709"><span class="ln">25709 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25710"><span class="ln">25710 </span></a><span class="s2">def </span><span class="s1">remainder</span><span class="s3">(</span>
<a name="l25711"><span class="ln">25711 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25712"><span class="ln">25712 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l25713"><span class="ln">25713 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25714"><span class="ln">25714 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25715"><span class="ln">25715 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25716"><span class="ln">25716 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25717"><span class="ln">25717 </span></a>    remainder(input, other, *, out=None) -&gt; Tensor 
<a name="l25718"><span class="ln">25718 </span></a> 
<a name="l25719"><span class="ln">25719 </span></a>    Computes 
<a name="l25720"><span class="ln">25720 </span></a>    `Python's modulus operation &lt;https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations&gt;`_ 
<a name="l25721"><span class="ln">25721 </span></a>    entrywise.  The result has the same sign as the divisor :attr:`other` and its absolute value 
<a name="l25722"><span class="ln">25722 </span></a>    is less than that of :attr:`other`. 
<a name="l25723"><span class="ln">25723 </span></a> 
<a name="l25724"><span class="ln">25724 </span></a>    It may also be defined in terms of :func:`torch.div` as 
<a name="l25725"><span class="ln">25725 </span></a> 
<a name="l25726"><span class="ln">25726 </span></a>    .. code:: python 
<a name="l25727"><span class="ln">25727 </span></a> 
<a name="l25728"><span class="ln">25728 </span></a>        torch.remainder(a, b) == a - a.div(b, rounding_mode=&quot;floor&quot;) * b 
<a name="l25729"><span class="ln">25729 </span></a> 
<a name="l25730"><span class="ln">25730 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l25731"><span class="ln">25731 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer and float inputs. 
<a name="l25732"><span class="ln">25732 </span></a> 
<a name="l25733"><span class="ln">25733 </span></a>    .. note:: 
<a name="l25734"><span class="ln">25734 </span></a>        Complex inputs are not supported. In some cases, it is not mathematically 
<a name="l25735"><span class="ln">25735 </span></a>        possible to satisfy the definition of a modulo operation with complex numbers. 
<a name="l25736"><span class="ln">25736 </span></a>        See :func:`torch.fmod` for how division by zero is handled. 
<a name="l25737"><span class="ln">25737 </span></a> 
<a name="l25738"><span class="ln">25738 </span></a>    .. seealso:: 
<a name="l25739"><span class="ln">25739 </span></a> 
<a name="l25740"><span class="ln">25740 </span></a>        :func:`torch.fmod` which implements C++'s `std::fmod &lt;https://en.cppreference.com/w/cpp/numeric/math/fmod&gt;`_. 
<a name="l25741"><span class="ln">25741 </span></a>        This one is defined in terms of division rounding towards zero. 
<a name="l25742"><span class="ln">25742 </span></a> 
<a name="l25743"><span class="ln">25743 </span></a>    Args: 
<a name="l25744"><span class="ln">25744 </span></a>        input (Tensor or Scalar): the dividend 
<a name="l25745"><span class="ln">25745 </span></a>        other (Tensor or Scalar): the divisor 
<a name="l25746"><span class="ln">25746 </span></a> 
<a name="l25747"><span class="ln">25747 </span></a>    Keyword args: 
<a name="l25748"><span class="ln">25748 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25749"><span class="ln">25749 </span></a> 
<a name="l25750"><span class="ln">25750 </span></a>    Example:: 
<a name="l25751"><span class="ln">25751 </span></a> 
<a name="l25752"><span class="ln">25752 </span></a>        &gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) 
<a name="l25753"><span class="ln">25753 </span></a>        tensor([ 1.,  0.,  1.,  1.,  0.,  1.]) 
<a name="l25754"><span class="ln">25754 </span></a>        &gt;&gt;&gt; torch.remainder(torch.tensor([1, 2, 3, 4, 5]), -1.5) 
<a name="l25755"><span class="ln">25755 </span></a>        tensor([ -0.5000, -1.0000,  0.0000, -0.5000, -1.0000 ]) 
<a name="l25756"><span class="ln">25756 </span></a>    &quot;&quot;&quot;</span>
<a name="l25757"><span class="ln">25757 </span></a>
<a name="l25758"><span class="ln">25758 </span></a><span class="s2">def </span><span class="s1">renorm</span><span class="s3">(</span>
<a name="l25759"><span class="ln">25759 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25760"><span class="ln">25760 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l25761"><span class="ln">25761 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l25762"><span class="ln">25762 </span></a>    <span class="s1">maxnorm</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l25763"><span class="ln">25763 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25764"><span class="ln">25764 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25765"><span class="ln">25765 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25766"><span class="ln">25766 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25767"><span class="ln">25767 </span></a>    renorm(input, p, dim, maxnorm, *, out=None) -&gt; Tensor 
<a name="l25768"><span class="ln">25768 </span></a> 
<a name="l25769"><span class="ln">25769 </span></a>    Returns a tensor where each sub-tensor of :attr:`input` along dimension 
<a name="l25770"><span class="ln">25770 </span></a>    :attr:`dim` is normalized such that the `p`-norm of the sub-tensor is lower 
<a name="l25771"><span class="ln">25771 </span></a>    than the value :attr:`maxnorm` 
<a name="l25772"><span class="ln">25772 </span></a> 
<a name="l25773"><span class="ln">25773 </span></a>    .. note:: If the norm of a row is lower than `maxnorm`, the row is unchanged 
<a name="l25774"><span class="ln">25774 </span></a> 
<a name="l25775"><span class="ln">25775 </span></a>    Args: 
<a name="l25776"><span class="ln">25776 </span></a>        input (Tensor): the input tensor. 
<a name="l25777"><span class="ln">25777 </span></a>        p (float): the power for the norm computation 
<a name="l25778"><span class="ln">25778 </span></a>        dim (int): the dimension to slice over to get the sub-tensors 
<a name="l25779"><span class="ln">25779 </span></a>        maxnorm (float): the maximum norm to keep each sub-tensor under 
<a name="l25780"><span class="ln">25780 </span></a> 
<a name="l25781"><span class="ln">25781 </span></a>    Keyword args: 
<a name="l25782"><span class="ln">25782 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l25783"><span class="ln">25783 </span></a> 
<a name="l25784"><span class="ln">25784 </span></a>    Example:: 
<a name="l25785"><span class="ln">25785 </span></a> 
<a name="l25786"><span class="ln">25786 </span></a>        &gt;&gt;&gt; x = torch.ones(3, 3) 
<a name="l25787"><span class="ln">25787 </span></a>        &gt;&gt;&gt; x[1].fill_(2) 
<a name="l25788"><span class="ln">25788 </span></a>        tensor([ 2.,  2.,  2.]) 
<a name="l25789"><span class="ln">25789 </span></a>        &gt;&gt;&gt; x[2].fill_(3) 
<a name="l25790"><span class="ln">25790 </span></a>        tensor([ 3.,  3.,  3.]) 
<a name="l25791"><span class="ln">25791 </span></a>        &gt;&gt;&gt; x 
<a name="l25792"><span class="ln">25792 </span></a>        tensor([[ 1.,  1.,  1.], 
<a name="l25793"><span class="ln">25793 </span></a>                [ 2.,  2.,  2.], 
<a name="l25794"><span class="ln">25794 </span></a>                [ 3.,  3.,  3.]]) 
<a name="l25795"><span class="ln">25795 </span></a>        &gt;&gt;&gt; torch.renorm(x, 1, 0, 5) 
<a name="l25796"><span class="ln">25796 </span></a>        tensor([[ 1.0000,  1.0000,  1.0000], 
<a name="l25797"><span class="ln">25797 </span></a>                [ 1.6667,  1.6667,  1.6667], 
<a name="l25798"><span class="ln">25798 </span></a>                [ 1.6667,  1.6667,  1.6667]]) 
<a name="l25799"><span class="ln">25799 </span></a>    &quot;&quot;&quot;</span>
<a name="l25800"><span class="ln">25800 </span></a>
<a name="l25801"><span class="ln">25801 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25802"><span class="ln">25802 </span></a><span class="s2">def </span><span class="s1">repeat_interleave</span><span class="s3">(</span>
<a name="l25803"><span class="ln">25803 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25804"><span class="ln">25804 </span></a>    <span class="s1">repeats</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25805"><span class="ln">25805 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25806"><span class="ln">25806 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25807"><span class="ln">25807 </span></a>    <span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25808"><span class="ln">25808 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25809"><span class="ln">25809 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25810"><span class="ln">25810 </span></a>    repeat_interleave(input, repeats, dim=None, *, output_size=None) -&gt; Tensor 
<a name="l25811"><span class="ln">25811 </span></a> 
<a name="l25812"><span class="ln">25812 </span></a>    Repeat elements of a tensor. 
<a name="l25813"><span class="ln">25813 </span></a> 
<a name="l25814"><span class="ln">25814 </span></a>    .. warning:: 
<a name="l25815"><span class="ln">25815 </span></a> 
<a name="l25816"><span class="ln">25816 </span></a>        This is different from :meth:`torch.Tensor.repeat` but similar to ``numpy.repeat``. 
<a name="l25817"><span class="ln">25817 </span></a> 
<a name="l25818"><span class="ln">25818 </span></a>    Args: 
<a name="l25819"><span class="ln">25819 </span></a>        input (Tensor): the input tensor. 
<a name="l25820"><span class="ln">25820 </span></a>        repeats (Tensor or int): The number of repetitions for each element. 
<a name="l25821"><span class="ln">25821 </span></a>            repeats is broadcasted to fit the shape of the given axis. 
<a name="l25822"><span class="ln">25822 </span></a>        dim (int, optional): The dimension along which to repeat values. 
<a name="l25823"><span class="ln">25823 </span></a>            By default, use the flattened input array, and return a flat output 
<a name="l25824"><span class="ln">25824 </span></a>            array. 
<a name="l25825"><span class="ln">25825 </span></a> 
<a name="l25826"><span class="ln">25826 </span></a>    Keyword args: 
<a name="l25827"><span class="ln">25827 </span></a>        output_size (int, optional): Total output size for the given axis 
<a name="l25828"><span class="ln">25828 </span></a>            ( e.g. sum of repeats). If given, it will avoid stream synchronization 
<a name="l25829"><span class="ln">25829 </span></a>            needed to calculate output shape of the tensor. 
<a name="l25830"><span class="ln">25830 </span></a> 
<a name="l25831"><span class="ln">25831 </span></a>    Returns: 
<a name="l25832"><span class="ln">25832 </span></a>        Tensor: Repeated tensor which has the same shape as input, except along the given axis. 
<a name="l25833"><span class="ln">25833 </span></a> 
<a name="l25834"><span class="ln">25834 </span></a>    Example:: 
<a name="l25835"><span class="ln">25835 </span></a> 
<a name="l25836"><span class="ln">25836 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l25837"><span class="ln">25837 </span></a>        &gt;&gt;&gt; x.repeat_interleave(2) 
<a name="l25838"><span class="ln">25838 </span></a>        tensor([1, 1, 2, 2, 3, 3]) 
<a name="l25839"><span class="ln">25839 </span></a>        &gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]]) 
<a name="l25840"><span class="ln">25840 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, 2) 
<a name="l25841"><span class="ln">25841 </span></a>        tensor([1, 1, 2, 2, 3, 3, 4, 4]) 
<a name="l25842"><span class="ln">25842 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1) 
<a name="l25843"><span class="ln">25843 </span></a>        tensor([[1, 1, 1, 2, 2, 2], 
<a name="l25844"><span class="ln">25844 </span></a>                [3, 3, 3, 4, 4, 4]]) 
<a name="l25845"><span class="ln">25845 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0) 
<a name="l25846"><span class="ln">25846 </span></a>        tensor([[1, 2], 
<a name="l25847"><span class="ln">25847 </span></a>                [3, 4], 
<a name="l25848"><span class="ln">25848 </span></a>                [3, 4]]) 
<a name="l25849"><span class="ln">25849 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0, output_size=3) 
<a name="l25850"><span class="ln">25850 </span></a>        tensor([[1, 2], 
<a name="l25851"><span class="ln">25851 </span></a>                [3, 4], 
<a name="l25852"><span class="ln">25852 </span></a>                [3, 4]]) 
<a name="l25853"><span class="ln">25853 </span></a> 
<a name="l25854"><span class="ln">25854 </span></a>    If the `repeats` is `tensor([n1, n2, n3, ...])`, then the output will be 
<a name="l25855"><span class="ln">25855 </span></a>    `tensor([0, 0, ..., 1, 1, ..., 2, 2, ..., ...])` where `0` appears `n1` times, 
<a name="l25856"><span class="ln">25856 </span></a>    `1` appears `n2` times, `2` appears `n3` times, etc. 
<a name="l25857"><span class="ln">25857 </span></a> 
<a name="l25858"><span class="ln">25858 </span></a>    .. function:: repeat_interleave(repeats, *) -&gt; Tensor 
<a name="l25859"><span class="ln">25859 </span></a>       :noindex: 
<a name="l25860"><span class="ln">25860 </span></a> 
<a name="l25861"><span class="ln">25861 </span></a>    Repeats 0 repeats[0] times, 1 repeats[1] times, 2 repeats[2] times, etc. 
<a name="l25862"><span class="ln">25862 </span></a> 
<a name="l25863"><span class="ln">25863 </span></a>    Args: 
<a name="l25864"><span class="ln">25864 </span></a>        repeats (Tensor): The number of repetitions for each element. 
<a name="l25865"><span class="ln">25865 </span></a> 
<a name="l25866"><span class="ln">25866 </span></a>    Returns: 
<a name="l25867"><span class="ln">25867 </span></a>        Tensor: Repeated tensor of size `sum(repeats)`. 
<a name="l25868"><span class="ln">25868 </span></a> 
<a name="l25869"><span class="ln">25869 </span></a>    Example:: 
<a name="l25870"><span class="ln">25870 </span></a> 
<a name="l25871"><span class="ln">25871 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(torch.tensor([1, 2, 3])) 
<a name="l25872"><span class="ln">25872 </span></a>        tensor([0, 1, 1, 2, 2, 2]) 
<a name="l25873"><span class="ln">25873 </span></a>    &quot;&quot;&quot;</span>
<a name="l25874"><span class="ln">25874 </span></a>
<a name="l25875"><span class="ln">25875 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25876"><span class="ln">25876 </span></a><span class="s2">def </span><span class="s1">repeat_interleave</span><span class="s3">(</span>
<a name="l25877"><span class="ln">25877 </span></a>    <span class="s1">repeats</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25878"><span class="ln">25878 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25879"><span class="ln">25879 </span></a>    <span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25880"><span class="ln">25880 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25881"><span class="ln">25881 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25882"><span class="ln">25882 </span></a>    repeat_interleave(input, repeats, dim=None, *, output_size=None) -&gt; Tensor 
<a name="l25883"><span class="ln">25883 </span></a> 
<a name="l25884"><span class="ln">25884 </span></a>    Repeat elements of a tensor. 
<a name="l25885"><span class="ln">25885 </span></a> 
<a name="l25886"><span class="ln">25886 </span></a>    .. warning:: 
<a name="l25887"><span class="ln">25887 </span></a> 
<a name="l25888"><span class="ln">25888 </span></a>        This is different from :meth:`torch.Tensor.repeat` but similar to ``numpy.repeat``. 
<a name="l25889"><span class="ln">25889 </span></a> 
<a name="l25890"><span class="ln">25890 </span></a>    Args: 
<a name="l25891"><span class="ln">25891 </span></a>        input (Tensor): the input tensor. 
<a name="l25892"><span class="ln">25892 </span></a>        repeats (Tensor or int): The number of repetitions for each element. 
<a name="l25893"><span class="ln">25893 </span></a>            repeats is broadcasted to fit the shape of the given axis. 
<a name="l25894"><span class="ln">25894 </span></a>        dim (int, optional): The dimension along which to repeat values. 
<a name="l25895"><span class="ln">25895 </span></a>            By default, use the flattened input array, and return a flat output 
<a name="l25896"><span class="ln">25896 </span></a>            array. 
<a name="l25897"><span class="ln">25897 </span></a> 
<a name="l25898"><span class="ln">25898 </span></a>    Keyword args: 
<a name="l25899"><span class="ln">25899 </span></a>        output_size (int, optional): Total output size for the given axis 
<a name="l25900"><span class="ln">25900 </span></a>            ( e.g. sum of repeats). If given, it will avoid stream synchronization 
<a name="l25901"><span class="ln">25901 </span></a>            needed to calculate output shape of the tensor. 
<a name="l25902"><span class="ln">25902 </span></a> 
<a name="l25903"><span class="ln">25903 </span></a>    Returns: 
<a name="l25904"><span class="ln">25904 </span></a>        Tensor: Repeated tensor which has the same shape as input, except along the given axis. 
<a name="l25905"><span class="ln">25905 </span></a> 
<a name="l25906"><span class="ln">25906 </span></a>    Example:: 
<a name="l25907"><span class="ln">25907 </span></a> 
<a name="l25908"><span class="ln">25908 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l25909"><span class="ln">25909 </span></a>        &gt;&gt;&gt; x.repeat_interleave(2) 
<a name="l25910"><span class="ln">25910 </span></a>        tensor([1, 1, 2, 2, 3, 3]) 
<a name="l25911"><span class="ln">25911 </span></a>        &gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]]) 
<a name="l25912"><span class="ln">25912 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, 2) 
<a name="l25913"><span class="ln">25913 </span></a>        tensor([1, 1, 2, 2, 3, 3, 4, 4]) 
<a name="l25914"><span class="ln">25914 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1) 
<a name="l25915"><span class="ln">25915 </span></a>        tensor([[1, 1, 1, 2, 2, 2], 
<a name="l25916"><span class="ln">25916 </span></a>                [3, 3, 3, 4, 4, 4]]) 
<a name="l25917"><span class="ln">25917 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0) 
<a name="l25918"><span class="ln">25918 </span></a>        tensor([[1, 2], 
<a name="l25919"><span class="ln">25919 </span></a>                [3, 4], 
<a name="l25920"><span class="ln">25920 </span></a>                [3, 4]]) 
<a name="l25921"><span class="ln">25921 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0, output_size=3) 
<a name="l25922"><span class="ln">25922 </span></a>        tensor([[1, 2], 
<a name="l25923"><span class="ln">25923 </span></a>                [3, 4], 
<a name="l25924"><span class="ln">25924 </span></a>                [3, 4]]) 
<a name="l25925"><span class="ln">25925 </span></a> 
<a name="l25926"><span class="ln">25926 </span></a>    If the `repeats` is `tensor([n1, n2, n3, ...])`, then the output will be 
<a name="l25927"><span class="ln">25927 </span></a>    `tensor([0, 0, ..., 1, 1, ..., 2, 2, ..., ...])` where `0` appears `n1` times, 
<a name="l25928"><span class="ln">25928 </span></a>    `1` appears `n2` times, `2` appears `n3` times, etc. 
<a name="l25929"><span class="ln">25929 </span></a> 
<a name="l25930"><span class="ln">25930 </span></a>    .. function:: repeat_interleave(repeats, *) -&gt; Tensor 
<a name="l25931"><span class="ln">25931 </span></a>       :noindex: 
<a name="l25932"><span class="ln">25932 </span></a> 
<a name="l25933"><span class="ln">25933 </span></a>    Repeats 0 repeats[0] times, 1 repeats[1] times, 2 repeats[2] times, etc. 
<a name="l25934"><span class="ln">25934 </span></a> 
<a name="l25935"><span class="ln">25935 </span></a>    Args: 
<a name="l25936"><span class="ln">25936 </span></a>        repeats (Tensor): The number of repetitions for each element. 
<a name="l25937"><span class="ln">25937 </span></a> 
<a name="l25938"><span class="ln">25938 </span></a>    Returns: 
<a name="l25939"><span class="ln">25939 </span></a>        Tensor: Repeated tensor of size `sum(repeats)`. 
<a name="l25940"><span class="ln">25940 </span></a> 
<a name="l25941"><span class="ln">25941 </span></a>    Example:: 
<a name="l25942"><span class="ln">25942 </span></a> 
<a name="l25943"><span class="ln">25943 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(torch.tensor([1, 2, 3])) 
<a name="l25944"><span class="ln">25944 </span></a>        tensor([0, 1, 1, 2, 2, 2]) 
<a name="l25945"><span class="ln">25945 </span></a>    &quot;&quot;&quot;</span>
<a name="l25946"><span class="ln">25946 </span></a>
<a name="l25947"><span class="ln">25947 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l25948"><span class="ln">25948 </span></a><span class="s2">def </span><span class="s1">repeat_interleave</span><span class="s3">(</span>
<a name="l25949"><span class="ln">25949 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l25950"><span class="ln">25950 </span></a>    <span class="s1">repeats</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l25951"><span class="ln">25951 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25952"><span class="ln">25952 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l25953"><span class="ln">25953 </span></a>    <span class="s1">output_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l25954"><span class="ln">25954 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l25955"><span class="ln">25955 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l25956"><span class="ln">25956 </span></a>    repeat_interleave(input, repeats, dim=None, *, output_size=None) -&gt; Tensor 
<a name="l25957"><span class="ln">25957 </span></a> 
<a name="l25958"><span class="ln">25958 </span></a>    Repeat elements of a tensor. 
<a name="l25959"><span class="ln">25959 </span></a> 
<a name="l25960"><span class="ln">25960 </span></a>    .. warning:: 
<a name="l25961"><span class="ln">25961 </span></a> 
<a name="l25962"><span class="ln">25962 </span></a>        This is different from :meth:`torch.Tensor.repeat` but similar to ``numpy.repeat``. 
<a name="l25963"><span class="ln">25963 </span></a> 
<a name="l25964"><span class="ln">25964 </span></a>    Args: 
<a name="l25965"><span class="ln">25965 </span></a>        input (Tensor): the input tensor. 
<a name="l25966"><span class="ln">25966 </span></a>        repeats (Tensor or int): The number of repetitions for each element. 
<a name="l25967"><span class="ln">25967 </span></a>            repeats is broadcasted to fit the shape of the given axis. 
<a name="l25968"><span class="ln">25968 </span></a>        dim (int, optional): The dimension along which to repeat values. 
<a name="l25969"><span class="ln">25969 </span></a>            By default, use the flattened input array, and return a flat output 
<a name="l25970"><span class="ln">25970 </span></a>            array. 
<a name="l25971"><span class="ln">25971 </span></a> 
<a name="l25972"><span class="ln">25972 </span></a>    Keyword args: 
<a name="l25973"><span class="ln">25973 </span></a>        output_size (int, optional): Total output size for the given axis 
<a name="l25974"><span class="ln">25974 </span></a>            ( e.g. sum of repeats). If given, it will avoid stream synchronization 
<a name="l25975"><span class="ln">25975 </span></a>            needed to calculate output shape of the tensor. 
<a name="l25976"><span class="ln">25976 </span></a> 
<a name="l25977"><span class="ln">25977 </span></a>    Returns: 
<a name="l25978"><span class="ln">25978 </span></a>        Tensor: Repeated tensor which has the same shape as input, except along the given axis. 
<a name="l25979"><span class="ln">25979 </span></a> 
<a name="l25980"><span class="ln">25980 </span></a>    Example:: 
<a name="l25981"><span class="ln">25981 </span></a> 
<a name="l25982"><span class="ln">25982 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l25983"><span class="ln">25983 </span></a>        &gt;&gt;&gt; x.repeat_interleave(2) 
<a name="l25984"><span class="ln">25984 </span></a>        tensor([1, 1, 2, 2, 3, 3]) 
<a name="l25985"><span class="ln">25985 </span></a>        &gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]]) 
<a name="l25986"><span class="ln">25986 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, 2) 
<a name="l25987"><span class="ln">25987 </span></a>        tensor([1, 1, 2, 2, 3, 3, 4, 4]) 
<a name="l25988"><span class="ln">25988 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1) 
<a name="l25989"><span class="ln">25989 </span></a>        tensor([[1, 1, 1, 2, 2, 2], 
<a name="l25990"><span class="ln">25990 </span></a>                [3, 3, 3, 4, 4, 4]]) 
<a name="l25991"><span class="ln">25991 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0) 
<a name="l25992"><span class="ln">25992 </span></a>        tensor([[1, 2], 
<a name="l25993"><span class="ln">25993 </span></a>                [3, 4], 
<a name="l25994"><span class="ln">25994 </span></a>                [3, 4]]) 
<a name="l25995"><span class="ln">25995 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0, output_size=3) 
<a name="l25996"><span class="ln">25996 </span></a>        tensor([[1, 2], 
<a name="l25997"><span class="ln">25997 </span></a>                [3, 4], 
<a name="l25998"><span class="ln">25998 </span></a>                [3, 4]]) 
<a name="l25999"><span class="ln">25999 </span></a> 
<a name="l26000"><span class="ln">26000 </span></a>    If the `repeats` is `tensor([n1, n2, n3, ...])`, then the output will be 
<a name="l26001"><span class="ln">26001 </span></a>    `tensor([0, 0, ..., 1, 1, ..., 2, 2, ..., ...])` where `0` appears `n1` times, 
<a name="l26002"><span class="ln">26002 </span></a>    `1` appears `n2` times, `2` appears `n3` times, etc. 
<a name="l26003"><span class="ln">26003 </span></a> 
<a name="l26004"><span class="ln">26004 </span></a>    .. function:: repeat_interleave(repeats, *) -&gt; Tensor 
<a name="l26005"><span class="ln">26005 </span></a>       :noindex: 
<a name="l26006"><span class="ln">26006 </span></a> 
<a name="l26007"><span class="ln">26007 </span></a>    Repeats 0 repeats[0] times, 1 repeats[1] times, 2 repeats[2] times, etc. 
<a name="l26008"><span class="ln">26008 </span></a> 
<a name="l26009"><span class="ln">26009 </span></a>    Args: 
<a name="l26010"><span class="ln">26010 </span></a>        repeats (Tensor): The number of repetitions for each element. 
<a name="l26011"><span class="ln">26011 </span></a> 
<a name="l26012"><span class="ln">26012 </span></a>    Returns: 
<a name="l26013"><span class="ln">26013 </span></a>        Tensor: Repeated tensor of size `sum(repeats)`. 
<a name="l26014"><span class="ln">26014 </span></a> 
<a name="l26015"><span class="ln">26015 </span></a>    Example:: 
<a name="l26016"><span class="ln">26016 </span></a> 
<a name="l26017"><span class="ln">26017 </span></a>        &gt;&gt;&gt; torch.repeat_interleave(torch.tensor([1, 2, 3])) 
<a name="l26018"><span class="ln">26018 </span></a>        tensor([0, 1, 1, 2, 2, 2]) 
<a name="l26019"><span class="ln">26019 </span></a>    &quot;&quot;&quot;</span>
<a name="l26020"><span class="ln">26020 </span></a>
<a name="l26021"><span class="ln">26021 </span></a><span class="s2">def </span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">shape</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">]) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26022"><span class="ln">26022 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26023"><span class="ln">26023 </span></a>    reshape(input, shape) -&gt; Tensor 
<a name="l26024"><span class="ln">26024 </span></a> 
<a name="l26025"><span class="ln">26025 </span></a>    Returns a tensor with the same data and number of elements as :attr:`input`, 
<a name="l26026"><span class="ln">26026 </span></a>    but with the specified shape. When possible, the returned tensor will be a view 
<a name="l26027"><span class="ln">26027 </span></a>    of :attr:`input`. Otherwise, it will be a copy. Contiguous inputs and inputs 
<a name="l26028"><span class="ln">26028 </span></a>    with compatible strides can be reshaped without copying, but you should not 
<a name="l26029"><span class="ln">26029 </span></a>    depend on the copying vs. viewing behavior. 
<a name="l26030"><span class="ln">26030 </span></a> 
<a name="l26031"><span class="ln">26031 </span></a>    See :meth:`torch.Tensor.view` on when it is possible to return a view. 
<a name="l26032"><span class="ln">26032 </span></a> 
<a name="l26033"><span class="ln">26033 </span></a>    A single dimension may be -1, in which case it's inferred from the remaining 
<a name="l26034"><span class="ln">26034 </span></a>    dimensions and the number of elements in :attr:`input`. 
<a name="l26035"><span class="ln">26035 </span></a> 
<a name="l26036"><span class="ln">26036 </span></a>    Args: 
<a name="l26037"><span class="ln">26037 </span></a>        input (Tensor): the tensor to be reshaped 
<a name="l26038"><span class="ln">26038 </span></a>        shape (tuple of int): the new shape 
<a name="l26039"><span class="ln">26039 </span></a> 
<a name="l26040"><span class="ln">26040 </span></a>    Example:: 
<a name="l26041"><span class="ln">26041 </span></a> 
<a name="l26042"><span class="ln">26042 </span></a>        &gt;&gt;&gt; a = torch.arange(4.) 
<a name="l26043"><span class="ln">26043 </span></a>        &gt;&gt;&gt; torch.reshape(a, (2, 2)) 
<a name="l26044"><span class="ln">26044 </span></a>        tensor([[ 0.,  1.], 
<a name="l26045"><span class="ln">26045 </span></a>                [ 2.,  3.]]) 
<a name="l26046"><span class="ln">26046 </span></a>        &gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]]) 
<a name="l26047"><span class="ln">26047 </span></a>        &gt;&gt;&gt; torch.reshape(b, (-1,)) 
<a name="l26048"><span class="ln">26048 </span></a>        tensor([ 0,  1,  2,  3]) 
<a name="l26049"><span class="ln">26049 </span></a>    &quot;&quot;&quot;</span>
<a name="l26050"><span class="ln">26050 </span></a>
<a name="l26051"><span class="ln">26051 </span></a><span class="s2">def </span><span class="s1">resize_as_</span><span class="s3">(</span>
<a name="l26052"><span class="ln">26052 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26053"><span class="ln">26053 </span></a>    <span class="s1">the_template</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26054"><span class="ln">26054 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26055"><span class="ln">26055 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26056"><span class="ln">26056 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26057"><span class="ln">26057 </span></a><span class="s2">def </span><span class="s1">resize_as_sparse_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">the_template</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26058"><span class="ln">26058 </span></a><span class="s2">def </span><span class="s1">resolve_conj</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26059"><span class="ln">26059 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26060"><span class="ln">26060 </span></a>    resolve_conj(input) -&gt; Tensor 
<a name="l26061"><span class="ln">26061 </span></a> 
<a name="l26062"><span class="ln">26062 </span></a>    Returns a new tensor with materialized conjugation if :attr:`input`'s conjugate bit is set to `True`, 
<a name="l26063"><span class="ln">26063 </span></a>    else returns :attr:`input`. The output tensor will always have its conjugate bit set to `False`. 
<a name="l26064"><span class="ln">26064 </span></a> 
<a name="l26065"><span class="ln">26065 </span></a>    Args: 
<a name="l26066"><span class="ln">26066 </span></a>        input (Tensor): the input tensor. 
<a name="l26067"><span class="ln">26067 </span></a> 
<a name="l26068"><span class="ln">26068 </span></a>    Example:: 
<a name="l26069"><span class="ln">26069 </span></a> 
<a name="l26070"><span class="ln">26070 </span></a>        &gt;&gt;&gt; x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]) 
<a name="l26071"><span class="ln">26071 </span></a>        &gt;&gt;&gt; y = x.conj() 
<a name="l26072"><span class="ln">26072 </span></a>        &gt;&gt;&gt; y.is_conj() 
<a name="l26073"><span class="ln">26073 </span></a>        True 
<a name="l26074"><span class="ln">26074 </span></a>        &gt;&gt;&gt; z = y.resolve_conj() 
<a name="l26075"><span class="ln">26075 </span></a>        &gt;&gt;&gt; z 
<a name="l26076"><span class="ln">26076 </span></a>        tensor([-1 - 1j, -2 - 2j, 3 + 3j]) 
<a name="l26077"><span class="ln">26077 </span></a>        &gt;&gt;&gt; z.is_conj() 
<a name="l26078"><span class="ln">26078 </span></a>        False 
<a name="l26079"><span class="ln">26079 </span></a>    &quot;&quot;&quot;</span>
<a name="l26080"><span class="ln">26080 </span></a>
<a name="l26081"><span class="ln">26081 </span></a><span class="s2">def </span><span class="s1">resolve_neg</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26082"><span class="ln">26082 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26083"><span class="ln">26083 </span></a>    resolve_neg(input) -&gt; Tensor 
<a name="l26084"><span class="ln">26084 </span></a> 
<a name="l26085"><span class="ln">26085 </span></a>    Returns a new tensor with materialized negation if :attr:`input`'s negative bit is set to `True`, 
<a name="l26086"><span class="ln">26086 </span></a>    else returns :attr:`input`. The output tensor will always have its negative bit set to `False`. 
<a name="l26087"><span class="ln">26087 </span></a> 
<a name="l26088"><span class="ln">26088 </span></a>    Args: 
<a name="l26089"><span class="ln">26089 </span></a>        input (Tensor): the input tensor. 
<a name="l26090"><span class="ln">26090 </span></a> 
<a name="l26091"><span class="ln">26091 </span></a>    Example:: 
<a name="l26092"><span class="ln">26092 </span></a> 
<a name="l26093"><span class="ln">26093 </span></a>        &gt;&gt;&gt; x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]) 
<a name="l26094"><span class="ln">26094 </span></a>        &gt;&gt;&gt; y = x.conj() 
<a name="l26095"><span class="ln">26095 </span></a>        &gt;&gt;&gt; z = y.imag 
<a name="l26096"><span class="ln">26096 </span></a>        &gt;&gt;&gt; z.is_neg() 
<a name="l26097"><span class="ln">26097 </span></a>        True 
<a name="l26098"><span class="ln">26098 </span></a>        &gt;&gt;&gt; out = z.resolve_neg() 
<a name="l26099"><span class="ln">26099 </span></a>        &gt;&gt;&gt; out 
<a name="l26100"><span class="ln">26100 </span></a>        tensor([-1., -2., 3.]) 
<a name="l26101"><span class="ln">26101 </span></a>        &gt;&gt;&gt; out.is_neg() 
<a name="l26102"><span class="ln">26102 </span></a>        False 
<a name="l26103"><span class="ln">26103 </span></a>    &quot;&quot;&quot;</span>
<a name="l26104"><span class="ln">26104 </span></a>
<a name="l26105"><span class="ln">26105 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26106"><span class="ln">26106 </span></a><span class="s2">def </span><span class="s1">result_type</span><span class="s3">(</span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _dtype</span><span class="s2">:</span>
<a name="l26107"><span class="ln">26107 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26108"><span class="ln">26108 </span></a>    result_type(tensor1, tensor2) -&gt; dtype 
<a name="l26109"><span class="ln">26109 </span></a> 
<a name="l26110"><span class="ln">26110 </span></a>    Returns the :class:`torch.dtype` that would result from performing an arithmetic 
<a name="l26111"><span class="ln">26111 </span></a>    operation on the provided input tensors. See type promotion :ref:`documentation &lt;type-promotion-doc&gt;` 
<a name="l26112"><span class="ln">26112 </span></a>    for more information on the type promotion logic. 
<a name="l26113"><span class="ln">26113 </span></a> 
<a name="l26114"><span class="ln">26114 </span></a>    Args: 
<a name="l26115"><span class="ln">26115 </span></a>        tensor1 (Tensor or Number): an input tensor or number 
<a name="l26116"><span class="ln">26116 </span></a>        tensor2 (Tensor or Number): an input tensor or number 
<a name="l26117"><span class="ln">26117 </span></a> 
<a name="l26118"><span class="ln">26118 </span></a>    Example:: 
<a name="l26119"><span class="ln">26119 </span></a> 
<a name="l26120"><span class="ln">26120 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0) 
<a name="l26121"><span class="ln">26121 </span></a>        torch.float32 
<a name="l26122"><span class="ln">26122 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1)) 
<a name="l26123"><span class="ln">26123 </span></a>        torch.uint8 
<a name="l26124"><span class="ln">26124 </span></a>    &quot;&quot;&quot;</span>
<a name="l26125"><span class="ln">26125 </span></a>
<a name="l26126"><span class="ln">26126 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26127"><span class="ln">26127 </span></a><span class="s2">def </span><span class="s1">result_type</span><span class="s3">(</span><span class="s1">scalar</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; _dtype</span><span class="s2">:</span>
<a name="l26128"><span class="ln">26128 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26129"><span class="ln">26129 </span></a>    result_type(tensor1, tensor2) -&gt; dtype 
<a name="l26130"><span class="ln">26130 </span></a> 
<a name="l26131"><span class="ln">26131 </span></a>    Returns the :class:`torch.dtype` that would result from performing an arithmetic 
<a name="l26132"><span class="ln">26132 </span></a>    operation on the provided input tensors. See type promotion :ref:`documentation &lt;type-promotion-doc&gt;` 
<a name="l26133"><span class="ln">26133 </span></a>    for more information on the type promotion logic. 
<a name="l26134"><span class="ln">26134 </span></a> 
<a name="l26135"><span class="ln">26135 </span></a>    Args: 
<a name="l26136"><span class="ln">26136 </span></a>        tensor1 (Tensor or Number): an input tensor or number 
<a name="l26137"><span class="ln">26137 </span></a>        tensor2 (Tensor or Number): an input tensor or number 
<a name="l26138"><span class="ln">26138 </span></a> 
<a name="l26139"><span class="ln">26139 </span></a>    Example:: 
<a name="l26140"><span class="ln">26140 </span></a> 
<a name="l26141"><span class="ln">26141 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0) 
<a name="l26142"><span class="ln">26142 </span></a>        torch.float32 
<a name="l26143"><span class="ln">26143 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1)) 
<a name="l26144"><span class="ln">26144 </span></a>        torch.uint8 
<a name="l26145"><span class="ln">26145 </span></a>    &quot;&quot;&quot;</span>
<a name="l26146"><span class="ln">26146 </span></a>
<a name="l26147"><span class="ln">26147 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26148"><span class="ln">26148 </span></a><span class="s2">def </span><span class="s1">result_type</span><span class="s3">(</span><span class="s1">tensor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; _dtype</span><span class="s2">:</span>
<a name="l26149"><span class="ln">26149 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26150"><span class="ln">26150 </span></a>    result_type(tensor1, tensor2) -&gt; dtype 
<a name="l26151"><span class="ln">26151 </span></a> 
<a name="l26152"><span class="ln">26152 </span></a>    Returns the :class:`torch.dtype` that would result from performing an arithmetic 
<a name="l26153"><span class="ln">26153 </span></a>    operation on the provided input tensors. See type promotion :ref:`documentation &lt;type-promotion-doc&gt;` 
<a name="l26154"><span class="ln">26154 </span></a>    for more information on the type promotion logic. 
<a name="l26155"><span class="ln">26155 </span></a> 
<a name="l26156"><span class="ln">26156 </span></a>    Args: 
<a name="l26157"><span class="ln">26157 </span></a>        tensor1 (Tensor or Number): an input tensor or number 
<a name="l26158"><span class="ln">26158 </span></a>        tensor2 (Tensor or Number): an input tensor or number 
<a name="l26159"><span class="ln">26159 </span></a> 
<a name="l26160"><span class="ln">26160 </span></a>    Example:: 
<a name="l26161"><span class="ln">26161 </span></a> 
<a name="l26162"><span class="ln">26162 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0) 
<a name="l26163"><span class="ln">26163 </span></a>        torch.float32 
<a name="l26164"><span class="ln">26164 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1)) 
<a name="l26165"><span class="ln">26165 </span></a>        torch.uint8 
<a name="l26166"><span class="ln">26166 </span></a>    &quot;&quot;&quot;</span>
<a name="l26167"><span class="ln">26167 </span></a>
<a name="l26168"><span class="ln">26168 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26169"><span class="ln">26169 </span></a><span class="s2">def </span><span class="s1">result_type</span><span class="s3">(</span>
<a name="l26170"><span class="ln">26170 </span></a>    <span class="s1">scalar1</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26171"><span class="ln">26171 </span></a>    <span class="s1">scalar2</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26172"><span class="ln">26172 </span></a><span class="s3">) </span><span class="s1">-&gt; _dtype</span><span class="s2">:</span>
<a name="l26173"><span class="ln">26173 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26174"><span class="ln">26174 </span></a>    result_type(tensor1, tensor2) -&gt; dtype 
<a name="l26175"><span class="ln">26175 </span></a> 
<a name="l26176"><span class="ln">26176 </span></a>    Returns the :class:`torch.dtype` that would result from performing an arithmetic 
<a name="l26177"><span class="ln">26177 </span></a>    operation on the provided input tensors. See type promotion :ref:`documentation &lt;type-promotion-doc&gt;` 
<a name="l26178"><span class="ln">26178 </span></a>    for more information on the type promotion logic. 
<a name="l26179"><span class="ln">26179 </span></a> 
<a name="l26180"><span class="ln">26180 </span></a>    Args: 
<a name="l26181"><span class="ln">26181 </span></a>        tensor1 (Tensor or Number): an input tensor or number 
<a name="l26182"><span class="ln">26182 </span></a>        tensor2 (Tensor or Number): an input tensor or number 
<a name="l26183"><span class="ln">26183 </span></a> 
<a name="l26184"><span class="ln">26184 </span></a>    Example:: 
<a name="l26185"><span class="ln">26185 </span></a> 
<a name="l26186"><span class="ln">26186 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0) 
<a name="l26187"><span class="ln">26187 </span></a>        torch.float32 
<a name="l26188"><span class="ln">26188 </span></a>        &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1)) 
<a name="l26189"><span class="ln">26189 </span></a>        torch.uint8 
<a name="l26190"><span class="ln">26190 </span></a>    &quot;&quot;&quot;</span>
<a name="l26191"><span class="ln">26191 </span></a>
<a name="l26192"><span class="ln">26192 </span></a><span class="s2">def </span><span class="s1">rms_norm</span><span class="s3">(</span>
<a name="l26193"><span class="ln">26193 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26194"><span class="ln">26194 </span></a>    <span class="s1">normalized_shape</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l26195"><span class="ln">26195 </span></a>    <span class="s1">weight</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26196"><span class="ln">26196 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26197"><span class="ln">26197 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26198"><span class="ln">26198 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26199"><span class="ln">26199 </span></a><span class="s2">def </span><span class="s1">rnn_relu</span><span class="s3">(</span>
<a name="l26200"><span class="ln">26200 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26201"><span class="ln">26201 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26202"><span class="ln">26202 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26203"><span class="ln">26203 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26204"><span class="ln">26204 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26205"><span class="ln">26205 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26206"><span class="ln">26206 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l26207"><span class="ln">26207 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26208"><span class="ln">26208 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26209"><span class="ln">26209 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26210"><span class="ln">26210 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26211"><span class="ln">26211 </span></a><span class="s2">def </span><span class="s1">rnn_relu</span><span class="s3">(</span>
<a name="l26212"><span class="ln">26212 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26213"><span class="ln">26213 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26214"><span class="ln">26214 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26215"><span class="ln">26215 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26216"><span class="ln">26216 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26217"><span class="ln">26217 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l26218"><span class="ln">26218 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26219"><span class="ln">26219 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26220"><span class="ln">26220 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26221"><span class="ln">26221 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26222"><span class="ln">26222 </span></a><span class="s2">def </span><span class="s1">rnn_relu_cell</span><span class="s3">(</span>
<a name="l26223"><span class="ln">26223 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26224"><span class="ln">26224 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26225"><span class="ln">26225 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26226"><span class="ln">26226 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26227"><span class="ln">26227 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26228"><span class="ln">26228 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26229"><span class="ln">26229 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26230"><span class="ln">26230 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26231"><span class="ln">26231 </span></a><span class="s2">def </span><span class="s1">rnn_tanh</span><span class="s3">(</span>
<a name="l26232"><span class="ln">26232 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26233"><span class="ln">26233 </span></a>    <span class="s1">batch_sizes</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26234"><span class="ln">26234 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26235"><span class="ln">26235 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26236"><span class="ln">26236 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26237"><span class="ln">26237 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26238"><span class="ln">26238 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l26239"><span class="ln">26239 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26240"><span class="ln">26240 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26241"><span class="ln">26241 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26242"><span class="ln">26242 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26243"><span class="ln">26243 </span></a><span class="s2">def </span><span class="s1">rnn_tanh</span><span class="s3">(</span>
<a name="l26244"><span class="ln">26244 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26245"><span class="ln">26245 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26246"><span class="ln">26246 </span></a>    <span class="s1">params</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26247"><span class="ln">26247 </span></a>    <span class="s1">has_biases</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26248"><span class="ln">26248 </span></a>    <span class="s1">num_layers</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26249"><span class="ln">26249 </span></a>    <span class="s1">dropout</span><span class="s2">: </span><span class="s1">_float</span><span class="s3">,</span>
<a name="l26250"><span class="ln">26250 </span></a>    <span class="s1">train</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26251"><span class="ln">26251 </span></a>    <span class="s1">bidirectional</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26252"><span class="ln">26252 </span></a>    <span class="s1">batch_first</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">,</span>
<a name="l26253"><span class="ln">26253 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26254"><span class="ln">26254 </span></a><span class="s2">def </span><span class="s1">rnn_tanh_cell</span><span class="s3">(</span>
<a name="l26255"><span class="ln">26255 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26256"><span class="ln">26256 </span></a>    <span class="s1">hx</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26257"><span class="ln">26257 </span></a>    <span class="s1">w_ih</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26258"><span class="ln">26258 </span></a>    <span class="s1">w_hh</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26259"><span class="ln">26259 </span></a>    <span class="s1">b_ih</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26260"><span class="ln">26260 </span></a>    <span class="s1">b_hh</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26261"><span class="ln">26261 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26262"><span class="ln">26262 </span></a><span class="s2">def </span><span class="s1">roll</span><span class="s3">(</span>
<a name="l26263"><span class="ln">26263 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26264"><span class="ln">26264 </span></a>    <span class="s1">shifts</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l26265"><span class="ln">26265 </span></a>    <span class="s1">dims</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(),</span>
<a name="l26266"><span class="ln">26266 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26267"><span class="ln">26267 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26268"><span class="ln">26268 </span></a>    roll(input, shifts, dims=None) -&gt; Tensor 
<a name="l26269"><span class="ln">26269 </span></a> 
<a name="l26270"><span class="ln">26270 </span></a>    Roll the tensor :attr:`input` along the given dimension(s). Elements that are 
<a name="l26271"><span class="ln">26271 </span></a>    shifted beyond the last position are re-introduced at the first position. If 
<a name="l26272"><span class="ln">26272 </span></a>    :attr:`dims` is `None`, the tensor will be flattened before rolling and then 
<a name="l26273"><span class="ln">26273 </span></a>    restored to the original shape. 
<a name="l26274"><span class="ln">26274 </span></a> 
<a name="l26275"><span class="ln">26275 </span></a>    Args: 
<a name="l26276"><span class="ln">26276 </span></a>        input (Tensor): the input tensor. 
<a name="l26277"><span class="ln">26277 </span></a>        shifts (int or tuple of ints): The number of places by which the elements 
<a name="l26278"><span class="ln">26278 </span></a>            of the tensor are shifted. If shifts is a tuple, dims must be a tuple of 
<a name="l26279"><span class="ln">26279 </span></a>            the same size, and each dimension will be rolled by the corresponding 
<a name="l26280"><span class="ln">26280 </span></a>            value 
<a name="l26281"><span class="ln">26281 </span></a>        dims (int or tuple of ints): Axis along which to roll 
<a name="l26282"><span class="ln">26282 </span></a> 
<a name="l26283"><span class="ln">26283 </span></a>    Example:: 
<a name="l26284"><span class="ln">26284 </span></a> 
<a name="l26285"><span class="ln">26285 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2) 
<a name="l26286"><span class="ln">26286 </span></a>        &gt;&gt;&gt; x 
<a name="l26287"><span class="ln">26287 </span></a>        tensor([[1, 2], 
<a name="l26288"><span class="ln">26288 </span></a>                [3, 4], 
<a name="l26289"><span class="ln">26289 </span></a>                [5, 6], 
<a name="l26290"><span class="ln">26290 </span></a>                [7, 8]]) 
<a name="l26291"><span class="ln">26291 </span></a>        &gt;&gt;&gt; torch.roll(x, 1) 
<a name="l26292"><span class="ln">26292 </span></a>        tensor([[8, 1], 
<a name="l26293"><span class="ln">26293 </span></a>                [2, 3], 
<a name="l26294"><span class="ln">26294 </span></a>                [4, 5], 
<a name="l26295"><span class="ln">26295 </span></a>                [6, 7]]) 
<a name="l26296"><span class="ln">26296 </span></a>        &gt;&gt;&gt; torch.roll(x, 1, 0) 
<a name="l26297"><span class="ln">26297 </span></a>        tensor([[7, 8], 
<a name="l26298"><span class="ln">26298 </span></a>                [1, 2], 
<a name="l26299"><span class="ln">26299 </span></a>                [3, 4], 
<a name="l26300"><span class="ln">26300 </span></a>                [5, 6]]) 
<a name="l26301"><span class="ln">26301 </span></a>        &gt;&gt;&gt; torch.roll(x, -1, 0) 
<a name="l26302"><span class="ln">26302 </span></a>        tensor([[3, 4], 
<a name="l26303"><span class="ln">26303 </span></a>                [5, 6], 
<a name="l26304"><span class="ln">26304 </span></a>                [7, 8], 
<a name="l26305"><span class="ln">26305 </span></a>                [1, 2]]) 
<a name="l26306"><span class="ln">26306 </span></a>        &gt;&gt;&gt; torch.roll(x, shifts=(2, 1), dims=(0, 1)) 
<a name="l26307"><span class="ln">26307 </span></a>        tensor([[6, 5], 
<a name="l26308"><span class="ln">26308 </span></a>                [8, 7], 
<a name="l26309"><span class="ln">26309 </span></a>                [2, 1], 
<a name="l26310"><span class="ln">26310 </span></a>                [4, 3]]) 
<a name="l26311"><span class="ln">26311 </span></a>    &quot;&quot;&quot;</span>
<a name="l26312"><span class="ln">26312 </span></a>
<a name="l26313"><span class="ln">26313 </span></a><span class="s2">def </span><span class="s1">rot90</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">k</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">, </span><span class="s1">dims</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">= </span><span class="s3">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">)) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26314"><span class="ln">26314 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26315"><span class="ln">26315 </span></a>    rot90(input, k=1, dims=(0, 1)) -&gt; Tensor 
<a name="l26316"><span class="ln">26316 </span></a> 
<a name="l26317"><span class="ln">26317 </span></a>    Rotate an n-D tensor by 90 degrees in the plane specified by dims axis. 
<a name="l26318"><span class="ln">26318 </span></a>    Rotation direction is from the first towards the second axis if k &gt; 0, and from the second towards the first for k &lt; 0. 
<a name="l26319"><span class="ln">26319 </span></a> 
<a name="l26320"><span class="ln">26320 </span></a>    Args: 
<a name="l26321"><span class="ln">26321 </span></a>        input (Tensor): the input tensor. 
<a name="l26322"><span class="ln">26322 </span></a>        k (int): number of times to rotate. Default value is 1 
<a name="l26323"><span class="ln">26323 </span></a>        dims (a list or tuple): axis to rotate. Default value is [0, 1] 
<a name="l26324"><span class="ln">26324 </span></a> 
<a name="l26325"><span class="ln">26325 </span></a>    Example:: 
<a name="l26326"><span class="ln">26326 </span></a> 
<a name="l26327"><span class="ln">26327 </span></a>        &gt;&gt;&gt; x = torch.arange(4).view(2, 2) 
<a name="l26328"><span class="ln">26328 </span></a>        &gt;&gt;&gt; x 
<a name="l26329"><span class="ln">26329 </span></a>        tensor([[0, 1], 
<a name="l26330"><span class="ln">26330 </span></a>                [2, 3]]) 
<a name="l26331"><span class="ln">26331 </span></a>        &gt;&gt;&gt; torch.rot90(x, 1, [0, 1]) 
<a name="l26332"><span class="ln">26332 </span></a>        tensor([[1, 3], 
<a name="l26333"><span class="ln">26333 </span></a>                [0, 2]]) 
<a name="l26334"><span class="ln">26334 </span></a> 
<a name="l26335"><span class="ln">26335 </span></a>        &gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2) 
<a name="l26336"><span class="ln">26336 </span></a>        &gt;&gt;&gt; x 
<a name="l26337"><span class="ln">26337 </span></a>        tensor([[[0, 1], 
<a name="l26338"><span class="ln">26338 </span></a>                 [2, 3]], 
<a name="l26339"><span class="ln">26339 </span></a> 
<a name="l26340"><span class="ln">26340 </span></a>                [[4, 5], 
<a name="l26341"><span class="ln">26341 </span></a>                 [6, 7]]]) 
<a name="l26342"><span class="ln">26342 </span></a>        &gt;&gt;&gt; torch.rot90(x, 1, [1, 2]) 
<a name="l26343"><span class="ln">26343 </span></a>        tensor([[[1, 3], 
<a name="l26344"><span class="ln">26344 </span></a>                 [0, 2]], 
<a name="l26345"><span class="ln">26345 </span></a> 
<a name="l26346"><span class="ln">26346 </span></a>                [[5, 7], 
<a name="l26347"><span class="ln">26347 </span></a>                 [4, 6]]]) 
<a name="l26348"><span class="ln">26348 </span></a>    &quot;&quot;&quot;</span>
<a name="l26349"><span class="ln">26349 </span></a>
<a name="l26350"><span class="ln">26350 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26351"><span class="ln">26351 </span></a><span class="s2">def </span><span class="s1">round</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26352"><span class="ln">26352 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26353"><span class="ln">26353 </span></a>    round(input, *, decimals=0, out=None) -&gt; Tensor 
<a name="l26354"><span class="ln">26354 </span></a> 
<a name="l26355"><span class="ln">26355 </span></a>    Rounds elements of :attr:`input` to the nearest integer. 
<a name="l26356"><span class="ln">26356 </span></a> 
<a name="l26357"><span class="ln">26357 </span></a>    For integer inputs, follows the array-api convention of returning a 
<a name="l26358"><span class="ln">26358 </span></a>    copy of the input tensor. 
<a name="l26359"><span class="ln">26359 </span></a>    The return type of output is same as that of input's dtype. 
<a name="l26360"><span class="ln">26360 </span></a> 
<a name="l26361"><span class="ln">26361 </span></a>    .. note:: 
<a name="l26362"><span class="ln">26362 </span></a>        This function implements the &quot;round half to even&quot; to 
<a name="l26363"><span class="ln">26363 </span></a>        break ties when a number is equidistant from two 
<a name="l26364"><span class="ln">26364 </span></a>        integers (e.g. `round(2.5)` is 2). 
<a name="l26365"><span class="ln">26365 </span></a> 
<a name="l26366"><span class="ln">26366 </span></a>        When the :attr:\`decimals\` argument is specified the 
<a name="l26367"><span class="ln">26367 </span></a>        algorithm used is similar to NumPy's `around`. This 
<a name="l26368"><span class="ln">26368 </span></a>        algorithm is fast but inexact and it can easily 
<a name="l26369"><span class="ln">26369 </span></a>        overflow for low precision dtypes. 
<a name="l26370"><span class="ln">26370 </span></a>        Eg. `round(tensor([10000], dtype=torch.float16), decimals=3)` is `inf`. 
<a name="l26371"><span class="ln">26371 </span></a> 
<a name="l26372"><span class="ln">26372 </span></a>    .. seealso:: 
<a name="l26373"><span class="ln">26373 </span></a>        :func:`torch.ceil`, which rounds up. 
<a name="l26374"><span class="ln">26374 </span></a>        :func:`torch.floor`, which rounds down. 
<a name="l26375"><span class="ln">26375 </span></a>        :func:`torch.trunc`, which rounds towards zero. 
<a name="l26376"><span class="ln">26376 </span></a> 
<a name="l26377"><span class="ln">26377 </span></a>    Args: 
<a name="l26378"><span class="ln">26378 </span></a>        input (Tensor): the input tensor. 
<a name="l26379"><span class="ln">26379 </span></a>        decimals (int): Number of decimal places to round to (default: 0). 
<a name="l26380"><span class="ln">26380 </span></a>            If decimals is negative, it specifies the number of positions 
<a name="l26381"><span class="ln">26381 </span></a>            to the left of the decimal point. 
<a name="l26382"><span class="ln">26382 </span></a> 
<a name="l26383"><span class="ln">26383 </span></a>    Keyword args: 
<a name="l26384"><span class="ln">26384 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l26385"><span class="ln">26385 </span></a> 
<a name="l26386"><span class="ln">26386 </span></a>    Example:: 
<a name="l26387"><span class="ln">26387 </span></a> 
<a name="l26388"><span class="ln">26388 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor((4.7, -2.3, 9.1, -7.7))) 
<a name="l26389"><span class="ln">26389 </span></a>        tensor([ 5.,  -2.,  9., -8.]) 
<a name="l26390"><span class="ln">26390 </span></a> 
<a name="l26391"><span class="ln">26391 </span></a>        &gt;&gt;&gt; # Values equidistant from two integers are rounded towards the 
<a name="l26392"><span class="ln">26392 </span></a>        &gt;&gt;&gt; #   the nearest even value (zero is treated as even) 
<a name="l26393"><span class="ln">26393 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor([-0.5, 0.5, 1.5, 2.5])) 
<a name="l26394"><span class="ln">26394 </span></a>        tensor([-0., 0., 2., 2.]) 
<a name="l26395"><span class="ln">26395 </span></a> 
<a name="l26396"><span class="ln">26396 </span></a>        &gt;&gt;&gt; # A positive decimals argument rounds to the to that decimal place 
<a name="l26397"><span class="ln">26397 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor([0.1234567]), decimals=3) 
<a name="l26398"><span class="ln">26398 </span></a>        tensor([0.1230]) 
<a name="l26399"><span class="ln">26399 </span></a> 
<a name="l26400"><span class="ln">26400 </span></a>        &gt;&gt;&gt; # A negative decimals argument rounds to the left of the decimal 
<a name="l26401"><span class="ln">26401 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor([1200.1234567]), decimals=-3) 
<a name="l26402"><span class="ln">26402 </span></a>        tensor([1000.]) 
<a name="l26403"><span class="ln">26403 </span></a>    &quot;&quot;&quot;</span>
<a name="l26404"><span class="ln">26404 </span></a>
<a name="l26405"><span class="ln">26405 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26406"><span class="ln">26406 </span></a><span class="s2">def </span><span class="s1">round</span><span class="s3">(</span>
<a name="l26407"><span class="ln">26407 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26408"><span class="ln">26408 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26409"><span class="ln">26409 </span></a>    <span class="s1">decimals</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26410"><span class="ln">26410 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26411"><span class="ln">26411 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26412"><span class="ln">26412 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26413"><span class="ln">26413 </span></a>    round(input, *, decimals=0, out=None) -&gt; Tensor 
<a name="l26414"><span class="ln">26414 </span></a> 
<a name="l26415"><span class="ln">26415 </span></a>    Rounds elements of :attr:`input` to the nearest integer. 
<a name="l26416"><span class="ln">26416 </span></a> 
<a name="l26417"><span class="ln">26417 </span></a>    For integer inputs, follows the array-api convention of returning a 
<a name="l26418"><span class="ln">26418 </span></a>    copy of the input tensor. 
<a name="l26419"><span class="ln">26419 </span></a>    The return type of output is same as that of input's dtype. 
<a name="l26420"><span class="ln">26420 </span></a> 
<a name="l26421"><span class="ln">26421 </span></a>    .. note:: 
<a name="l26422"><span class="ln">26422 </span></a>        This function implements the &quot;round half to even&quot; to 
<a name="l26423"><span class="ln">26423 </span></a>        break ties when a number is equidistant from two 
<a name="l26424"><span class="ln">26424 </span></a>        integers (e.g. `round(2.5)` is 2). 
<a name="l26425"><span class="ln">26425 </span></a> 
<a name="l26426"><span class="ln">26426 </span></a>        When the :attr:\`decimals\` argument is specified the 
<a name="l26427"><span class="ln">26427 </span></a>        algorithm used is similar to NumPy's `around`. This 
<a name="l26428"><span class="ln">26428 </span></a>        algorithm is fast but inexact and it can easily 
<a name="l26429"><span class="ln">26429 </span></a>        overflow for low precision dtypes. 
<a name="l26430"><span class="ln">26430 </span></a>        Eg. `round(tensor([10000], dtype=torch.float16), decimals=3)` is `inf`. 
<a name="l26431"><span class="ln">26431 </span></a> 
<a name="l26432"><span class="ln">26432 </span></a>    .. seealso:: 
<a name="l26433"><span class="ln">26433 </span></a>        :func:`torch.ceil`, which rounds up. 
<a name="l26434"><span class="ln">26434 </span></a>        :func:`torch.floor`, which rounds down. 
<a name="l26435"><span class="ln">26435 </span></a>        :func:`torch.trunc`, which rounds towards zero. 
<a name="l26436"><span class="ln">26436 </span></a> 
<a name="l26437"><span class="ln">26437 </span></a>    Args: 
<a name="l26438"><span class="ln">26438 </span></a>        input (Tensor): the input tensor. 
<a name="l26439"><span class="ln">26439 </span></a>        decimals (int): Number of decimal places to round to (default: 0). 
<a name="l26440"><span class="ln">26440 </span></a>            If decimals is negative, it specifies the number of positions 
<a name="l26441"><span class="ln">26441 </span></a>            to the left of the decimal point. 
<a name="l26442"><span class="ln">26442 </span></a> 
<a name="l26443"><span class="ln">26443 </span></a>    Keyword args: 
<a name="l26444"><span class="ln">26444 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l26445"><span class="ln">26445 </span></a> 
<a name="l26446"><span class="ln">26446 </span></a>    Example:: 
<a name="l26447"><span class="ln">26447 </span></a> 
<a name="l26448"><span class="ln">26448 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor((4.7, -2.3, 9.1, -7.7))) 
<a name="l26449"><span class="ln">26449 </span></a>        tensor([ 5.,  -2.,  9., -8.]) 
<a name="l26450"><span class="ln">26450 </span></a> 
<a name="l26451"><span class="ln">26451 </span></a>        &gt;&gt;&gt; # Values equidistant from two integers are rounded towards the 
<a name="l26452"><span class="ln">26452 </span></a>        &gt;&gt;&gt; #   the nearest even value (zero is treated as even) 
<a name="l26453"><span class="ln">26453 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor([-0.5, 0.5, 1.5, 2.5])) 
<a name="l26454"><span class="ln">26454 </span></a>        tensor([-0., 0., 2., 2.]) 
<a name="l26455"><span class="ln">26455 </span></a> 
<a name="l26456"><span class="ln">26456 </span></a>        &gt;&gt;&gt; # A positive decimals argument rounds to the to that decimal place 
<a name="l26457"><span class="ln">26457 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor([0.1234567]), decimals=3) 
<a name="l26458"><span class="ln">26458 </span></a>        tensor([0.1230]) 
<a name="l26459"><span class="ln">26459 </span></a> 
<a name="l26460"><span class="ln">26460 </span></a>        &gt;&gt;&gt; # A negative decimals argument rounds to the left of the decimal 
<a name="l26461"><span class="ln">26461 </span></a>        &gt;&gt;&gt; torch.round(torch.tensor([1200.1234567]), decimals=-3) 
<a name="l26462"><span class="ln">26462 </span></a>        tensor([1000.]) 
<a name="l26463"><span class="ln">26463 </span></a>    &quot;&quot;&quot;</span>
<a name="l26464"><span class="ln">26464 </span></a>
<a name="l26465"><span class="ln">26465 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26466"><span class="ln">26466 </span></a><span class="s2">def </span><span class="s1">round_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26467"><span class="ln">26467 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26468"><span class="ln">26468 </span></a><span class="s2">def </span><span class="s1">round_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">decimals</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26469"><span class="ln">26469 </span></a><span class="s2">def </span><span class="s1">row_indices_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26470"><span class="ln">26470 </span></a><span class="s2">def </span><span class="s1">row_stack</span><span class="s3">(</span>
<a name="l26471"><span class="ln">26471 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26472"><span class="ln">26472 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26473"><span class="ln">26473 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26474"><span class="ln">26474 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26475"><span class="ln">26475 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26476"><span class="ln">26476 </span></a>    row_stack(tensors, *, out=None) -&gt; Tensor 
<a name="l26477"><span class="ln">26477 </span></a> 
<a name="l26478"><span class="ln">26478 </span></a>    Alias of :func:`torch.vstack`. 
<a name="l26479"><span class="ln">26479 </span></a>    &quot;&quot;&quot;</span>
<a name="l26480"><span class="ln">26480 </span></a>
<a name="l26481"><span class="ln">26481 </span></a><span class="s2">def </span><span class="s1">rrelu</span><span class="s3">(</span>
<a name="l26482"><span class="ln">26482 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26483"><span class="ln">26483 </span></a>    <span class="s1">lower</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0.125</span><span class="s3">,</span>
<a name="l26484"><span class="ln">26484 </span></a>    <span class="s1">upper</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0.3333333333333333</span><span class="s3">,</span>
<a name="l26485"><span class="ln">26485 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l26486"><span class="ln">26486 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26487"><span class="ln">26487 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26488"><span class="ln">26488 </span></a><span class="s2">def </span><span class="s1">rrelu_</span><span class="s3">(</span>
<a name="l26489"><span class="ln">26489 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26490"><span class="ln">26490 </span></a>    <span class="s1">lower</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0.125</span><span class="s3">,</span>
<a name="l26491"><span class="ln">26491 </span></a>    <span class="s1">upper</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">0.3333333333333333</span><span class="s3">,</span>
<a name="l26492"><span class="ln">26492 </span></a>    <span class="s1">training</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l26493"><span class="ln">26493 </span></a>    <span class="s1">generator</span><span class="s2">: </span><span class="s1">Generator </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26494"><span class="ln">26494 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26495"><span class="ln">26495 </span></a><span class="s2">def </span><span class="s1">rsqrt</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26496"><span class="ln">26496 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26497"><span class="ln">26497 </span></a>    rsqrt(input, *, out=None) -&gt; Tensor 
<a name="l26498"><span class="ln">26498 </span></a> 
<a name="l26499"><span class="ln">26499 </span></a>    Returns a new tensor with the reciprocal of the square-root of each of 
<a name="l26500"><span class="ln">26500 </span></a>    the elements of :attr:`input`. 
<a name="l26501"><span class="ln">26501 </span></a> 
<a name="l26502"><span class="ln">26502 </span></a>    .. math:: 
<a name="l26503"><span class="ln">26503 </span></a>        \text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}} 
<a name="l26504"><span class="ln">26504 </span></a> 
<a name="l26505"><span class="ln">26505 </span></a>    Args: 
<a name="l26506"><span class="ln">26506 </span></a>        input (Tensor): the input tensor. 
<a name="l26507"><span class="ln">26507 </span></a> 
<a name="l26508"><span class="ln">26508 </span></a>    Keyword args: 
<a name="l26509"><span class="ln">26509 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l26510"><span class="ln">26510 </span></a> 
<a name="l26511"><span class="ln">26511 </span></a>    Example:: 
<a name="l26512"><span class="ln">26512 </span></a> 
<a name="l26513"><span class="ln">26513 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l26514"><span class="ln">26514 </span></a>        &gt;&gt;&gt; a 
<a name="l26515"><span class="ln">26515 </span></a>        tensor([-0.0370,  0.2970,  1.5420, -0.9105]) 
<a name="l26516"><span class="ln">26516 </span></a>        &gt;&gt;&gt; torch.rsqrt(a) 
<a name="l26517"><span class="ln">26517 </span></a>        tensor([    nan,  1.8351,  0.8053,     nan]) 
<a name="l26518"><span class="ln">26518 </span></a>    &quot;&quot;&quot;</span>
<a name="l26519"><span class="ln">26519 </span></a>
<a name="l26520"><span class="ln">26520 </span></a><span class="s2">def </span><span class="s1">rsqrt_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26521"><span class="ln">26521 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26522"><span class="ln">26522 </span></a><span class="s2">def </span><span class="s1">rsub</span><span class="s3">(</span>
<a name="l26523"><span class="ln">26523 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26524"><span class="ln">26524 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26525"><span class="ln">26525 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26526"><span class="ln">26526 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l26527"><span class="ln">26527 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26528"><span class="ln">26528 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26529"><span class="ln">26529 </span></a><span class="s2">def </span><span class="s1">rsub</span><span class="s3">(</span>
<a name="l26530"><span class="ln">26530 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26531"><span class="ln">26531 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26532"><span class="ln">26532 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l26533"><span class="ln">26533 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26534"><span class="ln">26534 </span></a><span class="s2">def </span><span class="s1">saddmm</span><span class="s3">(</span>
<a name="l26535"><span class="ln">26535 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26536"><span class="ln">26536 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26537"><span class="ln">26537 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26538"><span class="ln">26538 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26539"><span class="ln">26539 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l26540"><span class="ln">26540 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l26541"><span class="ln">26541 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26542"><span class="ln">26542 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26543"><span class="ln">26543 </span></a><span class="s2">def </span><span class="s1">scalar_tensor</span><span class="s3">(</span>
<a name="l26544"><span class="ln">26544 </span></a>    <span class="s1">s</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26545"><span class="ln">26545 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26546"><span class="ln">26546 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26547"><span class="ln">26547 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26548"><span class="ln">26548 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26549"><span class="ln">26549 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l26550"><span class="ln">26550 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l26551"><span class="ln">26551 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26552"><span class="ln">26552 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26553"><span class="ln">26553 </span></a><span class="s2">def </span><span class="s1">scatter</span><span class="s3">(</span>
<a name="l26554"><span class="ln">26554 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26555"><span class="ln">26555 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26556"><span class="ln">26556 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26557"><span class="ln">26557 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26558"><span class="ln">26558 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26559"><span class="ln">26559 </span></a>    <span class="s1">reduce</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l26560"><span class="ln">26560 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26561"><span class="ln">26561 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26562"><span class="ln">26562 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26563"><span class="ln">26563 </span></a>    scatter(input, dim, index, src) -&gt; Tensor 
<a name="l26564"><span class="ln">26564 </span></a> 
<a name="l26565"><span class="ln">26565 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l26566"><span class="ln">26566 </span></a>    &quot;&quot;&quot;</span>
<a name="l26567"><span class="ln">26567 </span></a>
<a name="l26568"><span class="ln">26568 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26569"><span class="ln">26569 </span></a><span class="s2">def </span><span class="s1">scatter</span><span class="s3">(</span>
<a name="l26570"><span class="ln">26570 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26571"><span class="ln">26571 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26572"><span class="ln">26572 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26573"><span class="ln">26573 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26574"><span class="ln">26574 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26575"><span class="ln">26575 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26576"><span class="ln">26576 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26577"><span class="ln">26577 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26578"><span class="ln">26578 </span></a>    scatter(input, dim, index, src) -&gt; Tensor 
<a name="l26579"><span class="ln">26579 </span></a> 
<a name="l26580"><span class="ln">26580 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l26581"><span class="ln">26581 </span></a>    &quot;&quot;&quot;</span>
<a name="l26582"><span class="ln">26582 </span></a>
<a name="l26583"><span class="ln">26583 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26584"><span class="ln">26584 </span></a><span class="s2">def </span><span class="s1">scatter</span><span class="s3">(</span>
<a name="l26585"><span class="ln">26585 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26586"><span class="ln">26586 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26587"><span class="ln">26587 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26588"><span class="ln">26588 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26589"><span class="ln">26589 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26590"><span class="ln">26590 </span></a>    <span class="s1">reduce</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l26591"><span class="ln">26591 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26592"><span class="ln">26592 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26593"><span class="ln">26593 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26594"><span class="ln">26594 </span></a>    scatter(input, dim, index, src) -&gt; Tensor 
<a name="l26595"><span class="ln">26595 </span></a> 
<a name="l26596"><span class="ln">26596 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l26597"><span class="ln">26597 </span></a>    &quot;&quot;&quot;</span>
<a name="l26598"><span class="ln">26598 </span></a>
<a name="l26599"><span class="ln">26599 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26600"><span class="ln">26600 </span></a><span class="s2">def </span><span class="s1">scatter</span><span class="s3">(</span>
<a name="l26601"><span class="ln">26601 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26602"><span class="ln">26602 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26603"><span class="ln">26603 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26604"><span class="ln">26604 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26605"><span class="ln">26605 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26606"><span class="ln">26606 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26607"><span class="ln">26607 </span></a>    scatter(input, dim, index, src) -&gt; Tensor 
<a name="l26608"><span class="ln">26608 </span></a> 
<a name="l26609"><span class="ln">26609 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l26610"><span class="ln">26610 </span></a>    &quot;&quot;&quot;</span>
<a name="l26611"><span class="ln">26611 </span></a>
<a name="l26612"><span class="ln">26612 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26613"><span class="ln">26613 </span></a><span class="s2">def </span><span class="s1">scatter</span><span class="s3">(</span>
<a name="l26614"><span class="ln">26614 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26615"><span class="ln">26615 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26616"><span class="ln">26616 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26617"><span class="ln">26617 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26618"><span class="ln">26618 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26619"><span class="ln">26619 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26620"><span class="ln">26620 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26621"><span class="ln">26621 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26622"><span class="ln">26622 </span></a>    scatter(input, dim, index, src) -&gt; Tensor 
<a name="l26623"><span class="ln">26623 </span></a> 
<a name="l26624"><span class="ln">26624 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l26625"><span class="ln">26625 </span></a>    &quot;&quot;&quot;</span>
<a name="l26626"><span class="ln">26626 </span></a>
<a name="l26627"><span class="ln">26627 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26628"><span class="ln">26628 </span></a><span class="s2">def </span><span class="s1">scatter</span><span class="s3">(</span>
<a name="l26629"><span class="ln">26629 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26630"><span class="ln">26630 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26631"><span class="ln">26631 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26632"><span class="ln">26632 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26633"><span class="ln">26633 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26634"><span class="ln">26634 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26635"><span class="ln">26635 </span></a>    scatter(input, dim, index, src) -&gt; Tensor 
<a name="l26636"><span class="ln">26636 </span></a> 
<a name="l26637"><span class="ln">26637 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_` 
<a name="l26638"><span class="ln">26638 </span></a>    &quot;&quot;&quot;</span>
<a name="l26639"><span class="ln">26639 </span></a>
<a name="l26640"><span class="ln">26640 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26641"><span class="ln">26641 </span></a><span class="s2">def </span><span class="s1">scatter_add</span><span class="s3">(</span>
<a name="l26642"><span class="ln">26642 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26643"><span class="ln">26643 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26644"><span class="ln">26644 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26645"><span class="ln">26645 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26646"><span class="ln">26646 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26647"><span class="ln">26647 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26648"><span class="ln">26648 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26649"><span class="ln">26649 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26650"><span class="ln">26650 </span></a>    scatter_add(input, dim, index, src) -&gt; Tensor 
<a name="l26651"><span class="ln">26651 </span></a> 
<a name="l26652"><span class="ln">26652 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_add_` 
<a name="l26653"><span class="ln">26653 </span></a>    &quot;&quot;&quot;</span>
<a name="l26654"><span class="ln">26654 </span></a>
<a name="l26655"><span class="ln">26655 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26656"><span class="ln">26656 </span></a><span class="s2">def </span><span class="s1">scatter_add</span><span class="s3">(</span>
<a name="l26657"><span class="ln">26657 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26658"><span class="ln">26658 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26659"><span class="ln">26659 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26660"><span class="ln">26660 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26661"><span class="ln">26661 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26662"><span class="ln">26662 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26663"><span class="ln">26663 </span></a>    scatter_add(input, dim, index, src) -&gt; Tensor 
<a name="l26664"><span class="ln">26664 </span></a> 
<a name="l26665"><span class="ln">26665 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_add_` 
<a name="l26666"><span class="ln">26666 </span></a>    &quot;&quot;&quot;</span>
<a name="l26667"><span class="ln">26667 </span></a>
<a name="l26668"><span class="ln">26668 </span></a><span class="s2">def </span><span class="s1">scatter_reduce</span><span class="s3">(</span>
<a name="l26669"><span class="ln">26669 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26670"><span class="ln">26670 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26671"><span class="ln">26671 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26672"><span class="ln">26672 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26673"><span class="ln">26673 </span></a>    <span class="s1">reduce</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l26674"><span class="ln">26674 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26675"><span class="ln">26675 </span></a>    <span class="s1">include_self</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l26676"><span class="ln">26676 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26677"><span class="ln">26677 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26678"><span class="ln">26678 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26679"><span class="ln">26679 </span></a>    scatter_reduce(input, dim, index, src, reduce, *, include_self=True) -&gt; Tensor 
<a name="l26680"><span class="ln">26680 </span></a> 
<a name="l26681"><span class="ln">26681 </span></a>    Out-of-place version of :meth:`torch.Tensor.scatter_reduce_` 
<a name="l26682"><span class="ln">26682 </span></a>    &quot;&quot;&quot;</span>
<a name="l26683"><span class="ln">26683 </span></a>
<a name="l26684"><span class="ln">26684 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26685"><span class="ln">26685 </span></a><span class="s2">def </span><span class="s1">searchsorted</span><span class="s3">(</span>
<a name="l26686"><span class="ln">26686 </span></a>    <span class="s1">sorted_sequence</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26687"><span class="ln">26687 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26688"><span class="ln">26688 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26689"><span class="ln">26689 </span></a>    <span class="s1">out_int32</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l26690"><span class="ln">26690 </span></a>    <span class="s1">right</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l26691"><span class="ln">26691 </span></a>    <span class="s1">side</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26692"><span class="ln">26692 </span></a>    <span class="s1">sorter</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26693"><span class="ln">26693 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26694"><span class="ln">26694 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26695"><span class="ln">26695 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26696"><span class="ln">26696 </span></a>    searchsorted(sorted_sequence, values, *, out_int32=False, right=False, side=None, out=None, sorter=None) -&gt; Tensor 
<a name="l26697"><span class="ln">26697 </span></a> 
<a name="l26698"><span class="ln">26698 </span></a>    Find the indices from the *innermost* dimension of :attr:`sorted_sequence` such that, if the 
<a name="l26699"><span class="ln">26699 </span></a>    corresponding values in :attr:`values` were inserted before the indices, when sorted, the order 
<a name="l26700"><span class="ln">26700 </span></a>    of the corresponding *innermost* dimension within :attr:`sorted_sequence` would be preserved. 
<a name="l26701"><span class="ln">26701 </span></a>    Return a new tensor with the same size as :attr:`values`. More formally, 
<a name="l26702"><span class="ln">26702 </span></a>    the returned index satisfies the following rules: 
<a name="l26703"><span class="ln">26703 </span></a> 
<a name="l26704"><span class="ln">26704 </span></a>    .. list-table:: 
<a name="l26705"><span class="ln">26705 </span></a>       :widths: 12 10 78 
<a name="l26706"><span class="ln">26706 </span></a>       :header-rows: 1 
<a name="l26707"><span class="ln">26707 </span></a> 
<a name="l26708"><span class="ln">26708 </span></a>       * - :attr:`sorted_sequence` 
<a name="l26709"><span class="ln">26709 </span></a>         - :attr:`right` 
<a name="l26710"><span class="ln">26710 </span></a>         - *returned index satisfies* 
<a name="l26711"><span class="ln">26711 </span></a>       * - 1-D 
<a name="l26712"><span class="ln">26712 </span></a>         - False 
<a name="l26713"><span class="ln">26713 </span></a>         - ``sorted_sequence[i-1] &lt; values[m][n]...[l][x] &lt;= sorted_sequence[i]`` 
<a name="l26714"><span class="ln">26714 </span></a>       * - 1-D 
<a name="l26715"><span class="ln">26715 </span></a>         - True 
<a name="l26716"><span class="ln">26716 </span></a>         - ``sorted_sequence[i-1] &lt;= values[m][n]...[l][x] &lt; sorted_sequence[i]`` 
<a name="l26717"><span class="ln">26717 </span></a>       * - N-D 
<a name="l26718"><span class="ln">26718 </span></a>         - False 
<a name="l26719"><span class="ln">26719 </span></a>         - ``sorted_sequence[m][n]...[l][i-1] &lt; values[m][n]...[l][x] &lt;= sorted_sequence[m][n]...[l][i]`` 
<a name="l26720"><span class="ln">26720 </span></a>       * - N-D 
<a name="l26721"><span class="ln">26721 </span></a>         - True 
<a name="l26722"><span class="ln">26722 </span></a>         - ``sorted_sequence[m][n]...[l][i-1] &lt;= values[m][n]...[l][x] &lt; sorted_sequence[m][n]...[l][i]`` 
<a name="l26723"><span class="ln">26723 </span></a> 
<a name="l26724"><span class="ln">26724 </span></a>    Args: 
<a name="l26725"><span class="ln">26725 </span></a>        sorted_sequence (Tensor): N-D or 1-D tensor, containing monotonically increasing sequence on the *innermost* 
<a name="l26726"><span class="ln">26726 </span></a>                                  dimension unless :attr:`sorter` is provided, in which case the sequence does not 
<a name="l26727"><span class="ln">26727 </span></a>                                  need to be sorted 
<a name="l26728"><span class="ln">26728 </span></a>        values (Tensor or Scalar): N-D tensor or a Scalar containing the search value(s). 
<a name="l26729"><span class="ln">26729 </span></a> 
<a name="l26730"><span class="ln">26730 </span></a>    Keyword args: 
<a name="l26731"><span class="ln">26731 </span></a>        out_int32 (bool, optional): indicate the output data type. torch.int32 if True, torch.int64 otherwise. 
<a name="l26732"><span class="ln">26732 </span></a>                                    Default value is False, i.e. default output data type is torch.int64. 
<a name="l26733"><span class="ln">26733 </span></a>        right (bool, optional): if False, return the first suitable location that is found. If True, return the 
<a name="l26734"><span class="ln">26734 </span></a>                                last such index. If no suitable index found, return 0 for non-numerical value 
<a name="l26735"><span class="ln">26735 </span></a>                                (eg. nan, inf) or the size of *innermost* dimension within :attr:`sorted_sequence` 
<a name="l26736"><span class="ln">26736 </span></a>                                (one pass the last index of the *innermost* dimension). In other words, if False, 
<a name="l26737"><span class="ln">26737 </span></a>                                gets the lower bound index for each value in :attr:`values` on the corresponding 
<a name="l26738"><span class="ln">26738 </span></a>                                *innermost* dimension of the :attr:`sorted_sequence`. If True, gets the upper 
<a name="l26739"><span class="ln">26739 </span></a>                                bound index instead. Default value is False. :attr:`side` does the same and is 
<a name="l26740"><span class="ln">26740 </span></a>                                preferred. It will error if :attr:`side` is set to &quot;left&quot; while this is True. 
<a name="l26741"><span class="ln">26741 </span></a>        side (str, optional): the same as :attr:`right` but preferred. &quot;left&quot; corresponds to False for :attr:`right` 
<a name="l26742"><span class="ln">26742 </span></a>                                and &quot;right&quot; corresponds to True for :attr:`right`. It will error if this is set to 
<a name="l26743"><span class="ln">26743 </span></a>                                &quot;left&quot; while :attr:`right` is True. Default value is None. 
<a name="l26744"><span class="ln">26744 </span></a>        out (Tensor, optional): the output tensor, must be the same size as :attr:`values` if provided. 
<a name="l26745"><span class="ln">26745 </span></a>        sorter (LongTensor, optional): if provided, a tensor matching the shape of the unsorted 
<a name="l26746"><span class="ln">26746 </span></a>                                :attr:`sorted_sequence` containing a sequence of indices that sort it in the 
<a name="l26747"><span class="ln">26747 </span></a>                                ascending order on the innermost dimension 
<a name="l26748"><span class="ln">26748 </span></a> 
<a name="l26749"><span class="ln">26749 </span></a> 
<a name="l26750"><span class="ln">26750 </span></a>    Example:: 
<a name="l26751"><span class="ln">26751 </span></a> 
<a name="l26752"><span class="ln">26752 </span></a>        &gt;&gt;&gt; sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]]) 
<a name="l26753"><span class="ln">26753 </span></a>        &gt;&gt;&gt; sorted_sequence 
<a name="l26754"><span class="ln">26754 </span></a>        tensor([[ 1,  3,  5,  7,  9], 
<a name="l26755"><span class="ln">26755 </span></a>                [ 2,  4,  6,  8, 10]]) 
<a name="l26756"><span class="ln">26756 </span></a>        &gt;&gt;&gt; values = torch.tensor([[3, 6, 9], [3, 6, 9]]) 
<a name="l26757"><span class="ln">26757 </span></a>        &gt;&gt;&gt; values 
<a name="l26758"><span class="ln">26758 </span></a>        tensor([[3, 6, 9], 
<a name="l26759"><span class="ln">26759 </span></a>                [3, 6, 9]]) 
<a name="l26760"><span class="ln">26760 </span></a>        &gt;&gt;&gt; torch.searchsorted(sorted_sequence, values) 
<a name="l26761"><span class="ln">26761 </span></a>        tensor([[1, 3, 4], 
<a name="l26762"><span class="ln">26762 </span></a>                [1, 2, 4]]) 
<a name="l26763"><span class="ln">26763 </span></a>        &gt;&gt;&gt; torch.searchsorted(sorted_sequence, values, side='right') 
<a name="l26764"><span class="ln">26764 </span></a>        tensor([[2, 3, 5], 
<a name="l26765"><span class="ln">26765 </span></a>                [1, 3, 4]]) 
<a name="l26766"><span class="ln">26766 </span></a> 
<a name="l26767"><span class="ln">26767 </span></a>        &gt;&gt;&gt; sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9]) 
<a name="l26768"><span class="ln">26768 </span></a>        &gt;&gt;&gt; sorted_sequence_1d 
<a name="l26769"><span class="ln">26769 </span></a>        tensor([1, 3, 5, 7, 9]) 
<a name="l26770"><span class="ln">26770 </span></a>        &gt;&gt;&gt; torch.searchsorted(sorted_sequence_1d, values) 
<a name="l26771"><span class="ln">26771 </span></a>        tensor([[1, 3, 4], 
<a name="l26772"><span class="ln">26772 </span></a>                [1, 3, 4]]) 
<a name="l26773"><span class="ln">26773 </span></a>    &quot;&quot;&quot;</span>
<a name="l26774"><span class="ln">26774 </span></a>
<a name="l26775"><span class="ln">26775 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26776"><span class="ln">26776 </span></a><span class="s2">def </span><span class="s1">searchsorted</span><span class="s3">(</span>
<a name="l26777"><span class="ln">26777 </span></a>    <span class="s1">sorted_sequence</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26778"><span class="ln">26778 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l26779"><span class="ln">26779 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26780"><span class="ln">26780 </span></a>    <span class="s1">out_int32</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l26781"><span class="ln">26781 </span></a>    <span class="s1">right</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l26782"><span class="ln">26782 </span></a>    <span class="s1">side</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26783"><span class="ln">26783 </span></a>    <span class="s1">sorter</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26784"><span class="ln">26784 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26785"><span class="ln">26785 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26786"><span class="ln">26786 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26787"><span class="ln">26787 </span></a>    searchsorted(sorted_sequence, values, *, out_int32=False, right=False, side=None, out=None, sorter=None) -&gt; Tensor 
<a name="l26788"><span class="ln">26788 </span></a> 
<a name="l26789"><span class="ln">26789 </span></a>    Find the indices from the *innermost* dimension of :attr:`sorted_sequence` such that, if the 
<a name="l26790"><span class="ln">26790 </span></a>    corresponding values in :attr:`values` were inserted before the indices, when sorted, the order 
<a name="l26791"><span class="ln">26791 </span></a>    of the corresponding *innermost* dimension within :attr:`sorted_sequence` would be preserved. 
<a name="l26792"><span class="ln">26792 </span></a>    Return a new tensor with the same size as :attr:`values`. More formally, 
<a name="l26793"><span class="ln">26793 </span></a>    the returned index satisfies the following rules: 
<a name="l26794"><span class="ln">26794 </span></a> 
<a name="l26795"><span class="ln">26795 </span></a>    .. list-table:: 
<a name="l26796"><span class="ln">26796 </span></a>       :widths: 12 10 78 
<a name="l26797"><span class="ln">26797 </span></a>       :header-rows: 1 
<a name="l26798"><span class="ln">26798 </span></a> 
<a name="l26799"><span class="ln">26799 </span></a>       * - :attr:`sorted_sequence` 
<a name="l26800"><span class="ln">26800 </span></a>         - :attr:`right` 
<a name="l26801"><span class="ln">26801 </span></a>         - *returned index satisfies* 
<a name="l26802"><span class="ln">26802 </span></a>       * - 1-D 
<a name="l26803"><span class="ln">26803 </span></a>         - False 
<a name="l26804"><span class="ln">26804 </span></a>         - ``sorted_sequence[i-1] &lt; values[m][n]...[l][x] &lt;= sorted_sequence[i]`` 
<a name="l26805"><span class="ln">26805 </span></a>       * - 1-D 
<a name="l26806"><span class="ln">26806 </span></a>         - True 
<a name="l26807"><span class="ln">26807 </span></a>         - ``sorted_sequence[i-1] &lt;= values[m][n]...[l][x] &lt; sorted_sequence[i]`` 
<a name="l26808"><span class="ln">26808 </span></a>       * - N-D 
<a name="l26809"><span class="ln">26809 </span></a>         - False 
<a name="l26810"><span class="ln">26810 </span></a>         - ``sorted_sequence[m][n]...[l][i-1] &lt; values[m][n]...[l][x] &lt;= sorted_sequence[m][n]...[l][i]`` 
<a name="l26811"><span class="ln">26811 </span></a>       * - N-D 
<a name="l26812"><span class="ln">26812 </span></a>         - True 
<a name="l26813"><span class="ln">26813 </span></a>         - ``sorted_sequence[m][n]...[l][i-1] &lt;= values[m][n]...[l][x] &lt; sorted_sequence[m][n]...[l][i]`` 
<a name="l26814"><span class="ln">26814 </span></a> 
<a name="l26815"><span class="ln">26815 </span></a>    Args: 
<a name="l26816"><span class="ln">26816 </span></a>        sorted_sequence (Tensor): N-D or 1-D tensor, containing monotonically increasing sequence on the *innermost* 
<a name="l26817"><span class="ln">26817 </span></a>                                  dimension unless :attr:`sorter` is provided, in which case the sequence does not 
<a name="l26818"><span class="ln">26818 </span></a>                                  need to be sorted 
<a name="l26819"><span class="ln">26819 </span></a>        values (Tensor or Scalar): N-D tensor or a Scalar containing the search value(s). 
<a name="l26820"><span class="ln">26820 </span></a> 
<a name="l26821"><span class="ln">26821 </span></a>    Keyword args: 
<a name="l26822"><span class="ln">26822 </span></a>        out_int32 (bool, optional): indicate the output data type. torch.int32 if True, torch.int64 otherwise. 
<a name="l26823"><span class="ln">26823 </span></a>                                    Default value is False, i.e. default output data type is torch.int64. 
<a name="l26824"><span class="ln">26824 </span></a>        right (bool, optional): if False, return the first suitable location that is found. If True, return the 
<a name="l26825"><span class="ln">26825 </span></a>                                last such index. If no suitable index found, return 0 for non-numerical value 
<a name="l26826"><span class="ln">26826 </span></a>                                (eg. nan, inf) or the size of *innermost* dimension within :attr:`sorted_sequence` 
<a name="l26827"><span class="ln">26827 </span></a>                                (one pass the last index of the *innermost* dimension). In other words, if False, 
<a name="l26828"><span class="ln">26828 </span></a>                                gets the lower bound index for each value in :attr:`values` on the corresponding 
<a name="l26829"><span class="ln">26829 </span></a>                                *innermost* dimension of the :attr:`sorted_sequence`. If True, gets the upper 
<a name="l26830"><span class="ln">26830 </span></a>                                bound index instead. Default value is False. :attr:`side` does the same and is 
<a name="l26831"><span class="ln">26831 </span></a>                                preferred. It will error if :attr:`side` is set to &quot;left&quot; while this is True. 
<a name="l26832"><span class="ln">26832 </span></a>        side (str, optional): the same as :attr:`right` but preferred. &quot;left&quot; corresponds to False for :attr:`right` 
<a name="l26833"><span class="ln">26833 </span></a>                                and &quot;right&quot; corresponds to True for :attr:`right`. It will error if this is set to 
<a name="l26834"><span class="ln">26834 </span></a>                                &quot;left&quot; while :attr:`right` is True. Default value is None. 
<a name="l26835"><span class="ln">26835 </span></a>        out (Tensor, optional): the output tensor, must be the same size as :attr:`values` if provided. 
<a name="l26836"><span class="ln">26836 </span></a>        sorter (LongTensor, optional): if provided, a tensor matching the shape of the unsorted 
<a name="l26837"><span class="ln">26837 </span></a>                                :attr:`sorted_sequence` containing a sequence of indices that sort it in the 
<a name="l26838"><span class="ln">26838 </span></a>                                ascending order on the innermost dimension 
<a name="l26839"><span class="ln">26839 </span></a> 
<a name="l26840"><span class="ln">26840 </span></a> 
<a name="l26841"><span class="ln">26841 </span></a>    Example:: 
<a name="l26842"><span class="ln">26842 </span></a> 
<a name="l26843"><span class="ln">26843 </span></a>        &gt;&gt;&gt; sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]]) 
<a name="l26844"><span class="ln">26844 </span></a>        &gt;&gt;&gt; sorted_sequence 
<a name="l26845"><span class="ln">26845 </span></a>        tensor([[ 1,  3,  5,  7,  9], 
<a name="l26846"><span class="ln">26846 </span></a>                [ 2,  4,  6,  8, 10]]) 
<a name="l26847"><span class="ln">26847 </span></a>        &gt;&gt;&gt; values = torch.tensor([[3, 6, 9], [3, 6, 9]]) 
<a name="l26848"><span class="ln">26848 </span></a>        &gt;&gt;&gt; values 
<a name="l26849"><span class="ln">26849 </span></a>        tensor([[3, 6, 9], 
<a name="l26850"><span class="ln">26850 </span></a>                [3, 6, 9]]) 
<a name="l26851"><span class="ln">26851 </span></a>        &gt;&gt;&gt; torch.searchsorted(sorted_sequence, values) 
<a name="l26852"><span class="ln">26852 </span></a>        tensor([[1, 3, 4], 
<a name="l26853"><span class="ln">26853 </span></a>                [1, 2, 4]]) 
<a name="l26854"><span class="ln">26854 </span></a>        &gt;&gt;&gt; torch.searchsorted(sorted_sequence, values, side='right') 
<a name="l26855"><span class="ln">26855 </span></a>        tensor([[2, 3, 5], 
<a name="l26856"><span class="ln">26856 </span></a>                [1, 3, 4]]) 
<a name="l26857"><span class="ln">26857 </span></a> 
<a name="l26858"><span class="ln">26858 </span></a>        &gt;&gt;&gt; sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9]) 
<a name="l26859"><span class="ln">26859 </span></a>        &gt;&gt;&gt; sorted_sequence_1d 
<a name="l26860"><span class="ln">26860 </span></a>        tensor([1, 3, 5, 7, 9]) 
<a name="l26861"><span class="ln">26861 </span></a>        &gt;&gt;&gt; torch.searchsorted(sorted_sequence_1d, values) 
<a name="l26862"><span class="ln">26862 </span></a>        tensor([[1, 3, 4], 
<a name="l26863"><span class="ln">26863 </span></a>                [1, 3, 4]]) 
<a name="l26864"><span class="ln">26864 </span></a>    &quot;&quot;&quot;</span>
<a name="l26865"><span class="ln">26865 </span></a>
<a name="l26866"><span class="ln">26866 </span></a><span class="s2">def </span><span class="s1">segment_reduce</span><span class="s3">(</span>
<a name="l26867"><span class="ln">26867 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26868"><span class="ln">26868 </span></a>    <span class="s1">reduce</span><span class="s2">: </span><span class="s1">str</span><span class="s3">,</span>
<a name="l26869"><span class="ln">26869 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26870"><span class="ln">26870 </span></a>    <span class="s1">lengths</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26871"><span class="ln">26871 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26872"><span class="ln">26872 </span></a>    <span class="s1">offsets</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26873"><span class="ln">26873 </span></a>    <span class="s1">axis</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l26874"><span class="ln">26874 </span></a>    <span class="s1">unsafe</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l26875"><span class="ln">26875 </span></a>    <span class="s1">initial</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26876"><span class="ln">26876 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26877"><span class="ln">26877 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26878"><span class="ln">26878 </span></a><span class="s2">def </span><span class="s1">select</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">index</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26879"><span class="ln">26879 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26880"><span class="ln">26880 </span></a>    select(input, dim, index) -&gt; Tensor 
<a name="l26881"><span class="ln">26881 </span></a> 
<a name="l26882"><span class="ln">26882 </span></a>    Slices the :attr:`input` tensor along the selected dimension at the given index. 
<a name="l26883"><span class="ln">26883 </span></a>    This function returns a view of the original tensor with the given dimension removed. 
<a name="l26884"><span class="ln">26884 </span></a> 
<a name="l26885"><span class="ln">26885 </span></a>    .. note:: If :attr:`input` is a sparse tensor and returning a view of 
<a name="l26886"><span class="ln">26886 </span></a>              the tensor is not possible, a RuntimeError exception is 
<a name="l26887"><span class="ln">26887 </span></a>              raised. In this is the case, consider using 
<a name="l26888"><span class="ln">26888 </span></a>              :func:`torch.select_copy` function. 
<a name="l26889"><span class="ln">26889 </span></a> 
<a name="l26890"><span class="ln">26890 </span></a>    Args: 
<a name="l26891"><span class="ln">26891 </span></a>        input (Tensor): the input tensor. 
<a name="l26892"><span class="ln">26892 </span></a>        dim (int): the dimension to slice 
<a name="l26893"><span class="ln">26893 </span></a>        index (int): the index to select with 
<a name="l26894"><span class="ln">26894 </span></a> 
<a name="l26895"><span class="ln">26895 </span></a>    .. note:: 
<a name="l26896"><span class="ln">26896 </span></a> 
<a name="l26897"><span class="ln">26897 </span></a>        :meth:`select` is equivalent to slicing. For example, 
<a name="l26898"><span class="ln">26898 </span></a>        ``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and 
<a name="l26899"><span class="ln">26899 </span></a>        ``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``. 
<a name="l26900"><span class="ln">26900 </span></a>    &quot;&quot;&quot;</span>
<a name="l26901"><span class="ln">26901 </span></a>
<a name="l26902"><span class="ln">26902 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l26903"><span class="ln">26903 </span></a><span class="s2">def </span><span class="s1">select</span><span class="s3">(</span>
<a name="l26904"><span class="ln">26904 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26905"><span class="ln">26905 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l26906"><span class="ln">26906 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26907"><span class="ln">26907 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26908"><span class="ln">26908 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26909"><span class="ln">26909 </span></a>    select(input, dim, index) -&gt; Tensor 
<a name="l26910"><span class="ln">26910 </span></a> 
<a name="l26911"><span class="ln">26911 </span></a>    Slices the :attr:`input` tensor along the selected dimension at the given index. 
<a name="l26912"><span class="ln">26912 </span></a>    This function returns a view of the original tensor with the given dimension removed. 
<a name="l26913"><span class="ln">26913 </span></a> 
<a name="l26914"><span class="ln">26914 </span></a>    .. note:: If :attr:`input` is a sparse tensor and returning a view of 
<a name="l26915"><span class="ln">26915 </span></a>              the tensor is not possible, a RuntimeError exception is 
<a name="l26916"><span class="ln">26916 </span></a>              raised. In this is the case, consider using 
<a name="l26917"><span class="ln">26917 </span></a>              :func:`torch.select_copy` function. 
<a name="l26918"><span class="ln">26918 </span></a> 
<a name="l26919"><span class="ln">26919 </span></a>    Args: 
<a name="l26920"><span class="ln">26920 </span></a>        input (Tensor): the input tensor. 
<a name="l26921"><span class="ln">26921 </span></a>        dim (int): the dimension to slice 
<a name="l26922"><span class="ln">26922 </span></a>        index (int): the index to select with 
<a name="l26923"><span class="ln">26923 </span></a> 
<a name="l26924"><span class="ln">26924 </span></a>    .. note:: 
<a name="l26925"><span class="ln">26925 </span></a> 
<a name="l26926"><span class="ln">26926 </span></a>        :meth:`select` is equivalent to slicing. For example, 
<a name="l26927"><span class="ln">26927 </span></a>        ``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and 
<a name="l26928"><span class="ln">26928 </span></a>        ``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``. 
<a name="l26929"><span class="ln">26929 </span></a>    &quot;&quot;&quot;</span>
<a name="l26930"><span class="ln">26930 </span></a>
<a name="l26931"><span class="ln">26931 </span></a><span class="s2">def </span><span class="s1">select_copy</span><span class="s3">(</span>
<a name="l26932"><span class="ln">26932 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26933"><span class="ln">26933 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26934"><span class="ln">26934 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l26935"><span class="ln">26935 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l26936"><span class="ln">26936 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l26937"><span class="ln">26937 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26938"><span class="ln">26938 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26939"><span class="ln">26939 </span></a>    Performs the same operation as :func:`torch.select`, but all output tensors 
<a name="l26940"><span class="ln">26940 </span></a>    are freshly created instead of aliasing the input. 
<a name="l26941"><span class="ln">26941 </span></a>    &quot;&quot;&quot;</span>
<a name="l26942"><span class="ln">26942 </span></a>
<a name="l26943"><span class="ln">26943 </span></a><span class="s2">def </span><span class="s1">select_scatter</span><span class="s3">(</span>
<a name="l26944"><span class="ln">26944 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26945"><span class="ln">26945 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l26946"><span class="ln">26946 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l26947"><span class="ln">26947 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l26948"><span class="ln">26948 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l26949"><span class="ln">26949 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26950"><span class="ln">26950 </span></a>    select_scatter(input, src, dim, index) -&gt; Tensor 
<a name="l26951"><span class="ln">26951 </span></a> 
<a name="l26952"><span class="ln">26952 </span></a>    Embeds the values of the :attr:`src` tensor into :attr:`input` at the given index. 
<a name="l26953"><span class="ln">26953 </span></a>    This function returns a tensor with fresh storage; it does not create a view. 
<a name="l26954"><span class="ln">26954 </span></a> 
<a name="l26955"><span class="ln">26955 </span></a> 
<a name="l26956"><span class="ln">26956 </span></a>    Args: 
<a name="l26957"><span class="ln">26957 </span></a>        input (Tensor): the input tensor. 
<a name="l26958"><span class="ln">26958 </span></a>        src (Tensor): The tensor to embed into :attr:`input` 
<a name="l26959"><span class="ln">26959 </span></a>        dim (int): the dimension to insert the slice into. 
<a name="l26960"><span class="ln">26960 </span></a>        index (int): the index to select with 
<a name="l26961"><span class="ln">26961 </span></a> 
<a name="l26962"><span class="ln">26962 </span></a>    .. note:: 
<a name="l26963"><span class="ln">26963 </span></a> 
<a name="l26964"><span class="ln">26964 </span></a>        :attr:`src` must be of the proper size in order to be embedded 
<a name="l26965"><span class="ln">26965 </span></a>        into :attr:`input`. Specifically, it should have the same shape as 
<a name="l26966"><span class="ln">26966 </span></a>        ``torch.select(input, dim, index)`` 
<a name="l26967"><span class="ln">26967 </span></a> 
<a name="l26968"><span class="ln">26968 </span></a>    Example:: 
<a name="l26969"><span class="ln">26969 </span></a> 
<a name="l26970"><span class="ln">26970 </span></a>        &gt;&gt;&gt; a = torch.zeros(2, 2) 
<a name="l26971"><span class="ln">26971 </span></a>        &gt;&gt;&gt; b = torch.ones(2) 
<a name="l26972"><span class="ln">26972 </span></a>        &gt;&gt;&gt; a.select_scatter(b, 0, 0) 
<a name="l26973"><span class="ln">26973 </span></a>        tensor([[1., 1.], 
<a name="l26974"><span class="ln">26974 </span></a>                [0., 0.]]) 
<a name="l26975"><span class="ln">26975 </span></a>    &quot;&quot;&quot;</span>
<a name="l26976"><span class="ln">26976 </span></a>
<a name="l26977"><span class="ln">26977 </span></a><span class="s2">def </span><span class="s1">selu</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26978"><span class="ln">26978 </span></a><span class="s2">def </span><span class="s1">selu_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l26979"><span class="ln">26979 </span></a><span class="s2">def </span><span class="s1">set_flush_denormal</span><span class="s3">(</span><span class="s1">mode</span><span class="s2">: </span><span class="s1">_bool</span><span class="s3">) </span><span class="s1">-&gt; _bool</span><span class="s2">:</span>
<a name="l26980"><span class="ln">26980 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l26981"><span class="ln">26981 </span></a>    set_flush_denormal(mode) -&gt; bool 
<a name="l26982"><span class="ln">26982 </span></a> 
<a name="l26983"><span class="ln">26983 </span></a>    Disables denormal floating numbers on CPU. 
<a name="l26984"><span class="ln">26984 </span></a> 
<a name="l26985"><span class="ln">26985 </span></a>    Returns ``True`` if your system supports flushing denormal numbers and it 
<a name="l26986"><span class="ln">26986 </span></a>    successfully configures flush denormal mode.  :meth:`~torch.set_flush_denormal` 
<a name="l26987"><span class="ln">26987 </span></a>    is supported on x86 architectures supporting SSE3 and AArch64 architecture. 
<a name="l26988"><span class="ln">26988 </span></a> 
<a name="l26989"><span class="ln">26989 </span></a>    Args: 
<a name="l26990"><span class="ln">26990 </span></a>        mode (bool): Controls whether to enable flush denormal mode or not 
<a name="l26991"><span class="ln">26991 </span></a> 
<a name="l26992"><span class="ln">26992 </span></a>    Example:: 
<a name="l26993"><span class="ln">26993 </span></a> 
<a name="l26994"><span class="ln">26994 </span></a>        &gt;&gt;&gt; torch.set_flush_denormal(True) 
<a name="l26995"><span class="ln">26995 </span></a>        True 
<a name="l26996"><span class="ln">26996 </span></a>        &gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64) 
<a name="l26997"><span class="ln">26997 </span></a>        tensor([ 0.], dtype=torch.float64) 
<a name="l26998"><span class="ln">26998 </span></a>        &gt;&gt;&gt; torch.set_flush_denormal(False) 
<a name="l26999"><span class="ln">26999 </span></a>        True 
<a name="l27000"><span class="ln">27000 </span></a>        &gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64) 
<a name="l27001"><span class="ln">27001 </span></a>        tensor(9.88131e-324 * 
<a name="l27002"><span class="ln">27002 </span></a>               [ 1.0000], dtype=torch.float64) 
<a name="l27003"><span class="ln">27003 </span></a>    &quot;&quot;&quot;</span>
<a name="l27004"><span class="ln">27004 </span></a>
<a name="l27005"><span class="ln">27005 </span></a><span class="s2">def </span><span class="s1">set_num_interop_threads</span><span class="s3">(</span><span class="s1">num</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l27006"><span class="ln">27006 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27007"><span class="ln">27007 </span></a>    set_num_interop_threads(int) 
<a name="l27008"><span class="ln">27008 </span></a> 
<a name="l27009"><span class="ln">27009 </span></a>    Sets the number of threads used for interop parallelism 
<a name="l27010"><span class="ln">27010 </span></a>    (e.g. in JIT interpreter) on CPU. 
<a name="l27011"><span class="ln">27011 </span></a> 
<a name="l27012"><span class="ln">27012 </span></a>    .. warning:: 
<a name="l27013"><span class="ln">27013 </span></a>        Can only be called once and before any inter-op parallel work 
<a name="l27014"><span class="ln">27014 </span></a>        is started (e.g. JIT execution). 
<a name="l27015"><span class="ln">27015 </span></a>    &quot;&quot;&quot;</span>
<a name="l27016"><span class="ln">27016 </span></a>
<a name="l27017"><span class="ln">27017 </span></a><span class="s2">def </span><span class="s1">set_num_threads</span><span class="s3">(</span><span class="s1">num</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l27018"><span class="ln">27018 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27019"><span class="ln">27019 </span></a>    set_num_threads(int) 
<a name="l27020"><span class="ln">27020 </span></a> 
<a name="l27021"><span class="ln">27021 </span></a>    Sets the number of threads used for intraop parallelism on CPU. 
<a name="l27022"><span class="ln">27022 </span></a> 
<a name="l27023"><span class="ln">27023 </span></a>    .. warning:: 
<a name="l27024"><span class="ln">27024 </span></a>        To ensure that the correct number of threads is used, set_num_threads 
<a name="l27025"><span class="ln">27025 </span></a>        must be called before running eager, JIT or autograd code. 
<a name="l27026"><span class="ln">27026 </span></a>    &quot;&quot;&quot;</span>
<a name="l27027"><span class="ln">27027 </span></a>
<a name="l27028"><span class="ln">27028 </span></a><span class="s2">def </span><span class="s1">sgn</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27029"><span class="ln">27029 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27030"><span class="ln">27030 </span></a>    sgn(input, *, out=None) -&gt; Tensor 
<a name="l27031"><span class="ln">27031 </span></a> 
<a name="l27032"><span class="ln">27032 </span></a>    This function is an extension of torch.sign() to complex tensors. 
<a name="l27033"><span class="ln">27033 </span></a>    It computes a new tensor whose elements have 
<a name="l27034"><span class="ln">27034 </span></a>    the same angles as the corresponding elements of :attr:`input` and 
<a name="l27035"><span class="ln">27035 </span></a>    absolute values (i.e. magnitudes) of one for complex tensors and 
<a name="l27036"><span class="ln">27036 </span></a>    is equivalent to torch.sign() for non-complex tensors. 
<a name="l27037"><span class="ln">27037 </span></a> 
<a name="l27038"><span class="ln">27038 </span></a>    .. math:: 
<a name="l27039"><span class="ln">27039 </span></a>        \text{out}_{i} = \begin{cases} 
<a name="l27040"><span class="ln">27040 </span></a>                        0 &amp; |\text{{input}}_i| == 0 \\ 
<a name="l27041"><span class="ln">27041 </span></a>                        \frac{{\text{{input}}_i}}{|{\text{{input}}_i}|} &amp; \text{otherwise} 
<a name="l27042"><span class="ln">27042 </span></a>                        \end{cases} 
<a name="l27043"><span class="ln">27043 </span></a> 
<a name="l27044"><span class="ln">27044 </span></a> 
<a name="l27045"><span class="ln">27045 </span></a>    Args: 
<a name="l27046"><span class="ln">27046 </span></a>        input (Tensor): the input tensor. 
<a name="l27047"><span class="ln">27047 </span></a> 
<a name="l27048"><span class="ln">27048 </span></a>    Keyword args: 
<a name="l27049"><span class="ln">27049 </span></a>      out (Tensor, optional): the output tensor. 
<a name="l27050"><span class="ln">27050 </span></a> 
<a name="l27051"><span class="ln">27051 </span></a>    Example:: 
<a name="l27052"><span class="ln">27052 </span></a> 
<a name="l27053"><span class="ln">27053 </span></a>        &gt;&gt;&gt; t = torch.tensor([3+4j, 7-24j, 0, 1+2j]) 
<a name="l27054"><span class="ln">27054 </span></a>        &gt;&gt;&gt; t.sgn() 
<a name="l27055"><span class="ln">27055 </span></a>        tensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j]) 
<a name="l27056"><span class="ln">27056 </span></a>    &quot;&quot;&quot;</span>
<a name="l27057"><span class="ln">27057 </span></a>
<a name="l27058"><span class="ln">27058 </span></a><span class="s2">def </span><span class="s1">sigmoid</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27059"><span class="ln">27059 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27060"><span class="ln">27060 </span></a>    sigmoid(input, *, out=None) -&gt; Tensor 
<a name="l27061"><span class="ln">27061 </span></a> 
<a name="l27062"><span class="ln">27062 </span></a>    Alias for :func:`torch.special.expit`. 
<a name="l27063"><span class="ln">27063 </span></a>    &quot;&quot;&quot;</span>
<a name="l27064"><span class="ln">27064 </span></a>
<a name="l27065"><span class="ln">27065 </span></a><span class="s2">def </span><span class="s1">sigmoid_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l27066"><span class="ln">27066 </span></a><span class="s2">def </span><span class="s1">sign</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27067"><span class="ln">27067 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27068"><span class="ln">27068 </span></a>    sign(input, *, out=None) -&gt; Tensor 
<a name="l27069"><span class="ln">27069 </span></a> 
<a name="l27070"><span class="ln">27070 </span></a>    Returns a new tensor with the signs of the elements of :attr:`input`. 
<a name="l27071"><span class="ln">27071 </span></a> 
<a name="l27072"><span class="ln">27072 </span></a>    .. math:: 
<a name="l27073"><span class="ln">27073 </span></a>        \text{out}_{i} = \operatorname{sgn}(\text{input}_{i}) 
<a name="l27074"><span class="ln">27074 </span></a> 
<a name="l27075"><span class="ln">27075 </span></a>    Args: 
<a name="l27076"><span class="ln">27076 </span></a>        input (Tensor): the input tensor. 
<a name="l27077"><span class="ln">27077 </span></a> 
<a name="l27078"><span class="ln">27078 </span></a>    Keyword args: 
<a name="l27079"><span class="ln">27079 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l27080"><span class="ln">27080 </span></a> 
<a name="l27081"><span class="ln">27081 </span></a>    Example:: 
<a name="l27082"><span class="ln">27082 </span></a> 
<a name="l27083"><span class="ln">27083 </span></a>        &gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3]) 
<a name="l27084"><span class="ln">27084 </span></a>        &gt;&gt;&gt; a 
<a name="l27085"><span class="ln">27085 </span></a>        tensor([ 0.7000, -1.2000,  0.0000,  2.3000]) 
<a name="l27086"><span class="ln">27086 </span></a>        &gt;&gt;&gt; torch.sign(a) 
<a name="l27087"><span class="ln">27087 </span></a>        tensor([ 1., -1.,  0.,  1.]) 
<a name="l27088"><span class="ln">27088 </span></a>    &quot;&quot;&quot;</span>
<a name="l27089"><span class="ln">27089 </span></a>
<a name="l27090"><span class="ln">27090 </span></a><span class="s2">def </span><span class="s1">signbit</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27091"><span class="ln">27091 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27092"><span class="ln">27092 </span></a>    signbit(input, *, out=None) -&gt; Tensor 
<a name="l27093"><span class="ln">27093 </span></a> 
<a name="l27094"><span class="ln">27094 </span></a>    Tests if each element of :attr:`input` has its sign bit set or not. 
<a name="l27095"><span class="ln">27095 </span></a> 
<a name="l27096"><span class="ln">27096 </span></a>    Args: 
<a name="l27097"><span class="ln">27097 </span></a>      input (Tensor): the input tensor. 
<a name="l27098"><span class="ln">27098 </span></a> 
<a name="l27099"><span class="ln">27099 </span></a>    Keyword args: 
<a name="l27100"><span class="ln">27100 </span></a>      out (Tensor, optional): the output tensor. 
<a name="l27101"><span class="ln">27101 </span></a> 
<a name="l27102"><span class="ln">27102 </span></a>    Example:: 
<a name="l27103"><span class="ln">27103 </span></a> 
<a name="l27104"><span class="ln">27104 </span></a>        &gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3]) 
<a name="l27105"><span class="ln">27105 </span></a>        &gt;&gt;&gt; torch.signbit(a) 
<a name="l27106"><span class="ln">27106 </span></a>        tensor([ False, True,  False,  False]) 
<a name="l27107"><span class="ln">27107 </span></a>        &gt;&gt;&gt; a = torch.tensor([-0.0, 0.0]) 
<a name="l27108"><span class="ln">27108 </span></a>        &gt;&gt;&gt; torch.signbit(a) 
<a name="l27109"><span class="ln">27109 </span></a>        tensor([ True,  False]) 
<a name="l27110"><span class="ln">27110 </span></a> 
<a name="l27111"><span class="ln">27111 </span></a>    .. note:: 
<a name="l27112"><span class="ln">27112 </span></a>        signbit handles signed zeros, so negative zero (-0) returns True. 
<a name="l27113"><span class="ln">27113 </span></a>    &quot;&quot;&quot;</span>
<a name="l27114"><span class="ln">27114 </span></a>
<a name="l27115"><span class="ln">27115 </span></a><span class="s2">def </span><span class="s1">sin</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27116"><span class="ln">27116 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27117"><span class="ln">27117 </span></a>    sin(input, *, out=None) -&gt; Tensor 
<a name="l27118"><span class="ln">27118 </span></a> 
<a name="l27119"><span class="ln">27119 </span></a>    Returns a new tensor with the sine of the elements of :attr:`input`. 
<a name="l27120"><span class="ln">27120 </span></a> 
<a name="l27121"><span class="ln">27121 </span></a>    .. math:: 
<a name="l27122"><span class="ln">27122 </span></a>        \text{out}_{i} = \sin(\text{input}_{i}) 
<a name="l27123"><span class="ln">27123 </span></a> 
<a name="l27124"><span class="ln">27124 </span></a>    Args: 
<a name="l27125"><span class="ln">27125 </span></a>        input (Tensor): the input tensor. 
<a name="l27126"><span class="ln">27126 </span></a> 
<a name="l27127"><span class="ln">27127 </span></a>    Keyword args: 
<a name="l27128"><span class="ln">27128 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l27129"><span class="ln">27129 </span></a> 
<a name="l27130"><span class="ln">27130 </span></a>    Example:: 
<a name="l27131"><span class="ln">27131 </span></a> 
<a name="l27132"><span class="ln">27132 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l27133"><span class="ln">27133 </span></a>        &gt;&gt;&gt; a 
<a name="l27134"><span class="ln">27134 </span></a>        tensor([-0.5461,  0.1347, -2.7266, -0.2746]) 
<a name="l27135"><span class="ln">27135 </span></a>        &gt;&gt;&gt; torch.sin(a) 
<a name="l27136"><span class="ln">27136 </span></a>        tensor([-0.5194,  0.1343, -0.4032, -0.2711]) 
<a name="l27137"><span class="ln">27137 </span></a>    &quot;&quot;&quot;</span>
<a name="l27138"><span class="ln">27138 </span></a>
<a name="l27139"><span class="ln">27139 </span></a><span class="s2">def </span><span class="s1">sin_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l27140"><span class="ln">27140 </span></a><span class="s2">def </span><span class="s1">sinc</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27141"><span class="ln">27141 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27142"><span class="ln">27142 </span></a>    sinc(input, *, out=None) -&gt; Tensor 
<a name="l27143"><span class="ln">27143 </span></a> 
<a name="l27144"><span class="ln">27144 </span></a>    Alias for :func:`torch.special.sinc`. 
<a name="l27145"><span class="ln">27145 </span></a>    &quot;&quot;&quot;</span>
<a name="l27146"><span class="ln">27146 </span></a>
<a name="l27147"><span class="ln">27147 </span></a><span class="s2">def </span><span class="s1">sinc_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l27148"><span class="ln">27148 </span></a><span class="s2">def </span><span class="s1">sinh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27149"><span class="ln">27149 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27150"><span class="ln">27150 </span></a>    sinh(input, *, out=None) -&gt; Tensor 
<a name="l27151"><span class="ln">27151 </span></a> 
<a name="l27152"><span class="ln">27152 </span></a>    Returns a new tensor with the hyperbolic sine of the elements of 
<a name="l27153"><span class="ln">27153 </span></a>    :attr:`input`. 
<a name="l27154"><span class="ln">27154 </span></a> 
<a name="l27155"><span class="ln">27155 </span></a>    .. math:: 
<a name="l27156"><span class="ln">27156 </span></a>        \text{out}_{i} = \sinh(\text{input}_{i}) 
<a name="l27157"><span class="ln">27157 </span></a> 
<a name="l27158"><span class="ln">27158 </span></a>    Args: 
<a name="l27159"><span class="ln">27159 </span></a>        input (Tensor): the input tensor. 
<a name="l27160"><span class="ln">27160 </span></a> 
<a name="l27161"><span class="ln">27161 </span></a>    Keyword args: 
<a name="l27162"><span class="ln">27162 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l27163"><span class="ln">27163 </span></a> 
<a name="l27164"><span class="ln">27164 </span></a>    Example:: 
<a name="l27165"><span class="ln">27165 </span></a> 
<a name="l27166"><span class="ln">27166 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l27167"><span class="ln">27167 </span></a>        &gt;&gt;&gt; a 
<a name="l27168"><span class="ln">27168 </span></a>        tensor([ 0.5380, -0.8632, -0.1265,  0.9399]) 
<a name="l27169"><span class="ln">27169 </span></a>        &gt;&gt;&gt; torch.sinh(a) 
<a name="l27170"><span class="ln">27170 </span></a>        tensor([ 0.5644, -0.9744, -0.1268,  1.0845]) 
<a name="l27171"><span class="ln">27171 </span></a> 
<a name="l27172"><span class="ln">27172 </span></a>    .. note:: 
<a name="l27173"><span class="ln">27173 </span></a>       When :attr:`input` is on the CPU, the implementation of torch.sinh may use 
<a name="l27174"><span class="ln">27174 </span></a>       the Sleef library, which rounds very large results to infinity or negative 
<a name="l27175"><span class="ln">27175 </span></a>       infinity. See `here &lt;https://sleef.org/purec.xhtml&gt;`_ for details. 
<a name="l27176"><span class="ln">27176 </span></a>    &quot;&quot;&quot;</span>
<a name="l27177"><span class="ln">27177 </span></a>
<a name="l27178"><span class="ln">27178 </span></a><span class="s2">def </span><span class="s1">sinh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l27179"><span class="ln">27179 </span></a><span class="s2">def </span><span class="s1">slice_copy</span><span class="s3">(</span>
<a name="l27180"><span class="ln">27180 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27181"><span class="ln">27181 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l27182"><span class="ln">27182 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27183"><span class="ln">27183 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27184"><span class="ln">27184 </span></a>    <span class="s1">step</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l27185"><span class="ln">27185 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27186"><span class="ln">27186 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27187"><span class="ln">27187 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27188"><span class="ln">27188 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27189"><span class="ln">27189 </span></a>    Performs the same operation as :func:`torch.slice`, but all output tensors 
<a name="l27190"><span class="ln">27190 </span></a>    are freshly created instead of aliasing the input. 
<a name="l27191"><span class="ln">27191 </span></a>    &quot;&quot;&quot;</span>
<a name="l27192"><span class="ln">27192 </span></a>
<a name="l27193"><span class="ln">27193 </span></a><span class="s2">def </span><span class="s1">slice_inverse</span><span class="s3">(</span>
<a name="l27194"><span class="ln">27194 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27195"><span class="ln">27195 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27196"><span class="ln">27196 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l27197"><span class="ln">27197 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27198"><span class="ln">27198 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27199"><span class="ln">27199 </span></a>    <span class="s1">step</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l27200"><span class="ln">27200 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l27201"><span class="ln">27201 </span></a><span class="s2">def </span><span class="s1">slice_scatter</span><span class="s3">(</span>
<a name="l27202"><span class="ln">27202 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27203"><span class="ln">27203 </span></a>    <span class="s1">src</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27204"><span class="ln">27204 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l27205"><span class="ln">27205 </span></a>    <span class="s1">start</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27206"><span class="ln">27206 </span></a>    <span class="s1">end</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27207"><span class="ln">27207 </span></a>    <span class="s1">step</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l27208"><span class="ln">27208 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27209"><span class="ln">27209 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27210"><span class="ln">27210 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27211"><span class="ln">27211 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27212"><span class="ln">27212 </span></a>    slice_scatter(input, src, dim=0, start=None, end=None, step=1) -&gt; Tensor 
<a name="l27213"><span class="ln">27213 </span></a> 
<a name="l27214"><span class="ln">27214 </span></a>    Embeds the values of the :attr:`src` tensor into :attr:`input` at the given 
<a name="l27215"><span class="ln">27215 </span></a>    dimension. 
<a name="l27216"><span class="ln">27216 </span></a>    This function returns a tensor with fresh storage; it does not create a view. 
<a name="l27217"><span class="ln">27217 </span></a> 
<a name="l27218"><span class="ln">27218 </span></a> 
<a name="l27219"><span class="ln">27219 </span></a>    Args: 
<a name="l27220"><span class="ln">27220 </span></a>        input (Tensor): the input tensor. 
<a name="l27221"><span class="ln">27221 </span></a>        src (Tensor): The tensor to embed into :attr:`input` 
<a name="l27222"><span class="ln">27222 </span></a>        dim (int): the dimension to insert the slice into 
<a name="l27223"><span class="ln">27223 </span></a>        start (Optional[int]): the start index of where to insert the slice 
<a name="l27224"><span class="ln">27224 </span></a>        end (Optional[int]): the end index of where to insert the slice 
<a name="l27225"><span class="ln">27225 </span></a>        step (int): the how many elements to skip in 
<a name="l27226"><span class="ln">27226 </span></a> 
<a name="l27227"><span class="ln">27227 </span></a>    Example:: 
<a name="l27228"><span class="ln">27228 </span></a> 
<a name="l27229"><span class="ln">27229 </span></a>        &gt;&gt;&gt; a = torch.zeros(8, 8) 
<a name="l27230"><span class="ln">27230 </span></a>        &gt;&gt;&gt; b = torch.ones(2, 8) 
<a name="l27231"><span class="ln">27231 </span></a>        &gt;&gt;&gt; a.slice_scatter(b, start=6) 
<a name="l27232"><span class="ln">27232 </span></a>        tensor([[0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l27233"><span class="ln">27233 </span></a>                [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l27234"><span class="ln">27234 </span></a>                [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l27235"><span class="ln">27235 </span></a>                [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l27236"><span class="ln">27236 </span></a>                [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l27237"><span class="ln">27237 </span></a>                [0., 0., 0., 0., 0., 0., 0., 0.], 
<a name="l27238"><span class="ln">27238 </span></a>                [1., 1., 1., 1., 1., 1., 1., 1.], 
<a name="l27239"><span class="ln">27239 </span></a>                [1., 1., 1., 1., 1., 1., 1., 1.]]) 
<a name="l27240"><span class="ln">27240 </span></a> 
<a name="l27241"><span class="ln">27241 </span></a>        &gt;&gt;&gt; b = torch.ones(8, 2) 
<a name="l27242"><span class="ln">27242 </span></a>        &gt;&gt;&gt; a.slice_scatter(b, dim=1, start=2, end=6, step=2) 
<a name="l27243"><span class="ln">27243 </span></a>        tensor([[0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l27244"><span class="ln">27244 </span></a>                [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l27245"><span class="ln">27245 </span></a>                [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l27246"><span class="ln">27246 </span></a>                [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l27247"><span class="ln">27247 </span></a>                [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l27248"><span class="ln">27248 </span></a>                [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l27249"><span class="ln">27249 </span></a>                [0., 0., 1., 0., 1., 0., 0., 0.], 
<a name="l27250"><span class="ln">27250 </span></a>                [0., 0., 1., 0., 1., 0., 0., 0.]]) 
<a name="l27251"><span class="ln">27251 </span></a>    &quot;&quot;&quot;</span>
<a name="l27252"><span class="ln">27252 </span></a>
<a name="l27253"><span class="ln">27253 </span></a><span class="s2">def </span><span class="s1">slogdet</span><span class="s3">(</span>
<a name="l27254"><span class="ln">27254 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27255"><span class="ln">27255 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27256"><span class="ln">27256 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27257"><span class="ln">27257 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">slogdet</span><span class="s2">:</span>
<a name="l27258"><span class="ln">27258 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27259"><span class="ln">27259 </span></a>    slogdet(input) -&gt; (Tensor, Tensor) 
<a name="l27260"><span class="ln">27260 </span></a> 
<a name="l27261"><span class="ln">27261 </span></a>    Alias for :func:`torch.linalg.slogdet` 
<a name="l27262"><span class="ln">27262 </span></a>    &quot;&quot;&quot;</span>
<a name="l27263"><span class="ln">27263 </span></a>
<a name="l27264"><span class="ln">27264 </span></a><span class="s2">def </span><span class="s1">smm</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27265"><span class="ln">27265 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27266"><span class="ln">27266 </span></a>    smm(input, mat) -&gt; Tensor 
<a name="l27267"><span class="ln">27267 </span></a> 
<a name="l27268"><span class="ln">27268 </span></a>    Performs a matrix multiplication of the sparse matrix :attr:`input` 
<a name="l27269"><span class="ln">27269 </span></a>    with the dense matrix :attr:`mat`. 
<a name="l27270"><span class="ln">27270 </span></a> 
<a name="l27271"><span class="ln">27271 </span></a>    Args: 
<a name="l27272"><span class="ln">27272 </span></a>        input (Tensor): a sparse matrix to be matrix multiplied 
<a name="l27273"><span class="ln">27273 </span></a>        mat (Tensor): a dense matrix to be matrix multiplied 
<a name="l27274"><span class="ln">27274 </span></a>    &quot;&quot;&quot;</span>
<a name="l27275"><span class="ln">27275 </span></a>
<a name="l27276"><span class="ln">27276 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l27277"><span class="ln">27277 </span></a><span class="s2">def </span><span class="s1">softmax</span><span class="s3">(</span>
<a name="l27278"><span class="ln">27278 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27279"><span class="ln">27279 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l27280"><span class="ln">27280 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27281"><span class="ln">27281 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27282"><span class="ln">27282 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27283"><span class="ln">27283 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27284"><span class="ln">27284 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27285"><span class="ln">27285 </span></a>    softmax(input, dim, *, dtype=None) -&gt; Tensor 
<a name="l27286"><span class="ln">27286 </span></a> 
<a name="l27287"><span class="ln">27287 </span></a>    Alias for :func:`torch.nn.functional.softmax`. 
<a name="l27288"><span class="ln">27288 </span></a>    &quot;&quot;&quot;</span>
<a name="l27289"><span class="ln">27289 </span></a>
<a name="l27290"><span class="ln">27290 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l27291"><span class="ln">27291 </span></a><span class="s2">def </span><span class="s1">softmax</span><span class="s3">(</span>
<a name="l27292"><span class="ln">27292 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27293"><span class="ln">27293 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l27294"><span class="ln">27294 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27295"><span class="ln">27295 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27296"><span class="ln">27296 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27297"><span class="ln">27297 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27298"><span class="ln">27298 </span></a>    softmax(input, dim, *, dtype=None) -&gt; Tensor 
<a name="l27299"><span class="ln">27299 </span></a> 
<a name="l27300"><span class="ln">27300 </span></a>    Alias for :func:`torch.nn.functional.softmax`. 
<a name="l27301"><span class="ln">27301 </span></a>    &quot;&quot;&quot;</span>
<a name="l27302"><span class="ln">27302 </span></a>
<a name="l27303"><span class="ln">27303 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l27304"><span class="ln">27304 </span></a><span class="s2">def </span><span class="s1">sort</span><span class="s3">(</span>
<a name="l27305"><span class="ln">27305 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27306"><span class="ln">27306 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27307"><span class="ln">27307 </span></a>    <span class="s1">stable</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l27308"><span class="ln">27308 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l27309"><span class="ln">27309 </span></a>    <span class="s1">descending</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27310"><span class="ln">27310 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27311"><span class="ln">27311 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">sort</span><span class="s2">:</span>
<a name="l27312"><span class="ln">27312 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27313"><span class="ln">27313 </span></a>    sort(input, dim=-1, descending=False, stable=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l27314"><span class="ln">27314 </span></a> 
<a name="l27315"><span class="ln">27315 </span></a>    Sorts the elements of the :attr:`input` tensor along a given dimension 
<a name="l27316"><span class="ln">27316 </span></a>    in ascending order by value. 
<a name="l27317"><span class="ln">27317 </span></a> 
<a name="l27318"><span class="ln">27318 </span></a>    If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l27319"><span class="ln">27319 </span></a> 
<a name="l27320"><span class="ln">27320 </span></a>    If :attr:`descending` is ``True`` then the elements are sorted in descending 
<a name="l27321"><span class="ln">27321 </span></a>    order by value. 
<a name="l27322"><span class="ln">27322 </span></a> 
<a name="l27323"><span class="ln">27323 </span></a>    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l27324"><span class="ln">27324 </span></a>    the order of equivalent elements. 
<a name="l27325"><span class="ln">27325 </span></a> 
<a name="l27326"><span class="ln">27326 </span></a>    A namedtuple of (values, indices) is returned, where the `values` are the 
<a name="l27327"><span class="ln">27327 </span></a>    sorted values and `indices` are the indices of the elements in the original 
<a name="l27328"><span class="ln">27328 </span></a>    `input` tensor. 
<a name="l27329"><span class="ln">27329 </span></a> 
<a name="l27330"><span class="ln">27330 </span></a>    Args: 
<a name="l27331"><span class="ln">27331 </span></a>        input (Tensor): the input tensor. 
<a name="l27332"><span class="ln">27332 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l27333"><span class="ln">27333 </span></a>        descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l27334"><span class="ln">27334 </span></a>        stable (bool, optional): makes the sorting routine stable, which guarantees that the order 
<a name="l27335"><span class="ln">27335 </span></a>           of equivalent elements is preserved. 
<a name="l27336"><span class="ln">27336 </span></a> 
<a name="l27337"><span class="ln">27337 </span></a>    Keyword args: 
<a name="l27338"><span class="ln">27338 </span></a>        out (tuple, optional): the output tuple of (`Tensor`, `LongTensor`) that can 
<a name="l27339"><span class="ln">27339 </span></a>            be optionally given to be used as output buffers 
<a name="l27340"><span class="ln">27340 </span></a> 
<a name="l27341"><span class="ln">27341 </span></a>    Example:: 
<a name="l27342"><span class="ln">27342 </span></a> 
<a name="l27343"><span class="ln">27343 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l27344"><span class="ln">27344 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x) 
<a name="l27345"><span class="ln">27345 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27346"><span class="ln">27346 </span></a>        tensor([[-0.2162,  0.0608,  0.6719,  2.3332], 
<a name="l27347"><span class="ln">27347 </span></a>                [-0.5793,  0.0061,  0.6058,  0.9497], 
<a name="l27348"><span class="ln">27348 </span></a>                [-0.5071,  0.3343,  0.9553,  1.0960]]) 
<a name="l27349"><span class="ln">27349 </span></a>        &gt;&gt;&gt; indices 
<a name="l27350"><span class="ln">27350 </span></a>        tensor([[ 1,  0,  2,  3], 
<a name="l27351"><span class="ln">27351 </span></a>                [ 3,  1,  0,  2], 
<a name="l27352"><span class="ln">27352 </span></a>                [ 0,  3,  1,  2]]) 
<a name="l27353"><span class="ln">27353 </span></a> 
<a name="l27354"><span class="ln">27354 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) 
<a name="l27355"><span class="ln">27355 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27356"><span class="ln">27356 </span></a>        tensor([[-0.5071, -0.2162,  0.6719, -0.5793], 
<a name="l27357"><span class="ln">27357 </span></a>                [ 0.0608,  0.0061,  0.9497,  0.3343], 
<a name="l27358"><span class="ln">27358 </span></a>                [ 0.6058,  0.9553,  1.0960,  2.3332]]) 
<a name="l27359"><span class="ln">27359 </span></a>        &gt;&gt;&gt; indices 
<a name="l27360"><span class="ln">27360 </span></a>        tensor([[ 2,  0,  0,  1], 
<a name="l27361"><span class="ln">27361 </span></a>                [ 0,  1,  1,  2], 
<a name="l27362"><span class="ln">27362 </span></a>                [ 1,  2,  2,  0]]) 
<a name="l27363"><span class="ln">27363 </span></a>        &gt;&gt;&gt; x = torch.tensor([0, 1] * 9) 
<a name="l27364"><span class="ln">27364 </span></a>        &gt;&gt;&gt; x.sort() 
<a name="l27365"><span class="ln">27365 </span></a>        torch.return_types.sort( 
<a name="l27366"><span class="ln">27366 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27367"><span class="ln">27367 </span></a>            indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1])) 
<a name="l27368"><span class="ln">27368 </span></a>        &gt;&gt;&gt; x.sort(stable=True) 
<a name="l27369"><span class="ln">27369 </span></a>        torch.return_types.sort( 
<a name="l27370"><span class="ln">27370 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27371"><span class="ln">27371 </span></a>            indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17])) 
<a name="l27372"><span class="ln">27372 </span></a>    &quot;&quot;&quot;</span>
<a name="l27373"><span class="ln">27373 </span></a>
<a name="l27374"><span class="ln">27374 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l27375"><span class="ln">27375 </span></a><span class="s2">def </span><span class="s1">sort</span><span class="s3">(</span>
<a name="l27376"><span class="ln">27376 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27377"><span class="ln">27377 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l27378"><span class="ln">27378 </span></a>    <span class="s1">descending</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27379"><span class="ln">27379 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27380"><span class="ln">27380 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27381"><span class="ln">27381 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">sort</span><span class="s2">:</span>
<a name="l27382"><span class="ln">27382 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27383"><span class="ln">27383 </span></a>    sort(input, dim=-1, descending=False, stable=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l27384"><span class="ln">27384 </span></a> 
<a name="l27385"><span class="ln">27385 </span></a>    Sorts the elements of the :attr:`input` tensor along a given dimension 
<a name="l27386"><span class="ln">27386 </span></a>    in ascending order by value. 
<a name="l27387"><span class="ln">27387 </span></a> 
<a name="l27388"><span class="ln">27388 </span></a>    If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l27389"><span class="ln">27389 </span></a> 
<a name="l27390"><span class="ln">27390 </span></a>    If :attr:`descending` is ``True`` then the elements are sorted in descending 
<a name="l27391"><span class="ln">27391 </span></a>    order by value. 
<a name="l27392"><span class="ln">27392 </span></a> 
<a name="l27393"><span class="ln">27393 </span></a>    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l27394"><span class="ln">27394 </span></a>    the order of equivalent elements. 
<a name="l27395"><span class="ln">27395 </span></a> 
<a name="l27396"><span class="ln">27396 </span></a>    A namedtuple of (values, indices) is returned, where the `values` are the 
<a name="l27397"><span class="ln">27397 </span></a>    sorted values and `indices` are the indices of the elements in the original 
<a name="l27398"><span class="ln">27398 </span></a>    `input` tensor. 
<a name="l27399"><span class="ln">27399 </span></a> 
<a name="l27400"><span class="ln">27400 </span></a>    Args: 
<a name="l27401"><span class="ln">27401 </span></a>        input (Tensor): the input tensor. 
<a name="l27402"><span class="ln">27402 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l27403"><span class="ln">27403 </span></a>        descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l27404"><span class="ln">27404 </span></a>        stable (bool, optional): makes the sorting routine stable, which guarantees that the order 
<a name="l27405"><span class="ln">27405 </span></a>           of equivalent elements is preserved. 
<a name="l27406"><span class="ln">27406 </span></a> 
<a name="l27407"><span class="ln">27407 </span></a>    Keyword args: 
<a name="l27408"><span class="ln">27408 </span></a>        out (tuple, optional): the output tuple of (`Tensor`, `LongTensor`) that can 
<a name="l27409"><span class="ln">27409 </span></a>            be optionally given to be used as output buffers 
<a name="l27410"><span class="ln">27410 </span></a> 
<a name="l27411"><span class="ln">27411 </span></a>    Example:: 
<a name="l27412"><span class="ln">27412 </span></a> 
<a name="l27413"><span class="ln">27413 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l27414"><span class="ln">27414 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x) 
<a name="l27415"><span class="ln">27415 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27416"><span class="ln">27416 </span></a>        tensor([[-0.2162,  0.0608,  0.6719,  2.3332], 
<a name="l27417"><span class="ln">27417 </span></a>                [-0.5793,  0.0061,  0.6058,  0.9497], 
<a name="l27418"><span class="ln">27418 </span></a>                [-0.5071,  0.3343,  0.9553,  1.0960]]) 
<a name="l27419"><span class="ln">27419 </span></a>        &gt;&gt;&gt; indices 
<a name="l27420"><span class="ln">27420 </span></a>        tensor([[ 1,  0,  2,  3], 
<a name="l27421"><span class="ln">27421 </span></a>                [ 3,  1,  0,  2], 
<a name="l27422"><span class="ln">27422 </span></a>                [ 0,  3,  1,  2]]) 
<a name="l27423"><span class="ln">27423 </span></a> 
<a name="l27424"><span class="ln">27424 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) 
<a name="l27425"><span class="ln">27425 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27426"><span class="ln">27426 </span></a>        tensor([[-0.5071, -0.2162,  0.6719, -0.5793], 
<a name="l27427"><span class="ln">27427 </span></a>                [ 0.0608,  0.0061,  0.9497,  0.3343], 
<a name="l27428"><span class="ln">27428 </span></a>                [ 0.6058,  0.9553,  1.0960,  2.3332]]) 
<a name="l27429"><span class="ln">27429 </span></a>        &gt;&gt;&gt; indices 
<a name="l27430"><span class="ln">27430 </span></a>        tensor([[ 2,  0,  0,  1], 
<a name="l27431"><span class="ln">27431 </span></a>                [ 0,  1,  1,  2], 
<a name="l27432"><span class="ln">27432 </span></a>                [ 1,  2,  2,  0]]) 
<a name="l27433"><span class="ln">27433 </span></a>        &gt;&gt;&gt; x = torch.tensor([0, 1] * 9) 
<a name="l27434"><span class="ln">27434 </span></a>        &gt;&gt;&gt; x.sort() 
<a name="l27435"><span class="ln">27435 </span></a>        torch.return_types.sort( 
<a name="l27436"><span class="ln">27436 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27437"><span class="ln">27437 </span></a>            indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1])) 
<a name="l27438"><span class="ln">27438 </span></a>        &gt;&gt;&gt; x.sort(stable=True) 
<a name="l27439"><span class="ln">27439 </span></a>        torch.return_types.sort( 
<a name="l27440"><span class="ln">27440 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27441"><span class="ln">27441 </span></a>            indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17])) 
<a name="l27442"><span class="ln">27442 </span></a>    &quot;&quot;&quot;</span>
<a name="l27443"><span class="ln">27443 </span></a>
<a name="l27444"><span class="ln">27444 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l27445"><span class="ln">27445 </span></a><span class="s2">def </span><span class="s1">sort</span><span class="s3">(</span>
<a name="l27446"><span class="ln">27446 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27447"><span class="ln">27447 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27448"><span class="ln">27448 </span></a>    <span class="s1">stable</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l27449"><span class="ln">27449 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l27450"><span class="ln">27450 </span></a>    <span class="s1">descending</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27451"><span class="ln">27451 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27452"><span class="ln">27452 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">sort</span><span class="s2">:</span>
<a name="l27453"><span class="ln">27453 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27454"><span class="ln">27454 </span></a>    sort(input, dim=-1, descending=False, stable=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l27455"><span class="ln">27455 </span></a> 
<a name="l27456"><span class="ln">27456 </span></a>    Sorts the elements of the :attr:`input` tensor along a given dimension 
<a name="l27457"><span class="ln">27457 </span></a>    in ascending order by value. 
<a name="l27458"><span class="ln">27458 </span></a> 
<a name="l27459"><span class="ln">27459 </span></a>    If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l27460"><span class="ln">27460 </span></a> 
<a name="l27461"><span class="ln">27461 </span></a>    If :attr:`descending` is ``True`` then the elements are sorted in descending 
<a name="l27462"><span class="ln">27462 </span></a>    order by value. 
<a name="l27463"><span class="ln">27463 </span></a> 
<a name="l27464"><span class="ln">27464 </span></a>    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l27465"><span class="ln">27465 </span></a>    the order of equivalent elements. 
<a name="l27466"><span class="ln">27466 </span></a> 
<a name="l27467"><span class="ln">27467 </span></a>    A namedtuple of (values, indices) is returned, where the `values` are the 
<a name="l27468"><span class="ln">27468 </span></a>    sorted values and `indices` are the indices of the elements in the original 
<a name="l27469"><span class="ln">27469 </span></a>    `input` tensor. 
<a name="l27470"><span class="ln">27470 </span></a> 
<a name="l27471"><span class="ln">27471 </span></a>    Args: 
<a name="l27472"><span class="ln">27472 </span></a>        input (Tensor): the input tensor. 
<a name="l27473"><span class="ln">27473 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l27474"><span class="ln">27474 </span></a>        descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l27475"><span class="ln">27475 </span></a>        stable (bool, optional): makes the sorting routine stable, which guarantees that the order 
<a name="l27476"><span class="ln">27476 </span></a>           of equivalent elements is preserved. 
<a name="l27477"><span class="ln">27477 </span></a> 
<a name="l27478"><span class="ln">27478 </span></a>    Keyword args: 
<a name="l27479"><span class="ln">27479 </span></a>        out (tuple, optional): the output tuple of (`Tensor`, `LongTensor`) that can 
<a name="l27480"><span class="ln">27480 </span></a>            be optionally given to be used as output buffers 
<a name="l27481"><span class="ln">27481 </span></a> 
<a name="l27482"><span class="ln">27482 </span></a>    Example:: 
<a name="l27483"><span class="ln">27483 </span></a> 
<a name="l27484"><span class="ln">27484 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l27485"><span class="ln">27485 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x) 
<a name="l27486"><span class="ln">27486 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27487"><span class="ln">27487 </span></a>        tensor([[-0.2162,  0.0608,  0.6719,  2.3332], 
<a name="l27488"><span class="ln">27488 </span></a>                [-0.5793,  0.0061,  0.6058,  0.9497], 
<a name="l27489"><span class="ln">27489 </span></a>                [-0.5071,  0.3343,  0.9553,  1.0960]]) 
<a name="l27490"><span class="ln">27490 </span></a>        &gt;&gt;&gt; indices 
<a name="l27491"><span class="ln">27491 </span></a>        tensor([[ 1,  0,  2,  3], 
<a name="l27492"><span class="ln">27492 </span></a>                [ 3,  1,  0,  2], 
<a name="l27493"><span class="ln">27493 </span></a>                [ 0,  3,  1,  2]]) 
<a name="l27494"><span class="ln">27494 </span></a> 
<a name="l27495"><span class="ln">27495 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) 
<a name="l27496"><span class="ln">27496 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27497"><span class="ln">27497 </span></a>        tensor([[-0.5071, -0.2162,  0.6719, -0.5793], 
<a name="l27498"><span class="ln">27498 </span></a>                [ 0.0608,  0.0061,  0.9497,  0.3343], 
<a name="l27499"><span class="ln">27499 </span></a>                [ 0.6058,  0.9553,  1.0960,  2.3332]]) 
<a name="l27500"><span class="ln">27500 </span></a>        &gt;&gt;&gt; indices 
<a name="l27501"><span class="ln">27501 </span></a>        tensor([[ 2,  0,  0,  1], 
<a name="l27502"><span class="ln">27502 </span></a>                [ 0,  1,  1,  2], 
<a name="l27503"><span class="ln">27503 </span></a>                [ 1,  2,  2,  0]]) 
<a name="l27504"><span class="ln">27504 </span></a>        &gt;&gt;&gt; x = torch.tensor([0, 1] * 9) 
<a name="l27505"><span class="ln">27505 </span></a>        &gt;&gt;&gt; x.sort() 
<a name="l27506"><span class="ln">27506 </span></a>        torch.return_types.sort( 
<a name="l27507"><span class="ln">27507 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27508"><span class="ln">27508 </span></a>            indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1])) 
<a name="l27509"><span class="ln">27509 </span></a>        &gt;&gt;&gt; x.sort(stable=True) 
<a name="l27510"><span class="ln">27510 </span></a>        torch.return_types.sort( 
<a name="l27511"><span class="ln">27511 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27512"><span class="ln">27512 </span></a>            indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17])) 
<a name="l27513"><span class="ln">27513 </span></a>    &quot;&quot;&quot;</span>
<a name="l27514"><span class="ln">27514 </span></a>
<a name="l27515"><span class="ln">27515 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l27516"><span class="ln">27516 </span></a><span class="s2">def </span><span class="s1">sort</span><span class="s3">(</span>
<a name="l27517"><span class="ln">27517 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27518"><span class="ln">27518 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l27519"><span class="ln">27519 </span></a>    <span class="s1">descending</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27520"><span class="ln">27520 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27521"><span class="ln">27521 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27522"><span class="ln">27522 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">sort</span><span class="s2">:</span>
<a name="l27523"><span class="ln">27523 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27524"><span class="ln">27524 </span></a>    sort(input, dim=-1, descending=False, stable=False, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l27525"><span class="ln">27525 </span></a> 
<a name="l27526"><span class="ln">27526 </span></a>    Sorts the elements of the :attr:`input` tensor along a given dimension 
<a name="l27527"><span class="ln">27527 </span></a>    in ascending order by value. 
<a name="l27528"><span class="ln">27528 </span></a> 
<a name="l27529"><span class="ln">27529 </span></a>    If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l27530"><span class="ln">27530 </span></a> 
<a name="l27531"><span class="ln">27531 </span></a>    If :attr:`descending` is ``True`` then the elements are sorted in descending 
<a name="l27532"><span class="ln">27532 </span></a>    order by value. 
<a name="l27533"><span class="ln">27533 </span></a> 
<a name="l27534"><span class="ln">27534 </span></a>    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving 
<a name="l27535"><span class="ln">27535 </span></a>    the order of equivalent elements. 
<a name="l27536"><span class="ln">27536 </span></a> 
<a name="l27537"><span class="ln">27537 </span></a>    A namedtuple of (values, indices) is returned, where the `values` are the 
<a name="l27538"><span class="ln">27538 </span></a>    sorted values and `indices` are the indices of the elements in the original 
<a name="l27539"><span class="ln">27539 </span></a>    `input` tensor. 
<a name="l27540"><span class="ln">27540 </span></a> 
<a name="l27541"><span class="ln">27541 </span></a>    Args: 
<a name="l27542"><span class="ln">27542 </span></a>        input (Tensor): the input tensor. 
<a name="l27543"><span class="ln">27543 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l27544"><span class="ln">27544 </span></a>        descending (bool, optional): controls the sorting order (ascending or descending) 
<a name="l27545"><span class="ln">27545 </span></a>        stable (bool, optional): makes the sorting routine stable, which guarantees that the order 
<a name="l27546"><span class="ln">27546 </span></a>           of equivalent elements is preserved. 
<a name="l27547"><span class="ln">27547 </span></a> 
<a name="l27548"><span class="ln">27548 </span></a>    Keyword args: 
<a name="l27549"><span class="ln">27549 </span></a>        out (tuple, optional): the output tuple of (`Tensor`, `LongTensor`) that can 
<a name="l27550"><span class="ln">27550 </span></a>            be optionally given to be used as output buffers 
<a name="l27551"><span class="ln">27551 </span></a> 
<a name="l27552"><span class="ln">27552 </span></a>    Example:: 
<a name="l27553"><span class="ln">27553 </span></a> 
<a name="l27554"><span class="ln">27554 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 4) 
<a name="l27555"><span class="ln">27555 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x) 
<a name="l27556"><span class="ln">27556 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27557"><span class="ln">27557 </span></a>        tensor([[-0.2162,  0.0608,  0.6719,  2.3332], 
<a name="l27558"><span class="ln">27558 </span></a>                [-0.5793,  0.0061,  0.6058,  0.9497], 
<a name="l27559"><span class="ln">27559 </span></a>                [-0.5071,  0.3343,  0.9553,  1.0960]]) 
<a name="l27560"><span class="ln">27560 </span></a>        &gt;&gt;&gt; indices 
<a name="l27561"><span class="ln">27561 </span></a>        tensor([[ 1,  0,  2,  3], 
<a name="l27562"><span class="ln">27562 </span></a>                [ 3,  1,  0,  2], 
<a name="l27563"><span class="ln">27563 </span></a>                [ 0,  3,  1,  2]]) 
<a name="l27564"><span class="ln">27564 </span></a> 
<a name="l27565"><span class="ln">27565 </span></a>        &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) 
<a name="l27566"><span class="ln">27566 </span></a>        &gt;&gt;&gt; sorted 
<a name="l27567"><span class="ln">27567 </span></a>        tensor([[-0.5071, -0.2162,  0.6719, -0.5793], 
<a name="l27568"><span class="ln">27568 </span></a>                [ 0.0608,  0.0061,  0.9497,  0.3343], 
<a name="l27569"><span class="ln">27569 </span></a>                [ 0.6058,  0.9553,  1.0960,  2.3332]]) 
<a name="l27570"><span class="ln">27570 </span></a>        &gt;&gt;&gt; indices 
<a name="l27571"><span class="ln">27571 </span></a>        tensor([[ 2,  0,  0,  1], 
<a name="l27572"><span class="ln">27572 </span></a>                [ 0,  1,  1,  2], 
<a name="l27573"><span class="ln">27573 </span></a>                [ 1,  2,  2,  0]]) 
<a name="l27574"><span class="ln">27574 </span></a>        &gt;&gt;&gt; x = torch.tensor([0, 1] * 9) 
<a name="l27575"><span class="ln">27575 </span></a>        &gt;&gt;&gt; x.sort() 
<a name="l27576"><span class="ln">27576 </span></a>        torch.return_types.sort( 
<a name="l27577"><span class="ln">27577 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27578"><span class="ln">27578 </span></a>            indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1])) 
<a name="l27579"><span class="ln">27579 </span></a>        &gt;&gt;&gt; x.sort(stable=True) 
<a name="l27580"><span class="ln">27580 </span></a>        torch.return_types.sort( 
<a name="l27581"><span class="ln">27581 </span></a>            values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 
<a name="l27582"><span class="ln">27582 </span></a>            indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17])) 
<a name="l27583"><span class="ln">27583 </span></a>    &quot;&quot;&quot;</span>
<a name="l27584"><span class="ln">27584 </span></a>
<a name="l27585"><span class="ln">27585 </span></a><span class="s2">def </span><span class="s1">sparse_bsc_tensor</span><span class="s3">(</span>
<a name="l27586"><span class="ln">27586 </span></a>    <span class="s1">ccol_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27587"><span class="ln">27587 </span></a>    <span class="s1">row_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27588"><span class="ln">27588 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27589"><span class="ln">27589 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27590"><span class="ln">27590 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27591"><span class="ln">27591 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27592"><span class="ln">27592 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27593"><span class="ln">27593 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27594"><span class="ln">27594 </span></a>    <span class="s1">check_invariants</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27595"><span class="ln">27595 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27596"><span class="ln">27596 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27597"><span class="ln">27597 </span></a>    sparse_bsc_tensor(ccol_indices, row_indices, values, size=None, *, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l27598"><span class="ln">27598 </span></a> 
<a name="l27599"><span class="ln">27599 </span></a>    Constructs a :ref:`sparse tensor in BSC (Block Compressed Sparse 
<a name="l27600"><span class="ln">27600 </span></a>    Column)) &lt;sparse-bsc-docs&gt;` with specified 2-dimensional blocks at the 
<a name="l27601"><span class="ln">27601 </span></a>    given :attr:`ccol_indices` and :attr:`row_indices`. Sparse matrix 
<a name="l27602"><span class="ln">27602 </span></a>    multiplication operations in BSC format are typically faster than that 
<a name="l27603"><span class="ln">27603 </span></a>    for sparse tensors in COO format. Make you have a look at :ref:`the 
<a name="l27604"><span class="ln">27604 </span></a>    note on the data type of the indices &lt;sparse-bsc-docs&gt;`. 
<a name="l27605"><span class="ln">27605 </span></a> 
<a name="l27606"><span class="ln">27606 </span></a>    .. note:: 
<a name="l27607"><span class="ln">27607 </span></a> 
<a name="l27608"><span class="ln">27608 </span></a>       If the ``device`` argument is not specified the device of the given 
<a name="l27609"><span class="ln">27609 </span></a>       :attr:`values` and indices tensor(s) must match. If, however, the 
<a name="l27610"><span class="ln">27610 </span></a>       argument is specified the input Tensors will be converted to the 
<a name="l27611"><span class="ln">27611 </span></a>       given device and in turn determine the device of the constructed 
<a name="l27612"><span class="ln">27612 </span></a>       sparse tensor. 
<a name="l27613"><span class="ln">27613 </span></a> 
<a name="l27614"><span class="ln">27614 </span></a>    Args: 
<a name="l27615"><span class="ln">27615 </span></a>        ccol_indices (array_like): (B+1)-dimensional array of size 
<a name="l27616"><span class="ln">27616 </span></a>            ``(*batchsize, ncolblocks + 1)``. The last element of each 
<a name="l27617"><span class="ln">27617 </span></a>            batch is the number of non-zeros. This tensor encodes the 
<a name="l27618"><span class="ln">27618 </span></a>            index in values and row_indices depending on where the given 
<a name="l27619"><span class="ln">27619 </span></a>            column starts. Each successive number in the tensor subtracted 
<a name="l27620"><span class="ln">27620 </span></a>            by the number before it denotes the number of elements in a 
<a name="l27621"><span class="ln">27621 </span></a>            given column. 
<a name="l27622"><span class="ln">27622 </span></a>        row_indices (array_like): Row block co-ordinates of each block in 
<a name="l27623"><span class="ln">27623 </span></a>            values. (B+1)-dimensional tensor with the same length 
<a name="l27624"><span class="ln">27624 </span></a>            as values. 
<a name="l27625"><span class="ln">27625 </span></a>        values (array_list): Initial blocks for the tensor. Can be a list, 
<a name="l27626"><span class="ln">27626 </span></a>            tuple, NumPy ``ndarray``, and other types that 
<a name="l27627"><span class="ln">27627 </span></a>            represents a (1 + 2 + K)-dimensional tensor where ``K`` is the 
<a name="l27628"><span class="ln">27628 </span></a>            number of dense dimensions. 
<a name="l27629"><span class="ln">27629 </span></a>        size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l27630"><span class="ln">27630 </span></a>            sparse tensor: ``(*batchsize, nrows * blocksize[0], ncols * 
<a name="l27631"><span class="ln">27631 </span></a>            blocksize[1], *densesize)`` If not provided, the size will be 
<a name="l27632"><span class="ln">27632 </span></a>            inferred as the minimum size big enough to hold all non-zero 
<a name="l27633"><span class="ln">27633 </span></a>            blocks. 
<a name="l27634"><span class="ln">27634 </span></a> 
<a name="l27635"><span class="ln">27635 </span></a>    Keyword args: 
<a name="l27636"><span class="ln">27636 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l27637"><span class="ln">27637 </span></a>            returned tensor.  Default: if None, infers data type from 
<a name="l27638"><span class="ln">27638 </span></a>            :attr:`values`. 
<a name="l27639"><span class="ln">27639 </span></a>        device (:class:`torch.device`, optional): the desired device of 
<a name="l27640"><span class="ln">27640 </span></a>            returned tensor.  Default: if None, uses the current device 
<a name="l27641"><span class="ln">27641 </span></a>            for the default tensor type (see 
<a name="l27642"><span class="ln">27642 </span></a>            :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l27643"><span class="ln">27643 </span></a>            the CPU for CPU tensor types and the current CUDA device for 
<a name="l27644"><span class="ln">27644 </span></a>            CUDA tensor types. 
<a name="l27645"><span class="ln">27645 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l27646"><span class="ln">27646 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l27647"><span class="ln">27647 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l27648"><span class="ln">27648 </span></a>            returned tensor. Default: ``False``. 
<a name="l27649"><span class="ln">27649 </span></a>        check_invariants (bool, optional): If sparse tensor invariants are checked. 
<a name="l27650"><span class="ln">27650 </span></a>            Default: as returned by :func:`torch.sparse.check_sparse_tensor_invariants.is_enabled`, 
<a name="l27651"><span class="ln">27651 </span></a>            initially False. 
<a name="l27652"><span class="ln">27652 </span></a> 
<a name="l27653"><span class="ln">27653 </span></a>    Example:: 
<a name="l27654"><span class="ln">27654 </span></a> 
<a name="l27655"><span class="ln">27655 </span></a>        &gt;&gt;&gt; ccol_indices = [0, 1, 2] 
<a name="l27656"><span class="ln">27656 </span></a>        &gt;&gt;&gt; row_indices = [0, 1] 
<a name="l27657"><span class="ln">27657 </span></a>        &gt;&gt;&gt; values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] 
<a name="l27658"><span class="ln">27658 </span></a>        &gt;&gt;&gt; torch.sparse_bsc_tensor(torch.tensor(ccol_indices, dtype=torch.int64), 
<a name="l27659"><span class="ln">27659 </span></a>        ...                         torch.tensor(row_indices, dtype=torch.int64), 
<a name="l27660"><span class="ln">27660 </span></a>        ...                         torch.tensor(values), dtype=torch.double) 
<a name="l27661"><span class="ln">27661 </span></a>        tensor(ccol_indices=tensor([0, 1, 2]), 
<a name="l27662"><span class="ln">27662 </span></a>               row_indices=tensor([0, 1]), 
<a name="l27663"><span class="ln">27663 </span></a>               values=tensor([[[1., 2.], 
<a name="l27664"><span class="ln">27664 </span></a>                               [3., 4.]], 
<a name="l27665"><span class="ln">27665 </span></a>                              [[5., 6.], 
<a name="l27666"><span class="ln">27666 </span></a>                               [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64, 
<a name="l27667"><span class="ln">27667 </span></a>               layout=torch.sparse_bsc) 
<a name="l27668"><span class="ln">27668 </span></a>    &quot;&quot;&quot;</span>
<a name="l27669"><span class="ln">27669 </span></a>
<a name="l27670"><span class="ln">27670 </span></a><span class="s2">def </span><span class="s1">sparse_bsr_tensor</span><span class="s3">(</span>
<a name="l27671"><span class="ln">27671 </span></a>    <span class="s1">crow_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27672"><span class="ln">27672 </span></a>    <span class="s1">col_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27673"><span class="ln">27673 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27674"><span class="ln">27674 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27675"><span class="ln">27675 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27676"><span class="ln">27676 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27677"><span class="ln">27677 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27678"><span class="ln">27678 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27679"><span class="ln">27679 </span></a>    <span class="s1">check_invariants</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27680"><span class="ln">27680 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27681"><span class="ln">27681 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27682"><span class="ln">27682 </span></a>    sparse_bsr_tensor(crow_indices, col_indices, values, size=None, *, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l27683"><span class="ln">27683 </span></a> 
<a name="l27684"><span class="ln">27684 </span></a>    Constructs a :ref:`sparse tensor in BSR (Block Compressed Sparse Row)) 
<a name="l27685"><span class="ln">27685 </span></a>    &lt;sparse-bsr-docs&gt;` with specified 2-dimensional blocks at the given 
<a name="l27686"><span class="ln">27686 </span></a>    :attr:`crow_indices` and :attr:`col_indices`. Sparse matrix 
<a name="l27687"><span class="ln">27687 </span></a>    multiplication operations in BSR format are typically faster than that 
<a name="l27688"><span class="ln">27688 </span></a>    for sparse tensors in COO format. Make you have a look at :ref:`the 
<a name="l27689"><span class="ln">27689 </span></a>    note on the data type of the indices &lt;sparse-bsr-docs&gt;`. 
<a name="l27690"><span class="ln">27690 </span></a> 
<a name="l27691"><span class="ln">27691 </span></a>    .. note:: 
<a name="l27692"><span class="ln">27692 </span></a> 
<a name="l27693"><span class="ln">27693 </span></a>       If the ``device`` argument is not specified the device of the given 
<a name="l27694"><span class="ln">27694 </span></a>       :attr:`values` and indices tensor(s) must match. If, however, the 
<a name="l27695"><span class="ln">27695 </span></a>       argument is specified the input Tensors will be converted to the 
<a name="l27696"><span class="ln">27696 </span></a>       given device and in turn determine the device of the constructed 
<a name="l27697"><span class="ln">27697 </span></a>       sparse tensor. 
<a name="l27698"><span class="ln">27698 </span></a> 
<a name="l27699"><span class="ln">27699 </span></a>    Args: 
<a name="l27700"><span class="ln">27700 </span></a>        crow_indices (array_like): (B+1)-dimensional array of size 
<a name="l27701"><span class="ln">27701 </span></a>            ``(*batchsize, nrowblocks + 1)``.  The last element of each 
<a name="l27702"><span class="ln">27702 </span></a>            batch is the number of non-zeros. This tensor encodes the 
<a name="l27703"><span class="ln">27703 </span></a>            block index in values and col_indices depending on where the 
<a name="l27704"><span class="ln">27704 </span></a>            given row block starts. Each successive number in the tensor 
<a name="l27705"><span class="ln">27705 </span></a>            subtracted by the number before it denotes the number of 
<a name="l27706"><span class="ln">27706 </span></a>            blocks in a given row. 
<a name="l27707"><span class="ln">27707 </span></a>        col_indices (array_like): Column block co-ordinates of each block 
<a name="l27708"><span class="ln">27708 </span></a>            in values. (B+1)-dimensional tensor with the same length as 
<a name="l27709"><span class="ln">27709 </span></a>            values. 
<a name="l27710"><span class="ln">27710 </span></a>        values (array_list): Initial values for the tensor. Can be a list, 
<a name="l27711"><span class="ln">27711 </span></a>            tuple, NumPy ``ndarray``, scalar, and other types that 
<a name="l27712"><span class="ln">27712 </span></a>            represents a (1 + 2 + K)-dimensional tensor where ``K`` is the 
<a name="l27713"><span class="ln">27713 </span></a>            number of dense dimensions. 
<a name="l27714"><span class="ln">27714 </span></a>        size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l27715"><span class="ln">27715 </span></a>            sparse tensor: ``(*batchsize, nrows * blocksize[0], ncols * 
<a name="l27716"><span class="ln">27716 </span></a>            blocksize[1], *densesize)`` where ``blocksize == 
<a name="l27717"><span class="ln">27717 </span></a>            values.shape[1:3]``. If not provided, the size will be 
<a name="l27718"><span class="ln">27718 </span></a>            inferred as the minimum size big enough to hold all non-zero 
<a name="l27719"><span class="ln">27719 </span></a>            blocks. 
<a name="l27720"><span class="ln">27720 </span></a> 
<a name="l27721"><span class="ln">27721 </span></a>    Keyword args: 
<a name="l27722"><span class="ln">27722 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l27723"><span class="ln">27723 </span></a>            returned tensor.  Default: if None, infers data type from 
<a name="l27724"><span class="ln">27724 </span></a>            :attr:`values`. 
<a name="l27725"><span class="ln">27725 </span></a>        device (:class:`torch.device`, optional): the desired device of 
<a name="l27726"><span class="ln">27726 </span></a>            returned tensor.  Default: if None, uses the current device 
<a name="l27727"><span class="ln">27727 </span></a>            for the default tensor type (see 
<a name="l27728"><span class="ln">27728 </span></a>            :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l27729"><span class="ln">27729 </span></a>            the CPU for CPU tensor types and the current CUDA device for 
<a name="l27730"><span class="ln">27730 </span></a>            CUDA tensor types. 
<a name="l27731"><span class="ln">27731 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l27732"><span class="ln">27732 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l27733"><span class="ln">27733 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l27734"><span class="ln">27734 </span></a>            returned tensor. Default: ``False``. 
<a name="l27735"><span class="ln">27735 </span></a>        check_invariants (bool, optional): If sparse tensor invariants are checked. 
<a name="l27736"><span class="ln">27736 </span></a>            Default: as returned by :func:`torch.sparse.check_sparse_tensor_invariants.is_enabled`, 
<a name="l27737"><span class="ln">27737 </span></a>            initially False. 
<a name="l27738"><span class="ln">27738 </span></a> 
<a name="l27739"><span class="ln">27739 </span></a>    Example:: 
<a name="l27740"><span class="ln">27740 </span></a> 
<a name="l27741"><span class="ln">27741 </span></a>        &gt;&gt;&gt; crow_indices = [0, 1, 2] 
<a name="l27742"><span class="ln">27742 </span></a>        &gt;&gt;&gt; col_indices = [0, 1] 
<a name="l27743"><span class="ln">27743 </span></a>        &gt;&gt;&gt; values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] 
<a name="l27744"><span class="ln">27744 </span></a>        &gt;&gt;&gt; torch.sparse_bsr_tensor(torch.tensor(crow_indices, dtype=torch.int64), 
<a name="l27745"><span class="ln">27745 </span></a>        ...                         torch.tensor(col_indices, dtype=torch.int64), 
<a name="l27746"><span class="ln">27746 </span></a>        ...                         torch.tensor(values), dtype=torch.double) 
<a name="l27747"><span class="ln">27747 </span></a>        tensor(crow_indices=tensor([0, 1, 2]), 
<a name="l27748"><span class="ln">27748 </span></a>               col_indices=tensor([0, 1]), 
<a name="l27749"><span class="ln">27749 </span></a>               values=tensor([[[1., 2.], 
<a name="l27750"><span class="ln">27750 </span></a>                               [3., 4.]], 
<a name="l27751"><span class="ln">27751 </span></a>                              [[5., 6.], 
<a name="l27752"><span class="ln">27752 </span></a>                               [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64, 
<a name="l27753"><span class="ln">27753 </span></a>               layout=torch.sparse_bsr) 
<a name="l27754"><span class="ln">27754 </span></a>    &quot;&quot;&quot;</span>
<a name="l27755"><span class="ln">27755 </span></a>
<a name="l27756"><span class="ln">27756 </span></a><span class="s2">def </span><span class="s1">sparse_compressed_tensor</span><span class="s3">(</span>
<a name="l27757"><span class="ln">27757 </span></a>    <span class="s1">compressed_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27758"><span class="ln">27758 </span></a>    <span class="s1">plain_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27759"><span class="ln">27759 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27760"><span class="ln">27760 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27761"><span class="ln">27761 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27762"><span class="ln">27762 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27763"><span class="ln">27763 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27764"><span class="ln">27764 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27765"><span class="ln">27765 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27766"><span class="ln">27766 </span></a>    <span class="s1">check_invariants</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27767"><span class="ln">27767 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27768"><span class="ln">27768 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27769"><span class="ln">27769 </span></a>    sparse_compressed_tensor(compressed_indices, plain_indices, values, size=None, *, dtype=None, layout=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l27770"><span class="ln">27770 </span></a> 
<a name="l27771"><span class="ln">27771 </span></a>    Constructs a :ref:`sparse tensor in Compressed Sparse format - CSR, 
<a name="l27772"><span class="ln">27772 </span></a>    CSC, BSR, or BSC - &lt;sparse-compressed-docs&gt;` with specified values at 
<a name="l27773"><span class="ln">27773 </span></a>    the given :attr:`compressed_indices` and :attr:`plain_indices`. Sparse 
<a name="l27774"><span class="ln">27774 </span></a>    matrix multiplication operations in Compressed Sparse format are 
<a name="l27775"><span class="ln">27775 </span></a>    typically faster than that for sparse tensors in COO format. Make you 
<a name="l27776"><span class="ln">27776 </span></a>    have a look at :ref:`the note on the data type of the indices 
<a name="l27777"><span class="ln">27777 </span></a>    &lt;sparse-compressed-docs&gt;`. 
<a name="l27778"><span class="ln">27778 </span></a> 
<a name="l27779"><span class="ln">27779 </span></a>    .. note:: 
<a name="l27780"><span class="ln">27780 </span></a> 
<a name="l27781"><span class="ln">27781 </span></a>       If the ``device`` argument is not specified the device of the given 
<a name="l27782"><span class="ln">27782 </span></a>       :attr:`values` and indices tensor(s) must match. If, however, the 
<a name="l27783"><span class="ln">27783 </span></a>       argument is specified the input Tensors will be converted to the 
<a name="l27784"><span class="ln">27784 </span></a>       given device and in turn determine the device of the constructed 
<a name="l27785"><span class="ln">27785 </span></a>       sparse tensor. 
<a name="l27786"><span class="ln">27786 </span></a> 
<a name="l27787"><span class="ln">27787 </span></a>    Args: 
<a name="l27788"><span class="ln">27788 </span></a>        compressed_indices (array_like): (B+1)-dimensional array of size 
<a name="l27789"><span class="ln">27789 </span></a>            ``(*batchsize, compressed_dim_size + 1)``.  The last element of 
<a name="l27790"><span class="ln">27790 </span></a>            each batch is the number of non-zero elements or blocks. This 
<a name="l27791"><span class="ln">27791 </span></a>            tensor encodes the index in ``values`` and ``plain_indices`` 
<a name="l27792"><span class="ln">27792 </span></a>            depending on where the given compressed dimension (row or 
<a name="l27793"><span class="ln">27793 </span></a>            column) starts. Each successive number in the tensor 
<a name="l27794"><span class="ln">27794 </span></a>            subtracted by the number before it denotes the number of 
<a name="l27795"><span class="ln">27795 </span></a>            elements or blocks in a given compressed dimension. 
<a name="l27796"><span class="ln">27796 </span></a>        plain_indices (array_like): Plain dimension (column or row) 
<a name="l27797"><span class="ln">27797 </span></a>            co-ordinates of each element or block in values. (B+1)-dimensional 
<a name="l27798"><span class="ln">27798 </span></a>            tensor with the same length as values. 
<a name="l27799"><span class="ln">27799 </span></a> 
<a name="l27800"><span class="ln">27800 </span></a>        values (array_list): Initial values for the tensor. Can be a list, 
<a name="l27801"><span class="ln">27801 </span></a>            tuple, NumPy ``ndarray``, scalar, and other types.  that 
<a name="l27802"><span class="ln">27802 </span></a>            represents a (1+K)-dimensional (for CSR and CSC layouts) or 
<a name="l27803"><span class="ln">27803 </span></a>            (1+2+K)-dimensional tensor (for BSR and BSC layouts) where 
<a name="l27804"><span class="ln">27804 </span></a>            ``K`` is the number of dense dimensions. 
<a name="l27805"><span class="ln">27805 </span></a>        size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l27806"><span class="ln">27806 </span></a>            sparse tensor: ``(*batchsize, nrows * blocksize[0], ncols * 
<a name="l27807"><span class="ln">27807 </span></a>            blocksize[1], *densesize)`` where ``blocksize[0] == 
<a name="l27808"><span class="ln">27808 </span></a>            blocksize[1] == 1`` for CSR and CSC formats. If not provided, 
<a name="l27809"><span class="ln">27809 </span></a>            the size will be inferred as the minimum size big enough to 
<a name="l27810"><span class="ln">27810 </span></a>            hold all non-zero elements or blocks. 
<a name="l27811"><span class="ln">27811 </span></a> 
<a name="l27812"><span class="ln">27812 </span></a>    Keyword args: 
<a name="l27813"><span class="ln">27813 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l27814"><span class="ln">27814 </span></a>            returned tensor.  Default: if None, infers data type from 
<a name="l27815"><span class="ln">27815 </span></a>            :attr:`values`. 
<a name="l27816"><span class="ln">27816 </span></a>        layout (:class:`torch.layout`, required): the desired layout of 
<a name="l27817"><span class="ln">27817 </span></a>            returned tensor: :attr:`torch.sparse_csr`, 
<a name="l27818"><span class="ln">27818 </span></a>            :attr:`torch.sparse_csc`, :attr:`torch.sparse_bsr`, or 
<a name="l27819"><span class="ln">27819 </span></a>            :attr:`torch.sparse_bsc`. 
<a name="l27820"><span class="ln">27820 </span></a>        device (:class:`torch.device`, optional): the desired device of 
<a name="l27821"><span class="ln">27821 </span></a>            returned tensor.  Default: if None, uses the current device 
<a name="l27822"><span class="ln">27822 </span></a>            for the default tensor type (see 
<a name="l27823"><span class="ln">27823 </span></a>            :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l27824"><span class="ln">27824 </span></a>            the CPU for CPU tensor types and the current CUDA device for 
<a name="l27825"><span class="ln">27825 </span></a>            CUDA tensor types. 
<a name="l27826"><span class="ln">27826 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l27827"><span class="ln">27827 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l27828"><span class="ln">27828 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l27829"><span class="ln">27829 </span></a>            returned tensor. Default: ``False``. 
<a name="l27830"><span class="ln">27830 </span></a>        check_invariants (bool, optional): If sparse tensor invariants are checked. 
<a name="l27831"><span class="ln">27831 </span></a>            Default: as returned by :func:`torch.sparse.check_sparse_tensor_invariants.is_enabled`, 
<a name="l27832"><span class="ln">27832 </span></a>            initially False. 
<a name="l27833"><span class="ln">27833 </span></a> 
<a name="l27834"><span class="ln">27834 </span></a>    Example:: 
<a name="l27835"><span class="ln">27835 </span></a> 
<a name="l27836"><span class="ln">27836 </span></a>        &gt;&gt;&gt; compressed_indices = [0, 2, 4] 
<a name="l27837"><span class="ln">27837 </span></a>        &gt;&gt;&gt; plain_indices = [0, 1, 0, 1] 
<a name="l27838"><span class="ln">27838 </span></a>        &gt;&gt;&gt; values = [1, 2, 3, 4] 
<a name="l27839"><span class="ln">27839 </span></a>        &gt;&gt;&gt; torch.sparse_compressed_tensor(torch.tensor(compressed_indices, dtype=torch.int64), 
<a name="l27840"><span class="ln">27840 </span></a>        ...                                torch.tensor(plain_indices, dtype=torch.int64), 
<a name="l27841"><span class="ln">27841 </span></a>        ...                                torch.tensor(values), dtype=torch.double, layout=torch.sparse_csr) 
<a name="l27842"><span class="ln">27842 </span></a>        tensor(crow_indices=tensor([0, 2, 4]), 
<a name="l27843"><span class="ln">27843 </span></a>               col_indices=tensor([0, 1, 0, 1]), 
<a name="l27844"><span class="ln">27844 </span></a>               values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, 
<a name="l27845"><span class="ln">27845 </span></a>               dtype=torch.float64, layout=torch.sparse_csr) 
<a name="l27846"><span class="ln">27846 </span></a>    &quot;&quot;&quot;</span>
<a name="l27847"><span class="ln">27847 </span></a>
<a name="l27848"><span class="ln">27848 </span></a><span class="s2">def </span><span class="s1">sparse_coo_tensor</span><span class="s3">(</span>
<a name="l27849"><span class="ln">27849 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l27850"><span class="ln">27850 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27851"><span class="ln">27851 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27852"><span class="ln">27852 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27853"><span class="ln">27853 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27854"><span class="ln">27854 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27855"><span class="ln">27855 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27856"><span class="ln">27856 </span></a>    <span class="s1">check_invariants</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27857"><span class="ln">27857 </span></a>    <span class="s1">is_coalesced</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27858"><span class="ln">27858 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27859"><span class="ln">27859 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27860"><span class="ln">27860 </span></a>    sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None, is_coalesced=None) -&gt; Tensor 
<a name="l27861"><span class="ln">27861 </span></a> 
<a name="l27862"><span class="ln">27862 </span></a>    Constructs a :ref:`sparse tensor in COO(rdinate) format 
<a name="l27863"><span class="ln">27863 </span></a>    &lt;sparse-coo-docs&gt;` with specified values at the given 
<a name="l27864"><span class="ln">27864 </span></a>    :attr:`indices`. 
<a name="l27865"><span class="ln">27865 </span></a> 
<a name="l27866"><span class="ln">27866 </span></a>    .. note:: 
<a name="l27867"><span class="ln">27867 </span></a> 
<a name="l27868"><span class="ln">27868 </span></a>       This function returns an :ref:`uncoalesced tensor 
<a name="l27869"><span class="ln">27869 </span></a>       &lt;sparse-uncoalesced-coo-docs&gt;` when :attr:`is_coalesced` is 
<a name="l27870"><span class="ln">27870 </span></a>       unspecified or ``None``. 
<a name="l27871"><span class="ln">27871 </span></a> 
<a name="l27872"><span class="ln">27872 </span></a>    .. note:: 
<a name="l27873"><span class="ln">27873 </span></a> 
<a name="l27874"><span class="ln">27874 </span></a>       If the ``device`` argument is not specified the device of the given 
<a name="l27875"><span class="ln">27875 </span></a>       :attr:`values` and indices tensor(s) must match. If, however, the 
<a name="l27876"><span class="ln">27876 </span></a>       argument is specified the input Tensors will be converted to the 
<a name="l27877"><span class="ln">27877 </span></a>       given device and in turn determine the device of the constructed 
<a name="l27878"><span class="ln">27878 </span></a>       sparse tensor. 
<a name="l27879"><span class="ln">27879 </span></a> 
<a name="l27880"><span class="ln">27880 </span></a>    Args: 
<a name="l27881"><span class="ln">27881 </span></a>        indices (array_like): Initial data for the tensor. Can be a list, tuple, 
<a name="l27882"><span class="ln">27882 </span></a>            NumPy ``ndarray``, scalar, and other types. Will be cast to a :class:`torch.LongTensor` 
<a name="l27883"><span class="ln">27883 </span></a>            internally. The indices are the coordinates of the non-zero values in the matrix, and thus 
<a name="l27884"><span class="ln">27884 </span></a>            should be two-dimensional where the first dimension is the number of tensor dimensions and 
<a name="l27885"><span class="ln">27885 </span></a>            the second dimension is the number of non-zero values. 
<a name="l27886"><span class="ln">27886 </span></a>        values (array_like): Initial values for the tensor. Can be a list, tuple, 
<a name="l27887"><span class="ln">27887 </span></a>            NumPy ``ndarray``, scalar, and other types. 
<a name="l27888"><span class="ln">27888 </span></a>        size (list, tuple, or :class:`torch.Size`, optional): Size of the sparse tensor. If not 
<a name="l27889"><span class="ln">27889 </span></a>            provided the size will be inferred as the minimum size big enough to hold all non-zero 
<a name="l27890"><span class="ln">27890 </span></a>            elements. 
<a name="l27891"><span class="ln">27891 </span></a> 
<a name="l27892"><span class="ln">27892 </span></a>    Keyword args: 
<a name="l27893"><span class="ln">27893 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l27894"><span class="ln">27894 </span></a>            Default: if None, infers data type from :attr:`values`. 
<a name="l27895"><span class="ln">27895 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l27896"><span class="ln">27896 </span></a>            Default: if None, uses the current device for the default tensor type 
<a name="l27897"><span class="ln">27897 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l27898"><span class="ln">27898 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l27899"><span class="ln">27899 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l27900"><span class="ln">27900 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l27901"><span class="ln">27901 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l27902"><span class="ln">27902 </span></a>            returned tensor. Default: ``False``. 
<a name="l27903"><span class="ln">27903 </span></a>        check_invariants (bool, optional): If sparse tensor invariants are checked. 
<a name="l27904"><span class="ln">27904 </span></a>            Default: as returned by :func:`torch.sparse.check_sparse_tensor_invariants.is_enabled`, 
<a name="l27905"><span class="ln">27905 </span></a>            initially False. 
<a name="l27906"><span class="ln">27906 </span></a>        is_coalesced (bool, optional): When``True``, the caller is 
<a name="l27907"><span class="ln">27907 </span></a>            responsible for providing tensor indices that correspond to a 
<a name="l27908"><span class="ln">27908 </span></a>            coalesced tensor.  If the :attr:`check_invariants` flag is 
<a name="l27909"><span class="ln">27909 </span></a>            False, no error will be raised if the prerequisites are not 
<a name="l27910"><span class="ln">27910 </span></a>            met and this will lead to silently incorrect results. To force 
<a name="l27911"><span class="ln">27911 </span></a>            coalescion please use :meth:`coalesce` on the resulting 
<a name="l27912"><span class="ln">27912 </span></a>            Tensor. 
<a name="l27913"><span class="ln">27913 </span></a>            Default: None: except for trivial cases (e.g. nnz &lt; 2) the 
<a name="l27914"><span class="ln">27914 </span></a>            resulting Tensor has is_coalesced set to ``False```. 
<a name="l27915"><span class="ln">27915 </span></a> 
<a name="l27916"><span class="ln">27916 </span></a>    Example:: 
<a name="l27917"><span class="ln">27917 </span></a> 
<a name="l27918"><span class="ln">27918 </span></a>        &gt;&gt;&gt; i = torch.tensor([[0, 1, 1], 
<a name="l27919"><span class="ln">27919 </span></a>        ...                   [2, 0, 2]]) 
<a name="l27920"><span class="ln">27920 </span></a>        &gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32) 
<a name="l27921"><span class="ln">27921 </span></a>        &gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4]) 
<a name="l27922"><span class="ln">27922 </span></a>        tensor(indices=tensor([[0, 1, 1], 
<a name="l27923"><span class="ln">27923 </span></a>                               [2, 0, 2]]), 
<a name="l27924"><span class="ln">27924 </span></a>               values=tensor([3., 4., 5.]), 
<a name="l27925"><span class="ln">27925 </span></a>               size=(2, 4), nnz=3, layout=torch.sparse_coo) 
<a name="l27926"><span class="ln">27926 </span></a> 
<a name="l27927"><span class="ln">27927 </span></a>        &gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference 
<a name="l27928"><span class="ln">27928 </span></a>        tensor(indices=tensor([[0, 1, 1], 
<a name="l27929"><span class="ln">27929 </span></a>                               [2, 0, 2]]), 
<a name="l27930"><span class="ln">27930 </span></a>               values=tensor([3., 4., 5.]), 
<a name="l27931"><span class="ln">27931 </span></a>               size=(2, 3), nnz=3, layout=torch.sparse_coo) 
<a name="l27932"><span class="ln">27932 </span></a> 
<a name="l27933"><span class="ln">27933 </span></a>        &gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4], 
<a name="l27934"><span class="ln">27934 </span></a>        ...                         dtype=torch.float64, 
<a name="l27935"><span class="ln">27935 </span></a>        ...                         device=torch.device('cuda:0')) 
<a name="l27936"><span class="ln">27936 </span></a>        tensor(indices=tensor([[0, 1, 1], 
<a name="l27937"><span class="ln">27937 </span></a>                               [2, 0, 2]]), 
<a name="l27938"><span class="ln">27938 </span></a>               values=tensor([3., 4., 5.]), 
<a name="l27939"><span class="ln">27939 </span></a>               device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64, 
<a name="l27940"><span class="ln">27940 </span></a>               layout=torch.sparse_coo) 
<a name="l27941"><span class="ln">27941 </span></a> 
<a name="l27942"><span class="ln">27942 </span></a>        # Create an empty sparse tensor with the following invariants: 
<a name="l27943"><span class="ln">27943 </span></a>        #   1. sparse_dim + dense_dim = len(SparseTensor.shape) 
<a name="l27944"><span class="ln">27944 </span></a>        #   2. SparseTensor._indices().shape = (sparse_dim, nnz) 
<a name="l27945"><span class="ln">27945 </span></a>        #   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:]) 
<a name="l27946"><span class="ln">27946 </span></a>        # 
<a name="l27947"><span class="ln">27947 </span></a>        # For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and 
<a name="l27948"><span class="ln">27948 </span></a>        # sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0)) 
<a name="l27949"><span class="ln">27949 </span></a>        &gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1]) 
<a name="l27950"><span class="ln">27950 </span></a>        tensor(indices=tensor([], size=(1, 0)), 
<a name="l27951"><span class="ln">27951 </span></a>               values=tensor([], size=(0,)), 
<a name="l27952"><span class="ln">27952 </span></a>               size=(1,), nnz=0, layout=torch.sparse_coo) 
<a name="l27953"><span class="ln">27953 </span></a> 
<a name="l27954"><span class="ln">27954 </span></a>        # and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and 
<a name="l27955"><span class="ln">27955 </span></a>        # sparse_dim = 1 
<a name="l27956"><span class="ln">27956 </span></a>        &gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2]) 
<a name="l27957"><span class="ln">27957 </span></a>        tensor(indices=tensor([], size=(1, 0)), 
<a name="l27958"><span class="ln">27958 </span></a>               values=tensor([], size=(0, 2)), 
<a name="l27959"><span class="ln">27959 </span></a>               size=(1, 2), nnz=0, layout=torch.sparse_coo) 
<a name="l27960"><span class="ln">27960 </span></a> 
<a name="l27961"><span class="ln">27961 </span></a>    .. _torch.sparse: https://pytorch.org/docs/stable/sparse.html 
<a name="l27962"><span class="ln">27962 </span></a>    &quot;&quot;&quot;</span>
<a name="l27963"><span class="ln">27963 </span></a>
<a name="l27964"><span class="ln">27964 </span></a><span class="s2">def </span><span class="s1">sparse_csc_tensor</span><span class="s3">(</span>
<a name="l27965"><span class="ln">27965 </span></a>    <span class="s1">ccol_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27966"><span class="ln">27966 </span></a>    <span class="s1">row_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27967"><span class="ln">27967 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l27968"><span class="ln">27968 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27969"><span class="ln">27969 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l27970"><span class="ln">27970 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27971"><span class="ln">27971 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27972"><span class="ln">27972 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l27973"><span class="ln">27973 </span></a>    <span class="s1">check_invariants</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l27974"><span class="ln">27974 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l27975"><span class="ln">27975 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l27976"><span class="ln">27976 </span></a>    sparse_csc_tensor(ccol_indices, row_indices, values, size=None, *, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l27977"><span class="ln">27977 </span></a> 
<a name="l27978"><span class="ln">27978 </span></a>    Constructs a :ref:`sparse tensor in CSC (Compressed Sparse Column) 
<a name="l27979"><span class="ln">27979 </span></a>    &lt;sparse-csc-docs&gt;` with specified values at the given 
<a name="l27980"><span class="ln">27980 </span></a>    :attr:`ccol_indices` and :attr:`row_indices`. Sparse matrix 
<a name="l27981"><span class="ln">27981 </span></a>    multiplication operations in CSC format are typically faster than that 
<a name="l27982"><span class="ln">27982 </span></a>    for sparse tensors in COO format. Make you have a look at :ref:`the 
<a name="l27983"><span class="ln">27983 </span></a>    note on the data type of the indices &lt;sparse-csc-docs&gt;`. 
<a name="l27984"><span class="ln">27984 </span></a> 
<a name="l27985"><span class="ln">27985 </span></a>    .. note:: 
<a name="l27986"><span class="ln">27986 </span></a> 
<a name="l27987"><span class="ln">27987 </span></a>       If the ``device`` argument is not specified the device of the given 
<a name="l27988"><span class="ln">27988 </span></a>       :attr:`values` and indices tensor(s) must match. If, however, the 
<a name="l27989"><span class="ln">27989 </span></a>       argument is specified the input Tensors will be converted to the 
<a name="l27990"><span class="ln">27990 </span></a>       given device and in turn determine the device of the constructed 
<a name="l27991"><span class="ln">27991 </span></a>       sparse tensor. 
<a name="l27992"><span class="ln">27992 </span></a> 
<a name="l27993"><span class="ln">27993 </span></a>    Args: 
<a name="l27994"><span class="ln">27994 </span></a>        ccol_indices (array_like): (B+1)-dimensional array of size 
<a name="l27995"><span class="ln">27995 </span></a>            ``(*batchsize, ncols + 1)``.  The last element of each batch 
<a name="l27996"><span class="ln">27996 </span></a>            is the number of non-zeros. This tensor encodes the index in 
<a name="l27997"><span class="ln">27997 </span></a>            values and row_indices depending on where the given column 
<a name="l27998"><span class="ln">27998 </span></a>            starts. Each successive number in the tensor subtracted by the 
<a name="l27999"><span class="ln">27999 </span></a>            number before it denotes the number of elements in a given 
<a name="l28000"><span class="ln">28000 </span></a>            column. 
<a name="l28001"><span class="ln">28001 </span></a>        row_indices (array_like): Row co-ordinates of each element in 
<a name="l28002"><span class="ln">28002 </span></a>            values. (B+1)-dimensional tensor with the same length as 
<a name="l28003"><span class="ln">28003 </span></a>            values. 
<a name="l28004"><span class="ln">28004 </span></a>        values (array_list): Initial values for the tensor. Can be a list, 
<a name="l28005"><span class="ln">28005 </span></a>            tuple, NumPy ``ndarray``, scalar, and other types that 
<a name="l28006"><span class="ln">28006 </span></a>            represents a (1+K)-dimensional tensor where ``K`` is the number 
<a name="l28007"><span class="ln">28007 </span></a>            of dense dimensions. 
<a name="l28008"><span class="ln">28008 </span></a>        size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l28009"><span class="ln">28009 </span></a>            sparse tensor: ``(*batchsize, nrows, ncols, *densesize)``. If 
<a name="l28010"><span class="ln">28010 </span></a>            not provided, the size will be inferred as the minimum size 
<a name="l28011"><span class="ln">28011 </span></a>            big enough to hold all non-zero elements. 
<a name="l28012"><span class="ln">28012 </span></a> 
<a name="l28013"><span class="ln">28013 </span></a>    Keyword args: 
<a name="l28014"><span class="ln">28014 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l28015"><span class="ln">28015 </span></a>            returned tensor.  Default: if None, infers data type from 
<a name="l28016"><span class="ln">28016 </span></a>            :attr:`values`. 
<a name="l28017"><span class="ln">28017 </span></a>        device (:class:`torch.device`, optional): the desired device of 
<a name="l28018"><span class="ln">28018 </span></a>            returned tensor.  Default: if None, uses the current device 
<a name="l28019"><span class="ln">28019 </span></a>            for the default tensor type (see 
<a name="l28020"><span class="ln">28020 </span></a>            :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l28021"><span class="ln">28021 </span></a>            the CPU for CPU tensor types and the current CUDA device for 
<a name="l28022"><span class="ln">28022 </span></a>            CUDA tensor types. 
<a name="l28023"><span class="ln">28023 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l28024"><span class="ln">28024 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l28025"><span class="ln">28025 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l28026"><span class="ln">28026 </span></a>            returned tensor. Default: ``False``. 
<a name="l28027"><span class="ln">28027 </span></a>        check_invariants (bool, optional): If sparse tensor invariants are checked. 
<a name="l28028"><span class="ln">28028 </span></a>            Default: as returned by :func:`torch.sparse.check_sparse_tensor_invariants.is_enabled`, 
<a name="l28029"><span class="ln">28029 </span></a>            initially False. 
<a name="l28030"><span class="ln">28030 </span></a> 
<a name="l28031"><span class="ln">28031 </span></a>    Example:: 
<a name="l28032"><span class="ln">28032 </span></a> 
<a name="l28033"><span class="ln">28033 </span></a>        &gt;&gt;&gt; ccol_indices = [0, 2, 4] 
<a name="l28034"><span class="ln">28034 </span></a>        &gt;&gt;&gt; row_indices = [0, 1, 0, 1] 
<a name="l28035"><span class="ln">28035 </span></a>        &gt;&gt;&gt; values = [1, 2, 3, 4] 
<a name="l28036"><span class="ln">28036 </span></a>        &gt;&gt;&gt; torch.sparse_csc_tensor(torch.tensor(ccol_indices, dtype=torch.int64), 
<a name="l28037"><span class="ln">28037 </span></a>        ...                         torch.tensor(row_indices, dtype=torch.int64), 
<a name="l28038"><span class="ln">28038 </span></a>        ...                         torch.tensor(values), dtype=torch.double) 
<a name="l28039"><span class="ln">28039 </span></a>        tensor(ccol_indices=tensor([0, 2, 4]), 
<a name="l28040"><span class="ln">28040 </span></a>               row_indices=tensor([0, 1, 0, 1]), 
<a name="l28041"><span class="ln">28041 </span></a>               values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, 
<a name="l28042"><span class="ln">28042 </span></a>               dtype=torch.float64, layout=torch.sparse_csc) 
<a name="l28043"><span class="ln">28043 </span></a>    &quot;&quot;&quot;</span>
<a name="l28044"><span class="ln">28044 </span></a>
<a name="l28045"><span class="ln">28045 </span></a><span class="s2">def </span><span class="s1">sparse_csr_tensor</span><span class="s3">(</span>
<a name="l28046"><span class="ln">28046 </span></a>    <span class="s1">crow_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l28047"><span class="ln">28047 </span></a>    <span class="s1">col_indices</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l28048"><span class="ln">28048 </span></a>    <span class="s1">values</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">list</span><span class="s3">,</span>
<a name="l28049"><span class="ln">28049 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28050"><span class="ln">28050 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28051"><span class="ln">28051 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28052"><span class="ln">28052 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28053"><span class="ln">28053 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l28054"><span class="ln">28054 </span></a>    <span class="s1">check_invariants</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28055"><span class="ln">28055 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28056"><span class="ln">28056 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28057"><span class="ln">28057 </span></a>    sparse_csr_tensor(crow_indices, col_indices, values, size=None, *, dtype=None, device=None, pin_memory=False, requires_grad=False, check_invariants=None) -&gt; Tensor 
<a name="l28058"><span class="ln">28058 </span></a> 
<a name="l28059"><span class="ln">28059 </span></a>    Constructs a :ref:`sparse tensor in CSR (Compressed Sparse Row) &lt;sparse-csr-docs&gt;` with specified 
<a name="l28060"><span class="ln">28060 </span></a>    values at the given :attr:`crow_indices` and :attr:`col_indices`. Sparse matrix multiplication operations 
<a name="l28061"><span class="ln">28061 </span></a>    in CSR format are typically faster than that for sparse tensors in COO format. Make you have a look 
<a name="l28062"><span class="ln">28062 </span></a>    at :ref:`the note on the data type of the indices &lt;sparse-csr-docs&gt;`. 
<a name="l28063"><span class="ln">28063 </span></a> 
<a name="l28064"><span class="ln">28064 </span></a>    .. note:: 
<a name="l28065"><span class="ln">28065 </span></a> 
<a name="l28066"><span class="ln">28066 </span></a>       If the ``device`` argument is not specified the device of the given 
<a name="l28067"><span class="ln">28067 </span></a>       :attr:`values` and indices tensor(s) must match. If, however, the 
<a name="l28068"><span class="ln">28068 </span></a>       argument is specified the input Tensors will be converted to the 
<a name="l28069"><span class="ln">28069 </span></a>       given device and in turn determine the device of the constructed 
<a name="l28070"><span class="ln">28070 </span></a>       sparse tensor. 
<a name="l28071"><span class="ln">28071 </span></a> 
<a name="l28072"><span class="ln">28072 </span></a>    Args: 
<a name="l28073"><span class="ln">28073 </span></a>        crow_indices (array_like): (B+1)-dimensional array of size 
<a name="l28074"><span class="ln">28074 </span></a>            ``(*batchsize, nrows + 1)``.  The last element of each batch 
<a name="l28075"><span class="ln">28075 </span></a>            is the number of non-zeros. This tensor encodes the index in 
<a name="l28076"><span class="ln">28076 </span></a>            values and col_indices depending on where the given row 
<a name="l28077"><span class="ln">28077 </span></a>            starts. Each successive number in the tensor subtracted by the 
<a name="l28078"><span class="ln">28078 </span></a>            number before it denotes the number of elements in a given 
<a name="l28079"><span class="ln">28079 </span></a>            row. 
<a name="l28080"><span class="ln">28080 </span></a>        col_indices (array_like): Column co-ordinates of each element in 
<a name="l28081"><span class="ln">28081 </span></a>            values. (B+1)-dimensional tensor with the same length 
<a name="l28082"><span class="ln">28082 </span></a>            as values. 
<a name="l28083"><span class="ln">28083 </span></a>        values (array_list): Initial values for the tensor. Can be a list, 
<a name="l28084"><span class="ln">28084 </span></a>            tuple, NumPy ``ndarray``, scalar, and other types that 
<a name="l28085"><span class="ln">28085 </span></a>            represents a (1+K)-dimensional tensor where ``K`` is the number 
<a name="l28086"><span class="ln">28086 </span></a>            of dense dimensions. 
<a name="l28087"><span class="ln">28087 </span></a>        size (list, tuple, :class:`torch.Size`, optional): Size of the 
<a name="l28088"><span class="ln">28088 </span></a>            sparse tensor: ``(*batchsize, nrows, ncols, *densesize)``. If 
<a name="l28089"><span class="ln">28089 </span></a>            not provided, the size will be inferred as the minimum size 
<a name="l28090"><span class="ln">28090 </span></a>            big enough to hold all non-zero elements. 
<a name="l28091"><span class="ln">28091 </span></a> 
<a name="l28092"><span class="ln">28092 </span></a>    Keyword args: 
<a name="l28093"><span class="ln">28093 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of 
<a name="l28094"><span class="ln">28094 </span></a>            returned tensor.  Default: if None, infers data type from 
<a name="l28095"><span class="ln">28095 </span></a>            :attr:`values`. 
<a name="l28096"><span class="ln">28096 </span></a>        device (:class:`torch.device`, optional): the desired device of 
<a name="l28097"><span class="ln">28097 </span></a>            returned tensor.  Default: if None, uses the current device 
<a name="l28098"><span class="ln">28098 </span></a>            for the default tensor type (see 
<a name="l28099"><span class="ln">28099 </span></a>            :func:`torch.set_default_device`). :attr:`device` will be 
<a name="l28100"><span class="ln">28100 </span></a>            the CPU for CPU tensor types and the current CUDA device for 
<a name="l28101"><span class="ln">28101 </span></a>            CUDA tensor types. 
<a name="l28102"><span class="ln">28102 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l28103"><span class="ln">28103 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l28104"><span class="ln">28104 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l28105"><span class="ln">28105 </span></a>            returned tensor. Default: ``False``. 
<a name="l28106"><span class="ln">28106 </span></a>        check_invariants (bool, optional): If sparse tensor invariants are checked. 
<a name="l28107"><span class="ln">28107 </span></a>            Default: as returned by :func:`torch.sparse.check_sparse_tensor_invariants.is_enabled`, 
<a name="l28108"><span class="ln">28108 </span></a>            initially False. 
<a name="l28109"><span class="ln">28109 </span></a> 
<a name="l28110"><span class="ln">28110 </span></a>    Example:: 
<a name="l28111"><span class="ln">28111 </span></a> 
<a name="l28112"><span class="ln">28112 </span></a>        &gt;&gt;&gt; crow_indices = [0, 2, 4] 
<a name="l28113"><span class="ln">28113 </span></a>        &gt;&gt;&gt; col_indices = [0, 1, 0, 1] 
<a name="l28114"><span class="ln">28114 </span></a>        &gt;&gt;&gt; values = [1, 2, 3, 4] 
<a name="l28115"><span class="ln">28115 </span></a>        &gt;&gt;&gt; torch.sparse_csr_tensor(torch.tensor(crow_indices, dtype=torch.int64), 
<a name="l28116"><span class="ln">28116 </span></a>        ...                         torch.tensor(col_indices, dtype=torch.int64), 
<a name="l28117"><span class="ln">28117 </span></a>        ...                         torch.tensor(values), dtype=torch.double) 
<a name="l28118"><span class="ln">28118 </span></a>        tensor(crow_indices=tensor([0, 2, 4]), 
<a name="l28119"><span class="ln">28119 </span></a>               col_indices=tensor([0, 1, 0, 1]), 
<a name="l28120"><span class="ln">28120 </span></a>               values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, 
<a name="l28121"><span class="ln">28121 </span></a>               dtype=torch.float64, layout=torch.sparse_csr) 
<a name="l28122"><span class="ln">28122 </span></a>    &quot;&quot;&quot;</span>
<a name="l28123"><span class="ln">28123 </span></a>
<a name="l28124"><span class="ln">28124 </span></a><span class="s2">def </span><span class="s1">split_copy</span><span class="s3">(</span>
<a name="l28125"><span class="ln">28125 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28126"><span class="ln">28126 </span></a>    <span class="s1">split_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l28127"><span class="ln">28127 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l28128"><span class="ln">28128 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28129"><span class="ln">28129 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28130"><span class="ln">28130 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l28131"><span class="ln">28131 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28132"><span class="ln">28132 </span></a>    Performs the same operation as :func:`torch.split`, but all output tensors 
<a name="l28133"><span class="ln">28133 </span></a>    are freshly created instead of aliasing the input. 
<a name="l28134"><span class="ln">28134 </span></a>    &quot;&quot;&quot;</span>
<a name="l28135"><span class="ln">28135 </span></a>
<a name="l28136"><span class="ln">28136 </span></a><span class="s2">def </span><span class="s1">split_with_sizes</span><span class="s3">(</span>
<a name="l28137"><span class="ln">28137 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28138"><span class="ln">28138 </span></a>    <span class="s1">split_sizes</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l28139"><span class="ln">28139 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l28140"><span class="ln">28140 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l28141"><span class="ln">28141 </span></a><span class="s2">def </span><span class="s1">split_with_sizes_copy</span><span class="s3">(</span>
<a name="l28142"><span class="ln">28142 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28143"><span class="ln">28143 </span></a>    <span class="s1">split_sizes</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l28144"><span class="ln">28144 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l28145"><span class="ln">28145 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28146"><span class="ln">28146 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28147"><span class="ln">28147 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l28148"><span class="ln">28148 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28149"><span class="ln">28149 </span></a>    Performs the same operation as :func:`torch.split_with_sizes`, but all output tensors 
<a name="l28150"><span class="ln">28150 </span></a>    are freshly created instead of aliasing the input. 
<a name="l28151"><span class="ln">28151 </span></a>    &quot;&quot;&quot;</span>
<a name="l28152"><span class="ln">28152 </span></a>
<a name="l28153"><span class="ln">28153 </span></a><span class="s2">def </span><span class="s1">spmm</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l28154"><span class="ln">28154 </span></a><span class="s2">def </span><span class="s1">sqrt</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28155"><span class="ln">28155 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28156"><span class="ln">28156 </span></a>    sqrt(input, *, out=None) -&gt; Tensor 
<a name="l28157"><span class="ln">28157 </span></a> 
<a name="l28158"><span class="ln">28158 </span></a>    Returns a new tensor with the square-root of the elements of :attr:`input`. 
<a name="l28159"><span class="ln">28159 </span></a> 
<a name="l28160"><span class="ln">28160 </span></a>    .. math:: 
<a name="l28161"><span class="ln">28161 </span></a>        \text{out}_{i} = \sqrt{\text{input}_{i}} 
<a name="l28162"><span class="ln">28162 </span></a> 
<a name="l28163"><span class="ln">28163 </span></a>    Args: 
<a name="l28164"><span class="ln">28164 </span></a>        input (Tensor): the input tensor. 
<a name="l28165"><span class="ln">28165 </span></a> 
<a name="l28166"><span class="ln">28166 </span></a>    Keyword args: 
<a name="l28167"><span class="ln">28167 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28168"><span class="ln">28168 </span></a> 
<a name="l28169"><span class="ln">28169 </span></a>    Example:: 
<a name="l28170"><span class="ln">28170 </span></a> 
<a name="l28171"><span class="ln">28171 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l28172"><span class="ln">28172 </span></a>        &gt;&gt;&gt; a 
<a name="l28173"><span class="ln">28173 </span></a>        tensor([-2.0755,  1.0226,  0.0831,  0.4806]) 
<a name="l28174"><span class="ln">28174 </span></a>        &gt;&gt;&gt; torch.sqrt(a) 
<a name="l28175"><span class="ln">28175 </span></a>        tensor([    nan,  1.0112,  0.2883,  0.6933]) 
<a name="l28176"><span class="ln">28176 </span></a>    &quot;&quot;&quot;</span>
<a name="l28177"><span class="ln">28177 </span></a>
<a name="l28178"><span class="ln">28178 </span></a><span class="s2">def </span><span class="s1">sqrt_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l28179"><span class="ln">28179 </span></a><span class="s2">def </span><span class="s1">square</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28180"><span class="ln">28180 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28181"><span class="ln">28181 </span></a>    square(input: Tensor, *, out: Optional[Tensor]) -&gt; Tensor 
<a name="l28182"><span class="ln">28182 </span></a> 
<a name="l28183"><span class="ln">28183 </span></a>    Returns a new tensor with the square of the elements of :attr:`input`. 
<a name="l28184"><span class="ln">28184 </span></a> 
<a name="l28185"><span class="ln">28185 </span></a>    Args: 
<a name="l28186"><span class="ln">28186 </span></a>        input (Tensor): the input tensor. 
<a name="l28187"><span class="ln">28187 </span></a> 
<a name="l28188"><span class="ln">28188 </span></a>    Keyword args: 
<a name="l28189"><span class="ln">28189 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28190"><span class="ln">28190 </span></a> 
<a name="l28191"><span class="ln">28191 </span></a>    Example:: 
<a name="l28192"><span class="ln">28192 </span></a> 
<a name="l28193"><span class="ln">28193 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l28194"><span class="ln">28194 </span></a>        &gt;&gt;&gt; a 
<a name="l28195"><span class="ln">28195 </span></a>        tensor([-2.0755,  1.0226,  0.0831,  0.4806]) 
<a name="l28196"><span class="ln">28196 </span></a>        &gt;&gt;&gt; torch.square(a) 
<a name="l28197"><span class="ln">28197 </span></a>        tensor([ 4.3077,  1.0457,  0.0069,  0.2310]) 
<a name="l28198"><span class="ln">28198 </span></a>    &quot;&quot;&quot;</span>
<a name="l28199"><span class="ln">28199 </span></a>
<a name="l28200"><span class="ln">28200 </span></a><span class="s2">def </span><span class="s1">square_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l28201"><span class="ln">28201 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28202"><span class="ln">28202 </span></a><span class="s2">def </span><span class="s1">squeeze</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28203"><span class="ln">28203 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28204"><span class="ln">28204 </span></a>    squeeze(input: Tensor, dim: Optional[Union[int, List[int]]]) -&gt; Tensor 
<a name="l28205"><span class="ln">28205 </span></a> 
<a name="l28206"><span class="ln">28206 </span></a>    Returns a tensor with all specified dimensions of :attr:`input` of size `1` removed. 
<a name="l28207"><span class="ln">28207 </span></a> 
<a name="l28208"><span class="ln">28208 </span></a>    For example, if `input` is of shape: 
<a name="l28209"><span class="ln">28209 </span></a>    :math:`(A \times 1 \times B \times C \times 1 \times D)` then the `input.squeeze()` 
<a name="l28210"><span class="ln">28210 </span></a>    will be of shape: :math:`(A \times B \times C \times D)`. 
<a name="l28211"><span class="ln">28211 </span></a> 
<a name="l28212"><span class="ln">28212 </span></a>    When :attr:`dim` is given, a squeeze operation is done only in the given 
<a name="l28213"><span class="ln">28213 </span></a>    dimension(s). If `input` is of shape: :math:`(A \times 1 \times B)`, 
<a name="l28214"><span class="ln">28214 </span></a>    ``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)`` 
<a name="l28215"><span class="ln">28215 </span></a>    will squeeze the tensor to the shape :math:`(A \times B)`. 
<a name="l28216"><span class="ln">28216 </span></a> 
<a name="l28217"><span class="ln">28217 </span></a>    .. note:: The returned tensor shares the storage with the input tensor, 
<a name="l28218"><span class="ln">28218 </span></a>              so changing the contents of one will change the contents of the other. 
<a name="l28219"><span class="ln">28219 </span></a> 
<a name="l28220"><span class="ln">28220 </span></a>    .. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)` 
<a name="l28221"><span class="ln">28221 </span></a>              will also remove the batch dimension, which can lead to unexpected 
<a name="l28222"><span class="ln">28222 </span></a>              errors. Consider specifying only the dims you wish to be squeezed. 
<a name="l28223"><span class="ln">28223 </span></a> 
<a name="l28224"><span class="ln">28224 </span></a>    Args: 
<a name="l28225"><span class="ln">28225 </span></a>        input (Tensor): the input tensor. 
<a name="l28226"><span class="ln">28226 </span></a>        dim (int or tuple of ints, optional): if given, the input will be squeezed 
<a name="l28227"><span class="ln">28227 </span></a>               only in the specified dimensions. 
<a name="l28228"><span class="ln">28228 </span></a> 
<a name="l28229"><span class="ln">28229 </span></a>            .. versionchanged:: 2.0 
<a name="l28230"><span class="ln">28230 </span></a>               :attr:`dim` now accepts tuples of dimensions. 
<a name="l28231"><span class="ln">28231 </span></a> 
<a name="l28232"><span class="ln">28232 </span></a>    Example:: 
<a name="l28233"><span class="ln">28233 </span></a> 
<a name="l28234"><span class="ln">28234 </span></a>        &gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) 
<a name="l28235"><span class="ln">28235 </span></a>        &gt;&gt;&gt; x.size() 
<a name="l28236"><span class="ln">28236 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28237"><span class="ln">28237 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x) 
<a name="l28238"><span class="ln">28238 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28239"><span class="ln">28239 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28240"><span class="ln">28240 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 0) 
<a name="l28241"><span class="ln">28241 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28242"><span class="ln">28242 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28243"><span class="ln">28243 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 1) 
<a name="l28244"><span class="ln">28244 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28245"><span class="ln">28245 </span></a>        torch.Size([2, 2, 1, 2]) 
<a name="l28246"><span class="ln">28246 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) 
<a name="l28247"><span class="ln">28247 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28248"><span class="ln">28248 </span></a>    &quot;&quot;&quot;</span>
<a name="l28249"><span class="ln">28249 </span></a>
<a name="l28250"><span class="ln">28250 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28251"><span class="ln">28251 </span></a><span class="s2">def </span><span class="s1">squeeze</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28252"><span class="ln">28252 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28253"><span class="ln">28253 </span></a>    squeeze(input: Tensor, dim: Optional[Union[int, List[int]]]) -&gt; Tensor 
<a name="l28254"><span class="ln">28254 </span></a> 
<a name="l28255"><span class="ln">28255 </span></a>    Returns a tensor with all specified dimensions of :attr:`input` of size `1` removed. 
<a name="l28256"><span class="ln">28256 </span></a> 
<a name="l28257"><span class="ln">28257 </span></a>    For example, if `input` is of shape: 
<a name="l28258"><span class="ln">28258 </span></a>    :math:`(A \times 1 \times B \times C \times 1 \times D)` then the `input.squeeze()` 
<a name="l28259"><span class="ln">28259 </span></a>    will be of shape: :math:`(A \times B \times C \times D)`. 
<a name="l28260"><span class="ln">28260 </span></a> 
<a name="l28261"><span class="ln">28261 </span></a>    When :attr:`dim` is given, a squeeze operation is done only in the given 
<a name="l28262"><span class="ln">28262 </span></a>    dimension(s). If `input` is of shape: :math:`(A \times 1 \times B)`, 
<a name="l28263"><span class="ln">28263 </span></a>    ``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)`` 
<a name="l28264"><span class="ln">28264 </span></a>    will squeeze the tensor to the shape :math:`(A \times B)`. 
<a name="l28265"><span class="ln">28265 </span></a> 
<a name="l28266"><span class="ln">28266 </span></a>    .. note:: The returned tensor shares the storage with the input tensor, 
<a name="l28267"><span class="ln">28267 </span></a>              so changing the contents of one will change the contents of the other. 
<a name="l28268"><span class="ln">28268 </span></a> 
<a name="l28269"><span class="ln">28269 </span></a>    .. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)` 
<a name="l28270"><span class="ln">28270 </span></a>              will also remove the batch dimension, which can lead to unexpected 
<a name="l28271"><span class="ln">28271 </span></a>              errors. Consider specifying only the dims you wish to be squeezed. 
<a name="l28272"><span class="ln">28272 </span></a> 
<a name="l28273"><span class="ln">28273 </span></a>    Args: 
<a name="l28274"><span class="ln">28274 </span></a>        input (Tensor): the input tensor. 
<a name="l28275"><span class="ln">28275 </span></a>        dim (int or tuple of ints, optional): if given, the input will be squeezed 
<a name="l28276"><span class="ln">28276 </span></a>               only in the specified dimensions. 
<a name="l28277"><span class="ln">28277 </span></a> 
<a name="l28278"><span class="ln">28278 </span></a>            .. versionchanged:: 2.0 
<a name="l28279"><span class="ln">28279 </span></a>               :attr:`dim` now accepts tuples of dimensions. 
<a name="l28280"><span class="ln">28280 </span></a> 
<a name="l28281"><span class="ln">28281 </span></a>    Example:: 
<a name="l28282"><span class="ln">28282 </span></a> 
<a name="l28283"><span class="ln">28283 </span></a>        &gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) 
<a name="l28284"><span class="ln">28284 </span></a>        &gt;&gt;&gt; x.size() 
<a name="l28285"><span class="ln">28285 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28286"><span class="ln">28286 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x) 
<a name="l28287"><span class="ln">28287 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28288"><span class="ln">28288 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28289"><span class="ln">28289 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 0) 
<a name="l28290"><span class="ln">28290 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28291"><span class="ln">28291 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28292"><span class="ln">28292 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 1) 
<a name="l28293"><span class="ln">28293 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28294"><span class="ln">28294 </span></a>        torch.Size([2, 2, 1, 2]) 
<a name="l28295"><span class="ln">28295 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) 
<a name="l28296"><span class="ln">28296 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28297"><span class="ln">28297 </span></a>    &quot;&quot;&quot;</span>
<a name="l28298"><span class="ln">28298 </span></a>
<a name="l28299"><span class="ln">28299 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28300"><span class="ln">28300 </span></a><span class="s2">def </span><span class="s1">squeeze</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28301"><span class="ln">28301 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28302"><span class="ln">28302 </span></a>    squeeze(input: Tensor, dim: Optional[Union[int, List[int]]]) -&gt; Tensor 
<a name="l28303"><span class="ln">28303 </span></a> 
<a name="l28304"><span class="ln">28304 </span></a>    Returns a tensor with all specified dimensions of :attr:`input` of size `1` removed. 
<a name="l28305"><span class="ln">28305 </span></a> 
<a name="l28306"><span class="ln">28306 </span></a>    For example, if `input` is of shape: 
<a name="l28307"><span class="ln">28307 </span></a>    :math:`(A \times 1 \times B \times C \times 1 \times D)` then the `input.squeeze()` 
<a name="l28308"><span class="ln">28308 </span></a>    will be of shape: :math:`(A \times B \times C \times D)`. 
<a name="l28309"><span class="ln">28309 </span></a> 
<a name="l28310"><span class="ln">28310 </span></a>    When :attr:`dim` is given, a squeeze operation is done only in the given 
<a name="l28311"><span class="ln">28311 </span></a>    dimension(s). If `input` is of shape: :math:`(A \times 1 \times B)`, 
<a name="l28312"><span class="ln">28312 </span></a>    ``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)`` 
<a name="l28313"><span class="ln">28313 </span></a>    will squeeze the tensor to the shape :math:`(A \times B)`. 
<a name="l28314"><span class="ln">28314 </span></a> 
<a name="l28315"><span class="ln">28315 </span></a>    .. note:: The returned tensor shares the storage with the input tensor, 
<a name="l28316"><span class="ln">28316 </span></a>              so changing the contents of one will change the contents of the other. 
<a name="l28317"><span class="ln">28317 </span></a> 
<a name="l28318"><span class="ln">28318 </span></a>    .. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)` 
<a name="l28319"><span class="ln">28319 </span></a>              will also remove the batch dimension, which can lead to unexpected 
<a name="l28320"><span class="ln">28320 </span></a>              errors. Consider specifying only the dims you wish to be squeezed. 
<a name="l28321"><span class="ln">28321 </span></a> 
<a name="l28322"><span class="ln">28322 </span></a>    Args: 
<a name="l28323"><span class="ln">28323 </span></a>        input (Tensor): the input tensor. 
<a name="l28324"><span class="ln">28324 </span></a>        dim (int or tuple of ints, optional): if given, the input will be squeezed 
<a name="l28325"><span class="ln">28325 </span></a>               only in the specified dimensions. 
<a name="l28326"><span class="ln">28326 </span></a> 
<a name="l28327"><span class="ln">28327 </span></a>            .. versionchanged:: 2.0 
<a name="l28328"><span class="ln">28328 </span></a>               :attr:`dim` now accepts tuples of dimensions. 
<a name="l28329"><span class="ln">28329 </span></a> 
<a name="l28330"><span class="ln">28330 </span></a>    Example:: 
<a name="l28331"><span class="ln">28331 </span></a> 
<a name="l28332"><span class="ln">28332 </span></a>        &gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) 
<a name="l28333"><span class="ln">28333 </span></a>        &gt;&gt;&gt; x.size() 
<a name="l28334"><span class="ln">28334 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28335"><span class="ln">28335 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x) 
<a name="l28336"><span class="ln">28336 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28337"><span class="ln">28337 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28338"><span class="ln">28338 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 0) 
<a name="l28339"><span class="ln">28339 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28340"><span class="ln">28340 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28341"><span class="ln">28341 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 1) 
<a name="l28342"><span class="ln">28342 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28343"><span class="ln">28343 </span></a>        torch.Size([2, 2, 1, 2]) 
<a name="l28344"><span class="ln">28344 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) 
<a name="l28345"><span class="ln">28345 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28346"><span class="ln">28346 </span></a>    &quot;&quot;&quot;</span>
<a name="l28347"><span class="ln">28347 </span></a>
<a name="l28348"><span class="ln">28348 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28349"><span class="ln">28349 </span></a><span class="s2">def </span><span class="s1">squeeze</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28350"><span class="ln">28350 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28351"><span class="ln">28351 </span></a>    squeeze(input: Tensor, dim: Optional[Union[int, List[int]]]) -&gt; Tensor 
<a name="l28352"><span class="ln">28352 </span></a> 
<a name="l28353"><span class="ln">28353 </span></a>    Returns a tensor with all specified dimensions of :attr:`input` of size `1` removed. 
<a name="l28354"><span class="ln">28354 </span></a> 
<a name="l28355"><span class="ln">28355 </span></a>    For example, if `input` is of shape: 
<a name="l28356"><span class="ln">28356 </span></a>    :math:`(A \times 1 \times B \times C \times 1 \times D)` then the `input.squeeze()` 
<a name="l28357"><span class="ln">28357 </span></a>    will be of shape: :math:`(A \times B \times C \times D)`. 
<a name="l28358"><span class="ln">28358 </span></a> 
<a name="l28359"><span class="ln">28359 </span></a>    When :attr:`dim` is given, a squeeze operation is done only in the given 
<a name="l28360"><span class="ln">28360 </span></a>    dimension(s). If `input` is of shape: :math:`(A \times 1 \times B)`, 
<a name="l28361"><span class="ln">28361 </span></a>    ``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)`` 
<a name="l28362"><span class="ln">28362 </span></a>    will squeeze the tensor to the shape :math:`(A \times B)`. 
<a name="l28363"><span class="ln">28363 </span></a> 
<a name="l28364"><span class="ln">28364 </span></a>    .. note:: The returned tensor shares the storage with the input tensor, 
<a name="l28365"><span class="ln">28365 </span></a>              so changing the contents of one will change the contents of the other. 
<a name="l28366"><span class="ln">28366 </span></a> 
<a name="l28367"><span class="ln">28367 </span></a>    .. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)` 
<a name="l28368"><span class="ln">28368 </span></a>              will also remove the batch dimension, which can lead to unexpected 
<a name="l28369"><span class="ln">28369 </span></a>              errors. Consider specifying only the dims you wish to be squeezed. 
<a name="l28370"><span class="ln">28370 </span></a> 
<a name="l28371"><span class="ln">28371 </span></a>    Args: 
<a name="l28372"><span class="ln">28372 </span></a>        input (Tensor): the input tensor. 
<a name="l28373"><span class="ln">28373 </span></a>        dim (int or tuple of ints, optional): if given, the input will be squeezed 
<a name="l28374"><span class="ln">28374 </span></a>               only in the specified dimensions. 
<a name="l28375"><span class="ln">28375 </span></a> 
<a name="l28376"><span class="ln">28376 </span></a>            .. versionchanged:: 2.0 
<a name="l28377"><span class="ln">28377 </span></a>               :attr:`dim` now accepts tuples of dimensions. 
<a name="l28378"><span class="ln">28378 </span></a> 
<a name="l28379"><span class="ln">28379 </span></a>    Example:: 
<a name="l28380"><span class="ln">28380 </span></a> 
<a name="l28381"><span class="ln">28381 </span></a>        &gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) 
<a name="l28382"><span class="ln">28382 </span></a>        &gt;&gt;&gt; x.size() 
<a name="l28383"><span class="ln">28383 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28384"><span class="ln">28384 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x) 
<a name="l28385"><span class="ln">28385 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28386"><span class="ln">28386 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28387"><span class="ln">28387 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 0) 
<a name="l28388"><span class="ln">28388 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28389"><span class="ln">28389 </span></a>        torch.Size([2, 1, 2, 1, 2]) 
<a name="l28390"><span class="ln">28390 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, 1) 
<a name="l28391"><span class="ln">28391 </span></a>        &gt;&gt;&gt; y.size() 
<a name="l28392"><span class="ln">28392 </span></a>        torch.Size([2, 2, 1, 2]) 
<a name="l28393"><span class="ln">28393 </span></a>        &gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) 
<a name="l28394"><span class="ln">28394 </span></a>        torch.Size([2, 2, 2]) 
<a name="l28395"><span class="ln">28395 </span></a>    &quot;&quot;&quot;</span>
<a name="l28396"><span class="ln">28396 </span></a>
<a name="l28397"><span class="ln">28397 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28398"><span class="ln">28398 </span></a><span class="s2">def </span><span class="s1">squeeze_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28399"><span class="ln">28399 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28400"><span class="ln">28400 </span></a>    Performs the same operation as :func:`torch.squeeze`, but all output tensors 
<a name="l28401"><span class="ln">28401 </span></a>    are freshly created instead of aliasing the input. 
<a name="l28402"><span class="ln">28402 </span></a>    &quot;&quot;&quot;</span>
<a name="l28403"><span class="ln">28403 </span></a>
<a name="l28404"><span class="ln">28404 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28405"><span class="ln">28405 </span></a><span class="s2">def </span><span class="s1">squeeze_copy</span><span class="s3">(</span>
<a name="l28406"><span class="ln">28406 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28407"><span class="ln">28407 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l28408"><span class="ln">28408 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28409"><span class="ln">28409 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28410"><span class="ln">28410 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28411"><span class="ln">28411 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28412"><span class="ln">28412 </span></a>    Performs the same operation as :func:`torch.squeeze`, but all output tensors 
<a name="l28413"><span class="ln">28413 </span></a>    are freshly created instead of aliasing the input. 
<a name="l28414"><span class="ln">28414 </span></a>    &quot;&quot;&quot;</span>
<a name="l28415"><span class="ln">28415 </span></a>
<a name="l28416"><span class="ln">28416 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28417"><span class="ln">28417 </span></a><span class="s2">def </span><span class="s1">squeeze_copy</span><span class="s3">(</span>
<a name="l28418"><span class="ln">28418 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28419"><span class="ln">28419 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l28420"><span class="ln">28420 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28421"><span class="ln">28421 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28422"><span class="ln">28422 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28423"><span class="ln">28423 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28424"><span class="ln">28424 </span></a>    Performs the same operation as :func:`torch.squeeze`, but all output tensors 
<a name="l28425"><span class="ln">28425 </span></a>    are freshly created instead of aliasing the input. 
<a name="l28426"><span class="ln">28426 </span></a>    &quot;&quot;&quot;</span>
<a name="l28427"><span class="ln">28427 </span></a>
<a name="l28428"><span class="ln">28428 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28429"><span class="ln">28429 </span></a><span class="s2">def </span><span class="s1">sspaddmm</span><span class="s3">(</span>
<a name="l28430"><span class="ln">28430 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l28431"><span class="ln">28431 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28432"><span class="ln">28432 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l28433"><span class="ln">28433 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28434"><span class="ln">28434 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28435"><span class="ln">28435 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28436"><span class="ln">28436 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28437"><span class="ln">28437 </span></a>    sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l28438"><span class="ln">28438 </span></a> 
<a name="l28439"><span class="ln">28439 </span></a>    Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor 
<a name="l28440"><span class="ln">28440 </span></a>    :attr:`mat2`, then adds the sparse tensor :attr:`input` to the result. 
<a name="l28441"><span class="ln">28441 </span></a> 
<a name="l28442"><span class="ln">28442 </span></a>    Note: This function is equivalent to :func:`torch.addmm`, except 
<a name="l28443"><span class="ln">28443 </span></a>    :attr:`input` and :attr:`mat1` are sparse. 
<a name="l28444"><span class="ln">28444 </span></a> 
<a name="l28445"><span class="ln">28445 </span></a>    Args: 
<a name="l28446"><span class="ln">28446 </span></a>        input (Tensor): a sparse matrix to be added 
<a name="l28447"><span class="ln">28447 </span></a>        mat1 (Tensor): a sparse matrix to be matrix multiplied 
<a name="l28448"><span class="ln">28448 </span></a>        mat2 (Tensor): a dense matrix to be matrix multiplied 
<a name="l28449"><span class="ln">28449 </span></a> 
<a name="l28450"><span class="ln">28450 </span></a>    Keyword args: 
<a name="l28451"><span class="ln">28451 </span></a>        beta (Number, optional): multiplier for :attr:`mat` (:math:`\beta`) 
<a name="l28452"><span class="ln">28452 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l28453"><span class="ln">28453 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28454"><span class="ln">28454 </span></a>    &quot;&quot;&quot;</span>
<a name="l28455"><span class="ln">28455 </span></a>
<a name="l28456"><span class="ln">28456 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28457"><span class="ln">28457 </span></a><span class="s2">def </span><span class="s1">sspaddmm</span><span class="s3">(</span>
<a name="l28458"><span class="ln">28458 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28459"><span class="ln">28459 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28460"><span class="ln">28460 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28461"><span class="ln">28461 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28462"><span class="ln">28462 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l28463"><span class="ln">28463 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l28464"><span class="ln">28464 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28465"><span class="ln">28465 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28466"><span class="ln">28466 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28467"><span class="ln">28467 </span></a>    sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l28468"><span class="ln">28468 </span></a> 
<a name="l28469"><span class="ln">28469 </span></a>    Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor 
<a name="l28470"><span class="ln">28470 </span></a>    :attr:`mat2`, then adds the sparse tensor :attr:`input` to the result. 
<a name="l28471"><span class="ln">28471 </span></a> 
<a name="l28472"><span class="ln">28472 </span></a>    Note: This function is equivalent to :func:`torch.addmm`, except 
<a name="l28473"><span class="ln">28473 </span></a>    :attr:`input` and :attr:`mat1` are sparse. 
<a name="l28474"><span class="ln">28474 </span></a> 
<a name="l28475"><span class="ln">28475 </span></a>    Args: 
<a name="l28476"><span class="ln">28476 </span></a>        input (Tensor): a sparse matrix to be added 
<a name="l28477"><span class="ln">28477 </span></a>        mat1 (Tensor): a sparse matrix to be matrix multiplied 
<a name="l28478"><span class="ln">28478 </span></a>        mat2 (Tensor): a dense matrix to be matrix multiplied 
<a name="l28479"><span class="ln">28479 </span></a> 
<a name="l28480"><span class="ln">28480 </span></a>    Keyword args: 
<a name="l28481"><span class="ln">28481 </span></a>        beta (Number, optional): multiplier for :attr:`mat` (:math:`\beta`) 
<a name="l28482"><span class="ln">28482 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l28483"><span class="ln">28483 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28484"><span class="ln">28484 </span></a>    &quot;&quot;&quot;</span>
<a name="l28485"><span class="ln">28485 </span></a>
<a name="l28486"><span class="ln">28486 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28487"><span class="ln">28487 </span></a><span class="s2">def </span><span class="s1">sspaddmm</span><span class="s3">(</span>
<a name="l28488"><span class="ln">28488 </span></a>    <span class="s1">beta</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l28489"><span class="ln">28489 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28490"><span class="ln">28490 </span></a>    <span class="s1">mat1</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28491"><span class="ln">28491 </span></a>    <span class="s1">mat2</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28492"><span class="ln">28492 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28493"><span class="ln">28493 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28494"><span class="ln">28494 </span></a>    sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -&gt; Tensor 
<a name="l28495"><span class="ln">28495 </span></a> 
<a name="l28496"><span class="ln">28496 </span></a>    Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor 
<a name="l28497"><span class="ln">28497 </span></a>    :attr:`mat2`, then adds the sparse tensor :attr:`input` to the result. 
<a name="l28498"><span class="ln">28498 </span></a> 
<a name="l28499"><span class="ln">28499 </span></a>    Note: This function is equivalent to :func:`torch.addmm`, except 
<a name="l28500"><span class="ln">28500 </span></a>    :attr:`input` and :attr:`mat1` are sparse. 
<a name="l28501"><span class="ln">28501 </span></a> 
<a name="l28502"><span class="ln">28502 </span></a>    Args: 
<a name="l28503"><span class="ln">28503 </span></a>        input (Tensor): a sparse matrix to be added 
<a name="l28504"><span class="ln">28504 </span></a>        mat1 (Tensor): a sparse matrix to be matrix multiplied 
<a name="l28505"><span class="ln">28505 </span></a>        mat2 (Tensor): a dense matrix to be matrix multiplied 
<a name="l28506"><span class="ln">28506 </span></a> 
<a name="l28507"><span class="ln">28507 </span></a>    Keyword args: 
<a name="l28508"><span class="ln">28508 </span></a>        beta (Number, optional): multiplier for :attr:`mat` (:math:`\beta`) 
<a name="l28509"><span class="ln">28509 </span></a>        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`) 
<a name="l28510"><span class="ln">28510 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28511"><span class="ln">28511 </span></a>    &quot;&quot;&quot;</span>
<a name="l28512"><span class="ln">28512 </span></a>
<a name="l28513"><span class="ln">28513 </span></a><span class="s2">def </span><span class="s1">stack</span><span class="s3">(</span>
<a name="l28514"><span class="ln">28514 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l28515"><span class="ln">28515 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l28516"><span class="ln">28516 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28517"><span class="ln">28517 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28518"><span class="ln">28518 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28519"><span class="ln">28519 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28520"><span class="ln">28520 </span></a>    stack(tensors, dim=0, *, out=None) -&gt; Tensor 
<a name="l28521"><span class="ln">28521 </span></a> 
<a name="l28522"><span class="ln">28522 </span></a>    Concatenates a sequence of tensors along a new dimension. 
<a name="l28523"><span class="ln">28523 </span></a> 
<a name="l28524"><span class="ln">28524 </span></a>    All tensors need to be of the same size. 
<a name="l28525"><span class="ln">28525 </span></a> 
<a name="l28526"><span class="ln">28526 </span></a>    .. seealso:: 
<a name="l28527"><span class="ln">28527 </span></a> 
<a name="l28528"><span class="ln">28528 </span></a>        :func:`torch.cat` concatenates the given sequence along an existing dimension. 
<a name="l28529"><span class="ln">28529 </span></a> 
<a name="l28530"><span class="ln">28530 </span></a>    Arguments: 
<a name="l28531"><span class="ln">28531 </span></a>        tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l28532"><span class="ln">28532 </span></a>        dim (int, optional): dimension to insert. Has to be between 0 and the number 
<a name="l28533"><span class="ln">28533 </span></a>            of dimensions of concatenated tensors (inclusive). Default: 0 
<a name="l28534"><span class="ln">28534 </span></a> 
<a name="l28535"><span class="ln">28535 </span></a>    Keyword args: 
<a name="l28536"><span class="ln">28536 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28537"><span class="ln">28537 </span></a> 
<a name="l28538"><span class="ln">28538 </span></a>    Example:: 
<a name="l28539"><span class="ln">28539 </span></a> 
<a name="l28540"><span class="ln">28540 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l28541"><span class="ln">28541 </span></a>        &gt;&gt;&gt; x 
<a name="l28542"><span class="ln">28542 </span></a>        tensor([[ 0.3367,  0.1288,  0.2345], 
<a name="l28543"><span class="ln">28543 </span></a>                [ 0.2303, -1.1229, -0.1863]]) 
<a name="l28544"><span class="ln">28544 </span></a>        &gt;&gt;&gt; torch.stack((x, x)) # same as torch.stack((x, x), dim=0) 
<a name="l28545"><span class="ln">28545 </span></a>        tensor([[[ 0.3367,  0.1288,  0.2345], 
<a name="l28546"><span class="ln">28546 </span></a>                 [ 0.2303, -1.1229, -0.1863]], 
<a name="l28547"><span class="ln">28547 </span></a> 
<a name="l28548"><span class="ln">28548 </span></a>                [[ 0.3367,  0.1288,  0.2345], 
<a name="l28549"><span class="ln">28549 </span></a>                 [ 0.2303, -1.1229, -0.1863]]]) 
<a name="l28550"><span class="ln">28550 </span></a>        &gt;&gt;&gt; torch.stack((x, x)).size() 
<a name="l28551"><span class="ln">28551 </span></a>        torch.Size([2, 2, 3]) 
<a name="l28552"><span class="ln">28552 </span></a>        &gt;&gt;&gt; torch.stack((x, x), dim=1) 
<a name="l28553"><span class="ln">28553 </span></a>        tensor([[[ 0.3367,  0.1288,  0.2345], 
<a name="l28554"><span class="ln">28554 </span></a>                 [ 0.3367,  0.1288,  0.2345]], 
<a name="l28555"><span class="ln">28555 </span></a> 
<a name="l28556"><span class="ln">28556 </span></a>                [[ 0.2303, -1.1229, -0.1863], 
<a name="l28557"><span class="ln">28557 </span></a>                 [ 0.2303, -1.1229, -0.1863]]]) 
<a name="l28558"><span class="ln">28558 </span></a>        &gt;&gt;&gt; torch.stack((x, x), dim=2) 
<a name="l28559"><span class="ln">28559 </span></a>        tensor([[[ 0.3367,  0.3367], 
<a name="l28560"><span class="ln">28560 </span></a>                 [ 0.1288,  0.1288], 
<a name="l28561"><span class="ln">28561 </span></a>                 [ 0.2345,  0.2345]], 
<a name="l28562"><span class="ln">28562 </span></a> 
<a name="l28563"><span class="ln">28563 </span></a>                [[ 0.2303,  0.2303], 
<a name="l28564"><span class="ln">28564 </span></a>                 [-1.1229, -1.1229], 
<a name="l28565"><span class="ln">28565 </span></a>                 [-0.1863, -0.1863]]]) 
<a name="l28566"><span class="ln">28566 </span></a>        &gt;&gt;&gt; torch.stack((x, x), dim=-1) 
<a name="l28567"><span class="ln">28567 </span></a>        tensor([[[ 0.3367,  0.3367], 
<a name="l28568"><span class="ln">28568 </span></a>                 [ 0.1288,  0.1288], 
<a name="l28569"><span class="ln">28569 </span></a>                 [ 0.2345,  0.2345]], 
<a name="l28570"><span class="ln">28570 </span></a> 
<a name="l28571"><span class="ln">28571 </span></a>                [[ 0.2303,  0.2303], 
<a name="l28572"><span class="ln">28572 </span></a>                 [-1.1229, -1.1229], 
<a name="l28573"><span class="ln">28573 </span></a>                 [-0.1863, -0.1863]]]) 
<a name="l28574"><span class="ln">28574 </span></a>    &quot;&quot;&quot;</span>
<a name="l28575"><span class="ln">28575 </span></a>
<a name="l28576"><span class="ln">28576 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28577"><span class="ln">28577 </span></a><span class="s2">def </span><span class="s1">std</span><span class="s3">(</span>
<a name="l28578"><span class="ln">28578 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28579"><span class="ln">28579 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l28580"><span class="ln">28580 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l28581"><span class="ln">28581 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l28582"><span class="ln">28582 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28583"><span class="ln">28583 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28584"><span class="ln">28584 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28585"><span class="ln">28585 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28586"><span class="ln">28586 </span></a>    std(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l28587"><span class="ln">28587 </span></a> 
<a name="l28588"><span class="ln">28588 </span></a>    Calculates the standard deviation over the dimensions specified by :attr:`dim`. 
<a name="l28589"><span class="ln">28589 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l28590"><span class="ln">28590 </span></a>    reduce over all dimensions. 
<a name="l28591"><span class="ln">28591 </span></a> 
<a name="l28592"><span class="ln">28592 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l28593"><span class="ln">28593 </span></a> 
<a name="l28594"><span class="ln">28594 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l28595"><span class="ln">28595 </span></a> 
<a name="l28596"><span class="ln">28596 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l28597"><span class="ln">28597 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l28598"><span class="ln">28598 </span></a>    the :attr:`correction`. 
<a name="l28599"><span class="ln">28599 </span></a> 
<a name="l28600"><span class="ln">28600 </span></a> 
<a name="l28601"><span class="ln">28601 </span></a> 
<a name="l28602"><span class="ln">28602 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l28603"><span class="ln">28603 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l28604"><span class="ln">28604 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l28605"><span class="ln">28605 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l28606"><span class="ln">28606 </span></a> 
<a name="l28607"><span class="ln">28607 </span></a> 
<a name="l28608"><span class="ln">28608 </span></a>    Args: 
<a name="l28609"><span class="ln">28609 </span></a>        input (Tensor): the input tensor. 
<a name="l28610"><span class="ln">28610 </span></a> 
<a name="l28611"><span class="ln">28611 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l28612"><span class="ln">28612 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l28613"><span class="ln">28613 </span></a> 
<a name="l28614"><span class="ln">28614 </span></a> 
<a name="l28615"><span class="ln">28615 </span></a>    Keyword args: 
<a name="l28616"><span class="ln">28616 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l28617"><span class="ln">28617 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l28618"><span class="ln">28618 </span></a> 
<a name="l28619"><span class="ln">28619 </span></a>            .. versionchanged:: 2.0 
<a name="l28620"><span class="ln">28620 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l28621"><span class="ln">28621 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l28622"><span class="ln">28622 </span></a>                ``correction=0``. 
<a name="l28623"><span class="ln">28623 </span></a> 
<a name="l28624"><span class="ln">28624 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l28625"><span class="ln">28625 </span></a> 
<a name="l28626"><span class="ln">28626 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28627"><span class="ln">28627 </span></a> 
<a name="l28628"><span class="ln">28628 </span></a>    Example: 
<a name="l28629"><span class="ln">28629 </span></a> 
<a name="l28630"><span class="ln">28630 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l28631"><span class="ln">28631 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l28632"><span class="ln">28632 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l28633"><span class="ln">28633 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l28634"><span class="ln">28634 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l28635"><span class="ln">28635 </span></a>        ... )  # fmt: skip 
<a name="l28636"><span class="ln">28636 </span></a>        &gt;&gt;&gt; torch.std(a, dim=1, keepdim=True) 
<a name="l28637"><span class="ln">28637 </span></a>        tensor([[1.0311], 
<a name="l28638"><span class="ln">28638 </span></a>                [0.7477], 
<a name="l28639"><span class="ln">28639 </span></a>                [1.2204], 
<a name="l28640"><span class="ln">28640 </span></a>                [0.9087]]) 
<a name="l28641"><span class="ln">28641 </span></a> 
<a name="l28642"><span class="ln">28642 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l28643"><span class="ln">28643 </span></a>    &quot;&quot;&quot;</span>
<a name="l28644"><span class="ln">28644 </span></a>
<a name="l28645"><span class="ln">28645 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28646"><span class="ln">28646 </span></a><span class="s2">def </span><span class="s1">std</span><span class="s3">(</span>
<a name="l28647"><span class="ln">28647 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28648"><span class="ln">28648 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28649"><span class="ln">28649 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28650"><span class="ln">28650 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28651"><span class="ln">28651 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l28652"><span class="ln">28652 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28653"><span class="ln">28653 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28654"><span class="ln">28654 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28655"><span class="ln">28655 </span></a>    std(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l28656"><span class="ln">28656 </span></a> 
<a name="l28657"><span class="ln">28657 </span></a>    Calculates the standard deviation over the dimensions specified by :attr:`dim`. 
<a name="l28658"><span class="ln">28658 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l28659"><span class="ln">28659 </span></a>    reduce over all dimensions. 
<a name="l28660"><span class="ln">28660 </span></a> 
<a name="l28661"><span class="ln">28661 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l28662"><span class="ln">28662 </span></a> 
<a name="l28663"><span class="ln">28663 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l28664"><span class="ln">28664 </span></a> 
<a name="l28665"><span class="ln">28665 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l28666"><span class="ln">28666 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l28667"><span class="ln">28667 </span></a>    the :attr:`correction`. 
<a name="l28668"><span class="ln">28668 </span></a> 
<a name="l28669"><span class="ln">28669 </span></a> 
<a name="l28670"><span class="ln">28670 </span></a> 
<a name="l28671"><span class="ln">28671 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l28672"><span class="ln">28672 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l28673"><span class="ln">28673 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l28674"><span class="ln">28674 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l28675"><span class="ln">28675 </span></a> 
<a name="l28676"><span class="ln">28676 </span></a> 
<a name="l28677"><span class="ln">28677 </span></a>    Args: 
<a name="l28678"><span class="ln">28678 </span></a>        input (Tensor): the input tensor. 
<a name="l28679"><span class="ln">28679 </span></a> 
<a name="l28680"><span class="ln">28680 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l28681"><span class="ln">28681 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l28682"><span class="ln">28682 </span></a> 
<a name="l28683"><span class="ln">28683 </span></a> 
<a name="l28684"><span class="ln">28684 </span></a>    Keyword args: 
<a name="l28685"><span class="ln">28685 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l28686"><span class="ln">28686 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l28687"><span class="ln">28687 </span></a> 
<a name="l28688"><span class="ln">28688 </span></a>            .. versionchanged:: 2.0 
<a name="l28689"><span class="ln">28689 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l28690"><span class="ln">28690 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l28691"><span class="ln">28691 </span></a>                ``correction=0``. 
<a name="l28692"><span class="ln">28692 </span></a> 
<a name="l28693"><span class="ln">28693 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l28694"><span class="ln">28694 </span></a> 
<a name="l28695"><span class="ln">28695 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28696"><span class="ln">28696 </span></a> 
<a name="l28697"><span class="ln">28697 </span></a>    Example: 
<a name="l28698"><span class="ln">28698 </span></a> 
<a name="l28699"><span class="ln">28699 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l28700"><span class="ln">28700 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l28701"><span class="ln">28701 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l28702"><span class="ln">28702 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l28703"><span class="ln">28703 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l28704"><span class="ln">28704 </span></a>        ... )  # fmt: skip 
<a name="l28705"><span class="ln">28705 </span></a>        &gt;&gt;&gt; torch.std(a, dim=1, keepdim=True) 
<a name="l28706"><span class="ln">28706 </span></a>        tensor([[1.0311], 
<a name="l28707"><span class="ln">28707 </span></a>                [0.7477], 
<a name="l28708"><span class="ln">28708 </span></a>                [1.2204], 
<a name="l28709"><span class="ln">28709 </span></a>                [0.9087]]) 
<a name="l28710"><span class="ln">28710 </span></a> 
<a name="l28711"><span class="ln">28711 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l28712"><span class="ln">28712 </span></a>    &quot;&quot;&quot;</span>
<a name="l28713"><span class="ln">28713 </span></a>
<a name="l28714"><span class="ln">28714 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28715"><span class="ln">28715 </span></a><span class="s2">def </span><span class="s1">std</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28716"><span class="ln">28716 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28717"><span class="ln">28717 </span></a>    std(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l28718"><span class="ln">28718 </span></a> 
<a name="l28719"><span class="ln">28719 </span></a>    Calculates the standard deviation over the dimensions specified by :attr:`dim`. 
<a name="l28720"><span class="ln">28720 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l28721"><span class="ln">28721 </span></a>    reduce over all dimensions. 
<a name="l28722"><span class="ln">28722 </span></a> 
<a name="l28723"><span class="ln">28723 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l28724"><span class="ln">28724 </span></a> 
<a name="l28725"><span class="ln">28725 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l28726"><span class="ln">28726 </span></a> 
<a name="l28727"><span class="ln">28727 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l28728"><span class="ln">28728 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l28729"><span class="ln">28729 </span></a>    the :attr:`correction`. 
<a name="l28730"><span class="ln">28730 </span></a> 
<a name="l28731"><span class="ln">28731 </span></a> 
<a name="l28732"><span class="ln">28732 </span></a> 
<a name="l28733"><span class="ln">28733 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l28734"><span class="ln">28734 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l28735"><span class="ln">28735 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l28736"><span class="ln">28736 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l28737"><span class="ln">28737 </span></a> 
<a name="l28738"><span class="ln">28738 </span></a> 
<a name="l28739"><span class="ln">28739 </span></a>    Args: 
<a name="l28740"><span class="ln">28740 </span></a>        input (Tensor): the input tensor. 
<a name="l28741"><span class="ln">28741 </span></a> 
<a name="l28742"><span class="ln">28742 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l28743"><span class="ln">28743 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l28744"><span class="ln">28744 </span></a> 
<a name="l28745"><span class="ln">28745 </span></a> 
<a name="l28746"><span class="ln">28746 </span></a>    Keyword args: 
<a name="l28747"><span class="ln">28747 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l28748"><span class="ln">28748 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l28749"><span class="ln">28749 </span></a> 
<a name="l28750"><span class="ln">28750 </span></a>            .. versionchanged:: 2.0 
<a name="l28751"><span class="ln">28751 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l28752"><span class="ln">28752 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l28753"><span class="ln">28753 </span></a>                ``correction=0``. 
<a name="l28754"><span class="ln">28754 </span></a> 
<a name="l28755"><span class="ln">28755 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l28756"><span class="ln">28756 </span></a> 
<a name="l28757"><span class="ln">28757 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28758"><span class="ln">28758 </span></a> 
<a name="l28759"><span class="ln">28759 </span></a>    Example: 
<a name="l28760"><span class="ln">28760 </span></a> 
<a name="l28761"><span class="ln">28761 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l28762"><span class="ln">28762 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l28763"><span class="ln">28763 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l28764"><span class="ln">28764 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l28765"><span class="ln">28765 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l28766"><span class="ln">28766 </span></a>        ... )  # fmt: skip 
<a name="l28767"><span class="ln">28767 </span></a>        &gt;&gt;&gt; torch.std(a, dim=1, keepdim=True) 
<a name="l28768"><span class="ln">28768 </span></a>        tensor([[1.0311], 
<a name="l28769"><span class="ln">28769 </span></a>                [0.7477], 
<a name="l28770"><span class="ln">28770 </span></a>                [1.2204], 
<a name="l28771"><span class="ln">28771 </span></a>                [0.9087]]) 
<a name="l28772"><span class="ln">28772 </span></a> 
<a name="l28773"><span class="ln">28773 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l28774"><span class="ln">28774 </span></a>    &quot;&quot;&quot;</span>
<a name="l28775"><span class="ln">28775 </span></a>
<a name="l28776"><span class="ln">28776 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28777"><span class="ln">28777 </span></a><span class="s2">def </span><span class="s1">std</span><span class="s3">(</span>
<a name="l28778"><span class="ln">28778 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28779"><span class="ln">28779 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l28780"><span class="ln">28780 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28781"><span class="ln">28781 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28782"><span class="ln">28782 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l28783"><span class="ln">28783 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28784"><span class="ln">28784 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28785"><span class="ln">28785 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28786"><span class="ln">28786 </span></a>    std(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l28787"><span class="ln">28787 </span></a> 
<a name="l28788"><span class="ln">28788 </span></a>    Calculates the standard deviation over the dimensions specified by :attr:`dim`. 
<a name="l28789"><span class="ln">28789 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l28790"><span class="ln">28790 </span></a>    reduce over all dimensions. 
<a name="l28791"><span class="ln">28791 </span></a> 
<a name="l28792"><span class="ln">28792 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l28793"><span class="ln">28793 </span></a> 
<a name="l28794"><span class="ln">28794 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l28795"><span class="ln">28795 </span></a> 
<a name="l28796"><span class="ln">28796 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l28797"><span class="ln">28797 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l28798"><span class="ln">28798 </span></a>    the :attr:`correction`. 
<a name="l28799"><span class="ln">28799 </span></a> 
<a name="l28800"><span class="ln">28800 </span></a> 
<a name="l28801"><span class="ln">28801 </span></a> 
<a name="l28802"><span class="ln">28802 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l28803"><span class="ln">28803 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l28804"><span class="ln">28804 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l28805"><span class="ln">28805 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l28806"><span class="ln">28806 </span></a> 
<a name="l28807"><span class="ln">28807 </span></a> 
<a name="l28808"><span class="ln">28808 </span></a>    Args: 
<a name="l28809"><span class="ln">28809 </span></a>        input (Tensor): the input tensor. 
<a name="l28810"><span class="ln">28810 </span></a> 
<a name="l28811"><span class="ln">28811 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l28812"><span class="ln">28812 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l28813"><span class="ln">28813 </span></a> 
<a name="l28814"><span class="ln">28814 </span></a> 
<a name="l28815"><span class="ln">28815 </span></a>    Keyword args: 
<a name="l28816"><span class="ln">28816 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l28817"><span class="ln">28817 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l28818"><span class="ln">28818 </span></a> 
<a name="l28819"><span class="ln">28819 </span></a>            .. versionchanged:: 2.0 
<a name="l28820"><span class="ln">28820 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l28821"><span class="ln">28821 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l28822"><span class="ln">28822 </span></a>                ``correction=0``. 
<a name="l28823"><span class="ln">28823 </span></a> 
<a name="l28824"><span class="ln">28824 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l28825"><span class="ln">28825 </span></a> 
<a name="l28826"><span class="ln">28826 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28827"><span class="ln">28827 </span></a> 
<a name="l28828"><span class="ln">28828 </span></a>    Example: 
<a name="l28829"><span class="ln">28829 </span></a> 
<a name="l28830"><span class="ln">28830 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l28831"><span class="ln">28831 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l28832"><span class="ln">28832 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l28833"><span class="ln">28833 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l28834"><span class="ln">28834 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l28835"><span class="ln">28835 </span></a>        ... )  # fmt: skip 
<a name="l28836"><span class="ln">28836 </span></a>        &gt;&gt;&gt; torch.std(a, dim=1, keepdim=True) 
<a name="l28837"><span class="ln">28837 </span></a>        tensor([[1.0311], 
<a name="l28838"><span class="ln">28838 </span></a>                [0.7477], 
<a name="l28839"><span class="ln">28839 </span></a>                [1.2204], 
<a name="l28840"><span class="ln">28840 </span></a>                [0.9087]]) 
<a name="l28841"><span class="ln">28841 </span></a> 
<a name="l28842"><span class="ln">28842 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l28843"><span class="ln">28843 </span></a>    &quot;&quot;&quot;</span>
<a name="l28844"><span class="ln">28844 </span></a>
<a name="l28845"><span class="ln">28845 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28846"><span class="ln">28846 </span></a><span class="s2">def </span><span class="s1">std</span><span class="s3">(</span>
<a name="l28847"><span class="ln">28847 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28848"><span class="ln">28848 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l28849"><span class="ln">28849 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l28850"><span class="ln">28850 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l28851"><span class="ln">28851 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28852"><span class="ln">28852 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28853"><span class="ln">28853 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l28854"><span class="ln">28854 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28855"><span class="ln">28855 </span></a>    std(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l28856"><span class="ln">28856 </span></a> 
<a name="l28857"><span class="ln">28857 </span></a>    Calculates the standard deviation over the dimensions specified by :attr:`dim`. 
<a name="l28858"><span class="ln">28858 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l28859"><span class="ln">28859 </span></a>    reduce over all dimensions. 
<a name="l28860"><span class="ln">28860 </span></a> 
<a name="l28861"><span class="ln">28861 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l28862"><span class="ln">28862 </span></a> 
<a name="l28863"><span class="ln">28863 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l28864"><span class="ln">28864 </span></a> 
<a name="l28865"><span class="ln">28865 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l28866"><span class="ln">28866 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l28867"><span class="ln">28867 </span></a>    the :attr:`correction`. 
<a name="l28868"><span class="ln">28868 </span></a> 
<a name="l28869"><span class="ln">28869 </span></a> 
<a name="l28870"><span class="ln">28870 </span></a> 
<a name="l28871"><span class="ln">28871 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l28872"><span class="ln">28872 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l28873"><span class="ln">28873 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l28874"><span class="ln">28874 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l28875"><span class="ln">28875 </span></a> 
<a name="l28876"><span class="ln">28876 </span></a> 
<a name="l28877"><span class="ln">28877 </span></a>    Args: 
<a name="l28878"><span class="ln">28878 </span></a>        input (Tensor): the input tensor. 
<a name="l28879"><span class="ln">28879 </span></a> 
<a name="l28880"><span class="ln">28880 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l28881"><span class="ln">28881 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l28882"><span class="ln">28882 </span></a> 
<a name="l28883"><span class="ln">28883 </span></a> 
<a name="l28884"><span class="ln">28884 </span></a>    Keyword args: 
<a name="l28885"><span class="ln">28885 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l28886"><span class="ln">28886 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l28887"><span class="ln">28887 </span></a> 
<a name="l28888"><span class="ln">28888 </span></a>            .. versionchanged:: 2.0 
<a name="l28889"><span class="ln">28889 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l28890"><span class="ln">28890 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l28891"><span class="ln">28891 </span></a>                ``correction=0``. 
<a name="l28892"><span class="ln">28892 </span></a> 
<a name="l28893"><span class="ln">28893 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l28894"><span class="ln">28894 </span></a> 
<a name="l28895"><span class="ln">28895 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28896"><span class="ln">28896 </span></a> 
<a name="l28897"><span class="ln">28897 </span></a>    Example: 
<a name="l28898"><span class="ln">28898 </span></a> 
<a name="l28899"><span class="ln">28899 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l28900"><span class="ln">28900 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l28901"><span class="ln">28901 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l28902"><span class="ln">28902 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l28903"><span class="ln">28903 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l28904"><span class="ln">28904 </span></a>        ... )  # fmt: skip 
<a name="l28905"><span class="ln">28905 </span></a>        &gt;&gt;&gt; torch.std(a, dim=1, keepdim=True) 
<a name="l28906"><span class="ln">28906 </span></a>        tensor([[1.0311], 
<a name="l28907"><span class="ln">28907 </span></a>                [0.7477], 
<a name="l28908"><span class="ln">28908 </span></a>                [1.2204], 
<a name="l28909"><span class="ln">28909 </span></a>                [0.9087]]) 
<a name="l28910"><span class="ln">28910 </span></a> 
<a name="l28911"><span class="ln">28911 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l28912"><span class="ln">28912 </span></a>    &quot;&quot;&quot;</span>
<a name="l28913"><span class="ln">28913 </span></a>
<a name="l28914"><span class="ln">28914 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28915"><span class="ln">28915 </span></a><span class="s2">def </span><span class="s1">std_mean</span><span class="s3">(</span>
<a name="l28916"><span class="ln">28916 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28917"><span class="ln">28917 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l28918"><span class="ln">28918 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l28919"><span class="ln">28919 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l28920"><span class="ln">28920 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l28921"><span class="ln">28921 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28922"><span class="ln">28922 </span></a>    std_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l28923"><span class="ln">28923 </span></a> 
<a name="l28924"><span class="ln">28924 </span></a>    Calculates the standard deviation and mean over the dimensions specified by 
<a name="l28925"><span class="ln">28925 </span></a>    :attr:`dim`. :attr:`dim` can be a single dimension, list of dimensions, or 
<a name="l28926"><span class="ln">28926 </span></a>    ``None`` to reduce over all dimensions. 
<a name="l28927"><span class="ln">28927 </span></a> 
<a name="l28928"><span class="ln">28928 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l28929"><span class="ln">28929 </span></a> 
<a name="l28930"><span class="ln">28930 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l28931"><span class="ln">28931 </span></a> 
<a name="l28932"><span class="ln">28932 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l28933"><span class="ln">28933 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l28934"><span class="ln">28934 </span></a>    the :attr:`correction`. 
<a name="l28935"><span class="ln">28935 </span></a> 
<a name="l28936"><span class="ln">28936 </span></a> 
<a name="l28937"><span class="ln">28937 </span></a> 
<a name="l28938"><span class="ln">28938 </span></a> 
<a name="l28939"><span class="ln">28939 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l28940"><span class="ln">28940 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l28941"><span class="ln">28941 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l28942"><span class="ln">28942 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l28943"><span class="ln">28943 </span></a> 
<a name="l28944"><span class="ln">28944 </span></a> 
<a name="l28945"><span class="ln">28945 </span></a>    Args: 
<a name="l28946"><span class="ln">28946 </span></a>        input (Tensor): the input tensor. 
<a name="l28947"><span class="ln">28947 </span></a> 
<a name="l28948"><span class="ln">28948 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l28949"><span class="ln">28949 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l28950"><span class="ln">28950 </span></a> 
<a name="l28951"><span class="ln">28951 </span></a> 
<a name="l28952"><span class="ln">28952 </span></a>    Keyword args: 
<a name="l28953"><span class="ln">28953 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l28954"><span class="ln">28954 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l28955"><span class="ln">28955 </span></a> 
<a name="l28956"><span class="ln">28956 </span></a>            .. versionchanged:: 2.0 
<a name="l28957"><span class="ln">28957 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l28958"><span class="ln">28958 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l28959"><span class="ln">28959 </span></a>                ``correction=0``. 
<a name="l28960"><span class="ln">28960 </span></a> 
<a name="l28961"><span class="ln">28961 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l28962"><span class="ln">28962 </span></a> 
<a name="l28963"><span class="ln">28963 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l28964"><span class="ln">28964 </span></a> 
<a name="l28965"><span class="ln">28965 </span></a>    Returns: 
<a name="l28966"><span class="ln">28966 </span></a>        A tuple (std, mean) containing the standard deviation and mean. 
<a name="l28967"><span class="ln">28967 </span></a> 
<a name="l28968"><span class="ln">28968 </span></a>    Example: 
<a name="l28969"><span class="ln">28969 </span></a> 
<a name="l28970"><span class="ln">28970 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l28971"><span class="ln">28971 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l28972"><span class="ln">28972 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l28973"><span class="ln">28973 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l28974"><span class="ln">28974 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l28975"><span class="ln">28975 </span></a>        ... )  # fmt: skip 
<a name="l28976"><span class="ln">28976 </span></a>        &gt;&gt;&gt; torch.std_mean(a, dim=0, keepdim=True) 
<a name="l28977"><span class="ln">28977 </span></a>        (tensor([[1.2620, 1.0028, 1.0957, 0.6038]]), 
<a name="l28978"><span class="ln">28978 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l28979"><span class="ln">28979 </span></a> 
<a name="l28980"><span class="ln">28980 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l28981"><span class="ln">28981 </span></a>    &quot;&quot;&quot;</span>
<a name="l28982"><span class="ln">28982 </span></a>
<a name="l28983"><span class="ln">28983 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l28984"><span class="ln">28984 </span></a><span class="s2">def </span><span class="s1">std_mean</span><span class="s3">(</span>
<a name="l28985"><span class="ln">28985 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l28986"><span class="ln">28986 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28987"><span class="ln">28987 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l28988"><span class="ln">28988 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l28989"><span class="ln">28989 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l28990"><span class="ln">28990 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l28991"><span class="ln">28991 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l28992"><span class="ln">28992 </span></a>    std_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l28993"><span class="ln">28993 </span></a> 
<a name="l28994"><span class="ln">28994 </span></a>    Calculates the standard deviation and mean over the dimensions specified by 
<a name="l28995"><span class="ln">28995 </span></a>    :attr:`dim`. :attr:`dim` can be a single dimension, list of dimensions, or 
<a name="l28996"><span class="ln">28996 </span></a>    ``None`` to reduce over all dimensions. 
<a name="l28997"><span class="ln">28997 </span></a> 
<a name="l28998"><span class="ln">28998 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l28999"><span class="ln">28999 </span></a> 
<a name="l29000"><span class="ln">29000 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l29001"><span class="ln">29001 </span></a> 
<a name="l29002"><span class="ln">29002 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l29003"><span class="ln">29003 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l29004"><span class="ln">29004 </span></a>    the :attr:`correction`. 
<a name="l29005"><span class="ln">29005 </span></a> 
<a name="l29006"><span class="ln">29006 </span></a> 
<a name="l29007"><span class="ln">29007 </span></a> 
<a name="l29008"><span class="ln">29008 </span></a> 
<a name="l29009"><span class="ln">29009 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l29010"><span class="ln">29010 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l29011"><span class="ln">29011 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l29012"><span class="ln">29012 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l29013"><span class="ln">29013 </span></a> 
<a name="l29014"><span class="ln">29014 </span></a> 
<a name="l29015"><span class="ln">29015 </span></a>    Args: 
<a name="l29016"><span class="ln">29016 </span></a>        input (Tensor): the input tensor. 
<a name="l29017"><span class="ln">29017 </span></a> 
<a name="l29018"><span class="ln">29018 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l29019"><span class="ln">29019 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l29020"><span class="ln">29020 </span></a> 
<a name="l29021"><span class="ln">29021 </span></a> 
<a name="l29022"><span class="ln">29022 </span></a>    Keyword args: 
<a name="l29023"><span class="ln">29023 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l29024"><span class="ln">29024 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l29025"><span class="ln">29025 </span></a> 
<a name="l29026"><span class="ln">29026 </span></a>            .. versionchanged:: 2.0 
<a name="l29027"><span class="ln">29027 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l29028"><span class="ln">29028 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l29029"><span class="ln">29029 </span></a>                ``correction=0``. 
<a name="l29030"><span class="ln">29030 </span></a> 
<a name="l29031"><span class="ln">29031 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l29032"><span class="ln">29032 </span></a> 
<a name="l29033"><span class="ln">29033 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29034"><span class="ln">29034 </span></a> 
<a name="l29035"><span class="ln">29035 </span></a>    Returns: 
<a name="l29036"><span class="ln">29036 </span></a>        A tuple (std, mean) containing the standard deviation and mean. 
<a name="l29037"><span class="ln">29037 </span></a> 
<a name="l29038"><span class="ln">29038 </span></a>    Example: 
<a name="l29039"><span class="ln">29039 </span></a> 
<a name="l29040"><span class="ln">29040 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l29041"><span class="ln">29041 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l29042"><span class="ln">29042 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l29043"><span class="ln">29043 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l29044"><span class="ln">29044 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l29045"><span class="ln">29045 </span></a>        ... )  # fmt: skip 
<a name="l29046"><span class="ln">29046 </span></a>        &gt;&gt;&gt; torch.std_mean(a, dim=0, keepdim=True) 
<a name="l29047"><span class="ln">29047 </span></a>        (tensor([[1.2620, 1.0028, 1.0957, 0.6038]]), 
<a name="l29048"><span class="ln">29048 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l29049"><span class="ln">29049 </span></a> 
<a name="l29050"><span class="ln">29050 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l29051"><span class="ln">29051 </span></a>    &quot;&quot;&quot;</span>
<a name="l29052"><span class="ln">29052 </span></a>
<a name="l29053"><span class="ln">29053 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29054"><span class="ln">29054 </span></a><span class="s2">def </span><span class="s1">std_mean</span><span class="s3">(</span>
<a name="l29055"><span class="ln">29055 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29056"><span class="ln">29056 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l29057"><span class="ln">29057 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l29058"><span class="ln">29058 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29059"><span class="ln">29059 </span></a>    std_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l29060"><span class="ln">29060 </span></a> 
<a name="l29061"><span class="ln">29061 </span></a>    Calculates the standard deviation and mean over the dimensions specified by 
<a name="l29062"><span class="ln">29062 </span></a>    :attr:`dim`. :attr:`dim` can be a single dimension, list of dimensions, or 
<a name="l29063"><span class="ln">29063 </span></a>    ``None`` to reduce over all dimensions. 
<a name="l29064"><span class="ln">29064 </span></a> 
<a name="l29065"><span class="ln">29065 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l29066"><span class="ln">29066 </span></a> 
<a name="l29067"><span class="ln">29067 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l29068"><span class="ln">29068 </span></a> 
<a name="l29069"><span class="ln">29069 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l29070"><span class="ln">29070 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l29071"><span class="ln">29071 </span></a>    the :attr:`correction`. 
<a name="l29072"><span class="ln">29072 </span></a> 
<a name="l29073"><span class="ln">29073 </span></a> 
<a name="l29074"><span class="ln">29074 </span></a> 
<a name="l29075"><span class="ln">29075 </span></a> 
<a name="l29076"><span class="ln">29076 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l29077"><span class="ln">29077 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l29078"><span class="ln">29078 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l29079"><span class="ln">29079 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l29080"><span class="ln">29080 </span></a> 
<a name="l29081"><span class="ln">29081 </span></a> 
<a name="l29082"><span class="ln">29082 </span></a>    Args: 
<a name="l29083"><span class="ln">29083 </span></a>        input (Tensor): the input tensor. 
<a name="l29084"><span class="ln">29084 </span></a> 
<a name="l29085"><span class="ln">29085 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l29086"><span class="ln">29086 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l29087"><span class="ln">29087 </span></a> 
<a name="l29088"><span class="ln">29088 </span></a> 
<a name="l29089"><span class="ln">29089 </span></a>    Keyword args: 
<a name="l29090"><span class="ln">29090 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l29091"><span class="ln">29091 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l29092"><span class="ln">29092 </span></a> 
<a name="l29093"><span class="ln">29093 </span></a>            .. versionchanged:: 2.0 
<a name="l29094"><span class="ln">29094 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l29095"><span class="ln">29095 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l29096"><span class="ln">29096 </span></a>                ``correction=0``. 
<a name="l29097"><span class="ln">29097 </span></a> 
<a name="l29098"><span class="ln">29098 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l29099"><span class="ln">29099 </span></a> 
<a name="l29100"><span class="ln">29100 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29101"><span class="ln">29101 </span></a> 
<a name="l29102"><span class="ln">29102 </span></a>    Returns: 
<a name="l29103"><span class="ln">29103 </span></a>        A tuple (std, mean) containing the standard deviation and mean. 
<a name="l29104"><span class="ln">29104 </span></a> 
<a name="l29105"><span class="ln">29105 </span></a>    Example: 
<a name="l29106"><span class="ln">29106 </span></a> 
<a name="l29107"><span class="ln">29107 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l29108"><span class="ln">29108 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l29109"><span class="ln">29109 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l29110"><span class="ln">29110 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l29111"><span class="ln">29111 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l29112"><span class="ln">29112 </span></a>        ... )  # fmt: skip 
<a name="l29113"><span class="ln">29113 </span></a>        &gt;&gt;&gt; torch.std_mean(a, dim=0, keepdim=True) 
<a name="l29114"><span class="ln">29114 </span></a>        (tensor([[1.2620, 1.0028, 1.0957, 0.6038]]), 
<a name="l29115"><span class="ln">29115 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l29116"><span class="ln">29116 </span></a> 
<a name="l29117"><span class="ln">29117 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l29118"><span class="ln">29118 </span></a>    &quot;&quot;&quot;</span>
<a name="l29119"><span class="ln">29119 </span></a>
<a name="l29120"><span class="ln">29120 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29121"><span class="ln">29121 </span></a><span class="s2">def </span><span class="s1">std_mean</span><span class="s3">(</span>
<a name="l29122"><span class="ln">29122 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29123"><span class="ln">29123 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l29124"><span class="ln">29124 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29125"><span class="ln">29125 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29126"><span class="ln">29126 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l29127"><span class="ln">29127 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l29128"><span class="ln">29128 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29129"><span class="ln">29129 </span></a>    std_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l29130"><span class="ln">29130 </span></a> 
<a name="l29131"><span class="ln">29131 </span></a>    Calculates the standard deviation and mean over the dimensions specified by 
<a name="l29132"><span class="ln">29132 </span></a>    :attr:`dim`. :attr:`dim` can be a single dimension, list of dimensions, or 
<a name="l29133"><span class="ln">29133 </span></a>    ``None`` to reduce over all dimensions. 
<a name="l29134"><span class="ln">29134 </span></a> 
<a name="l29135"><span class="ln">29135 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l29136"><span class="ln">29136 </span></a> 
<a name="l29137"><span class="ln">29137 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l29138"><span class="ln">29138 </span></a> 
<a name="l29139"><span class="ln">29139 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l29140"><span class="ln">29140 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l29141"><span class="ln">29141 </span></a>    the :attr:`correction`. 
<a name="l29142"><span class="ln">29142 </span></a> 
<a name="l29143"><span class="ln">29143 </span></a> 
<a name="l29144"><span class="ln">29144 </span></a> 
<a name="l29145"><span class="ln">29145 </span></a> 
<a name="l29146"><span class="ln">29146 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l29147"><span class="ln">29147 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l29148"><span class="ln">29148 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l29149"><span class="ln">29149 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l29150"><span class="ln">29150 </span></a> 
<a name="l29151"><span class="ln">29151 </span></a> 
<a name="l29152"><span class="ln">29152 </span></a>    Args: 
<a name="l29153"><span class="ln">29153 </span></a>        input (Tensor): the input tensor. 
<a name="l29154"><span class="ln">29154 </span></a> 
<a name="l29155"><span class="ln">29155 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l29156"><span class="ln">29156 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l29157"><span class="ln">29157 </span></a> 
<a name="l29158"><span class="ln">29158 </span></a> 
<a name="l29159"><span class="ln">29159 </span></a>    Keyword args: 
<a name="l29160"><span class="ln">29160 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l29161"><span class="ln">29161 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l29162"><span class="ln">29162 </span></a> 
<a name="l29163"><span class="ln">29163 </span></a>            .. versionchanged:: 2.0 
<a name="l29164"><span class="ln">29164 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l29165"><span class="ln">29165 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l29166"><span class="ln">29166 </span></a>                ``correction=0``. 
<a name="l29167"><span class="ln">29167 </span></a> 
<a name="l29168"><span class="ln">29168 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l29169"><span class="ln">29169 </span></a> 
<a name="l29170"><span class="ln">29170 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29171"><span class="ln">29171 </span></a> 
<a name="l29172"><span class="ln">29172 </span></a>    Returns: 
<a name="l29173"><span class="ln">29173 </span></a>        A tuple (std, mean) containing the standard deviation and mean. 
<a name="l29174"><span class="ln">29174 </span></a> 
<a name="l29175"><span class="ln">29175 </span></a>    Example: 
<a name="l29176"><span class="ln">29176 </span></a> 
<a name="l29177"><span class="ln">29177 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l29178"><span class="ln">29178 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l29179"><span class="ln">29179 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l29180"><span class="ln">29180 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l29181"><span class="ln">29181 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l29182"><span class="ln">29182 </span></a>        ... )  # fmt: skip 
<a name="l29183"><span class="ln">29183 </span></a>        &gt;&gt;&gt; torch.std_mean(a, dim=0, keepdim=True) 
<a name="l29184"><span class="ln">29184 </span></a>        (tensor([[1.2620, 1.0028, 1.0957, 0.6038]]), 
<a name="l29185"><span class="ln">29185 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l29186"><span class="ln">29186 </span></a> 
<a name="l29187"><span class="ln">29187 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l29188"><span class="ln">29188 </span></a>    &quot;&quot;&quot;</span>
<a name="l29189"><span class="ln">29189 </span></a>
<a name="l29190"><span class="ln">29190 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29191"><span class="ln">29191 </span></a><span class="s2">def </span><span class="s1">std_mean</span><span class="s3">(</span>
<a name="l29192"><span class="ln">29192 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29193"><span class="ln">29193 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l29194"><span class="ln">29194 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l29195"><span class="ln">29195 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l29196"><span class="ln">29196 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l29197"><span class="ln">29197 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29198"><span class="ln">29198 </span></a>    std_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l29199"><span class="ln">29199 </span></a> 
<a name="l29200"><span class="ln">29200 </span></a>    Calculates the standard deviation and mean over the dimensions specified by 
<a name="l29201"><span class="ln">29201 </span></a>    :attr:`dim`. :attr:`dim` can be a single dimension, list of dimensions, or 
<a name="l29202"><span class="ln">29202 </span></a>    ``None`` to reduce over all dimensions. 
<a name="l29203"><span class="ln">29203 </span></a> 
<a name="l29204"><span class="ln">29204 </span></a>    The standard deviation (:math:`\sigma`) is calculated as 
<a name="l29205"><span class="ln">29205 </span></a> 
<a name="l29206"><span class="ln">29206 </span></a>    .. math:: \sigma = \sqrt{\frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2} 
<a name="l29207"><span class="ln">29207 </span></a> 
<a name="l29208"><span class="ln">29208 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l29209"><span class="ln">29209 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l29210"><span class="ln">29210 </span></a>    the :attr:`correction`. 
<a name="l29211"><span class="ln">29211 </span></a> 
<a name="l29212"><span class="ln">29212 </span></a> 
<a name="l29213"><span class="ln">29213 </span></a> 
<a name="l29214"><span class="ln">29214 </span></a> 
<a name="l29215"><span class="ln">29215 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l29216"><span class="ln">29216 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l29217"><span class="ln">29217 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l29218"><span class="ln">29218 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l29219"><span class="ln">29219 </span></a> 
<a name="l29220"><span class="ln">29220 </span></a> 
<a name="l29221"><span class="ln">29221 </span></a>    Args: 
<a name="l29222"><span class="ln">29222 </span></a>        input (Tensor): the input tensor. 
<a name="l29223"><span class="ln">29223 </span></a> 
<a name="l29224"><span class="ln">29224 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l29225"><span class="ln">29225 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l29226"><span class="ln">29226 </span></a> 
<a name="l29227"><span class="ln">29227 </span></a> 
<a name="l29228"><span class="ln">29228 </span></a>    Keyword args: 
<a name="l29229"><span class="ln">29229 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l29230"><span class="ln">29230 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l29231"><span class="ln">29231 </span></a> 
<a name="l29232"><span class="ln">29232 </span></a>            .. versionchanged:: 2.0 
<a name="l29233"><span class="ln">29233 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l29234"><span class="ln">29234 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l29235"><span class="ln">29235 </span></a>                ``correction=0``. 
<a name="l29236"><span class="ln">29236 </span></a> 
<a name="l29237"><span class="ln">29237 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l29238"><span class="ln">29238 </span></a> 
<a name="l29239"><span class="ln">29239 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29240"><span class="ln">29240 </span></a> 
<a name="l29241"><span class="ln">29241 </span></a>    Returns: 
<a name="l29242"><span class="ln">29242 </span></a>        A tuple (std, mean) containing the standard deviation and mean. 
<a name="l29243"><span class="ln">29243 </span></a> 
<a name="l29244"><span class="ln">29244 </span></a>    Example: 
<a name="l29245"><span class="ln">29245 </span></a> 
<a name="l29246"><span class="ln">29246 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l29247"><span class="ln">29247 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l29248"><span class="ln">29248 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l29249"><span class="ln">29249 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l29250"><span class="ln">29250 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l29251"><span class="ln">29251 </span></a>        ... )  # fmt: skip 
<a name="l29252"><span class="ln">29252 </span></a>        &gt;&gt;&gt; torch.std_mean(a, dim=0, keepdim=True) 
<a name="l29253"><span class="ln">29253 </span></a>        (tensor([[1.2620, 1.0028, 1.0957, 0.6038]]), 
<a name="l29254"><span class="ln">29254 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l29255"><span class="ln">29255 </span></a> 
<a name="l29256"><span class="ln">29256 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l29257"><span class="ln">29257 </span></a>    &quot;&quot;&quot;</span>
<a name="l29258"><span class="ln">29258 </span></a>
<a name="l29259"><span class="ln">29259 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29260"><span class="ln">29260 </span></a><span class="s2">def </span><span class="s1">sub</span><span class="s3">(</span>
<a name="l29261"><span class="ln">29261 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l29262"><span class="ln">29262 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l29263"><span class="ln">29263 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29264"><span class="ln">29264 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = </span><span class="s5">1</span><span class="s3">,</span>
<a name="l29265"><span class="ln">29265 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29266"><span class="ln">29266 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29267"><span class="ln">29267 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29268"><span class="ln">29268 </span></a>    sub(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l29269"><span class="ln">29269 </span></a> 
<a name="l29270"><span class="ln">29270 </span></a>    Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`. 
<a name="l29271"><span class="ln">29271 </span></a> 
<a name="l29272"><span class="ln">29272 </span></a>    .. math:: 
<a name="l29273"><span class="ln">29273 </span></a>        \text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i 
<a name="l29274"><span class="ln">29274 </span></a> 
<a name="l29275"><span class="ln">29275 </span></a> 
<a name="l29276"><span class="ln">29276 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l29277"><span class="ln">29277 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l29278"><span class="ln">29278 </span></a> 
<a name="l29279"><span class="ln">29279 </span></a>    Args: 
<a name="l29280"><span class="ln">29280 </span></a>        input (Tensor): the input tensor. 
<a name="l29281"><span class="ln">29281 </span></a>        other (Tensor or Number): the tensor or number to subtract from :attr:`input`. 
<a name="l29282"><span class="ln">29282 </span></a> 
<a name="l29283"><span class="ln">29283 </span></a>    Keyword args: 
<a name="l29284"><span class="ln">29284 </span></a>        alpha (Number): the multiplier for :attr:`other`. 
<a name="l29285"><span class="ln">29285 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29286"><span class="ln">29286 </span></a> 
<a name="l29287"><span class="ln">29287 </span></a>    Example:: 
<a name="l29288"><span class="ln">29288 </span></a> 
<a name="l29289"><span class="ln">29289 </span></a>        &gt;&gt;&gt; a = torch.tensor((1, 2)) 
<a name="l29290"><span class="ln">29290 </span></a>        &gt;&gt;&gt; b = torch.tensor((0, 1)) 
<a name="l29291"><span class="ln">29291 </span></a>        &gt;&gt;&gt; torch.sub(a, b, alpha=2) 
<a name="l29292"><span class="ln">29292 </span></a>        tensor([1, 0]) 
<a name="l29293"><span class="ln">29293 </span></a>    &quot;&quot;&quot;</span>
<a name="l29294"><span class="ln">29294 </span></a>
<a name="l29295"><span class="ln">29295 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29296"><span class="ln">29296 </span></a><span class="s2">def </span><span class="s1">sub</span><span class="s3">(</span><span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29297"><span class="ln">29297 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29298"><span class="ln">29298 </span></a>    sub(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l29299"><span class="ln">29299 </span></a> 
<a name="l29300"><span class="ln">29300 </span></a>    Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`. 
<a name="l29301"><span class="ln">29301 </span></a> 
<a name="l29302"><span class="ln">29302 </span></a>    .. math:: 
<a name="l29303"><span class="ln">29303 </span></a>        \text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i 
<a name="l29304"><span class="ln">29304 </span></a> 
<a name="l29305"><span class="ln">29305 </span></a> 
<a name="l29306"><span class="ln">29306 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l29307"><span class="ln">29307 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l29308"><span class="ln">29308 </span></a> 
<a name="l29309"><span class="ln">29309 </span></a>    Args: 
<a name="l29310"><span class="ln">29310 </span></a>        input (Tensor): the input tensor. 
<a name="l29311"><span class="ln">29311 </span></a>        other (Tensor or Number): the tensor or number to subtract from :attr:`input`. 
<a name="l29312"><span class="ln">29312 </span></a> 
<a name="l29313"><span class="ln">29313 </span></a>    Keyword args: 
<a name="l29314"><span class="ln">29314 </span></a>        alpha (Number): the multiplier for :attr:`other`. 
<a name="l29315"><span class="ln">29315 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29316"><span class="ln">29316 </span></a> 
<a name="l29317"><span class="ln">29317 </span></a>    Example:: 
<a name="l29318"><span class="ln">29318 </span></a> 
<a name="l29319"><span class="ln">29319 </span></a>        &gt;&gt;&gt; a = torch.tensor((1, 2)) 
<a name="l29320"><span class="ln">29320 </span></a>        &gt;&gt;&gt; b = torch.tensor((0, 1)) 
<a name="l29321"><span class="ln">29321 </span></a>        &gt;&gt;&gt; torch.sub(a, b, alpha=2) 
<a name="l29322"><span class="ln">29322 </span></a>        tensor([1, 0]) 
<a name="l29323"><span class="ln">29323 </span></a>    &quot;&quot;&quot;</span>
<a name="l29324"><span class="ln">29324 </span></a>
<a name="l29325"><span class="ln">29325 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29326"><span class="ln">29326 </span></a><span class="s2">def </span><span class="s1">sub</span><span class="s3">(</span>
<a name="l29327"><span class="ln">29327 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29328"><span class="ln">29328 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l29329"><span class="ln">29329 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29330"><span class="ln">29330 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29331"><span class="ln">29331 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29332"><span class="ln">29332 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29333"><span class="ln">29333 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29334"><span class="ln">29334 </span></a>    sub(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l29335"><span class="ln">29335 </span></a> 
<a name="l29336"><span class="ln">29336 </span></a>    Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`. 
<a name="l29337"><span class="ln">29337 </span></a> 
<a name="l29338"><span class="ln">29338 </span></a>    .. math:: 
<a name="l29339"><span class="ln">29339 </span></a>        \text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i 
<a name="l29340"><span class="ln">29340 </span></a> 
<a name="l29341"><span class="ln">29341 </span></a> 
<a name="l29342"><span class="ln">29342 </span></a>    Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`, 
<a name="l29343"><span class="ln">29343 </span></a>    :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs. 
<a name="l29344"><span class="ln">29344 </span></a> 
<a name="l29345"><span class="ln">29345 </span></a>    Args: 
<a name="l29346"><span class="ln">29346 </span></a>        input (Tensor): the input tensor. 
<a name="l29347"><span class="ln">29347 </span></a>        other (Tensor or Number): the tensor or number to subtract from :attr:`input`. 
<a name="l29348"><span class="ln">29348 </span></a> 
<a name="l29349"><span class="ln">29349 </span></a>    Keyword args: 
<a name="l29350"><span class="ln">29350 </span></a>        alpha (Number): the multiplier for :attr:`other`. 
<a name="l29351"><span class="ln">29351 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29352"><span class="ln">29352 </span></a> 
<a name="l29353"><span class="ln">29353 </span></a>    Example:: 
<a name="l29354"><span class="ln">29354 </span></a> 
<a name="l29355"><span class="ln">29355 </span></a>        &gt;&gt;&gt; a = torch.tensor((1, 2)) 
<a name="l29356"><span class="ln">29356 </span></a>        &gt;&gt;&gt; b = torch.tensor((0, 1)) 
<a name="l29357"><span class="ln">29357 </span></a>        &gt;&gt;&gt; torch.sub(a, b, alpha=2) 
<a name="l29358"><span class="ln">29358 </span></a>        tensor([1, 0]) 
<a name="l29359"><span class="ln">29359 </span></a>    &quot;&quot;&quot;</span>
<a name="l29360"><span class="ln">29360 </span></a>
<a name="l29361"><span class="ln">29361 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29362"><span class="ln">29362 </span></a><span class="s2">def </span><span class="s1">subtract</span><span class="s3">(</span>
<a name="l29363"><span class="ln">29363 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29364"><span class="ln">29364 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29365"><span class="ln">29365 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29366"><span class="ln">29366 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l29367"><span class="ln">29367 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29368"><span class="ln">29368 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29369"><span class="ln">29369 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29370"><span class="ln">29370 </span></a>    subtract(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l29371"><span class="ln">29371 </span></a> 
<a name="l29372"><span class="ln">29372 </span></a>    Alias for :func:`torch.sub`. 
<a name="l29373"><span class="ln">29373 </span></a>    &quot;&quot;&quot;</span>
<a name="l29374"><span class="ln">29374 </span></a>
<a name="l29375"><span class="ln">29375 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29376"><span class="ln">29376 </span></a><span class="s2">def </span><span class="s1">subtract</span><span class="s3">(</span>
<a name="l29377"><span class="ln">29377 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29378"><span class="ln">29378 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l29379"><span class="ln">29379 </span></a>    <span class="s1">alpha</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l29380"><span class="ln">29380 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29381"><span class="ln">29381 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29382"><span class="ln">29382 </span></a>    subtract(input, other, *, alpha=1, out=None) -&gt; Tensor 
<a name="l29383"><span class="ln">29383 </span></a> 
<a name="l29384"><span class="ln">29384 </span></a>    Alias for :func:`torch.sub`. 
<a name="l29385"><span class="ln">29385 </span></a>    &quot;&quot;&quot;</span>
<a name="l29386"><span class="ln">29386 </span></a>
<a name="l29387"><span class="ln">29387 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29388"><span class="ln">29388 </span></a><span class="s2">def </span><span class="s1">sum</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29389"><span class="ln">29389 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29390"><span class="ln">29390 </span></a>    sum(input, *, dtype=None) -&gt; Tensor 
<a name="l29391"><span class="ln">29391 </span></a> 
<a name="l29392"><span class="ln">29392 </span></a>    Returns the sum of all elements in the :attr:`input` tensor. 
<a name="l29393"><span class="ln">29393 </span></a> 
<a name="l29394"><span class="ln">29394 </span></a>    Args: 
<a name="l29395"><span class="ln">29395 </span></a>        input (Tensor): the input tensor. 
<a name="l29396"><span class="ln">29396 </span></a> 
<a name="l29397"><span class="ln">29397 </span></a>    Keyword args: 
<a name="l29398"><span class="ln">29398 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l29399"><span class="ln">29399 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l29400"><span class="ln">29400 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l29401"><span class="ln">29401 </span></a> 
<a name="l29402"><span class="ln">29402 </span></a>    .. note:: Use the `dtype` argument if you need the result in a specific tensor type. 
<a name="l29403"><span class="ln">29403 </span></a>              Otherwise, the result type may be automatically promoted (e.g., from `torch.int32` to `torch.int64`). 
<a name="l29404"><span class="ln">29404 </span></a> 
<a name="l29405"><span class="ln">29405 </span></a>    Example:: 
<a name="l29406"><span class="ln">29406 </span></a> 
<a name="l29407"><span class="ln">29407 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l29408"><span class="ln">29408 </span></a>        &gt;&gt;&gt; a 
<a name="l29409"><span class="ln">29409 </span></a>        tensor([[ 0.1133, -0.9567,  0.2958]]) 
<a name="l29410"><span class="ln">29410 </span></a>        &gt;&gt;&gt; torch.sum(a) 
<a name="l29411"><span class="ln">29411 </span></a>        tensor(-0.5475) 
<a name="l29412"><span class="ln">29412 </span></a> 
<a name="l29413"><span class="ln">29413 </span></a>    .. function:: sum(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l29414"><span class="ln">29414 </span></a>       :noindex: 
<a name="l29415"><span class="ln">29415 </span></a> 
<a name="l29416"><span class="ln">29416 </span></a>    Returns the sum of each row of the :attr:`input` tensor in the given 
<a name="l29417"><span class="ln">29417 </span></a>    dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l29418"><span class="ln">29418 </span></a>    reduce over all of them. 
<a name="l29419"><span class="ln">29419 </span></a> 
<a name="l29420"><span class="ln">29420 </span></a> 
<a name="l29421"><span class="ln">29421 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l29422"><span class="ln">29422 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l29423"><span class="ln">29423 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l29424"><span class="ln">29424 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l29425"><span class="ln">29425 </span></a> 
<a name="l29426"><span class="ln">29426 </span></a> 
<a name="l29427"><span class="ln">29427 </span></a>    Args: 
<a name="l29428"><span class="ln">29428 </span></a>        input (Tensor): the input tensor. 
<a name="l29429"><span class="ln">29429 </span></a> 
<a name="l29430"><span class="ln">29430 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l29431"><span class="ln">29431 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l29432"><span class="ln">29432 </span></a> 
<a name="l29433"><span class="ln">29433 </span></a> 
<a name="l29434"><span class="ln">29434 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l29435"><span class="ln">29435 </span></a> 
<a name="l29436"><span class="ln">29436 </span></a> 
<a name="l29437"><span class="ln">29437 </span></a>    Keyword args: 
<a name="l29438"><span class="ln">29438 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l29439"><span class="ln">29439 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l29440"><span class="ln">29440 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l29441"><span class="ln">29441 </span></a> 
<a name="l29442"><span class="ln">29442 </span></a>    Example:: 
<a name="l29443"><span class="ln">29443 </span></a> 
<a name="l29444"><span class="ln">29444 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l29445"><span class="ln">29445 </span></a>        &gt;&gt;&gt; a 
<a name="l29446"><span class="ln">29446 </span></a>        tensor([[ 0.0569, -0.2475,  0.0737, -0.3429], 
<a name="l29447"><span class="ln">29447 </span></a>                [-0.2993,  0.9138,  0.9337, -1.6864], 
<a name="l29448"><span class="ln">29448 </span></a>                [ 0.1132,  0.7892, -0.1003,  0.5688], 
<a name="l29449"><span class="ln">29449 </span></a>                [ 0.3637, -0.9906, -0.4752, -1.5197]]) 
<a name="l29450"><span class="ln">29450 </span></a>        &gt;&gt;&gt; torch.sum(a, 1) 
<a name="l29451"><span class="ln">29451 </span></a>        tensor([-0.4598, -0.1381,  1.3708, -2.6217]) 
<a name="l29452"><span class="ln">29452 </span></a>        &gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6) 
<a name="l29453"><span class="ln">29453 </span></a>        &gt;&gt;&gt; torch.sum(b, (2, 1)) 
<a name="l29454"><span class="ln">29454 </span></a>        tensor([  435.,  1335.,  2235.,  3135.]) 
<a name="l29455"><span class="ln">29455 </span></a>    &quot;&quot;&quot;</span>
<a name="l29456"><span class="ln">29456 </span></a>
<a name="l29457"><span class="ln">29457 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29458"><span class="ln">29458 </span></a><span class="s2">def </span><span class="s1">sum</span><span class="s3">(</span>
<a name="l29459"><span class="ln">29459 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29460"><span class="ln">29460 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l29461"><span class="ln">29461 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l29462"><span class="ln">29462 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29463"><span class="ln">29463 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29464"><span class="ln">29464 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29465"><span class="ln">29465 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29466"><span class="ln">29466 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29467"><span class="ln">29467 </span></a>    sum(input, *, dtype=None) -&gt; Tensor 
<a name="l29468"><span class="ln">29468 </span></a> 
<a name="l29469"><span class="ln">29469 </span></a>    Returns the sum of all elements in the :attr:`input` tensor. 
<a name="l29470"><span class="ln">29470 </span></a> 
<a name="l29471"><span class="ln">29471 </span></a>    Args: 
<a name="l29472"><span class="ln">29472 </span></a>        input (Tensor): the input tensor. 
<a name="l29473"><span class="ln">29473 </span></a> 
<a name="l29474"><span class="ln">29474 </span></a>    Keyword args: 
<a name="l29475"><span class="ln">29475 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l29476"><span class="ln">29476 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l29477"><span class="ln">29477 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l29478"><span class="ln">29478 </span></a> 
<a name="l29479"><span class="ln">29479 </span></a>    .. note:: Use the `dtype` argument if you need the result in a specific tensor type. 
<a name="l29480"><span class="ln">29480 </span></a>              Otherwise, the result type may be automatically promoted (e.g., from `torch.int32` to `torch.int64`). 
<a name="l29481"><span class="ln">29481 </span></a> 
<a name="l29482"><span class="ln">29482 </span></a>    Example:: 
<a name="l29483"><span class="ln">29483 </span></a> 
<a name="l29484"><span class="ln">29484 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l29485"><span class="ln">29485 </span></a>        &gt;&gt;&gt; a 
<a name="l29486"><span class="ln">29486 </span></a>        tensor([[ 0.1133, -0.9567,  0.2958]]) 
<a name="l29487"><span class="ln">29487 </span></a>        &gt;&gt;&gt; torch.sum(a) 
<a name="l29488"><span class="ln">29488 </span></a>        tensor(-0.5475) 
<a name="l29489"><span class="ln">29489 </span></a> 
<a name="l29490"><span class="ln">29490 </span></a>    .. function:: sum(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l29491"><span class="ln">29491 </span></a>       :noindex: 
<a name="l29492"><span class="ln">29492 </span></a> 
<a name="l29493"><span class="ln">29493 </span></a>    Returns the sum of each row of the :attr:`input` tensor in the given 
<a name="l29494"><span class="ln">29494 </span></a>    dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l29495"><span class="ln">29495 </span></a>    reduce over all of them. 
<a name="l29496"><span class="ln">29496 </span></a> 
<a name="l29497"><span class="ln">29497 </span></a> 
<a name="l29498"><span class="ln">29498 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l29499"><span class="ln">29499 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l29500"><span class="ln">29500 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l29501"><span class="ln">29501 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l29502"><span class="ln">29502 </span></a> 
<a name="l29503"><span class="ln">29503 </span></a> 
<a name="l29504"><span class="ln">29504 </span></a>    Args: 
<a name="l29505"><span class="ln">29505 </span></a>        input (Tensor): the input tensor. 
<a name="l29506"><span class="ln">29506 </span></a> 
<a name="l29507"><span class="ln">29507 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l29508"><span class="ln">29508 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l29509"><span class="ln">29509 </span></a> 
<a name="l29510"><span class="ln">29510 </span></a> 
<a name="l29511"><span class="ln">29511 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l29512"><span class="ln">29512 </span></a> 
<a name="l29513"><span class="ln">29513 </span></a> 
<a name="l29514"><span class="ln">29514 </span></a>    Keyword args: 
<a name="l29515"><span class="ln">29515 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l29516"><span class="ln">29516 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l29517"><span class="ln">29517 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l29518"><span class="ln">29518 </span></a> 
<a name="l29519"><span class="ln">29519 </span></a>    Example:: 
<a name="l29520"><span class="ln">29520 </span></a> 
<a name="l29521"><span class="ln">29521 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l29522"><span class="ln">29522 </span></a>        &gt;&gt;&gt; a 
<a name="l29523"><span class="ln">29523 </span></a>        tensor([[ 0.0569, -0.2475,  0.0737, -0.3429], 
<a name="l29524"><span class="ln">29524 </span></a>                [-0.2993,  0.9138,  0.9337, -1.6864], 
<a name="l29525"><span class="ln">29525 </span></a>                [ 0.1132,  0.7892, -0.1003,  0.5688], 
<a name="l29526"><span class="ln">29526 </span></a>                [ 0.3637, -0.9906, -0.4752, -1.5197]]) 
<a name="l29527"><span class="ln">29527 </span></a>        &gt;&gt;&gt; torch.sum(a, 1) 
<a name="l29528"><span class="ln">29528 </span></a>        tensor([-0.4598, -0.1381,  1.3708, -2.6217]) 
<a name="l29529"><span class="ln">29529 </span></a>        &gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6) 
<a name="l29530"><span class="ln">29530 </span></a>        &gt;&gt;&gt; torch.sum(b, (2, 1)) 
<a name="l29531"><span class="ln">29531 </span></a>        tensor([  435.,  1335.,  2235.,  3135.]) 
<a name="l29532"><span class="ln">29532 </span></a>    &quot;&quot;&quot;</span>
<a name="l29533"><span class="ln">29533 </span></a>
<a name="l29534"><span class="ln">29534 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l29535"><span class="ln">29535 </span></a><span class="s2">def </span><span class="s1">sum</span><span class="s3">(</span>
<a name="l29536"><span class="ln">29536 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29537"><span class="ln">29537 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l29538"><span class="ln">29538 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l29539"><span class="ln">29539 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29540"><span class="ln">29540 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29541"><span class="ln">29541 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29542"><span class="ln">29542 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29543"><span class="ln">29543 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29544"><span class="ln">29544 </span></a>    sum(input, *, dtype=None) -&gt; Tensor 
<a name="l29545"><span class="ln">29545 </span></a> 
<a name="l29546"><span class="ln">29546 </span></a>    Returns the sum of all elements in the :attr:`input` tensor. 
<a name="l29547"><span class="ln">29547 </span></a> 
<a name="l29548"><span class="ln">29548 </span></a>    Args: 
<a name="l29549"><span class="ln">29549 </span></a>        input (Tensor): the input tensor. 
<a name="l29550"><span class="ln">29550 </span></a> 
<a name="l29551"><span class="ln">29551 </span></a>    Keyword args: 
<a name="l29552"><span class="ln">29552 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l29553"><span class="ln">29553 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l29554"><span class="ln">29554 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l29555"><span class="ln">29555 </span></a> 
<a name="l29556"><span class="ln">29556 </span></a>    .. note:: Use the `dtype` argument if you need the result in a specific tensor type. 
<a name="l29557"><span class="ln">29557 </span></a>              Otherwise, the result type may be automatically promoted (e.g., from `torch.int32` to `torch.int64`). 
<a name="l29558"><span class="ln">29558 </span></a> 
<a name="l29559"><span class="ln">29559 </span></a>    Example:: 
<a name="l29560"><span class="ln">29560 </span></a> 
<a name="l29561"><span class="ln">29561 </span></a>        &gt;&gt;&gt; a = torch.randn(1, 3) 
<a name="l29562"><span class="ln">29562 </span></a>        &gt;&gt;&gt; a 
<a name="l29563"><span class="ln">29563 </span></a>        tensor([[ 0.1133, -0.9567,  0.2958]]) 
<a name="l29564"><span class="ln">29564 </span></a>        &gt;&gt;&gt; torch.sum(a) 
<a name="l29565"><span class="ln">29565 </span></a>        tensor(-0.5475) 
<a name="l29566"><span class="ln">29566 </span></a> 
<a name="l29567"><span class="ln">29567 </span></a>    .. function:: sum(input, dim, keepdim=False, *, dtype=None) -&gt; Tensor 
<a name="l29568"><span class="ln">29568 </span></a>       :noindex: 
<a name="l29569"><span class="ln">29569 </span></a> 
<a name="l29570"><span class="ln">29570 </span></a>    Returns the sum of each row of the :attr:`input` tensor in the given 
<a name="l29571"><span class="ln">29571 </span></a>    dimension :attr:`dim`. If :attr:`dim` is a list of dimensions, 
<a name="l29572"><span class="ln">29572 </span></a>    reduce over all of them. 
<a name="l29573"><span class="ln">29573 </span></a> 
<a name="l29574"><span class="ln">29574 </span></a> 
<a name="l29575"><span class="ln">29575 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l29576"><span class="ln">29576 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l29577"><span class="ln">29577 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l29578"><span class="ln">29578 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l29579"><span class="ln">29579 </span></a> 
<a name="l29580"><span class="ln">29580 </span></a> 
<a name="l29581"><span class="ln">29581 </span></a>    Args: 
<a name="l29582"><span class="ln">29582 </span></a>        input (Tensor): the input tensor. 
<a name="l29583"><span class="ln">29583 </span></a> 
<a name="l29584"><span class="ln">29584 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l29585"><span class="ln">29585 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l29586"><span class="ln">29586 </span></a> 
<a name="l29587"><span class="ln">29587 </span></a> 
<a name="l29588"><span class="ln">29588 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l29589"><span class="ln">29589 </span></a> 
<a name="l29590"><span class="ln">29590 </span></a> 
<a name="l29591"><span class="ln">29591 </span></a>    Keyword args: 
<a name="l29592"><span class="ln">29592 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l29593"><span class="ln">29593 </span></a>            If specified, the input tensor is casted to :attr:`dtype` before the operation 
<a name="l29594"><span class="ln">29594 </span></a>            is performed. This is useful for preventing data type overflows. Default: None. 
<a name="l29595"><span class="ln">29595 </span></a> 
<a name="l29596"><span class="ln">29596 </span></a>    Example:: 
<a name="l29597"><span class="ln">29597 </span></a> 
<a name="l29598"><span class="ln">29598 </span></a>        &gt;&gt;&gt; a = torch.randn(4, 4) 
<a name="l29599"><span class="ln">29599 </span></a>        &gt;&gt;&gt; a 
<a name="l29600"><span class="ln">29600 </span></a>        tensor([[ 0.0569, -0.2475,  0.0737, -0.3429], 
<a name="l29601"><span class="ln">29601 </span></a>                [-0.2993,  0.9138,  0.9337, -1.6864], 
<a name="l29602"><span class="ln">29602 </span></a>                [ 0.1132,  0.7892, -0.1003,  0.5688], 
<a name="l29603"><span class="ln">29603 </span></a>                [ 0.3637, -0.9906, -0.4752, -1.5197]]) 
<a name="l29604"><span class="ln">29604 </span></a>        &gt;&gt;&gt; torch.sum(a, 1) 
<a name="l29605"><span class="ln">29605 </span></a>        tensor([-0.4598, -0.1381,  1.3708, -2.6217]) 
<a name="l29606"><span class="ln">29606 </span></a>        &gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6) 
<a name="l29607"><span class="ln">29607 </span></a>        &gt;&gt;&gt; torch.sum(b, (2, 1)) 
<a name="l29608"><span class="ln">29608 </span></a>        tensor([  435.,  1335.,  2235.,  3135.]) 
<a name="l29609"><span class="ln">29609 </span></a>    &quot;&quot;&quot;</span>
<a name="l29610"><span class="ln">29610 </span></a>
<a name="l29611"><span class="ln">29611 </span></a><span class="s2">def </span><span class="s1">svd</span><span class="s3">(</span>
<a name="l29612"><span class="ln">29612 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29613"><span class="ln">29613 </span></a>    <span class="s1">some</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l29614"><span class="ln">29614 </span></a>    <span class="s1">compute_uv</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l29615"><span class="ln">29615 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29616"><span class="ln">29616 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29617"><span class="ln">29617 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">svd</span><span class="s2">:</span>
<a name="l29618"><span class="ln">29618 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29619"><span class="ln">29619 </span></a>    svd(input, some=True, compute_uv=True, *, out=None) -&gt; (Tensor, Tensor, Tensor) 
<a name="l29620"><span class="ln">29620 </span></a> 
<a name="l29621"><span class="ln">29621 </span></a>    Computes the singular value decomposition of either a matrix or batch of 
<a name="l29622"><span class="ln">29622 </span></a>    matrices :attr:`input`. The singular value decomposition is represented as a 
<a name="l29623"><span class="ln">29623 </span></a>    namedtuple `(U, S, V)`, such that :attr:`input` :math:`= U \text{diag}(S) V^{\text{H}}`. 
<a name="l29624"><span class="ln">29624 </span></a>    where :math:`V^{\text{H}}` is the transpose of `V` for real inputs, 
<a name="l29625"><span class="ln">29625 </span></a>    and the conjugate transpose of `V` for complex inputs. 
<a name="l29626"><span class="ln">29626 </span></a>    If :attr:`input` is a batch of matrices, then `U`, `S`, and `V` are also 
<a name="l29627"><span class="ln">29627 </span></a>    batched with the same batch dimensions as :attr:`input`. 
<a name="l29628"><span class="ln">29628 </span></a> 
<a name="l29629"><span class="ln">29629 </span></a>    If :attr:`some` is `True` (default), the method returns the reduced singular 
<a name="l29630"><span class="ln">29630 </span></a>    value decomposition. In this case, if the last two dimensions of :attr:`input` are 
<a name="l29631"><span class="ln">29631 </span></a>    `m` and `n`, then the returned `U` and `V` matrices will contain only 
<a name="l29632"><span class="ln">29632 </span></a>    `min(n, m)` orthonormal columns. 
<a name="l29633"><span class="ln">29633 </span></a> 
<a name="l29634"><span class="ln">29634 </span></a>    If :attr:`compute_uv` is `False`, the returned `U` and `V` will be 
<a name="l29635"><span class="ln">29635 </span></a>    zero-filled matrices of shape `(m, m)` and `(n, n)` 
<a name="l29636"><span class="ln">29636 </span></a>    respectively, and the same device as :attr:`input`. The argument :attr:`some` 
<a name="l29637"><span class="ln">29637 </span></a>    has no effect when :attr:`compute_uv` is `False`. 
<a name="l29638"><span class="ln">29638 </span></a> 
<a name="l29639"><span class="ln">29639 </span></a>    Supports :attr:`input` of float, double, cfloat and cdouble data types. 
<a name="l29640"><span class="ln">29640 </span></a>    The dtypes of `U` and `V` are the same as :attr:`input`'s. `S` will 
<a name="l29641"><span class="ln">29641 </span></a>    always be real-valued, even if :attr:`input` is complex. 
<a name="l29642"><span class="ln">29642 </span></a> 
<a name="l29643"><span class="ln">29643 </span></a>    .. warning:: 
<a name="l29644"><span class="ln">29644 </span></a> 
<a name="l29645"><span class="ln">29645 </span></a>        :func:`torch.svd` is deprecated in favor of :func:`torch.linalg.svd` 
<a name="l29646"><span class="ln">29646 </span></a>        and will be removed in a future PyTorch release. 
<a name="l29647"><span class="ln">29647 </span></a> 
<a name="l29648"><span class="ln">29648 </span></a>        ``U, S, V = torch.svd(A, some=some, compute_uv=True)`` (default) should be replaced with 
<a name="l29649"><span class="ln">29649 </span></a> 
<a name="l29650"><span class="ln">29650 </span></a>        .. code:: python 
<a name="l29651"><span class="ln">29651 </span></a> 
<a name="l29652"><span class="ln">29652 </span></a>            U, S, Vh = torch.linalg.svd(A, full_matrices=not some) 
<a name="l29653"><span class="ln">29653 </span></a>            V = Vh.mH 
<a name="l29654"><span class="ln">29654 </span></a> 
<a name="l29655"><span class="ln">29655 </span></a>        ``_, S, _ = torch.svd(A, some=some, compute_uv=False)`` should be replaced with 
<a name="l29656"><span class="ln">29656 </span></a> 
<a name="l29657"><span class="ln">29657 </span></a>        .. code:: python 
<a name="l29658"><span class="ln">29658 </span></a> 
<a name="l29659"><span class="ln">29659 </span></a>            S = torch.linalg.svdvals(A) 
<a name="l29660"><span class="ln">29660 </span></a> 
<a name="l29661"><span class="ln">29661 </span></a>    .. note:: Differences with :func:`torch.linalg.svd`: 
<a name="l29662"><span class="ln">29662 </span></a> 
<a name="l29663"><span class="ln">29663 </span></a>                 * :attr:`some` is the opposite of 
<a name="l29664"><span class="ln">29664 </span></a>                   :func:`torch.linalg.svd`'s :attr:`full_matrices`. Note that 
<a name="l29665"><span class="ln">29665 </span></a>                   default value for both is `True`, so the default behavior is 
<a name="l29666"><span class="ln">29666 </span></a>                   effectively the opposite. 
<a name="l29667"><span class="ln">29667 </span></a>                 * :func:`torch.svd` returns `V`, whereas :func:`torch.linalg.svd` returns 
<a name="l29668"><span class="ln">29668 </span></a>                   `Vh`, that is, :math:`V^{\text{H}}`. 
<a name="l29669"><span class="ln">29669 </span></a>                 * If :attr:`compute_uv` is `False`, :func:`torch.svd` returns zero-filled 
<a name="l29670"><span class="ln">29670 </span></a>                   tensors for `U` and `Vh`, whereas :func:`torch.linalg.svd` returns 
<a name="l29671"><span class="ln">29671 </span></a>                   empty tensors. 
<a name="l29672"><span class="ln">29672 </span></a> 
<a name="l29673"><span class="ln">29673 </span></a>    .. note:: The singular values are returned in descending order. If :attr:`input` is a batch of matrices, 
<a name="l29674"><span class="ln">29674 </span></a>              then the singular values of each matrix in the batch are returned in descending order. 
<a name="l29675"><span class="ln">29675 </span></a> 
<a name="l29676"><span class="ln">29676 </span></a>    .. note:: The `S` tensor can only be used to compute gradients if :attr:`compute_uv` is `True`. 
<a name="l29677"><span class="ln">29677 </span></a> 
<a name="l29678"><span class="ln">29678 </span></a>    .. note:: When :attr:`some` is `False`, the gradients on `U[..., :, min(m, n):]` 
<a name="l29679"><span class="ln">29679 </span></a>              and `V[..., :, min(m, n):]` will be ignored in the backward pass, as those vectors 
<a name="l29680"><span class="ln">29680 </span></a>              can be arbitrary bases of the corresponding subspaces. 
<a name="l29681"><span class="ln">29681 </span></a> 
<a name="l29682"><span class="ln">29682 </span></a>    .. note:: The implementation of :func:`torch.linalg.svd` on CPU uses LAPACK's routine `?gesdd` 
<a name="l29683"><span class="ln">29683 </span></a>              (a divide-and-conquer algorithm) instead of `?gesvd` for speed. Analogously, 
<a name="l29684"><span class="ln">29684 </span></a>              on GPU, it uses cuSOLVER's routines `gesvdj` and `gesvdjBatched` on CUDA 10.1.243 
<a name="l29685"><span class="ln">29685 </span></a>              and later, and MAGMA's routine `gesdd` on earlier versions of CUDA. 
<a name="l29686"><span class="ln">29686 </span></a> 
<a name="l29687"><span class="ln">29687 </span></a>    .. note:: The returned `U` will not be contiguous. The matrix (or batch of matrices) will 
<a name="l29688"><span class="ln">29688 </span></a>              be represented as a column-major matrix (i.e. Fortran-contiguous). 
<a name="l29689"><span class="ln">29689 </span></a> 
<a name="l29690"><span class="ln">29690 </span></a>    .. warning:: The gradients with respect to `U` and `V` will only be finite when the input does not 
<a name="l29691"><span class="ln">29691 </span></a>                 have zero nor repeated singular values. 
<a name="l29692"><span class="ln">29692 </span></a> 
<a name="l29693"><span class="ln">29693 </span></a>    .. warning:: If the distance between any two singular values is close to zero, the gradients with respect to 
<a name="l29694"><span class="ln">29694 </span></a>                 `U` and `V` will be numerically unstable, as they depends on 
<a name="l29695"><span class="ln">29695 </span></a>                 :math:`\frac{1}{\min_{i \neq j} \sigma_i^2 - \sigma_j^2}`. The same happens when the matrix 
<a name="l29696"><span class="ln">29696 </span></a>                 has small singular values, as these gradients also depend on `S^{-1}`. 
<a name="l29697"><span class="ln">29697 </span></a> 
<a name="l29698"><span class="ln">29698 </span></a>    .. warning:: For complex-valued :attr:`input` the singular value decomposition is not unique, 
<a name="l29699"><span class="ln">29699 </span></a>                 as `U` and `V` may be multiplied by an arbitrary phase factor :math:`e^{i \phi}` on every column. 
<a name="l29700"><span class="ln">29700 </span></a>                 The same happens when :attr:`input` has repeated singular values, where one may multiply 
<a name="l29701"><span class="ln">29701 </span></a>                 the columns of the spanning subspace in `U` and `V` by a rotation matrix 
<a name="l29702"><span class="ln">29702 </span></a>                 and `the resulting vectors will span the same subspace`_. 
<a name="l29703"><span class="ln">29703 </span></a>                 Different platforms, like NumPy, or inputs on different device types, 
<a name="l29704"><span class="ln">29704 </span></a>                 may produce different `U` and `V` tensors. 
<a name="l29705"><span class="ln">29705 </span></a> 
<a name="l29706"><span class="ln">29706 </span></a>    Args: 
<a name="l29707"><span class="ln">29707 </span></a>        input (Tensor): the input tensor of size `(*, m, n)` where `*` is zero or more 
<a name="l29708"><span class="ln">29708 </span></a>                        batch dimensions consisting of `(m, n)` matrices. 
<a name="l29709"><span class="ln">29709 </span></a>        some (bool, optional): controls whether to compute the reduced or full decomposition, and 
<a name="l29710"><span class="ln">29710 </span></a>                               consequently, the shape of returned `U` and `V`. Default: `True`. 
<a name="l29711"><span class="ln">29711 </span></a>        compute_uv (bool, optional): controls whether to compute `U` and `V`. Default: `True`. 
<a name="l29712"><span class="ln">29712 </span></a> 
<a name="l29713"><span class="ln">29713 </span></a>    Keyword args: 
<a name="l29714"><span class="ln">29714 </span></a>        out (tuple, optional): the output tuple of tensors 
<a name="l29715"><span class="ln">29715 </span></a> 
<a name="l29716"><span class="ln">29716 </span></a>    Example:: 
<a name="l29717"><span class="ln">29717 </span></a> 
<a name="l29718"><span class="ln">29718 </span></a>        &gt;&gt;&gt; a = torch.randn(5, 3) 
<a name="l29719"><span class="ln">29719 </span></a>        &gt;&gt;&gt; a 
<a name="l29720"><span class="ln">29720 </span></a>        tensor([[ 0.2364, -0.7752,  0.6372], 
<a name="l29721"><span class="ln">29721 </span></a>                [ 1.7201,  0.7394, -0.0504], 
<a name="l29722"><span class="ln">29722 </span></a>                [-0.3371, -1.0584,  0.5296], 
<a name="l29723"><span class="ln">29723 </span></a>                [ 0.3550, -0.4022,  1.5569], 
<a name="l29724"><span class="ln">29724 </span></a>                [ 0.2445, -0.0158,  1.1414]]) 
<a name="l29725"><span class="ln">29725 </span></a>        &gt;&gt;&gt; u, s, v = torch.svd(a) 
<a name="l29726"><span class="ln">29726 </span></a>        &gt;&gt;&gt; u 
<a name="l29727"><span class="ln">29727 </span></a>        tensor([[ 0.4027,  0.0287,  0.5434], 
<a name="l29728"><span class="ln">29728 </span></a>                [-0.1946,  0.8833,  0.3679], 
<a name="l29729"><span class="ln">29729 </span></a>                [ 0.4296, -0.2890,  0.5261], 
<a name="l29730"><span class="ln">29730 </span></a>                [ 0.6604,  0.2717, -0.2618], 
<a name="l29731"><span class="ln">29731 </span></a>                [ 0.4234,  0.2481, -0.4733]]) 
<a name="l29732"><span class="ln">29732 </span></a>        &gt;&gt;&gt; s 
<a name="l29733"><span class="ln">29733 </span></a>        tensor([2.3289, 2.0315, 0.7806]) 
<a name="l29734"><span class="ln">29734 </span></a>        &gt;&gt;&gt; v 
<a name="l29735"><span class="ln">29735 </span></a>        tensor([[-0.0199,  0.8766,  0.4809], 
<a name="l29736"><span class="ln">29736 </span></a>                [-0.5080,  0.4054, -0.7600], 
<a name="l29737"><span class="ln">29737 </span></a>                [ 0.8611,  0.2594, -0.4373]]) 
<a name="l29738"><span class="ln">29738 </span></a>        &gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) 
<a name="l29739"><span class="ln">29739 </span></a>        tensor(8.6531e-07) 
<a name="l29740"><span class="ln">29740 </span></a>        &gt;&gt;&gt; a_big = torch.randn(7, 5, 3) 
<a name="l29741"><span class="ln">29741 </span></a>        &gt;&gt;&gt; u, s, v = torch.svd(a_big) 
<a name="l29742"><span class="ln">29742 </span></a>        &gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.mT)) 
<a name="l29743"><span class="ln">29743 </span></a>        tensor(2.6503e-06) 
<a name="l29744"><span class="ln">29744 </span></a> 
<a name="l29745"><span class="ln">29745 </span></a>    .. _the resulting vectors will span the same subspace: 
<a name="l29746"><span class="ln">29746 </span></a>           (https://en.wikipedia.org/wiki/Singular_value_decomposition#Singular_values,_singular_vectors,_and_their_relation_to_the_SVD) 
<a name="l29747"><span class="ln">29747 </span></a>    &quot;&quot;&quot;</span>
<a name="l29748"><span class="ln">29748 </span></a>
<a name="l29749"><span class="ln">29749 </span></a><span class="s2">def </span><span class="s1">swapaxes</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">axis0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">axis1</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29750"><span class="ln">29750 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29751"><span class="ln">29751 </span></a>    swapaxes(input, axis0, axis1) -&gt; Tensor 
<a name="l29752"><span class="ln">29752 </span></a> 
<a name="l29753"><span class="ln">29753 </span></a>    Alias for :func:`torch.transpose`. 
<a name="l29754"><span class="ln">29754 </span></a> 
<a name="l29755"><span class="ln">29755 </span></a>    This function is equivalent to NumPy's swapaxes function. 
<a name="l29756"><span class="ln">29756 </span></a> 
<a name="l29757"><span class="ln">29757 </span></a>    Examples:: 
<a name="l29758"><span class="ln">29758 </span></a> 
<a name="l29759"><span class="ln">29759 </span></a>        &gt;&gt;&gt; x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]]) 
<a name="l29760"><span class="ln">29760 </span></a>        &gt;&gt;&gt; x 
<a name="l29761"><span class="ln">29761 </span></a>        tensor([[[0, 1], 
<a name="l29762"><span class="ln">29762 </span></a>                [2, 3]], 
<a name="l29763"><span class="ln">29763 </span></a> 
<a name="l29764"><span class="ln">29764 </span></a>                [[4, 5], 
<a name="l29765"><span class="ln">29765 </span></a>                [6, 7]]]) 
<a name="l29766"><span class="ln">29766 </span></a>        &gt;&gt;&gt; torch.swapaxes(x, 0, 1) 
<a name="l29767"><span class="ln">29767 </span></a>        tensor([[[0, 1], 
<a name="l29768"><span class="ln">29768 </span></a>                [4, 5]], 
<a name="l29769"><span class="ln">29769 </span></a> 
<a name="l29770"><span class="ln">29770 </span></a>                [[2, 3], 
<a name="l29771"><span class="ln">29771 </span></a>                [6, 7]]]) 
<a name="l29772"><span class="ln">29772 </span></a>        &gt;&gt;&gt; torch.swapaxes(x, 0, 2) 
<a name="l29773"><span class="ln">29773 </span></a>        tensor([[[0, 4], 
<a name="l29774"><span class="ln">29774 </span></a>                [2, 6]], 
<a name="l29775"><span class="ln">29775 </span></a> 
<a name="l29776"><span class="ln">29776 </span></a>                [[1, 5], 
<a name="l29777"><span class="ln">29777 </span></a>                [3, 7]]]) 
<a name="l29778"><span class="ln">29778 </span></a>    &quot;&quot;&quot;</span>
<a name="l29779"><span class="ln">29779 </span></a>
<a name="l29780"><span class="ln">29780 </span></a><span class="s2">def </span><span class="s1">swapdims</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29781"><span class="ln">29781 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29782"><span class="ln">29782 </span></a>    swapdims(input, dim0, dim1) -&gt; Tensor 
<a name="l29783"><span class="ln">29783 </span></a> 
<a name="l29784"><span class="ln">29784 </span></a>    Alias for :func:`torch.transpose`. 
<a name="l29785"><span class="ln">29785 </span></a> 
<a name="l29786"><span class="ln">29786 </span></a>    This function is equivalent to NumPy's swapaxes function. 
<a name="l29787"><span class="ln">29787 </span></a> 
<a name="l29788"><span class="ln">29788 </span></a>    Examples:: 
<a name="l29789"><span class="ln">29789 </span></a> 
<a name="l29790"><span class="ln">29790 </span></a>        &gt;&gt;&gt; x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]]) 
<a name="l29791"><span class="ln">29791 </span></a>        &gt;&gt;&gt; x 
<a name="l29792"><span class="ln">29792 </span></a>        tensor([[[0, 1], 
<a name="l29793"><span class="ln">29793 </span></a>                [2, 3]], 
<a name="l29794"><span class="ln">29794 </span></a> 
<a name="l29795"><span class="ln">29795 </span></a>                [[4, 5], 
<a name="l29796"><span class="ln">29796 </span></a>                [6, 7]]]) 
<a name="l29797"><span class="ln">29797 </span></a>        &gt;&gt;&gt; torch.swapdims(x, 0, 1) 
<a name="l29798"><span class="ln">29798 </span></a>        tensor([[[0, 1], 
<a name="l29799"><span class="ln">29799 </span></a>                [4, 5]], 
<a name="l29800"><span class="ln">29800 </span></a> 
<a name="l29801"><span class="ln">29801 </span></a>                [[2, 3], 
<a name="l29802"><span class="ln">29802 </span></a>                [6, 7]]]) 
<a name="l29803"><span class="ln">29803 </span></a>        &gt;&gt;&gt; torch.swapdims(x, 0, 2) 
<a name="l29804"><span class="ln">29804 </span></a>        tensor([[[0, 4], 
<a name="l29805"><span class="ln">29805 </span></a>                [2, 6]], 
<a name="l29806"><span class="ln">29806 </span></a> 
<a name="l29807"><span class="ln">29807 </span></a>                [[1, 5], 
<a name="l29808"><span class="ln">29808 </span></a>                [3, 7]]]) 
<a name="l29809"><span class="ln">29809 </span></a>    &quot;&quot;&quot;</span>
<a name="l29810"><span class="ln">29810 </span></a>
<a name="l29811"><span class="ln">29811 </span></a><span class="s2">def </span><span class="s1">sym_constrain_range</span><span class="s3">(</span>
<a name="l29812"><span class="ln">29812 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l29813"><span class="ln">29813 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29814"><span class="ln">29814 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29815"><span class="ln">29815 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29816"><span class="ln">29816 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l29817"><span class="ln">29817 </span></a><span class="s2">def </span><span class="s1">sym_constrain_range_for_size</span><span class="s3">(</span>
<a name="l29818"><span class="ln">29818 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l29819"><span class="ln">29819 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29820"><span class="ln">29820 </span></a>    <span class="s1">min</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29821"><span class="ln">29821 </span></a>    <span class="s1">max</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29822"><span class="ln">29822 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None: </span><span class="s3">...</span>
<a name="l29823"><span class="ln">29823 </span></a><span class="s2">def </span><span class="s1">t</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29824"><span class="ln">29824 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29825"><span class="ln">29825 </span></a>    t(input) -&gt; Tensor 
<a name="l29826"><span class="ln">29826 </span></a> 
<a name="l29827"><span class="ln">29827 </span></a>    Expects :attr:`input` to be &lt;= 2-D tensor and transposes dimensions 0 
<a name="l29828"><span class="ln">29828 </span></a>    and 1. 
<a name="l29829"><span class="ln">29829 </span></a> 
<a name="l29830"><span class="ln">29830 </span></a>    0-D and 1-D tensors are returned as is. When input is a 2-D tensor this 
<a name="l29831"><span class="ln">29831 </span></a>    is equivalent to ``transpose(input, 0, 1)``. 
<a name="l29832"><span class="ln">29832 </span></a> 
<a name="l29833"><span class="ln">29833 </span></a>    Args: 
<a name="l29834"><span class="ln">29834 </span></a>        input (Tensor): the input tensor. 
<a name="l29835"><span class="ln">29835 </span></a> 
<a name="l29836"><span class="ln">29836 </span></a>    Example:: 
<a name="l29837"><span class="ln">29837 </span></a> 
<a name="l29838"><span class="ln">29838 </span></a>        &gt;&gt;&gt; x = torch.randn(()) 
<a name="l29839"><span class="ln">29839 </span></a>        &gt;&gt;&gt; x 
<a name="l29840"><span class="ln">29840 </span></a>        tensor(0.1995) 
<a name="l29841"><span class="ln">29841 </span></a>        &gt;&gt;&gt; torch.t(x) 
<a name="l29842"><span class="ln">29842 </span></a>        tensor(0.1995) 
<a name="l29843"><span class="ln">29843 </span></a>        &gt;&gt;&gt; x = torch.randn(3) 
<a name="l29844"><span class="ln">29844 </span></a>        &gt;&gt;&gt; x 
<a name="l29845"><span class="ln">29845 </span></a>        tensor([ 2.4320, -0.4608,  0.7702]) 
<a name="l29846"><span class="ln">29846 </span></a>        &gt;&gt;&gt; torch.t(x) 
<a name="l29847"><span class="ln">29847 </span></a>        tensor([ 2.4320, -0.4608,  0.7702]) 
<a name="l29848"><span class="ln">29848 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l29849"><span class="ln">29849 </span></a>        &gt;&gt;&gt; x 
<a name="l29850"><span class="ln">29850 </span></a>        tensor([[ 0.4875,  0.9158, -0.5872], 
<a name="l29851"><span class="ln">29851 </span></a>                [ 0.3938, -0.6929,  0.6932]]) 
<a name="l29852"><span class="ln">29852 </span></a>        &gt;&gt;&gt; torch.t(x) 
<a name="l29853"><span class="ln">29853 </span></a>        tensor([[ 0.4875,  0.3938], 
<a name="l29854"><span class="ln">29854 </span></a>                [ 0.9158, -0.6929], 
<a name="l29855"><span class="ln">29855 </span></a>                [-0.5872,  0.6932]]) 
<a name="l29856"><span class="ln">29856 </span></a> 
<a name="l29857"><span class="ln">29857 </span></a>    See also :func:`torch.transpose`. 
<a name="l29858"><span class="ln">29858 </span></a>    &quot;&quot;&quot;</span>
<a name="l29859"><span class="ln">29859 </span></a>
<a name="l29860"><span class="ln">29860 </span></a><span class="s2">def </span><span class="s1">t_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29861"><span class="ln">29861 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29862"><span class="ln">29862 </span></a>    Performs the same operation as :func:`torch.t`, but all output tensors 
<a name="l29863"><span class="ln">29863 </span></a>    are freshly created instead of aliasing the input. 
<a name="l29864"><span class="ln">29864 </span></a>    &quot;&quot;&quot;</span>
<a name="l29865"><span class="ln">29865 </span></a>
<a name="l29866"><span class="ln">29866 </span></a><span class="s2">def </span><span class="s1">take</span><span class="s3">(</span>
<a name="l29867"><span class="ln">29867 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29868"><span class="ln">29868 </span></a>    <span class="s1">index</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29869"><span class="ln">29869 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29870"><span class="ln">29870 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29871"><span class="ln">29871 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29872"><span class="ln">29872 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29873"><span class="ln">29873 </span></a>    take(input, index) -&gt; Tensor 
<a name="l29874"><span class="ln">29874 </span></a> 
<a name="l29875"><span class="ln">29875 </span></a>    Returns a new tensor with the elements of :attr:`input` at the given indices. 
<a name="l29876"><span class="ln">29876 </span></a>    The input tensor is treated as if it were viewed as a 1-D tensor. The result 
<a name="l29877"><span class="ln">29877 </span></a>    takes the same shape as the indices. 
<a name="l29878"><span class="ln">29878 </span></a> 
<a name="l29879"><span class="ln">29879 </span></a>    Args: 
<a name="l29880"><span class="ln">29880 </span></a>        input (Tensor): the input tensor. 
<a name="l29881"><span class="ln">29881 </span></a>        index (LongTensor): the indices into tensor 
<a name="l29882"><span class="ln">29882 </span></a> 
<a name="l29883"><span class="ln">29883 </span></a>    Example:: 
<a name="l29884"><span class="ln">29884 </span></a> 
<a name="l29885"><span class="ln">29885 </span></a>        &gt;&gt;&gt; src = torch.tensor([[4, 3, 5], 
<a name="l29886"><span class="ln">29886 </span></a>        ...                     [6, 7, 8]]) 
<a name="l29887"><span class="ln">29887 </span></a>        &gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5])) 
<a name="l29888"><span class="ln">29888 </span></a>        tensor([ 4,  5,  8]) 
<a name="l29889"><span class="ln">29889 </span></a>    &quot;&quot;&quot;</span>
<a name="l29890"><span class="ln">29890 </span></a>
<a name="l29891"><span class="ln">29891 </span></a><span class="s2">def </span><span class="s1">take_along_dim</span><span class="s3">(</span>
<a name="l29892"><span class="ln">29892 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29893"><span class="ln">29893 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l29894"><span class="ln">29894 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29895"><span class="ln">29895 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l29896"><span class="ln">29896 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29897"><span class="ln">29897 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29898"><span class="ln">29898 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29899"><span class="ln">29899 </span></a>    take_along_dim(input, indices, dim=None, *, out=None) -&gt; Tensor 
<a name="l29900"><span class="ln">29900 </span></a> 
<a name="l29901"><span class="ln">29901 </span></a>    Selects values from :attr:`input` at the 1-dimensional indices from :attr:`indices` along the given :attr:`dim`. 
<a name="l29902"><span class="ln">29902 </span></a> 
<a name="l29903"><span class="ln">29903 </span></a>    If :attr:`dim` is None, the input array is treated as if it has been flattened to 1d. 
<a name="l29904"><span class="ln">29904 </span></a> 
<a name="l29905"><span class="ln">29905 </span></a>    Functions that return indices along a dimension, like :func:`torch.argmax` and :func:`torch.argsort`, 
<a name="l29906"><span class="ln">29906 </span></a>    are designed to work with this function. See the examples below. 
<a name="l29907"><span class="ln">29907 </span></a> 
<a name="l29908"><span class="ln">29908 </span></a>    .. note:: 
<a name="l29909"><span class="ln">29909 </span></a>        This function is similar to NumPy's `take_along_axis`. 
<a name="l29910"><span class="ln">29910 </span></a>        See also :func:`torch.gather`. 
<a name="l29911"><span class="ln">29911 </span></a> 
<a name="l29912"><span class="ln">29912 </span></a>    Args: 
<a name="l29913"><span class="ln">29913 </span></a>        input (Tensor): the input tensor. 
<a name="l29914"><span class="ln">29914 </span></a>        indices (LongTensor): the indices into :attr:`input`. Must have long dtype. 
<a name="l29915"><span class="ln">29915 </span></a>        dim (int, optional): dimension to select along. Default: 0 
<a name="l29916"><span class="ln">29916 </span></a> 
<a name="l29917"><span class="ln">29917 </span></a>    Keyword args: 
<a name="l29918"><span class="ln">29918 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29919"><span class="ln">29919 </span></a> 
<a name="l29920"><span class="ln">29920 </span></a>    Example:: 
<a name="l29921"><span class="ln">29921 </span></a> 
<a name="l29922"><span class="ln">29922 </span></a>        &gt;&gt;&gt; t = torch.tensor([[10, 30, 20], [60, 40, 50]]) 
<a name="l29923"><span class="ln">29923 </span></a>        &gt;&gt;&gt; max_idx = torch.argmax(t) 
<a name="l29924"><span class="ln">29924 </span></a>        &gt;&gt;&gt; torch.take_along_dim(t, max_idx) 
<a name="l29925"><span class="ln">29925 </span></a>        tensor([60]) 
<a name="l29926"><span class="ln">29926 </span></a>        &gt;&gt;&gt; sorted_idx = torch.argsort(t, dim=1) 
<a name="l29927"><span class="ln">29927 </span></a>        &gt;&gt;&gt; torch.take_along_dim(t, sorted_idx, dim=1) 
<a name="l29928"><span class="ln">29928 </span></a>        tensor([[10, 20, 30], 
<a name="l29929"><span class="ln">29929 </span></a>                [40, 50, 60]]) 
<a name="l29930"><span class="ln">29930 </span></a>    &quot;&quot;&quot;</span>
<a name="l29931"><span class="ln">29931 </span></a>
<a name="l29932"><span class="ln">29932 </span></a><span class="s2">def </span><span class="s1">tan</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29933"><span class="ln">29933 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29934"><span class="ln">29934 </span></a>    tan(input, *, out=None) -&gt; Tensor 
<a name="l29935"><span class="ln">29935 </span></a> 
<a name="l29936"><span class="ln">29936 </span></a>    Returns a new tensor with the tangent of the elements of :attr:`input`. 
<a name="l29937"><span class="ln">29937 </span></a> 
<a name="l29938"><span class="ln">29938 </span></a>    .. math:: 
<a name="l29939"><span class="ln">29939 </span></a>        \text{out}_{i} = \tan(\text{input}_{i}) 
<a name="l29940"><span class="ln">29940 </span></a> 
<a name="l29941"><span class="ln">29941 </span></a>    Args: 
<a name="l29942"><span class="ln">29942 </span></a>        input (Tensor): the input tensor. 
<a name="l29943"><span class="ln">29943 </span></a> 
<a name="l29944"><span class="ln">29944 </span></a>    Keyword args: 
<a name="l29945"><span class="ln">29945 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29946"><span class="ln">29946 </span></a> 
<a name="l29947"><span class="ln">29947 </span></a>    Example:: 
<a name="l29948"><span class="ln">29948 </span></a> 
<a name="l29949"><span class="ln">29949 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l29950"><span class="ln">29950 </span></a>        &gt;&gt;&gt; a 
<a name="l29951"><span class="ln">29951 </span></a>        tensor([-1.2027, -1.7687,  0.4412, -1.3856]) 
<a name="l29952"><span class="ln">29952 </span></a>        &gt;&gt;&gt; torch.tan(a) 
<a name="l29953"><span class="ln">29953 </span></a>        tensor([-2.5930,  4.9859,  0.4722, -5.3366]) 
<a name="l29954"><span class="ln">29954 </span></a>    &quot;&quot;&quot;</span>
<a name="l29955"><span class="ln">29955 </span></a>
<a name="l29956"><span class="ln">29956 </span></a><span class="s2">def </span><span class="s1">tan_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l29957"><span class="ln">29957 </span></a><span class="s2">def </span><span class="s1">tanh</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29958"><span class="ln">29958 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29959"><span class="ln">29959 </span></a>    tanh(input, *, out=None) -&gt; Tensor 
<a name="l29960"><span class="ln">29960 </span></a> 
<a name="l29961"><span class="ln">29961 </span></a>    Returns a new tensor with the hyperbolic tangent of the elements 
<a name="l29962"><span class="ln">29962 </span></a>    of :attr:`input`. 
<a name="l29963"><span class="ln">29963 </span></a> 
<a name="l29964"><span class="ln">29964 </span></a>    .. math:: 
<a name="l29965"><span class="ln">29965 </span></a>        \text{out}_{i} = \tanh(\text{input}_{i}) 
<a name="l29966"><span class="ln">29966 </span></a> 
<a name="l29967"><span class="ln">29967 </span></a>    Args: 
<a name="l29968"><span class="ln">29968 </span></a>        input (Tensor): the input tensor. 
<a name="l29969"><span class="ln">29969 </span></a> 
<a name="l29970"><span class="ln">29970 </span></a>    Keyword args: 
<a name="l29971"><span class="ln">29971 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l29972"><span class="ln">29972 </span></a> 
<a name="l29973"><span class="ln">29973 </span></a>    Example:: 
<a name="l29974"><span class="ln">29974 </span></a> 
<a name="l29975"><span class="ln">29975 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l29976"><span class="ln">29976 </span></a>        &gt;&gt;&gt; a 
<a name="l29977"><span class="ln">29977 </span></a>        tensor([ 0.8986, -0.7279,  1.1745,  0.2611]) 
<a name="l29978"><span class="ln">29978 </span></a>        &gt;&gt;&gt; torch.tanh(a) 
<a name="l29979"><span class="ln">29979 </span></a>        tensor([ 0.7156, -0.6218,  0.8257,  0.2553]) 
<a name="l29980"><span class="ln">29980 </span></a>    &quot;&quot;&quot;</span>
<a name="l29981"><span class="ln">29981 </span></a>
<a name="l29982"><span class="ln">29982 </span></a><span class="s2">def </span><span class="s1">tanh_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l29983"><span class="ln">29983 </span></a><span class="s2">def </span><span class="s1">tensor</span><span class="s3">(</span>
<a name="l29984"><span class="ln">29984 </span></a>    <span class="s1">data</span><span class="s2">: </span><span class="s1">Any</span><span class="s3">,</span>
<a name="l29985"><span class="ln">29985 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29986"><span class="ln">29986 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l29987"><span class="ln">29987 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l29988"><span class="ln">29988 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l29989"><span class="ln">29989 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l29990"><span class="ln">29990 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l29991"><span class="ln">29991 </span></a>    tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor 
<a name="l29992"><span class="ln">29992 </span></a> 
<a name="l29993"><span class="ln">29993 </span></a>    Constructs a tensor with no autograd history (also known as a &quot;leaf tensor&quot;, see :doc:`/notes/autograd`) by copying :attr:`data`. 
<a name="l29994"><span class="ln">29994 </span></a> 
<a name="l29995"><span class="ln">29995 </span></a>    .. warning:: 
<a name="l29996"><span class="ln">29996 </span></a> 
<a name="l29997"><span class="ln">29997 </span></a>        When working with tensors prefer using :func:`torch.Tensor.clone`, 
<a name="l29998"><span class="ln">29998 </span></a>        :func:`torch.Tensor.detach`, and :func:`torch.Tensor.requires_grad_` for 
<a name="l29999"><span class="ln">29999 </span></a>        readability. Letting `t` be a tensor, ``torch.tensor(t)`` is equivalent to 
<a name="l30000"><span class="ln">30000 </span></a>        ``t.detach().clone()``, and ``torch.tensor(t, requires_grad=True)`` 
<a name="l30001"><span class="ln">30001 </span></a>        is equivalent to ``t.detach().clone().requires_grad_(True)``. 
<a name="l30002"><span class="ln">30002 </span></a> 
<a name="l30003"><span class="ln">30003 </span></a>    .. seealso:: 
<a name="l30004"><span class="ln">30004 </span></a> 
<a name="l30005"><span class="ln">30005 </span></a>        :func:`torch.as_tensor` preserves autograd history and avoids copies where possible. 
<a name="l30006"><span class="ln">30006 </span></a>        :func:`torch.from_numpy` creates a tensor that shares storage with a NumPy array. 
<a name="l30007"><span class="ln">30007 </span></a> 
<a name="l30008"><span class="ln">30008 </span></a>    Args: 
<a name="l30009"><span class="ln">30009 </span></a>        data (array_like): Initial data for the tensor. Can be a list, tuple, 
<a name="l30010"><span class="ln">30010 </span></a>            NumPy ``ndarray``, scalar, and other types. 
<a name="l30011"><span class="ln">30011 </span></a> 
<a name="l30012"><span class="ln">30012 </span></a>    Keyword args: 
<a name="l30013"><span class="ln">30013 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l30014"><span class="ln">30014 </span></a>            Default: if ``None``, infers data type from :attr:`data`. 
<a name="l30015"><span class="ln">30015 </span></a>        device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor 
<a name="l30016"><span class="ln">30016 </span></a>            then the device of data is used. If None and data is not a tensor then 
<a name="l30017"><span class="ln">30017 </span></a>            the result tensor is constructed on the current device. 
<a name="l30018"><span class="ln">30018 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l30019"><span class="ln">30019 </span></a>            returned tensor. Default: ``False``. 
<a name="l30020"><span class="ln">30020 </span></a>        pin_memory (bool, optional): If set, returned tensor would be allocated in 
<a name="l30021"><span class="ln">30021 </span></a>            the pinned memory. Works only for CPU tensors. Default: ``False``. 
<a name="l30022"><span class="ln">30022 </span></a> 
<a name="l30023"><span class="ln">30023 </span></a> 
<a name="l30024"><span class="ln">30024 </span></a>    Example:: 
<a name="l30025"><span class="ln">30025 </span></a> 
<a name="l30026"><span class="ln">30026 </span></a>        &gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]]) 
<a name="l30027"><span class="ln">30027 </span></a>        tensor([[ 0.1000,  1.2000], 
<a name="l30028"><span class="ln">30028 </span></a>                [ 2.2000,  3.1000], 
<a name="l30029"><span class="ln">30029 </span></a>                [ 4.9000,  5.2000]]) 
<a name="l30030"><span class="ln">30030 </span></a> 
<a name="l30031"><span class="ln">30031 </span></a>        &gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data 
<a name="l30032"><span class="ln">30032 </span></a>        tensor([ 0,  1]) 
<a name="l30033"><span class="ln">30033 </span></a> 
<a name="l30034"><span class="ln">30034 </span></a>        &gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]], 
<a name="l30035"><span class="ln">30035 </span></a>        ...              dtype=torch.float64, 
<a name="l30036"><span class="ln">30036 </span></a>        ...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device 
<a name="l30037"><span class="ln">30037 </span></a>        tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0') 
<a name="l30038"><span class="ln">30038 </span></a> 
<a name="l30039"><span class="ln">30039 </span></a>        &gt;&gt;&gt; torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor 
<a name="l30040"><span class="ln">30040 </span></a>        tensor(3.1416) 
<a name="l30041"><span class="ln">30041 </span></a> 
<a name="l30042"><span class="ln">30042 </span></a>        &gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,)) 
<a name="l30043"><span class="ln">30043 </span></a>        tensor([]) 
<a name="l30044"><span class="ln">30044 </span></a>    &quot;&quot;&quot;</span>
<a name="l30045"><span class="ln">30045 </span></a>
<a name="l30046"><span class="ln">30046 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30047"><span class="ln">30047 </span></a><span class="s2">def </span><span class="s1">tensor_split</span><span class="s3">(</span>
<a name="l30048"><span class="ln">30048 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30049"><span class="ln">30049 </span></a>    <span class="s1">tensor_indices_or_sections</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30050"><span class="ln">30050 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l30051"><span class="ln">30051 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l30052"><span class="ln">30052 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30053"><span class="ln">30053 </span></a>    tensor_split(input, indices_or_sections, dim=0) -&gt; List of Tensors 
<a name="l30054"><span class="ln">30054 </span></a> 
<a name="l30055"><span class="ln">30055 </span></a>    Splits a tensor into multiple sub-tensors, all of which are views of :attr:`input`, 
<a name="l30056"><span class="ln">30056 </span></a>    along dimension :attr:`dim` according to the indices or number of sections specified 
<a name="l30057"><span class="ln">30057 </span></a>    by :attr:`indices_or_sections`. This function is based on NumPy's 
<a name="l30058"><span class="ln">30058 </span></a>    :func:`numpy.array_split`. 
<a name="l30059"><span class="ln">30059 </span></a> 
<a name="l30060"><span class="ln">30060 </span></a>    Args: 
<a name="l30061"><span class="ln">30061 </span></a>        input (Tensor): the tensor to split 
<a name="l30062"><span class="ln">30062 </span></a>        indices_or_sections (Tensor, int or list or tuple of ints): 
<a name="l30063"><span class="ln">30063 </span></a>            If :attr:`indices_or_sections` is an integer ``n`` or a zero dimensional long tensor 
<a name="l30064"><span class="ln">30064 </span></a>            with value ``n``, :attr:`input` is split into ``n`` sections along dimension :attr:`dim`. 
<a name="l30065"><span class="ln">30065 </span></a>            If :attr:`input` is divisible by ``n`` along dimension :attr:`dim`, each 
<a name="l30066"><span class="ln">30066 </span></a>            section will be of equal size, :code:`input.size(dim) / n`. If :attr:`input` 
<a name="l30067"><span class="ln">30067 </span></a>            is not divisible by ``n``, the sizes of the first :code:`int(input.size(dim) % n)` 
<a name="l30068"><span class="ln">30068 </span></a>            sections will have size :code:`int(input.size(dim) / n) + 1`, and the rest will 
<a name="l30069"><span class="ln">30069 </span></a>            have size :code:`int(input.size(dim) / n)`. 
<a name="l30070"><span class="ln">30070 </span></a> 
<a name="l30071"><span class="ln">30071 </span></a>            If :attr:`indices_or_sections` is a list or tuple of ints, or a one-dimensional long 
<a name="l30072"><span class="ln">30072 </span></a>            tensor, then :attr:`input` is split along dimension :attr:`dim` at each of the indices 
<a name="l30073"><span class="ln">30073 </span></a>            in the list, tuple or tensor. For instance, :code:`indices_or_sections=[2, 3]` and :code:`dim=0` 
<a name="l30074"><span class="ln">30074 </span></a>            would result in the tensors :code:`input[:2]`, :code:`input[2:3]`, and :code:`input[3:]`. 
<a name="l30075"><span class="ln">30075 </span></a> 
<a name="l30076"><span class="ln">30076 </span></a>            If :attr:`indices_or_sections` is a tensor, it must be a zero-dimensional or one-dimensional 
<a name="l30077"><span class="ln">30077 </span></a>            long tensor on the CPU. 
<a name="l30078"><span class="ln">30078 </span></a> 
<a name="l30079"><span class="ln">30079 </span></a>        dim (int, optional): dimension along which to split the tensor. Default: ``0`` 
<a name="l30080"><span class="ln">30080 </span></a> 
<a name="l30081"><span class="ln">30081 </span></a>    Example:: 
<a name="l30082"><span class="ln">30082 </span></a> 
<a name="l30083"><span class="ln">30083 </span></a>        &gt;&gt;&gt; x = torch.arange(8) 
<a name="l30084"><span class="ln">30084 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l30085"><span class="ln">30085 </span></a>        (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7])) 
<a name="l30086"><span class="ln">30086 </span></a> 
<a name="l30087"><span class="ln">30087 </span></a>        &gt;&gt;&gt; x = torch.arange(7) 
<a name="l30088"><span class="ln">30088 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l30089"><span class="ln">30089 </span></a>        (tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6])) 
<a name="l30090"><span class="ln">30090 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, (1, 6)) 
<a name="l30091"><span class="ln">30091 </span></a>        (tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6])) 
<a name="l30092"><span class="ln">30092 </span></a> 
<a name="l30093"><span class="ln">30093 </span></a>        &gt;&gt;&gt; x = torch.arange(14).reshape(2, 7) 
<a name="l30094"><span class="ln">30094 </span></a>        &gt;&gt;&gt; x 
<a name="l30095"><span class="ln">30095 </span></a>        tensor([[ 0,  1,  2,  3,  4,  5,  6], 
<a name="l30096"><span class="ln">30096 </span></a>                [ 7,  8,  9, 10, 11, 12, 13]]) 
<a name="l30097"><span class="ln">30097 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3, dim=1) 
<a name="l30098"><span class="ln">30098 </span></a>        (tensor([[0, 1, 2], 
<a name="l30099"><span class="ln">30099 </span></a>                [7, 8, 9]]), 
<a name="l30100"><span class="ln">30100 </span></a>         tensor([[ 3,  4], 
<a name="l30101"><span class="ln">30101 </span></a>                [10, 11]]), 
<a name="l30102"><span class="ln">30102 </span></a>         tensor([[ 5,  6], 
<a name="l30103"><span class="ln">30103 </span></a>                [12, 13]])) 
<a name="l30104"><span class="ln">30104 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, (1, 6), dim=1) 
<a name="l30105"><span class="ln">30105 </span></a>        (tensor([[0], 
<a name="l30106"><span class="ln">30106 </span></a>                [7]]), 
<a name="l30107"><span class="ln">30107 </span></a>         tensor([[ 1,  2,  3,  4,  5], 
<a name="l30108"><span class="ln">30108 </span></a>                [ 8,  9, 10, 11, 12]]), 
<a name="l30109"><span class="ln">30109 </span></a>         tensor([[ 6], 
<a name="l30110"><span class="ln">30110 </span></a>                [13]])) 
<a name="l30111"><span class="ln">30111 </span></a>    &quot;&quot;&quot;</span>
<a name="l30112"><span class="ln">30112 </span></a>
<a name="l30113"><span class="ln">30113 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30114"><span class="ln">30114 </span></a><span class="s2">def </span><span class="s1">tensor_split</span><span class="s3">(</span>
<a name="l30115"><span class="ln">30115 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30116"><span class="ln">30116 </span></a>    <span class="s1">sections</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l30117"><span class="ln">30117 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l30118"><span class="ln">30118 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l30119"><span class="ln">30119 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30120"><span class="ln">30120 </span></a>    tensor_split(input, indices_or_sections, dim=0) -&gt; List of Tensors 
<a name="l30121"><span class="ln">30121 </span></a> 
<a name="l30122"><span class="ln">30122 </span></a>    Splits a tensor into multiple sub-tensors, all of which are views of :attr:`input`, 
<a name="l30123"><span class="ln">30123 </span></a>    along dimension :attr:`dim` according to the indices or number of sections specified 
<a name="l30124"><span class="ln">30124 </span></a>    by :attr:`indices_or_sections`. This function is based on NumPy's 
<a name="l30125"><span class="ln">30125 </span></a>    :func:`numpy.array_split`. 
<a name="l30126"><span class="ln">30126 </span></a> 
<a name="l30127"><span class="ln">30127 </span></a>    Args: 
<a name="l30128"><span class="ln">30128 </span></a>        input (Tensor): the tensor to split 
<a name="l30129"><span class="ln">30129 </span></a>        indices_or_sections (Tensor, int or list or tuple of ints): 
<a name="l30130"><span class="ln">30130 </span></a>            If :attr:`indices_or_sections` is an integer ``n`` or a zero dimensional long tensor 
<a name="l30131"><span class="ln">30131 </span></a>            with value ``n``, :attr:`input` is split into ``n`` sections along dimension :attr:`dim`. 
<a name="l30132"><span class="ln">30132 </span></a>            If :attr:`input` is divisible by ``n`` along dimension :attr:`dim`, each 
<a name="l30133"><span class="ln">30133 </span></a>            section will be of equal size, :code:`input.size(dim) / n`. If :attr:`input` 
<a name="l30134"><span class="ln">30134 </span></a>            is not divisible by ``n``, the sizes of the first :code:`int(input.size(dim) % n)` 
<a name="l30135"><span class="ln">30135 </span></a>            sections will have size :code:`int(input.size(dim) / n) + 1`, and the rest will 
<a name="l30136"><span class="ln">30136 </span></a>            have size :code:`int(input.size(dim) / n)`. 
<a name="l30137"><span class="ln">30137 </span></a> 
<a name="l30138"><span class="ln">30138 </span></a>            If :attr:`indices_or_sections` is a list or tuple of ints, or a one-dimensional long 
<a name="l30139"><span class="ln">30139 </span></a>            tensor, then :attr:`input` is split along dimension :attr:`dim` at each of the indices 
<a name="l30140"><span class="ln">30140 </span></a>            in the list, tuple or tensor. For instance, :code:`indices_or_sections=[2, 3]` and :code:`dim=0` 
<a name="l30141"><span class="ln">30141 </span></a>            would result in the tensors :code:`input[:2]`, :code:`input[2:3]`, and :code:`input[3:]`. 
<a name="l30142"><span class="ln">30142 </span></a> 
<a name="l30143"><span class="ln">30143 </span></a>            If :attr:`indices_or_sections` is a tensor, it must be a zero-dimensional or one-dimensional 
<a name="l30144"><span class="ln">30144 </span></a>            long tensor on the CPU. 
<a name="l30145"><span class="ln">30145 </span></a> 
<a name="l30146"><span class="ln">30146 </span></a>        dim (int, optional): dimension along which to split the tensor. Default: ``0`` 
<a name="l30147"><span class="ln">30147 </span></a> 
<a name="l30148"><span class="ln">30148 </span></a>    Example:: 
<a name="l30149"><span class="ln">30149 </span></a> 
<a name="l30150"><span class="ln">30150 </span></a>        &gt;&gt;&gt; x = torch.arange(8) 
<a name="l30151"><span class="ln">30151 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l30152"><span class="ln">30152 </span></a>        (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7])) 
<a name="l30153"><span class="ln">30153 </span></a> 
<a name="l30154"><span class="ln">30154 </span></a>        &gt;&gt;&gt; x = torch.arange(7) 
<a name="l30155"><span class="ln">30155 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l30156"><span class="ln">30156 </span></a>        (tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6])) 
<a name="l30157"><span class="ln">30157 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, (1, 6)) 
<a name="l30158"><span class="ln">30158 </span></a>        (tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6])) 
<a name="l30159"><span class="ln">30159 </span></a> 
<a name="l30160"><span class="ln">30160 </span></a>        &gt;&gt;&gt; x = torch.arange(14).reshape(2, 7) 
<a name="l30161"><span class="ln">30161 </span></a>        &gt;&gt;&gt; x 
<a name="l30162"><span class="ln">30162 </span></a>        tensor([[ 0,  1,  2,  3,  4,  5,  6], 
<a name="l30163"><span class="ln">30163 </span></a>                [ 7,  8,  9, 10, 11, 12, 13]]) 
<a name="l30164"><span class="ln">30164 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3, dim=1) 
<a name="l30165"><span class="ln">30165 </span></a>        (tensor([[0, 1, 2], 
<a name="l30166"><span class="ln">30166 </span></a>                [7, 8, 9]]), 
<a name="l30167"><span class="ln">30167 </span></a>         tensor([[ 3,  4], 
<a name="l30168"><span class="ln">30168 </span></a>                [10, 11]]), 
<a name="l30169"><span class="ln">30169 </span></a>         tensor([[ 5,  6], 
<a name="l30170"><span class="ln">30170 </span></a>                [12, 13]])) 
<a name="l30171"><span class="ln">30171 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, (1, 6), dim=1) 
<a name="l30172"><span class="ln">30172 </span></a>        (tensor([[0], 
<a name="l30173"><span class="ln">30173 </span></a>                [7]]), 
<a name="l30174"><span class="ln">30174 </span></a>         tensor([[ 1,  2,  3,  4,  5], 
<a name="l30175"><span class="ln">30175 </span></a>                [ 8,  9, 10, 11, 12]]), 
<a name="l30176"><span class="ln">30176 </span></a>         tensor([[ 6], 
<a name="l30177"><span class="ln">30177 </span></a>                [13]])) 
<a name="l30178"><span class="ln">30178 </span></a>    &quot;&quot;&quot;</span>
<a name="l30179"><span class="ln">30179 </span></a>
<a name="l30180"><span class="ln">30180 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30181"><span class="ln">30181 </span></a><span class="s2">def </span><span class="s1">tensor_split</span><span class="s3">(</span>
<a name="l30182"><span class="ln">30182 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30183"><span class="ln">30183 </span></a>    <span class="s1">indices</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l30184"><span class="ln">30184 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l30185"><span class="ln">30185 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l30186"><span class="ln">30186 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30187"><span class="ln">30187 </span></a>    tensor_split(input, indices_or_sections, dim=0) -&gt; List of Tensors 
<a name="l30188"><span class="ln">30188 </span></a> 
<a name="l30189"><span class="ln">30189 </span></a>    Splits a tensor into multiple sub-tensors, all of which are views of :attr:`input`, 
<a name="l30190"><span class="ln">30190 </span></a>    along dimension :attr:`dim` according to the indices or number of sections specified 
<a name="l30191"><span class="ln">30191 </span></a>    by :attr:`indices_or_sections`. This function is based on NumPy's 
<a name="l30192"><span class="ln">30192 </span></a>    :func:`numpy.array_split`. 
<a name="l30193"><span class="ln">30193 </span></a> 
<a name="l30194"><span class="ln">30194 </span></a>    Args: 
<a name="l30195"><span class="ln">30195 </span></a>        input (Tensor): the tensor to split 
<a name="l30196"><span class="ln">30196 </span></a>        indices_or_sections (Tensor, int or list or tuple of ints): 
<a name="l30197"><span class="ln">30197 </span></a>            If :attr:`indices_or_sections` is an integer ``n`` or a zero dimensional long tensor 
<a name="l30198"><span class="ln">30198 </span></a>            with value ``n``, :attr:`input` is split into ``n`` sections along dimension :attr:`dim`. 
<a name="l30199"><span class="ln">30199 </span></a>            If :attr:`input` is divisible by ``n`` along dimension :attr:`dim`, each 
<a name="l30200"><span class="ln">30200 </span></a>            section will be of equal size, :code:`input.size(dim) / n`. If :attr:`input` 
<a name="l30201"><span class="ln">30201 </span></a>            is not divisible by ``n``, the sizes of the first :code:`int(input.size(dim) % n)` 
<a name="l30202"><span class="ln">30202 </span></a>            sections will have size :code:`int(input.size(dim) / n) + 1`, and the rest will 
<a name="l30203"><span class="ln">30203 </span></a>            have size :code:`int(input.size(dim) / n)`. 
<a name="l30204"><span class="ln">30204 </span></a> 
<a name="l30205"><span class="ln">30205 </span></a>            If :attr:`indices_or_sections` is a list or tuple of ints, or a one-dimensional long 
<a name="l30206"><span class="ln">30206 </span></a>            tensor, then :attr:`input` is split along dimension :attr:`dim` at each of the indices 
<a name="l30207"><span class="ln">30207 </span></a>            in the list, tuple or tensor. For instance, :code:`indices_or_sections=[2, 3]` and :code:`dim=0` 
<a name="l30208"><span class="ln">30208 </span></a>            would result in the tensors :code:`input[:2]`, :code:`input[2:3]`, and :code:`input[3:]`. 
<a name="l30209"><span class="ln">30209 </span></a> 
<a name="l30210"><span class="ln">30210 </span></a>            If :attr:`indices_or_sections` is a tensor, it must be a zero-dimensional or one-dimensional 
<a name="l30211"><span class="ln">30211 </span></a>            long tensor on the CPU. 
<a name="l30212"><span class="ln">30212 </span></a> 
<a name="l30213"><span class="ln">30213 </span></a>        dim (int, optional): dimension along which to split the tensor. Default: ``0`` 
<a name="l30214"><span class="ln">30214 </span></a> 
<a name="l30215"><span class="ln">30215 </span></a>    Example:: 
<a name="l30216"><span class="ln">30216 </span></a> 
<a name="l30217"><span class="ln">30217 </span></a>        &gt;&gt;&gt; x = torch.arange(8) 
<a name="l30218"><span class="ln">30218 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l30219"><span class="ln">30219 </span></a>        (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7])) 
<a name="l30220"><span class="ln">30220 </span></a> 
<a name="l30221"><span class="ln">30221 </span></a>        &gt;&gt;&gt; x = torch.arange(7) 
<a name="l30222"><span class="ln">30222 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3) 
<a name="l30223"><span class="ln">30223 </span></a>        (tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6])) 
<a name="l30224"><span class="ln">30224 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, (1, 6)) 
<a name="l30225"><span class="ln">30225 </span></a>        (tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6])) 
<a name="l30226"><span class="ln">30226 </span></a> 
<a name="l30227"><span class="ln">30227 </span></a>        &gt;&gt;&gt; x = torch.arange(14).reshape(2, 7) 
<a name="l30228"><span class="ln">30228 </span></a>        &gt;&gt;&gt; x 
<a name="l30229"><span class="ln">30229 </span></a>        tensor([[ 0,  1,  2,  3,  4,  5,  6], 
<a name="l30230"><span class="ln">30230 </span></a>                [ 7,  8,  9, 10, 11, 12, 13]]) 
<a name="l30231"><span class="ln">30231 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, 3, dim=1) 
<a name="l30232"><span class="ln">30232 </span></a>        (tensor([[0, 1, 2], 
<a name="l30233"><span class="ln">30233 </span></a>                [7, 8, 9]]), 
<a name="l30234"><span class="ln">30234 </span></a>         tensor([[ 3,  4], 
<a name="l30235"><span class="ln">30235 </span></a>                [10, 11]]), 
<a name="l30236"><span class="ln">30236 </span></a>         tensor([[ 5,  6], 
<a name="l30237"><span class="ln">30237 </span></a>                [12, 13]])) 
<a name="l30238"><span class="ln">30238 </span></a>        &gt;&gt;&gt; torch.tensor_split(x, (1, 6), dim=1) 
<a name="l30239"><span class="ln">30239 </span></a>        (tensor([[0], 
<a name="l30240"><span class="ln">30240 </span></a>                [7]]), 
<a name="l30241"><span class="ln">30241 </span></a>         tensor([[ 1,  2,  3,  4,  5], 
<a name="l30242"><span class="ln">30242 </span></a>                [ 8,  9, 10, 11, 12]]), 
<a name="l30243"><span class="ln">30243 </span></a>         tensor([[ 6], 
<a name="l30244"><span class="ln">30244 </span></a>                [13]])) 
<a name="l30245"><span class="ln">30245 </span></a>    &quot;&quot;&quot;</span>
<a name="l30246"><span class="ln">30246 </span></a>
<a name="l30247"><span class="ln">30247 </span></a><span class="s2">def </span><span class="s1">threshold</span><span class="s3">(</span>
<a name="l30248"><span class="ln">30248 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30249"><span class="ln">30249 </span></a>    <span class="s1">threshold</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l30250"><span class="ln">30250 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l30251"><span class="ln">30251 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30252"><span class="ln">30252 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30253"><span class="ln">30253 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l30254"><span class="ln">30254 </span></a><span class="s2">def </span><span class="s1">threshold_</span><span class="s3">(</span>
<a name="l30255"><span class="ln">30255 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30256"><span class="ln">30256 </span></a>    <span class="s1">threshold</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l30257"><span class="ln">30257 </span></a>    <span class="s1">value</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l30258"><span class="ln">30258 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l30259"><span class="ln">30259 </span></a><span class="s2">def </span><span class="s1">tile</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dims</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">]) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30260"><span class="ln">30260 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30261"><span class="ln">30261 </span></a>    tile(input, dims) -&gt; Tensor 
<a name="l30262"><span class="ln">30262 </span></a> 
<a name="l30263"><span class="ln">30263 </span></a>    Constructs a tensor by repeating the elements of :attr:`input`. 
<a name="l30264"><span class="ln">30264 </span></a>    The :attr:`dims` argument specifies the number of repetitions 
<a name="l30265"><span class="ln">30265 </span></a>    in each dimension. 
<a name="l30266"><span class="ln">30266 </span></a> 
<a name="l30267"><span class="ln">30267 </span></a>    If :attr:`dims` specifies fewer dimensions than :attr:`input` has, then 
<a name="l30268"><span class="ln">30268 </span></a>    ones are prepended to :attr:`dims` until all dimensions are specified. 
<a name="l30269"><span class="ln">30269 </span></a>    For example, if :attr:`input` has shape (8, 6, 4, 2) and :attr:`dims` 
<a name="l30270"><span class="ln">30270 </span></a>    is (2, 2), then :attr:`dims` is treated as (1, 1, 2, 2). 
<a name="l30271"><span class="ln">30271 </span></a> 
<a name="l30272"><span class="ln">30272 </span></a>    Analogously, if :attr:`input` has fewer dimensions than :attr:`dims` 
<a name="l30273"><span class="ln">30273 </span></a>    specifies, then :attr:`input` is treated as if it were unsqueezed at 
<a name="l30274"><span class="ln">30274 </span></a>    dimension zero until it has as many dimensions as :attr:`dims` specifies. 
<a name="l30275"><span class="ln">30275 </span></a>    For example, if :attr:`input` has shape (4, 2) and :attr:`dims` 
<a name="l30276"><span class="ln">30276 </span></a>    is (3, 3, 2, 2), then :attr:`input` is treated as if it had the 
<a name="l30277"><span class="ln">30277 </span></a>    shape (1, 1, 4, 2). 
<a name="l30278"><span class="ln">30278 </span></a> 
<a name="l30279"><span class="ln">30279 </span></a>    .. note:: 
<a name="l30280"><span class="ln">30280 </span></a> 
<a name="l30281"><span class="ln">30281 </span></a>        This function is similar to NumPy's tile function. 
<a name="l30282"><span class="ln">30282 </span></a> 
<a name="l30283"><span class="ln">30283 </span></a>    Args: 
<a name="l30284"><span class="ln">30284 </span></a>        input (Tensor): the tensor whose elements to repeat. 
<a name="l30285"><span class="ln">30285 </span></a>        dims (tuple): the number of repetitions per dimension. 
<a name="l30286"><span class="ln">30286 </span></a> 
<a name="l30287"><span class="ln">30287 </span></a>    Example:: 
<a name="l30288"><span class="ln">30288 </span></a> 
<a name="l30289"><span class="ln">30289 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) 
<a name="l30290"><span class="ln">30290 </span></a>        &gt;&gt;&gt; x.tile((2,)) 
<a name="l30291"><span class="ln">30291 </span></a>        tensor([1, 2, 3, 1, 2, 3]) 
<a name="l30292"><span class="ln">30292 </span></a>        &gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]]) 
<a name="l30293"><span class="ln">30293 </span></a>        &gt;&gt;&gt; torch.tile(y, (2, 2)) 
<a name="l30294"><span class="ln">30294 </span></a>        tensor([[1, 2, 1, 2], 
<a name="l30295"><span class="ln">30295 </span></a>                [3, 4, 3, 4], 
<a name="l30296"><span class="ln">30296 </span></a>                [1, 2, 1, 2], 
<a name="l30297"><span class="ln">30297 </span></a>                [3, 4, 3, 4]]) 
<a name="l30298"><span class="ln">30298 </span></a>    &quot;&quot;&quot;</span>
<a name="l30299"><span class="ln">30299 </span></a>
<a name="l30300"><span class="ln">30300 </span></a><span class="s2">def </span><span class="s1">topk</span><span class="s3">(</span>
<a name="l30301"><span class="ln">30301 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30302"><span class="ln">30302 </span></a>    <span class="s1">k</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l30303"><span class="ln">30303 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l30304"><span class="ln">30304 </span></a>    <span class="s1">largest</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l30305"><span class="ln">30305 </span></a>    <span class="s1">sorted</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l30306"><span class="ln">30306 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30307"><span class="ln">30307 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30308"><span class="ln">30308 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">topk</span><span class="s2">:</span>
<a name="l30309"><span class="ln">30309 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30310"><span class="ln">30310 </span></a>    topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -&gt; (Tensor, LongTensor) 
<a name="l30311"><span class="ln">30311 </span></a> 
<a name="l30312"><span class="ln">30312 </span></a>    Returns the :attr:`k` largest elements of the given :attr:`input` tensor along 
<a name="l30313"><span class="ln">30313 </span></a>    a given dimension. 
<a name="l30314"><span class="ln">30314 </span></a> 
<a name="l30315"><span class="ln">30315 </span></a>    If :attr:`dim` is not given, the last dimension of the `input` is chosen. 
<a name="l30316"><span class="ln">30316 </span></a> 
<a name="l30317"><span class="ln">30317 </span></a>    If :attr:`largest` is ``False`` then the `k` smallest elements are returned. 
<a name="l30318"><span class="ln">30318 </span></a> 
<a name="l30319"><span class="ln">30319 </span></a>    A namedtuple of `(values, indices)` is returned with the `values` and 
<a name="l30320"><span class="ln">30320 </span></a>    `indices` of the largest `k` elements of each row of the `input` tensor in the 
<a name="l30321"><span class="ln">30321 </span></a>    given dimension `dim`. 
<a name="l30322"><span class="ln">30322 </span></a> 
<a name="l30323"><span class="ln">30323 </span></a>    The boolean option :attr:`sorted` if ``True``, will make sure that the returned 
<a name="l30324"><span class="ln">30324 </span></a>    `k` elements are themselves sorted 
<a name="l30325"><span class="ln">30325 </span></a> 
<a name="l30326"><span class="ln">30326 </span></a>    .. note:: 
<a name="l30327"><span class="ln">30327 </span></a>        When using `torch.topk`, the indices of tied elements are not guaranteed to be stable 
<a name="l30328"><span class="ln">30328 </span></a>        and may vary across different invocations. 
<a name="l30329"><span class="ln">30329 </span></a> 
<a name="l30330"><span class="ln">30330 </span></a>    Args: 
<a name="l30331"><span class="ln">30331 </span></a>        input (Tensor): the input tensor. 
<a name="l30332"><span class="ln">30332 </span></a>        k (int): the k in &quot;top-k&quot; 
<a name="l30333"><span class="ln">30333 </span></a>        dim (int, optional): the dimension to sort along 
<a name="l30334"><span class="ln">30334 </span></a>        largest (bool, optional): controls whether to return largest or 
<a name="l30335"><span class="ln">30335 </span></a>               smallest elements 
<a name="l30336"><span class="ln">30336 </span></a>        sorted (bool, optional): controls whether to return the elements 
<a name="l30337"><span class="ln">30337 </span></a>               in sorted order 
<a name="l30338"><span class="ln">30338 </span></a> 
<a name="l30339"><span class="ln">30339 </span></a>    Keyword args: 
<a name="l30340"><span class="ln">30340 </span></a>        out (tuple, optional): the output tuple of (Tensor, LongTensor) that can be 
<a name="l30341"><span class="ln">30341 </span></a>            optionally given to be used as output buffers 
<a name="l30342"><span class="ln">30342 </span></a> 
<a name="l30343"><span class="ln">30343 </span></a>    Example:: 
<a name="l30344"><span class="ln">30344 </span></a> 
<a name="l30345"><span class="ln">30345 </span></a>        &gt;&gt;&gt; x = torch.arange(1., 6.) 
<a name="l30346"><span class="ln">30346 </span></a>        &gt;&gt;&gt; x 
<a name="l30347"><span class="ln">30347 </span></a>        tensor([ 1.,  2.,  3.,  4.,  5.]) 
<a name="l30348"><span class="ln">30348 </span></a>        &gt;&gt;&gt; torch.topk(x, 3) 
<a name="l30349"><span class="ln">30349 </span></a>        torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2])) 
<a name="l30350"><span class="ln">30350 </span></a>    &quot;&quot;&quot;</span>
<a name="l30351"><span class="ln">30351 </span></a>
<a name="l30352"><span class="ln">30352 </span></a><span class="s2">def </span><span class="s1">trace</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30353"><span class="ln">30353 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30354"><span class="ln">30354 </span></a>    trace(input) -&gt; Tensor 
<a name="l30355"><span class="ln">30355 </span></a> 
<a name="l30356"><span class="ln">30356 </span></a>    Returns the sum of the elements of the diagonal of the input 2-D matrix. 
<a name="l30357"><span class="ln">30357 </span></a> 
<a name="l30358"><span class="ln">30358 </span></a>    Example:: 
<a name="l30359"><span class="ln">30359 </span></a> 
<a name="l30360"><span class="ln">30360 </span></a>        &gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3) 
<a name="l30361"><span class="ln">30361 </span></a>        &gt;&gt;&gt; x 
<a name="l30362"><span class="ln">30362 </span></a>        tensor([[ 1.,  2.,  3.], 
<a name="l30363"><span class="ln">30363 </span></a>                [ 4.,  5.,  6.], 
<a name="l30364"><span class="ln">30364 </span></a>                [ 7.,  8.,  9.]]) 
<a name="l30365"><span class="ln">30365 </span></a>        &gt;&gt;&gt; torch.trace(x) 
<a name="l30366"><span class="ln">30366 </span></a>        tensor(15.) 
<a name="l30367"><span class="ln">30367 </span></a>    &quot;&quot;&quot;</span>
<a name="l30368"><span class="ln">30368 </span></a>
<a name="l30369"><span class="ln">30369 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30370"><span class="ln">30370 </span></a><span class="s2">def </span><span class="s1">transpose</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">, </span><span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30371"><span class="ln">30371 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30372"><span class="ln">30372 </span></a>    transpose(input, dim0, dim1) -&gt; Tensor 
<a name="l30373"><span class="ln">30373 </span></a> 
<a name="l30374"><span class="ln">30374 </span></a>    Returns a tensor that is a transposed version of :attr:`input`. 
<a name="l30375"><span class="ln">30375 </span></a>    The given dimensions :attr:`dim0` and :attr:`dim1` are swapped. 
<a name="l30376"><span class="ln">30376 </span></a> 
<a name="l30377"><span class="ln">30377 </span></a>    If :attr:`input` is a strided tensor then the resulting :attr:`out` 
<a name="l30378"><span class="ln">30378 </span></a>    tensor shares its underlying storage with the :attr:`input` tensor, so 
<a name="l30379"><span class="ln">30379 </span></a>    changing the content of one would change the content of the other. 
<a name="l30380"><span class="ln">30380 </span></a> 
<a name="l30381"><span class="ln">30381 </span></a>    If :attr:`input` is a :ref:`sparse tensor &lt;sparse-docs&gt;` then the 
<a name="l30382"><span class="ln">30382 </span></a>    resulting :attr:`out` tensor *does not* share the underlying storage 
<a name="l30383"><span class="ln">30383 </span></a>    with the :attr:`input` tensor. 
<a name="l30384"><span class="ln">30384 </span></a> 
<a name="l30385"><span class="ln">30385 </span></a>    If :attr:`input` is a :ref:`sparse tensor &lt;sparse-docs&gt;` with compressed 
<a name="l30386"><span class="ln">30386 </span></a>    layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments 
<a name="l30387"><span class="ln">30387 </span></a>    :attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must 
<a name="l30388"><span class="ln">30388 </span></a>    both be sparse dimensions. The batch dimensions of a sparse tensor are the 
<a name="l30389"><span class="ln">30389 </span></a>    dimensions preceding the sparse dimensions. 
<a name="l30390"><span class="ln">30390 </span></a> 
<a name="l30391"><span class="ln">30391 </span></a>    .. note:: 
<a name="l30392"><span class="ln">30392 </span></a>        Transpositions which interchange the sparse dimensions of a `SparseCSR` 
<a name="l30393"><span class="ln">30393 </span></a>        or `SparseCSC` layout tensor will result in the layout changing between 
<a name="l30394"><span class="ln">30394 </span></a>        the two options. Transposition of the sparse dimensions of a ` SparseBSR` 
<a name="l30395"><span class="ln">30395 </span></a>        or `SparseBSC` layout tensor will likewise generate a result with the 
<a name="l30396"><span class="ln">30396 </span></a>        opposite layout. 
<a name="l30397"><span class="ln">30397 </span></a> 
<a name="l30398"><span class="ln">30398 </span></a> 
<a name="l30399"><span class="ln">30399 </span></a>    Args: 
<a name="l30400"><span class="ln">30400 </span></a>        input (Tensor): the input tensor. 
<a name="l30401"><span class="ln">30401 </span></a>        dim0 (int): the first dimension to be transposed 
<a name="l30402"><span class="ln">30402 </span></a>        dim1 (int): the second dimension to be transposed 
<a name="l30403"><span class="ln">30403 </span></a> 
<a name="l30404"><span class="ln">30404 </span></a>    Example:: 
<a name="l30405"><span class="ln">30405 </span></a> 
<a name="l30406"><span class="ln">30406 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l30407"><span class="ln">30407 </span></a>        &gt;&gt;&gt; x 
<a name="l30408"><span class="ln">30408 </span></a>        tensor([[ 1.0028, -0.9893,  0.5809], 
<a name="l30409"><span class="ln">30409 </span></a>                [-0.1669,  0.7299,  0.4942]]) 
<a name="l30410"><span class="ln">30410 </span></a>        &gt;&gt;&gt; torch.transpose(x, 0, 1) 
<a name="l30411"><span class="ln">30411 </span></a>        tensor([[ 1.0028, -0.1669], 
<a name="l30412"><span class="ln">30412 </span></a>                [-0.9893,  0.7299], 
<a name="l30413"><span class="ln">30413 </span></a>                [ 0.5809,  0.4942]]) 
<a name="l30414"><span class="ln">30414 </span></a> 
<a name="l30415"><span class="ln">30415 </span></a>    See also :func:`torch.t`. 
<a name="l30416"><span class="ln">30416 </span></a>    &quot;&quot;&quot;</span>
<a name="l30417"><span class="ln">30417 </span></a>
<a name="l30418"><span class="ln">30418 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30419"><span class="ln">30419 </span></a><span class="s2">def </span><span class="s1">transpose</span><span class="s3">(</span>
<a name="l30420"><span class="ln">30420 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30421"><span class="ln">30421 </span></a>    <span class="s1">dim0</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l30422"><span class="ln">30422 </span></a>    <span class="s1">dim1</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l30423"><span class="ln">30423 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30424"><span class="ln">30424 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30425"><span class="ln">30425 </span></a>    transpose(input, dim0, dim1) -&gt; Tensor 
<a name="l30426"><span class="ln">30426 </span></a> 
<a name="l30427"><span class="ln">30427 </span></a>    Returns a tensor that is a transposed version of :attr:`input`. 
<a name="l30428"><span class="ln">30428 </span></a>    The given dimensions :attr:`dim0` and :attr:`dim1` are swapped. 
<a name="l30429"><span class="ln">30429 </span></a> 
<a name="l30430"><span class="ln">30430 </span></a>    If :attr:`input` is a strided tensor then the resulting :attr:`out` 
<a name="l30431"><span class="ln">30431 </span></a>    tensor shares its underlying storage with the :attr:`input` tensor, so 
<a name="l30432"><span class="ln">30432 </span></a>    changing the content of one would change the content of the other. 
<a name="l30433"><span class="ln">30433 </span></a> 
<a name="l30434"><span class="ln">30434 </span></a>    If :attr:`input` is a :ref:`sparse tensor &lt;sparse-docs&gt;` then the 
<a name="l30435"><span class="ln">30435 </span></a>    resulting :attr:`out` tensor *does not* share the underlying storage 
<a name="l30436"><span class="ln">30436 </span></a>    with the :attr:`input` tensor. 
<a name="l30437"><span class="ln">30437 </span></a> 
<a name="l30438"><span class="ln">30438 </span></a>    If :attr:`input` is a :ref:`sparse tensor &lt;sparse-docs&gt;` with compressed 
<a name="l30439"><span class="ln">30439 </span></a>    layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments 
<a name="l30440"><span class="ln">30440 </span></a>    :attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must 
<a name="l30441"><span class="ln">30441 </span></a>    both be sparse dimensions. The batch dimensions of a sparse tensor are the 
<a name="l30442"><span class="ln">30442 </span></a>    dimensions preceding the sparse dimensions. 
<a name="l30443"><span class="ln">30443 </span></a> 
<a name="l30444"><span class="ln">30444 </span></a>    .. note:: 
<a name="l30445"><span class="ln">30445 </span></a>        Transpositions which interchange the sparse dimensions of a `SparseCSR` 
<a name="l30446"><span class="ln">30446 </span></a>        or `SparseCSC` layout tensor will result in the layout changing between 
<a name="l30447"><span class="ln">30447 </span></a>        the two options. Transposition of the sparse dimensions of a ` SparseBSR` 
<a name="l30448"><span class="ln">30448 </span></a>        or `SparseBSC` layout tensor will likewise generate a result with the 
<a name="l30449"><span class="ln">30449 </span></a>        opposite layout. 
<a name="l30450"><span class="ln">30450 </span></a> 
<a name="l30451"><span class="ln">30451 </span></a> 
<a name="l30452"><span class="ln">30452 </span></a>    Args: 
<a name="l30453"><span class="ln">30453 </span></a>        input (Tensor): the input tensor. 
<a name="l30454"><span class="ln">30454 </span></a>        dim0 (int): the first dimension to be transposed 
<a name="l30455"><span class="ln">30455 </span></a>        dim1 (int): the second dimension to be transposed 
<a name="l30456"><span class="ln">30456 </span></a> 
<a name="l30457"><span class="ln">30457 </span></a>    Example:: 
<a name="l30458"><span class="ln">30458 </span></a> 
<a name="l30459"><span class="ln">30459 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 3) 
<a name="l30460"><span class="ln">30460 </span></a>        &gt;&gt;&gt; x 
<a name="l30461"><span class="ln">30461 </span></a>        tensor([[ 1.0028, -0.9893,  0.5809], 
<a name="l30462"><span class="ln">30462 </span></a>                [-0.1669,  0.7299,  0.4942]]) 
<a name="l30463"><span class="ln">30463 </span></a>        &gt;&gt;&gt; torch.transpose(x, 0, 1) 
<a name="l30464"><span class="ln">30464 </span></a>        tensor([[ 1.0028, -0.1669], 
<a name="l30465"><span class="ln">30465 </span></a>                [-0.9893,  0.7299], 
<a name="l30466"><span class="ln">30466 </span></a>                [ 0.5809,  0.4942]]) 
<a name="l30467"><span class="ln">30467 </span></a> 
<a name="l30468"><span class="ln">30468 </span></a>    See also :func:`torch.t`. 
<a name="l30469"><span class="ln">30469 </span></a>    &quot;&quot;&quot;</span>
<a name="l30470"><span class="ln">30470 </span></a>
<a name="l30471"><span class="ln">30471 </span></a><span class="s2">def </span><span class="s1">transpose_copy</span><span class="s3">(</span>
<a name="l30472"><span class="ln">30472 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30473"><span class="ln">30473 </span></a>    <span class="s1">dim0</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l30474"><span class="ln">30474 </span></a>    <span class="s1">dim1</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l30475"><span class="ln">30475 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30476"><span class="ln">30476 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30477"><span class="ln">30477 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30478"><span class="ln">30478 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30479"><span class="ln">30479 </span></a>    Performs the same operation as :func:`torch.transpose`, but all output tensors 
<a name="l30480"><span class="ln">30480 </span></a>    are freshly created instead of aliasing the input. 
<a name="l30481"><span class="ln">30481 </span></a>    &quot;&quot;&quot;</span>
<a name="l30482"><span class="ln">30482 </span></a>
<a name="l30483"><span class="ln">30483 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30484"><span class="ln">30484 </span></a><span class="s2">def </span><span class="s1">trapezoid</span><span class="s3">(</span><span class="s1">y</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">x</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30485"><span class="ln">30485 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30486"><span class="ln">30486 </span></a>    trapezoid(y, x=None, *, dx=None, dim=-1) -&gt; Tensor 
<a name="l30487"><span class="ln">30487 </span></a> 
<a name="l30488"><span class="ln">30488 </span></a>    Computes the `trapezoidal rule &lt;https://en.wikipedia.org/wiki/Trapezoidal_rule&gt;`_ along 
<a name="l30489"><span class="ln">30489 </span></a>    :attr:`dim`. By default the spacing between elements is assumed to be 1, but 
<a name="l30490"><span class="ln">30490 </span></a>    :attr:`dx` can be used to specify a different constant spacing, and :attr:`x` can be 
<a name="l30491"><span class="ln">30491 </span></a>    used to specify arbitrary spacing along :attr:`dim`. Only one of :attr:`x` or :attr:`dx` should be specified. 
<a name="l30492"><span class="ln">30492 </span></a> 
<a name="l30493"><span class="ln">30493 </span></a> 
<a name="l30494"><span class="ln">30494 </span></a>    Assuming :attr:`y` is a one-dimensional tensor with elements :math:`{y_0, y_1, ..., y_n}`, 
<a name="l30495"><span class="ln">30495 </span></a>    the default computation is 
<a name="l30496"><span class="ln">30496 </span></a> 
<a name="l30497"><span class="ln">30497 </span></a>    .. math:: 
<a name="l30498"><span class="ln">30498 </span></a>        \begin{aligned} 
<a name="l30499"><span class="ln">30499 </span></a>            \sum_{i = 1}^{n} \frac{1}{2} (y_i + y_{i-1}) 
<a name="l30500"><span class="ln">30500 </span></a>        \end{aligned} 
<a name="l30501"><span class="ln">30501 </span></a> 
<a name="l30502"><span class="ln">30502 </span></a>    When :attr:`dx` is specified the computation becomes 
<a name="l30503"><span class="ln">30503 </span></a> 
<a name="l30504"><span class="ln">30504 </span></a>    .. math:: 
<a name="l30505"><span class="ln">30505 </span></a>        \begin{aligned} 
<a name="l30506"><span class="ln">30506 </span></a>            \sum_{i = 1}^{n} \frac{\Delta x}{2} (y_i + y_{i-1}) 
<a name="l30507"><span class="ln">30507 </span></a>        \end{aligned} 
<a name="l30508"><span class="ln">30508 </span></a> 
<a name="l30509"><span class="ln">30509 </span></a>    effectively multiplying the result by :attr:`dx`. When :attr:`x` is specified, 
<a name="l30510"><span class="ln">30510 </span></a>    assuming :attr:`x` is also a one-dimensional tensor with 
<a name="l30511"><span class="ln">30511 </span></a>    elements :math:`{x_0, x_1, ..., x_n}`, the computation becomes 
<a name="l30512"><span class="ln">30512 </span></a> 
<a name="l30513"><span class="ln">30513 </span></a>    .. math:: 
<a name="l30514"><span class="ln">30514 </span></a>        \begin{aligned} 
<a name="l30515"><span class="ln">30515 </span></a>            \sum_{i = 1}^{n} \frac{(x_i - x_{i-1})}{2} (y_i + y_{i-1}) 
<a name="l30516"><span class="ln">30516 </span></a>        \end{aligned} 
<a name="l30517"><span class="ln">30517 </span></a> 
<a name="l30518"><span class="ln">30518 </span></a>    When :attr:`x` and :attr:`y` have the same size, the computation is as described above and no broadcasting is needed. 
<a name="l30519"><span class="ln">30519 </span></a>    The broadcasting behavior of this function is as follows when their sizes are different. For both :attr:`x` 
<a name="l30520"><span class="ln">30520 </span></a>    and :attr:`y`, the function computes the difference between consecutive elements along 
<a name="l30521"><span class="ln">30521 </span></a>    dimension :attr:`dim`. This effectively creates two tensors, `x_diff` and `y_diff`, that have 
<a name="l30522"><span class="ln">30522 </span></a>    the same shape as the original tensors except their lengths along the dimension :attr:`dim` is reduced by 1. 
<a name="l30523"><span class="ln">30523 </span></a>    After that, those two tensors are broadcast together to compute final output as part of the trapezoidal rule. 
<a name="l30524"><span class="ln">30524 </span></a>    See the examples below for details. 
<a name="l30525"><span class="ln">30525 </span></a> 
<a name="l30526"><span class="ln">30526 </span></a>    .. note:: 
<a name="l30527"><span class="ln">30527 </span></a>        The trapezoidal rule is a technique for approximating the definite integral of a function 
<a name="l30528"><span class="ln">30528 </span></a>        by averaging its left and right Riemann sums. The approximation becomes more accurate as 
<a name="l30529"><span class="ln">30529 </span></a>        the resolution of the partition increases. 
<a name="l30530"><span class="ln">30530 </span></a> 
<a name="l30531"><span class="ln">30531 </span></a>    Arguments: 
<a name="l30532"><span class="ln">30532 </span></a>        y (Tensor): Values to use when computing the trapezoidal rule. 
<a name="l30533"><span class="ln">30533 </span></a>        x (Tensor): If specified, defines spacing between values as specified above. 
<a name="l30534"><span class="ln">30534 </span></a> 
<a name="l30535"><span class="ln">30535 </span></a>    Keyword arguments: 
<a name="l30536"><span class="ln">30536 </span></a>        dx (float): constant spacing between values. If neither :attr:`x` or :attr:`dx` 
<a name="l30537"><span class="ln">30537 </span></a>            are specified then this defaults to 1. Effectively multiplies the result by its value. 
<a name="l30538"><span class="ln">30538 </span></a>        dim (int): The dimension along which to compute the trapezoidal rule. 
<a name="l30539"><span class="ln">30539 </span></a>            The last (inner-most) dimension by default. 
<a name="l30540"><span class="ln">30540 </span></a> 
<a name="l30541"><span class="ln">30541 </span></a>    Examples:: 
<a name="l30542"><span class="ln">30542 </span></a> 
<a name="l30543"><span class="ln">30543 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule in 1D, spacing is implicitly 1 
<a name="l30544"><span class="ln">30544 </span></a>        &gt;&gt;&gt; y = torch.tensor([1, 5, 10]) 
<a name="l30545"><span class="ln">30545 </span></a>        &gt;&gt;&gt; torch.trapezoid(y) 
<a name="l30546"><span class="ln">30546 </span></a>        tensor(10.5) 
<a name="l30547"><span class="ln">30547 </span></a> 
<a name="l30548"><span class="ln">30548 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly to verify 
<a name="l30549"><span class="ln">30549 </span></a>        &gt;&gt;&gt; (1 + 10 + 10) / 2 
<a name="l30550"><span class="ln">30550 </span></a>        10.5 
<a name="l30551"><span class="ln">30551 </span></a> 
<a name="l30552"><span class="ln">30552 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule in 1D with constant spacing of 2 
<a name="l30553"><span class="ln">30553 </span></a>        &gt;&gt;&gt; # NOTE: the result is the same as before, but multiplied by 2 
<a name="l30554"><span class="ln">30554 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, dx=2) 
<a name="l30555"><span class="ln">30555 </span></a>        21.0 
<a name="l30556"><span class="ln">30556 </span></a> 
<a name="l30557"><span class="ln">30557 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule in 1D with arbitrary spacing 
<a name="l30558"><span class="ln">30558 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l30559"><span class="ln">30559 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l30560"><span class="ln">30560 </span></a>        28.5 
<a name="l30561"><span class="ln">30561 </span></a> 
<a name="l30562"><span class="ln">30562 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly to verify 
<a name="l30563"><span class="ln">30563 </span></a>        &gt;&gt;&gt; ((3 - 1) * (1 + 5) + (6 - 3) * (5 + 10)) / 2 
<a name="l30564"><span class="ln">30564 </span></a>        28.5 
<a name="l30565"><span class="ln">30565 </span></a> 
<a name="l30566"><span class="ln">30566 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 matrix 
<a name="l30567"><span class="ln">30567 </span></a>        &gt;&gt;&gt; y = torch.arange(9).reshape(3, 3) 
<a name="l30568"><span class="ln">30568 </span></a>        tensor([[0, 1, 2], 
<a name="l30569"><span class="ln">30569 </span></a>                [3, 4, 5], 
<a name="l30570"><span class="ln">30570 </span></a>                [6, 7, 8]]) 
<a name="l30571"><span class="ln">30571 </span></a>        &gt;&gt;&gt; torch.trapezoid(y) 
<a name="l30572"><span class="ln">30572 </span></a>        tensor([ 2., 8., 14.]) 
<a name="l30573"><span class="ln">30573 </span></a> 
<a name="l30574"><span class="ln">30574 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each column of the matrix 
<a name="l30575"><span class="ln">30575 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, dim=0) 
<a name="l30576"><span class="ln">30576 </span></a>        tensor([ 6., 8., 10.]) 
<a name="l30577"><span class="ln">30577 </span></a> 
<a name="l30578"><span class="ln">30578 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l30579"><span class="ln">30579 </span></a>        &gt;&gt;&gt; #   with the same arbitrary spacing 
<a name="l30580"><span class="ln">30580 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l30581"><span class="ln">30581 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l30582"><span class="ln">30582 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l30583"><span class="ln">30583 </span></a>        array([5., 5., 5.]) 
<a name="l30584"><span class="ln">30584 </span></a> 
<a name="l30585"><span class="ln">30585 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l30586"><span class="ln">30586 </span></a>        &gt;&gt;&gt; #   with different arbitrary spacing per row 
<a name="l30587"><span class="ln">30587 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l30588"><span class="ln">30588 </span></a>        &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [1, 3, 5], [1, 4, 7]]) 
<a name="l30589"><span class="ln">30589 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l30590"><span class="ln">30590 </span></a>        array([2., 4., 6.]) 
<a name="l30591"><span class="ln">30591 </span></a>    &quot;&quot;&quot;</span>
<a name="l30592"><span class="ln">30592 </span></a>
<a name="l30593"><span class="ln">30593 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30594"><span class="ln">30594 </span></a><span class="s2">def </span><span class="s1">trapezoid</span><span class="s3">(</span>
<a name="l30595"><span class="ln">30595 </span></a>    <span class="s1">y</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30596"><span class="ln">30596 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30597"><span class="ln">30597 </span></a>    <span class="s1">dx</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l30598"><span class="ln">30598 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">,</span>
<a name="l30599"><span class="ln">30599 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30600"><span class="ln">30600 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30601"><span class="ln">30601 </span></a>    trapezoid(y, x=None, *, dx=None, dim=-1) -&gt; Tensor 
<a name="l30602"><span class="ln">30602 </span></a> 
<a name="l30603"><span class="ln">30603 </span></a>    Computes the `trapezoidal rule &lt;https://en.wikipedia.org/wiki/Trapezoidal_rule&gt;`_ along 
<a name="l30604"><span class="ln">30604 </span></a>    :attr:`dim`. By default the spacing between elements is assumed to be 1, but 
<a name="l30605"><span class="ln">30605 </span></a>    :attr:`dx` can be used to specify a different constant spacing, and :attr:`x` can be 
<a name="l30606"><span class="ln">30606 </span></a>    used to specify arbitrary spacing along :attr:`dim`. Only one of :attr:`x` or :attr:`dx` should be specified. 
<a name="l30607"><span class="ln">30607 </span></a> 
<a name="l30608"><span class="ln">30608 </span></a> 
<a name="l30609"><span class="ln">30609 </span></a>    Assuming :attr:`y` is a one-dimensional tensor with elements :math:`{y_0, y_1, ..., y_n}`, 
<a name="l30610"><span class="ln">30610 </span></a>    the default computation is 
<a name="l30611"><span class="ln">30611 </span></a> 
<a name="l30612"><span class="ln">30612 </span></a>    .. math:: 
<a name="l30613"><span class="ln">30613 </span></a>        \begin{aligned} 
<a name="l30614"><span class="ln">30614 </span></a>            \sum_{i = 1}^{n} \frac{1}{2} (y_i + y_{i-1}) 
<a name="l30615"><span class="ln">30615 </span></a>        \end{aligned} 
<a name="l30616"><span class="ln">30616 </span></a> 
<a name="l30617"><span class="ln">30617 </span></a>    When :attr:`dx` is specified the computation becomes 
<a name="l30618"><span class="ln">30618 </span></a> 
<a name="l30619"><span class="ln">30619 </span></a>    .. math:: 
<a name="l30620"><span class="ln">30620 </span></a>        \begin{aligned} 
<a name="l30621"><span class="ln">30621 </span></a>            \sum_{i = 1}^{n} \frac{\Delta x}{2} (y_i + y_{i-1}) 
<a name="l30622"><span class="ln">30622 </span></a>        \end{aligned} 
<a name="l30623"><span class="ln">30623 </span></a> 
<a name="l30624"><span class="ln">30624 </span></a>    effectively multiplying the result by :attr:`dx`. When :attr:`x` is specified, 
<a name="l30625"><span class="ln">30625 </span></a>    assuming :attr:`x` is also a one-dimensional tensor with 
<a name="l30626"><span class="ln">30626 </span></a>    elements :math:`{x_0, x_1, ..., x_n}`, the computation becomes 
<a name="l30627"><span class="ln">30627 </span></a> 
<a name="l30628"><span class="ln">30628 </span></a>    .. math:: 
<a name="l30629"><span class="ln">30629 </span></a>        \begin{aligned} 
<a name="l30630"><span class="ln">30630 </span></a>            \sum_{i = 1}^{n} \frac{(x_i - x_{i-1})}{2} (y_i + y_{i-1}) 
<a name="l30631"><span class="ln">30631 </span></a>        \end{aligned} 
<a name="l30632"><span class="ln">30632 </span></a> 
<a name="l30633"><span class="ln">30633 </span></a>    When :attr:`x` and :attr:`y` have the same size, the computation is as described above and no broadcasting is needed. 
<a name="l30634"><span class="ln">30634 </span></a>    The broadcasting behavior of this function is as follows when their sizes are different. For both :attr:`x` 
<a name="l30635"><span class="ln">30635 </span></a>    and :attr:`y`, the function computes the difference between consecutive elements along 
<a name="l30636"><span class="ln">30636 </span></a>    dimension :attr:`dim`. This effectively creates two tensors, `x_diff` and `y_diff`, that have 
<a name="l30637"><span class="ln">30637 </span></a>    the same shape as the original tensors except their lengths along the dimension :attr:`dim` is reduced by 1. 
<a name="l30638"><span class="ln">30638 </span></a>    After that, those two tensors are broadcast together to compute final output as part of the trapezoidal rule. 
<a name="l30639"><span class="ln">30639 </span></a>    See the examples below for details. 
<a name="l30640"><span class="ln">30640 </span></a> 
<a name="l30641"><span class="ln">30641 </span></a>    .. note:: 
<a name="l30642"><span class="ln">30642 </span></a>        The trapezoidal rule is a technique for approximating the definite integral of a function 
<a name="l30643"><span class="ln">30643 </span></a>        by averaging its left and right Riemann sums. The approximation becomes more accurate as 
<a name="l30644"><span class="ln">30644 </span></a>        the resolution of the partition increases. 
<a name="l30645"><span class="ln">30645 </span></a> 
<a name="l30646"><span class="ln">30646 </span></a>    Arguments: 
<a name="l30647"><span class="ln">30647 </span></a>        y (Tensor): Values to use when computing the trapezoidal rule. 
<a name="l30648"><span class="ln">30648 </span></a>        x (Tensor): If specified, defines spacing between values as specified above. 
<a name="l30649"><span class="ln">30649 </span></a> 
<a name="l30650"><span class="ln">30650 </span></a>    Keyword arguments: 
<a name="l30651"><span class="ln">30651 </span></a>        dx (float): constant spacing between values. If neither :attr:`x` or :attr:`dx` 
<a name="l30652"><span class="ln">30652 </span></a>            are specified then this defaults to 1. Effectively multiplies the result by its value. 
<a name="l30653"><span class="ln">30653 </span></a>        dim (int): The dimension along which to compute the trapezoidal rule. 
<a name="l30654"><span class="ln">30654 </span></a>            The last (inner-most) dimension by default. 
<a name="l30655"><span class="ln">30655 </span></a> 
<a name="l30656"><span class="ln">30656 </span></a>    Examples:: 
<a name="l30657"><span class="ln">30657 </span></a> 
<a name="l30658"><span class="ln">30658 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule in 1D, spacing is implicitly 1 
<a name="l30659"><span class="ln">30659 </span></a>        &gt;&gt;&gt; y = torch.tensor([1, 5, 10]) 
<a name="l30660"><span class="ln">30660 </span></a>        &gt;&gt;&gt; torch.trapezoid(y) 
<a name="l30661"><span class="ln">30661 </span></a>        tensor(10.5) 
<a name="l30662"><span class="ln">30662 </span></a> 
<a name="l30663"><span class="ln">30663 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly to verify 
<a name="l30664"><span class="ln">30664 </span></a>        &gt;&gt;&gt; (1 + 10 + 10) / 2 
<a name="l30665"><span class="ln">30665 </span></a>        10.5 
<a name="l30666"><span class="ln">30666 </span></a> 
<a name="l30667"><span class="ln">30667 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule in 1D with constant spacing of 2 
<a name="l30668"><span class="ln">30668 </span></a>        &gt;&gt;&gt; # NOTE: the result is the same as before, but multiplied by 2 
<a name="l30669"><span class="ln">30669 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, dx=2) 
<a name="l30670"><span class="ln">30670 </span></a>        21.0 
<a name="l30671"><span class="ln">30671 </span></a> 
<a name="l30672"><span class="ln">30672 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule in 1D with arbitrary spacing 
<a name="l30673"><span class="ln">30673 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l30674"><span class="ln">30674 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l30675"><span class="ln">30675 </span></a>        28.5 
<a name="l30676"><span class="ln">30676 </span></a> 
<a name="l30677"><span class="ln">30677 </span></a>        &gt;&gt;&gt; # Computes the same trapezoidal rule directly to verify 
<a name="l30678"><span class="ln">30678 </span></a>        &gt;&gt;&gt; ((3 - 1) * (1 + 5) + (6 - 3) * (5 + 10)) / 2 
<a name="l30679"><span class="ln">30679 </span></a>        28.5 
<a name="l30680"><span class="ln">30680 </span></a> 
<a name="l30681"><span class="ln">30681 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 matrix 
<a name="l30682"><span class="ln">30682 </span></a>        &gt;&gt;&gt; y = torch.arange(9).reshape(3, 3) 
<a name="l30683"><span class="ln">30683 </span></a>        tensor([[0, 1, 2], 
<a name="l30684"><span class="ln">30684 </span></a>                [3, 4, 5], 
<a name="l30685"><span class="ln">30685 </span></a>                [6, 7, 8]]) 
<a name="l30686"><span class="ln">30686 </span></a>        &gt;&gt;&gt; torch.trapezoid(y) 
<a name="l30687"><span class="ln">30687 </span></a>        tensor([ 2., 8., 14.]) 
<a name="l30688"><span class="ln">30688 </span></a> 
<a name="l30689"><span class="ln">30689 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each column of the matrix 
<a name="l30690"><span class="ln">30690 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, dim=0) 
<a name="l30691"><span class="ln">30691 </span></a>        tensor([ 6., 8., 10.]) 
<a name="l30692"><span class="ln">30692 </span></a> 
<a name="l30693"><span class="ln">30693 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l30694"><span class="ln">30694 </span></a>        &gt;&gt;&gt; #   with the same arbitrary spacing 
<a name="l30695"><span class="ln">30695 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l30696"><span class="ln">30696 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 3, 6]) 
<a name="l30697"><span class="ln">30697 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l30698"><span class="ln">30698 </span></a>        array([5., 5., 5.]) 
<a name="l30699"><span class="ln">30699 </span></a> 
<a name="l30700"><span class="ln">30700 </span></a>        &gt;&gt;&gt; # Computes the trapezoidal rule for each row of a 3x3 ones matrix 
<a name="l30701"><span class="ln">30701 </span></a>        &gt;&gt;&gt; #   with different arbitrary spacing per row 
<a name="l30702"><span class="ln">30702 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 3) 
<a name="l30703"><span class="ln">30703 </span></a>        &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [1, 3, 5], [1, 4, 7]]) 
<a name="l30704"><span class="ln">30704 </span></a>        &gt;&gt;&gt; torch.trapezoid(y, x) 
<a name="l30705"><span class="ln">30705 </span></a>        array([2., 4., 6.]) 
<a name="l30706"><span class="ln">30706 </span></a>    &quot;&quot;&quot;</span>
<a name="l30707"><span class="ln">30707 </span></a>
<a name="l30708"><span class="ln">30708 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30709"><span class="ln">30709 </span></a><span class="s2">def </span><span class="s1">trapz</span><span class="s3">(</span><span class="s1">y</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">dx</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30710"><span class="ln">30710 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30711"><span class="ln">30711 </span></a>    trapz(y, x, *, dim=-1) -&gt; Tensor 
<a name="l30712"><span class="ln">30712 </span></a> 
<a name="l30713"><span class="ln">30713 </span></a>    Alias for :func:`torch.trapezoid`. 
<a name="l30714"><span class="ln">30714 </span></a>    &quot;&quot;&quot;</span>
<a name="l30715"><span class="ln">30715 </span></a>
<a name="l30716"><span class="ln">30716 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l30717"><span class="ln">30717 </span></a><span class="s2">def </span><span class="s1">trapz</span><span class="s3">(</span><span class="s1">y</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">x</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= -</span><span class="s5">1</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30718"><span class="ln">30718 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30719"><span class="ln">30719 </span></a>    trapz(y, x, *, dim=-1) -&gt; Tensor 
<a name="l30720"><span class="ln">30720 </span></a> 
<a name="l30721"><span class="ln">30721 </span></a>    Alias for :func:`torch.trapezoid`. 
<a name="l30722"><span class="ln">30722 </span></a>    &quot;&quot;&quot;</span>
<a name="l30723"><span class="ln">30723 </span></a>
<a name="l30724"><span class="ln">30724 </span></a><span class="s2">def </span><span class="s1">triangular_solve</span><span class="s3">(</span>
<a name="l30725"><span class="ln">30725 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30726"><span class="ln">30726 </span></a>    <span class="s1">A</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30727"><span class="ln">30727 </span></a>    <span class="s1">upper</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l30728"><span class="ln">30728 </span></a>    <span class="s1">transpose</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l30729"><span class="ln">30729 </span></a>    <span class="s1">unitriangular</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l30730"><span class="ln">30730 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30731"><span class="ln">30731 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30732"><span class="ln">30732 </span></a><span class="s3">) </span><span class="s1">-&gt; torch</span><span class="s3">.</span><span class="s1">return_types</span><span class="s3">.</span><span class="s1">triangular_solve</span><span class="s2">:</span>
<a name="l30733"><span class="ln">30733 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30734"><span class="ln">30734 </span></a>    triangular_solve(b, A, upper=True, transpose=False, unitriangular=False, *, out=None) -&gt; (Tensor, Tensor) 
<a name="l30735"><span class="ln">30735 </span></a> 
<a name="l30736"><span class="ln">30736 </span></a>    Solves a system of equations with a square upper or lower triangular invertible matrix :math:`A` 
<a name="l30737"><span class="ln">30737 </span></a>    and multiple right-hand sides :math:`b`. 
<a name="l30738"><span class="ln">30738 </span></a> 
<a name="l30739"><span class="ln">30739 </span></a>    In symbols, it solves :math:`AX = b` and assumes :math:`A` is square upper-triangular 
<a name="l30740"><span class="ln">30740 </span></a>    (or lower-triangular if :attr:`upper`\ `= False`) and does not have zeros on the diagonal. 
<a name="l30741"><span class="ln">30741 </span></a> 
<a name="l30742"><span class="ln">30742 </span></a>    `torch.triangular_solve(b, A)` can take in 2D inputs `b, A` or inputs that are 
<a name="l30743"><span class="ln">30743 </span></a>    batches of 2D matrices. If the inputs are batches, then returns 
<a name="l30744"><span class="ln">30744 </span></a>    batched outputs `X` 
<a name="l30745"><span class="ln">30745 </span></a> 
<a name="l30746"><span class="ln">30746 </span></a>    If the diagonal of :attr:`A` contains zeros or elements that are very close to zero and 
<a name="l30747"><span class="ln">30747 </span></a>    :attr:`unitriangular`\ `= False` (default) or if the input matrix is badly conditioned, 
<a name="l30748"><span class="ln">30748 </span></a>    the result may contain `NaN` s. 
<a name="l30749"><span class="ln">30749 </span></a> 
<a name="l30750"><span class="ln">30750 </span></a>    Supports input of float, double, cfloat and cdouble data types. 
<a name="l30751"><span class="ln">30751 </span></a> 
<a name="l30752"><span class="ln">30752 </span></a>    .. warning:: 
<a name="l30753"><span class="ln">30753 </span></a> 
<a name="l30754"><span class="ln">30754 </span></a>        :func:`torch.triangular_solve` is deprecated in favor of :func:`torch.linalg.solve_triangular` 
<a name="l30755"><span class="ln">30755 </span></a>        and will be removed in a future PyTorch release. 
<a name="l30756"><span class="ln">30756 </span></a>        :func:`torch.linalg.solve_triangular` has its arguments reversed and does not return a 
<a name="l30757"><span class="ln">30757 </span></a>        copy of one of the inputs. 
<a name="l30758"><span class="ln">30758 </span></a> 
<a name="l30759"><span class="ln">30759 </span></a>        ``X = torch.triangular_solve(B, A).solution`` should be replaced with 
<a name="l30760"><span class="ln">30760 </span></a> 
<a name="l30761"><span class="ln">30761 </span></a>        .. code:: python 
<a name="l30762"><span class="ln">30762 </span></a> 
<a name="l30763"><span class="ln">30763 </span></a>            X = torch.linalg.solve_triangular(A, B) 
<a name="l30764"><span class="ln">30764 </span></a> 
<a name="l30765"><span class="ln">30765 </span></a>    Args: 
<a name="l30766"><span class="ln">30766 </span></a>        b (Tensor): multiple right-hand sides of size :math:`(*, m, k)` where 
<a name="l30767"><span class="ln">30767 </span></a>                    :math:`*` is zero of more batch dimensions 
<a name="l30768"><span class="ln">30768 </span></a>        A (Tensor): the input triangular coefficient matrix of size :math:`(*, m, m)` 
<a name="l30769"><span class="ln">30769 </span></a>                    where :math:`*` is zero or more batch dimensions 
<a name="l30770"><span class="ln">30770 </span></a>        upper (bool, optional): whether :math:`A` is upper or lower triangular. Default: ``True``. 
<a name="l30771"><span class="ln">30771 </span></a>        transpose (bool, optional): solves `op(A)X = b` where `op(A) = A^T` if this flag is ``True``, 
<a name="l30772"><span class="ln">30772 </span></a>                                    and `op(A) = A` if it is ``False``. Default: ``False``. 
<a name="l30773"><span class="ln">30773 </span></a>        unitriangular (bool, optional): whether :math:`A` is unit triangular. 
<a name="l30774"><span class="ln">30774 </span></a>            If True, the diagonal elements of :math:`A` are assumed to be 
<a name="l30775"><span class="ln">30775 </span></a>            1 and not referenced from :math:`A`. Default: ``False``. 
<a name="l30776"><span class="ln">30776 </span></a> 
<a name="l30777"><span class="ln">30777 </span></a>    Keyword args: 
<a name="l30778"><span class="ln">30778 </span></a>        out ((Tensor, Tensor), optional): tuple of two tensors to write 
<a name="l30779"><span class="ln">30779 </span></a>            the output to. Ignored if `None`. Default: `None`. 
<a name="l30780"><span class="ln">30780 </span></a> 
<a name="l30781"><span class="ln">30781 </span></a>    Returns: 
<a name="l30782"><span class="ln">30782 </span></a>        A namedtuple `(solution, cloned_coefficient)` where `cloned_coefficient` 
<a name="l30783"><span class="ln">30783 </span></a>        is a clone of :math:`A` and `solution` is the solution :math:`X` to :math:`AX = b` 
<a name="l30784"><span class="ln">30784 </span></a>        (or whatever variant of the system of equations, depending on the keyword arguments.) 
<a name="l30785"><span class="ln">30785 </span></a> 
<a name="l30786"><span class="ln">30786 </span></a>    Examples:: 
<a name="l30787"><span class="ln">30787 </span></a> 
<a name="l30788"><span class="ln">30788 </span></a>        &gt;&gt;&gt; A = torch.randn(2, 2).triu() 
<a name="l30789"><span class="ln">30789 </span></a>        &gt;&gt;&gt; A 
<a name="l30790"><span class="ln">30790 </span></a>        tensor([[ 1.1527, -1.0753], 
<a name="l30791"><span class="ln">30791 </span></a>                [ 0.0000,  0.7986]]) 
<a name="l30792"><span class="ln">30792 </span></a>        &gt;&gt;&gt; b = torch.randn(2, 3) 
<a name="l30793"><span class="ln">30793 </span></a>        &gt;&gt;&gt; b 
<a name="l30794"><span class="ln">30794 </span></a>        tensor([[-0.0210,  2.3513, -1.5492], 
<a name="l30795"><span class="ln">30795 </span></a>                [ 1.5429,  0.7403, -1.0243]]) 
<a name="l30796"><span class="ln">30796 </span></a>        &gt;&gt;&gt; torch.triangular_solve(b, A) 
<a name="l30797"><span class="ln">30797 </span></a>        torch.return_types.triangular_solve( 
<a name="l30798"><span class="ln">30798 </span></a>        solution=tensor([[ 1.7841,  2.9046, -2.5405], 
<a name="l30799"><span class="ln">30799 </span></a>                [ 1.9320,  0.9270, -1.2826]]), 
<a name="l30800"><span class="ln">30800 </span></a>        cloned_coefficient=tensor([[ 1.1527, -1.0753], 
<a name="l30801"><span class="ln">30801 </span></a>                [ 0.0000,  0.7986]])) 
<a name="l30802"><span class="ln">30802 </span></a>    &quot;&quot;&quot;</span>
<a name="l30803"><span class="ln">30803 </span></a>
<a name="l30804"><span class="ln">30804 </span></a><span class="s2">def </span><span class="s1">tril</span><span class="s3">(</span>
<a name="l30805"><span class="ln">30805 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30806"><span class="ln">30806 </span></a>    <span class="s1">diagonal</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l30807"><span class="ln">30807 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30808"><span class="ln">30808 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30809"><span class="ln">30809 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30810"><span class="ln">30810 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30811"><span class="ln">30811 </span></a>    tril(input, diagonal=0, *, out=None) -&gt; Tensor 
<a name="l30812"><span class="ln">30812 </span></a> 
<a name="l30813"><span class="ln">30813 </span></a>    Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices 
<a name="l30814"><span class="ln">30814 </span></a>    :attr:`input`, the other elements of the result tensor :attr:`out` are set to 0. 
<a name="l30815"><span class="ln">30815 </span></a> 
<a name="l30816"><span class="ln">30816 </span></a>    The lower triangular part of the matrix is defined as the elements on and 
<a name="l30817"><span class="ln">30817 </span></a>    below the diagonal. 
<a name="l30818"><span class="ln">30818 </span></a> 
<a name="l30819"><span class="ln">30819 </span></a>    The argument :attr:`diagonal` controls which diagonal to consider. If 
<a name="l30820"><span class="ln">30820 </span></a>    :attr:`diagonal` = 0, all elements on and below the main diagonal are 
<a name="l30821"><span class="ln">30821 </span></a>    retained. A positive value includes just as many diagonals above the main 
<a name="l30822"><span class="ln">30822 </span></a>    diagonal, and similarly a negative value excludes just as many diagonals below 
<a name="l30823"><span class="ln">30823 </span></a>    the main diagonal. The main diagonal are the set of indices 
<a name="l30824"><span class="ln">30824 </span></a>    :math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where 
<a name="l30825"><span class="ln">30825 </span></a>    :math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l30826"><span class="ln">30826 </span></a> 
<a name="l30827"><span class="ln">30827 </span></a>    Args: 
<a name="l30828"><span class="ln">30828 </span></a>        input (Tensor): the input tensor. 
<a name="l30829"><span class="ln">30829 </span></a>        diagonal (int, optional): the diagonal to consider 
<a name="l30830"><span class="ln">30830 </span></a> 
<a name="l30831"><span class="ln">30831 </span></a>    Keyword args: 
<a name="l30832"><span class="ln">30832 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l30833"><span class="ln">30833 </span></a> 
<a name="l30834"><span class="ln">30834 </span></a>    Example:: 
<a name="l30835"><span class="ln">30835 </span></a> 
<a name="l30836"><span class="ln">30836 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l30837"><span class="ln">30837 </span></a>        &gt;&gt;&gt; a 
<a name="l30838"><span class="ln">30838 </span></a>        tensor([[-1.0813, -0.8619,  0.7105], 
<a name="l30839"><span class="ln">30839 </span></a>                [ 0.0935,  0.1380,  2.2112], 
<a name="l30840"><span class="ln">30840 </span></a>                [-0.3409, -0.9828,  0.0289]]) 
<a name="l30841"><span class="ln">30841 </span></a>        &gt;&gt;&gt; torch.tril(a) 
<a name="l30842"><span class="ln">30842 </span></a>        tensor([[-1.0813,  0.0000,  0.0000], 
<a name="l30843"><span class="ln">30843 </span></a>                [ 0.0935,  0.1380,  0.0000], 
<a name="l30844"><span class="ln">30844 </span></a>                [-0.3409, -0.9828,  0.0289]]) 
<a name="l30845"><span class="ln">30845 </span></a> 
<a name="l30846"><span class="ln">30846 </span></a>        &gt;&gt;&gt; b = torch.randn(4, 6) 
<a name="l30847"><span class="ln">30847 </span></a>        &gt;&gt;&gt; b 
<a name="l30848"><span class="ln">30848 </span></a>        tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461], 
<a name="l30849"><span class="ln">30849 </span></a>                [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145], 
<a name="l30850"><span class="ln">30850 </span></a>                [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864], 
<a name="l30851"><span class="ln">30851 </span></a>                [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]]) 
<a name="l30852"><span class="ln">30852 </span></a>        &gt;&gt;&gt; torch.tril(b, diagonal=1) 
<a name="l30853"><span class="ln">30853 </span></a>        tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l30854"><span class="ln">30854 </span></a>                [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000], 
<a name="l30855"><span class="ln">30855 </span></a>                [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000], 
<a name="l30856"><span class="ln">30856 </span></a>                [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]]) 
<a name="l30857"><span class="ln">30857 </span></a>        &gt;&gt;&gt; torch.tril(b, diagonal=-1) 
<a name="l30858"><span class="ln">30858 </span></a>        tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l30859"><span class="ln">30859 </span></a>                [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l30860"><span class="ln">30860 </span></a>                [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000], 
<a name="l30861"><span class="ln">30861 </span></a>                [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]]) 
<a name="l30862"><span class="ln">30862 </span></a>    &quot;&quot;&quot;</span>
<a name="l30863"><span class="ln">30863 </span></a>
<a name="l30864"><span class="ln">30864 </span></a><span class="s2">def </span><span class="s1">tril_indices</span><span class="s3">(</span>
<a name="l30865"><span class="ln">30865 </span></a>    <span class="s1">row</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l30866"><span class="ln">30866 </span></a>    <span class="s1">col</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l30867"><span class="ln">30867 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l30868"><span class="ln">30868 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30869"><span class="ln">30869 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30870"><span class="ln">30870 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30871"><span class="ln">30871 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30872"><span class="ln">30872 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l30873"><span class="ln">30873 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l30874"><span class="ln">30874 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30875"><span class="ln">30875 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30876"><span class="ln">30876 </span></a>    tril_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -&gt; Tensor 
<a name="l30877"><span class="ln">30877 </span></a> 
<a name="l30878"><span class="ln">30878 </span></a>    Returns the indices of the lower triangular part of a :attr:`row`-by- 
<a name="l30879"><span class="ln">30879 </span></a>    :attr:`col` matrix in a 2-by-N Tensor, where the first row contains row 
<a name="l30880"><span class="ln">30880 </span></a>    coordinates of all indices and the second row contains column coordinates. 
<a name="l30881"><span class="ln">30881 </span></a>    Indices are ordered based on rows and then columns. 
<a name="l30882"><span class="ln">30882 </span></a> 
<a name="l30883"><span class="ln">30883 </span></a>    The lower triangular part of the matrix is defined as the elements on and 
<a name="l30884"><span class="ln">30884 </span></a>    below the diagonal. 
<a name="l30885"><span class="ln">30885 </span></a> 
<a name="l30886"><span class="ln">30886 </span></a>    The argument :attr:`offset` controls which diagonal to consider. If 
<a name="l30887"><span class="ln">30887 </span></a>    :attr:`offset` = 0, all elements on and below the main diagonal are 
<a name="l30888"><span class="ln">30888 </span></a>    retained. A positive value includes just as many diagonals above the main 
<a name="l30889"><span class="ln">30889 </span></a>    diagonal, and similarly a negative value excludes just as many diagonals below 
<a name="l30890"><span class="ln">30890 </span></a>    the main diagonal. The main diagonal are the set of indices 
<a name="l30891"><span class="ln">30891 </span></a>    :math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` 
<a name="l30892"><span class="ln">30892 </span></a>    where :math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l30893"><span class="ln">30893 </span></a> 
<a name="l30894"><span class="ln">30894 </span></a>    .. note:: 
<a name="l30895"><span class="ln">30895 </span></a>        When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to 
<a name="l30896"><span class="ln">30896 </span></a>        prevent overflow during calculation. 
<a name="l30897"><span class="ln">30897 </span></a> 
<a name="l30898"><span class="ln">30898 </span></a>    Args: 
<a name="l30899"><span class="ln">30899 </span></a>        row (``int``): number of rows in the 2-D matrix. 
<a name="l30900"><span class="ln">30900 </span></a>        col (``int``): number of columns in the 2-D matrix. 
<a name="l30901"><span class="ln">30901 </span></a>        offset (``int``): diagonal offset from the main diagonal. 
<a name="l30902"><span class="ln">30902 </span></a>            Default: if not provided, 0. 
<a name="l30903"><span class="ln">30903 </span></a> 
<a name="l30904"><span class="ln">30904 </span></a>    Keyword args: 
<a name="l30905"><span class="ln">30905 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor, 
<a name="l30906"><span class="ln">30906 </span></a>            only support ``torch.int``, ``torch.long``. Default: if ``None``, ``torch.long``. 
<a name="l30907"><span class="ln">30907 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l30908"><span class="ln">30908 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l30909"><span class="ln">30909 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l30910"><span class="ln">30910 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l30911"><span class="ln">30911 </span></a>        layout (:class:`torch.layout`, optional): currently only support ``torch.strided``. 
<a name="l30912"><span class="ln">30912 </span></a> 
<a name="l30913"><span class="ln">30913 </span></a>    Example:: 
<a name="l30914"><span class="ln">30914 </span></a> 
<a name="l30915"><span class="ln">30915 </span></a>        &gt;&gt;&gt; a = torch.tril_indices(3, 3) 
<a name="l30916"><span class="ln">30916 </span></a>        &gt;&gt;&gt; a 
<a name="l30917"><span class="ln">30917 </span></a>        tensor([[0, 1, 1, 2, 2, 2], 
<a name="l30918"><span class="ln">30918 </span></a>                [0, 0, 1, 0, 1, 2]]) 
<a name="l30919"><span class="ln">30919 </span></a> 
<a name="l30920"><span class="ln">30920 </span></a>        &gt;&gt;&gt; a = torch.tril_indices(4, 3, -1) 
<a name="l30921"><span class="ln">30921 </span></a>        &gt;&gt;&gt; a 
<a name="l30922"><span class="ln">30922 </span></a>        tensor([[1, 2, 2, 3, 3, 3], 
<a name="l30923"><span class="ln">30923 </span></a>                [0, 0, 1, 0, 1, 2]]) 
<a name="l30924"><span class="ln">30924 </span></a> 
<a name="l30925"><span class="ln">30925 </span></a>        &gt;&gt;&gt; a = torch.tril_indices(4, 3, 1) 
<a name="l30926"><span class="ln">30926 </span></a>        &gt;&gt;&gt; a 
<a name="l30927"><span class="ln">30927 </span></a>        tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], 
<a name="l30928"><span class="ln">30928 </span></a>                [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]]) 
<a name="l30929"><span class="ln">30929 </span></a>    &quot;&quot;&quot;</span>
<a name="l30930"><span class="ln">30930 </span></a>
<a name="l30931"><span class="ln">30931 </span></a><span class="s2">def </span><span class="s1">triplet_margin_loss</span><span class="s3">(</span>
<a name="l30932"><span class="ln">30932 </span></a>    <span class="s1">anchor</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30933"><span class="ln">30933 </span></a>    <span class="s1">positive</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30934"><span class="ln">30934 </span></a>    <span class="s1">negative</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30935"><span class="ln">30935 </span></a>    <span class="s1">margin</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1.0</span><span class="s3">,</span>
<a name="l30936"><span class="ln">30936 </span></a>    <span class="s1">p</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">2</span><span class="s3">,</span>
<a name="l30937"><span class="ln">30937 </span></a>    <span class="s1">eps</span><span class="s2">: </span><span class="s1">_float </span><span class="s2">= </span><span class="s5">1e-06</span><span class="s3">,</span>
<a name="l30938"><span class="ln">30938 </span></a>    <span class="s1">swap</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l30939"><span class="ln">30939 </span></a>    <span class="s1">reduction</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">1</span><span class="s3">,</span>
<a name="l30940"><span class="ln">30940 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l30941"><span class="ln">30941 </span></a><span class="s2">def </span><span class="s1">triu</span><span class="s3">(</span>
<a name="l30942"><span class="ln">30942 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l30943"><span class="ln">30943 </span></a>    <span class="s1">diagonal</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l30944"><span class="ln">30944 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l30945"><span class="ln">30945 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l30946"><span class="ln">30946 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l30947"><span class="ln">30947 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l30948"><span class="ln">30948 </span></a>    triu(input, diagonal=0, *, out=None) -&gt; Tensor 
<a name="l30949"><span class="ln">30949 </span></a> 
<a name="l30950"><span class="ln">30950 </span></a>    Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices 
<a name="l30951"><span class="ln">30951 </span></a>    :attr:`input`, the other elements of the result tensor :attr:`out` are set to 0. 
<a name="l30952"><span class="ln">30952 </span></a> 
<a name="l30953"><span class="ln">30953 </span></a>    The upper triangular part of the matrix is defined as the elements on and 
<a name="l30954"><span class="ln">30954 </span></a>    above the diagonal. 
<a name="l30955"><span class="ln">30955 </span></a> 
<a name="l30956"><span class="ln">30956 </span></a>    The argument :attr:`diagonal` controls which diagonal to consider. If 
<a name="l30957"><span class="ln">30957 </span></a>    :attr:`diagonal` = 0, all elements on and above the main diagonal are 
<a name="l30958"><span class="ln">30958 </span></a>    retained. A positive value excludes just as many diagonals above the main 
<a name="l30959"><span class="ln">30959 </span></a>    diagonal, and similarly a negative value includes just as many diagonals below 
<a name="l30960"><span class="ln">30960 </span></a>    the main diagonal. The main diagonal are the set of indices 
<a name="l30961"><span class="ln">30961 </span></a>    :math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where 
<a name="l30962"><span class="ln">30962 </span></a>    :math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l30963"><span class="ln">30963 </span></a> 
<a name="l30964"><span class="ln">30964 </span></a>    Args: 
<a name="l30965"><span class="ln">30965 </span></a>        input (Tensor): the input tensor. 
<a name="l30966"><span class="ln">30966 </span></a>        diagonal (int, optional): the diagonal to consider 
<a name="l30967"><span class="ln">30967 </span></a> 
<a name="l30968"><span class="ln">30968 </span></a>    Keyword args: 
<a name="l30969"><span class="ln">30969 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l30970"><span class="ln">30970 </span></a> 
<a name="l30971"><span class="ln">30971 </span></a>    Example:: 
<a name="l30972"><span class="ln">30972 </span></a> 
<a name="l30973"><span class="ln">30973 </span></a>        &gt;&gt;&gt; a = torch.randn(3, 3) 
<a name="l30974"><span class="ln">30974 </span></a>        &gt;&gt;&gt; a 
<a name="l30975"><span class="ln">30975 </span></a>        tensor([[ 0.2309,  0.5207,  2.0049], 
<a name="l30976"><span class="ln">30976 </span></a>                [ 0.2072, -1.0680,  0.6602], 
<a name="l30977"><span class="ln">30977 </span></a>                [ 0.3480, -0.5211, -0.4573]]) 
<a name="l30978"><span class="ln">30978 </span></a>        &gt;&gt;&gt; torch.triu(a) 
<a name="l30979"><span class="ln">30979 </span></a>        tensor([[ 0.2309,  0.5207,  2.0049], 
<a name="l30980"><span class="ln">30980 </span></a>                [ 0.0000, -1.0680,  0.6602], 
<a name="l30981"><span class="ln">30981 </span></a>                [ 0.0000,  0.0000, -0.4573]]) 
<a name="l30982"><span class="ln">30982 </span></a>        &gt;&gt;&gt; torch.triu(a, diagonal=1) 
<a name="l30983"><span class="ln">30983 </span></a>        tensor([[ 0.0000,  0.5207,  2.0049], 
<a name="l30984"><span class="ln">30984 </span></a>                [ 0.0000,  0.0000,  0.6602], 
<a name="l30985"><span class="ln">30985 </span></a>                [ 0.0000,  0.0000,  0.0000]]) 
<a name="l30986"><span class="ln">30986 </span></a>        &gt;&gt;&gt; torch.triu(a, diagonal=-1) 
<a name="l30987"><span class="ln">30987 </span></a>        tensor([[ 0.2309,  0.5207,  2.0049], 
<a name="l30988"><span class="ln">30988 </span></a>                [ 0.2072, -1.0680,  0.6602], 
<a name="l30989"><span class="ln">30989 </span></a>                [ 0.0000, -0.5211, -0.4573]]) 
<a name="l30990"><span class="ln">30990 </span></a> 
<a name="l30991"><span class="ln">30991 </span></a>        &gt;&gt;&gt; b = torch.randn(4, 6) 
<a name="l30992"><span class="ln">30992 </span></a>        &gt;&gt;&gt; b 
<a name="l30993"><span class="ln">30993 </span></a>        tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235], 
<a name="l30994"><span class="ln">30994 </span></a>                [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857], 
<a name="l30995"><span class="ln">30995 </span></a>                [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410], 
<a name="l30996"><span class="ln">30996 </span></a>                [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]]) 
<a name="l30997"><span class="ln">30997 </span></a>        &gt;&gt;&gt; torch.triu(b, diagonal=1) 
<a name="l30998"><span class="ln">30998 </span></a>        tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235], 
<a name="l30999"><span class="ln">30999 </span></a>                [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857], 
<a name="l31000"><span class="ln">31000 </span></a>                [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410], 
<a name="l31001"><span class="ln">31001 </span></a>                [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]]) 
<a name="l31002"><span class="ln">31002 </span></a>        &gt;&gt;&gt; torch.triu(b, diagonal=-1) 
<a name="l31003"><span class="ln">31003 </span></a>        tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235], 
<a name="l31004"><span class="ln">31004 </span></a>                [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857], 
<a name="l31005"><span class="ln">31005 </span></a>                [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410], 
<a name="l31006"><span class="ln">31006 </span></a>                [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]]) 
<a name="l31007"><span class="ln">31007 </span></a>    &quot;&quot;&quot;</span>
<a name="l31008"><span class="ln">31008 </span></a>
<a name="l31009"><span class="ln">31009 </span></a><span class="s2">def </span><span class="s1">triu_indices</span><span class="s3">(</span>
<a name="l31010"><span class="ln">31010 </span></a>    <span class="s1">row</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31011"><span class="ln">31011 </span></a>    <span class="s1">col</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31012"><span class="ln">31012 </span></a>    <span class="s1">offset</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l31013"><span class="ln">31013 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31014"><span class="ln">31014 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31015"><span class="ln">31015 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31016"><span class="ln">31016 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31017"><span class="ln">31017 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l31018"><span class="ln">31018 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l31019"><span class="ln">31019 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31020"><span class="ln">31020 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31021"><span class="ln">31021 </span></a>    triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -&gt; Tensor 
<a name="l31022"><span class="ln">31022 </span></a> 
<a name="l31023"><span class="ln">31023 </span></a>    Returns the indices of the upper triangular part of a :attr:`row` by 
<a name="l31024"><span class="ln">31024 </span></a>    :attr:`col` matrix in a 2-by-N Tensor, where the first row contains row 
<a name="l31025"><span class="ln">31025 </span></a>    coordinates of all indices and the second row contains column coordinates. 
<a name="l31026"><span class="ln">31026 </span></a>    Indices are ordered based on rows and then columns. 
<a name="l31027"><span class="ln">31027 </span></a> 
<a name="l31028"><span class="ln">31028 </span></a>    The upper triangular part of the matrix is defined as the elements on and 
<a name="l31029"><span class="ln">31029 </span></a>    above the diagonal. 
<a name="l31030"><span class="ln">31030 </span></a> 
<a name="l31031"><span class="ln">31031 </span></a>    The argument :attr:`offset` controls which diagonal to consider. If 
<a name="l31032"><span class="ln">31032 </span></a>    :attr:`offset` = 0, all elements on and above the main diagonal are 
<a name="l31033"><span class="ln">31033 </span></a>    retained. A positive value excludes just as many diagonals above the main 
<a name="l31034"><span class="ln">31034 </span></a>    diagonal, and similarly a negative value includes just as many diagonals below 
<a name="l31035"><span class="ln">31035 </span></a>    the main diagonal. The main diagonal are the set of indices 
<a name="l31036"><span class="ln">31036 </span></a>    :math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` 
<a name="l31037"><span class="ln">31037 </span></a>    where :math:`d_{1}, d_{2}` are the dimensions of the matrix. 
<a name="l31038"><span class="ln">31038 </span></a> 
<a name="l31039"><span class="ln">31039 </span></a>    .. note:: 
<a name="l31040"><span class="ln">31040 </span></a>        When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to 
<a name="l31041"><span class="ln">31041 </span></a>        prevent overflow during calculation. 
<a name="l31042"><span class="ln">31042 </span></a> 
<a name="l31043"><span class="ln">31043 </span></a>    Args: 
<a name="l31044"><span class="ln">31044 </span></a>        row (``int``): number of rows in the 2-D matrix. 
<a name="l31045"><span class="ln">31045 </span></a>        col (``int``): number of columns in the 2-D matrix. 
<a name="l31046"><span class="ln">31046 </span></a>        offset (``int``): diagonal offset from the main diagonal. 
<a name="l31047"><span class="ln">31047 </span></a>            Default: if not provided, 0. 
<a name="l31048"><span class="ln">31048 </span></a> 
<a name="l31049"><span class="ln">31049 </span></a>    Keyword args: 
<a name="l31050"><span class="ln">31050 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor, 
<a name="l31051"><span class="ln">31051 </span></a>            only support ``torch.int``, ``torch.long``. Default: if ``None``, ``torch.long``. 
<a name="l31052"><span class="ln">31052 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l31053"><span class="ln">31053 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l31054"><span class="ln">31054 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l31055"><span class="ln">31055 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l31056"><span class="ln">31056 </span></a>        layout (:class:`torch.layout`, optional): currently only support ``torch.strided``. 
<a name="l31057"><span class="ln">31057 </span></a> 
<a name="l31058"><span class="ln">31058 </span></a>    Example:: 
<a name="l31059"><span class="ln">31059 </span></a> 
<a name="l31060"><span class="ln">31060 </span></a>        &gt;&gt;&gt; a = torch.triu_indices(3, 3) 
<a name="l31061"><span class="ln">31061 </span></a>        &gt;&gt;&gt; a 
<a name="l31062"><span class="ln">31062 </span></a>        tensor([[0, 0, 0, 1, 1, 2], 
<a name="l31063"><span class="ln">31063 </span></a>                [0, 1, 2, 1, 2, 2]]) 
<a name="l31064"><span class="ln">31064 </span></a> 
<a name="l31065"><span class="ln">31065 </span></a>        &gt;&gt;&gt; a = torch.triu_indices(4, 3, -1) 
<a name="l31066"><span class="ln">31066 </span></a>        &gt;&gt;&gt; a 
<a name="l31067"><span class="ln">31067 </span></a>        tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3], 
<a name="l31068"><span class="ln">31068 </span></a>                [0, 1, 2, 0, 1, 2, 1, 2, 2]]) 
<a name="l31069"><span class="ln">31069 </span></a> 
<a name="l31070"><span class="ln">31070 </span></a>        &gt;&gt;&gt; a = torch.triu_indices(4, 3, 1) 
<a name="l31071"><span class="ln">31071 </span></a>        &gt;&gt;&gt; a 
<a name="l31072"><span class="ln">31072 </span></a>        tensor([[0, 0, 1], 
<a name="l31073"><span class="ln">31073 </span></a>                [1, 2, 2]]) 
<a name="l31074"><span class="ln">31074 </span></a>    &quot;&quot;&quot;</span>
<a name="l31075"><span class="ln">31075 </span></a>
<a name="l31076"><span class="ln">31076 </span></a><span class="s2">def </span><span class="s1">true_divide</span><span class="s3">(</span>
<a name="l31077"><span class="ln">31077 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l31078"><span class="ln">31078 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| </span><span class="s1">Number</span><span class="s3">,</span>
<a name="l31079"><span class="ln">31079 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31080"><span class="ln">31080 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31081"><span class="ln">31081 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31082"><span class="ln">31082 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31083"><span class="ln">31083 </span></a>    true_divide(dividend, divisor, *, out) -&gt; Tensor 
<a name="l31084"><span class="ln">31084 </span></a> 
<a name="l31085"><span class="ln">31085 </span></a>    Alias for :func:`torch.div` with ``rounding_mode=None``. 
<a name="l31086"><span class="ln">31086 </span></a>    &quot;&quot;&quot;</span>
<a name="l31087"><span class="ln">31087 </span></a>
<a name="l31088"><span class="ln">31088 </span></a><span class="s2">def </span><span class="s1">trunc</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31089"><span class="ln">31089 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31090"><span class="ln">31090 </span></a>    trunc(input, *, out=None) -&gt; Tensor 
<a name="l31091"><span class="ln">31091 </span></a> 
<a name="l31092"><span class="ln">31092 </span></a>    Returns a new tensor with the truncated integer values of 
<a name="l31093"><span class="ln">31093 </span></a>    the elements of :attr:`input`. 
<a name="l31094"><span class="ln">31094 </span></a> 
<a name="l31095"><span class="ln">31095 </span></a>    For integer inputs, follows the array-api convention of returning a 
<a name="l31096"><span class="ln">31096 </span></a>    copy of the input tensor. 
<a name="l31097"><span class="ln">31097 </span></a> 
<a name="l31098"><span class="ln">31098 </span></a>    Args: 
<a name="l31099"><span class="ln">31099 </span></a>        input (Tensor): the input tensor. 
<a name="l31100"><span class="ln">31100 </span></a> 
<a name="l31101"><span class="ln">31101 </span></a>    Keyword args: 
<a name="l31102"><span class="ln">31102 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31103"><span class="ln">31103 </span></a> 
<a name="l31104"><span class="ln">31104 </span></a>    Example:: 
<a name="l31105"><span class="ln">31105 </span></a> 
<a name="l31106"><span class="ln">31106 </span></a>        &gt;&gt;&gt; a = torch.randn(4) 
<a name="l31107"><span class="ln">31107 </span></a>        &gt;&gt;&gt; a 
<a name="l31108"><span class="ln">31108 </span></a>        tensor([ 3.4742,  0.5466, -0.8008, -0.9079]) 
<a name="l31109"><span class="ln">31109 </span></a>        &gt;&gt;&gt; torch.trunc(a) 
<a name="l31110"><span class="ln">31110 </span></a>        tensor([ 3.,  0., -0., -0.]) 
<a name="l31111"><span class="ln">31111 </span></a>    &quot;&quot;&quot;</span>
<a name="l31112"><span class="ln">31112 </span></a>
<a name="l31113"><span class="ln">31113 </span></a><span class="s2">def </span><span class="s1">trunc_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l31114"><span class="ln">31114 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31115"><span class="ln">31115 </span></a><span class="s2">def </span><span class="s1">unbind</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l31116"><span class="ln">31116 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31117"><span class="ln">31117 </span></a>    unbind(input, dim=0) -&gt; seq 
<a name="l31118"><span class="ln">31118 </span></a> 
<a name="l31119"><span class="ln">31119 </span></a>    Removes a tensor dimension. 
<a name="l31120"><span class="ln">31120 </span></a> 
<a name="l31121"><span class="ln">31121 </span></a>    Returns a tuple of all slices along a given dimension, already without it. 
<a name="l31122"><span class="ln">31122 </span></a> 
<a name="l31123"><span class="ln">31123 </span></a>    Arguments: 
<a name="l31124"><span class="ln">31124 </span></a>        input (Tensor): the tensor to unbind 
<a name="l31125"><span class="ln">31125 </span></a>        dim (int): dimension to remove 
<a name="l31126"><span class="ln">31126 </span></a> 
<a name="l31127"><span class="ln">31127 </span></a>    Example:: 
<a name="l31128"><span class="ln">31128 </span></a> 
<a name="l31129"><span class="ln">31129 </span></a>        &gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3], 
<a name="l31130"><span class="ln">31130 </span></a>        &gt;&gt;&gt;                            [4, 5, 6], 
<a name="l31131"><span class="ln">31131 </span></a>        &gt;&gt;&gt;                            [7, 8, 9]])) 
<a name="l31132"><span class="ln">31132 </span></a>        (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9])) 
<a name="l31133"><span class="ln">31133 </span></a>    &quot;&quot;&quot;</span>
<a name="l31134"><span class="ln">31134 </span></a>
<a name="l31135"><span class="ln">31135 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31136"><span class="ln">31136 </span></a><span class="s2">def </span><span class="s1">unbind</span><span class="s3">(</span>
<a name="l31137"><span class="ln">31137 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31138"><span class="ln">31138 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l31139"><span class="ln">31139 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l31140"><span class="ln">31140 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31141"><span class="ln">31141 </span></a>    unbind(input, dim=0) -&gt; seq 
<a name="l31142"><span class="ln">31142 </span></a> 
<a name="l31143"><span class="ln">31143 </span></a>    Removes a tensor dimension. 
<a name="l31144"><span class="ln">31144 </span></a> 
<a name="l31145"><span class="ln">31145 </span></a>    Returns a tuple of all slices along a given dimension, already without it. 
<a name="l31146"><span class="ln">31146 </span></a> 
<a name="l31147"><span class="ln">31147 </span></a>    Arguments: 
<a name="l31148"><span class="ln">31148 </span></a>        input (Tensor): the tensor to unbind 
<a name="l31149"><span class="ln">31149 </span></a>        dim (int): dimension to remove 
<a name="l31150"><span class="ln">31150 </span></a> 
<a name="l31151"><span class="ln">31151 </span></a>    Example:: 
<a name="l31152"><span class="ln">31152 </span></a> 
<a name="l31153"><span class="ln">31153 </span></a>        &gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3], 
<a name="l31154"><span class="ln">31154 </span></a>        &gt;&gt;&gt;                            [4, 5, 6], 
<a name="l31155"><span class="ln">31155 </span></a>        &gt;&gt;&gt;                            [7, 8, 9]])) 
<a name="l31156"><span class="ln">31156 </span></a>        (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9])) 
<a name="l31157"><span class="ln">31157 </span></a>    &quot;&quot;&quot;</span>
<a name="l31158"><span class="ln">31158 </span></a>
<a name="l31159"><span class="ln">31159 </span></a><span class="s2">def </span><span class="s1">unbind_copy</span><span class="s3">(</span>
<a name="l31160"><span class="ln">31160 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31161"><span class="ln">31161 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l31162"><span class="ln">31162 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31163"><span class="ln">31163 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31164"><span class="ln">31164 </span></a><span class="s3">) </span><span class="s1">-&gt; </span><span class="s2">None:</span>
<a name="l31165"><span class="ln">31165 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31166"><span class="ln">31166 </span></a>    Performs the same operation as :func:`torch.unbind`, but all output tensors 
<a name="l31167"><span class="ln">31167 </span></a>    are freshly created instead of aliasing the input. 
<a name="l31168"><span class="ln">31168 </span></a>    &quot;&quot;&quot;</span>
<a name="l31169"><span class="ln">31169 </span></a>
<a name="l31170"><span class="ln">31170 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31171"><span class="ln">31171 </span></a><span class="s2">def </span><span class="s1">unflatten</span><span class="s3">(</span>
<a name="l31172"><span class="ln">31172 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31173"><span class="ln">31173 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l31174"><span class="ln">31174 </span></a>    <span class="s1">sizes</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l31175"><span class="ln">31175 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l31176"><span class="ln">31176 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31177"><span class="ln">31177 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31178"><span class="ln">31178 </span></a>    unflatten(input, dim, sizes) -&gt; Tensor 
<a name="l31179"><span class="ln">31179 </span></a> 
<a name="l31180"><span class="ln">31180 </span></a>    Expands a dimension of the input tensor over multiple dimensions. 
<a name="l31181"><span class="ln">31181 </span></a> 
<a name="l31182"><span class="ln">31182 </span></a>    .. seealso:: 
<a name="l31183"><span class="ln">31183 </span></a> 
<a name="l31184"><span class="ln">31184 </span></a>        :func:`torch.flatten` the inverse of this function. It coalesces several dimensions into one. 
<a name="l31185"><span class="ln">31185 </span></a> 
<a name="l31186"><span class="ln">31186 </span></a>    Args: 
<a name="l31187"><span class="ln">31187 </span></a>        input (Tensor): the input tensor. 
<a name="l31188"><span class="ln">31188 </span></a>        dim (int): Dimension to be unflattened, specified as an index into 
<a name="l31189"><span class="ln">31189 </span></a>             ``input.shape``. 
<a name="l31190"><span class="ln">31190 </span></a>        sizes (Tuple[int]): New shape of the unflattened dimension. 
<a name="l31191"><span class="ln">31191 </span></a>             One of its elements can be `-1` in which case the corresponding output 
<a name="l31192"><span class="ln">31192 </span></a>             dimension is inferred. Otherwise, the product of ``sizes`` *must* 
<a name="l31193"><span class="ln">31193 </span></a>             equal ``input.shape[dim]``. 
<a name="l31194"><span class="ln">31194 </span></a> 
<a name="l31195"><span class="ln">31195 </span></a>    Returns: 
<a name="l31196"><span class="ln">31196 </span></a>        A View of input with the specified dimension unflattened. 
<a name="l31197"><span class="ln">31197 </span></a> 
<a name="l31198"><span class="ln">31198 </span></a>    Examples:: 
<a name="l31199"><span class="ln">31199 </span></a>        &gt;&gt;&gt; torch.unflatten(torch.randn(3, 4, 1), 1, (2, 2)).shape 
<a name="l31200"><span class="ln">31200 </span></a>        torch.Size([3, 2, 2, 1]) 
<a name="l31201"><span class="ln">31201 </span></a>        &gt;&gt;&gt; torch.unflatten(torch.randn(3, 4, 1), 1, (-1, 2)).shape 
<a name="l31202"><span class="ln">31202 </span></a>        torch.Size([3, 2, 2, 1]) 
<a name="l31203"><span class="ln">31203 </span></a>        &gt;&gt;&gt; torch.unflatten(torch.randn(5, 12, 3), -2, (2, 2, 3, 1, 1)).shape 
<a name="l31204"><span class="ln">31204 </span></a>        torch.Size([5, 2, 2, 3, 1, 1, 3]) 
<a name="l31205"><span class="ln">31205 </span></a>    &quot;&quot;&quot;</span>
<a name="l31206"><span class="ln">31206 </span></a>
<a name="l31207"><span class="ln">31207 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31208"><span class="ln">31208 </span></a><span class="s2">def </span><span class="s1">unflatten</span><span class="s3">(</span>
<a name="l31209"><span class="ln">31209 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31210"><span class="ln">31210 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31211"><span class="ln">31211 </span></a>    <span class="s1">sizes</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l31212"><span class="ln">31212 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31213"><span class="ln">31213 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31214"><span class="ln">31214 </span></a>    unflatten(input, dim, sizes) -&gt; Tensor 
<a name="l31215"><span class="ln">31215 </span></a> 
<a name="l31216"><span class="ln">31216 </span></a>    Expands a dimension of the input tensor over multiple dimensions. 
<a name="l31217"><span class="ln">31217 </span></a> 
<a name="l31218"><span class="ln">31218 </span></a>    .. seealso:: 
<a name="l31219"><span class="ln">31219 </span></a> 
<a name="l31220"><span class="ln">31220 </span></a>        :func:`torch.flatten` the inverse of this function. It coalesces several dimensions into one. 
<a name="l31221"><span class="ln">31221 </span></a> 
<a name="l31222"><span class="ln">31222 </span></a>    Args: 
<a name="l31223"><span class="ln">31223 </span></a>        input (Tensor): the input tensor. 
<a name="l31224"><span class="ln">31224 </span></a>        dim (int): Dimension to be unflattened, specified as an index into 
<a name="l31225"><span class="ln">31225 </span></a>             ``input.shape``. 
<a name="l31226"><span class="ln">31226 </span></a>        sizes (Tuple[int]): New shape of the unflattened dimension. 
<a name="l31227"><span class="ln">31227 </span></a>             One of its elements can be `-1` in which case the corresponding output 
<a name="l31228"><span class="ln">31228 </span></a>             dimension is inferred. Otherwise, the product of ``sizes`` *must* 
<a name="l31229"><span class="ln">31229 </span></a>             equal ``input.shape[dim]``. 
<a name="l31230"><span class="ln">31230 </span></a> 
<a name="l31231"><span class="ln">31231 </span></a>    Returns: 
<a name="l31232"><span class="ln">31232 </span></a>        A View of input with the specified dimension unflattened. 
<a name="l31233"><span class="ln">31233 </span></a> 
<a name="l31234"><span class="ln">31234 </span></a>    Examples:: 
<a name="l31235"><span class="ln">31235 </span></a>        &gt;&gt;&gt; torch.unflatten(torch.randn(3, 4, 1), 1, (2, 2)).shape 
<a name="l31236"><span class="ln">31236 </span></a>        torch.Size([3, 2, 2, 1]) 
<a name="l31237"><span class="ln">31237 </span></a>        &gt;&gt;&gt; torch.unflatten(torch.randn(3, 4, 1), 1, (-1, 2)).shape 
<a name="l31238"><span class="ln">31238 </span></a>        torch.Size([3, 2, 2, 1]) 
<a name="l31239"><span class="ln">31239 </span></a>        &gt;&gt;&gt; torch.unflatten(torch.randn(5, 12, 3), -2, (2, 2, 3, 1, 1)).shape 
<a name="l31240"><span class="ln">31240 </span></a>        torch.Size([5, 2, 2, 3, 1, 1, 3]) 
<a name="l31241"><span class="ln">31241 </span></a>    &quot;&quot;&quot;</span>
<a name="l31242"><span class="ln">31242 </span></a>
<a name="l31243"><span class="ln">31243 </span></a><span class="s2">def </span><span class="s1">unfold_copy</span><span class="s3">(</span>
<a name="l31244"><span class="ln">31244 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31245"><span class="ln">31245 </span></a>    <span class="s1">dimension</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31246"><span class="ln">31246 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31247"><span class="ln">31247 </span></a>    <span class="s1">step</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31248"><span class="ln">31248 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31249"><span class="ln">31249 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31250"><span class="ln">31250 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31251"><span class="ln">31251 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31252"><span class="ln">31252 </span></a>    Performs the same operation as :func:`torch.unfold`, but all output tensors 
<a name="l31253"><span class="ln">31253 </span></a>    are freshly created instead of aliasing the input. 
<a name="l31254"><span class="ln">31254 </span></a>    &quot;&quot;&quot;</span>
<a name="l31255"><span class="ln">31255 </span></a>
<a name="l31256"><span class="ln">31256 </span></a><span class="s2">def </span><span class="s1">unique_dim</span><span class="s3">(</span>
<a name="l31257"><span class="ln">31257 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31258"><span class="ln">31258 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31259"><span class="ln">31259 </span></a>    <span class="s1">sorted</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l31260"><span class="ln">31260 </span></a>    <span class="s1">return_inverse</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31261"><span class="ln">31261 </span></a>    <span class="s1">return_counts</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31262"><span class="ln">31262 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l31263"><span class="ln">31263 </span></a><span class="s2">def </span><span class="s1">unsafe_chunk</span><span class="s3">(</span>
<a name="l31264"><span class="ln">31264 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31265"><span class="ln">31265 </span></a>    <span class="s1">chunks</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31266"><span class="ln">31266 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l31267"><span class="ln">31267 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l31268"><span class="ln">31268 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31269"><span class="ln">31269 </span></a>    unsafe_chunk(input, chunks, dim=0) -&gt; List of Tensors 
<a name="l31270"><span class="ln">31270 </span></a> 
<a name="l31271"><span class="ln">31271 </span></a>    Works like :func:`torch.chunk` but without enforcing the autograd restrictions 
<a name="l31272"><span class="ln">31272 </span></a>    on inplace modification of the outputs. 
<a name="l31273"><span class="ln">31273 </span></a> 
<a name="l31274"><span class="ln">31274 </span></a>    .. warning:: 
<a name="l31275"><span class="ln">31275 </span></a>        This function is safe to use as long as only the input, or only the outputs 
<a name="l31276"><span class="ln">31276 </span></a>        are modified inplace after calling this function. It is user's 
<a name="l31277"><span class="ln">31277 </span></a>        responsibility to ensure that is the case. If both the input and one or more 
<a name="l31278"><span class="ln">31278 </span></a>        of the outputs are modified inplace, gradients computed by autograd will be 
<a name="l31279"><span class="ln">31279 </span></a>        silently incorrect. 
<a name="l31280"><span class="ln">31280 </span></a>    &quot;&quot;&quot;</span>
<a name="l31281"><span class="ln">31281 </span></a>
<a name="l31282"><span class="ln">31282 </span></a><span class="s2">def </span><span class="s1">unsafe_split</span><span class="s3">(</span>
<a name="l31283"><span class="ln">31283 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31284"><span class="ln">31284 </span></a>    <span class="s1">split_size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l31285"><span class="ln">31285 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l31286"><span class="ln">31286 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l31287"><span class="ln">31287 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31288"><span class="ln">31288 </span></a>    unsafe_split(tensor, split_size_or_sections, dim=0) -&gt; List of Tensors 
<a name="l31289"><span class="ln">31289 </span></a> 
<a name="l31290"><span class="ln">31290 </span></a>    Works like :func:`torch.split` but without enforcing the autograd restrictions 
<a name="l31291"><span class="ln">31291 </span></a>    on inplace modification of the outputs. 
<a name="l31292"><span class="ln">31292 </span></a> 
<a name="l31293"><span class="ln">31293 </span></a>    .. warning:: 
<a name="l31294"><span class="ln">31294 </span></a>        This function is safe to use as long as only the input, or only the outputs 
<a name="l31295"><span class="ln">31295 </span></a>        are modified inplace after calling this function. It is user's 
<a name="l31296"><span class="ln">31296 </span></a>        responsibility to ensure that is the case. If both the input and one or more 
<a name="l31297"><span class="ln">31297 </span></a>        of the outputs are modified inplace, gradients computed by autograd will be 
<a name="l31298"><span class="ln">31298 </span></a>        silently incorrect. 
<a name="l31299"><span class="ln">31299 </span></a>    &quot;&quot;&quot;</span>
<a name="l31300"><span class="ln">31300 </span></a>
<a name="l31301"><span class="ln">31301 </span></a><span class="s2">def </span><span class="s1">unsafe_split_with_sizes</span><span class="s3">(</span>
<a name="l31302"><span class="ln">31302 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31303"><span class="ln">31303 </span></a>    <span class="s1">split_sizes</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l31304"><span class="ln">31304 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">= </span><span class="s5">0</span><span class="s3">,</span>
<a name="l31305"><span class="ln">31305 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">: </span><span class="s3">...</span>
<a name="l31306"><span class="ln">31306 </span></a><span class="s2">def </span><span class="s1">unsqueeze</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31307"><span class="ln">31307 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31308"><span class="ln">31308 </span></a>    unsqueeze(input, dim) -&gt; Tensor 
<a name="l31309"><span class="ln">31309 </span></a> 
<a name="l31310"><span class="ln">31310 </span></a>    Returns a new tensor with a dimension of size one inserted at the 
<a name="l31311"><span class="ln">31311 </span></a>    specified position. 
<a name="l31312"><span class="ln">31312 </span></a> 
<a name="l31313"><span class="ln">31313 </span></a>    The returned tensor shares the same underlying data with this tensor. 
<a name="l31314"><span class="ln">31314 </span></a> 
<a name="l31315"><span class="ln">31315 </span></a>    A :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)`` 
<a name="l31316"><span class="ln">31316 </span></a>    can be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze` 
<a name="l31317"><span class="ln">31317 </span></a>    applied at :attr:`dim` = ``dim + input.dim() + 1``. 
<a name="l31318"><span class="ln">31318 </span></a> 
<a name="l31319"><span class="ln">31319 </span></a>    Args: 
<a name="l31320"><span class="ln">31320 </span></a>        input (Tensor): the input tensor. 
<a name="l31321"><span class="ln">31321 </span></a>        dim (int): the index at which to insert the singleton dimension 
<a name="l31322"><span class="ln">31322 </span></a> 
<a name="l31323"><span class="ln">31323 </span></a>    Example:: 
<a name="l31324"><span class="ln">31324 </span></a> 
<a name="l31325"><span class="ln">31325 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4]) 
<a name="l31326"><span class="ln">31326 </span></a>        &gt;&gt;&gt; torch.unsqueeze(x, 0) 
<a name="l31327"><span class="ln">31327 </span></a>        tensor([[ 1,  2,  3,  4]]) 
<a name="l31328"><span class="ln">31328 </span></a>        &gt;&gt;&gt; torch.unsqueeze(x, 1) 
<a name="l31329"><span class="ln">31329 </span></a>        tensor([[ 1], 
<a name="l31330"><span class="ln">31330 </span></a>                [ 2], 
<a name="l31331"><span class="ln">31331 </span></a>                [ 3], 
<a name="l31332"><span class="ln">31332 </span></a>                [ 4]]) 
<a name="l31333"><span class="ln">31333 </span></a>    &quot;&quot;&quot;</span>
<a name="l31334"><span class="ln">31334 </span></a>
<a name="l31335"><span class="ln">31335 </span></a><span class="s2">def </span><span class="s1">unsqueeze_copy</span><span class="s3">(</span>
<a name="l31336"><span class="ln">31336 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31337"><span class="ln">31337 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l31338"><span class="ln">31338 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31339"><span class="ln">31339 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31340"><span class="ln">31340 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31341"><span class="ln">31341 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31342"><span class="ln">31342 </span></a>    Performs the same operation as :func:`torch.unsqueeze`, but all output tensors 
<a name="l31343"><span class="ln">31343 </span></a>    are freshly created instead of aliasing the input. 
<a name="l31344"><span class="ln">31344 </span></a>    &quot;&quot;&quot;</span>
<a name="l31345"><span class="ln">31345 </span></a>
<a name="l31346"><span class="ln">31346 </span></a><span class="s2">def </span><span class="s1">values_copy</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s2">*</span><span class="s3">, </span><span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31347"><span class="ln">31347 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31348"><span class="ln">31348 </span></a>    Performs the same operation as :func:`torch.values`, but all output tensors 
<a name="l31349"><span class="ln">31349 </span></a>    are freshly created instead of aliasing the input. 
<a name="l31350"><span class="ln">31350 </span></a>    &quot;&quot;&quot;</span>
<a name="l31351"><span class="ln">31351 </span></a>
<a name="l31352"><span class="ln">31352 </span></a><span class="s2">def </span><span class="s1">vander</span><span class="s3">(</span>
<a name="l31353"><span class="ln">31353 </span></a>    <span class="s1">x</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31354"><span class="ln">31354 </span></a>    <span class="s1">N</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31355"><span class="ln">31355 </span></a>    <span class="s1">increasing</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31356"><span class="ln">31356 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31357"><span class="ln">31357 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31358"><span class="ln">31358 </span></a>    vander(x, N=None, increasing=False) -&gt; Tensor 
<a name="l31359"><span class="ln">31359 </span></a> 
<a name="l31360"><span class="ln">31360 </span></a>    Generates a Vandermonde matrix. 
<a name="l31361"><span class="ln">31361 </span></a> 
<a name="l31362"><span class="ln">31362 </span></a>    The columns of the output matrix are elementwise powers of the input vector :math:`x^{(N-1)}, x^{(N-2)}, ..., x^0`. 
<a name="l31363"><span class="ln">31363 </span></a>    If increasing is True, the order of the columns is reversed :math:`x^0, x^1, ..., x^{(N-1)}`. Such a 
<a name="l31364"><span class="ln">31364 </span></a>    matrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. 
<a name="l31365"><span class="ln">31365 </span></a> 
<a name="l31366"><span class="ln">31366 </span></a>    Arguments: 
<a name="l31367"><span class="ln">31367 </span></a>        x (Tensor): 1-D input tensor. 
<a name="l31368"><span class="ln">31368 </span></a>        N (int, optional): Number of columns in the output. If N is not specified, 
<a name="l31369"><span class="ln">31369 </span></a>            a square array is returned :math:`(N = len(x))`. 
<a name="l31370"><span class="ln">31370 </span></a>        increasing (bool, optional): Order of the powers of the columns. If True, 
<a name="l31371"><span class="ln">31371 </span></a>            the powers increase from left to right, if False (the default) they are reversed. 
<a name="l31372"><span class="ln">31372 </span></a> 
<a name="l31373"><span class="ln">31373 </span></a>    Returns: 
<a name="l31374"><span class="ln">31374 </span></a>        Tensor: Vandermonde matrix. If increasing is False, the first column is :math:`x^{(N-1)}`, 
<a name="l31375"><span class="ln">31375 </span></a>        the second :math:`x^{(N-2)}` and so forth. If increasing is True, the columns 
<a name="l31376"><span class="ln">31376 </span></a>        are :math:`x^0, x^1, ..., x^{(N-1)}`. 
<a name="l31377"><span class="ln">31377 </span></a> 
<a name="l31378"><span class="ln">31378 </span></a>    Example:: 
<a name="l31379"><span class="ln">31379 </span></a> 
<a name="l31380"><span class="ln">31380 </span></a>        &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 5]) 
<a name="l31381"><span class="ln">31381 </span></a>        &gt;&gt;&gt; torch.vander(x) 
<a name="l31382"><span class="ln">31382 </span></a>        tensor([[  1,   1,   1,   1], 
<a name="l31383"><span class="ln">31383 </span></a>                [  8,   4,   2,   1], 
<a name="l31384"><span class="ln">31384 </span></a>                [ 27,   9,   3,   1], 
<a name="l31385"><span class="ln">31385 </span></a>                [125,  25,   5,   1]]) 
<a name="l31386"><span class="ln">31386 </span></a>        &gt;&gt;&gt; torch.vander(x, N=3) 
<a name="l31387"><span class="ln">31387 </span></a>        tensor([[ 1,  1,  1], 
<a name="l31388"><span class="ln">31388 </span></a>                [ 4,  2,  1], 
<a name="l31389"><span class="ln">31389 </span></a>                [ 9,  3,  1], 
<a name="l31390"><span class="ln">31390 </span></a>                [25,  5,  1]]) 
<a name="l31391"><span class="ln">31391 </span></a>        &gt;&gt;&gt; torch.vander(x, N=3, increasing=True) 
<a name="l31392"><span class="ln">31392 </span></a>        tensor([[ 1,  1,  1], 
<a name="l31393"><span class="ln">31393 </span></a>                [ 1,  2,  4], 
<a name="l31394"><span class="ln">31394 </span></a>                [ 1,  3,  9], 
<a name="l31395"><span class="ln">31395 </span></a>                [ 1,  5, 25]]) 
<a name="l31396"><span class="ln">31396 </span></a>    &quot;&quot;&quot;</span>
<a name="l31397"><span class="ln">31397 </span></a>
<a name="l31398"><span class="ln">31398 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31399"><span class="ln">31399 </span></a><span class="s2">def </span><span class="s1">var</span><span class="s3">(</span>
<a name="l31400"><span class="ln">31400 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31401"><span class="ln">31401 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l31402"><span class="ln">31402 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l31403"><span class="ln">31403 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31404"><span class="ln">31404 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31405"><span class="ln">31405 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31406"><span class="ln">31406 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31407"><span class="ln">31407 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31408"><span class="ln">31408 </span></a>    var(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l31409"><span class="ln">31409 </span></a> 
<a name="l31410"><span class="ln">31410 </span></a>    Calculates the variance over the dimensions specified by :attr:`dim`. :attr:`dim` 
<a name="l31411"><span class="ln">31411 </span></a>    can be a single dimension, list of dimensions, or ``None`` to reduce over all 
<a name="l31412"><span class="ln">31412 </span></a>    dimensions. 
<a name="l31413"><span class="ln">31413 </span></a> 
<a name="l31414"><span class="ln">31414 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31415"><span class="ln">31415 </span></a> 
<a name="l31416"><span class="ln">31416 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31417"><span class="ln">31417 </span></a> 
<a name="l31418"><span class="ln">31418 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31419"><span class="ln">31419 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31420"><span class="ln">31420 </span></a>    the :attr:`correction`. 
<a name="l31421"><span class="ln">31421 </span></a> 
<a name="l31422"><span class="ln">31422 </span></a> 
<a name="l31423"><span class="ln">31423 </span></a> 
<a name="l31424"><span class="ln">31424 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31425"><span class="ln">31425 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31426"><span class="ln">31426 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31427"><span class="ln">31427 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31428"><span class="ln">31428 </span></a> 
<a name="l31429"><span class="ln">31429 </span></a> 
<a name="l31430"><span class="ln">31430 </span></a>    Args: 
<a name="l31431"><span class="ln">31431 </span></a>        input (Tensor): the input tensor. 
<a name="l31432"><span class="ln">31432 </span></a> 
<a name="l31433"><span class="ln">31433 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31434"><span class="ln">31434 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31435"><span class="ln">31435 </span></a> 
<a name="l31436"><span class="ln">31436 </span></a> 
<a name="l31437"><span class="ln">31437 </span></a>    Keyword args: 
<a name="l31438"><span class="ln">31438 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31439"><span class="ln">31439 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31440"><span class="ln">31440 </span></a> 
<a name="l31441"><span class="ln">31441 </span></a>            .. versionchanged:: 2.0 
<a name="l31442"><span class="ln">31442 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31443"><span class="ln">31443 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31444"><span class="ln">31444 </span></a>                ``correction=0``. 
<a name="l31445"><span class="ln">31445 </span></a> 
<a name="l31446"><span class="ln">31446 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31447"><span class="ln">31447 </span></a> 
<a name="l31448"><span class="ln">31448 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31449"><span class="ln">31449 </span></a> 
<a name="l31450"><span class="ln">31450 </span></a>    Example: 
<a name="l31451"><span class="ln">31451 </span></a> 
<a name="l31452"><span class="ln">31452 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31453"><span class="ln">31453 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31454"><span class="ln">31454 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31455"><span class="ln">31455 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31456"><span class="ln">31456 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31457"><span class="ln">31457 </span></a>        ... )  # fmt: skip 
<a name="l31458"><span class="ln">31458 </span></a>        &gt;&gt;&gt; torch.var(a, dim=1, keepdim=True) 
<a name="l31459"><span class="ln">31459 </span></a>        tensor([[1.0631], 
<a name="l31460"><span class="ln">31460 </span></a>                [0.5590], 
<a name="l31461"><span class="ln">31461 </span></a>                [1.4893], 
<a name="l31462"><span class="ln">31462 </span></a>                [0.8258]]) 
<a name="l31463"><span class="ln">31463 </span></a> 
<a name="l31464"><span class="ln">31464 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31465"><span class="ln">31465 </span></a>    &quot;&quot;&quot;</span>
<a name="l31466"><span class="ln">31466 </span></a>
<a name="l31467"><span class="ln">31467 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31468"><span class="ln">31468 </span></a><span class="s2">def </span><span class="s1">var</span><span class="s3">(</span>
<a name="l31469"><span class="ln">31469 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31470"><span class="ln">31470 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31471"><span class="ln">31471 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31472"><span class="ln">31472 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31473"><span class="ln">31473 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31474"><span class="ln">31474 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31475"><span class="ln">31475 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31476"><span class="ln">31476 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31477"><span class="ln">31477 </span></a>    var(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l31478"><span class="ln">31478 </span></a> 
<a name="l31479"><span class="ln">31479 </span></a>    Calculates the variance over the dimensions specified by :attr:`dim`. :attr:`dim` 
<a name="l31480"><span class="ln">31480 </span></a>    can be a single dimension, list of dimensions, or ``None`` to reduce over all 
<a name="l31481"><span class="ln">31481 </span></a>    dimensions. 
<a name="l31482"><span class="ln">31482 </span></a> 
<a name="l31483"><span class="ln">31483 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31484"><span class="ln">31484 </span></a> 
<a name="l31485"><span class="ln">31485 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31486"><span class="ln">31486 </span></a> 
<a name="l31487"><span class="ln">31487 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31488"><span class="ln">31488 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31489"><span class="ln">31489 </span></a>    the :attr:`correction`. 
<a name="l31490"><span class="ln">31490 </span></a> 
<a name="l31491"><span class="ln">31491 </span></a> 
<a name="l31492"><span class="ln">31492 </span></a> 
<a name="l31493"><span class="ln">31493 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31494"><span class="ln">31494 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31495"><span class="ln">31495 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31496"><span class="ln">31496 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31497"><span class="ln">31497 </span></a> 
<a name="l31498"><span class="ln">31498 </span></a> 
<a name="l31499"><span class="ln">31499 </span></a>    Args: 
<a name="l31500"><span class="ln">31500 </span></a>        input (Tensor): the input tensor. 
<a name="l31501"><span class="ln">31501 </span></a> 
<a name="l31502"><span class="ln">31502 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31503"><span class="ln">31503 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31504"><span class="ln">31504 </span></a> 
<a name="l31505"><span class="ln">31505 </span></a> 
<a name="l31506"><span class="ln">31506 </span></a>    Keyword args: 
<a name="l31507"><span class="ln">31507 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31508"><span class="ln">31508 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31509"><span class="ln">31509 </span></a> 
<a name="l31510"><span class="ln">31510 </span></a>            .. versionchanged:: 2.0 
<a name="l31511"><span class="ln">31511 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31512"><span class="ln">31512 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31513"><span class="ln">31513 </span></a>                ``correction=0``. 
<a name="l31514"><span class="ln">31514 </span></a> 
<a name="l31515"><span class="ln">31515 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31516"><span class="ln">31516 </span></a> 
<a name="l31517"><span class="ln">31517 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31518"><span class="ln">31518 </span></a> 
<a name="l31519"><span class="ln">31519 </span></a>    Example: 
<a name="l31520"><span class="ln">31520 </span></a> 
<a name="l31521"><span class="ln">31521 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31522"><span class="ln">31522 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31523"><span class="ln">31523 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31524"><span class="ln">31524 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31525"><span class="ln">31525 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31526"><span class="ln">31526 </span></a>        ... )  # fmt: skip 
<a name="l31527"><span class="ln">31527 </span></a>        &gt;&gt;&gt; torch.var(a, dim=1, keepdim=True) 
<a name="l31528"><span class="ln">31528 </span></a>        tensor([[1.0631], 
<a name="l31529"><span class="ln">31529 </span></a>                [0.5590], 
<a name="l31530"><span class="ln">31530 </span></a>                [1.4893], 
<a name="l31531"><span class="ln">31531 </span></a>                [0.8258]]) 
<a name="l31532"><span class="ln">31532 </span></a> 
<a name="l31533"><span class="ln">31533 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31534"><span class="ln">31534 </span></a>    &quot;&quot;&quot;</span>
<a name="l31535"><span class="ln">31535 </span></a>
<a name="l31536"><span class="ln">31536 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31537"><span class="ln">31537 </span></a><span class="s2">def </span><span class="s1">var</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31538"><span class="ln">31538 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31539"><span class="ln">31539 </span></a>    var(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l31540"><span class="ln">31540 </span></a> 
<a name="l31541"><span class="ln">31541 </span></a>    Calculates the variance over the dimensions specified by :attr:`dim`. :attr:`dim` 
<a name="l31542"><span class="ln">31542 </span></a>    can be a single dimension, list of dimensions, or ``None`` to reduce over all 
<a name="l31543"><span class="ln">31543 </span></a>    dimensions. 
<a name="l31544"><span class="ln">31544 </span></a> 
<a name="l31545"><span class="ln">31545 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31546"><span class="ln">31546 </span></a> 
<a name="l31547"><span class="ln">31547 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31548"><span class="ln">31548 </span></a> 
<a name="l31549"><span class="ln">31549 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31550"><span class="ln">31550 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31551"><span class="ln">31551 </span></a>    the :attr:`correction`. 
<a name="l31552"><span class="ln">31552 </span></a> 
<a name="l31553"><span class="ln">31553 </span></a> 
<a name="l31554"><span class="ln">31554 </span></a> 
<a name="l31555"><span class="ln">31555 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31556"><span class="ln">31556 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31557"><span class="ln">31557 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31558"><span class="ln">31558 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31559"><span class="ln">31559 </span></a> 
<a name="l31560"><span class="ln">31560 </span></a> 
<a name="l31561"><span class="ln">31561 </span></a>    Args: 
<a name="l31562"><span class="ln">31562 </span></a>        input (Tensor): the input tensor. 
<a name="l31563"><span class="ln">31563 </span></a> 
<a name="l31564"><span class="ln">31564 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31565"><span class="ln">31565 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31566"><span class="ln">31566 </span></a> 
<a name="l31567"><span class="ln">31567 </span></a> 
<a name="l31568"><span class="ln">31568 </span></a>    Keyword args: 
<a name="l31569"><span class="ln">31569 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31570"><span class="ln">31570 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31571"><span class="ln">31571 </span></a> 
<a name="l31572"><span class="ln">31572 </span></a>            .. versionchanged:: 2.0 
<a name="l31573"><span class="ln">31573 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31574"><span class="ln">31574 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31575"><span class="ln">31575 </span></a>                ``correction=0``. 
<a name="l31576"><span class="ln">31576 </span></a> 
<a name="l31577"><span class="ln">31577 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31578"><span class="ln">31578 </span></a> 
<a name="l31579"><span class="ln">31579 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31580"><span class="ln">31580 </span></a> 
<a name="l31581"><span class="ln">31581 </span></a>    Example: 
<a name="l31582"><span class="ln">31582 </span></a> 
<a name="l31583"><span class="ln">31583 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31584"><span class="ln">31584 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31585"><span class="ln">31585 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31586"><span class="ln">31586 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31587"><span class="ln">31587 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31588"><span class="ln">31588 </span></a>        ... )  # fmt: skip 
<a name="l31589"><span class="ln">31589 </span></a>        &gt;&gt;&gt; torch.var(a, dim=1, keepdim=True) 
<a name="l31590"><span class="ln">31590 </span></a>        tensor([[1.0631], 
<a name="l31591"><span class="ln">31591 </span></a>                [0.5590], 
<a name="l31592"><span class="ln">31592 </span></a>                [1.4893], 
<a name="l31593"><span class="ln">31593 </span></a>                [0.8258]]) 
<a name="l31594"><span class="ln">31594 </span></a> 
<a name="l31595"><span class="ln">31595 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31596"><span class="ln">31596 </span></a>    &quot;&quot;&quot;</span>
<a name="l31597"><span class="ln">31597 </span></a>
<a name="l31598"><span class="ln">31598 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31599"><span class="ln">31599 </span></a><span class="s2">def </span><span class="s1">var</span><span class="s3">(</span>
<a name="l31600"><span class="ln">31600 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31601"><span class="ln">31601 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l31602"><span class="ln">31602 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31603"><span class="ln">31603 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31604"><span class="ln">31604 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31605"><span class="ln">31605 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31606"><span class="ln">31606 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31607"><span class="ln">31607 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31608"><span class="ln">31608 </span></a>    var(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l31609"><span class="ln">31609 </span></a> 
<a name="l31610"><span class="ln">31610 </span></a>    Calculates the variance over the dimensions specified by :attr:`dim`. :attr:`dim` 
<a name="l31611"><span class="ln">31611 </span></a>    can be a single dimension, list of dimensions, or ``None`` to reduce over all 
<a name="l31612"><span class="ln">31612 </span></a>    dimensions. 
<a name="l31613"><span class="ln">31613 </span></a> 
<a name="l31614"><span class="ln">31614 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31615"><span class="ln">31615 </span></a> 
<a name="l31616"><span class="ln">31616 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31617"><span class="ln">31617 </span></a> 
<a name="l31618"><span class="ln">31618 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31619"><span class="ln">31619 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31620"><span class="ln">31620 </span></a>    the :attr:`correction`. 
<a name="l31621"><span class="ln">31621 </span></a> 
<a name="l31622"><span class="ln">31622 </span></a> 
<a name="l31623"><span class="ln">31623 </span></a> 
<a name="l31624"><span class="ln">31624 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31625"><span class="ln">31625 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31626"><span class="ln">31626 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31627"><span class="ln">31627 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31628"><span class="ln">31628 </span></a> 
<a name="l31629"><span class="ln">31629 </span></a> 
<a name="l31630"><span class="ln">31630 </span></a>    Args: 
<a name="l31631"><span class="ln">31631 </span></a>        input (Tensor): the input tensor. 
<a name="l31632"><span class="ln">31632 </span></a> 
<a name="l31633"><span class="ln">31633 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31634"><span class="ln">31634 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31635"><span class="ln">31635 </span></a> 
<a name="l31636"><span class="ln">31636 </span></a> 
<a name="l31637"><span class="ln">31637 </span></a>    Keyword args: 
<a name="l31638"><span class="ln">31638 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31639"><span class="ln">31639 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31640"><span class="ln">31640 </span></a> 
<a name="l31641"><span class="ln">31641 </span></a>            .. versionchanged:: 2.0 
<a name="l31642"><span class="ln">31642 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31643"><span class="ln">31643 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31644"><span class="ln">31644 </span></a>                ``correction=0``. 
<a name="l31645"><span class="ln">31645 </span></a> 
<a name="l31646"><span class="ln">31646 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31647"><span class="ln">31647 </span></a> 
<a name="l31648"><span class="ln">31648 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31649"><span class="ln">31649 </span></a> 
<a name="l31650"><span class="ln">31650 </span></a>    Example: 
<a name="l31651"><span class="ln">31651 </span></a> 
<a name="l31652"><span class="ln">31652 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31653"><span class="ln">31653 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31654"><span class="ln">31654 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31655"><span class="ln">31655 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31656"><span class="ln">31656 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31657"><span class="ln">31657 </span></a>        ... )  # fmt: skip 
<a name="l31658"><span class="ln">31658 </span></a>        &gt;&gt;&gt; torch.var(a, dim=1, keepdim=True) 
<a name="l31659"><span class="ln">31659 </span></a>        tensor([[1.0631], 
<a name="l31660"><span class="ln">31660 </span></a>                [0.5590], 
<a name="l31661"><span class="ln">31661 </span></a>                [1.4893], 
<a name="l31662"><span class="ln">31662 </span></a>                [0.8258]]) 
<a name="l31663"><span class="ln">31663 </span></a> 
<a name="l31664"><span class="ln">31664 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31665"><span class="ln">31665 </span></a>    &quot;&quot;&quot;</span>
<a name="l31666"><span class="ln">31666 </span></a>
<a name="l31667"><span class="ln">31667 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31668"><span class="ln">31668 </span></a><span class="s2">def </span><span class="s1">var</span><span class="s3">(</span>
<a name="l31669"><span class="ln">31669 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31670"><span class="ln">31670 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l31671"><span class="ln">31671 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l31672"><span class="ln">31672 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31673"><span class="ln">31673 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31674"><span class="ln">31674 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31675"><span class="ln">31675 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l31676"><span class="ln">31676 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31677"><span class="ln">31677 </span></a>    var(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor 
<a name="l31678"><span class="ln">31678 </span></a> 
<a name="l31679"><span class="ln">31679 </span></a>    Calculates the variance over the dimensions specified by :attr:`dim`. :attr:`dim` 
<a name="l31680"><span class="ln">31680 </span></a>    can be a single dimension, list of dimensions, or ``None`` to reduce over all 
<a name="l31681"><span class="ln">31681 </span></a>    dimensions. 
<a name="l31682"><span class="ln">31682 </span></a> 
<a name="l31683"><span class="ln">31683 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31684"><span class="ln">31684 </span></a> 
<a name="l31685"><span class="ln">31685 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31686"><span class="ln">31686 </span></a> 
<a name="l31687"><span class="ln">31687 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31688"><span class="ln">31688 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31689"><span class="ln">31689 </span></a>    the :attr:`correction`. 
<a name="l31690"><span class="ln">31690 </span></a> 
<a name="l31691"><span class="ln">31691 </span></a> 
<a name="l31692"><span class="ln">31692 </span></a> 
<a name="l31693"><span class="ln">31693 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31694"><span class="ln">31694 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31695"><span class="ln">31695 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31696"><span class="ln">31696 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31697"><span class="ln">31697 </span></a> 
<a name="l31698"><span class="ln">31698 </span></a> 
<a name="l31699"><span class="ln">31699 </span></a>    Args: 
<a name="l31700"><span class="ln">31700 </span></a>        input (Tensor): the input tensor. 
<a name="l31701"><span class="ln">31701 </span></a> 
<a name="l31702"><span class="ln">31702 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31703"><span class="ln">31703 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31704"><span class="ln">31704 </span></a> 
<a name="l31705"><span class="ln">31705 </span></a> 
<a name="l31706"><span class="ln">31706 </span></a>    Keyword args: 
<a name="l31707"><span class="ln">31707 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31708"><span class="ln">31708 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31709"><span class="ln">31709 </span></a> 
<a name="l31710"><span class="ln">31710 </span></a>            .. versionchanged:: 2.0 
<a name="l31711"><span class="ln">31711 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31712"><span class="ln">31712 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31713"><span class="ln">31713 </span></a>                ``correction=0``. 
<a name="l31714"><span class="ln">31714 </span></a> 
<a name="l31715"><span class="ln">31715 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31716"><span class="ln">31716 </span></a> 
<a name="l31717"><span class="ln">31717 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31718"><span class="ln">31718 </span></a> 
<a name="l31719"><span class="ln">31719 </span></a>    Example: 
<a name="l31720"><span class="ln">31720 </span></a> 
<a name="l31721"><span class="ln">31721 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31722"><span class="ln">31722 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31723"><span class="ln">31723 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31724"><span class="ln">31724 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31725"><span class="ln">31725 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31726"><span class="ln">31726 </span></a>        ... )  # fmt: skip 
<a name="l31727"><span class="ln">31727 </span></a>        &gt;&gt;&gt; torch.var(a, dim=1, keepdim=True) 
<a name="l31728"><span class="ln">31728 </span></a>        tensor([[1.0631], 
<a name="l31729"><span class="ln">31729 </span></a>                [0.5590], 
<a name="l31730"><span class="ln">31730 </span></a>                [1.4893], 
<a name="l31731"><span class="ln">31731 </span></a>                [0.8258]]) 
<a name="l31732"><span class="ln">31732 </span></a> 
<a name="l31733"><span class="ln">31733 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31734"><span class="ln">31734 </span></a>    &quot;&quot;&quot;</span>
<a name="l31735"><span class="ln">31735 </span></a>
<a name="l31736"><span class="ln">31736 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31737"><span class="ln">31737 </span></a><span class="s2">def </span><span class="s1">var_mean</span><span class="s3">(</span>
<a name="l31738"><span class="ln">31738 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31739"><span class="ln">31739 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l31740"><span class="ln">31740 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l31741"><span class="ln">31741 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31742"><span class="ln">31742 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l31743"><span class="ln">31743 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31744"><span class="ln">31744 </span></a>    var_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l31745"><span class="ln">31745 </span></a> 
<a name="l31746"><span class="ln">31746 </span></a>    Calculates the variance and mean over the dimensions specified by :attr:`dim`. 
<a name="l31747"><span class="ln">31747 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l31748"><span class="ln">31748 </span></a>    reduce over all dimensions. 
<a name="l31749"><span class="ln">31749 </span></a> 
<a name="l31750"><span class="ln">31750 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31751"><span class="ln">31751 </span></a> 
<a name="l31752"><span class="ln">31752 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31753"><span class="ln">31753 </span></a> 
<a name="l31754"><span class="ln">31754 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31755"><span class="ln">31755 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31756"><span class="ln">31756 </span></a>    the :attr:`correction`. 
<a name="l31757"><span class="ln">31757 </span></a> 
<a name="l31758"><span class="ln">31758 </span></a> 
<a name="l31759"><span class="ln">31759 </span></a> 
<a name="l31760"><span class="ln">31760 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31761"><span class="ln">31761 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31762"><span class="ln">31762 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31763"><span class="ln">31763 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31764"><span class="ln">31764 </span></a> 
<a name="l31765"><span class="ln">31765 </span></a> 
<a name="l31766"><span class="ln">31766 </span></a>    Args: 
<a name="l31767"><span class="ln">31767 </span></a>        input (Tensor): the input tensor. 
<a name="l31768"><span class="ln">31768 </span></a> 
<a name="l31769"><span class="ln">31769 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31770"><span class="ln">31770 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31771"><span class="ln">31771 </span></a> 
<a name="l31772"><span class="ln">31772 </span></a> 
<a name="l31773"><span class="ln">31773 </span></a>    Keyword args: 
<a name="l31774"><span class="ln">31774 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31775"><span class="ln">31775 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31776"><span class="ln">31776 </span></a> 
<a name="l31777"><span class="ln">31777 </span></a>            .. versionchanged:: 2.0 
<a name="l31778"><span class="ln">31778 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31779"><span class="ln">31779 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31780"><span class="ln">31780 </span></a>                ``correction=0``. 
<a name="l31781"><span class="ln">31781 </span></a> 
<a name="l31782"><span class="ln">31782 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31783"><span class="ln">31783 </span></a> 
<a name="l31784"><span class="ln">31784 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31785"><span class="ln">31785 </span></a> 
<a name="l31786"><span class="ln">31786 </span></a>    Returns: 
<a name="l31787"><span class="ln">31787 </span></a>        A tuple (var, mean) containing the variance and mean. 
<a name="l31788"><span class="ln">31788 </span></a> 
<a name="l31789"><span class="ln">31789 </span></a>    Example: 
<a name="l31790"><span class="ln">31790 </span></a> 
<a name="l31791"><span class="ln">31791 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31792"><span class="ln">31792 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31793"><span class="ln">31793 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31794"><span class="ln">31794 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31795"><span class="ln">31795 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31796"><span class="ln">31796 </span></a>        ... )  # fmt: skip 
<a name="l31797"><span class="ln">31797 </span></a>        &gt;&gt;&gt; torch.var_mean(a, dim=0, keepdim=True) 
<a name="l31798"><span class="ln">31798 </span></a>        (tensor([[1.5926, 1.0056, 1.2005, 0.3646]]), 
<a name="l31799"><span class="ln">31799 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l31800"><span class="ln">31800 </span></a> 
<a name="l31801"><span class="ln">31801 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31802"><span class="ln">31802 </span></a>    &quot;&quot;&quot;</span>
<a name="l31803"><span class="ln">31803 </span></a>
<a name="l31804"><span class="ln">31804 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31805"><span class="ln">31805 </span></a><span class="s2">def </span><span class="s1">var_mean</span><span class="s3">(</span>
<a name="l31806"><span class="ln">31806 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31807"><span class="ln">31807 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">_size </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31808"><span class="ln">31808 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31809"><span class="ln">31809 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31810"><span class="ln">31810 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31811"><span class="ln">31811 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l31812"><span class="ln">31812 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31813"><span class="ln">31813 </span></a>    var_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l31814"><span class="ln">31814 </span></a> 
<a name="l31815"><span class="ln">31815 </span></a>    Calculates the variance and mean over the dimensions specified by :attr:`dim`. 
<a name="l31816"><span class="ln">31816 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l31817"><span class="ln">31817 </span></a>    reduce over all dimensions. 
<a name="l31818"><span class="ln">31818 </span></a> 
<a name="l31819"><span class="ln">31819 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31820"><span class="ln">31820 </span></a> 
<a name="l31821"><span class="ln">31821 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31822"><span class="ln">31822 </span></a> 
<a name="l31823"><span class="ln">31823 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31824"><span class="ln">31824 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31825"><span class="ln">31825 </span></a>    the :attr:`correction`. 
<a name="l31826"><span class="ln">31826 </span></a> 
<a name="l31827"><span class="ln">31827 </span></a> 
<a name="l31828"><span class="ln">31828 </span></a> 
<a name="l31829"><span class="ln">31829 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31830"><span class="ln">31830 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31831"><span class="ln">31831 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31832"><span class="ln">31832 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31833"><span class="ln">31833 </span></a> 
<a name="l31834"><span class="ln">31834 </span></a> 
<a name="l31835"><span class="ln">31835 </span></a>    Args: 
<a name="l31836"><span class="ln">31836 </span></a>        input (Tensor): the input tensor. 
<a name="l31837"><span class="ln">31837 </span></a> 
<a name="l31838"><span class="ln">31838 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31839"><span class="ln">31839 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31840"><span class="ln">31840 </span></a> 
<a name="l31841"><span class="ln">31841 </span></a> 
<a name="l31842"><span class="ln">31842 </span></a>    Keyword args: 
<a name="l31843"><span class="ln">31843 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31844"><span class="ln">31844 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31845"><span class="ln">31845 </span></a> 
<a name="l31846"><span class="ln">31846 </span></a>            .. versionchanged:: 2.0 
<a name="l31847"><span class="ln">31847 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31848"><span class="ln">31848 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31849"><span class="ln">31849 </span></a>                ``correction=0``. 
<a name="l31850"><span class="ln">31850 </span></a> 
<a name="l31851"><span class="ln">31851 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31852"><span class="ln">31852 </span></a> 
<a name="l31853"><span class="ln">31853 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31854"><span class="ln">31854 </span></a> 
<a name="l31855"><span class="ln">31855 </span></a>    Returns: 
<a name="l31856"><span class="ln">31856 </span></a>        A tuple (var, mean) containing the variance and mean. 
<a name="l31857"><span class="ln">31857 </span></a> 
<a name="l31858"><span class="ln">31858 </span></a>    Example: 
<a name="l31859"><span class="ln">31859 </span></a> 
<a name="l31860"><span class="ln">31860 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31861"><span class="ln">31861 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31862"><span class="ln">31862 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31863"><span class="ln">31863 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31864"><span class="ln">31864 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31865"><span class="ln">31865 </span></a>        ... )  # fmt: skip 
<a name="l31866"><span class="ln">31866 </span></a>        &gt;&gt;&gt; torch.var_mean(a, dim=0, keepdim=True) 
<a name="l31867"><span class="ln">31867 </span></a>        (tensor([[1.5926, 1.0056, 1.2005, 0.3646]]), 
<a name="l31868"><span class="ln">31868 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l31869"><span class="ln">31869 </span></a> 
<a name="l31870"><span class="ln">31870 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31871"><span class="ln">31871 </span></a>    &quot;&quot;&quot;</span>
<a name="l31872"><span class="ln">31872 </span></a>
<a name="l31873"><span class="ln">31873 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31874"><span class="ln">31874 </span></a><span class="s2">def </span><span class="s1">var_mean</span><span class="s3">(</span>
<a name="l31875"><span class="ln">31875 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31876"><span class="ln">31876 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l31877"><span class="ln">31877 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l31878"><span class="ln">31878 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31879"><span class="ln">31879 </span></a>    var_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l31880"><span class="ln">31880 </span></a> 
<a name="l31881"><span class="ln">31881 </span></a>    Calculates the variance and mean over the dimensions specified by :attr:`dim`. 
<a name="l31882"><span class="ln">31882 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l31883"><span class="ln">31883 </span></a>    reduce over all dimensions. 
<a name="l31884"><span class="ln">31884 </span></a> 
<a name="l31885"><span class="ln">31885 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31886"><span class="ln">31886 </span></a> 
<a name="l31887"><span class="ln">31887 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31888"><span class="ln">31888 </span></a> 
<a name="l31889"><span class="ln">31889 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31890"><span class="ln">31890 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31891"><span class="ln">31891 </span></a>    the :attr:`correction`. 
<a name="l31892"><span class="ln">31892 </span></a> 
<a name="l31893"><span class="ln">31893 </span></a> 
<a name="l31894"><span class="ln">31894 </span></a> 
<a name="l31895"><span class="ln">31895 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31896"><span class="ln">31896 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31897"><span class="ln">31897 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31898"><span class="ln">31898 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31899"><span class="ln">31899 </span></a> 
<a name="l31900"><span class="ln">31900 </span></a> 
<a name="l31901"><span class="ln">31901 </span></a>    Args: 
<a name="l31902"><span class="ln">31902 </span></a>        input (Tensor): the input tensor. 
<a name="l31903"><span class="ln">31903 </span></a> 
<a name="l31904"><span class="ln">31904 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31905"><span class="ln">31905 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31906"><span class="ln">31906 </span></a> 
<a name="l31907"><span class="ln">31907 </span></a> 
<a name="l31908"><span class="ln">31908 </span></a>    Keyword args: 
<a name="l31909"><span class="ln">31909 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31910"><span class="ln">31910 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31911"><span class="ln">31911 </span></a> 
<a name="l31912"><span class="ln">31912 </span></a>            .. versionchanged:: 2.0 
<a name="l31913"><span class="ln">31913 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31914"><span class="ln">31914 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31915"><span class="ln">31915 </span></a>                ``correction=0``. 
<a name="l31916"><span class="ln">31916 </span></a> 
<a name="l31917"><span class="ln">31917 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31918"><span class="ln">31918 </span></a> 
<a name="l31919"><span class="ln">31919 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31920"><span class="ln">31920 </span></a> 
<a name="l31921"><span class="ln">31921 </span></a>    Returns: 
<a name="l31922"><span class="ln">31922 </span></a>        A tuple (var, mean) containing the variance and mean. 
<a name="l31923"><span class="ln">31923 </span></a> 
<a name="l31924"><span class="ln">31924 </span></a>    Example: 
<a name="l31925"><span class="ln">31925 </span></a> 
<a name="l31926"><span class="ln">31926 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31927"><span class="ln">31927 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31928"><span class="ln">31928 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31929"><span class="ln">31929 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31930"><span class="ln">31930 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l31931"><span class="ln">31931 </span></a>        ... )  # fmt: skip 
<a name="l31932"><span class="ln">31932 </span></a>        &gt;&gt;&gt; torch.var_mean(a, dim=0, keepdim=True) 
<a name="l31933"><span class="ln">31933 </span></a>        (tensor([[1.5926, 1.0056, 1.2005, 0.3646]]), 
<a name="l31934"><span class="ln">31934 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l31935"><span class="ln">31935 </span></a> 
<a name="l31936"><span class="ln">31936 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l31937"><span class="ln">31937 </span></a>    &quot;&quot;&quot;</span>
<a name="l31938"><span class="ln">31938 </span></a>
<a name="l31939"><span class="ln">31939 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l31940"><span class="ln">31940 </span></a><span class="s2">def </span><span class="s1">var_mean</span><span class="s3">(</span>
<a name="l31941"><span class="ln">31941 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l31942"><span class="ln">31942 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l31943"><span class="ln">31943 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l31944"><span class="ln">31944 </span></a>    <span class="s1">correction</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l31945"><span class="ln">31945 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l31946"><span class="ln">31946 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l31947"><span class="ln">31947 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l31948"><span class="ln">31948 </span></a>    var_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l31949"><span class="ln">31949 </span></a> 
<a name="l31950"><span class="ln">31950 </span></a>    Calculates the variance and mean over the dimensions specified by :attr:`dim`. 
<a name="l31951"><span class="ln">31951 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l31952"><span class="ln">31952 </span></a>    reduce over all dimensions. 
<a name="l31953"><span class="ln">31953 </span></a> 
<a name="l31954"><span class="ln">31954 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l31955"><span class="ln">31955 </span></a> 
<a name="l31956"><span class="ln">31956 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l31957"><span class="ln">31957 </span></a> 
<a name="l31958"><span class="ln">31958 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l31959"><span class="ln">31959 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l31960"><span class="ln">31960 </span></a>    the :attr:`correction`. 
<a name="l31961"><span class="ln">31961 </span></a> 
<a name="l31962"><span class="ln">31962 </span></a> 
<a name="l31963"><span class="ln">31963 </span></a> 
<a name="l31964"><span class="ln">31964 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l31965"><span class="ln">31965 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l31966"><span class="ln">31966 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l31967"><span class="ln">31967 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l31968"><span class="ln">31968 </span></a> 
<a name="l31969"><span class="ln">31969 </span></a> 
<a name="l31970"><span class="ln">31970 </span></a>    Args: 
<a name="l31971"><span class="ln">31971 </span></a>        input (Tensor): the input tensor. 
<a name="l31972"><span class="ln">31972 </span></a> 
<a name="l31973"><span class="ln">31973 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l31974"><span class="ln">31974 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l31975"><span class="ln">31975 </span></a> 
<a name="l31976"><span class="ln">31976 </span></a> 
<a name="l31977"><span class="ln">31977 </span></a>    Keyword args: 
<a name="l31978"><span class="ln">31978 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l31979"><span class="ln">31979 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l31980"><span class="ln">31980 </span></a> 
<a name="l31981"><span class="ln">31981 </span></a>            .. versionchanged:: 2.0 
<a name="l31982"><span class="ln">31982 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l31983"><span class="ln">31983 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l31984"><span class="ln">31984 </span></a>                ``correction=0``. 
<a name="l31985"><span class="ln">31985 </span></a> 
<a name="l31986"><span class="ln">31986 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l31987"><span class="ln">31987 </span></a> 
<a name="l31988"><span class="ln">31988 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l31989"><span class="ln">31989 </span></a> 
<a name="l31990"><span class="ln">31990 </span></a>    Returns: 
<a name="l31991"><span class="ln">31991 </span></a>        A tuple (var, mean) containing the variance and mean. 
<a name="l31992"><span class="ln">31992 </span></a> 
<a name="l31993"><span class="ln">31993 </span></a>    Example: 
<a name="l31994"><span class="ln">31994 </span></a> 
<a name="l31995"><span class="ln">31995 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l31996"><span class="ln">31996 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l31997"><span class="ln">31997 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l31998"><span class="ln">31998 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l31999"><span class="ln">31999 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l32000"><span class="ln">32000 </span></a>        ... )  # fmt: skip 
<a name="l32001"><span class="ln">32001 </span></a>        &gt;&gt;&gt; torch.var_mean(a, dim=0, keepdim=True) 
<a name="l32002"><span class="ln">32002 </span></a>        (tensor([[1.5926, 1.0056, 1.2005, 0.3646]]), 
<a name="l32003"><span class="ln">32003 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l32004"><span class="ln">32004 </span></a> 
<a name="l32005"><span class="ln">32005 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l32006"><span class="ln">32006 </span></a>    &quot;&quot;&quot;</span>
<a name="l32007"><span class="ln">32007 </span></a>
<a name="l32008"><span class="ln">32008 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32009"><span class="ln">32009 </span></a><span class="s2">def </span><span class="s1">var_mean</span><span class="s3">(</span>
<a name="l32010"><span class="ln">32010 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32011"><span class="ln">32011 </span></a>    <span class="s1">dim</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">],</span>
<a name="l32012"><span class="ln">32012 </span></a>    <span class="s1">unbiased</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= True</span><span class="s3">,</span>
<a name="l32013"><span class="ln">32013 </span></a>    <span class="s1">keepdim</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">= False</span><span class="s3">,</span>
<a name="l32014"><span class="ln">32014 </span></a><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">Tensor</span><span class="s3">]</span><span class="s2">:</span>
<a name="l32015"><span class="ln">32015 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32016"><span class="ln">32016 </span></a>    var_mean(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; (Tensor, Tensor) 
<a name="l32017"><span class="ln">32017 </span></a> 
<a name="l32018"><span class="ln">32018 </span></a>    Calculates the variance and mean over the dimensions specified by :attr:`dim`. 
<a name="l32019"><span class="ln">32019 </span></a>    :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to 
<a name="l32020"><span class="ln">32020 </span></a>    reduce over all dimensions. 
<a name="l32021"><span class="ln">32021 </span></a> 
<a name="l32022"><span class="ln">32022 </span></a>    The variance (:math:`\sigma^2`) is calculated as 
<a name="l32023"><span class="ln">32023 </span></a> 
<a name="l32024"><span class="ln">32024 </span></a>    .. math:: \sigma^2 = \frac{1}{\max(0,~N - \delta N)}\sum_{i=0}^{N-1}(x_i-\bar{x})^2 
<a name="l32025"><span class="ln">32025 </span></a> 
<a name="l32026"><span class="ln">32026 </span></a>    where :math:`x` is the sample set of elements, :math:`\bar{x}` is the 
<a name="l32027"><span class="ln">32027 </span></a>    sample mean, :math:`N` is the number of samples and :math:`\delta N` is 
<a name="l32028"><span class="ln">32028 </span></a>    the :attr:`correction`. 
<a name="l32029"><span class="ln">32029 </span></a> 
<a name="l32030"><span class="ln">32030 </span></a> 
<a name="l32031"><span class="ln">32031 </span></a> 
<a name="l32032"><span class="ln">32032 </span></a>    If :attr:`keepdim` is ``True``, the output tensor is of the same size 
<a name="l32033"><span class="ln">32033 </span></a>    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. 
<a name="l32034"><span class="ln">32034 </span></a>    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the 
<a name="l32035"><span class="ln">32035 </span></a>    output tensor having 1 (or ``len(dim)``) fewer dimension(s). 
<a name="l32036"><span class="ln">32036 </span></a> 
<a name="l32037"><span class="ln">32037 </span></a> 
<a name="l32038"><span class="ln">32038 </span></a>    Args: 
<a name="l32039"><span class="ln">32039 </span></a>        input (Tensor): the input tensor. 
<a name="l32040"><span class="ln">32040 </span></a> 
<a name="l32041"><span class="ln">32041 </span></a>        dim (int or tuple of ints, optional): the dimension or dimensions to reduce. 
<a name="l32042"><span class="ln">32042 </span></a>            If ``None``, all dimensions are reduced. 
<a name="l32043"><span class="ln">32043 </span></a> 
<a name="l32044"><span class="ln">32044 </span></a> 
<a name="l32045"><span class="ln">32045 </span></a>    Keyword args: 
<a name="l32046"><span class="ln">32046 </span></a>        correction (int): difference between the sample size and sample degrees of freedom. 
<a name="l32047"><span class="ln">32047 </span></a>            Defaults to `Bessel's correction`_, ``correction=1``. 
<a name="l32048"><span class="ln">32048 </span></a> 
<a name="l32049"><span class="ln">32049 </span></a>            .. versionchanged:: 2.0 
<a name="l32050"><span class="ln">32050 </span></a>                Previously this argument was called ``unbiased`` and was a boolean 
<a name="l32051"><span class="ln">32051 </span></a>                with ``True`` corresponding to ``correction=1`` and ``False`` being 
<a name="l32052"><span class="ln">32052 </span></a>                ``correction=0``. 
<a name="l32053"><span class="ln">32053 </span></a> 
<a name="l32054"><span class="ln">32054 </span></a>        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. 
<a name="l32055"><span class="ln">32055 </span></a> 
<a name="l32056"><span class="ln">32056 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32057"><span class="ln">32057 </span></a> 
<a name="l32058"><span class="ln">32058 </span></a>    Returns: 
<a name="l32059"><span class="ln">32059 </span></a>        A tuple (var, mean) containing the variance and mean. 
<a name="l32060"><span class="ln">32060 </span></a> 
<a name="l32061"><span class="ln">32061 </span></a>    Example: 
<a name="l32062"><span class="ln">32062 </span></a> 
<a name="l32063"><span class="ln">32063 </span></a>        &gt;&gt;&gt; a = torch.tensor( 
<a name="l32064"><span class="ln">32064 </span></a>        ...     [[ 0.2035,  1.2959,  1.8101, -0.4644], 
<a name="l32065"><span class="ln">32065 </span></a>        ...      [ 1.5027, -0.3270,  0.5905,  0.6538], 
<a name="l32066"><span class="ln">32066 </span></a>        ...      [-1.5745,  1.3330, -0.5596, -0.6548], 
<a name="l32067"><span class="ln">32067 </span></a>        ...      [ 0.1264, -0.5080,  1.6420,  0.1992]] 
<a name="l32068"><span class="ln">32068 </span></a>        ... )  # fmt: skip 
<a name="l32069"><span class="ln">32069 </span></a>        &gt;&gt;&gt; torch.var_mean(a, dim=0, keepdim=True) 
<a name="l32070"><span class="ln">32070 </span></a>        (tensor([[1.5926, 1.0056, 1.2005, 0.3646]]), 
<a name="l32071"><span class="ln">32071 </span></a>         tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]])) 
<a name="l32072"><span class="ln">32072 </span></a> 
<a name="l32073"><span class="ln">32073 </span></a>    .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction 
<a name="l32074"><span class="ln">32074 </span></a>    &quot;&quot;&quot;</span>
<a name="l32075"><span class="ln">32075 </span></a>
<a name="l32076"><span class="ln">32076 </span></a><span class="s2">def </span><span class="s1">vdot</span><span class="s3">(</span>
<a name="l32077"><span class="ln">32077 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32078"><span class="ln">32078 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32079"><span class="ln">32079 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32080"><span class="ln">32080 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32081"><span class="ln">32081 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32082"><span class="ln">32082 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32083"><span class="ln">32083 </span></a>    vdot(input, other, *, out=None) -&gt; Tensor 
<a name="l32084"><span class="ln">32084 </span></a> 
<a name="l32085"><span class="ln">32085 </span></a>    Computes the dot product of two 1D vectors along a dimension. 
<a name="l32086"><span class="ln">32086 </span></a> 
<a name="l32087"><span class="ln">32087 </span></a>    In symbols, this function computes 
<a name="l32088"><span class="ln">32088 </span></a> 
<a name="l32089"><span class="ln">32089 </span></a>    .. math:: 
<a name="l32090"><span class="ln">32090 </span></a> 
<a name="l32091"><span class="ln">32091 </span></a>        \sum_{i=1}^n \overline{x_i}y_i. 
<a name="l32092"><span class="ln">32092 </span></a> 
<a name="l32093"><span class="ln">32093 </span></a>    where :math:`\overline{x_i}` denotes the conjugate for complex 
<a name="l32094"><span class="ln">32094 </span></a>    vectors, and it is the identity for real vectors. 
<a name="l32095"><span class="ln">32095 </span></a> 
<a name="l32096"><span class="ln">32096 </span></a>    .. note:: 
<a name="l32097"><span class="ln">32097 </span></a> 
<a name="l32098"><span class="ln">32098 </span></a>        Unlike NumPy's vdot, torch.vdot intentionally only supports computing the dot product 
<a name="l32099"><span class="ln">32099 </span></a>        of two 1D tensors with the same number of elements. 
<a name="l32100"><span class="ln">32100 </span></a> 
<a name="l32101"><span class="ln">32101 </span></a>    .. seealso:: 
<a name="l32102"><span class="ln">32102 </span></a> 
<a name="l32103"><span class="ln">32103 </span></a>            :func:`torch.linalg.vecdot` computes the dot product of two batches of vectors along a dimension. 
<a name="l32104"><span class="ln">32104 </span></a> 
<a name="l32105"><span class="ln">32105 </span></a>    Args: 
<a name="l32106"><span class="ln">32106 </span></a>        input (Tensor): first tensor in the dot product, must be 1D. Its conjugate is used if it's complex. 
<a name="l32107"><span class="ln">32107 </span></a>        other (Tensor): second tensor in the dot product, must be 1D. 
<a name="l32108"><span class="ln">32108 </span></a> 
<a name="l32109"><span class="ln">32109 </span></a>    Keyword args: 
<a name="l32110"><span class="ln">32110 </span></a> 
<a name="l32111"><span class="ln">32111 </span></a>    .. note:: out (Tensor, optional): the output tensor. 
<a name="l32112"><span class="ln">32112 </span></a> 
<a name="l32113"><span class="ln">32113 </span></a> 
<a name="l32114"><span class="ln">32114 </span></a>    Example:: 
<a name="l32115"><span class="ln">32115 </span></a> 
<a name="l32116"><span class="ln">32116 </span></a>        &gt;&gt;&gt; torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1])) 
<a name="l32117"><span class="ln">32117 </span></a>        tensor(7) 
<a name="l32118"><span class="ln">32118 </span></a>        &gt;&gt;&gt; a = torch.tensor((1 +2j, 3 - 1j)) 
<a name="l32119"><span class="ln">32119 </span></a>        &gt;&gt;&gt; b = torch.tensor((2 +1j, 4 - 0j)) 
<a name="l32120"><span class="ln">32120 </span></a>        &gt;&gt;&gt; torch.vdot(a, b) 
<a name="l32121"><span class="ln">32121 </span></a>        tensor([16.+1.j]) 
<a name="l32122"><span class="ln">32122 </span></a>        &gt;&gt;&gt; torch.vdot(b, a) 
<a name="l32123"><span class="ln">32123 </span></a>        tensor([16.-1.j]) 
<a name="l32124"><span class="ln">32124 </span></a>    &quot;&quot;&quot;</span>
<a name="l32125"><span class="ln">32125 </span></a>
<a name="l32126"><span class="ln">32126 </span></a><span class="s2">def </span><span class="s1">view_as_complex</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32127"><span class="ln">32127 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32128"><span class="ln">32128 </span></a>    view_as_complex(input) -&gt; Tensor 
<a name="l32129"><span class="ln">32129 </span></a> 
<a name="l32130"><span class="ln">32130 </span></a>    Returns a view of :attr:`input` as a complex tensor. For an input complex 
<a name="l32131"><span class="ln">32131 </span></a>    tensor of :attr:`size` :math:`m1, m2, \dots, mi, 2`, this function returns a 
<a name="l32132"><span class="ln">32132 </span></a>    new complex tensor of :attr:`size` :math:`m1, m2, \dots, mi` where the last 
<a name="l32133"><span class="ln">32133 </span></a>    dimension of the input tensor is expected to represent the real and imaginary 
<a name="l32134"><span class="ln">32134 </span></a>    components of complex numbers. 
<a name="l32135"><span class="ln">32135 </span></a> 
<a name="l32136"><span class="ln">32136 </span></a>    .. warning:: 
<a name="l32137"><span class="ln">32137 </span></a>        :func:`view_as_complex` is only supported for tensors with 
<a name="l32138"><span class="ln">32138 </span></a>        :class:`torch.dtype` ``torch.float64`` and ``torch.float32``.  The input is 
<a name="l32139"><span class="ln">32139 </span></a>        expected to have the last dimension of :attr:`size` 2. In addition, the 
<a name="l32140"><span class="ln">32140 </span></a>        tensor must have a `stride` of 1 for its last dimension. The strides of all 
<a name="l32141"><span class="ln">32141 </span></a>        other dimensions must be even numbers. 
<a name="l32142"><span class="ln">32142 </span></a> 
<a name="l32143"><span class="ln">32143 </span></a>    Args: 
<a name="l32144"><span class="ln">32144 </span></a>        input (Tensor): the input tensor. 
<a name="l32145"><span class="ln">32145 </span></a> 
<a name="l32146"><span class="ln">32146 </span></a>    Example:: 
<a name="l32147"><span class="ln">32147 </span></a> 
<a name="l32148"><span class="ln">32148 </span></a>        &gt;&gt;&gt; x=torch.randn(4, 2) 
<a name="l32149"><span class="ln">32149 </span></a>        &gt;&gt;&gt; x 
<a name="l32150"><span class="ln">32150 </span></a>        tensor([[ 1.6116, -0.5772], 
<a name="l32151"><span class="ln">32151 </span></a>                [-1.4606, -0.9120], 
<a name="l32152"><span class="ln">32152 </span></a>                [ 0.0786, -1.7497], 
<a name="l32153"><span class="ln">32153 </span></a>                [-0.6561, -1.6623]]) 
<a name="l32154"><span class="ln">32154 </span></a>        &gt;&gt;&gt; torch.view_as_complex(x) 
<a name="l32155"><span class="ln">32155 </span></a>        tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)]) 
<a name="l32156"><span class="ln">32156 </span></a>    &quot;&quot;&quot;</span>
<a name="l32157"><span class="ln">32157 </span></a>
<a name="l32158"><span class="ln">32158 </span></a><span class="s2">def </span><span class="s1">view_as_complex_copy</span><span class="s3">(</span>
<a name="l32159"><span class="ln">32159 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32160"><span class="ln">32160 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32161"><span class="ln">32161 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32162"><span class="ln">32162 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32163"><span class="ln">32163 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32164"><span class="ln">32164 </span></a>    Performs the same operation as :func:`torch.view_as_complex`, but all output tensors 
<a name="l32165"><span class="ln">32165 </span></a>    are freshly created instead of aliasing the input. 
<a name="l32166"><span class="ln">32166 </span></a>    &quot;&quot;&quot;</span>
<a name="l32167"><span class="ln">32167 </span></a>
<a name="l32168"><span class="ln">32168 </span></a><span class="s2">def </span><span class="s1">view_as_real</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32169"><span class="ln">32169 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32170"><span class="ln">32170 </span></a>    view_as_real(input) -&gt; Tensor 
<a name="l32171"><span class="ln">32171 </span></a> 
<a name="l32172"><span class="ln">32172 </span></a>    Returns a view of :attr:`input` as a real tensor. For an input complex tensor of 
<a name="l32173"><span class="ln">32173 </span></a>    :attr:`size` :math:`m1, m2, \dots, mi`, this function returns a new 
<a name="l32174"><span class="ln">32174 </span></a>    real tensor of size :math:`m1, m2, \dots, mi, 2`, where the last dimension of size 2 
<a name="l32175"><span class="ln">32175 </span></a>    represents the real and imaginary components of complex numbers. 
<a name="l32176"><span class="ln">32176 </span></a> 
<a name="l32177"><span class="ln">32177 </span></a>    .. warning:: 
<a name="l32178"><span class="ln">32178 </span></a>        :func:`view_as_real` is only supported for tensors with ``complex dtypes``. 
<a name="l32179"><span class="ln">32179 </span></a> 
<a name="l32180"><span class="ln">32180 </span></a>    Args: 
<a name="l32181"><span class="ln">32181 </span></a>        input (Tensor): the input tensor. 
<a name="l32182"><span class="ln">32182 </span></a> 
<a name="l32183"><span class="ln">32183 </span></a>    Example:: 
<a name="l32184"><span class="ln">32184 </span></a> 
<a name="l32185"><span class="ln">32185 </span></a>        &gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat) 
<a name="l32186"><span class="ln">32186 </span></a>        &gt;&gt;&gt; x 
<a name="l32187"><span class="ln">32187 </span></a>        tensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)]) 
<a name="l32188"><span class="ln">32188 </span></a>        &gt;&gt;&gt; torch.view_as_real(x) 
<a name="l32189"><span class="ln">32189 </span></a>        tensor([[ 0.4737, -0.3839], 
<a name="l32190"><span class="ln">32190 </span></a>                [-0.2098, -0.6699], 
<a name="l32191"><span class="ln">32191 </span></a>                [ 0.3470, -0.9451], 
<a name="l32192"><span class="ln">32192 </span></a>                [-0.5174, -1.3136]]) 
<a name="l32193"><span class="ln">32193 </span></a>    &quot;&quot;&quot;</span>
<a name="l32194"><span class="ln">32194 </span></a>
<a name="l32195"><span class="ln">32195 </span></a><span class="s2">def </span><span class="s1">view_as_real_copy</span><span class="s3">(</span>
<a name="l32196"><span class="ln">32196 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32197"><span class="ln">32197 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32198"><span class="ln">32198 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32199"><span class="ln">32199 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32200"><span class="ln">32200 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32201"><span class="ln">32201 </span></a>    Performs the same operation as :func:`torch.view_as_real`, but all output tensors 
<a name="l32202"><span class="ln">32202 </span></a>    are freshly created instead of aliasing the input. 
<a name="l32203"><span class="ln">32203 </span></a>    &quot;&quot;&quot;</span>
<a name="l32204"><span class="ln">32204 </span></a>
<a name="l32205"><span class="ln">32205 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32206"><span class="ln">32206 </span></a><span class="s2">def </span><span class="s1">view_copy</span><span class="s3">(</span>
<a name="l32207"><span class="ln">32207 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32208"><span class="ln">32208 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype</span><span class="s3">,</span>
<a name="l32209"><span class="ln">32209 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32210"><span class="ln">32210 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32211"><span class="ln">32211 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32212"><span class="ln">32212 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32213"><span class="ln">32213 </span></a>    Performs the same operation as :func:`torch.view`, but all output tensors 
<a name="l32214"><span class="ln">32214 </span></a>    are freshly created instead of aliasing the input. 
<a name="l32215"><span class="ln">32215 </span></a>    &quot;&quot;&quot;</span>
<a name="l32216"><span class="ln">32216 </span></a>
<a name="l32217"><span class="ln">32217 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32218"><span class="ln">32218 </span></a><span class="s2">def </span><span class="s1">view_copy</span><span class="s3">(</span>
<a name="l32219"><span class="ln">32219 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32220"><span class="ln">32220 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l32221"><span class="ln">32221 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32222"><span class="ln">32222 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32223"><span class="ln">32223 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32224"><span class="ln">32224 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32225"><span class="ln">32225 </span></a>    Performs the same operation as :func:`torch.view`, but all output tensors 
<a name="l32226"><span class="ln">32226 </span></a>    are freshly created instead of aliasing the input. 
<a name="l32227"><span class="ln">32227 </span></a>    &quot;&quot;&quot;</span>
<a name="l32228"><span class="ln">32228 </span></a>
<a name="l32229"><span class="ln">32229 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32230"><span class="ln">32230 </span></a><span class="s2">def </span><span class="s1">vsplit</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">sections</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l32231"><span class="ln">32231 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32232"><span class="ln">32232 </span></a>    vsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l32233"><span class="ln">32233 </span></a> 
<a name="l32234"><span class="ln">32234 </span></a>    Splits :attr:`input`, a tensor with two or more dimensions, into multiple tensors 
<a name="l32235"><span class="ln">32235 </span></a>    vertically according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l32236"><span class="ln">32236 </span></a>    :attr:`input`. 
<a name="l32237"><span class="ln">32237 </span></a> 
<a name="l32238"><span class="ln">32238 </span></a>    This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0) 
<a name="l32239"><span class="ln">32239 </span></a>    (the split dimension is 0), except that if :attr:`indices_or_sections` is an integer 
<a name="l32240"><span class="ln">32240 </span></a>    it must evenly divide the split dimension or a runtime error will be thrown. 
<a name="l32241"><span class="ln">32241 </span></a> 
<a name="l32242"><span class="ln">32242 </span></a>    This function is based on NumPy's :func:`numpy.vsplit`. 
<a name="l32243"><span class="ln">32243 </span></a> 
<a name="l32244"><span class="ln">32244 </span></a>    Args: 
<a name="l32245"><span class="ln">32245 </span></a>        input (Tensor): tensor to split. 
<a name="l32246"><span class="ln">32246 </span></a>        indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l32247"><span class="ln">32247 </span></a> 
<a name="l32248"><span class="ln">32248 </span></a>    Example:: 
<a name="l32249"><span class="ln">32249 </span></a> 
<a name="l32250"><span class="ln">32250 </span></a>        &gt;&gt;&gt; t = torch.arange(16.0).reshape(4,4) 
<a name="l32251"><span class="ln">32251 </span></a>        &gt;&gt;&gt; t 
<a name="l32252"><span class="ln">32252 </span></a>        tensor([[ 0.,  1.,  2.,  3.], 
<a name="l32253"><span class="ln">32253 </span></a>                [ 4.,  5.,  6.,  7.], 
<a name="l32254"><span class="ln">32254 </span></a>                [ 8.,  9., 10., 11.], 
<a name="l32255"><span class="ln">32255 </span></a>                [12., 13., 14., 15.]]) 
<a name="l32256"><span class="ln">32256 </span></a>        &gt;&gt;&gt; torch.vsplit(t, 2) 
<a name="l32257"><span class="ln">32257 </span></a>        (tensor([[0., 1., 2., 3.], 
<a name="l32258"><span class="ln">32258 </span></a>                 [4., 5., 6., 7.]]), 
<a name="l32259"><span class="ln">32259 </span></a>         tensor([[ 8.,  9., 10., 11.], 
<a name="l32260"><span class="ln">32260 </span></a>                 [12., 13., 14., 15.]])) 
<a name="l32261"><span class="ln">32261 </span></a>        &gt;&gt;&gt; torch.vsplit(t, [3, 6]) 
<a name="l32262"><span class="ln">32262 </span></a>        (tensor([[ 0.,  1.,  2.,  3.], 
<a name="l32263"><span class="ln">32263 </span></a>                 [ 4.,  5.,  6.,  7.], 
<a name="l32264"><span class="ln">32264 </span></a>                 [ 8.,  9., 10., 11.]]), 
<a name="l32265"><span class="ln">32265 </span></a>         tensor([[12., 13., 14., 15.]]), 
<a name="l32266"><span class="ln">32266 </span></a>         tensor([], size=(0, 4))) 
<a name="l32267"><span class="ln">32267 </span></a>    &quot;&quot;&quot;</span>
<a name="l32268"><span class="ln">32268 </span></a>
<a name="l32269"><span class="ln">32269 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32270"><span class="ln">32270 </span></a><span class="s2">def </span><span class="s1">vsplit</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">indices</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l32271"><span class="ln">32271 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32272"><span class="ln">32272 </span></a>    vsplit(input, indices_or_sections) -&gt; List of Tensors 
<a name="l32273"><span class="ln">32273 </span></a> 
<a name="l32274"><span class="ln">32274 </span></a>    Splits :attr:`input`, a tensor with two or more dimensions, into multiple tensors 
<a name="l32275"><span class="ln">32275 </span></a>    vertically according to :attr:`indices_or_sections`. Each split is a view of 
<a name="l32276"><span class="ln">32276 </span></a>    :attr:`input`. 
<a name="l32277"><span class="ln">32277 </span></a> 
<a name="l32278"><span class="ln">32278 </span></a>    This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0) 
<a name="l32279"><span class="ln">32279 </span></a>    (the split dimension is 0), except that if :attr:`indices_or_sections` is an integer 
<a name="l32280"><span class="ln">32280 </span></a>    it must evenly divide the split dimension or a runtime error will be thrown. 
<a name="l32281"><span class="ln">32281 </span></a> 
<a name="l32282"><span class="ln">32282 </span></a>    This function is based on NumPy's :func:`numpy.vsplit`. 
<a name="l32283"><span class="ln">32283 </span></a> 
<a name="l32284"><span class="ln">32284 </span></a>    Args: 
<a name="l32285"><span class="ln">32285 </span></a>        input (Tensor): tensor to split. 
<a name="l32286"><span class="ln">32286 </span></a>        indices_or_sections (int or list or tuple of ints): See argument in :func:`torch.tensor_split`. 
<a name="l32287"><span class="ln">32287 </span></a> 
<a name="l32288"><span class="ln">32288 </span></a>    Example:: 
<a name="l32289"><span class="ln">32289 </span></a> 
<a name="l32290"><span class="ln">32290 </span></a>        &gt;&gt;&gt; t = torch.arange(16.0).reshape(4,4) 
<a name="l32291"><span class="ln">32291 </span></a>        &gt;&gt;&gt; t 
<a name="l32292"><span class="ln">32292 </span></a>        tensor([[ 0.,  1.,  2.,  3.], 
<a name="l32293"><span class="ln">32293 </span></a>                [ 4.,  5.,  6.,  7.], 
<a name="l32294"><span class="ln">32294 </span></a>                [ 8.,  9., 10., 11.], 
<a name="l32295"><span class="ln">32295 </span></a>                [12., 13., 14., 15.]]) 
<a name="l32296"><span class="ln">32296 </span></a>        &gt;&gt;&gt; torch.vsplit(t, 2) 
<a name="l32297"><span class="ln">32297 </span></a>        (tensor([[0., 1., 2., 3.], 
<a name="l32298"><span class="ln">32298 </span></a>                 [4., 5., 6., 7.]]), 
<a name="l32299"><span class="ln">32299 </span></a>         tensor([[ 8.,  9., 10., 11.], 
<a name="l32300"><span class="ln">32300 </span></a>                 [12., 13., 14., 15.]])) 
<a name="l32301"><span class="ln">32301 </span></a>        &gt;&gt;&gt; torch.vsplit(t, [3, 6]) 
<a name="l32302"><span class="ln">32302 </span></a>        (tensor([[ 0.,  1.,  2.,  3.], 
<a name="l32303"><span class="ln">32303 </span></a>                 [ 4.,  5.,  6.,  7.], 
<a name="l32304"><span class="ln">32304 </span></a>                 [ 8.,  9., 10., 11.]]), 
<a name="l32305"><span class="ln">32305 </span></a>         tensor([[12., 13., 14., 15.]]), 
<a name="l32306"><span class="ln">32306 </span></a>         tensor([], size=(0, 4))) 
<a name="l32307"><span class="ln">32307 </span></a>    &quot;&quot;&quot;</span>
<a name="l32308"><span class="ln">32308 </span></a>
<a name="l32309"><span class="ln">32309 </span></a><span class="s2">def </span><span class="s1">vstack</span><span class="s3">(</span>
<a name="l32310"><span class="ln">32310 </span></a>    <span class="s1">tensors</span><span class="s2">: </span><span class="s1">tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...] </span><span class="s2">| </span><span class="s1">list</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l32311"><span class="ln">32311 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32312"><span class="ln">32312 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32313"><span class="ln">32313 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32314"><span class="ln">32314 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32315"><span class="ln">32315 </span></a>    vstack(tensors, *, out=None) -&gt; Tensor 
<a name="l32316"><span class="ln">32316 </span></a> 
<a name="l32317"><span class="ln">32317 </span></a>    Stack tensors in sequence vertically (row wise). 
<a name="l32318"><span class="ln">32318 </span></a> 
<a name="l32319"><span class="ln">32319 </span></a>    This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by :func:`torch.atleast_2d`. 
<a name="l32320"><span class="ln">32320 </span></a> 
<a name="l32321"><span class="ln">32321 </span></a>    Args: 
<a name="l32322"><span class="ln">32322 </span></a>        tensors (sequence of Tensors): sequence of tensors to concatenate 
<a name="l32323"><span class="ln">32323 </span></a> 
<a name="l32324"><span class="ln">32324 </span></a>    Keyword args: 
<a name="l32325"><span class="ln">32325 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32326"><span class="ln">32326 </span></a> 
<a name="l32327"><span class="ln">32327 </span></a>    Example:: 
<a name="l32328"><span class="ln">32328 </span></a> 
<a name="l32329"><span class="ln">32329 </span></a>        &gt;&gt;&gt; a = torch.tensor([1, 2, 3]) 
<a name="l32330"><span class="ln">32330 </span></a>        &gt;&gt;&gt; b = torch.tensor([4, 5, 6]) 
<a name="l32331"><span class="ln">32331 </span></a>        &gt;&gt;&gt; torch.vstack((a,b)) 
<a name="l32332"><span class="ln">32332 </span></a>        tensor([[1, 2, 3], 
<a name="l32333"><span class="ln">32333 </span></a>                [4, 5, 6]]) 
<a name="l32334"><span class="ln">32334 </span></a>        &gt;&gt;&gt; a = torch.tensor([[1],[2],[3]]) 
<a name="l32335"><span class="ln">32335 </span></a>        &gt;&gt;&gt; b = torch.tensor([[4],[5],[6]]) 
<a name="l32336"><span class="ln">32336 </span></a>        &gt;&gt;&gt; torch.vstack((a,b)) 
<a name="l32337"><span class="ln">32337 </span></a>        tensor([[1], 
<a name="l32338"><span class="ln">32338 </span></a>                [2], 
<a name="l32339"><span class="ln">32339 </span></a>                [3], 
<a name="l32340"><span class="ln">32340 </span></a>                [4], 
<a name="l32341"><span class="ln">32341 </span></a>                [5], 
<a name="l32342"><span class="ln">32342 </span></a>                [6]]) 
<a name="l32343"><span class="ln">32343 </span></a>    &quot;&quot;&quot;</span>
<a name="l32344"><span class="ln">32344 </span></a>
<a name="l32345"><span class="ln">32345 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32346"><span class="ln">32346 </span></a><span class="s2">def </span><span class="s1">where</span><span class="s3">(</span><span class="s1">condition</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; tuple</span><span class="s3">[</span><span class="s1">Tensor</span><span class="s3">, ...]</span><span class="s2">:</span>
<a name="l32347"><span class="ln">32347 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32348"><span class="ln">32348 </span></a>    where(condition, input, other, *, out=None) -&gt; Tensor 
<a name="l32349"><span class="ln">32349 </span></a> 
<a name="l32350"><span class="ln">32350 </span></a>    Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`. 
<a name="l32351"><span class="ln">32351 </span></a> 
<a name="l32352"><span class="ln">32352 </span></a>    The operation is defined as: 
<a name="l32353"><span class="ln">32353 </span></a> 
<a name="l32354"><span class="ln">32354 </span></a>    .. math:: 
<a name="l32355"><span class="ln">32355 </span></a>        \text{out}_i = \begin{cases} 
<a name="l32356"><span class="ln">32356 </span></a>            \text{input}_i &amp; \text{if } \text{condition}_i \\ 
<a name="l32357"><span class="ln">32357 </span></a>            \text{other}_i &amp; \text{otherwise} \\ 
<a name="l32358"><span class="ln">32358 </span></a>        \end{cases} 
<a name="l32359"><span class="ln">32359 </span></a> 
<a name="l32360"><span class="ln">32360 </span></a>    .. note:: 
<a name="l32361"><span class="ln">32361 </span></a>        The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l32362"><span class="ln">32362 </span></a> 
<a name="l32363"><span class="ln">32363 </span></a>    Arguments: 
<a name="l32364"><span class="ln">32364 </span></a>        condition (BoolTensor): When True (nonzero), yield input, otherwise yield other 
<a name="l32365"><span class="ln">32365 </span></a>        input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices 
<a name="l32366"><span class="ln">32366 </span></a>                              where :attr:`condition` is ``True`` 
<a name="l32367"><span class="ln">32367 </span></a>        other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices 
<a name="l32368"><span class="ln">32368 </span></a>                              where :attr:`condition` is ``False`` 
<a name="l32369"><span class="ln">32369 </span></a> 
<a name="l32370"><span class="ln">32370 </span></a>    Keyword args: 
<a name="l32371"><span class="ln">32371 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32372"><span class="ln">32372 </span></a> 
<a name="l32373"><span class="ln">32373 </span></a>    Returns: 
<a name="l32374"><span class="ln">32374 </span></a>        Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other` 
<a name="l32375"><span class="ln">32375 </span></a> 
<a name="l32376"><span class="ln">32376 </span></a>    Example:: 
<a name="l32377"><span class="ln">32377 </span></a> 
<a name="l32378"><span class="ln">32378 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 2) 
<a name="l32379"><span class="ln">32379 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 2) 
<a name="l32380"><span class="ln">32380 </span></a>        &gt;&gt;&gt; x 
<a name="l32381"><span class="ln">32381 </span></a>        tensor([[-0.4620,  0.3139], 
<a name="l32382"><span class="ln">32382 </span></a>                [ 0.3898, -0.7197], 
<a name="l32383"><span class="ln">32383 </span></a>                [ 0.0478, -0.1657]]) 
<a name="l32384"><span class="ln">32384 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, 1.0, 0.0) 
<a name="l32385"><span class="ln">32385 </span></a>        tensor([[0., 1.], 
<a name="l32386"><span class="ln">32386 </span></a>                [1., 0.], 
<a name="l32387"><span class="ln">32387 </span></a>                [1., 0.]]) 
<a name="l32388"><span class="ln">32388 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, y) 
<a name="l32389"><span class="ln">32389 </span></a>        tensor([[ 1.0000,  0.3139], 
<a name="l32390"><span class="ln">32390 </span></a>                [ 0.3898,  1.0000], 
<a name="l32391"><span class="ln">32391 </span></a>                [ 0.0478,  1.0000]]) 
<a name="l32392"><span class="ln">32392 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 2, dtype=torch.double) 
<a name="l32393"><span class="ln">32393 </span></a>        &gt;&gt;&gt; x 
<a name="l32394"><span class="ln">32394 </span></a>        tensor([[ 1.0779,  0.0383], 
<a name="l32395"><span class="ln">32395 </span></a>                [-0.8785, -1.1089]], dtype=torch.float64) 
<a name="l32396"><span class="ln">32396 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, 0.) 
<a name="l32397"><span class="ln">32397 </span></a>        tensor([[1.0779, 0.0383], 
<a name="l32398"><span class="ln">32398 </span></a>                [0.0000, 0.0000]], dtype=torch.float64) 
<a name="l32399"><span class="ln">32399 </span></a> 
<a name="l32400"><span class="ln">32400 </span></a>    .. function:: where(condition) -&gt; tuple of LongTensor 
<a name="l32401"><span class="ln">32401 </span></a>       :noindex: 
<a name="l32402"><span class="ln">32402 </span></a> 
<a name="l32403"><span class="ln">32403 </span></a>    ``torch.where(condition)`` is identical to 
<a name="l32404"><span class="ln">32404 </span></a>    ``torch.nonzero(condition, as_tuple=True)``. 
<a name="l32405"><span class="ln">32405 </span></a> 
<a name="l32406"><span class="ln">32406 </span></a>    .. note:: 
<a name="l32407"><span class="ln">32407 </span></a>        See also :func:`torch.nonzero`. 
<a name="l32408"><span class="ln">32408 </span></a>    &quot;&quot;&quot;</span>
<a name="l32409"><span class="ln">32409 </span></a>
<a name="l32410"><span class="ln">32410 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32411"><span class="ln">32411 </span></a><span class="s2">def </span><span class="s1">where</span><span class="s3">(</span>
<a name="l32412"><span class="ln">32412 </span></a>    <span class="s1">condition</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32413"><span class="ln">32413 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32414"><span class="ln">32414 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32415"><span class="ln">32415 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32416"><span class="ln">32416 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32417"><span class="ln">32417 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32418"><span class="ln">32418 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32419"><span class="ln">32419 </span></a>    where(condition, input, other, *, out=None) -&gt; Tensor 
<a name="l32420"><span class="ln">32420 </span></a> 
<a name="l32421"><span class="ln">32421 </span></a>    Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`. 
<a name="l32422"><span class="ln">32422 </span></a> 
<a name="l32423"><span class="ln">32423 </span></a>    The operation is defined as: 
<a name="l32424"><span class="ln">32424 </span></a> 
<a name="l32425"><span class="ln">32425 </span></a>    .. math:: 
<a name="l32426"><span class="ln">32426 </span></a>        \text{out}_i = \begin{cases} 
<a name="l32427"><span class="ln">32427 </span></a>            \text{input}_i &amp; \text{if } \text{condition}_i \\ 
<a name="l32428"><span class="ln">32428 </span></a>            \text{other}_i &amp; \text{otherwise} \\ 
<a name="l32429"><span class="ln">32429 </span></a>        \end{cases} 
<a name="l32430"><span class="ln">32430 </span></a> 
<a name="l32431"><span class="ln">32431 </span></a>    .. note:: 
<a name="l32432"><span class="ln">32432 </span></a>        The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l32433"><span class="ln">32433 </span></a> 
<a name="l32434"><span class="ln">32434 </span></a>    Arguments: 
<a name="l32435"><span class="ln">32435 </span></a>        condition (BoolTensor): When True (nonzero), yield input, otherwise yield other 
<a name="l32436"><span class="ln">32436 </span></a>        input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices 
<a name="l32437"><span class="ln">32437 </span></a>                              where :attr:`condition` is ``True`` 
<a name="l32438"><span class="ln">32438 </span></a>        other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices 
<a name="l32439"><span class="ln">32439 </span></a>                              where :attr:`condition` is ``False`` 
<a name="l32440"><span class="ln">32440 </span></a> 
<a name="l32441"><span class="ln">32441 </span></a>    Keyword args: 
<a name="l32442"><span class="ln">32442 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32443"><span class="ln">32443 </span></a> 
<a name="l32444"><span class="ln">32444 </span></a>    Returns: 
<a name="l32445"><span class="ln">32445 </span></a>        Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other` 
<a name="l32446"><span class="ln">32446 </span></a> 
<a name="l32447"><span class="ln">32447 </span></a>    Example:: 
<a name="l32448"><span class="ln">32448 </span></a> 
<a name="l32449"><span class="ln">32449 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 2) 
<a name="l32450"><span class="ln">32450 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 2) 
<a name="l32451"><span class="ln">32451 </span></a>        &gt;&gt;&gt; x 
<a name="l32452"><span class="ln">32452 </span></a>        tensor([[-0.4620,  0.3139], 
<a name="l32453"><span class="ln">32453 </span></a>                [ 0.3898, -0.7197], 
<a name="l32454"><span class="ln">32454 </span></a>                [ 0.0478, -0.1657]]) 
<a name="l32455"><span class="ln">32455 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, 1.0, 0.0) 
<a name="l32456"><span class="ln">32456 </span></a>        tensor([[0., 1.], 
<a name="l32457"><span class="ln">32457 </span></a>                [1., 0.], 
<a name="l32458"><span class="ln">32458 </span></a>                [1., 0.]]) 
<a name="l32459"><span class="ln">32459 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, y) 
<a name="l32460"><span class="ln">32460 </span></a>        tensor([[ 1.0000,  0.3139], 
<a name="l32461"><span class="ln">32461 </span></a>                [ 0.3898,  1.0000], 
<a name="l32462"><span class="ln">32462 </span></a>                [ 0.0478,  1.0000]]) 
<a name="l32463"><span class="ln">32463 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 2, dtype=torch.double) 
<a name="l32464"><span class="ln">32464 </span></a>        &gt;&gt;&gt; x 
<a name="l32465"><span class="ln">32465 </span></a>        tensor([[ 1.0779,  0.0383], 
<a name="l32466"><span class="ln">32466 </span></a>                [-0.8785, -1.1089]], dtype=torch.float64) 
<a name="l32467"><span class="ln">32467 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, 0.) 
<a name="l32468"><span class="ln">32468 </span></a>        tensor([[1.0779, 0.0383], 
<a name="l32469"><span class="ln">32469 </span></a>                [0.0000, 0.0000]], dtype=torch.float64) 
<a name="l32470"><span class="ln">32470 </span></a> 
<a name="l32471"><span class="ln">32471 </span></a>    .. function:: where(condition) -&gt; tuple of LongTensor 
<a name="l32472"><span class="ln">32472 </span></a>       :noindex: 
<a name="l32473"><span class="ln">32473 </span></a> 
<a name="l32474"><span class="ln">32474 </span></a>    ``torch.where(condition)`` is identical to 
<a name="l32475"><span class="ln">32475 </span></a>    ``torch.nonzero(condition, as_tuple=True)``. 
<a name="l32476"><span class="ln">32476 </span></a> 
<a name="l32477"><span class="ln">32477 </span></a>    .. note:: 
<a name="l32478"><span class="ln">32478 </span></a>        See also :func:`torch.nonzero`. 
<a name="l32479"><span class="ln">32479 </span></a>    &quot;&quot;&quot;</span>
<a name="l32480"><span class="ln">32480 </span></a>
<a name="l32481"><span class="ln">32481 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32482"><span class="ln">32482 </span></a><span class="s2">def </span><span class="s1">where</span><span class="s3">(</span>
<a name="l32483"><span class="ln">32483 </span></a>    <span class="s1">condition</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32484"><span class="ln">32484 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l32485"><span class="ln">32485 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32486"><span class="ln">32486 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32487"><span class="ln">32487 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32488"><span class="ln">32488 </span></a>    where(condition, input, other, *, out=None) -&gt; Tensor 
<a name="l32489"><span class="ln">32489 </span></a> 
<a name="l32490"><span class="ln">32490 </span></a>    Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`. 
<a name="l32491"><span class="ln">32491 </span></a> 
<a name="l32492"><span class="ln">32492 </span></a>    The operation is defined as: 
<a name="l32493"><span class="ln">32493 </span></a> 
<a name="l32494"><span class="ln">32494 </span></a>    .. math:: 
<a name="l32495"><span class="ln">32495 </span></a>        \text{out}_i = \begin{cases} 
<a name="l32496"><span class="ln">32496 </span></a>            \text{input}_i &amp; \text{if } \text{condition}_i \\ 
<a name="l32497"><span class="ln">32497 </span></a>            \text{other}_i &amp; \text{otherwise} \\ 
<a name="l32498"><span class="ln">32498 </span></a>        \end{cases} 
<a name="l32499"><span class="ln">32499 </span></a> 
<a name="l32500"><span class="ln">32500 </span></a>    .. note:: 
<a name="l32501"><span class="ln">32501 </span></a>        The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l32502"><span class="ln">32502 </span></a> 
<a name="l32503"><span class="ln">32503 </span></a>    Arguments: 
<a name="l32504"><span class="ln">32504 </span></a>        condition (BoolTensor): When True (nonzero), yield input, otherwise yield other 
<a name="l32505"><span class="ln">32505 </span></a>        input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices 
<a name="l32506"><span class="ln">32506 </span></a>                              where :attr:`condition` is ``True`` 
<a name="l32507"><span class="ln">32507 </span></a>        other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices 
<a name="l32508"><span class="ln">32508 </span></a>                              where :attr:`condition` is ``False`` 
<a name="l32509"><span class="ln">32509 </span></a> 
<a name="l32510"><span class="ln">32510 </span></a>    Keyword args: 
<a name="l32511"><span class="ln">32511 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32512"><span class="ln">32512 </span></a> 
<a name="l32513"><span class="ln">32513 </span></a>    Returns: 
<a name="l32514"><span class="ln">32514 </span></a>        Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other` 
<a name="l32515"><span class="ln">32515 </span></a> 
<a name="l32516"><span class="ln">32516 </span></a>    Example:: 
<a name="l32517"><span class="ln">32517 </span></a> 
<a name="l32518"><span class="ln">32518 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 2) 
<a name="l32519"><span class="ln">32519 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 2) 
<a name="l32520"><span class="ln">32520 </span></a>        &gt;&gt;&gt; x 
<a name="l32521"><span class="ln">32521 </span></a>        tensor([[-0.4620,  0.3139], 
<a name="l32522"><span class="ln">32522 </span></a>                [ 0.3898, -0.7197], 
<a name="l32523"><span class="ln">32523 </span></a>                [ 0.0478, -0.1657]]) 
<a name="l32524"><span class="ln">32524 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, 1.0, 0.0) 
<a name="l32525"><span class="ln">32525 </span></a>        tensor([[0., 1.], 
<a name="l32526"><span class="ln">32526 </span></a>                [1., 0.], 
<a name="l32527"><span class="ln">32527 </span></a>                [1., 0.]]) 
<a name="l32528"><span class="ln">32528 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, y) 
<a name="l32529"><span class="ln">32529 </span></a>        tensor([[ 1.0000,  0.3139], 
<a name="l32530"><span class="ln">32530 </span></a>                [ 0.3898,  1.0000], 
<a name="l32531"><span class="ln">32531 </span></a>                [ 0.0478,  1.0000]]) 
<a name="l32532"><span class="ln">32532 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 2, dtype=torch.double) 
<a name="l32533"><span class="ln">32533 </span></a>        &gt;&gt;&gt; x 
<a name="l32534"><span class="ln">32534 </span></a>        tensor([[ 1.0779,  0.0383], 
<a name="l32535"><span class="ln">32535 </span></a>                [-0.8785, -1.1089]], dtype=torch.float64) 
<a name="l32536"><span class="ln">32536 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, 0.) 
<a name="l32537"><span class="ln">32537 </span></a>        tensor([[1.0779, 0.0383], 
<a name="l32538"><span class="ln">32538 </span></a>                [0.0000, 0.0000]], dtype=torch.float64) 
<a name="l32539"><span class="ln">32539 </span></a> 
<a name="l32540"><span class="ln">32540 </span></a>    .. function:: where(condition) -&gt; tuple of LongTensor 
<a name="l32541"><span class="ln">32541 </span></a>       :noindex: 
<a name="l32542"><span class="ln">32542 </span></a> 
<a name="l32543"><span class="ln">32543 </span></a>    ``torch.where(condition)`` is identical to 
<a name="l32544"><span class="ln">32544 </span></a>    ``torch.nonzero(condition, as_tuple=True)``. 
<a name="l32545"><span class="ln">32545 </span></a> 
<a name="l32546"><span class="ln">32546 </span></a>    .. note:: 
<a name="l32547"><span class="ln">32547 </span></a>        See also :func:`torch.nonzero`. 
<a name="l32548"><span class="ln">32548 </span></a>    &quot;&quot;&quot;</span>
<a name="l32549"><span class="ln">32549 </span></a>
<a name="l32550"><span class="ln">32550 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32551"><span class="ln">32551 </span></a><span class="s2">def </span><span class="s1">where</span><span class="s3">(</span>
<a name="l32552"><span class="ln">32552 </span></a>    <span class="s1">condition</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32553"><span class="ln">32553 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32554"><span class="ln">32554 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l32555"><span class="ln">32555 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32556"><span class="ln">32556 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32557"><span class="ln">32557 </span></a>    where(condition, input, other, *, out=None) -&gt; Tensor 
<a name="l32558"><span class="ln">32558 </span></a> 
<a name="l32559"><span class="ln">32559 </span></a>    Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`. 
<a name="l32560"><span class="ln">32560 </span></a> 
<a name="l32561"><span class="ln">32561 </span></a>    The operation is defined as: 
<a name="l32562"><span class="ln">32562 </span></a> 
<a name="l32563"><span class="ln">32563 </span></a>    .. math:: 
<a name="l32564"><span class="ln">32564 </span></a>        \text{out}_i = \begin{cases} 
<a name="l32565"><span class="ln">32565 </span></a>            \text{input}_i &amp; \text{if } \text{condition}_i \\ 
<a name="l32566"><span class="ln">32566 </span></a>            \text{other}_i &amp; \text{otherwise} \\ 
<a name="l32567"><span class="ln">32567 </span></a>        \end{cases} 
<a name="l32568"><span class="ln">32568 </span></a> 
<a name="l32569"><span class="ln">32569 </span></a>    .. note:: 
<a name="l32570"><span class="ln">32570 </span></a>        The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l32571"><span class="ln">32571 </span></a> 
<a name="l32572"><span class="ln">32572 </span></a>    Arguments: 
<a name="l32573"><span class="ln">32573 </span></a>        condition (BoolTensor): When True (nonzero), yield input, otherwise yield other 
<a name="l32574"><span class="ln">32574 </span></a>        input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices 
<a name="l32575"><span class="ln">32575 </span></a>                              where :attr:`condition` is ``True`` 
<a name="l32576"><span class="ln">32576 </span></a>        other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices 
<a name="l32577"><span class="ln">32577 </span></a>                              where :attr:`condition` is ``False`` 
<a name="l32578"><span class="ln">32578 </span></a> 
<a name="l32579"><span class="ln">32579 </span></a>    Keyword args: 
<a name="l32580"><span class="ln">32580 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32581"><span class="ln">32581 </span></a> 
<a name="l32582"><span class="ln">32582 </span></a>    Returns: 
<a name="l32583"><span class="ln">32583 </span></a>        Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other` 
<a name="l32584"><span class="ln">32584 </span></a> 
<a name="l32585"><span class="ln">32585 </span></a>    Example:: 
<a name="l32586"><span class="ln">32586 </span></a> 
<a name="l32587"><span class="ln">32587 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 2) 
<a name="l32588"><span class="ln">32588 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 2) 
<a name="l32589"><span class="ln">32589 </span></a>        &gt;&gt;&gt; x 
<a name="l32590"><span class="ln">32590 </span></a>        tensor([[-0.4620,  0.3139], 
<a name="l32591"><span class="ln">32591 </span></a>                [ 0.3898, -0.7197], 
<a name="l32592"><span class="ln">32592 </span></a>                [ 0.0478, -0.1657]]) 
<a name="l32593"><span class="ln">32593 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, 1.0, 0.0) 
<a name="l32594"><span class="ln">32594 </span></a>        tensor([[0., 1.], 
<a name="l32595"><span class="ln">32595 </span></a>                [1., 0.], 
<a name="l32596"><span class="ln">32596 </span></a>                [1., 0.]]) 
<a name="l32597"><span class="ln">32597 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, y) 
<a name="l32598"><span class="ln">32598 </span></a>        tensor([[ 1.0000,  0.3139], 
<a name="l32599"><span class="ln">32599 </span></a>                [ 0.3898,  1.0000], 
<a name="l32600"><span class="ln">32600 </span></a>                [ 0.0478,  1.0000]]) 
<a name="l32601"><span class="ln">32601 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 2, dtype=torch.double) 
<a name="l32602"><span class="ln">32602 </span></a>        &gt;&gt;&gt; x 
<a name="l32603"><span class="ln">32603 </span></a>        tensor([[ 1.0779,  0.0383], 
<a name="l32604"><span class="ln">32604 </span></a>                [-0.8785, -1.1089]], dtype=torch.float64) 
<a name="l32605"><span class="ln">32605 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, 0.) 
<a name="l32606"><span class="ln">32606 </span></a>        tensor([[1.0779, 0.0383], 
<a name="l32607"><span class="ln">32607 </span></a>                [0.0000, 0.0000]], dtype=torch.float64) 
<a name="l32608"><span class="ln">32608 </span></a> 
<a name="l32609"><span class="ln">32609 </span></a>    .. function:: where(condition) -&gt; tuple of LongTensor 
<a name="l32610"><span class="ln">32610 </span></a>       :noindex: 
<a name="l32611"><span class="ln">32611 </span></a> 
<a name="l32612"><span class="ln">32612 </span></a>    ``torch.where(condition)`` is identical to 
<a name="l32613"><span class="ln">32613 </span></a>    ``torch.nonzero(condition, as_tuple=True)``. 
<a name="l32614"><span class="ln">32614 </span></a> 
<a name="l32615"><span class="ln">32615 </span></a>    .. note:: 
<a name="l32616"><span class="ln">32616 </span></a>        See also :func:`torch.nonzero`. 
<a name="l32617"><span class="ln">32617 </span></a>    &quot;&quot;&quot;</span>
<a name="l32618"><span class="ln">32618 </span></a>
<a name="l32619"><span class="ln">32619 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32620"><span class="ln">32620 </span></a><span class="s2">def </span><span class="s1">where</span><span class="s3">(</span>
<a name="l32621"><span class="ln">32621 </span></a>    <span class="s1">condition</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32622"><span class="ln">32622 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l32623"><span class="ln">32623 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l32624"><span class="ln">32624 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32625"><span class="ln">32625 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32626"><span class="ln">32626 </span></a>    where(condition, input, other, *, out=None) -&gt; Tensor 
<a name="l32627"><span class="ln">32627 </span></a> 
<a name="l32628"><span class="ln">32628 </span></a>    Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`. 
<a name="l32629"><span class="ln">32629 </span></a> 
<a name="l32630"><span class="ln">32630 </span></a>    The operation is defined as: 
<a name="l32631"><span class="ln">32631 </span></a> 
<a name="l32632"><span class="ln">32632 </span></a>    .. math:: 
<a name="l32633"><span class="ln">32633 </span></a>        \text{out}_i = \begin{cases} 
<a name="l32634"><span class="ln">32634 </span></a>            \text{input}_i &amp; \text{if } \text{condition}_i \\ 
<a name="l32635"><span class="ln">32635 </span></a>            \text{other}_i &amp; \text{otherwise} \\ 
<a name="l32636"><span class="ln">32636 </span></a>        \end{cases} 
<a name="l32637"><span class="ln">32637 </span></a> 
<a name="l32638"><span class="ln">32638 </span></a>    .. note:: 
<a name="l32639"><span class="ln">32639 </span></a>        The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics&gt;`. 
<a name="l32640"><span class="ln">32640 </span></a> 
<a name="l32641"><span class="ln">32641 </span></a>    Arguments: 
<a name="l32642"><span class="ln">32642 </span></a>        condition (BoolTensor): When True (nonzero), yield input, otherwise yield other 
<a name="l32643"><span class="ln">32643 </span></a>        input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices 
<a name="l32644"><span class="ln">32644 </span></a>                              where :attr:`condition` is ``True`` 
<a name="l32645"><span class="ln">32645 </span></a>        other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices 
<a name="l32646"><span class="ln">32646 </span></a>                              where :attr:`condition` is ``False`` 
<a name="l32647"><span class="ln">32647 </span></a> 
<a name="l32648"><span class="ln">32648 </span></a>    Keyword args: 
<a name="l32649"><span class="ln">32649 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32650"><span class="ln">32650 </span></a> 
<a name="l32651"><span class="ln">32651 </span></a>    Returns: 
<a name="l32652"><span class="ln">32652 </span></a>        Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other` 
<a name="l32653"><span class="ln">32653 </span></a> 
<a name="l32654"><span class="ln">32654 </span></a>    Example:: 
<a name="l32655"><span class="ln">32655 </span></a> 
<a name="l32656"><span class="ln">32656 </span></a>        &gt;&gt;&gt; x = torch.randn(3, 2) 
<a name="l32657"><span class="ln">32657 </span></a>        &gt;&gt;&gt; y = torch.ones(3, 2) 
<a name="l32658"><span class="ln">32658 </span></a>        &gt;&gt;&gt; x 
<a name="l32659"><span class="ln">32659 </span></a>        tensor([[-0.4620,  0.3139], 
<a name="l32660"><span class="ln">32660 </span></a>                [ 0.3898, -0.7197], 
<a name="l32661"><span class="ln">32661 </span></a>                [ 0.0478, -0.1657]]) 
<a name="l32662"><span class="ln">32662 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, 1.0, 0.0) 
<a name="l32663"><span class="ln">32663 </span></a>        tensor([[0., 1.], 
<a name="l32664"><span class="ln">32664 </span></a>                [1., 0.], 
<a name="l32665"><span class="ln">32665 </span></a>                [1., 0.]]) 
<a name="l32666"><span class="ln">32666 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, y) 
<a name="l32667"><span class="ln">32667 </span></a>        tensor([[ 1.0000,  0.3139], 
<a name="l32668"><span class="ln">32668 </span></a>                [ 0.3898,  1.0000], 
<a name="l32669"><span class="ln">32669 </span></a>                [ 0.0478,  1.0000]]) 
<a name="l32670"><span class="ln">32670 </span></a>        &gt;&gt;&gt; x = torch.randn(2, 2, dtype=torch.double) 
<a name="l32671"><span class="ln">32671 </span></a>        &gt;&gt;&gt; x 
<a name="l32672"><span class="ln">32672 </span></a>        tensor([[ 1.0779,  0.0383], 
<a name="l32673"><span class="ln">32673 </span></a>                [-0.8785, -1.1089]], dtype=torch.float64) 
<a name="l32674"><span class="ln">32674 </span></a>        &gt;&gt;&gt; torch.where(x &gt; 0, x, 0.) 
<a name="l32675"><span class="ln">32675 </span></a>        tensor([[1.0779, 0.0383], 
<a name="l32676"><span class="ln">32676 </span></a>                [0.0000, 0.0000]], dtype=torch.float64) 
<a name="l32677"><span class="ln">32677 </span></a> 
<a name="l32678"><span class="ln">32678 </span></a>    .. function:: where(condition) -&gt; tuple of LongTensor 
<a name="l32679"><span class="ln">32679 </span></a>       :noindex: 
<a name="l32680"><span class="ln">32680 </span></a> 
<a name="l32681"><span class="ln">32681 </span></a>    ``torch.where(condition)`` is identical to 
<a name="l32682"><span class="ln">32682 </span></a>    ``torch.nonzero(condition, as_tuple=True)``. 
<a name="l32683"><span class="ln">32683 </span></a> 
<a name="l32684"><span class="ln">32684 </span></a>    .. note:: 
<a name="l32685"><span class="ln">32685 </span></a>        See also :func:`torch.nonzero`. 
<a name="l32686"><span class="ln">32686 </span></a>    &quot;&quot;&quot;</span>
<a name="l32687"><span class="ln">32687 </span></a>
<a name="l32688"><span class="ln">32688 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32689"><span class="ln">32689 </span></a><span class="s2">def </span><span class="s1">xlogy</span><span class="s3">(</span>
<a name="l32690"><span class="ln">32690 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32691"><span class="ln">32691 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32692"><span class="ln">32692 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32693"><span class="ln">32693 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32694"><span class="ln">32694 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32695"><span class="ln">32695 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32696"><span class="ln">32696 </span></a>    xlogy(input, other, *, out=None) -&gt; Tensor 
<a name="l32697"><span class="ln">32697 </span></a> 
<a name="l32698"><span class="ln">32698 </span></a>    Alias for :func:`torch.special.xlogy`. 
<a name="l32699"><span class="ln">32699 </span></a>    &quot;&quot;&quot;</span>
<a name="l32700"><span class="ln">32700 </span></a>
<a name="l32701"><span class="ln">32701 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32702"><span class="ln">32702 </span></a><span class="s2">def </span><span class="s1">xlogy</span><span class="s3">(</span>
<a name="l32703"><span class="ln">32703 </span></a>    <span class="s1">self</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l32704"><span class="ln">32704 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32705"><span class="ln">32705 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32706"><span class="ln">32706 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32707"><span class="ln">32707 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32708"><span class="ln">32708 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32709"><span class="ln">32709 </span></a>    xlogy(input, other, *, out=None) -&gt; Tensor 
<a name="l32710"><span class="ln">32710 </span></a> 
<a name="l32711"><span class="ln">32711 </span></a>    Alias for :func:`torch.special.xlogy`. 
<a name="l32712"><span class="ln">32712 </span></a>    &quot;&quot;&quot;</span>
<a name="l32713"><span class="ln">32713 </span></a>
<a name="l32714"><span class="ln">32714 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32715"><span class="ln">32715 </span></a><span class="s2">def </span><span class="s1">xlogy</span><span class="s3">(</span>
<a name="l32716"><span class="ln">32716 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32717"><span class="ln">32717 </span></a>    <span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">,</span>
<a name="l32718"><span class="ln">32718 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32719"><span class="ln">32719 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32720"><span class="ln">32720 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32721"><span class="ln">32721 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32722"><span class="ln">32722 </span></a>    xlogy(input, other, *, out=None) -&gt; Tensor 
<a name="l32723"><span class="ln">32723 </span></a> 
<a name="l32724"><span class="ln">32724 </span></a>    Alias for :func:`torch.special.xlogy`. 
<a name="l32725"><span class="ln">32725 </span></a>    &quot;&quot;&quot;</span>
<a name="l32726"><span class="ln">32726 </span></a>
<a name="l32727"><span class="ln">32727 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32728"><span class="ln">32728 </span></a><span class="s2">def </span><span class="s1">xlogy_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l32729"><span class="ln">32729 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32730"><span class="ln">32730 </span></a><span class="s2">def </span><span class="s1">xlogy_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">, </span><span class="s1">other</span><span class="s2">: </span><span class="s1">Number </span><span class="s2">| </span><span class="s1">_complex</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l32731"><span class="ln">32731 </span></a><span class="s2">def </span><span class="s1">zero_</span><span class="s3">(</span><span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">: </span><span class="s3">...</span>
<a name="l32732"><span class="ln">32732 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32733"><span class="ln">32733 </span></a><span class="s2">def </span><span class="s1">zeros</span><span class="s3">(</span>
<a name="l32734"><span class="ln">32734 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">],</span>
<a name="l32735"><span class="ln">32735 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32736"><span class="ln">32736 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32737"><span class="ln">32737 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32738"><span class="ln">32738 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32739"><span class="ln">32739 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32740"><span class="ln">32740 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32741"><span class="ln">32741 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32742"><span class="ln">32742 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32743"><span class="ln">32743 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32744"><span class="ln">32744 </span></a>    zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l32745"><span class="ln">32745 </span></a> 
<a name="l32746"><span class="ln">32746 </span></a>    Returns a tensor filled with the scalar value `0`, with the shape defined 
<a name="l32747"><span class="ln">32747 </span></a>    by the variable argument :attr:`size`. 
<a name="l32748"><span class="ln">32748 </span></a> 
<a name="l32749"><span class="ln">32749 </span></a>    Args: 
<a name="l32750"><span class="ln">32750 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l32751"><span class="ln">32751 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l32752"><span class="ln">32752 </span></a> 
<a name="l32753"><span class="ln">32753 </span></a>    Keyword args: 
<a name="l32754"><span class="ln">32754 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32755"><span class="ln">32755 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l32756"><span class="ln">32756 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l32757"><span class="ln">32757 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l32758"><span class="ln">32758 </span></a>            Default: ``torch.strided``. 
<a name="l32759"><span class="ln">32759 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l32760"><span class="ln">32760 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l32761"><span class="ln">32761 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l32762"><span class="ln">32762 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l32763"><span class="ln">32763 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l32764"><span class="ln">32764 </span></a>            returned tensor. Default: ``False``. 
<a name="l32765"><span class="ln">32765 </span></a> 
<a name="l32766"><span class="ln">32766 </span></a>    Example:: 
<a name="l32767"><span class="ln">32767 </span></a> 
<a name="l32768"><span class="ln">32768 </span></a>        &gt;&gt;&gt; torch.zeros(2, 3) 
<a name="l32769"><span class="ln">32769 </span></a>        tensor([[ 0.,  0.,  0.], 
<a name="l32770"><span class="ln">32770 </span></a>                [ 0.,  0.,  0.]]) 
<a name="l32771"><span class="ln">32771 </span></a> 
<a name="l32772"><span class="ln">32772 </span></a>        &gt;&gt;&gt; torch.zeros(5) 
<a name="l32773"><span class="ln">32773 </span></a>        tensor([ 0.,  0.,  0.,  0.,  0.]) 
<a name="l32774"><span class="ln">32774 </span></a>    &quot;&quot;&quot;</span>
<a name="l32775"><span class="ln">32775 </span></a>
<a name="l32776"><span class="ln">32776 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32777"><span class="ln">32777 </span></a><span class="s2">def </span><span class="s1">zeros</span><span class="s3">(</span>
<a name="l32778"><span class="ln">32778 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int </span><span class="s2">| </span><span class="s1">SymInt</span><span class="s3">,</span>
<a name="l32779"><span class="ln">32779 </span></a>    <span class="s1">out</span><span class="s2">: </span><span class="s1">Tensor </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32780"><span class="ln">32780 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32781"><span class="ln">32781 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32782"><span class="ln">32782 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32783"><span class="ln">32783 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32784"><span class="ln">32784 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32785"><span class="ln">32785 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32786"><span class="ln">32786 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32787"><span class="ln">32787 </span></a>    zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l32788"><span class="ln">32788 </span></a> 
<a name="l32789"><span class="ln">32789 </span></a>    Returns a tensor filled with the scalar value `0`, with the shape defined 
<a name="l32790"><span class="ln">32790 </span></a>    by the variable argument :attr:`size`. 
<a name="l32791"><span class="ln">32791 </span></a> 
<a name="l32792"><span class="ln">32792 </span></a>    Args: 
<a name="l32793"><span class="ln">32793 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l32794"><span class="ln">32794 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l32795"><span class="ln">32795 </span></a> 
<a name="l32796"><span class="ln">32796 </span></a>    Keyword args: 
<a name="l32797"><span class="ln">32797 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32798"><span class="ln">32798 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l32799"><span class="ln">32799 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l32800"><span class="ln">32800 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l32801"><span class="ln">32801 </span></a>            Default: ``torch.strided``. 
<a name="l32802"><span class="ln">32802 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l32803"><span class="ln">32803 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l32804"><span class="ln">32804 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l32805"><span class="ln">32805 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l32806"><span class="ln">32806 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l32807"><span class="ln">32807 </span></a>            returned tensor. Default: ``False``. 
<a name="l32808"><span class="ln">32808 </span></a> 
<a name="l32809"><span class="ln">32809 </span></a>    Example:: 
<a name="l32810"><span class="ln">32810 </span></a> 
<a name="l32811"><span class="ln">32811 </span></a>        &gt;&gt;&gt; torch.zeros(2, 3) 
<a name="l32812"><span class="ln">32812 </span></a>        tensor([[ 0.,  0.,  0.], 
<a name="l32813"><span class="ln">32813 </span></a>                [ 0.,  0.,  0.]]) 
<a name="l32814"><span class="ln">32814 </span></a> 
<a name="l32815"><span class="ln">32815 </span></a>        &gt;&gt;&gt; torch.zeros(5) 
<a name="l32816"><span class="ln">32816 </span></a>        tensor([ 0.,  0.,  0.,  0.,  0.]) 
<a name="l32817"><span class="ln">32817 </span></a>    &quot;&quot;&quot;</span>
<a name="l32818"><span class="ln">32818 </span></a>
<a name="l32819"><span class="ln">32819 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32820"><span class="ln">32820 </span></a><span class="s2">def </span><span class="s1">zeros</span><span class="s3">(</span>
<a name="l32821"><span class="ln">32821 </span></a>    <span class="s1">size</span><span class="s2">: </span><span class="s1">_size</span><span class="s3">,</span>
<a name="l32822"><span class="ln">32822 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32823"><span class="ln">32823 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l32824"><span class="ln">32824 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32825"><span class="ln">32825 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32826"><span class="ln">32826 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32827"><span class="ln">32827 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32828"><span class="ln">32828 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32829"><span class="ln">32829 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32830"><span class="ln">32830 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32831"><span class="ln">32831 </span></a>    zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l32832"><span class="ln">32832 </span></a> 
<a name="l32833"><span class="ln">32833 </span></a>    Returns a tensor filled with the scalar value `0`, with the shape defined 
<a name="l32834"><span class="ln">32834 </span></a>    by the variable argument :attr:`size`. 
<a name="l32835"><span class="ln">32835 </span></a> 
<a name="l32836"><span class="ln">32836 </span></a>    Args: 
<a name="l32837"><span class="ln">32837 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l32838"><span class="ln">32838 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l32839"><span class="ln">32839 </span></a> 
<a name="l32840"><span class="ln">32840 </span></a>    Keyword args: 
<a name="l32841"><span class="ln">32841 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32842"><span class="ln">32842 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l32843"><span class="ln">32843 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l32844"><span class="ln">32844 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l32845"><span class="ln">32845 </span></a>            Default: ``torch.strided``. 
<a name="l32846"><span class="ln">32846 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l32847"><span class="ln">32847 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l32848"><span class="ln">32848 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l32849"><span class="ln">32849 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l32850"><span class="ln">32850 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l32851"><span class="ln">32851 </span></a>            returned tensor. Default: ``False``. 
<a name="l32852"><span class="ln">32852 </span></a> 
<a name="l32853"><span class="ln">32853 </span></a>    Example:: 
<a name="l32854"><span class="ln">32854 </span></a> 
<a name="l32855"><span class="ln">32855 </span></a>        &gt;&gt;&gt; torch.zeros(2, 3) 
<a name="l32856"><span class="ln">32856 </span></a>        tensor([[ 0.,  0.,  0.], 
<a name="l32857"><span class="ln">32857 </span></a>                [ 0.,  0.,  0.]]) 
<a name="l32858"><span class="ln">32858 </span></a> 
<a name="l32859"><span class="ln">32859 </span></a>        &gt;&gt;&gt; torch.zeros(5) 
<a name="l32860"><span class="ln">32860 </span></a>        tensor([ 0.,  0.,  0.,  0.,  0.]) 
<a name="l32861"><span class="ln">32861 </span></a>    &quot;&quot;&quot;</span>
<a name="l32862"><span class="ln">32862 </span></a>
<a name="l32863"><span class="ln">32863 </span></a><span class="s2">@</span><span class="s1">overload</span>
<a name="l32864"><span class="ln">32864 </span></a><span class="s2">def </span><span class="s1">zeros</span><span class="s3">(</span>
<a name="l32865"><span class="ln">32865 </span></a>    <span class="s2">*</span><span class="s1">size</span><span class="s2">: </span><span class="s1">_int</span><span class="s3">,</span>
<a name="l32866"><span class="ln">32866 </span></a>    <span class="s1">names</span><span class="s2">: </span><span class="s1">Sequence</span><span class="s3">[</span><span class="s1">str </span><span class="s2">| </span><span class="s1">EllipsisType </span><span class="s2">| None</span><span class="s3">] </span><span class="s2">| None</span><span class="s3">,</span>
<a name="l32867"><span class="ln">32867 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32868"><span class="ln">32868 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32869"><span class="ln">32869 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32870"><span class="ln">32870 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32871"><span class="ln">32871 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32872"><span class="ln">32872 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32873"><span class="ln">32873 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32874"><span class="ln">32874 </span></a>    zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor 
<a name="l32875"><span class="ln">32875 </span></a> 
<a name="l32876"><span class="ln">32876 </span></a>    Returns a tensor filled with the scalar value `0`, with the shape defined 
<a name="l32877"><span class="ln">32877 </span></a>    by the variable argument :attr:`size`. 
<a name="l32878"><span class="ln">32878 </span></a> 
<a name="l32879"><span class="ln">32879 </span></a>    Args: 
<a name="l32880"><span class="ln">32880 </span></a>        size (int...): a sequence of integers defining the shape of the output tensor. 
<a name="l32881"><span class="ln">32881 </span></a>            Can be a variable number of arguments or a collection like a list or tuple. 
<a name="l32882"><span class="ln">32882 </span></a> 
<a name="l32883"><span class="ln">32883 </span></a>    Keyword args: 
<a name="l32884"><span class="ln">32884 </span></a>        out (Tensor, optional): the output tensor. 
<a name="l32885"><span class="ln">32885 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. 
<a name="l32886"><span class="ln">32886 </span></a>            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). 
<a name="l32887"><span class="ln">32887 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. 
<a name="l32888"><span class="ln">32888 </span></a>            Default: ``torch.strided``. 
<a name="l32889"><span class="ln">32889 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l32890"><span class="ln">32890 </span></a>            Default: if ``None``, uses the current device for the default tensor type 
<a name="l32891"><span class="ln">32891 </span></a>            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU 
<a name="l32892"><span class="ln">32892 </span></a>            for CPU tensor types and the current CUDA device for CUDA tensor types. 
<a name="l32893"><span class="ln">32893 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l32894"><span class="ln">32894 </span></a>            returned tensor. Default: ``False``. 
<a name="l32895"><span class="ln">32895 </span></a> 
<a name="l32896"><span class="ln">32896 </span></a>    Example:: 
<a name="l32897"><span class="ln">32897 </span></a> 
<a name="l32898"><span class="ln">32898 </span></a>        &gt;&gt;&gt; torch.zeros(2, 3) 
<a name="l32899"><span class="ln">32899 </span></a>        tensor([[ 0.,  0.,  0.], 
<a name="l32900"><span class="ln">32900 </span></a>                [ 0.,  0.,  0.]]) 
<a name="l32901"><span class="ln">32901 </span></a> 
<a name="l32902"><span class="ln">32902 </span></a>        &gt;&gt;&gt; torch.zeros(5) 
<a name="l32903"><span class="ln">32903 </span></a>        tensor([ 0.,  0.,  0.,  0.,  0.]) 
<a name="l32904"><span class="ln">32904 </span></a>    &quot;&quot;&quot;</span>
<a name="l32905"><span class="ln">32905 </span></a>
<a name="l32906"><span class="ln">32906 </span></a><span class="s2">def </span><span class="s1">zeros_like</span><span class="s3">(</span>
<a name="l32907"><span class="ln">32907 </span></a>    <span class="s1">input</span><span class="s2">: </span><span class="s1">Tensor</span><span class="s3">,</span>
<a name="l32908"><span class="ln">32908 </span></a>    <span class="s2">*</span><span class="s3">,</span>
<a name="l32909"><span class="ln">32909 </span></a>    <span class="s1">memory_format</span><span class="s2">: </span><span class="s1">memory_format </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32910"><span class="ln">32910 </span></a>    <span class="s1">dtype</span><span class="s2">: </span><span class="s1">_dtype </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32911"><span class="ln">32911 </span></a>    <span class="s1">layout</span><span class="s2">: </span><span class="s1">_layout </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32912"><span class="ln">32912 </span></a>    <span class="s1">device</span><span class="s2">: </span><span class="s1">DeviceLikeType </span><span class="s2">| None = None</span><span class="s3">,</span>
<a name="l32913"><span class="ln">32913 </span></a>    <span class="s1">pin_memory</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32914"><span class="ln">32914 </span></a>    <span class="s1">requires_grad</span><span class="s2">: </span><span class="s1">_bool </span><span class="s2">| None = False</span><span class="s3">,</span>
<a name="l32915"><span class="ln">32915 </span></a><span class="s3">) </span><span class="s1">-&gt; Tensor</span><span class="s2">:</span>
<a name="l32916"><span class="ln">32916 </span></a>    <span class="s0">r&quot;&quot;&quot; 
<a name="l32917"><span class="ln">32917 </span></a>    zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -&gt; Tensor 
<a name="l32918"><span class="ln">32918 </span></a> 
<a name="l32919"><span class="ln">32919 </span></a>    Returns a tensor filled with the scalar value `0`, with the same size as 
<a name="l32920"><span class="ln">32920 </span></a>    :attr:`input`. ``torch.zeros_like(input)`` is equivalent to 
<a name="l32921"><span class="ln">32921 </span></a>    ``torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``. 
<a name="l32922"><span class="ln">32922 </span></a> 
<a name="l32923"><span class="ln">32923 </span></a>    .. warning:: 
<a name="l32924"><span class="ln">32924 </span></a>        As of 0.4, this function does not support an :attr:`out` keyword. As an alternative, 
<a name="l32925"><span class="ln">32925 </span></a>        the old ``torch.zeros_like(input, out=output)`` is equivalent to 
<a name="l32926"><span class="ln">32926 </span></a>        ``torch.zeros(input.size(), out=output)``. 
<a name="l32927"><span class="ln">32927 </span></a> 
<a name="l32928"><span class="ln">32928 </span></a>    Args: 
<a name="l32929"><span class="ln">32929 </span></a>        input (Tensor): the size of :attr:`input` will determine size of the output tensor. 
<a name="l32930"><span class="ln">32930 </span></a> 
<a name="l32931"><span class="ln">32931 </span></a>    Keyword args: 
<a name="l32932"><span class="ln">32932 </span></a>        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor. 
<a name="l32933"><span class="ln">32933 </span></a>            Default: if ``None``, defaults to the dtype of :attr:`input`. 
<a name="l32934"><span class="ln">32934 </span></a>        layout (:class:`torch.layout`, optional): the desired layout of returned tensor. 
<a name="l32935"><span class="ln">32935 </span></a>            Default: if ``None``, defaults to the layout of :attr:`input`. 
<a name="l32936"><span class="ln">32936 </span></a>        device (:class:`torch.device`, optional): the desired device of returned tensor. 
<a name="l32937"><span class="ln">32937 </span></a>            Default: if ``None``, defaults to the device of :attr:`input`. 
<a name="l32938"><span class="ln">32938 </span></a>        requires_grad (bool, optional): If autograd should record operations on the 
<a name="l32939"><span class="ln">32939 </span></a>            returned tensor. Default: ``False``. 
<a name="l32940"><span class="ln">32940 </span></a>        memory_format (:class:`torch.memory_format`, optional): the desired memory format of 
<a name="l32941"><span class="ln">32941 </span></a>            returned Tensor. Default: ``torch.preserve_format``. 
<a name="l32942"><span class="ln">32942 </span></a> 
<a name="l32943"><span class="ln">32943 </span></a>    Example:: 
<a name="l32944"><span class="ln">32944 </span></a> 
<a name="l32945"><span class="ln">32945 </span></a>        &gt;&gt;&gt; input = torch.empty(2, 3) 
<a name="l32946"><span class="ln">32946 </span></a>        &gt;&gt;&gt; torch.zeros_like(input) 
<a name="l32947"><span class="ln">32947 </span></a>        tensor([[ 0.,  0.,  0.], 
<a name="l32948"><span class="ln">32948 </span></a>                [ 0.,  0.,  0.]]) 
<a name="l32949"><span class="ln">32949 </span></a>    &quot;&quot;&quot;</span>
<a name="l32950"><span class="ln">32950 </span></a></pre>
</body>
</html>