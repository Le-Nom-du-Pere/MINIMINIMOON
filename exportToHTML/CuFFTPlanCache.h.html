<html>
<head>
<title>CuFFTPlanCache.h</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #000080; font-weight: bold;}
.s1 { color: #333333;}
.s2 { color: #660e7a; font-weight: bold;}
.s3 { color: #969896; font-style: italic;}
.s4 { color: #0086b3;}
.s5 { color: #183691; font-weight: bold;}
.s6 { color: #006666; font-weight: bold;}
.ln { color: #333333; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
CuFFTPlanCache.h</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">#include </span><span class="s1">&lt;ATen/Config.h&gt;</span>
<a name="l2"><span class="ln">2    </span></a><span class="s0">#include </span><span class="s1">&lt;ATen/core/DimVector.h&gt;</span>
<a name="l3"><span class="ln">3    </span></a><span class="s0">#include </span><span class="s1">&lt;ATen/cuda/CUDAContext.h&gt;</span>
<a name="l4"><span class="ln">4    </span></a><span class="s0">#include </span><span class="s1">&lt;ATen/native/cuda/CuFFTUtils.h&gt;</span>
<a name="l5"><span class="ln">5    </span></a><span class="s0">#include </span><span class="s1">&lt;ATen/native/utils/ParamsHash.h&gt;</span>
<a name="l6"><span class="ln">6    </span></a><span class="s0">#include </span><span class="s1">&lt;c10/util/accumulate.h&gt;</span>
<a name="l7"><span class="ln">7    </span></a><span class="s0">#include </span><span class="s1">&lt;c10/util/irange.h&gt;</span>
<a name="l8"><span class="ln">8    </span></a>
<a name="l9"><span class="ln">9    </span></a><span class="s0">#include </span><span class="s1">&lt;cufft.h&gt;</span>
<a name="l10"><span class="ln">10   </span></a><span class="s0">#include </span><span class="s1">&lt;cufftXt.h&gt;</span>
<a name="l11"><span class="ln">11   </span></a>
<a name="l12"><span class="ln">12   </span></a><span class="s0">#include </span><span class="s1">&lt;limits&gt;</span>
<a name="l13"><span class="ln">13   </span></a><span class="s0">#include </span><span class="s1">&lt;list&gt;</span>
<a name="l14"><span class="ln">14   </span></a><span class="s0">#include </span><span class="s1">&lt;sstream&gt;</span>
<a name="l15"><span class="ln">15   </span></a><span class="s0">#include </span><span class="s1">&lt;stdexcept&gt;</span>
<a name="l16"><span class="ln">16   </span></a><span class="s0">#include </span><span class="s1">&lt;string&gt;</span>
<a name="l17"><span class="ln">17   </span></a><span class="s0">#include </span><span class="s1">&lt;unordered_map&gt;</span>
<a name="l18"><span class="ln">18   </span></a>
<a name="l19"><span class="ln">19   </span></a><span class="s2">namespace </span><span class="s1">at::native::detail {</span>
<a name="l20"><span class="ln">20   </span></a>
<a name="l21"><span class="ln">21   </span></a><span class="s3">// Enum representing the FFT type</span>
<a name="l22"><span class="ln">22   </span></a><span class="s0">enum </span><span class="s2">class </span><span class="s1">CuFFTTransformType : int8_t {</span>
<a name="l23"><span class="ln">23   </span></a>  <span class="s1">C2C,  </span><span class="s3">// Complex-to-complex</span>
<a name="l24"><span class="ln">24   </span></a>  <span class="s1">R2C,  </span><span class="s3">// Real-to-complex</span>
<a name="l25"><span class="ln">25   </span></a>  <span class="s1">C2R,  </span><span class="s3">// Complex-to-real</span>
<a name="l26"><span class="ln">26   </span></a><span class="s1">};</span>
<a name="l27"><span class="ln">27   </span></a>
<a name="l28"><span class="ln">28   </span></a><span class="s3">// This struct is used to let us easily compute hashes of the</span>
<a name="l29"><span class="ln">29   </span></a><span class="s3">// parameters.</span>
<a name="l30"><span class="ln">30   </span></a><span class="s3">// It will be the **key** to the plan cache.</span>
<a name="l31"><span class="ln">31   </span></a><span class="s0">struct </span><span class="s1">CuFFTParams</span>
<a name="l32"><span class="ln">32   </span></a><span class="s1">{</span>
<a name="l33"><span class="ln">33   </span></a>  <span class="s1">int64_t signal_ndim_; </span><span class="s3">// between 1 and max_rank, i.e., 1 &lt;= signal_ndim &lt;= 3</span>
<a name="l34"><span class="ln">34   </span></a>  <span class="s3">// These include additional batch dimension as well.</span>
<a name="l35"><span class="ln">35   </span></a>  <span class="s1">int64_t sizes_[max_rank + </span><span class="s4">1</span><span class="s1">];</span>
<a name="l36"><span class="ln">36   </span></a>  <span class="s1">int64_t input_strides_[max_rank + </span><span class="s4">1</span><span class="s1">];</span>
<a name="l37"><span class="ln">37   </span></a>  <span class="s1">int64_t output_strides_[max_rank + </span><span class="s4">1</span><span class="s1">];</span>
<a name="l38"><span class="ln">38   </span></a>  <span class="s1">CuFFTTransformType fft_type_;</span>
<a name="l39"><span class="ln">39   </span></a>  <span class="s1">ScalarType value_type_;</span>
<a name="l40"><span class="ln">40   </span></a>
<a name="l41"><span class="ln">41   </span></a>  <span class="s1">CuFFTParams() = </span><span class="s0">default</span><span class="s1">;</span>
<a name="l42"><span class="ln">42   </span></a>
<a name="l43"><span class="ln">43   </span></a>  <span class="s1">CuFFTParams(IntArrayRef in_strides, IntArrayRef out_strides,</span>
<a name="l44"><span class="ln">44   </span></a>      <span class="s1">IntArrayRef signal_sizes, CuFFTTransformType fft_type, ScalarType value_type) {</span>
<a name="l45"><span class="ln">45   </span></a>    <span class="s3">// Padding bits must be zeroed for hashing</span>
<a name="l46"><span class="ln">46   </span></a>    <span class="s1">memset(</span><span class="s2">this</span><span class="s1">, </span><span class="s4">0</span><span class="s1">, </span><span class="s0">sizeof</span><span class="s1">(*</span><span class="s2">this</span><span class="s1">));</span>
<a name="l47"><span class="ln">47   </span></a>    <span class="s1">signal_ndim_ = signal_sizes.size() - </span><span class="s4">1</span><span class="s1">;</span>
<a name="l48"><span class="ln">48   </span></a>    <span class="s1">fft_type_ = fft_type;</span>
<a name="l49"><span class="ln">49   </span></a>    <span class="s1">value_type_ = value_type;</span>
<a name="l50"><span class="ln">50   </span></a>
<a name="l51"><span class="ln">51   </span></a>    <span class="s1">TORCH_INTERNAL_ASSERT(in_strides.size() == signal_sizes.size());</span>
<a name="l52"><span class="ln">52   </span></a>    <span class="s1">TORCH_INTERNAL_ASSERT(out_strides.size() == signal_sizes.size());</span>
<a name="l53"><span class="ln">53   </span></a>    <span class="s1">TORCH_INTERNAL_ASSERT(</span><span class="s4">1 </span><span class="s1">&lt;= signal_ndim_ &amp;&amp; signal_ndim_ &lt;= max_rank);</span>
<a name="l54"><span class="ln">54   </span></a>
<a name="l55"><span class="ln">55   </span></a>    <span class="s1">std::copy(signal_sizes.cbegin(), signal_sizes.cend(), sizes_);</span>
<a name="l56"><span class="ln">56   </span></a>    <span class="s1">std::copy(in_strides.cbegin(), in_strides.cend(), input_strides_);</span>
<a name="l57"><span class="ln">57   </span></a>    <span class="s1">std::copy(out_strides.cbegin(), out_strides.cend(), output_strides_);</span>
<a name="l58"><span class="ln">58   </span></a>  <span class="s1">}</span>
<a name="l59"><span class="ln">59   </span></a><span class="s1">};</span>
<a name="l60"><span class="ln">60   </span></a>
<a name="l61"><span class="ln">61   </span></a><span class="s1">static_assert(std::is_trivial_v&lt;CuFFTParams&gt; );</span>
<a name="l62"><span class="ln">62   </span></a>
<a name="l63"><span class="ln">63   </span></a><span class="s3">// Returns true if the transform type has complex input</span>
<a name="l64"><span class="ln">64   </span></a><span class="s2">inline </span><span class="s0">bool </span><span class="s1">cufft_complex_input(CuFFTTransformType type) {</span>
<a name="l65"><span class="ln">65   </span></a>  <span class="s0">switch </span><span class="s1">(type) {</span>
<a name="l66"><span class="ln">66   </span></a>    <span class="s0">case </span><span class="s1">CuFFTTransformType::C2C:</span>
<a name="l67"><span class="ln">67   </span></a>    <span class="s0">case </span><span class="s1">CuFFTTransformType::C2R:</span>
<a name="l68"><span class="ln">68   </span></a>      <span class="s0">return </span><span class="s2">true</span><span class="s1">;</span>
<a name="l69"><span class="ln">69   </span></a>
<a name="l70"><span class="ln">70   </span></a>    <span class="s0">case </span><span class="s1">CuFFTTransformType::R2C:</span>
<a name="l71"><span class="ln">71   </span></a>      <span class="s0">return false</span><span class="s1">;</span>
<a name="l72"><span class="ln">72   </span></a>  <span class="s1">}</span>
<a name="l73"><span class="ln">73   </span></a>  <span class="s1">TORCH_INTERNAL_ASSERT(</span><span class="s0">false</span><span class="s1">);</span>
<a name="l74"><span class="ln">74   </span></a><span class="s1">}</span>
<a name="l75"><span class="ln">75   </span></a>
<a name="l76"><span class="ln">76   </span></a><span class="s3">// Returns true if the transform type has complex output</span>
<a name="l77"><span class="ln">77   </span></a><span class="s2">inline </span><span class="s0">bool </span><span class="s1">cufft_complex_output(CuFFTTransformType type) {</span>
<a name="l78"><span class="ln">78   </span></a>  <span class="s0">switch </span><span class="s1">(type) {</span>
<a name="l79"><span class="ln">79   </span></a>    <span class="s0">case </span><span class="s1">CuFFTTransformType::C2C:</span>
<a name="l80"><span class="ln">80   </span></a>    <span class="s0">case </span><span class="s1">CuFFTTransformType::R2C:</span>
<a name="l81"><span class="ln">81   </span></a>      <span class="s0">return </span><span class="s2">true</span><span class="s1">;</span>
<a name="l82"><span class="ln">82   </span></a>
<a name="l83"><span class="ln">83   </span></a>    <span class="s0">case </span><span class="s1">CuFFTTransformType::C2R:</span>
<a name="l84"><span class="ln">84   </span></a>      <span class="s0">return false</span><span class="s1">;</span>
<a name="l85"><span class="ln">85   </span></a>  <span class="s1">}</span>
<a name="l86"><span class="ln">86   </span></a>  <span class="s1">TORCH_INTERNAL_ASSERT(</span><span class="s0">false</span><span class="s1">);</span>
<a name="l87"><span class="ln">87   </span></a><span class="s1">}</span>
<a name="l88"><span class="ln">88   </span></a>
<a name="l89"><span class="ln">89   </span></a><span class="s3">// Create transform type enum from bools representing if input and output are complex</span>
<a name="l90"><span class="ln">90   </span></a><span class="s2">inline </span><span class="s1">CuFFTTransformType GetCuFFTTransformType(</span><span class="s0">bool </span><span class="s1">complex_input, </span><span class="s0">bool </span><span class="s1">complex_output) {</span>
<a name="l91"><span class="ln">91   </span></a>  <span class="s0">if </span><span class="s1">(complex_input &amp;&amp; complex_output) {</span>
<a name="l92"><span class="ln">92   </span></a>    <span class="s0">return </span><span class="s1">CuFFTTransformType::C2C;</span>
<a name="l93"><span class="ln">93   </span></a>  <span class="s1">} </span><span class="s0">else if </span><span class="s1">(complex_input &amp;&amp; !complex_output) {</span>
<a name="l94"><span class="ln">94   </span></a>    <span class="s0">return </span><span class="s1">CuFFTTransformType::C2R;</span>
<a name="l95"><span class="ln">95   </span></a>  <span class="s1">} </span><span class="s0">else if </span><span class="s1">(!complex_input &amp;&amp; complex_output) {</span>
<a name="l96"><span class="ln">96   </span></a>    <span class="s0">return </span><span class="s1">CuFFTTransformType::R2C;</span>
<a name="l97"><span class="ln">97   </span></a>  <span class="s1">}</span>
<a name="l98"><span class="ln">98   </span></a>  <span class="s1">TORCH_INTERNAL_ASSERT(</span><span class="s0">false</span><span class="s1">, </span><span class="s5">&quot;Real to real FFTs are not supported&quot;</span><span class="s1">);</span>
<a name="l99"><span class="ln">99   </span></a><span class="s1">}</span>
<a name="l100"><span class="ln">100  </span></a>
<a name="l101"><span class="ln">101  </span></a>
<a name="l102"><span class="ln">102  </span></a><span class="s2">class </span><span class="s1">CuFFTHandle {</span>
<a name="l103"><span class="ln">103  </span></a>  <span class="s1">::cufftHandle handle_;</span>
<a name="l104"><span class="ln">104  </span></a><span class="s2">public</span><span class="s1">:</span>
<a name="l105"><span class="ln">105  </span></a>
<a name="l106"><span class="ln">106  </span></a>  <span class="s1">CuFFTHandle() {</span>
<a name="l107"><span class="ln">107  </span></a>    <span class="s1">CUFFT_CHECK(cufftCreate(&amp;handle_));</span>
<a name="l108"><span class="ln">108  </span></a>  <span class="s1">}</span>
<a name="l109"><span class="ln">109  </span></a>
<a name="l110"><span class="ln">110  </span></a>  <span class="s1">::cufftHandle &amp; get() { </span><span class="s0">return </span><span class="s1">handle_; }</span>
<a name="l111"><span class="ln">111  </span></a>  <span class="s0">const </span><span class="s1">::cufftHandle &amp; get() </span><span class="s0">const </span><span class="s1">{ </span><span class="s0">return </span><span class="s1">handle_; }</span>
<a name="l112"><span class="ln">112  </span></a>
<a name="l113"><span class="ln">113  </span></a>  <span class="s1">~CuFFTHandle() {</span>
<a name="l114"><span class="ln">114  </span></a><span class="s3">// Not using fftDestroy() for rocFFT to work around double freeing of handles</span>
<a name="l115"><span class="ln">115  </span></a><span class="s0">#if </span><span class="s1">!defined(USE_ROCM)</span>
<a name="l116"><span class="ln">116  </span></a>    <span class="s1">cufftDestroy(handle_);</span>
<a name="l117"><span class="ln">117  </span></a><span class="s0">#endif</span>
<a name="l118"><span class="ln">118  </span></a>  <span class="s1">}</span>
<a name="l119"><span class="ln">119  </span></a><span class="s1">};</span>
<a name="l120"><span class="ln">120  </span></a>
<a name="l121"><span class="ln">121  </span></a><span class="s1">__forceinline__</span>
<a name="l122"><span class="ln">122  </span></a><span class="s0">static bool </span><span class="s1">is_pow_of_two(int64_t x) {</span>
<a name="l123"><span class="ln">123  </span></a>  <span class="s0">return </span><span class="s1">(x &amp; (x - </span><span class="s4">1</span><span class="s1">)) == </span><span class="s4">0</span><span class="s1">;</span>
<a name="l124"><span class="ln">124  </span></a><span class="s1">}</span>
<a name="l125"><span class="ln">125  </span></a>
<a name="l126"><span class="ln">126  </span></a><span class="s2">using </span><span class="s1">cufft_size_type = </span><span class="s0">long long int</span><span class="s1">;</span>
<a name="l127"><span class="ln">127  </span></a>
<a name="l128"><span class="ln">128  </span></a><span class="s2">using </span><span class="s1">CuFFTDimVector = c10::SmallVector&lt;cufft_size_type, at::kDimVectorStaticSize&gt;;</span>
<a name="l129"><span class="ln">129  </span></a>
<a name="l130"><span class="ln">130  </span></a><span class="s3">// Struct representing a tensor in CuFFT's data layout for planning transforms</span>
<a name="l131"><span class="ln">131  </span></a><span class="s3">// See NOTE [ cuFFT Embedded Strides ].</span>
<a name="l132"><span class="ln">132  </span></a><span class="s0">struct </span><span class="s1">CuFFTDataLayout {</span>
<a name="l133"><span class="ln">133  </span></a>  <span class="s1">CuFFTDimVector embed;</span>
<a name="l134"><span class="ln">134  </span></a>  <span class="s1">cufft_size_type stride, dist;</span>
<a name="l135"><span class="ln">135  </span></a>  <span class="s0">bool </span><span class="s1">must_clone, simple;</span>
<a name="l136"><span class="ln">136  </span></a><span class="s1">};</span>
<a name="l137"><span class="ln">137  </span></a>
<a name="l138"><span class="ln">138  </span></a><span class="s3">// Returns a cufft embedding for a contiguous signal of the given size.</span>
<a name="l139"><span class="ln">139  </span></a><span class="s3">// e.g. if the input is cloned, this will be the resulting data layout</span>
<a name="l140"><span class="ln">140  </span></a><span class="s3">// See NOTE [ cuFFT Embedded Strides ].</span>
<a name="l141"><span class="ln">141  </span></a><span class="s2">inline </span><span class="s1">CuFFTDataLayout cufft_simple_embed(IntArrayRef sizes, </span><span class="s0">bool </span><span class="s1">onesided) {</span>
<a name="l142"><span class="ln">142  </span></a>  <span class="s1">CuFFTDataLayout layout;</span>
<a name="l143"><span class="ln">143  </span></a>  <span class="s1">layout.simple = </span><span class="s2">true</span><span class="s1">;</span>
<a name="l144"><span class="ln">144  </span></a>  <span class="s1">layout.must_clone = </span><span class="s0">false</span><span class="s1">;</span>
<a name="l145"><span class="ln">145  </span></a>  <span class="s1">layout.embed.assign(sizes.cbegin() + </span><span class="s4">1</span><span class="s1">, sizes.cend());</span>
<a name="l146"><span class="ln">146  </span></a>  <span class="s0">if </span><span class="s1">(onesided) {</span>
<a name="l147"><span class="ln">147  </span></a>    <span class="s1">layout.embed.back() = sizes.back() / </span><span class="s4">2 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">;</span>
<a name="l148"><span class="ln">148  </span></a>  <span class="s1">}</span>
<a name="l149"><span class="ln">149  </span></a>  <span class="s1">layout.stride = </span><span class="s4">1</span><span class="s1">;</span>
<a name="l150"><span class="ln">150  </span></a>  <span class="s1">layout.dist = </span><span class="s4">1</span><span class="s1">;</span>
<a name="l151"><span class="ln">151  </span></a>  <span class="s0">for </span><span class="s1">(</span><span class="s0">const auto</span><span class="s1">&amp; len : layout.embed) {</span>
<a name="l152"><span class="ln">152  </span></a>    <span class="s1">layout.dist *= len;</span>
<a name="l153"><span class="ln">153  </span></a>  <span class="s1">}</span>
<a name="l154"><span class="ln">154  </span></a>  <span class="s0">return </span><span class="s1">layout;</span>
<a name="l155"><span class="ln">155  </span></a><span class="s1">}</span>
<a name="l156"><span class="ln">156  </span></a>
<a name="l157"><span class="ln">157  </span></a><span class="s3">// Convert strides to a CuFFT embedded representation.</span>
<a name="l158"><span class="ln">158  </span></a><span class="s3">// If strides cannot be embedded, returns a simple layout and sets must_clone flag</span>
<a name="l159"><span class="ln">159  </span></a><span class="s3">// See NOTE [ cuFFT Embedded Strides ].</span>
<a name="l160"><span class="ln">160  </span></a><span class="s2">inline </span><span class="s1">CuFFTDataLayout as_cufft_embed(IntArrayRef strides, IntArrayRef sizes, </span><span class="s0">bool </span><span class="s1">onesided) {</span>
<a name="l161"><span class="ln">161  </span></a>  <span class="s0">const auto </span><span class="s1">signal_ndim = strides.size() - </span><span class="s4">1</span><span class="s1">;</span>
<a name="l162"><span class="ln">162  </span></a>  <span class="s1">CuFFTDataLayout layout;</span>
<a name="l163"><span class="ln">163  </span></a>  <span class="s0">auto </span><span class="s1">last_stride = strides[signal_ndim];</span>
<a name="l164"><span class="ln">164  </span></a>  <span class="s1">layout.must_clone = (last_stride &lt;= </span><span class="s4">0</span><span class="s1">);</span>
<a name="l165"><span class="ln">165  </span></a>
<a name="l166"><span class="ln">166  </span></a>  <span class="s0">const auto </span><span class="s1">last_dim_size = onesided ?</span>
<a name="l167"><span class="ln">167  </span></a>      <span class="s1">sizes[signal_ndim] / </span><span class="s4">2 </span><span class="s1">+ </span><span class="s4">1 </span><span class="s1">: sizes[signal_ndim];</span>
<a name="l168"><span class="ln">168  </span></a>  <span class="s0">const auto </span><span class="s1">signal_numel = c10::multiply_integers(sizes.slice(</span><span class="s4">1</span><span class="s1">, sizes.size() - </span><span class="s4">2</span><span class="s1">)) * last_dim_size;</span>
<a name="l169"><span class="ln">169  </span></a>
<a name="l170"><span class="ln">170  </span></a>  <span class="s3">// Zero stides are not allowed, even if the batch size is one.</span>
<a name="l171"><span class="ln">171  </span></a>  <span class="s3">// If that happens just set a dummy case</span>
<a name="l172"><span class="ln">172  </span></a>  <span class="s0">if </span><span class="s1">(sizes[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">) {</span>
<a name="l173"><span class="ln">173  </span></a>    <span class="s1">layout.dist = signal_numel;</span>
<a name="l174"><span class="ln">174  </span></a>  <span class="s1">} </span><span class="s0">else if </span><span class="s1">(strides[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0</span><span class="s1">) {</span>
<a name="l175"><span class="ln">175  </span></a>    <span class="s1">layout.must_clone = </span><span class="s2">true</span><span class="s1">;</span>
<a name="l176"><span class="ln">176  </span></a>  <span class="s1">} </span><span class="s0">else </span><span class="s1">{</span>
<a name="l177"><span class="ln">177  </span></a>    <span class="s1">layout.dist = strides[</span><span class="s4">0</span><span class="s1">];</span>
<a name="l178"><span class="ln">178  </span></a>  <span class="s1">}</span>
<a name="l179"><span class="ln">179  </span></a>
<a name="l180"><span class="ln">180  </span></a>  <span class="s3">// Calculate the embedding shape, or set must_clone if the strides cannot be embedded</span>
<a name="l181"><span class="ln">181  </span></a>  <span class="s1">layout.embed.resize(signal_ndim);</span>
<a name="l182"><span class="ln">182  </span></a>  <span class="s0">for </span><span class="s1">(</span><span class="s0">auto </span><span class="s1">i = signal_ndim - </span><span class="s4">1</span><span class="s1">; !layout.must_clone &amp;&amp; i &gt; </span><span class="s4">0</span><span class="s1">; i--) {</span>
<a name="l183"><span class="ln">183  </span></a>    <span class="s0">auto </span><span class="s1">stride = strides[i];</span>
<a name="l184"><span class="ln">184  </span></a>    <span class="s0">if </span><span class="s1">(sizes[i] == </span><span class="s4">1</span><span class="s1">) {</span>
<a name="l185"><span class="ln">185  </span></a>      <span class="s1">layout.embed[i] = </span><span class="s4">1</span><span class="s1">;</span>
<a name="l186"><span class="ln">186  </span></a>    <span class="s1">} </span><span class="s0">else if </span><span class="s1">(stride &gt; </span><span class="s4">0 </span><span class="s1">&amp;&amp; stride % last_stride == </span><span class="s4">0</span><span class="s1">) {</span>
<a name="l187"><span class="ln">187  </span></a>      <span class="s1">layout.embed[i] = stride / last_stride;</span>
<a name="l188"><span class="ln">188  </span></a>      <span class="s1">last_stride = stride;</span>
<a name="l189"><span class="ln">189  </span></a>    <span class="s1">} </span><span class="s0">else </span><span class="s1">{</span>
<a name="l190"><span class="ln">190  </span></a>      <span class="s1">layout.must_clone = </span><span class="s2">true</span><span class="s1">;</span>
<a name="l191"><span class="ln">191  </span></a>    <span class="s1">}</span>
<a name="l192"><span class="ln">192  </span></a>  <span class="s1">}</span>
<a name="l193"><span class="ln">193  </span></a>
<a name="l194"><span class="ln">194  </span></a>  <span class="s0">if </span><span class="s1">(layout.must_clone) {</span>
<a name="l195"><span class="ln">195  </span></a>    <span class="s3">// If the input needs to be cloned, assume it will be contiguous</span>
<a name="l196"><span class="ln">196  </span></a>    <span class="s1">layout = cufft_simple_embed(sizes, onesided);</span>
<a name="l197"><span class="ln">197  </span></a>    <span class="s1">layout.must_clone = </span><span class="s2">true</span><span class="s1">;</span>
<a name="l198"><span class="ln">198  </span></a>  <span class="s1">} </span><span class="s0">else </span><span class="s1">{</span>
<a name="l199"><span class="ln">199  </span></a>    <span class="s1">layout.embed[</span><span class="s4">0</span><span class="s1">] = sizes[</span><span class="s4">1</span><span class="s1">];</span>
<a name="l200"><span class="ln">200  </span></a>    <span class="s1">layout.stride = strides[signal_ndim];</span>
<a name="l201"><span class="ln">201  </span></a>    <span class="s3">// Determine if layout represents a simple embedding (contiguous data)</span>
<a name="l202"><span class="ln">202  </span></a>    <span class="s1">layout.simple = [&amp;] {</span>
<a name="l203"><span class="ln">203  </span></a>      <span class="s0">for </span><span class="s1">(</span><span class="s0">const auto </span><span class="s1">i : c10::irange(</span><span class="s4">1</span><span class="s1">, signal_ndim - </span><span class="s4">1</span><span class="s1">)) {</span>
<a name="l204"><span class="ln">204  </span></a>        <span class="s0">if </span><span class="s1">(layout.embed[i] != sizes[i + </span><span class="s4">1</span><span class="s1">]) {</span>
<a name="l205"><span class="ln">205  </span></a>          <span class="s0">return false</span><span class="s1">;</span>
<a name="l206"><span class="ln">206  </span></a>        <span class="s1">}</span>
<a name="l207"><span class="ln">207  </span></a>      <span class="s1">}</span>
<a name="l208"><span class="ln">208  </span></a>
<a name="l209"><span class="ln">209  </span></a>      <span class="s0">return </span><span class="s1">(layout.stride == </span><span class="s4">1 </span><span class="s1">&amp;&amp; layout.dist == signal_numel &amp;&amp;</span>
<a name="l210"><span class="ln">210  </span></a>          <span class="s1">layout.embed.back() == last_dim_size);</span>
<a name="l211"><span class="ln">211  </span></a>    <span class="s1">}();</span>
<a name="l212"><span class="ln">212  </span></a>  <span class="s1">}</span>
<a name="l213"><span class="ln">213  </span></a>  <span class="s0">return </span><span class="s1">layout;</span>
<a name="l214"><span class="ln">214  </span></a><span class="s1">}</span>
<a name="l215"><span class="ln">215  </span></a>
<a name="l216"><span class="ln">216  </span></a><span class="s3">// This class contains all the information needed to execute a cuFFT plan:</span>
<a name="l217"><span class="ln">217  </span></a><span class="s3">//   1. the plan</span>
<a name="l218"><span class="ln">218  </span></a><span class="s3">//   2. whether to clone input before executing the plan</span>
<a name="l219"><span class="ln">219  </span></a><span class="s3">//   3. the workspace size needed</span>
<a name="l220"><span class="ln">220  </span></a><span class="s3">//</span>
<a name="l221"><span class="ln">221  </span></a><span class="s3">// This class will be the **value** in the plan cache.</span>
<a name="l222"><span class="ln">222  </span></a><span class="s3">// It **owns** the raw plan via a unique_ptr.</span>
<a name="l223"><span class="ln">223  </span></a><span class="s2">class </span><span class="s1">CuFFTConfig {</span>
<a name="l224"><span class="ln">224  </span></a><span class="s2">public</span><span class="s1">:</span>
<a name="l225"><span class="ln">225  </span></a>
<a name="l226"><span class="ln">226  </span></a>  <span class="s3">// Only move semantics is enought for this class. Although we already use</span>
<a name="l227"><span class="ln">227  </span></a>  <span class="s3">// unique_ptr for the plan, still remove copy constructor and assignment op so</span>
<a name="l228"><span class="ln">228  </span></a>  <span class="s3">// we don't accidentally copy and take perf hit.</span>
<a name="l229"><span class="ln">229  </span></a>  <span class="s1">CuFFTConfig(</span><span class="s0">const </span><span class="s1">CuFFTConfig&amp;) = </span><span class="s0">delete</span><span class="s1">;</span>
<a name="l230"><span class="ln">230  </span></a>  <span class="s1">CuFFTConfig&amp; </span><span class="s2">operator</span><span class="s1">=(CuFFTConfig </span><span class="s0">const</span><span class="s1">&amp;) = </span><span class="s0">delete</span><span class="s1">;</span>
<a name="l231"><span class="ln">231  </span></a>
<a name="l232"><span class="ln">232  </span></a>  <span class="s2">explicit </span><span class="s1">CuFFTConfig(</span><span class="s0">const </span><span class="s1">CuFFTParams&amp; params):</span>
<a name="l233"><span class="ln">233  </span></a>      <span class="s1">CuFFTConfig(</span>
<a name="l234"><span class="ln">234  </span></a>          <span class="s1">IntArrayRef(params.input_strides_, params.signal_ndim_ + </span><span class="s4">1</span><span class="s1">),</span>
<a name="l235"><span class="ln">235  </span></a>          <span class="s1">IntArrayRef(params.output_strides_, params.signal_ndim_ + </span><span class="s4">1</span><span class="s1">),</span>
<a name="l236"><span class="ln">236  </span></a>          <span class="s1">IntArrayRef(params.sizes_, params.signal_ndim_ + </span><span class="s4">1</span><span class="s1">),</span>
<a name="l237"><span class="ln">237  </span></a>          <span class="s1">params.fft_type_,</span>
<a name="l238"><span class="ln">238  </span></a>          <span class="s1">params.value_type_) {}</span>
<a name="l239"><span class="ln">239  </span></a>
<a name="l240"><span class="ln">240  </span></a>  <span class="s3">// For complex types, strides are in units of 2 * element_size(dtype)</span>
<a name="l241"><span class="ln">241  </span></a>  <span class="s3">// sizes are for the full signal, including batch size and always two-sided</span>
<a name="l242"><span class="ln">242  </span></a>  <span class="s1">CuFFTConfig(IntArrayRef in_strides, IntArrayRef out_strides,</span>
<a name="l243"><span class="ln">243  </span></a>      <span class="s1">IntArrayRef sizes, CuFFTTransformType fft_type, ScalarType dtype):</span>
<a name="l244"><span class="ln">244  </span></a>        <span class="s1">fft_type_(fft_type), value_type_(dtype) {</span>
<a name="l245"><span class="ln">245  </span></a>
<a name="l246"><span class="ln">246  </span></a>    <span class="s3">// signal sizes (excluding batch dim)</span>
<a name="l247"><span class="ln">247  </span></a>    <span class="s1">CuFFTDimVector signal_sizes(sizes.begin() + </span><span class="s4">1</span><span class="s1">, sizes.end());</span>
<a name="l248"><span class="ln">248  </span></a>
<a name="l249"><span class="ln">249  </span></a>    <span class="s3">// input batch size</span>
<a name="l250"><span class="ln">250  </span></a>    <span class="s0">const </span><span class="s1">int64_t batch = sizes[</span><span class="s4">0</span><span class="s1">];</span>
<a name="l251"><span class="ln">251  </span></a>    <span class="s0">const </span><span class="s1">int64_t signal_ndim = sizes.size() - </span><span class="s4">1</span><span class="s1">;</span>
<a name="l252"><span class="ln">252  </span></a>
<a name="l253"><span class="ln">253  </span></a>    <span class="s3">// Since cuFFT has limited non-unit stride support and various constraints, we</span>
<a name="l254"><span class="ln">254  </span></a>    <span class="s3">// use a flag to keep track throughout this function to see if we need to</span>
<a name="l255"><span class="ln">255  </span></a>    <span class="s3">// input = input.clone();</span>
<a name="l256"><span class="ln">256  </span></a>
<a name="l257"><span class="ln">257  </span></a><span class="s0">#if </span><span class="s1">defined(USE_ROCM)</span>
<a name="l258"><span class="ln">258  </span></a>    <span class="s3">// clone input to avoid issues with hipfft clobering the input and failing tests</span>
<a name="l259"><span class="ln">259  </span></a>    <span class="s1">clone_input = </span><span class="s2">true</span><span class="s1">;</span>
<a name="l260"><span class="ln">260  </span></a><span class="s0">#else</span>
<a name="l261"><span class="ln">261  </span></a>    <span class="s1">clone_input = </span><span class="s0">false</span><span class="s1">;</span>
<a name="l262"><span class="ln">262  </span></a><span class="s0">#endif</span>
<a name="l263"><span class="ln">263  </span></a>
<a name="l264"><span class="ln">264  </span></a>    <span class="s3">// For half, base strides on the real part of real-to-complex and</span>
<a name="l265"><span class="ln">265  </span></a>    <span class="s3">// complex-to-real transforms are not supported. Since our output is always</span>
<a name="l266"><span class="ln">266  </span></a>    <span class="s3">// contiguous, only need to check real-to-complex case.</span>
<a name="l267"><span class="ln">267  </span></a>    <span class="s0">if </span><span class="s1">(dtype == ScalarType::Half) {</span>
<a name="l268"><span class="ln">268  </span></a>      <span class="s3">// cuFFT on half requires compute capability of at least SM_53</span>
<a name="l269"><span class="ln">269  </span></a>      <span class="s0">auto </span><span class="s1">dev_prop = at::cuda::getCurrentDeviceProperties();</span>
<a name="l270"><span class="ln">270  </span></a>      <span class="s1">TORCH_CHECK(dev_prop</span><span class="s6">-&gt;</span><span class="s1">major &gt;= </span><span class="s4">5 </span><span class="s1">&amp;&amp; !(dev_prop</span><span class="s6">-&gt;</span><span class="s1">major == </span><span class="s4">5 </span><span class="s1">&amp;&amp; dev_prop</span><span class="s6">-&gt;</span><span class="s1">minor &lt; </span><span class="s4">3</span><span class="s1">),</span>
<a name="l271"><span class="ln">271  </span></a>               <span class="s5">&quot;cuFFT doesn't support signals of half type with compute &quot;</span>
<a name="l272"><span class="ln">272  </span></a>               <span class="s5">&quot;capability less than SM_53, but the device containing input half &quot;</span>
<a name="l273"><span class="ln">273  </span></a>               <span class="s5">&quot;tensor only has SM_&quot;</span><span class="s1">, dev_prop</span><span class="s6">-&gt;</span><span class="s1">major, dev_prop</span><span class="s6">-&gt;</span><span class="s1">minor);</span>
<a name="l274"><span class="ln">274  </span></a>      <span class="s0">for </span><span class="s1">(</span><span class="s0">const auto </span><span class="s1">i : c10::irange(signal_ndim)) {</span>
<a name="l275"><span class="ln">275  </span></a>        <span class="s1">TORCH_CHECK(is_pow_of_two(sizes[i + </span><span class="s4">1</span><span class="s1">]),</span>
<a name="l276"><span class="ln">276  </span></a>            <span class="s5">&quot;cuFFT only supports dimensions whose sizes are powers of two when&quot;</span>
<a name="l277"><span class="ln">277  </span></a>            <span class="s5">&quot; computing in half precision, but got a signal size of&quot;</span><span class="s1">,</span>
<a name="l278"><span class="ln">278  </span></a>            <span class="s1">sizes.slice(</span><span class="s4">1</span><span class="s1">));</span>
<a name="l279"><span class="ln">279  </span></a>      <span class="s1">}</span>
<a name="l280"><span class="ln">280  </span></a>      <span class="s1">clone_input |= in_strides.back() != </span><span class="s4">1</span><span class="s1">;</span>
<a name="l281"><span class="ln">281  </span></a>    <span class="s1">}</span>
<a name="l282"><span class="ln">282  </span></a>
<a name="l283"><span class="ln">283  </span></a>    <span class="s1">CuFFTDataLayout in_layout;</span>
<a name="l284"><span class="ln">284  </span></a>    <span class="s0">if </span><span class="s1">(clone_input) {</span>
<a name="l285"><span class="ln">285  </span></a>      <span class="s1">in_layout = cufft_simple_embed(sizes, fft_type == CuFFTTransformType::C2R);</span>
<a name="l286"><span class="ln">286  </span></a>    <span class="s1">} </span><span class="s0">else </span><span class="s1">{</span>
<a name="l287"><span class="ln">287  </span></a>      <span class="s1">in_layout = as_cufft_embed(in_strides, sizes, fft_type == CuFFTTransformType::C2R);</span>
<a name="l288"><span class="ln">288  </span></a>    <span class="s1">}</span>
<a name="l289"><span class="ln">289  </span></a>    <span class="s0">auto </span><span class="s1">out_layout = as_cufft_embed(out_strides, sizes, fft_type == CuFFTTransformType::R2C);</span>
<a name="l290"><span class="ln">290  </span></a>    <span class="s1">TORCH_INTERNAL_ASSERT(!out_layout.must_clone, </span><span class="s5">&quot;Out strides cannot be represented as CuFFT embedding&quot;</span><span class="s1">);</span>
<a name="l291"><span class="ln">291  </span></a>    <span class="s1">clone_input |= in_layout.must_clone;</span>
<a name="l292"><span class="ln">292  </span></a>
<a name="l293"><span class="ln">293  </span></a>    <span class="s3">// Check if we can take advantage of simple data layout.</span>
<a name="l294"><span class="ln">294  </span></a>    <span class="s3">//</span>
<a name="l295"><span class="ln">295  </span></a>    <span class="s3">// See NOTE [ cuFFT Embedded Strides ] in native/cuda/SpectralOps.cu.</span>
<a name="l296"><span class="ln">296  </span></a>
<a name="l297"><span class="ln">297  </span></a>    <span class="s0">const bool </span><span class="s1">simple_layout = in_layout.simple &amp;&amp; out_layout.simple;</span>
<a name="l298"><span class="ln">298  </span></a>    <span class="s1">cudaDataType itype, otype, exec_type;</span>
<a name="l299"><span class="ln">299  </span></a>    <span class="s0">const auto </span><span class="s1">complex_input = cufft_complex_input(fft_type);</span>
<a name="l300"><span class="ln">300  </span></a>    <span class="s0">const auto </span><span class="s1">complex_output = cufft_complex_output(fft_type);</span>
<a name="l301"><span class="ln">301  </span></a>    <span class="s0">if </span><span class="s1">(dtype == ScalarType::Float) {</span>
<a name="l302"><span class="ln">302  </span></a>      <span class="s1">itype = complex_input ? CUDA_C_32F : CUDA_R_32F;</span>
<a name="l303"><span class="ln">303  </span></a>      <span class="s1">otype = complex_output ? CUDA_C_32F : CUDA_R_32F;</span>
<a name="l304"><span class="ln">304  </span></a>      <span class="s1">exec_type = CUDA_C_32F;</span>
<a name="l305"><span class="ln">305  </span></a>    <span class="s1">} </span><span class="s0">else if </span><span class="s1">(dtype == ScalarType::Double) {</span>
<a name="l306"><span class="ln">306  </span></a>      <span class="s1">itype = complex_input ? CUDA_C_64F : CUDA_R_64F;</span>
<a name="l307"><span class="ln">307  </span></a>      <span class="s1">otype = complex_output ? CUDA_C_64F : CUDA_R_64F;</span>
<a name="l308"><span class="ln">308  </span></a>      <span class="s1">exec_type = CUDA_C_64F;</span>
<a name="l309"><span class="ln">309  </span></a>    <span class="s1">} </span><span class="s0">else if </span><span class="s1">(dtype == ScalarType::Half) {</span>
<a name="l310"><span class="ln">310  </span></a>      <span class="s1">itype = complex_input ? CUDA_C_16F : CUDA_R_16F;</span>
<a name="l311"><span class="ln">311  </span></a>      <span class="s1">otype = complex_output ? CUDA_C_16F : CUDA_R_16F;</span>
<a name="l312"><span class="ln">312  </span></a>      <span class="s1">exec_type = CUDA_C_16F;</span>
<a name="l313"><span class="ln">313  </span></a>    <span class="s1">} </span><span class="s0">else </span><span class="s1">{</span>
<a name="l314"><span class="ln">314  </span></a>      <span class="s1">TORCH_CHECK(</span><span class="s0">false</span><span class="s1">, </span><span class="s5">&quot;cuFFT doesn't support tensor of type: &quot;</span><span class="s1">, dtype);</span>
<a name="l315"><span class="ln">315  </span></a>    <span class="s1">}</span>
<a name="l316"><span class="ln">316  </span></a>
<a name="l317"><span class="ln">317  </span></a>    <span class="s3">// disable auto allocation of workspace to use THC allocator</span>
<a name="l318"><span class="ln">318  </span></a>    <span class="s1">CUFFT_CHECK(cufftSetAutoAllocation(plan(), </span><span class="s3">/* autoAllocate */ </span><span class="s4">0</span><span class="s1">));</span>
<a name="l319"><span class="ln">319  </span></a>
<a name="l320"><span class="ln">320  </span></a>    <span class="s1">size_t ws_size_t;</span>
<a name="l321"><span class="ln">321  </span></a>
<a name="l322"><span class="ln">322  </span></a>    <span class="s3">// make plan</span>
<a name="l323"><span class="ln">323  </span></a>    <span class="s0">if </span><span class="s1">(simple_layout) {</span>
<a name="l324"><span class="ln">324  </span></a>      <span class="s3">// If with unit-stride, we tell cuFFT by setting inembed == onembed == NULL.</span>
<a name="l325"><span class="ln">325  </span></a>      <span class="s3">// In such case, cuFFT ignores istride, ostride, idist, and odist</span>
<a name="l326"><span class="ln">326  </span></a>      <span class="s3">// by assuming istride = ostride = 1.</span>
<a name="l327"><span class="ln">327  </span></a>      <span class="s3">//</span>
<a name="l328"><span class="ln">328  </span></a>      <span class="s3">// See NOTE [ cuFFT Embedded Strides ] in native/cuda/SpectralOps.cu.</span>
<a name="l329"><span class="ln">329  </span></a>      <span class="s1">CUFFT_CHECK(cufftXtMakePlanMany(plan(), signal_ndim, signal_sizes.data(),</span>
<a name="l330"><span class="ln">330  </span></a>        <span class="s3">/* inembed */ </span><span class="s1">nullptr, </span><span class="s3">/* base_istride */ </span><span class="s4">1</span><span class="s1">, </span><span class="s3">/* idist */ </span><span class="s4">1</span><span class="s1">, itype,</span>
<a name="l331"><span class="ln">331  </span></a>        <span class="s3">/* onembed */ </span><span class="s1">nullptr, </span><span class="s3">/* base_ostride */ </span><span class="s4">1</span><span class="s1">, </span><span class="s3">/* odist */ </span><span class="s4">1</span><span class="s1">, otype,</span>
<a name="l332"><span class="ln">332  </span></a>        <span class="s1">batch, &amp;ws_size_t, exec_type));</span>
<a name="l333"><span class="ln">333  </span></a>    <span class="s1">} </span><span class="s0">else </span><span class="s1">{</span>
<a name="l334"><span class="ln">334  </span></a>      <span class="s1">CUFFT_CHECK(cufftXtMakePlanMany(plan(), signal_ndim, signal_sizes.data(),</span>
<a name="l335"><span class="ln">335  </span></a>            <span class="s1">in_layout.embed.data(), in_layout.stride, in_layout.dist, itype,</span>
<a name="l336"><span class="ln">336  </span></a>            <span class="s1">out_layout.embed.data(), out_layout.stride, out_layout.dist, otype,</span>
<a name="l337"><span class="ln">337  </span></a>            <span class="s1">batch, &amp;ws_size_t, exec_type));</span>
<a name="l338"><span class="ln">338  </span></a>    <span class="s1">}</span>
<a name="l339"><span class="ln">339  </span></a>    <span class="s1">ws_size = </span><span class="s2">static_cast</span><span class="s1">&lt;int64_t&gt;(ws_size_t);</span>
<a name="l340"><span class="ln">340  </span></a>  <span class="s1">}</span>
<a name="l341"><span class="ln">341  </span></a>
<a name="l342"><span class="ln">342  </span></a>  <span class="s0">const </span><span class="s1">cufftHandle &amp;plan() </span><span class="s0">const </span><span class="s1">{ </span><span class="s0">return </span><span class="s1">plan_ptr.get(); }</span>
<a name="l343"><span class="ln">343  </span></a>
<a name="l344"><span class="ln">344  </span></a>  <span class="s1">CuFFTTransformType transform_type() </span><span class="s0">const </span><span class="s1">{ </span><span class="s0">return </span><span class="s1">fft_type_; }</span>
<a name="l345"><span class="ln">345  </span></a>  <span class="s1">ScalarType data_type() </span><span class="s0">const </span><span class="s1">{ </span><span class="s0">return </span><span class="s1">value_type_; }</span>
<a name="l346"><span class="ln">346  </span></a>  <span class="s0">bool </span><span class="s1">should_clone_input() </span><span class="s0">const </span><span class="s1">{ </span><span class="s0">return </span><span class="s1">clone_input; }</span>
<a name="l347"><span class="ln">347  </span></a>  <span class="s1">int64_t workspace_size() </span><span class="s0">const </span><span class="s1">{ </span><span class="s0">return </span><span class="s1">ws_size; }</span>
<a name="l348"><span class="ln">348  </span></a>
<a name="l349"><span class="ln">349  </span></a><span class="s2">private</span><span class="s1">:</span>
<a name="l350"><span class="ln">350  </span></a>  <span class="s1">CuFFTHandle plan_ptr;</span>
<a name="l351"><span class="ln">351  </span></a>  <span class="s0">bool </span><span class="s1">clone_input;</span>
<a name="l352"><span class="ln">352  </span></a>  <span class="s1">int64_t ws_size;</span>
<a name="l353"><span class="ln">353  </span></a>  <span class="s1">CuFFTTransformType fft_type_;</span>
<a name="l354"><span class="ln">354  </span></a>  <span class="s1">ScalarType value_type_;</span>
<a name="l355"><span class="ln">355  </span></a><span class="s1">};</span>
<a name="l356"><span class="ln">356  </span></a>
<a name="l357"><span class="ln">357  </span></a><span class="s0">#if </span><span class="s1">defined(USE_ROCM)</span>
<a name="l358"><span class="ln">358  </span></a>  <span class="s3">// Note that the max plan number for CUDA version &lt; 10 has to be 1023</span>
<a name="l359"><span class="ln">359  </span></a>  <span class="s3">// due to a bug that fails on the 1024th plan</span>
<a name="l360"><span class="ln">360  </span></a>  <span class="s1">constexpr int64_t CUFFT_MAX_PLAN_NUM = </span><span class="s4">1023</span><span class="s1">;</span>
<a name="l361"><span class="ln">361  </span></a>  <span class="s1">constexpr int64_t CUFFT_DEFAULT_CACHE_SIZE = CUFFT_MAX_PLAN_NUM;</span>
<a name="l362"><span class="ln">362  </span></a><span class="s0">#else</span>
<a name="l363"><span class="ln">363  </span></a>  <span class="s1">constexpr int64_t CUFFT_MAX_PLAN_NUM = std::numeric_limits&lt;int64_t&gt;::max();</span>
<a name="l364"><span class="ln">364  </span></a>  <span class="s3">// The default max cache size chosen for CUDA version &gt; 10 is arbitrary.</span>
<a name="l365"><span class="ln">365  </span></a>  <span class="s3">// This number puts a limit on how big of a plan cache should we maintain by</span>
<a name="l366"><span class="ln">366  </span></a>  <span class="s3">// default. Users can always configure it via cufft_set_plan_cache_max_size.</span>
<a name="l367"><span class="ln">367  </span></a>  <span class="s1">constexpr int64_t CUFFT_DEFAULT_CACHE_SIZE = </span><span class="s4">4096</span><span class="s1">;</span>
<a name="l368"><span class="ln">368  </span></a><span class="s0">#endif</span>
<a name="l369"><span class="ln">369  </span></a><span class="s1">static_assert(</span><span class="s4">0 </span><span class="s1">&lt;= CUFFT_MAX_PLAN_NUM &amp;&amp; CUFFT_MAX_PLAN_NUM &lt;= std::numeric_limits&lt;int64_t&gt;::max(),</span>
<a name="l370"><span class="ln">370  </span></a>              <span class="s5">&quot;CUFFT_MAX_PLAN_NUM not in size_t range&quot;</span><span class="s1">);</span>
<a name="l371"><span class="ln">371  </span></a><span class="s1">static_assert(CUFFT_DEFAULT_CACHE_SIZE &gt;= </span><span class="s4">0 </span><span class="s1">&amp;&amp; CUFFT_DEFAULT_CACHE_SIZE &lt;= CUFFT_MAX_PLAN_NUM,</span>
<a name="l372"><span class="ln">372  </span></a>              <span class="s5">&quot;CUFFT_DEFAULT_CACHE_SIZE not in [0, CUFFT_MAX_PLAN_NUM] range&quot;</span><span class="s1">);</span>
<a name="l373"><span class="ln">373  </span></a>
<a name="l374"><span class="ln">374  </span></a><span class="s3">// This cache assumes that the mapping from key to value never changes.</span>
<a name="l375"><span class="ln">375  </span></a><span class="s3">// This is **NOT** thread-safe. Please use a mutex when using it **AND** the</span>
<a name="l376"><span class="ln">376  </span></a><span class="s3">// value returned from try_emplace_value.</span>
<a name="l377"><span class="ln">377  </span></a><span class="s3">// The contract of using this cache is that try_emplace_value should only be</span>
<a name="l378"><span class="ln">378  </span></a><span class="s3">// used when the max_size is positive.</span>
<a name="l379"><span class="ln">379  </span></a><span class="s2">class </span><span class="s1">CuFFTParamsLRUCache {</span>
<a name="l380"><span class="ln">380  </span></a><span class="s2">public</span><span class="s1">:</span>
<a name="l381"><span class="ln">381  </span></a>  <span class="s2">using </span><span class="s1">kv_t = </span><span class="s2">typename </span><span class="s1">std::pair&lt;CuFFTParams, CuFFTConfig&gt;;</span>
<a name="l382"><span class="ln">382  </span></a>  <span class="s2">using </span><span class="s1">map_t = </span><span class="s2">typename </span><span class="s1">std::unordered_map&lt;std::reference_wrapper&lt;CuFFTParams&gt;,</span>
<a name="l383"><span class="ln">383  </span></a>                                            <span class="s2">typename </span><span class="s1">std::list&lt;kv_t&gt;::iterator,</span>
<a name="l384"><span class="ln">384  </span></a>                                            <span class="s1">ParamsHash&lt;CuFFTParams&gt;,</span>
<a name="l385"><span class="ln">385  </span></a>                                            <span class="s1">ParamsEqual&lt;CuFFTParams&gt;&gt;;</span>
<a name="l386"><span class="ln">386  </span></a>  <span class="s2">using </span><span class="s1">map_kkv_iter_t = </span><span class="s2">typename </span><span class="s1">map_t::iterator;</span>
<a name="l387"><span class="ln">387  </span></a>
<a name="l388"><span class="ln">388  </span></a>
<a name="l389"><span class="ln">389  </span></a>  <span class="s1">CuFFTParamsLRUCache() : CuFFTParamsLRUCache(CUFFT_DEFAULT_CACHE_SIZE) {}</span>
<a name="l390"><span class="ln">390  </span></a>
<a name="l391"><span class="ln">391  </span></a>  <span class="s1">CuFFTParamsLRUCache(int64_t max_size) {</span>
<a name="l392"><span class="ln">392  </span></a>    <span class="s1">_set_max_size(max_size);</span>
<a name="l393"><span class="ln">393  </span></a>  <span class="s1">}</span>
<a name="l394"><span class="ln">394  </span></a>
<a name="l395"><span class="ln">395  </span></a>  <span class="s1">CuFFTParamsLRUCache(CuFFTParamsLRUCache&amp;&amp; other) noexcept :</span>
<a name="l396"><span class="ln">396  </span></a>    <span class="s1">_usage_list(std::move(other._usage_list)),</span>
<a name="l397"><span class="ln">397  </span></a>    <span class="s1">_cache_map(std::move(other._cache_map)),</span>
<a name="l398"><span class="ln">398  </span></a>    <span class="s1">_max_size(other._max_size) {}</span>
<a name="l399"><span class="ln">399  </span></a>
<a name="l400"><span class="ln">400  </span></a>  <span class="s1">CuFFTParamsLRUCache&amp; </span><span class="s2">operator</span><span class="s1">=(CuFFTParamsLRUCache&amp;&amp; other) noexcept {</span>
<a name="l401"><span class="ln">401  </span></a>    <span class="s1">_usage_list = std::move(other._usage_list);</span>
<a name="l402"><span class="ln">402  </span></a>    <span class="s1">_cache_map = std::move(other._cache_map);</span>
<a name="l403"><span class="ln">403  </span></a>    <span class="s1">_max_size = other._max_size;</span>
<a name="l404"><span class="ln">404  </span></a>    <span class="s0">return </span><span class="s1">*</span><span class="s2">this</span><span class="s1">;</span>
<a name="l405"><span class="ln">405  </span></a>  <span class="s1">}</span>
<a name="l406"><span class="ln">406  </span></a>
<a name="l407"><span class="ln">407  </span></a>  <span class="s3">// If key is in this cache, return the cached config. Otherwise, emplace the</span>
<a name="l408"><span class="ln">408  </span></a>  <span class="s3">// config in this cache and return it.</span>
<a name="l409"><span class="ln">409  </span></a>  <span class="s3">// Return const reference because CuFFTConfig shouldn't be tampered with once</span>
<a name="l410"><span class="ln">410  </span></a>  <span class="s3">// created.</span>
<a name="l411"><span class="ln">411  </span></a>  <span class="s0">const </span><span class="s1">CuFFTConfig &amp;lookup(CuFFTParams params) {</span>
<a name="l412"><span class="ln">412  </span></a>    <span class="s1">AT_ASSERT(_max_size &gt; </span><span class="s4">0</span><span class="s1">);</span>
<a name="l413"><span class="ln">413  </span></a>
<a name="l414"><span class="ln">414  </span></a>    <span class="s1">map_kkv_iter_t map_it = _cache_map.find(params);</span>
<a name="l415"><span class="ln">415  </span></a>    <span class="s3">// Hit, put to list front</span>
<a name="l416"><span class="ln">416  </span></a>    <span class="s0">if </span><span class="s1">(map_it != _cache_map.end()) {</span>
<a name="l417"><span class="ln">417  </span></a>      <span class="s1">_usage_list.splice(_usage_list.begin(), _usage_list, map_it</span><span class="s6">-&gt;</span><span class="s1">second);</span>
<a name="l418"><span class="ln">418  </span></a>      <span class="s0">return </span><span class="s1">map_it</span><span class="s6">-&gt;</span><span class="s1">second</span><span class="s6">-&gt;</span><span class="s1">second;</span>
<a name="l419"><span class="ln">419  </span></a>    <span class="s1">}</span>
<a name="l420"><span class="ln">420  </span></a>
<a name="l421"><span class="ln">421  </span></a>    <span class="s3">// Miss</span>
<a name="l422"><span class="ln">422  </span></a>    <span class="s3">// remove if needed</span>
<a name="l423"><span class="ln">423  </span></a>    <span class="s0">if </span><span class="s1">(_usage_list.size() &gt;= _max_size) {</span>
<a name="l424"><span class="ln">424  </span></a>      <span class="s0">auto </span><span class="s1">last = _usage_list.end();</span>
<a name="l425"><span class="ln">425  </span></a>      <span class="s1">last--;</span>
<a name="l426"><span class="ln">426  </span></a>      <span class="s1">_cache_map.erase(last</span><span class="s6">-&gt;</span><span class="s1">first);</span>
<a name="l427"><span class="ln">427  </span></a>      <span class="s1">_usage_list.pop_back();</span>
<a name="l428"><span class="ln">428  </span></a>    <span class="s1">}</span>
<a name="l429"><span class="ln">429  </span></a>
<a name="l430"><span class="ln">430  </span></a>    <span class="s3">// construct new plan at list front, then insert into _cache_map</span>
<a name="l431"><span class="ln">431  </span></a>    <span class="s1">_usage_list.emplace_front(std::piecewise_construct,</span>
<a name="l432"><span class="ln">432  </span></a>                       <span class="s1">std::forward_as_tuple(params),</span>
<a name="l433"><span class="ln">433  </span></a>                       <span class="s1">std::forward_as_tuple(params));</span>
<a name="l434"><span class="ln">434  </span></a>    <span class="s0">auto </span><span class="s1">kv_it = _usage_list.begin();</span>
<a name="l435"><span class="ln">435  </span></a>    <span class="s1">_cache_map.emplace(std::piecewise_construct,</span>
<a name="l436"><span class="ln">436  </span></a>                <span class="s1">std::forward_as_tuple(kv_it</span><span class="s6">-&gt;</span><span class="s1">first),</span>
<a name="l437"><span class="ln">437  </span></a>                <span class="s1">std::forward_as_tuple(kv_it));</span>
<a name="l438"><span class="ln">438  </span></a>    <span class="s0">return </span><span class="s1">kv_it</span><span class="s6">-&gt;</span><span class="s1">second;</span>
<a name="l439"><span class="ln">439  </span></a>  <span class="s1">}</span>
<a name="l440"><span class="ln">440  </span></a>
<a name="l441"><span class="ln">441  </span></a>  <span class="s0">void </span><span class="s1">clear() {</span>
<a name="l442"><span class="ln">442  </span></a>    <span class="s1">_cache_map.clear();</span>
<a name="l443"><span class="ln">443  </span></a>    <span class="s1">_usage_list.clear();</span>
<a name="l444"><span class="ln">444  </span></a>  <span class="s1">}</span>
<a name="l445"><span class="ln">445  </span></a>
<a name="l446"><span class="ln">446  </span></a>  <span class="s0">void </span><span class="s1">resize(int64_t new_size) {</span>
<a name="l447"><span class="ln">447  </span></a>    <span class="s1">_set_max_size(new_size);</span>
<a name="l448"><span class="ln">448  </span></a>    <span class="s0">auto </span><span class="s1">cur_size = _usage_list.size();</span>
<a name="l449"><span class="ln">449  </span></a>    <span class="s0">if </span><span class="s1">(cur_size &gt; _max_size) {</span>
<a name="l450"><span class="ln">450  </span></a>      <span class="s0">auto </span><span class="s1">delete_it = _usage_list.end();</span>
<a name="l451"><span class="ln">451  </span></a>      <span class="s0">for </span><span class="s1">(size_t i = </span><span class="s4">0</span><span class="s1">; i &lt; cur_size - _max_size; i++) {</span>
<a name="l452"><span class="ln">452  </span></a>        <span class="s1">delete_it--;</span>
<a name="l453"><span class="ln">453  </span></a>        <span class="s1">_cache_map.erase(delete_it</span><span class="s6">-&gt;</span><span class="s1">first);</span>
<a name="l454"><span class="ln">454  </span></a>      <span class="s1">}</span>
<a name="l455"><span class="ln">455  </span></a>      <span class="s1">_usage_list.erase(delete_it, _usage_list.end());</span>
<a name="l456"><span class="ln">456  </span></a>    <span class="s1">}</span>
<a name="l457"><span class="ln">457  </span></a>  <span class="s1">}</span>
<a name="l458"><span class="ln">458  </span></a>
<a name="l459"><span class="ln">459  </span></a>  <span class="s1">size_t size() </span><span class="s0">const </span><span class="s1">{ </span><span class="s0">return </span><span class="s1">_cache_map.size(); }</span>
<a name="l460"><span class="ln">460  </span></a>
<a name="l461"><span class="ln">461  </span></a>  <span class="s1">size_t max_size() </span><span class="s0">const </span><span class="s1">noexcept { </span><span class="s0">return </span><span class="s1">_max_size; }</span>
<a name="l462"><span class="ln">462  </span></a>
<a name="l463"><span class="ln">463  </span></a>  <span class="s1">std::mutex mutex;</span>
<a name="l464"><span class="ln">464  </span></a>
<a name="l465"><span class="ln">465  </span></a><span class="s2">private</span><span class="s1">:</span>
<a name="l466"><span class="ln">466  </span></a>  <span class="s3">// Only sets size and does value check. Does not resize the data structures.</span>
<a name="l467"><span class="ln">467  </span></a>  <span class="s0">void </span><span class="s1">_set_max_size(int64_t new_size) {</span>
<a name="l468"><span class="ln">468  </span></a>    <span class="s3">// We check that 0 &lt;= new_size &lt;= CUFFT_MAX_PLAN_NUM here. Since</span>
<a name="l469"><span class="ln">469  </span></a>    <span class="s3">// CUFFT_MAX_PLAN_NUM is of type size_t, we need to do non-negativity check</span>
<a name="l470"><span class="ln">470  </span></a>    <span class="s3">// first.</span>
<a name="l471"><span class="ln">471  </span></a>    <span class="s1">TORCH_CHECK(new_size &gt;= </span><span class="s4">0</span><span class="s1">,</span>
<a name="l472"><span class="ln">472  </span></a>             <span class="s5">&quot;cuFFT plan cache size must be non-negative, but got &quot;</span><span class="s1">, new_size);</span>
<a name="l473"><span class="ln">473  </span></a>    <span class="s1">TORCH_CHECK(new_size &lt;= CUFFT_MAX_PLAN_NUM,</span>
<a name="l474"><span class="ln">474  </span></a>             <span class="s5">&quot;cuFFT plan cache size can not be larger than &quot;</span><span class="s1">, CUFFT_MAX_PLAN_NUM, </span><span class="s5">&quot;, but got &quot;</span><span class="s1">, new_size);</span>
<a name="l475"><span class="ln">475  </span></a>    <span class="s1">_max_size = </span><span class="s2">static_cast</span><span class="s1">&lt;size_t&gt;(new_size);</span>
<a name="l476"><span class="ln">476  </span></a>  <span class="s1">}</span>
<a name="l477"><span class="ln">477  </span></a>
<a name="l478"><span class="ln">478  </span></a>  <span class="s1">std::list&lt;kv_t&gt; _usage_list;</span>
<a name="l479"><span class="ln">479  </span></a>  <span class="s1">map_t _cache_map;</span>
<a name="l480"><span class="ln">480  </span></a>  <span class="s1">size_t _max_size;</span>
<a name="l481"><span class="ln">481  </span></a><span class="s1">};</span>
<a name="l482"><span class="ln">482  </span></a>
<a name="l483"><span class="ln">483  </span></a><span class="s3">// Since ATen is separated into CPU build and CUDA build, we need a way to call</span>
<a name="l484"><span class="ln">484  </span></a><span class="s3">// these functions only when CUDA is loaded. We use CUDA hooks for this purpose</span>
<a name="l485"><span class="ln">485  </span></a><span class="s3">// (at cuda/detail/CUDAHooks.cpp), and call the hooked functions from the actual</span>
<a name="l486"><span class="ln">486  </span></a><span class="s3">// native function counterparts (at native/SpectralOps.cpp), i.e.,</span>
<a name="l487"><span class="ln">487  </span></a><span class="s3">// _cufft_get_plan_cache_max_size, _cufft_set_plan_cache_max_size</span>
<a name="l488"><span class="ln">488  </span></a><span class="s3">// _cufft_get_plan_cache_size, and _cufft_clear_plan_cache.</span>
<a name="l489"><span class="ln">489  </span></a><span class="s1">int64_t cufft_get_plan_cache_max_size_impl(DeviceIndex device_index);</span>
<a name="l490"><span class="ln">490  </span></a><span class="s0">void </span><span class="s1">cufft_set_plan_cache_max_size_impl(DeviceIndex device_index, int64_t max_size);</span>
<a name="l491"><span class="ln">491  </span></a><span class="s1">int64_t cufft_get_plan_cache_size_impl(DeviceIndex device_index);</span>
<a name="l492"><span class="ln">492  </span></a><span class="s0">void </span><span class="s1">cufft_clear_plan_cache_impl(DeviceIndex device_index);</span>
<a name="l493"><span class="ln">493  </span></a>
<a name="l494"><span class="ln">494  </span></a><span class="s1">} </span><span class="s3">// namespace at::native::detail</span>
<a name="l495"><span class="ln">495  </span></a></pre>
</body>
</html>