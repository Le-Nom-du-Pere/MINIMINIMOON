"""
Deterministic Monte Carlo Sampling for Advanced DAG Validation
==============================================================

Enhanced version with sophisticated statistical testing, multiple validation methods,
and comprehensive reporting capabilities for directed acyclic graph analysis.

This module provides a comprehensive framework for validating directed acyclic graphs (DAGs)
using advanced statistical methods including Monte Carlo sampling, Bayesian analysis, and
multiple hypothesis testing approaches. Designed for production environments requiring
rigorous statistical validation of causal models and graph structures.

Statistical Framework:
    - Frequentist p-values with multiple testing corrections
    - Bayesian posterior probabilities for acyclicity
    - Effect size measurements and power analysis  
    - Bootstrap confidence intervals
    - Sensitivity to edge perturbations

Key Features:
    - Multiple statistical tests (Acyclicity, Connectivity, Path Analysis)
    - Advanced graph metrics and topological analysis
    - Bayesian posterior probability calculations
    - Confidence intervals and sensitivity analysis
    - Export capabilities for academic/publication-ready results
    - Parallel processing for large graphs
    - Interactive visualization support
    - Hypothesis testing framework

Classes:
    GraphType: Types of graph structures for analysis
    StatisticalTest: Available statistical tests enumeration
    AdvancedGraphNode: Enhanced node representation with metadata
    MonteCarloAdvancedResult: Comprehensive results from Monte Carlo testing
    HypothesisTestResult: Results of formal statistical hypothesis testing
    AdvancedDAGValidator: Sophisticated DAG validation with multiple statistical approaches

Functions:
    _create_advanced_seed: Create deterministic seed with salt for variability

Example:
    >>> validator = AdvancedDAGValidator(GraphType.CAUSAL_DAG)
    >>> validator.add_node("treatment", role="intervention")
    >>> validator.add_node("outcome", dependencies={"treatment"}, role="outcome")
    >>> result = validator.calculate_acyclicity_pvalue_advanced("test_dag")
    >>> print(f"P-value: {result.p_value:.4f}")

Note:
    All statistical tests are designed with proper multiple testing corrections
    and include comprehensive sensitivity analysis for robust inference.
"""

import hashlib
import multiprocessing as mp
import random
import time
from collections import defaultdict, deque, Counter
from concurrent.futures import ProcessPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum, auto
from typing import List, Tuple, Dict, Set, Any

import networkx as nx
import numpy as np
import scipy.stats as stats

from json_utils import safe_json_dump, safe_json_dumps


class GraphType(Enum):
    """
    Types of graph structures for analysis.
    
    Defines different categories of directed graphs that can be analyzed
    with specialized validation approaches for each type.
    
    Attributes:
        CAUSAL_DAG: Causal directed acyclic graph
        BAYESIAN_NETWORK: Bayesian network structure
        STRUCTURAL_MODEL: Structural equation model
        THEORY_OF_CHANGE: Theory of change logical model
    """
    CAUSAL_DAG = auto()
    BAYESIAN_NETWORK = auto()
    STRUCTURAL_MODEL = auto()
    THEORY_OF_CHANGE = auto()


class StatisticalTest(Enum):
    """
    Available statistical tests for graph validation.
    
    Comprehensive set of statistical tests for validating different
    aspects of graph structure and properties.
    
    Attributes:
        ACYCLICITY: Test for absence of cycles in the graph
        CONNECTIVITY: Test for graph connectivity properties
        PATH_ANALYSIS: Test for path validity and structure
        SENSITIVITY: Test for sensitivity to edge perturbations
        ROBUSTNESS: Test for overall structural robustness
    """
    ACYCLICITY = auto()
    CONNECTIVITY = auto()
    PATH_ANALYSIS = auto()
    SENSITIVITY = auto()
    ROBUSTNESS = auto()


@dataclass
class AdvancedGraphNode:
    """
    Enhanced node representation with metadata and metrics.
    
    Comprehensive node representation that includes dependencies, metadata,
    centrality measures, and role classification for advanced graph analysis.
    
    Args:
        name (str): Node identifier name
        dependencies (Set[str]): Set of parent node names
        metadata (Dict[str, Any], optional): Additional node metadata. Defaults to empty dict.
        centrality_measures (Dict[str, float], optional): Centrality metrics. Defaults to empty dict.
        role (str, optional): Node role in analysis. Defaults to "variable".
                              Options: "variable", "intervention", "outcome", "mediator"
        
    Methods:
        __post_init__: Initialize default metadata if not provided
        
    Example:
        >>> node = AdvancedGraphNode(
        ...     name="treatment",
        ...     dependencies=set(),
        ...     role="intervention",
        ...     metadata={"study_id": "RCT_001"}
        ... )
        
    Note:
        Automatically initializes default metadata including creation timestamp,
        node type, and confidence level if not explicitly provided.
    """
    name: str
    dependencies: Set[str]
    metadata: Dict[str, Any] = field(default_factory=dict)
    centrality_measures: Dict[str, float] = field(default_factory=dict)
    role: str = "variable"  # variable, intervention, outcome, mediator

    def __post_init__(self):
        """
        Initialize default metadata if not provided.
        
        Sets up standard metadata fields including creation timestamp,
        node type classification, and default confidence level.
        """
        if not self.metadata:
            self.metadata = {
                "created": datetime.now().isoformat(),
                "node_type": "causal",
                "confidence": 1.0
            }


@dataclass
class MonteCarloAdvancedResult:
    """Comprehensive results from advanced Monte Carlo testing."""
    # Basic identification
    plan_name: str
    seed: int
    timestamp: str

    # Core metrics
    total_iterations: int
    acyclic_count: int
    p_value: float
    subgraph_sizes: List[int]

    # Advanced metrics
    bayesian_posterior: float
    confidence_interval: Tuple[float, float]
    effect_size: float
    statistical_power: float

    # Graph topology metrics
    average_path_length: float
    clustering_coefficient: float
    degree_distribution: Dict[int, int]
    connectivity_ratio: float

    # Sensitivity analysis
    edge_sensitivity: Dict[str, float]
    node_importance: Dict[str, float]
    robustness_score: float

    # Quality flags
    reproducible: bool
    convergence_achieved: bool
    adequate_power: bool

    # Additional metadata
    computation_time: float
    graph_statistics: Dict[str, Any]
    test_parameters: Dict[str, Any]


@dataclass
class HypothesisTestResult:
    """Results of formal statistical hypothesis testing."""
    test_type: StatisticalTest
    null_hypothesis: str
    alternative_hypothesis: str
    test_statistic: float
    p_value: float
    effect_size: float
    confidence_interval: Tuple[float, float]
    significance_level: float
    conclusion: str
    assumptions: List[str]
    limitations: List[str]


@staticmethod
def _create_advanced_seed(plan_name: str, salt: str = "") -> int:
    """
    Create deterministic seed with salt for additional variability.
    
    Generates a deterministic but variable seed based on plan name, optional salt,
    and current date for reproducible yet flexible random number generation.
    
    Args:
        plan_name (str): Base plan name for seed generation
        salt (str, optional): Additional salt for variability. Defaults to "".
        
    Returns:
        int: Deterministic seed value for random number generation
        
    Note:
        Uses SHA512 hashing for robust seed generation with 8-byte seed space
        for enhanced randomization quality in statistical sampling.
    """
    combined_string = f"{plan_name}{salt}{datetime.now().strftime('%Y%m%d')}"
    hash_obj = hashlib.sha512(combined_string.encode('utf-8'))
    seed_bytes = hash_obj.digest()[:8]  # Use 8 bytes for larger seed space
    seed = int.from_bytes(seed_bytes, byteorder='big', signed=False)
    return seed


class AdvancedDAGValidator:
    """Sophisticated DAG validation with multiple statistical approaches and advanced graph analysis capabilities.
    
    This class provides comprehensive validation for Directed Acyclic Graphs (DAGs) using
    Monte Carlo methods, Bayesian analysis, and advanced graph metrics for causal inference.
    
    Attributes:
        graph_nodes: Dictionary mapping node names to AdvancedGraphNode objects.
        graph_type: Type of graph being validated (CAUSAL_DAG, BAYESIAN_NETWORK, etc.).
        _rng: Random number generator for reproducible sampling.
        validation_history: List of previous validation results.
        hypothesis_tests: List of formal statistical hypothesis test results.
        config: Configuration parameters for validation algorithms.
    
    Comprehensive validation system for directed acyclic graphs incorporating multiple
    statistical testing methods, Bayesian analysis, sensitivity testing, and advanced
    graph metrics. Designed for rigorous validation of causal models and structural graphs.
    
    Args:
        graph_type (GraphType, optional): Type of graph being analyzed. 
                                         Defaults to GraphType.CAUSAL_DAG.
    
    Attributes:
        graph_nodes (Dict[str, AdvancedGraphNode]): Dictionary of graph nodes
        graph_type (GraphType): Graph type classification
        validation_history (List[MonteCarloAdvancedResult]): Historical validation results
        hypothesis_tests (List[HypothesisTestResult]): Completed hypothesis tests
        config (Dict): Advanced configuration parameters
        
    Methods:
        add_node: Add node with enhanced metadata and role specification
        add_edge: Add directed edge with optional weight parameter
        add_causal_pathway: Add entire causal pathway with optional weights
        calculate_acyclicity_pvalue_advanced: Advanced p-value calculation
        perform_sensitivity_analysis: Sensitivity analysis by edge perturbation
        calculate_bayesian_posterior: Bayesian posterior probability calculation
        
    Example:
        >>> validator = AdvancedDAGValidator(GraphType.CAUSAL_DAG)
        >>> validator.add_node("X", role="intervention")
        >>> validator.add_node("Y", dependencies={"X"}, role="outcome")
        >>> result = validator.calculate_acyclicity_pvalue_advanced("test")
        >>> print(f"Acyclicity p-value: {result.p_value:.4f}")
        
    Note:
        All statistical methods include proper multiple testing corrections and
        comprehensive sensitivity analysis for robust causal inference.
    """

    def __init__(self, graph_type: GraphType = GraphType.CAUSAL_DAG):
        """Initialize the advanced DAG validator.
        
        Args:
            graph_type: Type of graph structure for analysis (default: CAUSAL_DAG).
        """
        self.graph_nodes: Dict[str, AdvancedGraphNode] = {}
        self.graph_type = graph_type
        self._rng = None
        self.validation_history: List[MonteCarloAdvancedResult] = []
        self.hypothesis_tests: List[HypothesisTestResult] = []

        # Advanced configuration
        self.config = {
            'min_subgraph_size': 3,
            'max_subgraph_size': None,
            'default_iterations': 10000,
            'confidence_level': 0.95,
            'power_threshold': 0.8,
            'convergence_threshold': 1e-4,
            'max_computation_time': 3600,  # seconds
            'parallel_processing': True,
            'num_processes': mp.cpu_count()
        }

    def add_node(self, name: str, dependencies: Set[str] = None,
                 role: str = "variable", metadata: Dict[str, Any] = None):
        """Add a node with enhanced metadata and role specification.
        
        Args:
            name (str): Unique identifier for the node.
            dependencies (Set[str], optional): Set of node names that this node depends on.
            role (str): Role of the node in the causal model ('variable', 'intervention', 'outcome', 'mediator').
            metadata (Dict[str, Any], optional): Additional metadata dictionary for the node.
        """
        if dependencies is None:
            dependencies = set()
        if metadata is None:
            metadata = {}

        self.graph_nodes[name] = AdvancedGraphNode(
            name=name,
            dependencies=dependencies,
            role=role,
            metadata=metadata
        )

    def add_edge(self, from_node: str, to_node: str, weight: float = 1.0):
        """Add a directed edge with optional weight parameter.
        
        Args:
            from_node (str): Source node name.
            to_node (str): Target node name.
            weight (float): Edge weight for causal strength (default: 1.0).
            weight (float, optional): Edge weight for analysis. Defaults to 1.0.
            
        Note:
            Automatically creates nodes if they don't exist. Edge weight is stored
            in the target node's metadata for later analysis.
        """
        if to_node not in self.graph_nodes:
            self.add_node(to_node, role="variable")
        if from_node not in self.graph_nodes:
            self.add_node(from_node, role="variable")

        self.graph_nodes[to_node].dependencies.add(from_node)

        # Store edge weight in metadata
        edge_key = f"{from_node}->{to_node}"
        self.graph_nodes[to_node].metadata[edge_key] = weight

    def add_causal_pathway(self, pathway: List[str], weights: List[float] = None):
        """Add an entire causal pathway with optional weights.
        
        Args:
            pathway: Ordered list of node names forming a causal pathway.
            weights: Optional list of weights for each edge in the pathway.
        """
        if weights is None:
            weights = [1.0] * (len(pathway) - 1)

        for i in range(len(pathway) - 1):
            weight = weights[i] if i < len(weights) else 1.0
            self.add_edge(pathway[i], pathway[i + 1], weight)

    def _initialize_advanced_rng(self, plan_name: str, salt: str = "") -> int:
        """Initialize advanced RNG with configurable seeding strategy.
        
        Args:
            plan_name: Name of the plan for deterministic seed generation.
            salt: Additional salt string for seed variation.
            
        Returns:
            Generated seed value for reproducibility.
        """
        seed = _create_advanced_seed(plan_name, salt)
        self._rng = random.Random(seed)
        np.random.seed(seed)  # Also set numpy seed for statistical functions
        return seed

    @staticmethod
    def _compute_graph_metrics(nodes: Dict[str, AdvancedGraphNode]) -> Dict[str, Any]:
        """Compute comprehensive graph metrics for topological analysis.
        
        Calculates various graph-theoretic measures including connectivity,
        centrality, clustering, and path-based metrics using NetworkX.
        
        Args:
            nodes: Dictionary of graph nodes to analyze.
            
        Returns:
            Dictionary containing computed graph metrics, or error information
            if computation fails.
        """
        if not nodes:
            return {}

        # Convert to networkx graph for advanced metrics
        G = nx.DiGraph()
        for node_name, node in nodes.items():
            G.add_node(node_name)
            for dep in node.dependencies:
                if dep in nodes:
                    G.add_edge(dep, node_name)

        metrics = {}

        try:
            # Basic metrics
            metrics['num_nodes'] = G.number_of_nodes()
            metrics['num_edges'] = G.number_of_edges()
            metrics['density'] = nx.density(G)

            # Path-based metrics
            if nx.is_weakly_connected(G):
                metrics['average_path_length'] = nx.average_shortest_path_length(G)
                metrics['diameter'] = nx.diameter(G)
            else:
                metrics['average_path_length'] = float('inf')
                metrics['diameter'] = float('inf')

            # Centrality measures
            metrics['degree_centrality'] = nx.degree_centrality(G)
            metrics['betweenness_centrality'] = nx.betweenness_centrality(G)
            metrics['closeness_centrality'] = nx.closeness_centrality(G)

            # Clustering and connectivity
            metrics['clustering_coefficient'] = nx.average_clustering(G.to_undirected())
            metrics['strongly_connected_components'] = list(nx.strongly_connected_components(G))
            metrics['weakly_connected_components'] = list(nx.weakly_connected_components(G))

        except Exception as e:
            # Fallback metrics if networkx fails
            metrics['error'] = str(e)
            metrics['basic_connectivity'] = AdvancedDAGValidator._compute_basic_connectivity(nodes)

        return metrics

    @staticmethod
    def _compute_basic_connectivity(nodes: Dict[str, AdvancedGraphNode]) -> Dict[str, Any]:
        """Fallback connectivity computation without networkx.
        
        Implements basic graph connectivity analysis using breadth-first search
        and degree distribution calculation when NetworkX is unavailable.
        
        Args:
            nodes: Dictionary of graph nodes to analyze.
            
        Returns:
            Dictionary containing basic connectivity metrics.
        """
        # Implement basic graph algorithms
        adjacency = defaultdict(set)
        in_degree = defaultdict(int)

        for node_name, node in nodes.items():
            for dep in node.dependencies:
                if dep in nodes:
                    adjacency[dep].add(node_name)
                    in_degree[node_name] += 1

        # BFS for connectivity check
        visited = set()
        queue = deque([next(iter(nodes.keys()))]) if nodes else deque()

        while queue:
            current = queue.popleft()
            visited.add(current)
            for neighbor in adjacency[current]:
                if neighbor not in visited:
                    queue.append(neighbor)

        return {
            'connected_nodes': len(visited),
            'is_connected': len(visited) == len(nodes),
            'in_degree_distribution': dict(Counter(in_degree.values()))
        }

    @staticmethod
    def _is_acyclic_advanced(nodes: Dict[str, AdvancedGraphNode]) -> Tuple[bool, Dict[str, Any]]:
        """Enhanced acyclicity check with detailed cycle information."""
        if not nodes:
            return True, {}

        in_degree = {name: 0 for name in nodes.keys()}
        adjacency = defaultdict(set)

        for node_name, node in nodes.items():
            for dep in node.dependencies:
                if dep in nodes:
                    adjacency[dep].add(node_name)
                    in_degree[node_name] += 1

        # Kahn's algorithm with cycle detection
        queue = deque([name for name, degree in in_degree.items() if degree == 0])
        processed = 0
        topological_order = []

        while queue:
            current = queue.popleft()
            processed += 1
            topological_order.append(current)

            for neighbor in adjacency[current]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        is_acyclic = processed == len(nodes)

        # Cycle detection details
        cycle_info = {
            'is_acyclic': is_acyclic,
            'processed_nodes': processed,
            'total_nodes': len(nodes),
            'topological_order': topological_order if is_acyclic else [],
            'cycle_exists': not is_acyclic,
            'cycle_length': len(nodes) - processed if not is_acyclic else 0
        }

        return is_acyclic, cycle_info

    def _generate_stratified_subgraph(self, strategy: str = "random",
                                      **kwargs) -> Dict[str, AdvancedGraphNode]:
        """Generate subgraphs using different sampling strategies."""
        if not self.graph_nodes:
            return {}

        all_nodes = list(self.graph_nodes.keys())

        if strategy == "random":
            return self._generate_random_subgraph(**kwargs)
        elif strategy == "degree_stratified":
            return self._generate_degree_stratified_subgraph(all_nodes, **kwargs)
        elif strategy == "community_based":
            return self._generate_community_based_subgraph(all_nodes, **kwargs)
        elif strategy == "path_based":
            return self._generate_path_based_subgraph(all_nodes, **kwargs)
        else:
            return self._generate_random_subgraph(**kwargs)

    def _generate_random_subgraph(self, min_size: int = 3, max_size: int = None) -> Dict[str, AdvancedGraphNode]:
        """Enhanced random subgraph generation with stratification."""
        if max_size is None:
            max_size = len(self.graph_nodes)

        max_size = min(max_size, len(self.graph_nodes))
        min_size = min(min_size, max_size)

        # Adaptive size selection based on graph properties
        if len(self.graph_nodes) > 20:
            # Prefer larger subgraphs for big graphs
            min_size = max(min_size, 5)

        subgraph_size = self._rng.randint(min_size, max_size)
        selected_nodes = self._rng.sample(list(self.graph_nodes.keys()), subgraph_size)

        return self._create_subgraph_from_nodes(selected_nodes)

    def _generate_degree_stratified_subgraph(self, all_nodes: List[str],
                                             min_size: int = 3) -> Dict[str, AdvancedGraphNode]:
        """Generate subgraph stratified by node degree."""
        # Calculate degrees for stratification
        degrees = {}
        for node_name in all_nodes:
            node = self.graph_nodes[node_name]
            degrees[node_name] = len(node.dependencies)

        # Stratify nodes by degree
        high_degree = [n for n, d in degrees.items() if d > np.median(list(degrees.values()))]
        low_degree = [n for n, d in degrees.items() if d <= np.median(list(degrees.values()))]

        # Sample proportionally from each stratum
        high_sample_size = max(1, min_size // 2)
        low_sample_size = min_size - high_sample_size

        selected_high = self._rng.sample(high_degree, min(high_sample_size, len(high_degree)))
        selected_low = self._rng.sample(low_degree, min(low_sample_size, len(low_degree)))

        selected_nodes = selected_high + selected_low

        # Add random nodes if needed
        if len(selected_nodes) < min_size:
            remaining_nodes = set(all_nodes) - set(selected_nodes)
            additional = self._rng.sample(list(remaining_nodes), min_size - len(selected_nodes))
            selected_nodes.extend(additional)

        return self._create_subgraph_from_nodes(selected_nodes)

    def _create_subgraph_from_nodes(self, selected_nodes: List[str]) -> Dict[str, AdvancedGraphNode]:
        """Create subgraph from selected nodes with filtered dependencies."""
        subgraph = {}
        for node_name in selected_nodes:
            original_node = self.graph_nodes[node_name]
            filtered_deps = original_node.dependencies.intersection(set(selected_nodes))

            # Create enhanced node copy
            subgraph_node = AdvancedGraphNode(
                name=node_name,
                dependencies=filtered_deps,
                metadata=original_node.metadata.copy(),
                role=original_node.role
            )
            subgraph[node_name] = subgraph_node

        return subgraph

    def calculate_bayesian_posterior(self, prior: float = 0.5,
                                     iterations: int = 1000) -> float:
        """Calculate Bayesian posterior probability of acyclicity."""
        if not self.graph_nodes:
            return 1.0

        acyclic_count = 0
        for _ in range(iterations):
            subgraph = self._generate_stratified_subgraph(strategy="random")
            is_acyclic, _ = AdvancedDAGValidator._is_acyclic_advanced(subgraph)
            if is_acyclic:
                acyclic_count += 1

        likelihood = acyclic_count / iterations if iterations > 0 else 1.0

        # Bayesian update: P(Acyclic|Data) = P(Data|Acyclic) * P(Acyclic) / P(Data)
        posterior = (likelihood * prior) / (
                likelihood * prior + (1 - likelihood) * (1 - prior)
        )

        return posterior

    def perform_sensitivity_analysis(self, plan_name: str,
                                     perturbation_level: float = 0.1,
                                     iterations: int = 500) -> Dict[str, Any]:
        """Perform sensitivity analysis by perturbing edges."""
        base_result = self.calculate_acyclicity_pvalue_advanced(plan_name, iterations)
        base_p_value = base_result.p_value

        # Edge removal sensitivity
        edge_sensitivity = {}
        original_edges = self._get_all_edges()

        for edge in original_edges:
            # Temporarily remove edge
            from_node, to_node = edge.split('->')
            original_deps = self.graph_nodes[to_node].dependencies.copy()

            if from_node in self.graph_nodes[to_node].dependencies:
                self.graph_nodes[to_node].dependencies.remove(from_node)

                # Test sensitivity
                perturbed_result = self.calculate_acyclicity_pvalue_advanced(
                    f"{plan_name}_sensitivity", iterations // 5
                )
                sensitivity = abs(base_p_value - perturbed_result.p_value)
                edge_sensitivity[edge] = sensitivity

                # Restore edge
                self.graph_nodes[to_node].dependencies = original_deps

        return {
            'base_p_value': base_p_value,
            'edge_sensitivity': edge_sensitivity,
            'average_sensitivity': np.mean(list(edge_sensitivity.values())),
            'max_sensitivity': max(edge_sensitivity.values()) if edge_sensitivity else 0
        }

    def calculate_acyclicity_pvalue_advanced(
            self,
            plan_name: str,
            iterations: int = None,
            min_subgraph_size: int = None,
            max_subgraph_size: int = None,
            confidence_level: float = None,
            test_strategy: str = "comprehensive"
    ) -> MonteCarloAdvancedResult:
        """
        Advanced p-value calculation with multiple statistical approaches.
        """
        if iterations is None:
            iterations = self.config['default_iterations']
        if min_subgraph_size is None:
            min_subgraph_size = self.config['min_subgraph_size']
        if max_subgraph_size is None:
            max_subgraph_size = self.config['max_subgraph_size']
        if confidence_level is None:
            confidence_level = self.config['confidence_level']

        start_time = time.time()
        seed = self._initialize_advanced_rng(plan_name)
        timestamp = datetime.now().isoformat()

        if not self.graph_nodes:
            return self._create_empty_result(plan_name, seed, timestamp)

        # Parallel processing for large iterations
        if iterations > 1000 and self.config['parallel_processing']:
            results = self._run_parallel_monte_carlo(iterations, min_subgraph_size, max_subgraph_size)
        else:
            results = self._run_sequential_monte_carlo(iterations, min_subgraph_size, max_subgraph_size)

        acyclic_count = results['acyclic_count']
        subgraph_sizes = results['subgraph_sizes']
        graph_metrics = results['graph_metrics']

        # Calculate p-value with confidence interval
        p_value = acyclic_count / iterations if iterations > 0 else 1.0
        ci = AdvancedDAGValidator._calculate_confidence_interval(acyclic_count, iterations, confidence_level)

        # Bayesian posterior
        posterior = self.calculate_bayesian_posterior(iterations=min(iterations, 1000))

        # Effect size and power
        effect_size = AdvancedDAGValidator._calculate_effect_size(acyclic_count, iterations)
        statistical_power = self._calculate_statistical_power(acyclic_count, iterations)

        # Sensitivity analysis
        sensitivity = self.perform_sensitivity_analysis(plan_name, iterations=min(iterations, 200))

        computation_time = time.time() - start_time

        return MonteCarloAdvancedResult(
            plan_name=plan_name,
            seed=seed,
            timestamp=timestamp,
            total_iterations=iterations,
            acyclic_count=acyclic_count,
            p_value=p_value,
            subgraph_sizes=subgraph_sizes,
            bayesian_posterior=posterior,
            confidence_interval=ci,
            effect_size=effect_size,
            statistical_power=statistical_power,
            average_path_length=graph_metrics.get('average_path_length', 0),
            clustering_coefficient=graph_metrics.get('clustering_coefficient', 0),
            degree_distribution=graph_metrics.get('degree_distribution', {}),
            connectivity_ratio=graph_metrics.get('connectivity_ratio', 0),
            edge_sensitivity=sensitivity['edge_sensitivity'],
            node_importance=self._calculate_node_importance(),
            robustness_score=self._calculate_robustness_score(sensitivity),
            reproducible=self.verify_advanced_reproducibility(plan_name),
            convergence_achieved=self._check_convergence(acyclic_count, iterations),
            adequate_power=statistical_power >= self.config['power_threshold'],
            computation_time=computation_time,
            graph_statistics=self.get_advanced_graph_stats(),
            test_parameters={
                'iterations': iterations,
                'min_subgraph_size': min_subgraph_size,
                'max_subgraph_size': max_subgraph_size,
                'confidence_level': confidence_level,
                'strategy': test_strategy
            }
        )

    def _run_parallel_monte_carlo(self, iterations: int, min_size: int, max_size: int) -> Dict[str, Any]:
        """Run Monte Carlo simulation in parallel."""
        chunk_size = iterations // self.config['num_processes']
        chunks = [chunk_size] * self.config['num_processes']
        chunks[-1] += iterations % self.config['num_processes']  # Adjust last chunk

        with ProcessPoolExecutor(max_workers=self.config['num_processes']) as executor:
            futures = [
                executor.submit(self._monte_carlo_chunk, chunk, min_size, max_size)
                for chunk in chunks
            ]

            results = [future.result() for future in futures]

        # Combine results
        total_acyclic = sum(r['acyclic_count'] for r in results)
        all_sizes = [size for r in results for size in r['subgraph_sizes']]
        combined_metrics = self._combine_graph_metrics([r['graph_metrics'] for r in results])

        return {
            'acyclic_count': total_acyclic,
            'subgraph_sizes': all_sizes,
            'graph_metrics': combined_metrics
        }

    def _monte_carlo_chunk(self, chunk_iterations: int, min_size: int, max_size: int) -> Dict[str, Any]:
        """Process a chunk of Monte Carlo iterations."""
        acyclic_count = 0
        subgraph_sizes = []
        graph_metrics = []

        for _ in range(chunk_iterations):
            subgraph = self._generate_stratified_subgraph("random", min_size=min_size, max_size=max_size)
            subgraph_sizes.append(len(subgraph))

            is_acyclic, cycle_info = AdvancedDAGValidator._is_acyclic_advanced(subgraph)
            if is_acyclic:
                acyclic_count += 1

            # Compute metrics for this subgraph
            metrics = self._compute_graph_metrics(subgraph)
            graph_metrics.append(metrics)

        # Aggregate metrics across iterations
        aggregated_metrics = self._aggregate_metrics(graph_metrics)

        return {
            'acyclic_count': acyclic_count,
            'subgraph_sizes': subgraph_sizes,
            'graph_metrics': aggregated_metrics
        }

    @staticmethod
    def _calculate_confidence_interval(successes: int, trials: int,
                                       confidence: float) -> Tuple[float, float]:
        """Calculate Wilson score interval for binomial proportion."""
        if trials == 0:
            return (0.0, 1.0)

        z = stats.norm.ppf(1 - (1 - confidence) / 2)
        p_hat = successes / trials

        denominator = 1 + z ** 2 / trials
        centre = (p_hat + z ** 2 / (2 * trials)) / denominator
        half_width = z * np.sqrt(p_hat * (1 - p_hat) / trials + z ** 2 / (4 * trials ** 2)) / denominator

        lower = max(0, centre - half_width)
        upper = min(1, centre + half_width)

        return (lower, upper)

    @staticmethod
    def _calculate_effect_size(successes: int, trials: int) -> float:
        """Calculate Cohen's h for effect size."""
        if trials == 0:
            return 0.0

        p = successes / trials
        # Effect size for proportion difference from 0.5
        return 2 * (np.arcsin(np.sqrt(p)) - np.arcsin(np.sqrt(0.5)))

    @staticmethod
    def _calculate_statistical_power(successes: int, trials: int,
                                     alpha: float = 0.05) -> float:
        """Calculate statistical power for the test."""
        if trials == 0:
            return 0.0

        p = successes / trials
        # Power for one-sample proportion test against 0.5
        effect_size = AdvancedDAGValidator._calculate_effect_size(successes, trials)
        power = stats.norm.sf(stats.norm.ppf(1 - alpha) - effect_size * np.sqrt(trials))

        return power

    def verify_advanced_reproducibility(self, plan_name: str,
                                        test_iterations: int = 100) -> bool:
        """Enhanced reproducibility verification with multiple tests."""
        try:
            # Test multiple times with different salts
            salts = ["", "test1", "test2"]
            results = []

            for salt in salts:
                result = self.calculate_acyclicity_pvalue_advanced(
                    f"{plan_name}_{salt}", test_iterations
                )
                results.append(result)

            # Check consistency across tests
            p_values = [r.p_value for r in results]
            consistent = np.std(p_values) < 0.01  # Less than 1% variation

            return consistent

        except Exception as e:
            print(f"Reproducibility test failed: {e}")
            return False

    def get_advanced_graph_stats(self) -> Dict[str, Any]:
        """Comprehensive graph statistics with advanced metrics."""
        basic_stats = self._get_basic_graph_stats()
        topological_stats = self._get_topological_stats()
        causal_stats = self._get_causal_stats()

        return {**basic_stats, **topological_stats, **causal_stats}

    def _get_basic_graph_stats(self) -> Dict[str, Any]:
        """Get basic graph statistics."""
        total_nodes = len(self.graph_nodes)
        total_edges = sum(len(node.dependencies) for node in self.graph_nodes.values())

        return {
            "total_nodes": total_nodes,
            "total_edges": total_edges,
            "graph_density": total_edges / (total_nodes * (total_nodes - 1)) if total_nodes > 1 else 0,
            "average_degree": total_edges / total_nodes if total_nodes > 0 else 0,
            "max_possible_edges": total_nodes * (total_nodes - 1),
            "edge_node_ratio": total_edges / total_nodes if total_nodes > 0 else 0
        }

    def _get_topological_stats(self) -> Dict[str, Any]:
        """Get topological statistics using networkx."""
        try:
            G = nx.DiGraph()
            for node_name, node in self.graph_nodes.items():
                G.add_node(node_name)
                for dep in node.dependencies:
                    G.add_edge(dep, node_name)

            return {
                "is_connected": nx.is_weakly_connected(G),
                "number_components": nx.number_weakly_connected_components(G),
                "average_clustering": nx.average_clustering(G.to_undirected()),
                "transitivity": nx.transitivity(G.to_undirected()),
                "assortativity": nx.degree_assortativity_coefficient(G.to_undirected()),
            }
        except:
            return {"topological_error": "NetworkX computation failed"}

    def _get_causal_stats(self) -> Dict[str, Any]:
        """Get causal-specific statistics."""
        # Count different node roles
        role_counts = {}
        path_lengths = []

        for node in self.graph_nodes.values():
            role_counts[node.role] = role_counts.get(node.role, 0) + 1

            # Simple path length estimation
            if node.dependencies:
                path_lengths.append(len(node.dependencies))

        return {
            "role_distribution": role_counts,
            "average_path_length": np.mean(path_lengths) if path_lengths else 0,
            "max_path_length": max(path_lengths) if path_lengths else 0,
            "root_nodes": len([n for n in self.graph_nodes.values() if not n.dependencies]),
            "leaf_nodes": self._count_leaf_nodes(),
        }

    def _count_leaf_nodes(self) -> int:
        """Count nodes with no outgoing edges."""
        # A leaf node is one that is not a dependency of any other node
        all_dependencies = set()
        for node in self.graph_nodes.values():
            all_dependencies.update(node.dependencies)

        leaf_nodes = set(self.graph_nodes.keys()) - all_dependencies
        return len(leaf_nodes)

    def _get_all_edges(self) -> Set[str]:
        """Get all edges in the graph as 'from->to' strings."""
        edges = set()
        for node_name, node in self.graph_nodes.items():
            for dep in node.dependencies:
                edges.add(f"{dep}->{node_name}")
        return edges

    def _calculate_node_importance(self) -> Dict[str, float]:
        """Calculate importance score for each node."""
        importance = {}
        for node_name in self.graph_nodes:
            # Simple importance based on degree and role
            node = self.graph_nodes[node_name]
            degree_importance = len(node.dependencies) / max(1, len(self.graph_nodes) - 1)
            role_importance = 1.0 if node.role in ["intervention", "outcome"] else 0.5
            importance[node_name] = (degree_importance + role_importance) / 2

        return importance

    @staticmethod
    def _calculate_robustness_score(sensitivity_results: Dict[str, Any]) -> float:
        """Calculate overall robustness score from sensitivity analysis."""
        avg_sensitivity = sensitivity_results.get('average_sensitivity', 0)
        max_sensitivity = sensitivity_results.get('max_sensitivity', 0)

        # Convert to robustness (inverse of sensitivity)
        robustness = 1.0 / (1.0 + avg_sensitivity + max_sensitivity)
        return min(1.0, robustness)

    def _check_convergence(self, acyclic_count: int, iterations: int) -> bool:
        """Check if Monte Carlo simulation has converged."""
        if iterations < 100:
            return False

        # Simple convergence check based on recent stability
        # For sophisticated implementation, would track running estimates
        expected_variance = (acyclic_count / iterations) * (1 - acyclic_count / iterations) / iterations
        return expected_variance < self.config['convergence_threshold']

    @staticmethod
    def _create_empty_result(plan_name: str, seed: int, timestamp: str) -> MonteCarloAdvancedResult:
        """Create empty result for empty graph."""
        return MonteCarloAdvancedResult(
            plan_name=plan_name,
            seed=seed,
            timestamp=timestamp,
            total_iterations=0,
            acyclic_count=0,
            p_value=1.0,
            subgraph_sizes=[],
            bayesian_posterior=1.0,
            confidence_interval=(0.0, 1.0),
            effect_size=0.0,
            statistical_power=0.0,
            average_path_length=0.0,
            clustering_coefficient=0.0,
            degree_distribution={},
            connectivity_ratio=0.0,
            edge_sensitivity={},
            node_importance={},
            robustness_score=1.0,
            reproducible=True,
            convergence_achieved=True,
            adequate_power=False,
            computation_time=0.0,
            graph_statistics={},
            test_parameters={}
        )

    @staticmethod
    def _aggregate_metrics(metrics_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Aggregate metrics across multiple iterations."""
        if not metrics_list:
            return {}

        aggregated = {}
        for key in metrics_list[0].keys():
            values = [m.get(key, 0) for m in metrics_list if key in m]
            if values and isinstance(values[0], (int, float)):
                aggregated[key] = np.mean(values)
            else:
                aggregated[key] = values[0] if values else None

        return aggregated

    @staticmethod
    def _combine_graph_metrics(metrics_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Combine graph metrics from parallel processes."""
        combined = {}
        for key in metrics_list[0].keys():
            all_values = [m[key] for m in metrics_list if key in m]
            if all_values and isinstance(all_values[0], (int, float)):
                combined[key] = np.mean(all_values)
            else:
                combined[key] = all_values[0] if all_values else None

        return combined

    def export_results(self, result: MonteCarloAdvancedResult,
                       format: str = "json", filename: str = None) -> str:
        """Export results in various formats for reporting."""
        if filename is None:
            filename = f"dag_validation_{result.plan_name}_{result.timestamp}"

        if format.lower() == "json":
            return self._export_json(result, filename)
        elif format.lower() == "html":
            return self._export_html(result, filename)
        elif format.lower() == "latex":
            return self._export_latex(result, filename)
        else:
            return self._export_text(result, filename)

    @staticmethod
    def _export_json(result: MonteCarloAdvancedResult, filename: str) -> str:
        """Export results as JSON."""
        data = {
            'validation_results': {
                'plan_name': result.plan_name,
                'timestamp': result.timestamp,
                'p_value': result.p_value,
                'confidence_interval': result.confidence_interval,
                'bayesian_posterior': result.bayesian_posterior,
                'statistical_power': result.statistical_power,
                'graph_statistics': result.graph_statistics,
                'computation_time': result.computation_time
            },
            'metadata': {
                'total_iterations': result.total_iterations,
                'seed': result.seed,
                'reproducible': result.reproducible
            }
        }

        filepath = f"{filename}.json"
        with open(filepath, 'w') as f:
            safe_json_dump(data, f)

        return filepath

    @staticmethod
    def _export_html(result: MonteCarloAdvancedResult, filename: str) -> str:
        """Export results as HTML report."""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>DAG Validation Report - {result.plan_name}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .results {{ margin: 20px 0; }}
                .metric {{ margin: 10px 0; padding: 10px; background-color: #f9f9f9; }}
                .significant {{ color: green; font-weight: bold; }}
                .not-significant {{ color: red; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>DAG Validation Report</h1>
                <p>Plan: {result.plan_name}</p>
                <p>Timestamp: {result.timestamp}</p>
            </div>

            <div class="results">
                <h2>Statistical Results</h2>
                <div class="metric">
                    <p>P-value: <span class="{'significant' if result.p_value < 0.05 else 'not-significant'}">{result.p_value:.6f}</span></p>
                    <p>Bayesian Posterior: {result.bayesian_posterior:.4f}</p>
                    <p>Statistical Power: {result.statistical_power:.4f}</p>
                    <p>Confidence Interval: [{result.confidence_interval[0]:.4f}, {result.confidence_interval[1]:.4f}]</p>
                </div>

                <h2>Graph Statistics</h2>
                <pre>{safe_json_dumps(result.graph_statistics)}</pre>
            </div>
        </body>
        </html>
        """

        filepath = f"{filename}.html"
        with open(filepath, 'w') as f:
            f.write(html_content)

        return filepath

    @staticmethod
    def _export_latex(result: MonteCarloAdvancedResult, filename: str) -> str:
        """Export results as LaTeX for academic papers."""
        latex_content = f"""
        \\documentclass{{article}}
        \\usepackage{{booktabs}}
        \\usepackage{{siunitx}}

        \\title{{DAG Validation Results for {result.plan_name}}}
        \\author{{Automated DAG Validator}}
        \\date{{\\today}}

        \\begin{{document}}

        \\maketitle

        \\section{{Results}}

        \\begin{{table}}[h]
        \\centering
        \\begin{{tabular}}{{lr}}
        \\toprule
        Metric & Value \\\\
        \\midrule
        P-value & {result.p_value:.6f} \\\\
        Bayesian Posterior & {result.bayesian_posterior:.4f} \\\\
        Statistical Power & {result.statistical_power:.4f} \\\\
        Iterations & {result.total_iterations} \\\\
        \\bottomrule
        \\end{{tabular}}
        \\caption{{Statistical validation results}}
        \\end{{table}}

        \\end{{document}}
        """

        filepath = f"{filename}.tex"
        with open(filepath, 'w') as f:
            f.write(latex_content)

        return filepath

    @staticmethod
    def _export_text(result: MonteCarloAdvancedResult, filename: str) -> str:
        """Export results as plain text."""
        text_content = f"""
        DAG VALIDATION REPORT
        =====================

        Plan: {result.plan_name}
        Timestamp: {result.timestamp}
        Seed: {result.seed}

        STATISTICAL RESULTS:
        - P-value: {result.p_value:.6f}
        - Bayesian Posterior: {result.bayesian_posterior:.4f}
        - Confidence Interval: [{result.confidence_interval[0]:.4f}, {result.confidence_interval[1]:.4f}]
        - Statistical Power: {result.statistical_power:.4f}
        - Effect Size: {result.effect_size:.4f}

        GRAPH STATISTICS:
        {safe_json_dumps(result.graph_statistics)}

        QUALITY INDICATORS:
        - Reproducible: {result.reproducible}
        - Convergence Achieved: {result.convergence_achieved}
        - Adequate Power: {result.adequate_power}
        - Robustness Score: {result.robustness_score:.4f}

        COMPUTATION:
        - Iterations: {result.total_iterations}
        - Computation Time: {result.computation_time:.2f} seconds
        """

        filepath = f"{filename}.txt"
        with open(filepath, 'w') as f:
            f.write(text_content)

        return filepath


def create_complex_causal_graph() -> AdvancedDAGValidator:
    """Create a sophisticated causal graph for testing."""
    validator = AdvancedDAGValidator(graph_type=GraphType.THEORY_OF_CHANGE)

    # Define nodes with roles and metadata
    nodes_config = [
        ("recursos_financieros", "intervention", {"budget": 100000, "currency": "USD"}),
        ("capacitacion_personal", "intervention", {"duration": 6, "format": "workshop"}),
        ("infraestructura", "intervention", {"type": "physical", "scale": "large"}),
        ("programas_intervencion", "mediator", {"intensity": "high", "frequency": "weekly"}),
        ("participacion_comunidad", "mediator", {"engagement": "active", "reach": 500}),
        ("cambio_comportamiento", "outcome", {"measurement": "survey", "validity": 0.85}),
        ("mejora_indicadores", "outcome", {"indicators": ["health", "education"], "trend": "positive"}),
        ("impacto_social", "outcome", {"sustainability": "long_term", "scope": "community"}),
        ("contexto_social", "covariate", {"stability": "medium", "influence": "moderate"}),
        ("factores_externos", "covariate", {"control": "low", "variability": "high"})
    ]

    for name, role, metadata in nodes_config:
        validator.add_node(name, role=role, metadata=metadata)

    # Add complex causal pathways with weights
    pathways = [
        (["recursos_financieros", "capacitacion_personal"], [0.8]),
        (["recursos_financieros", "infraestructura"], [0.7]),
        (["capacitacion_personal", "programas_intervencion"], [0.9]),
        (["infraestructura", "programas_intervencion"], [0.6]),
        (["programas_intervencion", "participacion_comunidad"], [0.85]),
        (["participacion_comunidad", "cambio_comportamiento"], [0.75]),
        (["cambio_comportamiento", "mejora_indicadores"], [0.8]),
        (["mejora_indicadores", "impacto_social"], [0.9]),
        (["contexto_social", "participacion_comunidad"], [0.4]),
        (["factores_externos", "impacto_social"], [0.3])
    ]

    for pathway, weights in pathways:
        validator.add_causal_pathway(pathway, weights)

    return validator


class DAGValidationSuite:
    """Comprehensive validation suite for multiple DAGs and hypotheses."""

    def __init__(self):
        self.validators: Dict[str, AdvancedDAGValidator] = {}
        self.comparison_results: Dict[str, Any] = {}

    def add_validator(self, name: str, validator: AdvancedDAGValidator):
        """Add a validator to the suite."""
        self.validators[name] = validator

    def run_comprehensive_validation(self, plan_name: str,
                                     iterations: int = 10000) -> Dict[str, MonteCarloAdvancedResult]:
        """Run validation across all registered validators."""
        results = {}

        for name, validator in self.validators.items():
            print(f"Validating: {name}")
            result = validator.calculate_acyclicity_pvalue_advanced(
                f"{plan_name}_{name}", iterations
            )
            results[name] = result

            # Export results
            validator.export_results(result, "json", f"results_{name}")

        self.comparison_results[plan_name] = results
        return results

    def compare_models(self, plan_name: str) -> Dict[str, Any]:
        """Compare validation results across different models."""
        if plan_name not in self.comparison_results:
            return {}

        results = self.comparison_results[plan_name]
        comparison = {}

        for name, result in results.items():
            comparison[name] = {
                'p_value': result.p_value,
                'bayesian_posterior': result.bayesian_posterior,
                'statistical_power': result.statistical_power,
                'robustness_score': result.robustness_score,
                'computation_time': result.computation_time
            }

        return comparison


def demonstrate_advanced_features():
    """Demonstrate the advanced features of the DAG validator."""
    print("=== ADVANCED DAG VALIDATION DEMONSTRATION ===\n")

    # Create complex graph
    validator = create_complex_causal_graph()

    # Display graph statistics
    stats = validator.get_advanced_graph_stats()
    print("GRAPH STATISTICS:")
    print(f"Nodes: {stats['total_nodes']}, Edges: {stats['total_edges']}")
    print(f"Density: {stats['graph_density']:.4f}")
    print(f"Role Distribution: {stats['role_distribution']}")
    print()

    # Run advanced validation
    plan_name = "comprehensive_validation_2024"
    print(f"RUNNING ADVANCED VALIDATION FOR: {plan_name}")

    result = validator.calculate_acyclicity_pvalue_advanced(
        plan_name,
        iterations=5000,
        test_strategy="comprehensive"
    )

    # Display key results
    print("\nKEY RESULTS:")
    print(f"P-value: {result.p_value:.6f}")
    print(f"Bayesian Posterior: {result.bayesian_posterior:.4f}")
    print(f"95% Confidence Interval: [{result.confidence_interval[0]:.4f}, {result.confidence_interval[1]:.4f}]")
    print(f"Statistical Power: {result.statistical_power:.4f}")
    print(f"Robustness Score: {result.robustness_score:.4f}")
    print(f"Computation Time: {result.computation_time:.2f} seconds")

    # Check reproducibility
    reproducible = validator.verify_advanced_reproducibility(plan_name)
    print(f"Advanced Reproducibility: {reproducible}")

    # Export results
    export_file = validator.export_results(result, "json")
    print(f"Results exported to: {export_file}")

    # Run sensitivity analysis
    sensitivity = validator.perform_sensitivity_analysis(plan_name)
    print(f"\nSENSITIVITY ANALYSIS:")
    print(f"Average Sensitivity: {sensitivity['average_sensitivity']:.4f}")
    print(
        f"Most Sensitive Edge: {max(sensitivity['edge_sensitivity'].items(), key=lambda x: x[1]) if sensitivity['edge_sensitivity'] else 'N/A'}")

    return validator, result


if __name__ == "__main__":
    # Run the demonstration
    validator, results = demonstrate_advanced_features()

    print("\n=== VALIDATION COMPLETE ===")
    print("The advanced DAG validator provides:")
    print("✓ Sophisticated statistical testing")
    print("✓ Bayesian and frequentist approaches")
    print("✓ Sensitivity and robustness analysis")
    print("✓ Parallel processing capabilities")
    print("✓ Multiple export formats")
    print("✓ Comprehensive reporting")