"""Lightweight document embedding mapper with duplicate handling.

This module provides an in-memory embedding index that keeps the
relationship between raw text segments, their source pages, and the
vector representations generated by a deterministic embedding model.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, List, Sequence, Tuple

import numpy as np


@dataclass(frozen=True)
class _ModelConfig:
    """Configuration holder to mimic sentence-transformer interface."""

    dimension: int = 384


class _DeterministicEmbeddingModel:
    """Simple embedding model that yields deterministic vectors per text."""

    def __init__(self, dimension: int = 384) -> None:
        self.model_config = _ModelConfig(dimension=dimension)

    def _vector_from_text(self, text: str) -> np.ndarray:
        """Create a deterministic unit vector based on the input text."""
        # Use a reproducible random generator seeded from the text hash
        seed = hash(text) % (2**32)
        rng = np.random.default_rng(seed)
        vec = rng.standard_normal(self.model_config.dimension)
        norm = np.linalg.norm(vec) or 1.0
        return vec / norm

    def encode(self, texts: Sequence[str]) -> np.ndarray:
        """Encode a list of texts into deterministic embeddings."""
        embeddings = [self._vector_from_text(text) for text in texts]
        return np.vstack(embeddings)


class DocumentEmbeddingMapper:
    """Manage text segments, their pages, and vector embeddings."""

    def __init__(self, dimension: int = 384) -> None:
        self.model = _DeterministicEmbeddingModel(dimension=dimension)
        self.text_segments: List[str] = []
        self.page_numbers: List[int] = []
        self.embeddings: np.ndarray | None = None

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _append_embeddings(self, new_embeddings: np.ndarray) -> None:
        if self.embeddings is None:
            self.embeddings = new_embeddings.astype(np.float32)
        else:
            self.embeddings = np.vstack((self.embeddings, new_embeddings))

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def verify_parallel_arrays(self) -> bool:
        return len(self.text_segments) == len(self.page_numbers) and (
            self.embeddings is None
            or self.embeddings.shape[0] == len(self.text_segments)
        )

    def add_document_segments(
        self, segments: Iterable[Tuple[str, int]]
    ) -> List[int]:
        segments = list(segments)
        if not segments:
            return []

        texts = [text for text, _ in segments]
        pages = [page for _, page in segments]
        new_embeddings = self.model.encode(texts)

        start_index = len(self.text_segments)
        indices = list(range(start_index, start_index + len(segments)))

        self.text_segments.extend(texts)
        self.page_numbers.extend(pages)
        self._append_embeddings(new_embeddings)

        return indices

    def get_segment_info(self, index: int) -> Tuple[str, int]:
        if index < 0 or index >= len(self.text_segments):
            raise IndexError("Segment index out of range")
        return self.text_segments[index], self.page_numbers[index]

    def get_embedding(self, index: int) -> np.ndarray:
        if self.embeddings is None:
            raise ValueError("No embeddings have been generated yet")
        if index < 0 or index >= self.embeddings.shape[0]:
            raise IndexError("Embedding index out of range")
        return self.embeddings[index]

    def find_duplicate_indices(self, text: str, page: int | None = None) -> List[int]:
        duplicates: List[int] = []
        for idx, (segment_text, segment_page) in enumerate(
            zip(self.text_segments, self.page_numbers)
        ):
            if segment_text == text and (page is None or segment_page == page):
                duplicates.append(idx)
        return duplicates

    def _cosine_similarity(self, vector: np.ndarray, matrix: np.ndarray) -> np.ndarray:
        # Avoid division by zero when norms are extremely small
        norm_vec = np.linalg.norm(vector) or 1.0
        norm_matrix = np.linalg.norm(matrix, axis=1)
        norm_matrix[norm_matrix == 0] = 1.0
        return matrix @ vector / (norm_matrix * norm_vec)

    def similarity_search(self, query: str, k: int = 5) -> List[Tuple[int, str, int, float]]:
        if self.embeddings is None or len(self.text_segments) == 0:
            return []

        query_embedding = self.model.encode([query])[0]
        scores = self._cosine_similarity(query_embedding, self.embeddings)
        top_indices = np.argsort(scores)[::-1][:k]

        results: List[Tuple[int, str, int, float]] = []
        for idx in top_indices:
            text, page = self.get_segment_info(int(idx))
            results.append((int(idx), text, page, float(scores[idx])))
        return results

    def batch_similarity_search(
        self, queries: Sequence[str], k: int = 5
    ) -> List[List[Tuple[int, str, int, float]]]:
        if not queries:
            return []

        return [self.similarity_search(query, k=k) for query in queries]

    def get_statistics(self) -> dict:
        total_segments = len(self.text_segments)
        unique_pairs = set(zip(self.text_segments, self.page_numbers))
        pair_counts: dict[tuple[str, int], int] = {}
        for text, page in zip(self.text_segments, self.page_numbers):
            key = (text, page)
            pair_counts[key] = pair_counts.get(key, 0) + 1

        duplicate_segments = total_segments - len(unique_pairs)
        max_duplicates = max(pair_counts.values(), default=0)

        return {
            "total_segments": total_segments,
            "unique_segments": len(unique_pairs),
            "duplicate_segments": duplicate_segments,
            "pages_covered": len(set(self.page_numbers)),
            "embedding_dimension": self.model.model_config.dimension,
            "max_duplicates_for_segment": max_duplicates,
        }


__all__ = ["DocumentEmbeddingMapper"]
